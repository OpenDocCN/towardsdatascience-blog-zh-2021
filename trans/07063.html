<html>
<head>
<title>COVID-19 Policies Multi-classification with Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于神经网络的新冠肺炎保单多分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/covid-19-policies-multi-classification-with-neural-network-d85cbc0f90c0?source=collection_archive---------25-----------------------#2021-06-26">https://towardsdatascience.com/covid-19-policies-multi-classification-with-neural-network-d85cbc0f90c0?source=collection_archive---------25-----------------------#2021-06-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="dc96" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><em class="kf">记录我调整ML模型的学习曲线，该模型可以分类20个类(将来甚至更多)</em></h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/7694e02bae4cd659e720db347ba2300f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*n73f9KYOR5oOEygD"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><a class="ae kw" href="https://unsplash.com/@thenewmalcolm?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">奥比·奥尼耶多尔</a>在<a class="ae kw" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="6e6a" class="kx ky iq bd kz la lb lc ld le lf lg lh jw li jx lj jz lk ka ll kc lm kd ln lo bi translated"><strong class="ak">小介绍~ </strong></h1><p id="672b" class="pw-post-body-paragraph lp lq iq lr b ls lt jr lu lv lw ju lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">在进入工作的细节之前，我想留出一些空间来分享这个项目和这篇文章背后的动机的总结。</p><p id="84ca" class="pw-post-body-paragraph lp lq iq lr b ls ml jr lu lv mm ju lx ly mn ma mb mc mo me mf mg mp mi mj mk ij bi translated">我在Coronanet.org度过了一段美好的时光，这是一个了不起的跨国团队，有500多名研究助理在一起工作，收集全球新冠肺炎政策的数据。我们的最终目标是以最有用的形式发布尽可能多的政策数据集，然后这些数据集可以用于不同类型的研究/新闻工作。我们组织收集的数据的许多方法之一是手动通读并将其分类为20种不同的策略类型(社交距离、锁定等，随便你怎么说)和其他子类型。事实上，许多保单看起来并不清楚它们属于任何特定的保单类型，而且每天都有大量的传入数据，这证明很难仔细检查每一份保单的标签准确性。这就是我们开始用不同的ML模型进行实验的地方，以寻找一种可以识别错误编码并比人工工作更有效地清除这些错误的算法。</p><p id="c387" class="pw-post-body-paragraph lp lq iq lr b ls ml jr lu lv mm ju lx ly mn ma mb mc mo me mf mg mp mi mj mk ij bi translated">随着对最佳模式的探索的继续，已经并将会有许多变化。因此，我想记录一些我个人取得的进步、成功和局限性，因为我在不断学习和寻找更好的模型，希望我能为团队的共同目标做出有意义的贡献，无论大小！所以…</p><h1 id="6361" class="kx ky iq bd kz la lb lc ld le lf lg lh jw li jx lj jz lk ka ll kc lm kd ln lo bi translated">让我们开始吧</h1><h2 id="6241" class="mq ky iq bd kz mr ms dn ld mt mu dp lh ly mv mw lj mc mx my ll mg mz na ln nb bi translated">数据收集</h2><p id="d08e" class="pw-post-body-paragraph lp lq iq lr b ls lt jr lu lv lw ju lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">我是从我们机构的公开回购上刮来的数据，可以在这里<a class="ae kw" href="https://github.com/CoronaNetDataScience/corona_tscs/tree/master/data/CoronaNet/data_country/coronanet_release" rel="noopener ugc nofollow" target="_blank">找到</a>。我使用s <em class="nc"> elenium </em>只从一些选定的国家收集数据集(这些国家有许多已公布的政策记录)。[注:如果您也使用Chrome，您需要先将<a class="ae kw" href="https://chromedriver.chromium.org/downloads" rel="noopener ugc nofollow" target="_blank">Chrome driver<em class="nc"/></a><em class="nc"/>下载到您的电脑上，以便运行并打开浏览器。</p><pre class="kh ki kj kk gt nd ne nf ng aw nh bi"><span id="c48b" class="mq ky iq ne b gy ni nj l nk nl">from selenium import webdriver</span><span id="7436" class="mq ky iq ne b gy nm nj l nk nl">driver=webdriver.Chrome('/Users/irenechang/Downloads/chromedriver')<br/>driver.get('<a class="ae kw" href="https://github.com/CoronaNetDataScience/corona_tscs/tree/master/data/CoronaNet/data_country/coronanet_release'" rel="noopener ugc nofollow" target="_blank">https://github.com/CoronaNetDataScience/corona_tscs/tree/master/data/CoronaNet/data_country/coronanet_release'</a>)</span></pre><p id="df61" class="pw-post-body-paragraph lp lq iq lr b ls ml jr lu lv mm ju lx ly mn ma mb mc mo me mf mg mp mi mj mk ij bi translated">由于数据集文件名具有相同的格式，我将为所有我想要抓取并解析成这种格式的国家名称制作一个列表。</p><pre class="kh ki kj kk gt nd ne nf ng aw nh bi"><span id="072a" class="mq ky iq ne b gy ni nj l nk nl"><em class="nc">#showing only a few countries</em><br/>country_names = ['Germany', 'United States of America', 'Spain', 'Australia', 'India']<br/>countries_to_scrape = []</span><span id="4a71" class="mq ky iq ne b gy nm nj l nk nl"><em class="nc">#parse string together</em><br/>for country in country_names:<br/>    countries_to_scrape.append("coronanet_release_" + country + ".csv")</span></pre><p id="2aa3" class="pw-post-body-paragraph lp lq iq lr b ls ml jr lu lv mm ju lx ly mn ma mb mc mo me mf mg mp mi mj mk ij bi translated">selenium的魅力来了，当你站在浏览器自己移动到页面的地方时，它使所有的刮擦都变得愉快。我们会的</p><p id="459f" class="pw-post-body-paragraph lp lq iq lr b ls ml jr lu lv mm ju lx ly mn ma mb mc mo me mf mg mp mi mj mk ij bi translated">(1)为每个链接创建xpath</p><p id="e5fb" class="pw-post-body-paragraph lp lq iq lr b ls ml jr lu lv mm ju lx ly mn ma mb mc mo me mf mg mp mi mj mk ij bi translated">(2)进入每个链接→导航到下载按钮→点击它，</p><p id="6ef0" class="pw-post-body-paragraph lp lq iq lr b ls ml jr lu lv mm ju lx ly mn ma mb mc mo me mf mg mp mi mj mk ij bi translated">以下代码片段实现了全部自动化:</p><pre class="kh ki kj kk gt nd ne nf ng aw nh bi"><span id="2c01" class="mq ky iq ne b gy ni nj l nk nl"><em class="nc"># get the link for each data file</em><br/>urls = []<br/>for country in countries_to_scrape:<br/>    xPath = "//a[<a class="ae kw" href="http://twitter.com/title" rel="noopener ugc nofollow" target="_blank">@title</a>='"+ country + "']"<br/>    print(xPath)<br/>    link = driver.find_element_by_xpath(xPath).get_attribute("href")<br/>    urls.append(link)</span></pre><p id="ecfe" class="pw-post-body-paragraph lp lq iq lr b ls ml jr lu lv mm ju lx ly mn ma mb mc mo me mf mg mp mi mj mk ij bi translated">[注意:如果您以前没有使用过，XPath是一种查询语言，就像SQL一样，但用于XML文档，也就是可以在“开发人员工具”中查看的web“结构”。这里，我们想要提取这些国家的csv的链接(也称为<em class="nc"> href </em>属性)，这些链接可以在&lt; a &gt;标签中找到，这些标签碰巧具有与所示文本相同的标题(<em class="nc">标题</em>属性)</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/a7fec19d5eec276e23da332d39eb40eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*ysFHkJwGdn6Pd_duFuSpOg.png"/></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">突出显示的部分是我们要导航到的地方</p></figure><pre class="kh ki kj kk gt nd ne nf ng aw nh bi"><span id="dac3" class="mq ky iq ne b gy ni nj l nk nl">//a[@title='coronanet_release_Germany.csv']<br/>//a[@title='coronanet_release_United States of America.csv']<br/>//a[@title='coronanet_release_Spain.csv']<br/>//a[@title='coronanet_release_Australia.csv']<br/>//a[@title='coronanet_release_India.csv']</span></pre><p id="714e" class="pw-post-body-paragraph lp lq iq lr b ls ml jr lu lv mm ju lx ly mn ma mb mc mo me mf mg mp mi mj mk ij bi translated">☝️是来自<em class="nc"> print() </em>的结果，这些是我们的<em class="nc">XPath</em>。接下来，按照类似的过程，我们进入第二阶段，获取原始csv文件，并连接所有这些数据帧。</p><pre class="kh ki kj kk gt nd ne nf ng aw nh bi"><span id="268e" class="mq ky iq ne b gy ni nj l nk nl">from parsel import Selector<br/>csv_urls = []<br/>for url in urls:<br/>    driver.get(url)<br/>    sel = Selector(text=driver.page_source)<br/>    raw_csv = driver.find_element_by_id('raw-url').get_attribute("href")<br/>    csv_urls.append(raw_csv)</span><span id="7f93" class="mq ky iq ne b gy nm nj l nk nl">dfs = []<br/>for csv in csv_urls:<br/>    # read each csv into a separate dataframe<br/>    dfs.append(pd.read_csv(csv))</span><span id="29cf" class="mq ky iq ne b gy nm nj l nk nl">big_frame = pd.concat(dfs, ignore_index=True)</span></pre><h2 id="1206" class="mq ky iq bd kz mr ms dn ld mt mu dp lh ly mv mw lj mc mx my ll mg mz na ln nb bi translated">数据预处理</h2><p id="fde3" class="pw-post-body-paragraph lp lq iq lr b ls lt jr lu lv lw ju lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">从big_frame开始，我只对2列感兴趣，“描述”和“类型”。</p><p id="d609" class="pw-post-body-paragraph lp lq iq lr b ls ml jr lu lv mm ju lx ly mn ma mb mc mo me mf mg mp mi mj mk ij bi translated">对于其余的预处理步骤，我密切关注Miguel的笔记本，可以在这里<a class="ae kw" href="https://github.com/miguelfzafra/Latest-News-Classifier/blob/master/0.%20Latest%20News%20Classifier/03.%20Feature%20Engineering/03.%20Feature%20Engineering.ipynb" rel="noopener ugc nofollow" target="_blank">找到</a>。他每一步的讲解都非常清晰，非常有帮助，尤其是对于我这样刚开始处理文本数据的人。一般来说，核对表应该是:</p><ol class=""><li id="09b8" class="no np iq lr b ls ml lv mm ly nq mc nr mg ns mk nt nu nv nw bi translated">特殊字符和标点符号</li><li id="aefa" class="no np iq lr b ls nx lv ny ly nz mc oa mg ob mk nt nu nv nw bi translated">下放箱子</li><li id="53b8" class="no np iq lr b ls nx lv ny ly nz mc oa mg ob mk nt nu nv nw bi translated">删除号码</li><li id="cb8c" class="no np iq lr b ls nx lv ny ly nz mc oa mg ob mk nt nu nv nw bi translated">词干化和词汇化</li><li id="6d52" class="no np iq lr b ls nx lv ny ly nz mc oa mg ob mk nt nu nv nw bi translated">停用字词(国家名称、地区名称以及在正式文件和政策中使用的字词)</li><li id="b8bc" class="no np iq lr b ls nx lv ny ly nz mc oa mg ob mk nt nu nv nw bi translated">标签编码</li><li id="4f26" class="no np iq lr b ls nx lv ny ly nz mc oa mg ob mk nt nu nv nw bi translated">列车-测试分离</li></ol><p id="af94" class="pw-post-body-paragraph lp lq iq lr b ls ml jr lu lv mm ju lx ly mn ma mb mc mo me mf mg mp mi mj mk ij bi translated">下面是我的步骤5的一个片段:</p><pre class="kh ki kj kk gt nd ne nf ng aw nh bi"><span id="a624" class="mq ky iq ne b gy ni nj l nk nl">#stopwords<br/>nltk.download('stopwords')<br/>stop_words = list(stopwords.words('english'))</span><span id="7f36" class="mq ky iq ne b gy nm nj l nk nl"># include country names in stopwords<br/>country_text = []<br/>for text in df["description_1"].tolist():<br/>    for c in pycountry.countries:<br/>        if c.name.lower() in text:<br/>            text = re.sub(c.name.lower(), '', text)<br/>    country_text.append(text)</span><span id="8799" class="mq ky iq ne b gy nm nj l nk nl">df["description_2"] = country_text</span><span id="3a74" class="mq ky iq ne b gy nm nj l nk nl">for stop_word in stop_words:<br/>    regex_stopword = r"\b" + stop_word + r"\b"     </span><span id="8cce" class="mq ky iq ne b gy nm nj l nk nl">    df['description_2'] = df['description_2'].str.replace(regex_stopword, '')</span></pre><h2 id="059e" class="mq ky iq bd kz mr ms dn ld mt mu dp lh ly mv mw lj mc mx my ll mg mz na ln nb bi translated">符合模型</h2><p id="88ae" class="pw-post-body-paragraph lp lq iq lr b ls lt jr lu lv lw ju lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">首先，由于数据是不平衡的，所以我们将计算类权重，以备以后在神经网络中使用:</p><pre class="kh ki kj kk gt nd ne nf ng aw nh bi"><span id="aec9" class="mq ky iq ne b gy ni nj l nk nl">from sklearn.utils import class_weight</span><span id="be3a" class="mq ky iq ne b gy nm nj l nk nl">class_weights = list(class_weight.compute_class_weight('balanced',                np.unique(df2['type']), f2['type']))</span><span id="b383" class="mq ky iq ne b gy nm nj l nk nl">class_weights.sort()<br/>class_weights</span><span id="4407" class="mq ky iq ne b gy nm nj l nk nl"><em class="nc"># assign labels to these weights</em><br/>weights = {}<br/>for index, weight in enumerate(class_weights) :<br/>    weights[index] = weight<br/>    <br/>weights</span></pre><p id="2ae8" class="pw-post-body-paragraph lp lq iq lr b ls ml jr lu lv mm ju lx ly mn ma mb mc mo me mf mg mp mi mj mk ij bi translated">Out: <br/> {0: 0.3085932721712538，<br/>1:0.481440839694655，<br/>2:0.491285285，29698，149953，<br/>3:0.682745，50609，<br/>4:0.66。5566646</p><p id="19d1" class="pw-post-body-paragraph lp lq iq lr b ls ml jr lu lv mm ju lx ly mn ma mb mc mo me mf mg mp mi mj mk ij bi translated">接下来，我们将我们训练和测试集转换成可用的对象形式，并打印出来以供查看:</p><pre class="kh ki kj kk gt nd ne nf ng aw nh bi"><span id="a063" class="mq ky iq ne b gy ni nj l nk nl">dataset_train = tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values))<br/>dataset_test = tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values))</span><span id="a241" class="mq ky iq ne b gy nm nj l nk nl">for text, target in dataset_train.take(5):<br/>    print('Desc: {}, label: {}'.format(text, target))</span></pre><p id="9d80" class="pw-post-body-paragraph lp lq iq lr b ls ml jr lu lv mm ju lx ly mn ma mb mc mo me mf mg mp mi mj mk ij bi translated">以下是贴有标签的前5个描述的结果:</p><pre class="kh ki kj kk gt nd ne nf ng aw nh bi"><span id="a1b3" class="mq ky iq ne b gy ni nj l nk nl">Desc: b' us embassy montevideo consular section  closed   routine consular services   notice    emergency situations   considered   time ', label: b'Restriction and Regulation of Government Services'</span><span id="703e" class="mq ky iq ne b gy nm nj l nk nl">Desc: b' pennsylvania governor signed  senate bill    waives  requirement  schools    session  least  days  provides  continuity  education plans  ensures school employees  paid   closure   provides  secretary  education  authority  waive student teacher  standardized assessments   march  ', label: b'Closure and Regulation of Schools'</span><span id="b920" class="mq ky iq ne b gy nm nj l nk nl">Desc: b"dumka   district   n state  jharkhand   defined  government services  would remain operational   lockdown   follows    law  order agencies -  function without  restrictions   officers attendance - compulsory  grade ''  'b' officers  reduced  %  grade 'c'      district administration  treasury officials -   function  restricted staff    wildlife  forest officers -  function  taking necessary precautions ", label: b'Restriction and Regulation of Government Services'</span><span id="9fd5" class="mq ky iq ne b gy nm nj l nk nl">Desc: b'texas     reopening  non-essential businesses starting may      per executive order ga-    hair salons  barber shops  nail salons   tanning salons must maintain mandatory ft distance  patrons    swimming pools  operate  % capacity     may   jail time  removed   enforcement mechanism  update   may   gov  abbott removed jail time   penalty  failing  follow covid- restrictions ', label: b'Restriction and Regulation of Businesses'</span><span id="33b7" class="mq ky iq ne b gy nm nj l nk nl">Desc: b' public safety curfew   imposed  mobile  alabama  effective   pm  april      remain  effect  april      persons shall remain   places  residence  shall    public places ', label: b'Lockdown'</span></pre><p id="eea1" class="pw-post-body-paragraph lp lq iq lr b ls ml jr lu lv mm ju lx ly mn ma mb mc mo me mf mg mp mi mj mk ij bi translated">我们的下一个任务是一次性编码20个标签。为此，我创建了一个散列表，以便按类别代码方便地查找值:</p><pre class="kh ki kj kk gt nd ne nf ng aw nh bi"><span id="424f" class="mq ky iq ne b gy ni nj l nk nl">table = tf.lookup.StaticHashTable(<br/>    initializer=tf.lookup.KeyValueTensorInitializer(<br/>        keys=tf.constant(list(category_codes.keys())),<br/>        values = tf.constant(list(category_codes.values()))<br/>    ),<br/>    default_value=tf.constant(-1),<br/>    name="target_encoding"<br/>)</span><span id="76df" class="mq ky iq ne b gy nm nj l nk nl"><a class="ae kw" href="http://twitter.com/tf" rel="noopener ugc nofollow" target="_blank">@tf</a>.function<br/>def target(x):<br/>    return table.lookup(x)</span></pre><p id="21af" class="pw-post-body-paragraph lp lq iq lr b ls ml jr lu lv mm ju lx ly mn ma mb mc mo me mf mg mp mi mj mk ij bi translated">使用这些函数，我们可以将训练集和测试集中的标签编码成数字数组。用<em class="nc"> next() </em>打印出结果</p><pre class="kh ki kj kk gt nd ne nf ng aw nh bi"><span id="c608" class="mq ky iq ne b gy ni nj l nk nl">def fetch(text, labels):<br/>    return text, tf.one_hot(target(labels), 19)</span><span id="b5f8" class="mq ky iq ne b gy nm nj l nk nl">train_data_fetch = dataset_train.map(fetch)<br/>test_data_fetch = dataset_test.map(fetch)</span><span id="08f1" class="mq ky iq ne b gy nm nj l nk nl">next(iter(train_data_fetch))</span></pre><p id="d5ba" class="pw-post-body-paragraph lp lq iq lr b ls ml jr lu lv mm ju lx ly mn ma mb mc mo me mf mg mp mi mj mk ij bi translated">输出(第一个tf。张量是我们的训练数据，第二个是我们转换的训练标签):</p><pre class="kh ki kj kk gt nd ne nf ng aw nh bi"><span id="22fa" class="mq ky iq ne b gy ni nj l nk nl">(&lt;tf.Tensor: shape=(), dtype=string, numpy=b'   april   mha writes  states  ensure smooth harvesting  sowing operations   maintaining social distancing   -day lockdown  fight covid-  -  union ministry  home affairs  mha   sent  advisory   states regarding granting   exception  agricultural operations  lockdown restrictions  fight covid-  keeping  mind  harvesting  sowing season   -   advisory  exceptions   allowed  farming operations  farmers  farm workers  procurement  agricultural productions  operation  mandis  movement  harvesting  sowing related machinery  etc '&gt;,<br/> &lt;tf.Tensor: shape=(19,), dtype=float32, numpy=<br/> array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,<br/>        0., 0.], dtype=float32)&gt;)</span></pre><h2 id="c544" class="mq ky iq bd kz mr ms dn ld mt mu dp lh ly mv mw lj mc mx my ll mg mz na ln nb bi translated">开始创建模型</h2><p id="8b49" class="pw-post-body-paragraph lp lq iq lr b ls lt jr lu lv lw ju lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">首先，我们需要在将文本数据输入模型之前对其进行标记。我使用了谷歌已经实现的模型，它将为我们创建一个嵌入层。简而言之，它是这样工作的:</p><blockquote class="oc od oe"><p id="acdf" class="lp lq nc lr b ls ml jr lu lv mm ju lx of mn ma mb og mo me mf oh mp mi mj mk ij bi translated">谷歌开发了一个<strong class="lr ir">嵌入</strong>模型，<strong class="lr ir"> nnlm </strong> -en- <strong class="lr ir"> dim128 </strong>，这是一个基于令牌的<strong class="lr ir">文本嵌入</strong>训练模型，在英文谷歌新闻200B语料库上使用了一个三隐层前馈神经网络语言模型。这个模型将文本的任何主体映射到128维的嵌入中</p></blockquote><p id="4142" class="pw-post-body-paragraph lp lq iq lr b ls ml jr lu lv mm ju lx ly mn ma mb mc mo me mf mg mp mi mj mk ij bi translated">要使用它，只需传入该型号的链接，该链接可以在这个<a class="ae kw" href="https://tfhub.dev/google/nnlm-en-dim128/2" rel="noopener ugc nofollow" target="_blank">网站</a>上找到【注意:您可能需要检查以确保您正在使用的版本是最新的，否则它会给你错误】:</p><pre class="kh ki kj kk gt nd ne nf ng aw nh bi"><span id="1ae4" class="mq ky iq ne b gy ni nj l nk nl">embedding = "<a class="ae kw" href="https://tfhub.dev/google/nnlm-en-dim128/2" rel="noopener ugc nofollow" target="_blank">https://tfhub.dev/google/nnlm-en-dim128/2</a>"<br/>hub_layer = hub.KerasLayer(embedding, output_shape=[128],input_shape=[], dtype=tf.string,<br/>                          trainable=True)<br/>hub_layer(train_data[:1])</span></pre><p id="305f" class="pw-post-body-paragraph lp lq iq lr b ls ml jr lu lv mm ju lx ly mn ma mb mc mo me mf mg mp mi mj mk ij bi translated">潜入这一层给了我们什么:</p><pre class="kh ki kj kk gt nd ne nf ng aw nh bi"><span id="6c32" class="mq ky iq ne b gy ni nj l nk nl">&lt;tf.Tensor: shape=(1, 128), dtype=float32, numpy=<br/>array([[ 1.85923278e-01,  3.82673025e-01,  8.69123638e-02,<br/>        -2.36745372e-01, -1.19763926e-01, -5.65516986e-02,<br/>         2.45870352e-01,  5.02816178e-02, -2.10541233e-01,<br/>        -4.42932360e-02,  1.28366366e-01,  1.47269592e-01,<br/>         1.41175740e-04,  4.45434526e-02,  2.13784329e-03,<br/>         1.61750317e-01, -2.32903764e-01, -2.10702419e-01,<br/>        -2.09106982e-01,  1.55449033e-01,  4.53584678e-02,<br/>         4.31233309e-02,  1.48296393e-02, -1.68935359e-01,<br/>         1.12579502e-01, -1.03304483e-01,  1.61703452e-01,<br/>         2.13061482e-01, -4.74388264e-02,  1.27027377e-01,<br/>        -3.04564610e-02, -1.92816645e-01, -3.22420187e-02, ... ]])</span></pre><p id="0809" class="pw-post-body-paragraph lp lq iq lr b ls ml jr lu lv mm ju lx ly mn ma mb mc mo me mf mg mp mi mj mk ij bi translated">神经网络模型的陷阱是过度拟合，一如既往，因此我们通常必须使用脱落层来正则化它。Dropout layer是一种正则化神经网络的工具，其中许多层的随机输出被忽略，因此稀疏网络将不得不适应纠正来自先前层的错误(阅读更多<a class="ae kw" href="https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/" rel="noopener ugc nofollow" target="_blank">此处</a>)。</p><p id="fdf4" class="pw-post-body-paragraph lp lq iq lr b ls ml jr lu lv mm ju lx ly mn ma mb mc mo me mf mg mp mi mj mk ij bi translated">在不知道我的模型是否会出现这种情况的情况下，我训练了两个模型，一个有脱落层，另一个没有，并比较了它们的结果。对于这些模型，不要忘记使用早期停止。</p><pre class="kh ki kj kk gt nd ne nf ng aw nh bi"><span id="2e94" class="mq ky iq ne b gy ni nj l nk nl"><em class="nc">#build the basic model without dropout</em><br/>model_wo_dropout = tf.keras.Sequential()<br/>model_wo_dropout.add(hub_layer)<br/>for units in [128, 128, 64, 32]:<br/>    model_wo_dropout.add(tf.keras.layers.Dense(units, activation='relu'))<br/>model_wo_dropout.add(tf.keras.layers.Dense(19, activation='softmax'))</span><span id="3f8a" class="mq ky iq ne b gy nm nj l nk nl">model_wo_dropout.summary()</span><span id="a8ad" class="mq ky iq ne b gy nm nj l nk nl"><em class="nc">#compile the model</em><br/>model_wo_dropout.compile(optimizer='adam',loss=tf.keras.losses.CategoricalCrossentropy(from_logits=<strong class="ne ir">True</strong>), metrics=['accuracy'])</span><span id="6ecc" class="mq ky iq ne b gy nm nj l nk nl">train_data_fetch = train_data_fetch.shuffle(70000).batch(512)<br/>test_data_fetch = test_data_fetch.batch(512)</span><span id="b226" class="mq ky iq ne b gy nm nj l nk nl"><em class="nc">#fit the model</em><br/>from keras import callbacks<br/>earlystopping = callbacks.EarlyStopping(monitor="val_loss", mode="min", patience=5, restore_best_weights=<strong class="ne ir">True</strong>, verbose=1)<br/>text_classifier_wo_dropout = model_wo_dropout.fit(train_data_fetch, epochs=25, validation_data = test_data_fetch,<br/>                   verbose=1, class_weight=weights, callbacks =[earlystopping])</span></pre><p id="7447" class="pw-post-body-paragraph lp lq iq lr b ls ml jr lu lv mm ju lx ly mn ma mb mc mo me mf mg mp mi mj mk ij bi translated">对我们的测试和训练集的评估~</p><pre class="kh ki kj kk gt nd ne nf ng aw nh bi"><span id="167b" class="mq ky iq ne b gy ni nj l nk nl"><em class="nc">#training errors</em><br/>y_train_pred = model_wo_dropout.predict(train_data)<br/><br/>print(classification_report(train_labels.numpy().argmax(axis=1), y_train_pred.argmax(axis=1)))</span><span id="899f" class="mq ky iq ne b gy nm nj l nk nl"><em class="nc"># test errors</em><br/>test_data, test_labels = next(iter(dataset_test.map(fetch).batch(test_length)))<br/>y_pred = model_wo_dropout.predict(test_data)<br/><br/>print(classification_report(test_labels.numpy().argmax(axis=1), y_pred.argmax(axis=1)))</span></pre><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/0af8c760c4a20f614ec202bfa81c0727.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*UyNlw3efoqdSK0z0iC3CgQ.png"/></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">培训结果</p></figure><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/8e62d6951c3437a37344d062cb42bb7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*E9dqUdasIojSsHpMPK3hgg.png"/></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">测试结果</p></figure><p id="2b70" class="pw-post-body-paragraph lp lq iq lr b ls ml jr lu lv mm ju lx ly mn ma mb mc mo me mf mg mp mi mj mk ij bi translated">结果，与我在建立这个神经网络之前尝试作为基线模型的逻辑回归模型相比，提高了10%，这是一个非常好的景象。我将在完成对漏失层模型的实现后进一步讨论这些结果。</p><h2 id="0990" class="mq ky iq bd kz mr ms dn ld mt mu dp lh ly mv mw lj mc mx my ll mg mz na ln nb bi translated">带有脱层的模型，用于确认过度拟合</h2><p id="ca62" class="pw-post-body-paragraph lp lq iq lr b ls lt jr lu lv lw ju lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">正如我之前所说，为了确认我们的模型是否过度拟合数据，我训练了一个具有几个丢弃层的二级神经网络，并比较了结果。我们可以在0.1-1.0的范围内网格搜索最佳压差参数。这里，我用0.3作为参数。</p><p id="6881" class="pw-post-body-paragraph lp lq iq lr b ls ml jr lu lv mm ju lx ly mn ma mb mc mo me mf mg mp mi mj mk ij bi translated">我们必须重建训练和测试设备。这种模型的唯一区别在于图层构建过程。从这一点开始，所有的一切都与没有脱落层的模型相同。重要的是要注意，我们不应该在输出层(也就是最后一层)之前有脱落层，因为输出层不能再纠正前面层的错误，所以添加另一个脱落层最终会损害模型的性能(我就是这种情况)。</p><pre class="kh ki kj kk gt nd ne nf ng aw nh bi"><span id="a45a" class="mq ky iq ne b gy ni nj l nk nl"><em class="nc"># re-create train, test data for fitting the model</em><br/>train_data_fetch_dropout = dataset_train.map(fetch)<br/>test_data_fetch_dropout = dataset_test.map(fetch)</span><span id="6b81" class="mq ky iq ne b gy nm nj l nk nl"><em class="nc"># build a similar model but with the dropout layers</em><br/>model = tf.keras.Sequential()<br/>model.add(hub_layer)<br/><strong class="ne ir">for</strong> units <strong class="ne ir">in</strong> [128, 128, 64]:<br/>    model.add(tf.keras.layers.Dense(units, activation='relu'))<br/>    model.add(tf.keras.layers.Dropout(0.3))<br/>model.add(tf.keras.layers.Dense(32, activation='relu'))<br/>model.add(tf.keras.layers.Dense(19, activation='softmax'))<br/><br/>model.summary()</span></pre><p id="ba81" class="pw-post-body-paragraph lp lq iq lr b ls ml jr lu lv mm ju lx ly mn ma mb mc mo me mf mg mp mi mj mk ij bi translated">结果如下:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/7e85a1236e004cc63ba29a225c62bd12.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*hGOQgx-4aIqWiWUTVe1o0g.png"/></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">培训结果</p></figure><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/a9a0cdae75df5b228201521785fe27b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*IZxqRwpNlObX3mTPoHhMEA.png"/></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">测试结果</p></figure><h1 id="c9e3" class="kx ky iq bd kz la lb lc ld le lf lg lh jw li jx lj jz lk ka ll kc lm kd ln lo bi translated">~讨论和扩展问题:</h1><ul class=""><li id="850e" class="no np iq lr b ls lt lv lw ly om mc on mg oo mk op nu nv nw bi translated">有和没有脱落层的模型的性能基本相同。即使使用了脱落层，训练和测试误差之间的差异也没有改善。事实上，与没有dropout层的模型相比，使用dropout层的模型无法预测更多类别的标签(许多类别的准确率为0%)。然而，在检查了大数据集和正则化没有显著改善模型的事实之后，我不认为过度拟合在这个模型中是个大问题。进一步的分析可能涉及在不同的辍学率值上拟合模型，以确认性能没有提高是因为我们没有选择一个好的辍学率，还是因为数据集本身。</li><li id="819d" class="no np iq lr b ls nx lv ny ly nz mc oa mg ob mk op nu nv nw bi translated">逻辑回归以及其他ML模型在训练和测试误差之间没有那么大的差距。可以说，这些最大似然模型不像神经网络模型那样遭受过度拟合，然而，假定我们已经使用了最佳参数，这些模型很难进一步改进。与此同时，神经网络有很好的机会超越这一点。由于我们的兴趣在于获得更准确的预测，我们将使用神经网络。</li><li id="737e" class="no np iq lr b ls nx lv ny ly nz mc oa mg ob mk op nu nv nw bi translated">据观察，在我添加了更多的数据点(更多的国家)后，两个模型的总体准确性都提高了。这让我想知道获得所有可用的数据是否会进一步提高准确性。</li><li id="ca6b" class="no np iq lr b ls nx lv ny ly nz mc oa mg ob mk op nu nv nw bi translated">在不同的拟合尝试中，精度显示出轻微的波动(由于神经网络模型可能落入“局部最大/最小”陷阱的事实)，但平均而言，结果在范围[0.66-0.71]内，这比我拟合的机器学习模型的平均结果(约59-60%)增加了约5-10%。查看混淆矩阵，每个类别中的指标也有所改善。</li><li id="ed8f" class="no np iq lr b ls nx lv ny ly nz mc oa mg ob mk op nu nv nw bi translated">另一个可能影响模型预测能力的来源是，许多类别(以健康资源、健康监控和健康测试为例)共享许多共同的词，因此接下来的步骤之一是识别和开发在不同类别中出现很多的停用词的集合，以减少这些类别之间的相似性，并查看这是否会进一步提高性能。</li></ul><p id="1f30" class="pw-post-body-paragraph lp lq iq lr b ls ml jr lu lv mm ju lx ly mn ma mb mc mo me mf mg mp mi mj mk ij bi translated">总之，这个神经网络模型非常简单，肯定有改进的空间，重点是数据预处理过程(更多停用词、过采样/欠采样)，以及进一步的交叉验证以达到平均精度。话虽如此，这对我来说是一次非常充实的学习经历，也是第一次接触神经网络！如果你已经走了这么远，非常感谢你！我期待着更新你更多关于我的这个模型的未来版本。</p><p id="327a" class="pw-post-body-paragraph lp lq iq lr b ls ml jr lu lv mm ju lx ly mn ma mb mc mo me mf mg mp mi mj mk ij bi translated">这个笔记本的完整版本可以在这个Github repo中找到:</p><div class="oq or gp gr os ot"><a href="https://github.com/irenechang1510/Topic-classification-NN/blob/main/Topic%20classification%20with%20Neural%20Network.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="ou ab fo"><div class="ov ab ow cl cj ox"><h2 class="bd ir gy z fp oy fr fs oz fu fw ip bi translated">Irene Chang 1510/主题-分类-NN</h2><div class="pa l"><h3 class="bd b gy z fp oy fr fs oz fu fw dk translated">在GitHub上创建一个帐户，为Irene Chang 1510/Topic-class ification-NN开发做贡献。</h3></div><div class="pb l"><p class="bd b dl z fp oy fr fs oz fu fw dk translated">github.com</p></div></div><div class="pc l"><div class="pd l pe pf pg pc ph kq ot"/></div></div></a></div><p id="8bf1" class="pw-post-body-paragraph lp lq iq lr b ls ml jr lu lv mm ju lx ly mn ma mb mc mo me mf mg mp mi mj mk ij bi translated">请随时通过我的LinkedIn联系我，如果您有任何问题，我很乐意与您联系！</p><p id="9bb7" class="pw-post-body-paragraph lp lq iq lr b ls ml jr lu lv mm ju lx ly mn ma mb mc mo me mf mg mp mi mj mk ij bi translated">参考资料:</p><p id="62da" class="pw-post-body-paragraph lp lq iq lr b ls ml jr lu lv mm ju lx ly mn ma mb mc mo me mf mg mp mi mj mk ij bi translated">[1] Dipanjan (DJ) Sarkar，<a class="ae kw" rel="noopener" target="_blank" href="/deep-transfer-learning-for-natural-language-processing-text-classification-with-universal-1a2c69e5baa9">面向自然语言处理的深度迁移学习—具有通用嵌入的文本分类</a> (2018)，面向数据科学</p><p id="68c3" class="pw-post-body-paragraph lp lq iq lr b ls ml jr lu lv mm ju lx ly mn ma mb mc mo me mf mg mp mi mj mk ij bi translated">[2] Miguel Fernández Zafra，<a class="ae kw" rel="noopener" target="_blank" href="/text-classification-in-python-dd95d264c802">Python中的文本分类</a> (2019)，towardsdatascience</p></div></div>    
</body>
</html>