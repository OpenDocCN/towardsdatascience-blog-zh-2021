<html>
<head>
<title>The Revolving Door For Machine Learning Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习模型的旋转门</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-revolving-door-for-machine-learning-models-14bdfc870906?source=collection_archive---------34-----------------------#2021-08-12">https://towardsdatascience.com/the-revolving-door-for-machine-learning-models-14bdfc870906?source=collection_archive---------34-----------------------#2021-08-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/a8337e5a846e2928b6eb1ff3f27934a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vcOPlHGHyHn3cfIF.jpg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">旋转门，ImaArtist，<a class="ae jg" href="https://pixabay.com/photos/dancer-empire-state-architecture-4645662/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>。</p></figure><div class=""/><div class=""><h2 id="9ad8" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">算法如何在非标准用例中重用。</h2></div><p id="1cc2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">众所周知，学者们会借用自然和其他领域的想法，同时以略微不同的方式将它们应用于新问题。这些天，在数据科学中，我们看到许多想法、技术和科学进步应用于三大领域(NLP、视觉、音频)。</p><p id="419e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我想论证一下，现在专攻某个领域，让你积累跨领域的工具，可以让你在领域间的转换变得轻松。一旦你掌握了统计学、概率论、信息论、数学、算法和机器学习的基础知识，你就会意识到你可以在各种用途和用例中重用几乎所有的算法。</p><p id="c162" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我认为方法和技术并不直接附属于一个单一的研究领域，事实上，大量的方法可以在不同的研究领域中互换使用。</p><h2 id="181f" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">核心概念</h2><ol class=""><li id="d54d" class="mn mo jj la b lb mp le mq lh mr ll ms lp mt lt mu mv mw mx bi translated">简化问题，以选择正确的工具。</li><li id="f32e" class="mn mo jj la b lb my le mz lh na ll nb lp nc lt mu mv mw mx bi translated">理解算法之间概念上的相似性，以及如何重用它们。</li><li id="05ec" class="mn mo jj la b lb my le mz lh na ll nb lp nc lt mu mv mw mx bi translated">借鉴其他领域的观点是数据科学家的基本特征。</li><li id="b30a" class="mn mo jj la b lb my le mz lh na ll nb lp nc lt mu mv mw mx bi translated">了解某些工具相对于其他工具的优势和劣势。</li></ol><p id="b40a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在接下来的部分，我将主要关注算法的相似性和再利用(第2点和第3点)，然而，这些点直接反映了借鉴思想和理解你的交易工具的重要性(第1点和第4点)。</p><h2 id="2d22" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">再利用算法</h2><p id="f90d" class="pw-post-body-paragraph ky kz jj la b lb mp kk ld le mq kn lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">似乎机器学习中的每个子领域都是其他领域的衍生物，然而，真实的“情况”要复杂得多。这个漂亮的模式可能应该有从每个子字段到其他子字段的连接边。让我们看看这些算法和想法是如何在不同领域之间传播的。</p><p id="fb5c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们从基础开始。几乎每个函数都可以用作预处理方法、特征工程、增强、模型或集成。</p><h2 id="5513" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">回归、分类和排名</h2><p id="7898" class="pw-post-body-paragraph ky kz jj la b lb mp kk ld le mq kn lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">回归算法可以用作分类或排序，反之亦然。这使我们能够变得灵活，通过降低复杂性来简化问题的解决。例如，您正在预测一个连续变量，但是，您可以简单地将其存储起来，并将解决方案转换为分类问题，而不是使用回归算法来解决它。</p><p id="93a9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">吴恩达已经很好地展示了逻辑回归是SVM的远亲。无独有偶，支持向量机可以用于<a class="ae jg" href="https://www.cs.cornell.edu/people/tj/svm_light/svm_rank.html" rel="noopener ugc nofollow" target="_blank">排序</a>、<a class="ae jg" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html" rel="noopener ugc nofollow" target="_blank">回归</a>、<a class="ae jg" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html" rel="noopener ugc nofollow" target="_blank">分类</a>、<a class="ae jg" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html" rel="noopener ugc nofollow" target="_blank">聚类</a>，大概还有很多其他用法。巧合的是，内核不仅限于SVM，我们还有<a class="ae jg" href="https://pydml.readthedocs.io/en/latest/dml.kda.html" rel="noopener ugc nofollow" target="_blank"> Kernel-PCA </a>、<a class="ae jg" href="https://tslearn.readthedocs.io/en/stable/auto_examples/clustering/plot_kernel_kmeans.html" rel="noopener ugc nofollow" target="_blank"> Kernel-Kmeans </a>和<a class="ae jg" href="https://pydml.readthedocs.io/en/latest/dml.kda.html" rel="noopener ugc nofollow" target="_blank"> Kernel-LDA </a>。</p><p id="b161" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">排名可以用于<a class="ae jg" href="https://www.researchgate.net/publication/225139953_Recommender_Systems_Using_Support_Vector_Machines" rel="noopener ugc nofollow" target="_blank">推荐</a>，<a class="ae jg" rel="noopener" target="_blank" href="/using-snorkel-for-multi-label-annotation-cc2aa217986a">概率</a>可以用于<a class="ae jg" href="https://medium.com/@m_n_malaeb/recall-and-precision-at-k-for-recommender-systems-618483226c54" rel="noopener">推荐</a>，推荐算法可以用于排名，并不止于此。</p><h2 id="0380" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">概率与信息论</h2><p id="92f7" class="pw-post-body-paragraph ky kz jj la b lb mp kk ld le mq kn lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">概率已被用于并整合到各种算法中，如朴素贝叶斯、贝叶斯信任网络、马尔可夫模型、<a class="ae jg" href="https://hmmlearn.readthedocs.io/en/latest/tutorial.html" rel="noopener ugc nofollow" target="_blank">隐马尔可夫模型</a>、条件随机场，以及可以说是利用了<a class="ae jg" href="https://arxiv.org/abs/1301.3781" rel="noopener ugc nofollow" target="_blank">移动窗口</a>和/或<a class="ae jg" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力机制</a>的任何算法。巧合的是，这些算法是在借鉴前人思想的基础上发展起来的。此外，使用概率时最常提到的软与硬的概念可用于标注、逻辑图层、堆叠算法、预测、弱监督、注意力等。</p><p id="bc5c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/entropy-cross-entropy-kl-divergence-binary-cross-entropy-cb8f72e72e65#:~:text=Cross%20Entropy%20is%20the%20expected,for%20a%20predicted%20distribution%20Q.&amp;text=This%20is%20the%20Cross%20Entropy,true%20Entropy%20H%20for%20P.">信息论</a> &amp;熵在实际中以各种形式和形态无处不在，如<a class="ae jg" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif" rel="noopener ugc nofollow" target="_blank">特征选择</a>，在<a class="ae jg" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy" rel="noopener ugc nofollow" target="_blank">神经网络</a>等。</p><h2 id="3d50" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">密度、分布和聚集</h2><p id="aeb5" class="pw-post-body-paragraph ky kz jj la b lb mp kk ld le mq kn lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">以下算法可以归为一个算法家族，尽管它们并不都是直接相关的。比如<a class="ae jg" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html" rel="noopener ugc nofollow" target="_blank"> PDF </a>，<a class="ae jg" href="https://scikit-learn.org/stable/modules/density.html" rel="noopener ugc nofollow" target="_blank"> KDE </a>，<a class="ae jg" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html" rel="noopener ugc nofollow" target="_blank"> K-means，</a> <a class="ae jg" href="https://scikit-learn.org/0.16/modules/generated/sklearn.mixture.GMM.html" rel="noopener ugc nofollow" target="_blank"> GMM </a>，自组织映射(<a class="ae jg" href="https://medium.com/@abhinavr8/self-organizing-maps-ff5853a118d4" rel="noopener"> SOM </a>，是自聚类的神经网络结构)。我们可以在各种看似相似但解决略有不同的问题的用例中使用它们，例如，我们可以将密度函数视为平均值、聚类、分布、生成模型和扩充。</p><h2 id="ab9c" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">CRF vs注意力</h2><p id="7242" class="pw-post-body-paragraph ky kz jj la b lb mp kk ld le mq kn lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">具有注意力的移动窗口可以被认为是条件随机场(CRF ),前者查看先前的隐藏状态，而后者查看先前的标签。<a class="ae jg" href="https://github.com/keras-team/keras-contrib/blob/master/keras_contrib/layers/crf.py" rel="noopener ugc nofollow" target="_blank"> CRF </a>也被应用为神经层，并在<a class="ae jg" href="https://academic.oup.com/bioinformatics/article/34/8/1381/4657076" rel="noopener ugc nofollow" target="_blank">猜想</a>中被关注使用。</p><h2 id="feb1" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">主题建模</h2><p id="5fd3" class="pw-post-body-paragraph ky kz jj la b lb mp kk ld le mq kn lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">在NLP中，我们可以使用通常与主题建模(TM)相关的算法、<a class="ae jg" href="https://radimrehurek.com/gensim/models/ldamodel.html" rel="noopener ugc nofollow" target="_blank">算法</a>，例如<a class="ae jg" href="https://jonathan-hui.medium.com/machine-learning-latent-dirichlet-allocation-lda-1d9d148f13a4" rel="noopener">潜在狄利克雷分配</a>，但是我们也可以使用<a class="ae jg" href="https://radimrehurek.com/gensim/models/phrases.html" rel="noopener ugc nofollow" target="_blank">搭配</a>、<a class="ae jg" href="https://medium.com/sciforce/towards-automatic-text-summarization-extractive-methods-e8439cd54715" rel="noopener">提取</a>、<a class="ae jg" href="https://medium.com/sciforce/towards-automatic-summarization-part-2-abstractive-methods-c424386a65ea" rel="noopener">和</a> <a class="ae jg" href="https://medium.com/towards-artificial-intelligence/understanding-abstractive-text-summarization-from-scratch-baaf83d446b3" rel="noopener">抽象概括</a>、K-Means聚类中心以及无数其他方法。</p><h2 id="9939" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">神经网络</h2><p id="ea78" class="pw-post-body-paragraph ky kz jj la b lb mp kk ld le mq kn lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">例如，在神经网络中，层是可以应用于任何算法的概念。例如，<a class="ae jg" href="http://yann.lecun.com/exdb/publis/pdf/lecun-99.pdf" rel="noopener ugc nofollow" target="_blank">卷积层</a>是<a class="ae jg" href="https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4" rel="noopener ugc nofollow" target="_blank">为卷积神经网络(CNN)开发的</a>。这种类型的层是第一个将算法编码为抽象神经层的抽象层之一。这个想法后来被应用于其他各种各样的领域，比如2D &amp; <a class="ae jg" href="https://keras.io/examples/vision/3D_image_classification/" rel="noopener ugc nofollow" target="_blank"> 3D CNN的</a>，递归神经网络层<a class="ae jg" href="https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html" rel="noopener ugc nofollow" target="_blank"> RNN </a>，<a class="ae jg" href="https://github.com/keras-team/keras-contrib/blob/master/keras_contrib/layers/crf.py" rel="noopener ugc nofollow" target="_blank"> CRF层</a>。</p><p id="74b5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">RNNs可以使用有限状态自动机<a class="ae jg" href="https://arxiv.org/abs/1711.09576" rel="noopener ugc nofollow" target="_blank">近似</a>，并且自动机可以被采样以生成对手样本。</p><p id="1be4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae jg" href="https://keras.io/api/layers/regularization_layers/dropout/" rel="noopener ugc nofollow" target="_blank"> Dropout </a>图层可以被认为是随机森林。想象一下，一个深度网络包含迷你路径和一个中途退出，作为一种创建小树或树桩的方法，这些小树或树桩是在试图实现正则化以防止过度拟合时生成的。有趣的是，<a class="ae jg" href="https://jack-clark.net/2016/08/15/import-ai-issue-3-synthetic-pokemon-brain-like-ai-and-the-history-of-dropout/" rel="noopener ugc nofollow" target="_blank">的神话</a>是辍学的想法来自对银行的一次访问，而不是从随机森林中借来的。</p><h2 id="dfe9" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">Word2Vec</h2><p id="b812" class="pw-post-body-paragraph ky kz jj la b lb mp kk ld le mq kn lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">每个人都喜欢的一个抽象概念是“<a class="ae jg" href="https://arxiv.org/abs/1301.3781" rel="noopener ugc nofollow" target="_blank">嵌入</a>”概念，它已经被应用于字符、单词、标记、句子、段落和文档之间的距离。这是一个很久以前就开始的想法，想想Dummies、CountVec或TFIDF。然而，在Word2Vec (W2V)发表后，立即出现了使用这种想法的算法的爆炸。基本上，如果你可以从你的数据中创建一个“句子”，你可以在那个句子上使用W2V，并创建一个嵌入空间。</p><p id="55e5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个想法已经<a class="ae jg" href="https://github.com/MaxwellRebo/awesome-2vec" rel="noopener ugc nofollow" target="_blank">蓬勃发展</a>，W2V现在到处都在使用，从实体到歌曲列表。W2V是真正的先锋，有很多关于它的文章。然而，本文的重点是它如何与嵌入方法本身有如此多的联系。就分辨率而言，正如我提到的，你可以将任何东西推入嵌入算法，从字符、单词、句子、段落和文档等等。您可以对文本(NLP)、分类变量(<a class="ae jg" href="https://openreview.net/pdf?id=HyNxRZ9xg" rel="noopener ugc nofollow" target="_blank"> CAT2VEC </a>)、图形(<a class="ae jg" href="https://www.singlelunch.com/2020/12/28/why-im-lukewarm-on-graph-neural-networks/" rel="noopener ugc nofollow" target="_blank"> GNN </a>)进行建模，您也可以对像素进行建模。事实上，有一个<a class="ae jg" href="https://github.com/MaxwellRebo/awesome-2vec" rel="noopener ugc nofollow" target="_blank">很棒的repo </a>策划了许多*2vec算法。</p><h2 id="4f34" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">图形神经网络</h2><p id="302a" class="pw-post-body-paragraph ky kz jj la b lb mp kk ld le mq kn lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/graph-embeddings-the-summary-cc6075aba007">图形神经网络</a> (GNN)允许你将图形嵌入到嵌入空间中。GNN是建立在W2V之上的，这是一个快速发展的领域，有许多算法和改进。然而，图可以被认为是邻接表，表可以被转换成关系图，图可以被转换成压缩嵌入空间。因此，我们可以使用经典和现代领域中都存在的算法，例如，一些最常见的例子是社区检测、页面排名、深度行走、node2vec。</p><h2 id="2eba" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">信号处理</h2><p id="74f6" class="pw-post-body-paragraph ky kz jj la b lb mp kk ld le mq kn lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">取自经典信号处理的<a class="ae jg" href="https://pywavelets.readthedocs.io/en/latest/#" rel="noopener ugc nofollow" target="_blank">小波</a>的思想可以用作神经网络层，即<a class="ae jg" href="https://tslearn.readthedocs.io/en/stable/user_guide/shapelets.html" rel="noopener ugc nofollow" target="_blank">小波</a>。</p><h2 id="bcef" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">异常检测</h2><p id="ce5b" class="pw-post-body-paragraph ky kz jj la b lb mp kk ld le mq kn lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated"><a class="ae jg" href="https://pyod.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">异常检测</a>可以使用传统的基于信号的异常检测算法、PCA、自动编码器、聚类、SVM等等。</p><h2 id="7ffe" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">合奏</h2><p id="e798" class="pw-post-body-paragraph ky kz jj la b lb mp kk ld le mq kn lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">你可以用任何东西来创建集合，不仅仅是经典的模型，也可以是特征选择、工程、增强、公式、距离、深层功能，基本上是你能得到的任何功能。</p><h2 id="25bd" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">变形金刚&amp;注意</h2><p id="0a0e" class="pw-post-body-paragraph ky kz jj la b lb mp kk ld le mq kn lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">回到变形金刚&amp;注意力作为一个概念，它现在正跨越到非NLP领域，如<a class="ae jg" href="https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html" rel="noopener ugc nofollow" target="_blank">计算机视觉</a>，我们看到这种趋势正在向其他研究领域<a class="ae jg" href="https://arxiv.org/abs/2006.03265" rel="noopener ugc nofollow" target="_blank">发展</a>。</p><h2 id="93d6" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">现实世界的例子:</h2><p id="b24b" class="pw-post-body-paragraph ky kz jj la b lb mp kk ld le mq kn lg lh nd lj lk ll ne ln lo lp nf lr ls lt im bi translated">以下是一些常见的问题，为了在解决问题时利用完全不同的工具，这些问题可以用不同的方式重新表述。</p><ol class=""><li id="b13e" class="mn mo jj la b lb lc le lf lh ng ll nh lp ni lt mu mv mw mx bi translated">以大脑fMRI图像为例，它们可以用作4D数据，即作为时间序列的3D数据(脑盒)、2D数据(脑切片)，或者甚至作为1D信号的基于体素的数据。</li><li id="0b5e" class="mn mo jj la b lb my le mz lh na ll nb lp nc lt mu mv mw mx bi translated">音频也可以转换成单个信号、声谱图或时间序列；与其他类型的方法一起使用，如二维<a class="ae jg" href="https://medium.com/@mikesmales/sound-classification-using-deep-learning-8bc2aa1990b7" rel="noopener">CNN</a>、RNNs等。</li><li id="3288" class="mn mo jj la b lb my le mz lh na ll nb lp nc lt mu mv mw mx bi translated">自然语言处理模型，如GPT,<a class="ae jg" href="https://openai.com/blog/image-gpt/" rel="noopener ugc nofollow" target="_blank">可以用在图像上</a>并且具有惊人的竞争力。</li><li id="b54c" class="mn mo jj la b lb my le mz lh na ll nb lp nc lt mu mv mw mx bi translated">你可以(试着)根据股票的(开盘价/收盘价)数据来预测股票，也可以根据2D用户界面来预测，也就是你在交易者屏幕上看到的东西。你可以使用<a class="ae jg" href="https://medium.com/better-programming/predicting-the-rise-of-gamestop-gme-in-20-lines-of-code-or-less-44810c19027a" rel="noopener">情绪</a>分析基于文本预测股票。</li><li id="1fb8" class="mn mo jj la b lb my le mz lh na ll nb lp nc lt mu mv mw mx bi translated">对于信号处理，您可以利用<a class="ae jg" href="https://en.wikipedia.org/wiki/Fourier_transform" rel="noopener ugc nofollow" target="_blank">傅立叶变换</a>将信号分解为子频率，这些子频率可以在以后用作特征工程技术。例如，考虑陀螺仪或加速度计信号，您可以使用它们来检测行走、跑步和骑行信号频率，或者作为分类任务的特征。</li></ol><p id="0048" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这篇文章中，我想传达的是，我们有创造力的自由，并且有无限的方法以新的和创新的方式混合数据和算法。我分享了我对算法和机器学习世界的看法和想法，希望我已经成功说服你，我们不需要将我们的工作分成数据类型、算法家族和语言、视觉或音频等领域。</p></div><div class="ab cl nj nk hx nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="im in io ip iq"><p id="105c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢<a class="ae jg" href="https://medium.com/@TalPerry" rel="noopener">塔尔·佩里</a>、<a class="ae jg" href="https://medium.com/@ndor123" rel="noopener">纳塔内尔·达维多维茨</a>、<a class="ae jg" href="https://medium.com/@ptannor" rel="noopener">菲利普·坦诺</a>、<a class="ae jg" href="https://medium.com/@ofir.bibi" rel="noopener">奥菲尔·比比</a>、<a class="ae jg" href="https://medium.com/@dean_pleban" rel="noopener">迪安·普莱班</a>、<a class="ae jg" href="https://medium.com/@orenrazon" rel="noopener">柳文欢·拉松</a>的宝贵意见。</p></div><div class="ab cl nj nk hx nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="im in io ip iq"><p id="c8bb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Ori Cohen博士拥有计算机科学博士学位，主要研究机器学习和脑机接口(BCI)。他在一家智能城市初创公司领导了一个数据科学团队，主要利用机器和深度学习进行自然语言处理(NLP)和理解(NLU)研究。目前，他是TLV新遗迹公司在AIOps领域的首席数据科学家。他定期在Medium.com上写关于管理、流程和所有数据科学的文章。</p><p id="97f2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">联系人:<a class="ae jg" href="https://www.linkedin.com/in/cohenori/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a> | <a class="ae jg" href="https://medium.com/@cohenori" rel="noopener">中</a>|【OriCohen.com】T4|<a class="ae jg" href="https://book.mlcompendium.com" rel="noopener ugc nofollow" target="_blank">book.mlcompendium.com</a></p></div></div>    
</body>
</html>