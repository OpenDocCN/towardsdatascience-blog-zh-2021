<html>
<head>
<title>You Can Compute ROC Curve Also for Regression Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你也可以计算回归模型ROC曲线</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-calculate-roc-auc-score-for-regression-models-c0be4fdf76bb?source=collection_archive---------6-----------------------#2021-09-16">https://towardsdatascience.com/how-to-calculate-roc-auc-score-for-regression-models-c0be4fdf76bb?source=collection_archive---------6-----------------------#2021-09-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e441" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">他们可能告诉你ROC曲线下的面积不能计算连续的目标变量。他们错了。下面是如何用Python实现的。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/34efc67165bad2309cf9f0e49aacb853.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fov88h9jKwAwIzg6DRyOZg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">[作者图]</p></figure><p id="6c5e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你是一家拍卖公司的数据科学家，你的老板让你建立一个模型来预测在售物品的锤价(即最终售价)。这种模式有两个目的:</p><ol class=""><li id="4719" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">为每件商品设定一个有意义的开价；</li><li id="a5f9" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">在拍卖过程中每隔一段时间放置最贵的物品。这样，你就会保持住观众的注意力。</li></ol><p id="6dba" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因为您想要预测一个点值(以美元为单位)，所以您决定使用一个<strong class="la iu">回归模型</strong>(例如，<code class="fe mi mj mk ml b">XGBRegressor()</code>)。现在，你如何评价你的模型的性能？</p><p id="50f5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们看看Scikit的回归模型度量工具箱:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mm"><img src="../Images/bc82f57ad48c8537708403a47fc5713c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nwxedp-nyvJRKjzFMzuhNA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Scikit-learn的回归度量[ <a class="ae mn" href="https://scikit-learn.org/stable/modules/model_evaluation.html" rel="noopener ugc nofollow" target="_blank">链接</a> ]。</p></figure><p id="47e7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所有这些指标都试图量化<strong class="la iu">模型预测与实际值的差距</strong>。事实上，如果你看一看他们的公式，你总会发现这个量:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mo"><img src="../Images/a4e9397cf651f14e820590be7144249e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lDUhbGFiGoujVXRmyfpKcg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">实际值和模型预测值之间的差异。[作者图]</p></figure><p id="4374" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">换句话说，这些指标非常有助于评估<strong class="la iu">接近真实价格</strong>(第一个目标)的能力。但是它们对评估第二个目标毫无用处，第二个目标是<strong class="la iu">将物品从最贵到最便宜排序的能力</strong>。</p><p id="581e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">想要个例子吗？假设您的模型的第一个版本提供了这些结果:</p><pre class="kj kk kl km gt mp ml mq mr aw ms bi"><span id="520e" class="mt mu it ml b gy mv mw l mx my">y_true = [1000.0, 2000.0, 3000.0, 4000.0, 5000.0]<br/>y_pred = [1100.0, 1300.0, 4000.0, 4800.0, 5200.0]</span></pre><p id="89b3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从图形上看，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mz"><img src="../Images/3b530d09dfb628382df5a36185d3e013.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bCo3b9GWMa584wtWeKCmPg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在回归问题中比较真实值和预测值。[作者图]</p></figure><p id="fb08" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果拿<code class="fe mi mj mk ml b">mean_absolute_error(y_true, y_pred)</code>的话，得到560美金，大概不太好。但是，排名很完美！这意味着该模型完全能够辨别哪些物品将以更高的价格拍卖。</p><p id="c127" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是关于我们模型的一个非常重要的信息，这是我们从其他回归指标中感受不到的。但是我们如何衡量回归模型的排序能力呢？</p><p id="75ef" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">评估预测模型排序能力的最流行的指标是<code class="fe mi mj mk ml b">roc_auc_score</code>。所以，让我们试着用我们的数据来计算一下…</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/92488e300ea54ff0defe56c20bbbef25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nvzkY-O5JtlDGww7MKQraA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">[作者图]</p></figure><p id="862c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们得到一个错误！</p><p id="7437" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是因为<code class="fe mi mj mk ml b">roc_auc_score</code>只适用于分类模型，要么是一个类对rest(“ovr”)，要么是一个对一个(“ovo”)。Scikit-learn希望找到离散的类到<code class="fe mi mj mk ml b">y_true</code>和<code class="fe mi mj mk ml b">y_pred</code>，而我们传递连续的值。</p><p id="9d99" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为此，我们需要<strong class="la iu">将</strong> <code class="fe mi mj mk ml b"><strong class="la iu">roc_auc_score</strong></code> <strong class="la iu">的概念扩展到回归问题</strong>。我们将这样的度量标准称为<strong class="la iu"> </strong> <code class="fe mi mj mk ml b">regression_roc_auc_score</code>。在下一段中，我们将了解如何计算它。</p><h1 id="e9ff" class="nb mu it bd nc nd ne nf ng nh ni nj nk jz nl ka nm kc nn kd no kf np kg nq nr bi translated">寻找“回归_roc_auc_score”</h1><p id="1629" class="pw-post-body-paragraph ky kz it la b lb ns ju ld le nt jx lg lh nu lj lk ll nv ln lo lp nw lr ls lt im bi translated">直观上，<code class="fe mi mj mk ml b">regression_roc_auc_score</code>应当具有以下性质:</p><ul class=""><li id="92bf" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt nx ma mb mc bi translated">和<code class="fe mi mj mk ml b">roc_auc_score</code>一模一样，应该是有界在0(最差可能排名)和1(最好可能排名)之间，0.5表示随机排名。</li><li id="16e3" class="lu lv it la b lb md le me lh mf ll mg lp mh lt nx ma mb mc bi translated">当目标变量为二进制时，<code class="fe mi mj mk ml b">regression_roc_auc_score</code>必须给出与<code class="fe mi mj mk ml b">roc_auc_score</code>相同的结果(这样，这个度量将是<code class="fe mi mj mk ml b">roc_auc_score</code>的推广)。</li></ul><p id="fefa" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，如何获得我们正在寻找的度量？</p><p id="d4b4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe mi mj mk ml b">roc_auc_score</code>定义为ROC曲线下的面积，ROC曲线是在所有分类阈值下，x轴为假阳性率，y轴为真阳性率的曲线。但是不可能用回归方法计算FPR和TPR，所以我们不能走这条路。</p><p id="9df5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">幸运的是，我们有另一个定义。事实上，根据<a class="ae mn" href="https://it.wikipedia.org/wiki/Receiver_operating_characteristic" rel="noopener ugc nofollow" target="_blank">维基百科</a>，<code class="fe mi mj mk ml b">roc_auc_score</code>与“一个分类器将随机选择的正面实例排列得比随机选择的负面实例更高的概率”相一致。</p><p id="c778" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">换句话说，<strong class="la iu">如果我们取任意两个观察值<em class="ny"> a </em>和<em class="ny"> b </em>使得<em class="ny"> a </em> &gt; <em class="ny"> b </em>，那么</strong> <code class="fe mi mj mk ml b"><strong class="la iu">roc_auc_score</strong></code> <strong class="la iu">等于我们的模型实际上排名<em class="ny"> a </em>高于<em class="ny"> b </em> </strong>的概率。</p><p id="6f09" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个定义对我们更有用，因为它对回归也有意义(事实上<em class="ny"> a </em>和<em class="ny"> b </em>可能不限于0或1，它们可以假设任何连续值)；</p><p id="89cd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，现在计算<code class="fe mi mj mk ml b">roc_auc_score</code>要容易得多。事实上，它归结为考虑每一对可能的项目<em class="ny"> a </em>和<em class="ny"> b </em>，这样<em class="ny"> a &gt; b </em>，并计算我们的模型对<em class="ny"> a </em>的预测值实际上比对<em class="ny"> b </em>的预测值高多少倍(最终的平局将被计算一半)。那么，<code class="fe mi mj mk ml b">roc_auc_score</code>就是成功的次数除以配对总数。</p><p id="b123" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在Python中，这将是:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nz oa l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">naive_roc_auc_score的Python代码。[作者代码]</p></figure><p id="b2bd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了确保维基百科提供的定义是可靠的，让我们将我们的函数<code class="fe mi mj mk ml b">naive_roc_auc_score</code>与Scikit-learn的结果进行比较。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/9106ff7781b4f1dcf18f4c8d810a9d2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KOFMypnHqM3_sCLuVL7FSA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">[作者图]</p></figure><p id="af11" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">太好了！</p><p id="7798" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如上所述——与Scikit-learn的<code class="fe mi mj mk ml b">roc_auc_score</code>不同——这个版本也适用于连续的目标变量。让我们试一试:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/bafc02db1a1c32173eac5b6140fdb8ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wyNMitouJH-LmEhq48S61A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">[作者图]</p></figure><p id="3613" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">产量正是我们所期望的。排名是完美的，因此<code class="fe mi mj mk ml b">roc_auc_score</code>等于1。</p><h1 id="4aad" class="nb mu it bd nc nd ne nf ng nh ni nj nk jz nl ka nm kc nn kd no kf np kg nq nr bi translated">从天真到自举</h1><p id="5d45" class="pw-post-body-paragraph ky kz it la b lb ns ju ld le nt jx lg lh nu lj lk ll nv ln lo lp nw lr ls lt im bi translated">一切看起来都很棒，但上面的实现有点幼稚。事实上，<code class="fe mi mj mk ml b">naive_roc_auc_score</code>评估每一对可能的观察值。这样就需要O( <em class="ny"> n </em>)次迭代(其中<em class="ny"> n </em>为样本数)，一旦<em class="ny"> n </em>变大一点就变得不可用。</p><p id="19ce" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然而，<strong class="la iu">如果我们不需要“精确”的答案，我们可以通过自举</strong>获得一个很好的近似值。我们可以使用许多随机选择的对，而不是评估每一个可能的对(这意味着不少于<em class="ny"> n </em> *( <em class="ny"> n </em> +1)/2对)。</p><p id="8161" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这将转化为以下Python代码:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nz oa l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">回归_roc_auc_score的Python代码。[作者代码]</p></figure><p id="dc00" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe mi mj mk ml b">regression_roc_auc_score</code>有三个参数:<code class="fe mi mj mk ml b">y_true</code>、<code class="fe mi mj mk ml b">y_pred</code>和<code class="fe mi mj mk ml b">num_rounds</code>。如果<code class="fe mi mj mk ml b">num_rounds</code>是整数，则作为随机对的个数来考虑(近似解)。然而，您也可以通过将字符串<code class="fe mi mj mk ml b">"exact"</code>传递给<code class="fe mi mj mk ml b">num_rounds</code>来计算“精确的”分数(即所有可能的配对)。</p><h1 id="d707" class="nb mu it bd nc nd ne nf ng nh ni nj nk jz nl ka nm kc nn kd no kf np kg nq nr bi translated">给我看一些真实的数据！</h1><p id="365a" class="pw-post-body-paragraph ky kz it la b lb ns ju ld le nt jx lg lh nu lj lk ll nv ln lo lp nw lr ls lt im bi translated">你很想看看函数<code class="fe mi mj mk ml b">regression_roc_auc_score</code>在真实数据集上的结果吗？我也是，所以让我们在一个经典数据集上使用它来解决回归问题:来自<a class="ae mn" href="http://lib.stat.cmu.edu/datasets/" rel="noopener ugc nofollow" target="_blank"> StatLib </a>的California Housing(数据可以直接从<a class="ae mn" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html#sklearn.datasets.fetch_california_housing" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a>导入，这是BSD许可下的)。</p><p id="c907" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">目标变量是加州各区的房价中值。该数据集由20，640个样本和8个观察到的特征组成。</p><pre class="kj kk kl km gt mp ml mq mr aw ms bi"><span id="c053" class="mt mu it ml b gy mv mw l mx my">from sklearn.datasets import fetch_california_housing<br/>from sklearn.model_selection import train_test_split</span><span id="c163" class="mt mu it ml b gy od mw l mx my">X, y = fetch_california_housing(return_X_y = True, as_frame = True)<br/>y *= 100000</span><span id="3cf3" class="mt mu it ml b gy od mw l mx my">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 4321)</span></pre><p id="d0ac" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，让我们在训练集上尝试许多不同的模型，然后在测试集上计算一些指标(包括前面段落中定义的<code class="fe mi mj mk ml b">regression_roc_auc_score</code>)。</p><pre class="kj kk kl km gt mp ml mq mr aw ms bi"><span id="10a0" class="mt mu it ml b gy mv mw l mx my">from sklearn.dummy import DummyRegressor<br/>from sklearn.neighbors import KNeighborsRegressor<br/>from sklearn.linear_model import LinearRegression<br/>from sklearn.svm import SVR<br/>from sklearn.neural_network import MLPRegressor<br/>from sklearn.tree import DecisionTreeRegressor<br/>from sklearn.ensemble import GradientBoostingRegressor<br/>from xgboost import XGBRegressor<br/>from catboost import CatBoostRegressor<br/>from sklearn.metrics import mean_absolute_error, median_absolute_error, roc_auc_score, r2_score, explained_variance_score</span><span id="fb7f" class="mt mu it ml b gy od mw l mx my">modelnames = [<br/> ‘DummyRegressor()’, <br/> ‘KNeighborsRegressor()’,<br/> ‘LinearRegression()’, <br/> ‘SVR()’,<br/> ‘MLPRegressor(hidden_layer_sizes = (16, 16))’,<br/> ‘DecisionTreeRegressor()’, <br/> ‘GradientBoostingRegressor()’,<br/> ‘XGBRegressor()’,<br/> ‘CatBoostRegressor()’<br/>]</span><span id="a274" class="mt mu it ml b gy od mw l mx my">metricnames = [<br/> ‘mean_absolute_error’, <br/> ‘median_absolute_error’,<br/> ‘r2_score’,<br/> ‘explained_variance_score’,<br/> ‘regression_roc_auc_score’<br/>]</span><span id="f393" class="mt mu it ml b gy od mw l mx my">metrics = pd.DataFrame(index = modelnames, columns = metricnames)</span><span id="cdc6" class="mt mu it ml b gy od mw l mx my">for modelname in modelnames:<br/>  model = eval(modelname)<br/>  pred_test = model.fit(X_train, y_train).predict(X_test)<br/>  for metricname in metricnames:<br/>    metrics.loc[modelname, metricname] = eval(f'{metricname}(y_test, pred_test)')</span></pre><p id="9ee6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面的图比较了所有训练模型的<code class="fe mi mj mk ml b">regression_roc_auc_score</code>和<code class="fe mi mj mk ml b">mean_absolute_error</code>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/d8aa9a52e36550ba047ef91ba983f43f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PlvQS4hDYgqWi7EXA6Yd3w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">比较不同模型的regression_roc_auc_score和mean_absolute_error。[作者图]</p></figure><p id="d6b7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如我们所料，这两个指标是反向相关的。当然，较低的<code class="fe mi mj mk ml b">mean_absolute_error</code>往往与较高的<code class="fe mi mj mk ml b">regression_roc_auc_score</code>联系在一起。</p><p id="7695" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">后一个指标提供了关于模型性能的额外知识:在计算了<code class="fe mi mj mk ml b">regression_roc_auc_score</code>之后，我们可以说，在给定<em class="ny"> a </em> &gt; <em class="ny"> b </em>的情况下，Catboost估计<em class="ny"> a </em>比<em class="ny"> b </em>更高的值的概率接近90%。</p></div><div class="ab cl of og hx oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="im in io ip iq"><p id="9507" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢您的阅读！我希望你喜欢这篇文章。</p><p id="3cd2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我感谢反馈和建设性的批评。如果你想谈论这篇文章或其他相关话题，你可以发短信给我<a class="ae mn" href="https://www.linkedin.com/in/samuelemazzanti/" rel="noopener ugc nofollow" target="_blank">我的Linkedin联系人</a>。</p></div></div>    
</body>
</html>