<html>
<head>
<title>Start with Graph Convolutional Neural Networks using DGL</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从使用DGL的图形卷积神经网络开始</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/start-with-graph-convolutional-neural-networks-using-dgl-cf9becc570e1?source=collection_archive---------15-----------------------#2021-07-27">https://towardsdatascience.com/start-with-graph-convolutional-neural-networks-using-dgl-cf9becc570e1?source=collection_archive---------15-----------------------#2021-07-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="aa8b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">轻松的介绍</h2></div><p id="484b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我查阅了一些现有的库来做图卷积神经网络(GCNN的),虽然总的来说它们很好，但我总是回到DGL，因为它有很好的文档和许多例子，以及其他东西[1]。在这里，我想分享我对GCNN氏症研究中一个经典例子的回顾，当然是使用DGL的CORA数据集。CORA数据集是一个引用网络，其中节点是文章，边是它们之间的引用。下面的gif有助于直观地了解这些联系。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi le"><img src="../Images/83f987251e910e4fc6aacc71e674bf89.png" data-original-src="https://miro.medium.com/v2/resize:fit:872/1*kDDkleXGIuU277-AWSiskg.gif"/></div><p class="lm ln gj gh gi lo lp bd b be z dk translated">用Pyvis可视化CORA图。作者GIF。</p></figure><p id="28e9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有7类2708个节点，每个节点与1433个特征相关联[2]。在这里，我们将使用该数据集来执行半监督分类任务，以预测已知少量节点的节点类(七个之一)。在这种情况下，已知的节点的数量是140，如在DGL实现的那样，但是当全部信息可用时，可以使用不同的数量。在开始之前，我们必须安装DGL库，目前是V0.7。然后我们继续以通常的方式导入一些模块，</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="lr ls l"/></div></figure><p id="5791" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，我们必须以下面的形式加载CORA数据集，</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="lr ls l"/></div></figure><p id="eaf2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在第4行中，我们将<em class="lq"> g </em>设置为图形对象，然后我们检索一些张量。<em class="lq">特征</em>张量具有2708个节点的1433个特征，<em class="lq">标签</em>张量具有为每个节点分配从0到6的数字作为标签的条目。另外两个张量，<em class="lq"> train_mask </em>和<em class="lq"> test_mask </em>只是<em class="lq"> </em>得到<em class="lq">真</em>或<em class="lq">假</em>如果节点分别为训练或测试。在下表中，我们可以看到该图在DGL的值:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="lr ls l"/></div></figure><p id="a240" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们必须定义图的卷积，但在此之前，简单回顾一下公式是很重要的。我们回想一下，原则上需要图的邻接矩阵<strong class="kk iu"> <em class="lq"> A </em> </strong>，但它是根据这些方程进行变换的，其中<strong class="kk iu"> <em class="lq"> I </em> </strong>是单位矩阵[3]:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/e74f9bc6148fb38b78182955741d1de4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*RzsJ66kpczWCjAB8a2Hu6A.png"/></div></figure><p id="11da" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将根据以下等式在python类中定义图形卷积:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi lu"><img src="../Images/d8019d213227a97dc5fd3d2709075781.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EwNf_mlV2ehLxylB62t69w.png"/></div></div></figure><p id="b0f4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里<em class="lq"> x1 </em>和<em class="lq"> x2 </em>分别是第一和第二卷积。在DGL，这可以通过调用<em class="lq"> GraphConv </em>模块来轻松完成，在这种情况下，该模块执行parentesis ( <em class="lq"> AXW +b </em>)之间的操作，因为默认激活函数被设置为<em class="lq"> None: </em></p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="lr ls l"/></div></figure><p id="585b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意，在正向方法中，我们按照上面的等式定义了<em class="lq"> x1 </em>和<em class="lq"> x2 </em>。下图显示了矩阵的大小是如何影响的。首先，在Conv1中，<em class="lq"> AX </em>是邻接矩阵(<em class="lq"> A </em>)与特征矩阵(<em class="lq"> X </em>)的矩阵乘法，给出2708x1433的矩阵。权重矩阵<em class="lq"> W </em>因此具有1433行和8*16=128列(这个数字是任意的，但是工作良好)。我们以2708x128的矩阵<em class="lq"> x1 </em>结束。其次，我们按照相同的过程对x1进行第二次卷积，但这一次我们只以7个特征(与总类别数相同)结束，从而得到2708x7的矩阵<em class="lq"> x2 </em>:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi lz"><img src="../Images/9c78ba3578477cf2a5aad4e3af851c7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qIgK_q6HW7Dr_vasowXiBQ.png"/></div></div><p class="lm ln gj gh gi lo lp bd b be z dk translated">两个回旋的示意图。图片作者。</p></figure><p id="9137" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们必须使火车的一部分。在这里，我们使用Adam optimizer和log_softmax进行了多达200个历元，这对于分类任务来说是很好的。为了检索损失、准确性和特征预测的值，我们在第4到第6行中引入了<em class="lq">损失列表</em>、<em class="lq"> acc列表</em>和<em class="lq">所有日志</em>。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="lr ls l"/></div></figure><p id="e0b2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">运行这段代码，经过200个周期后，我们得到大约0.78的精度，如下图所示:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/77f0499689118eee28718ceacbd85874.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*AfXTMN4oOBHeR9ItiCczEQ.png"/></div><p class="lm ln gj gh gi lo lp bd b be z dk translated">200个历元后的精确度。图片作者。</p></figure><p id="8a7e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，如果希望查看预测的要素和标注，我们可以执行以下代码，其中最后一个时期(此处为199)的结果存储在数据帧中，并对要素应用<em class="lq"> argmax </em>运算，我们获得给出类的较高值的索引(0到6之间的数字):</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="lr ls l"/></div></figure><p id="4209" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">下面是数据帧前5行的输出，其中7列是学习的特征值，最后一列是该类的结果:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="lr ls l"/></div></figure><p id="6330" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">仅此而已。现在你可以尝试一些其他的例子或者其他的GCN变体，比如SAGEConv，门控图卷积层或者图注意力层，这些都包含在大DGL库中。</p><p id="6bef" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">试试这个COLAB笔记本中的代码:</p><div class="mb mc gp gr md me"><a href="https://github.com/napoles-uach/MediumPost/blob/main/CoraDGL.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="mf ab fo"><div class="mg ab mh cl cj mi"><h2 class="bd iu gy z fp mj fr fs mk fu fw is bi translated">位于主那不勒斯的medium post/corad GL . ipynb-uach/medium post</h2><div class="ml l"><h3 class="bd b gy z fp mj fr fs mk fu fw dk translated">中型笔记本。在GitHub上创建一个帐户，为napoles-uach/MediumPost开发做出贡献。</h3></div><div class="mm l"><p class="bd b dl z fp mj fr fs mk fu fw dk translated">github.com</p></div></div><div class="mn l"><div class="mo l mp mq mr mn ms lk me"/></div></div></a></div><p id="27ec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我希望你喜欢这个讲座，如果是的话，请鼓掌50次！！</p><p id="9670" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在推特上关注我→ <a class="ae mt" href="https://twitter.com/napoles3D" rel="noopener ugc nofollow" target="_blank"> @napoles3d </a></p><p id="bbeb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">参考资料:</p><p id="45a2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[1]<a class="ae mt" href="https://www.dgl.ai/" rel="noopener ugc nofollow" target="_blank">https://www.dgl.ai/</a></p><p id="bdd9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2]<a class="ae mt" href="https://ojs.aaai.org//index.php/aimagazine/article/view/2157" rel="noopener ugc nofollow" target="_blank">https://ojs . aaai . org//index . PHP/aimagazine/article/view/2157</a></p><p id="7561" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae mt" href="https://arxiv.org/abs/1609.02907" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1609.02907</a></p></div></div>    
</body>
</html>