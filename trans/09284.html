<html>
<head>
<title>How to Handle the Exposure Offset with Boosted Trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何处理增强树的曝光偏移</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-handle-the-exposure-offset-with-boosted-trees-fd09cc946837?source=collection_archive---------16-----------------------#2021-08-28">https://towardsdatascience.com/how-to-handle-the-exposure-offset-with-boosted-trees-fd09cc946837?source=collection_archive---------16-----------------------#2021-08-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d4f4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">保险业从GLM/GAM模式转向GBM模式的三种方式。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/2ed708608c1372b4d507197103dcf127.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1LQt3fUOBd24__D0iQeMzA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@homajob?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">斯科特·格雷厄姆</a>在<a class="ae ky" href="https://unsplash.com/s/photos/insurance?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><p id="2e67" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在保险环境中，索赔频率建模的一个常见任务是处理不同的风险等级。暴露是一个宽泛的术语，<a class="ae ky" href="https://www.insuranceopedia.com/definition/72/exposure" rel="noopener ugc nofollow" target="_blank">保险媒体</a>将其定义为“<em class="lv">对损失或风险</em>的敏感性”，但在我们的例子中，它将与时间类似。</p><p id="d2a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们预计，如果一个实体遭受某项风险达一年之久，其索赔频率会高于仅遭受该风险一周的情况。如果我们有历史索赔信息，但不是我们观察到的所有政策都持续了一年，那么在建模时必须考虑到这一点。传统上，GLM或GAM模型用于预测索赔频率，这些包中的大多数都有一个所谓的偏移参数，让您自动添加暴露的影响。</p><blockquote class="lw"><p id="09ee" class="lx ly it bd lz ma mb mc md me mf lu dk translated">本质上，我们要求模型根据可用数据进行预测，但我们对暴露部分进行微观管理。</p></blockquote><p id="17db" class="pw-post-body-paragraph kz la it lb b lc mg ju le lf mh jx lh li mi lk ll lm mj lo lp lq mk ls lt lu im bi translated">如果您碰巧切换到梯度增强树模型，您将很快意识到在这些包中没有显式的偏移参数。在各种论坛上可以找到很多关于这个话题的讨论，有不同的解决方案，但我还没有真正找到一个值得信赖的综合来源。我决定我将得到这个结束，并彻底调查哪种方法工作，为什么。</p><h1 id="af1a" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">摘要</h1><p id="2bcc" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">我们将模拟一个数据集，并根据泊松分布(保险业中一个非常常见的假设)定义索赔计数。测试是在R中用LightGBM包完成的，但是应该很容易将结果转换成Python或其他包，如XGBoost。</p><p id="1215" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们将研究3种方法来处理不同程度的暴露。我们将看看预测有多准确，然后深入研究提升树算法，找出它们是如何工作的。</p><p id="b738" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我提到的，最初的动机是一个保险问题，但我们所谈论的将完全适用于任何其他你有时间因素的原则。这不是LightGBM的一般教程。然而，我在文章的最后列出了一些通用的技巧。</p><p id="4b09" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">完整代码在一个R脚本中，<strong class="lb iu">可以从</strong> <a class="ae ky" href="https://github.com/MatePocs/quick_projects/blob/main/lightgbm_exposure.R" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> my GitHub </strong> </a>中获取。</p><blockquote class="ni nj nk"><p id="5e6e" class="kz la lv lb b lc ld ju le lf lg jx lh nl lj lk ll nm ln lo lp nn lr ls lt lu im bi translated"><strong class="lb iu">重要提示</strong>:就像你在网上读到的任何东西一样，仔细阅读这些信息。这些软件包很可能会改变现状。总是自己尝试和测试事情。</p></blockquote><h1 id="8897" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">我们的测试环境</h1><h2 id="7ddf" class="no mm it bd mn np nq dn mr nr ns dp mv li nt nu mx lm nv nw mz lq nx ny nb nz bi translated">r包</h2><p id="8362" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">我们需要以下两个R包:</p><ul class=""><li id="f743" class="oa ob it lb b lc ld lf lg li oc lm od lq oe lu of og oh oi bi translated"><a class="ae ky" href="https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html" rel="noopener ugc nofollow" target="_blank">数据表</a>；</li><li id="0b6a" class="oa ob it lb b lc oj lf ok li ol lm om lq on lu of og oh oi bi translated"><a class="ae ky" href="https://lightgbm.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank"> lightgbm </a>。</li></ul><p id="24ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们用<code class="fe oo op oq or b">data.table</code>代替默认的<code class="fe oo op oq or b">data.frame</code>，这只是我个人的喜好。如果你看到类似<code class="fe oo op oq or b">table[,.(column_name)]</code>的东西，那就是<code class="fe oo op oq or b">data.table</code>语法。</p><h2 id="c383" class="no mm it bd mn np nq dn mr nr ns dp mv li nt nu mx lm nv nw mz lq nx ny nb nz bi translated">模拟数据</h2><p id="c8ae" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">他们有两个分类变量，<code class="fe oo op oq or b">var1</code>和<code class="fe oo op oq or b">var2</code>。在曝光为1的观察中，不同的组合在泊松分布中将具有以下λ:</p><pre class="kj kk kl km gt os or ot ou aw ov bi"><span id="eba7" class="no mm it or b gy ow ox l oy oz">   var1 var2 lambda_base<br/>1:    A    C         0.3<br/>2:    B    C         0.7<br/>3:    A    D         1.3<br/>4:    B    D         1.9</span></pre><p id="32fb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们将产生不同的曝光水平，并调整lambdas。</p><p id="deed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">例如，var1=A，var2=C，exposure = 0.4的观测，索赔计数将从λ为0.3 * 0.4 = 0.12的泊松分布中随机生成。</em></p><p id="0e26" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们创建了数据表的两个变体，一个简单的，一个适中的。这两个版本的区别在于我们产生风险的方式。对于<code class="fe oo op oq or b">data_easy</code>，它们被明确定义为0.1、0.2、…1.0的集合，而对于<code class="fe oo op oq or b">data_mod</code>，曝光在0和1之间随机生成。</p><p id="3b28" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还生成调整后的索赔计数、定义的claim_counts / expos(我们稍后会用到)。这是数据表的外观，列名缩写以适应屏幕:</p><pre class="kj kk kl km gt os or ot ou aw ov bi"><span id="d715" class="no mm it or b gy ow ox l oy oz">       var1 var2  expos lambda_base  lambda ccn  ccn_adj<br/>    1:    A    C 0.3078         0.3 0.09234   0 0.000000<br/>    2:    A    C 0.2577         0.3 0.07731   0 0.000000<br/>    3:    A    C 0.5523         0.3 0.16569   0 0.000000<br/>    4:    A    C 0.0564         0.3 0.01692   0 0.000000<br/>    5:    A    C 0.4685         0.3 0.14055   0 0.000000<br/>   ---                                                  <br/> 9996:    B    D 0.5999         1.9 1.13981   1 1.666944<br/> 9997:    B    D 0.9259         1.9 1.75921   0 0.000000<br/> 9998:    B    D 0.6517         1.9 1.23823   2 3.068897<br/> 9999:    B    D 0.6904         1.9 1.31176   4 5.793743<br/>10000:    B    D 0.6317         1.9 1.20023   3 4.749090</span></pre><h2 id="c6d2" class="no mm it bd mn np nq dn mr nr ns dp mv li nt nu mx lm nv nw mz lq nx ny nb nz bi translated">目标</h2><p id="3ade" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">在进入建模部分之前，重要的是考虑我们期望模型预测什么。</p><p id="0df4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有些人可能认为我们期望预测<code class="fe oo op oq or b">claim_counts</code>。这是不正确的。<code class="fe oo op oq or b">claim_counts</code>是一个随机变量，期望模型能够预测是“不公平的”。</p><blockquote class="lw"><p id="38ad" class="lx ly it bd lz ma mb mc md me mf lu dk translated">我们关心的是模型对λ参数的预测有多好。</p></blockquote><h1 id="4f20" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz pa ka mx kc pb kd mz kf pc kg nb nc bi translated">解决方案1 —初始分数</h1><p id="8fae" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">在第一个解决方案中，一个经常在论坛上建议的方案，我们将<code class="fe oo op oq or b">lgb.Dataset</code>的<code class="fe oo op oq or b">init_score</code>参数设置为曝光的对数。</p><h2 id="bb8e" class="no mm it bd mn np nq dn mr nr ns dp mv li nt nu mx lm nv nw mz lq nx ny nb nz bi translated">密码</h2><pre class="kj kk kl km gt os or ot ou aw ov bi"><span id="090f" class="no mm it or b gy ow ox l oy oz">solution_1_predict &lt;- function(data_curr){</span><span id="9b20" class="no mm it or b gy pd ox l oy oz">  data_curr_recoded &lt;- lgb.convert_with_rules(<br/>    data = data_curr[,.(var1, var2)])$data<br/>  <br/>  dtrain &lt;- lgb.Dataset(<br/>    data = as.matrix(data_curr_recoded),<br/>    label = as.matrix(data_curr[,.(claim_count)]),<br/>    init_score = as.matrix(data_curr[,.(log(expos))]),<br/>    categorical_feature = c(1,2))<br/>  <br/>  param &lt;- list(<br/>    objective = "poisson",<br/>    num_iterations = 100, <br/>    learning_rate = 0.5)<br/>  <br/>  lgb_model &lt;- lgb.train(<br/>    params = param, <br/>    data = dtrain, <br/>    verbose = -1)<br/>  <br/>  return(predict(lgb_model, as.matrix(data_curr_recoded)))<br/>}</span></pre><p id="5982" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，LightGBM预测没有受到<code class="fe oo op oq or b">init_score</code>的影响！所以在我们的例子中，当我们做一个预测时，我们必须手动乘以暴露量。(我认为它在XGBoost中的工作方式不同，除非另有说明，这里包含了<code class="fe oo op oq or b">base_margin</code>。)</p><p id="274f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">进行预测的正确方法是:</p><pre class="kj kk kl km gt os or ot ou aw ov bi"><span id="7153" class="no mm it or b gy ow ox l oy oz">temp_predict &lt;- solution_1_predict(data_easy)<br/>data_easy[,sol_1_predict_raw := temp_predict]<br/>data_easy[,sol_1_predict := sol_1_predict_raw * expos]</span></pre><p id="5539" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当然和<code class="fe oo op oq or b">data_mod</code>的过程相同。</p><h2 id="03f2" class="no mm it bd mn np nq dn mr nr ns dp mv li nt nu mx lm nv nw mz lq nx ny nb nz bi translated">结果</h2><p id="c46c" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">原始预测非常接近基本的lambdas，初看起来模型似乎是成功的。对于<code class="fe oo op oq or b">data_mod</code>，结果:</p><pre class="kj kk kl km gt os or ot ou aw ov bi"><span id="2711" class="no mm it or b gy ow ox l oy oz">data_mod[,.N, keyby = .(lambda_base, sol_1_predict_raw)]</span></pre><p id="3a68" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">返回</p><pre class="kj kk kl km gt os or ot ou aw ov bi"><span id="00d4" class="no mm it or b gy ow ox l oy oz">   lambda_base sol_1_predict_raw    N<br/>1:         0.3         0.2941616 2500<br/>2:         0.7         0.6677160 2500<br/>3:         1.3         1.2907763 2500<br/>4:         1.9         1.9207416 2500</span></pre><p id="ccd6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">索赔总数也与预测相符。</p><pre class="kj kk kl km gt os or ot ou aw ov bi"><span id="4cc2" class="no mm it or b gy ow ox l oy oz">   claim_count pred_claim_count theoretical_claim_count<br/>1:        5208             5208                5241.001</span></pre><p id="4ebb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">观察它和理论上的期望值，也就是lambdas的和，是不一样的。</p><h2 id="15c2" class="no mm it bd mn np nq dn mr nr ns dp mv li nt nu mx lm nv nw mz lq nx ny nb nz bi translated">预测的可能性</h2><p id="8fcc" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">让我们分析一下，如果预测值与理论值不完全匹配，我们是否会感到满意。我们将解决方案1)的性能与简单预测λ的模型的性能进行比较。</p><p id="8f81" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们计算对数似然性:</p><pre class="kj kk kl km gt os or ot ou aw ov bi"><span id="bd4a" class="no mm it or b gy ow ox l oy oz">data_mod[,sol_1_ll := dpois(x = claim_count, <br/>                            lambda = sol_1_predict, log = TRUE)]<br/>data_mod[,base_ll := dpois(x = claim_count, <br/>                           lambda = lambda, log = TRUE)]<br/>data_mod[,saturated_ll := dpois(x = claim_count, <br/>                            lambda = claim_count, log = TRUE)]<br/>data_mod[,null_ll := dpois(x = claim_count, <br/>                            lambda = mean(claim_count), log = TRUE)]<br/>data_mod[,.(sol_1_ll = sum(sol_1_ll), <br/>            base_ll = sum(base_ll), <br/>            saturated_ll = sum(saturated_ll), <br/>            null_ll = sum(null_ll))]</span></pre><p id="11cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">退货:</p><pre class="kj kk kl km gt os or ot ou aw ov bi"><span id="0d90" class="no mm it or b gy ow ox l oy oz">    sol_1_ll   base_ll saturated_ll   null_ll<br/>1: -8125.188 -8126.383     -3918.93 -10101.24</span></pre><p id="51ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们所看到的，我们的解决方案的对数似然比我们简单地使用lambdas进行预测的结果要高一点点。这意味着我们可以对我们的结果感到满意！</p><p id="7079" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">顺便说一下，从对数似然性，我们也可以计算出解释偏差的百分比。我不想跑题文章，代码请参考我的<a class="ae ky" href="https://github.com/MatePocs/quick_projects/blob/main/lightgbm_exposure.R" rel="noopener ugc nofollow" target="_blank"> GitHub </a>。</p><blockquote class="lw"><p id="2384" class="lx ly it bd lz ma mb mc md me mf lu dk translated">然而，我认为重要的是要强调，对于一个几乎完美描述了所有非随机效应的模型，解释的偏差百分比仅为32%。</p></blockquote><h2 id="27b7" class="no mm it bd mn np pe dn mr nr pf dp mv li pg nu mx lm ph nw mz lq pi ny nb nz bi translated">它是如何工作的？</h2><p id="d310" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">我最初对这种方法非常怀疑，我的想法是这样的:<em class="lv">“设置init_score可以加快这个过程，但最终，模型会远离这些数字。获取日志更没有意义，对于非负目标，它将是一个负数。”</em>T9】这是不正确的。</p><p id="493b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对数是必要的，因为<code class="fe oo op oq or b">“poisson”</code>物镜将使用对数链接，即使文档中没有提到这一点。这意味着计算出的原始分数的指数将是模型返回的最终预测值。</p><p id="d4e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">注:其实挺有意思的，无论是</em> <a class="ae ky" href="https://xgboost.readthedocs.io/en/latest/parameter.html" rel="noopener ugc nofollow" target="_blank"> <em class="lv"> XGBoost的</em> </a> <em class="lv">还是</em> <a class="ae ky" href="https://lightgbm.readthedocs.io/en/latest/Parameters.html" rel="noopener ugc nofollow" target="_blank"> <em class="lv"> LightGBM的</em> </a> <em class="lv">文档都会特别提到Tweedie和Gamma使用了log-link，但是没有指定用Poisson。我知道log-link是针对阿松的</em> <a class="ae ky" href="https://en.wikipedia.org/wiki/Poisson_regression" rel="noopener ugc nofollow" target="_blank"> <em class="lv">规范链接函数</em> </a> <em class="lv">，但看起来确实有人抄袭了他们的答案！</em></p><p id="edd1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们放入<code class="fe oo op oq or b">init_score</code>时，我们实际上设置了观测值之间的固定差异。如果曝光是两次观察之间的唯一差异，预测的比率将与曝光相同。</p><p id="a71c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，在我们的例子中，如果我们有以下两个观察结果:</p><pre class="kj kk kl km gt os or ot ou aw ov bi"><span id="a439" class="no mm it or b gy ow ox l oy oz">       var1 var2  expos<br/>    1:    A    C    0.5<br/>    2:    A    C    0.3</span></pre><p id="89a6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一个将得到一个修正对数(0.5)，第二个在其预测总和中得到一个修正对数(0.3)。一旦设置完毕，模型就只能使用<code class="fe oo op oq or b">var1</code>和<code class="fe oo op oq or b">var2</code>。由于这两个观察值相等，模型无法以任何其他方式区分它们，第一个预测值将始终等于第二个预测值的0.5/0.3倍。</p><p id="3903" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是我们想要的。拟合模型，同时考虑这种假设的线性暴露影响。</p><h1 id="cc7a" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">解决方案2——经调整的索赔</h1><p id="50bc" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">这是我经常看到的另一个解决方案。</p><ul class=""><li id="5f0c" class="oa ob it lb b lc ld lf lg li oc lm od lq oe lu of og oh oi bi translated">首先，计算调整后的索赔次数=索赔次数/风险。<em class="lv">例如，如果您只有0.5的风险敞口和3项索赔，则调整后的索赔数为3 / 0.5 = 6。</em></li><li id="d833" class="oa ob it lb b lc oj lf ok li ol lm om lq on lu of og oh oi bi translated">您根据调整后的索赔数量运行模型。</li><li id="ae62" class="oa ob it lb b lc oj lf ok li ol lm om lq on lu of og oh oi bi translated">你还需要用曝光率作为权重。(现在没有关于曝光的日志材料！)</li></ul><p id="1f4d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看这个解决方案的表现如何！</p><h2 id="36e6" class="no mm it bd mn np nq dn mr nr ns dp mv li nt nu mx lm nv nw mz lq nx ny nb nz bi translated">密码</h2><p id="2df5" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">代码与解决方案1)几乎相同，但是</p><ul class=""><li id="7566" class="oa ob it lb b lc ld lf lg li oc lm od lq oe lu of og oh oi bi translated">我们的目标不同；</li><li id="6680" class="oa ob it lb b lc oj lf ok li ol lm om lq on lu of og oh oi bi translated">我们没有使用<code class="fe oo op oq or b">log(expos)</code>作为<code class="fe oo op oq or b">init_score</code>，而是将<code class="fe oo op oq or b">expos</code>作为<code class="fe oo op oq or b">weight</code>放入。</li></ul><pre class="kj kk kl km gt os or ot ou aw ov bi"><span id="8005" class="no mm it or b gy ow ox l oy oz">solution_2_predict &lt;- function(data_curr){<br/>  <br/>  ... (same as solution 1) ...<br/>  <br/>  dtrain &lt;- lgb.Dataset(<br/>    data = as.matrix(data_curr_recoded),<br/>    label = as.matrix(data_curr[,.(claim_count_adjusted)]),<br/>    weight = as.matrix(data_curr[,.(expos)]),<br/>    categorical_feature = c(1,2))<br/>  <br/>  ... (same as solution 1) ...<br/>}</span></pre><h2 id="eb7b" class="no mm it bd mn np nq dn mr nr ns dp mv li nt nu mx lm nv nw mz lq nx ny nb nz bi translated">结果</h2><p id="67ab" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">不再赘述，结果将与解决方案1)完全匹配。</p><blockquote class="lw"><p id="522e" class="lx ly it bd lz ma mb mc md me mf lu dk translated">这使得关于应该使用哪种解决方案来处理偏移的争论变得毫无意义。</p></blockquote><h2 id="1561" class="no mm it bd mn np pe dn mr nr pf dp mv li pg nu mx lm ph nw mz lq pi ny nb nz bi translated">它是如何工作的？</h2><p id="782a" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">那么，为什么我们从两个看似不同的模型中得到了相同的预测呢？</p><p id="653c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们必须考虑提升树(以及一堆其他使用<a class="ae ky" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">梯度下降</a>的机器学习模型)如何处理最大似然估计。在引擎盖下，他们不会使用原始形式的似然函数。相反，他们将使用对数似然函数相对于预测值的一阶和二阶导数。这些被称为<em class="lv">梯度</em>和<em class="lv">黑森</em>。</p><p id="8bc2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个方向上走得更远真的会让我们偏离主题，XGBoost教程有一个很好的总结。(这里的<em class="lv"> g(i) </em>和<em class="lv"> h(i) </em>函数在文本中没有明确命名，但是它们是gradient和hessian。)底线是，如果你知道目标的梯度和hessian(比如“泊松”)，你就知道目标。</p><p id="8114" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看<a class="ae ky" href="https://github.com/microsoft/LightGBM/blob/4b1b412452218c5be5ac0f238454ec9309036798/src/objective/regression_objective.hpp" rel="noopener ugc nofollow" target="_blank"> LightGBM源代码</a>中的相关位。在<code class="fe oo op oq or b">RegressionPoissonLoss</code>类下，我们有这个位:</p><pre class="kj kk kl km gt os or ot ou aw ov bi"><span id="57c7" class="no mm it or b gy ow ox l oy oz">gradients[i] = static_cast&lt;score_t&gt;((std::exp(score[i]) - label_[i])    * weights_[i]);        <br/>hessians[i] = static_cast&lt;score_t&gt;(std::exp(score[i] + max_delta_step_) * weights_[i]);</span></pre><p id="07a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意事项:</p><ul class=""><li id="1e10" class="oa ob it lb b lc ld lf lg li oc lm od lq oe lu of og oh oi bi translated">在这个术语中，分数代表预测。</li><li id="8876" class="oa ob it lb b lc oj lf ok li ol lm om lq on lu of og oh oi bi translated">你可以通过计算导数来重现公式的基础。</li><li id="bcde" class="oa ob it lb b lc oj lf ok li ol lm om lq on lu of og oh oi bi translated">权重参数没有任何更深的数学意义，这是它们将如何被用来衡量<em class="lv">梯度</em>和<em class="lv">黑森</em>。当然，如果不定义权重，就不使用<code class="fe oo op oq or b">* weights_[i]</code>位。</li><li id="de89" class="oa ob it lb b lc oj lf ok li ol lm om lq on lu of og oh oi bi translated"><code class="fe oo op oq or b">max_delta_step_</code>在黑森只是一个确保收敛的值。如果我理解正确的话，XGBoost和LightGBM的默认值都是0.7。</li></ul><p id="f9aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，让我们看看给定的<code class="fe oo op oq or b">claim_count</code> — <code class="fe oo op oq or b">prediction</code>对会发生什么。</p><p id="e2a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在解决方案1)中，转折是<code class="fe oo op oq or b">log(exposure)</code>被自动添加到预测中，因此梯度将是:</p><pre class="kj kk kl km gt os or ot ou aw ov bi"><span id="c364" class="no mm it or b gy ow ox l oy oz">exp(prediction + log(exposure)) - claim_count</span></pre><p id="a05e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这等于:</p><pre class="kj kk kl km gt os or ot ou aw ov bi"><span id="f91b" class="no mm it or b gy ow ox l oy oz">exp(prediction) * exposure - claim_count</span></pre><p id="85b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">而对于解决方案2)，我们使用权重，因此梯度将简单地乘以曝光度，另外请注意，我们使用调整后的索赔计数作为标签。溶液2的梯度):</p><pre class="kj kk kl km gt os or ot ou aw ov bi"><span id="b58a" class="no mm it or b gy ow ox l oy oz">(exp(prediction) - claim_count_adjusted) * exposure</span></pre><p id="9828" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑到</p><pre class="kj kk kl km gt os or ot ou aw ov bi"><span id="b991" class="no mm it or b gy ow ox l oy oz">claim_count_adjusted = claim_count / exposure</span></pre><p id="4e80" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">解2)的梯度也将等于</p><pre class="kj kk kl km gt os or ot ou aw ov bi"><span id="e4aa" class="no mm it or b gy ow ox l oy oz">exp(prediction) * exposure - claim_count</span></pre><p id="c958" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样的思考过程可以重复用于<em class="lv"> hessian </em>计算。</p><p id="cf77" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于<em class="lv">(预测、曝光、索赔_计数)</em>的任何可能值，两个解决方案具有相同的<em class="lv">梯度</em>和<em class="lv"> hessian </em>，它们的所有学习参数都匹配，因此它们将返回相同的预测。</p><blockquote class="lw"><p id="e3e8" class="lx ly it bd lz ma mb mc md me mf lu dk translated">我会留下一点时间来处理这个，我认为这是一个很酷的结果。</p></blockquote><h1 id="81cb" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz pa ka mx kc pb kd mz kf pc kg nb nc bi translated">解决方案3 —自定义目标函数</h1><p id="d8c8" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">既然我们对“泊松”目标有了如此透彻的理解，我想我们还不如编写我们自己的自定义目标函数。</p><p id="c6c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我不打算在这里包括一个通用教程，我认为<a class="ae ky" href="https://github.com/microsoft/LightGBM/blob/master/R-package/demo/multiclass_custom_objective.R" rel="noopener ugc nofollow" target="_blank"> R演示</a>脚本是相当有帮助的。要点是定制目标函数需要返回两个向量:<code class="fe oo op oq or b">gradient</code>和<code class="fe oo op oq or b">hessian</code>。</p><h2 id="e8f1" class="no mm it bd mn np nq dn mr nr ns dp mv li nt nu mx lm nv nw mz lq nx ny nb nz bi translated">密码</h2><p id="6061" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">对于自定义目标函数，我们基本上使用了我们到目前为止讨论过的所有内容。无论我们预测什么，都会随着曝光率自动调整，下文称之为<code class="fe oo op oq or b">dtrain_expos</code>。黑森里的0.7，简直就是我们前面说的<code class="fe oo op oq or b">max_delta_step_</code>。</p><pre class="kj kk kl km gt os or ot ou aw ov bi"><span id="31c5" class="no mm it or b gy ow ox l oy oz">my_poisson_w_exposure &lt;- function(preds, dtrain){<br/>  <br/>  labels &lt;- getinfo(dtrain, "label")<br/>  preds &lt;- matrix(preds, nrow = length(labels))<br/>  preds_expos_adj &lt;- preds + log(dtrain_expos)<br/>  grad &lt;- exp(preds_expos_adj) - labels<br/>  hess &lt;- exp(preds_expos_adj + 0.7)<br/>  <br/>  return(list(grad = grad, hess = hess))<br/>}</span></pre><p id="867f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们可以使用这个新的目标函数，而不是内置的“泊松”函数。</p><pre class="kj kk kl km gt os or ot ou aw ov bi"><span id="2be9" class="no mm it or b gy ow ox l oy oz">solution_3_predict &lt;- function(data_curr){<br/>  <br/>  data_curr_recoded &lt;- lgb.convert_with_rules(<br/>    data = data_curr[,.(var1, var2)])$data<br/>  <br/>  dtrain &lt;- lgb.Dataset(<br/>    data = as.matrix(data_curr_recoded),<br/>    label = as.matrix(data_curr[,.(claim_count)]),<br/>    categorical_feature = c(1,2))<br/>  <br/>  param &lt;- list(<br/>    max_depth = 2,<br/>    objective = my_poisson_w_exposure,<br/>    metric = "mae",<br/>    num_iterations = 100, <br/>    learning_rate = 0.5)<br/>  <br/>  lgb_model &lt;- lgb.train(<br/>    params = param, <br/>    data = dtrain, <br/>    verbose = -1)<br/>  <br/>  return(predict(lgb_model, as.matrix(data_curr_recoded)))<br/>}</span></pre><p id="5123" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">瞧，基本上就是这样。请注意，我随机输入了“mae”作为度量。每个内置目标都有一个默认的度量对。当我们使用自定义目标时，需要对其进行定义。如果我们想要谨慎，我们还需要定义一个定制的度量函数。但是我们现在没有使用测试集，所以它不会在任何地方使用。</p><p id="2b3d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种方法有两个缺点:</p><ul class=""><li id="65dc" class="oa ob it lb b lc ld lf lg li oc lm od lq oe lu of og oh oi bi translated">我无法找到在自定义目标函数中包含另一个参数的方法。因此，<code class="fe oo op oq or b">data_expos</code>只是被假设为在全球环境中可用，这对我来说并不意味着稳定的结构。也许有一种方法可以输入除<code class="fe oo op oq or b">preds</code>和<code class="fe oo op oq or b">dtrain</code>之外的参数，<strong class="lb iu">一定要让我知道</strong>。</li><li id="c243" class="oa ob it lb b lc oj lf ok li ol lm om lq on lu of og oh oi bi translated">除了将从模型中得出的原始预测与暴露量相乘之外，我们还需要计算它们的指数。在内置的“泊松”目标中，有一个额外的步骤来计算原始分数的指数。换句话说，它使用了一个日志链接。这是我在自定义目标中无法做到的。</li></ul><p id="71e4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑到这两个问题，使用这种方法进行预测的正确方法是:</p><pre class="kj kk kl km gt os or ot ou aw ov bi"><span id="4d2e" class="no mm it or b gy ow ox l oy oz">dtrain_expos &lt;- as.matrix(data_mod[,.(expos)])<br/>temp_predict &lt;- solution_3_predict(data_mod)<br/>data_mod[,sol_3_predict_raw := temp_predict]<br/>data_mod[,sol_3_predict := exp(sol_3_predict_raw) * expos]</span></pre><h2 id="9779" class="no mm it bd mn np nq dn mr nr ns dp mv li nt nu mx lm nv nw mz lq nx ny nb nz bi translated">结果</h2><p id="0593" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">是的，与前两个解决方案相同。</p><h1 id="ce5c" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">哪个解决方案是最好的？</h1><p id="8d3e" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">这是一个个人偏好的问题，从计算的角度来看，它们是相同的模型，并将计算相同的预测。</p><ul class=""><li id="63a4" class="oa ob it lb b lc ld lf lg li oc lm od lq oe lu of og oh oi bi translated">我认为带有<code class="fe oo op oq or b">init_score</code>的解决方案1)是最优雅和简单的。我怀疑那也是最快的。</li><li id="9020" class="oa ob it lb b lc oj lf ok li ol lm om lq on lu of og oh oi bi translated">解决方案3)，自定义目标函数是最健壮的，一旦你理解了它的工作原理，你就可以用它做任何事情。</li><li id="c9c2" class="oa ob it lb b lc oj lf ok li ol lm om lq on lu of og oh oi bi translated">我个人最不喜欢的是解决方案2)，使用调整索赔计数的解决方案，我认为它有点绕弯，没有任何附加值。</li></ul><h1 id="b2e2" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">一般照明GBM提示</h1><p id="fd7f" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">我不想在上面的每个细节上花费时间，这里有一个通用提示列表，在使用LightGBM或某些情况下其他梯度增强树包时可能会派上用场。</p><h2 id="7a3e" class="no mm it bd mn np nq dn mr nr ns dp mv li nt nu mx lm nv nw mz lq nx ny nb nz bi translated">分类特征</h2><p id="5645" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">在LightGBM中，使用分类变量是一个两步过程:</p><ul class=""><li id="ea6c" class="oa ob it lb b lc ld lf lg li oc lm od lq oe lu of og oh oi bi translated">首先，你要用<code class="fe oo op oq or b">convert_with_rules</code>把它们转换成整数——这一步是必须的；</li><li id="d831" class="oa ob it lb b lc oj lf ok li ol lm om lq on lu of og oh oi bi translated">然后，当您创建<code class="fe oo op oq or b">lgb.DataSet</code>时，您可能还想用<code class="fe oo op oq or b">categorical_feature</code>指定数据中的哪些列是分类的——从技术上讲，模型将在没有此步骤的情况下运行，并将分类特征作为序数特征处理。</li></ul><p id="4928" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">详情请看这个<a class="ae ky" href="https://github.com/microsoft/LightGBM/blob/master/R-package/demo/categorical_features_rules.R" rel="noopener ugc nofollow" target="_blank">演示脚本</a>。</p><h2 id="be5d" class="no mm it bd mn np nq dn mr nr ns dp mv li nt nu mx lm nv nw mz lq nx ny nb nz bi translated">证明文件</h2><p id="b3dd" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">在我看来，软件包文档不是无缝的，你必须测试所有的东西。许多不同的版本(命令行接口，C / Python / R API)通常只是一起记录，这并没有什么帮助。</p><p id="c19b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，我不太清楚如何定义<code class="fe oo op oq or b">init_score</code>。根据相关的<a class="ae ky" href="https://github.com/microsoft/LightGBM/blob/master/R-package/demo/boost_from_prediction.R" rel="noopener ugc nofollow" target="_blank">演示脚本</a>，您必须使用以下语法:</p><pre class="kj kk kl km gt os or ot ou aw ov bi"><span id="f928" class="no mm it or b gy ow ox l oy oz">setinfo(dtrain, "init_score", ptrain)</span></pre><p id="fc89" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，这样做的时候用下面的定义<code class="fe oo op oq or b">lgb.DataSet</code>:</p><pre class="kj kk kl km gt os or ot ou aw ov bi"><span id="f8ac" class="no mm it or b gy ow ox l oy oz">...<br/>init_score = ptrain,<br/>...</span></pre><p id="ee24" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">似乎对我来说挺好的。<code class="fe oo op oq or b">set_init_score</code>另一方面，看起来像是Python特有的方法。</p><h2 id="e3dc" class="no mm it bd mn np nq dn mr nr ns dp mv li nt nu mx lm nv nw mz lq nx ny nb nz bi translated">链接功能</h2><p id="3159" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">您应该知道您所选择的目标所使用的链接功能。例如，它使用泊松的日志链接。如果你想放入一个<code class="fe oo op oq or b">init_score</code>，这很重要，你必须根据链接函数转换值。</p><p id="c5d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当您运行<code class="fe oo op oq or b">predict</code>方法时，您可以传递一个<code class="fe oo op oq or b">rawscore = TRUE</code>参数来在转换之前获取结果并检查它们。</p><h2 id="351d" class="no mm it bd mn np nq dn mr nr ns dp mv li nt nu mx lm nv nw mz lq nx ny nb nz bi translated">泊松对特威迪</h2><p id="92b9" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">如果您有非整数值，使用基本的“泊松”目标可能看起来很奇怪。毕竟泊松是离散分布。我们如何用泊松来拟合调整后的索赔数？</p><p id="b488" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，你可以<a class="ae ky" href="https://en.wikipedia.org/wiki/Factorial" rel="noopener ugc nofollow" target="_blank">将阶乘函数</a>扩展到非整数，所以从技术上来说，你可以在非整数处计算分布函数的值。gradient和hessian也不会关心标签是否为非整数。</p><p id="e976" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是你可能仍然会倾向于使用分散参数设置为1的<a class="ae ky" href="https://en.wikipedia.org/wiki/Tweedie_distribution" rel="noopener ugc nofollow" target="_blank"> Tweedie分布</a>。这样做将导致完全相同的结果。</p><p id="a1fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">换句话说，在模型设置中替换这一行:</p><pre class="kj kk kl km gt os or ot ou aw ov bi"><span id="ef16" class="no mm it or b gy ow ox l oy oz">...<br/>objective = "poisson",<br/>...</span></pre><p id="f04a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有了这个:</p><pre class="kj kk kl km gt os or ot ou aw ov bi"><span id="49cb" class="no mm it or b gy ow ox l oy oz">...<br/>objective = "tweedie",<br/>tweedie_variance_power = 1,<br/>...</span></pre><p id="d299" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不会改变你的结果。</p><p id="890d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你看一下<a class="ae ky" href="https://github.com/microsoft/LightGBM/blob/4b1b412452218c5be5ac0f238454ec9309036798/src/objective/regression_objective.hpp" rel="noopener ugc nofollow" target="_blank">源代码</a>中的泊松和特威迪损失函数，你可以看到当<code class="fe oo op oq or b">rho</code>(色散参数，又名<code class="fe oo op oq or b">tweedie_variance_power</code>)等于1时，它们是如何匹配的。</p><h2 id="e379" class="no mm it bd mn np nq dn mr nr ns dp mv li nt nu mx lm nv nw mz lq nx ny nb nz bi translated">不同的术语</h2><p id="f30c" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">在LightGBM和XGBoost中，Metric、objective和eval具有不同的含义。在LightGBM中，您需要一个目标函数来进行优化，只有在使用<a class="ae ky" href="https://github.com/microsoft/LightGBM/blob/master/R-package/demo/early_stopping.R" rel="noopener ugc nofollow" target="_blank">验证集</a>时，才会显示指标。</p><h1 id="ad0c" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">好消息来源</h1><div class="pj pk gp gr pl pm"><a href="https://lightgbm.readthedocs.io/en/latest/index.html" rel="noopener  ugc nofollow" target="_blank"><div class="pn ab fo"><div class="po ab pp cl cj pq"><h2 class="bd iu gy z fp pr fr fs ps fu fw is bi translated">欢迎阅读LightGBM的文档！-light GBM 3.2.1.99文档</h2><div class="pt l"><h3 class="bd b gy z fp pr fr fs ps fu fw dk translated">编辑描述</h3></div><div class="pu l"><p class="bd b dl z fp pr fr fs ps fu fw dk translated">lightgbm.readthedocs.io</p></div></div></div></a></div><div class="pj pk gp gr pl pm"><a href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html" rel="noopener  ugc nofollow" target="_blank"><div class="pn ab fo"><div class="po ab pp cl cj pq"><h2 class="bd iu gy z fp pr fr fs ps fu fw is bi translated">增强树简介- xgboost 1.5.0-dev文档</h2><div class="pt l"><h3 class="bd b gy z fp pr fr fs ps fu fw dk translated">XGBoost代表“极端梯度增强”，其中术语“梯度增强”源于论文Greedy…</h3></div><div class="pu l"><p class="bd b dl z fp pr fr fs ps fu fw dk translated">xgboost.readthedocs.io</p></div></div><div class="pv l"><div class="pw l px py pz pv qa ks pm"/></div></div></a></div><div class="pj pk gp gr pl pm"><a href="https://github.com/microsoft/LightGBM/tree/master/R-package/demo" rel="noopener  ugc nofollow" target="_blank"><div class="pn ab fo"><div class="po ab pp cl cj pq"><h2 class="bd iu gy z fp pr fr fs ps fu fw is bi translated">light GBM/R-主microsoft/LightGBM上的包/演示</h2><div class="pt l"><h3 class="bd b gy z fp pr fr fs ps fu fw dk translated">基于决策树的快速、分布式、高性能梯度推进(GBT、GBDT、GBRT、GBM或MART)框架…</h3></div><div class="pu l"><p class="bd b dl z fp pr fr fs ps fu fw dk translated">github.com</p></div></div><div class="pv l"><div class="qb l px py pz pv qa ks pm"/></div></div></a></div></div></div>    
</body>
</html>