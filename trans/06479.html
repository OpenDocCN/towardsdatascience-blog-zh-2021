<html>
<head>
<title>Stochastic Gradient Descent: Explanation and Complete Implementation from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">随机梯度下降:从头开始的解释和完整实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/stochastic-gradient-descent-explanation-and-complete-implementation-from-scratch-a2c6a02f28bd?source=collection_archive---------16-----------------------#2021-06-10">https://towardsdatascience.com/stochastic-gradient-descent-explanation-and-complete-implementation-from-scratch-a2c6a02f28bd?source=collection_archive---------16-----------------------#2021-06-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/8c3122ae25bdb45dba49c7de0a37345e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xQK8o3DPrIJnZtEx"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">照片由<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae jg" href="https://unsplash.com/@seteph?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Allef Vinicius </a>拍摄</p></figure><div class=""/><div class=""><h2 id="6155" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">使用单个感知器</h2></div><p id="6290" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">随机梯度下降是机器学习和深度学习中广泛使用的方法。这篇文章解释了随机梯度下降使用一个单一的感知器，使用著名的虹膜数据集。我假设你已经知道梯度下降的基本知识。如果你需要复习，请查看这个线性回归教程，它用一个简单的机器学习问题解释了梯度下降。</p><div class="is it gp gr iu lu"><a rel="noopener follow" target="_blank" href="/basic-linear-regression-algorithm-in-python-for-beginners-c519a808b5f8"><div class="lv ab fo"><div class="lw ab lx cl cj ly"><h2 class="bd jk gy z fp lz fr fs ma fu fw ji bi translated">Python中的线性回归算法:一步一步</h2><div class="mb l"><h3 class="bd b gy z fp lz fr fs ma fu fw dk translated">学习线性回归的概念，并使用python从头开始开发一个完整的线性回归算法</h3></div><div class="mc l"><p class="bd b dl z fp lz fr fs ma fu fw dk translated">towardsdatascience.com</p></div></div><div class="md l"><div class="me l mf mg mh md mi ja lu"/></div></div></a></div><h2 id="f835" class="mj mk jj bd ml mm mn dn mo mp mq dp mr lh ms mt mu ll mv mw mx lp my mz na nb bi translated">什么是随机梯度下降？</h2><p id="f863" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">在深入研究随机梯度下降之前，让我们先了解一下规则梯度下降。梯度下降是一种迭代算法。我们来举个简单的例子。正如我提到的，我将使用一个单一的感知器:</p><figure class="ni nj nk nl gt iv gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/3c5b1a7afa2019b928c248f102c0ee72.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*2_fPROG2HDUEN3ufE7zQgA.png"/></div></figure><p id="5b27" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面是简单的线性公式:</p><p id="581e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Y = WX+ W0</p><p id="2b74" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里W0是偏置项。如果有多个特征:</p><p id="9478" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Y = W1X1 + W2X2 + … WnXn + W0</p><p id="26a3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用此公式，让我们一步一步地检查如何执行梯度下降算法:</p><p id="63c9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">1.首先，随机初始化Ws。</p><p id="1ad4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">2.使用这个随机Ws计算预测输出Y_hat。</p><p id="5329" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">3.取Y和Y_hat的均方根误差。</p><p id="1d22" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">4.使用以下公式计算更新后的Ws:W = W—步长*均方根的导数，其中需要选择步长。</p><p id="780f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">5.不断重复步骤2到4，直到收敛。</p><p id="d13e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这个过程中，我们在每次迭代中使用整个数据集。如果你只有一百个数据点，那就可以了。但是，如果你有10，000或100，000个数据点，你必须运行200次迭代来收敛，计算变得缓慢和昂贵。为了解决这个问题，可以为每次迭代选择一个随机数据点，而不是使用所有100，000个数据。计算将变得非常快速和简单。<strong class="la jk">这叫做随机梯度下降。</strong></p><blockquote class="nm"><p id="2b1c" class="nn no jj bd np nq nr ns nt nu nv lt dk translated">除了选择一个数据点，我们还可以选择一个小批量的数据点，如10/15或20个数据点。我稍后将展示的例子将在每次迭代中使用12个数据点。</p></blockquote><h2 id="fb16" class="mj mk jj bd ml mm nw dn mo mp nx dp mr lh ny mt mu ll nz mw mx lp oa mz na nb bi translated">数据准备</h2><p id="5240" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">首先从sklearn库中加载iris数据集:</p><pre class="ni nj nk nl gt ob oc od oe aw of bi"><span id="5149" class="mj mk jj oc b gy og oh l oi oj">from sklearn.datasets import load_iris<br/>iris = load_iris()</span></pre><p id="0d3c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个虹膜数据集是字典格式的。特征在键“数据”中，目标在键“目标”中。这是所有的钥匙:</p><pre class="ni nj nk nl gt ob oc od oe aw of bi"><span id="ceb3" class="mj mk jj oc b gy og oh l oi oj">list(iris.keys())</span></pre><p id="1ef7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="ni nj nk nl gt ob oc od oe aw of bi"><span id="e290" class="mj mk jj oc b gy og oh l oi oj">['data',<br/> 'target',<br/> 'frame',<br/> 'target_names',<br/> 'DESCR',<br/> 'feature_names',<br/> 'filename']</span></pre><p id="c623" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们需要前两个键:数据和目标。我喜欢使用数据框架。因此，我将把数据转换成数据帧:</p><pre class="ni nj nk nl gt ob oc od oe aw of bi"><span id="042b" class="mj mk jj oc b gy og oh l oi oj">import pandas as pd<br/>ir = pd.DataFrame(iris['data'])<br/>ir['Species'] = pd.DataFrame(iris['target'])<br/>ir</span></pre><p id="1b6d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><figure class="ni nj nk nl gt iv gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/f0d8dba614ca418bbb0dce5cd7d388b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:476/format:webp/1*gFlQNsEPYrZ3nxkd8uKyMA.png"/></div></figure><p id="9bd2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在物种栏里，只有数字。但是这些数字代表了鸢尾花的种类。iris数据集中的Target_names键显示了它们。</p><pre class="ni nj nk nl gt ob oc od oe aw of bi"><span id="1bf0" class="mj mk jj oc b gy og oh l oi oj">ir['Species'] = ir['Species'].replace({0: "setosa", 1: "versicolor", 2: 'virginica'})</span></pre><p id="b208" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我只是把物种栏里的数字换成了真正的物种名称。有三种不同的物种。</p><p id="c121" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我的目标是演示随机梯度下降。所以，我想做一个简单的二元分类。因此，我将“setosa”作为阳性类，其余的物种作为阴性类。这意味着。这个分类器会告诉我们一朵花是不是“setosa”。这里，我们需要将“setosa”更改为1，将其余的物种更改为0。</p><pre class="ni nj nk nl gt ob oc od oe aw of bi"><span id="31aa" class="mj mk jj oc b gy og oh l oi oj"><strong class="oc jk">for</strong> i <strong class="oc jk">in</strong> range(len(ir['Species'])):<br/>    <strong class="oc jk">if</strong> ir['Species'][i] == 'setosa':<br/>        ir['Species'][i] = 1<br/>    <strong class="oc jk">else</strong>:<br/>        ir['Species'][i] = 0<br/>ir['Species']</span></pre><p id="f6fa" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="ni nj nk nl gt ob oc od oe aw of bi"><span id="5bc2" class="mj mk jj oc b gy og oh l oi oj">0      1<br/>1      1<br/>2      1<br/>3      1<br/>4      1<br/>      ..<br/>145    0<br/>146    0<br/>147    0<br/>148    0<br/>149    0<br/>Name: Species, Length: 150, dtype: object</span></pre><p id="f119" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">数据集准备好了！现在好戏开始了。</p><h2 id="87c4" class="mj mk jj bd ml mm mn dn mo mp mq dp mr lh ms mt mu ll mv mw mx lp my mz na nb bi translated">使用随机梯度下降方法开发分类器</h2><p id="1246" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">首先，我们将使用sklearn库中的train_test_split函数将数据集分离为训练集和测试集。在此之前，需要将特征和目标分开。</p><pre class="ni nj nk nl gt ob oc od oe aw of bi"><span id="6d6d" class="mj mk jj oc b gy og oh l oi oj">from sklearn.model_selection import train_test_split</span><span id="71bc" class="mj mk jj oc b gy ol oh l oi oj">ir_features = ir.drop(columns = 'Species')<br/>ir_label = ir['Species']</span><span id="9877" class="mj mk jj oc b gy ol oh l oi oj">x_train, x_test, y_train, y_test = train_test_split(<br/>    ir_features, ir_label, <br/>    test_size = 0.2,<br/>    random_state = 10<br/>)</span></pre><p id="0ed2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们分别设置了培训和测试。正如我之前解释的，我们将为每次迭代随机选择一些数据点。我将组合x_train和y_train，因为我们也需要y_train进行训练，</p><pre class="ni nj nk nl gt ob oc od oe aw of bi"><span id="4897" class="mj mk jj oc b gy og oh l oi oj">x_train['Species'] = y_train<br/>df = x_train</span></pre><p id="b452" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面是为每次迭代从训练集中随机选择12个数据点的函数。在原始的iris数据集中，有50个‘setosa’和100个其他物种。这意味着有50个正类和100个负类数据。样品应该符合那个比例。所以，我们从正类中取4个数据，从负类中取8个数据。</p><pre class="ni nj nk nl gt ob oc od oe aw of bi"><span id="bbf7" class="mj mk jj oc b gy og oh l oi oj">def stratified_spl(df):<br/>    df1 = df[df['Species'] == 1]<br/>    df0 = df[df['Species'] == 0]<br/>    df1_spl = df1.sample(n=4)<br/>    df0_spl = df0.sample(n=8)<br/>    return pd.concat([df1_spl, df0_spl])</span></pre><p id="f4fd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我将在这里使用sigmoid激活函数:</p><pre class="ni nj nk nl gt ob oc od oe aw of bi"><span id="eb5b" class="mj mk jj oc b gy og oh l oi oj">def hypothesis(X, w):<br/>    z = np.dot(w, X.T)<br/>    return 1/(1+np.exp(-(z)))</span></pre><p id="dea0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下一个函数将为训练目的再次分离12个数据的小型训练集的特征和目标。</p><p id="5787" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如你在上面的线性公式中看到的，我们需要一个偏差W0。1的额外特性是作为偏置项添加的。我们将在每次迭代中改进偏置项:</p><pre class="ni nj nk nl gt ob oc od oe aw of bi"><span id="78a6" class="mj mk jj oc b gy og oh l oi oj">def xy(df):<br/>    df_features = df.drop(columns = 'Species')<br/>    df_label = df['Species']<br/>    df_features['00'] = [1]*12<br/>    return df_features, df_label</span></pre><p id="7e9e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们定义误差函数来计算均方误差(MSE)。该函数将首先计算预测目标，然后计算MSE:</p><pre class="ni nj nk nl gt ob oc od oe aw of bi"><span id="ef53" class="mj mk jj oc b gy og oh l oi oj">def error(X, y, w):<br/>    n = len(X)<br/>    yp = hypothesis(X, w)<br/>    return np.sum((yp-y)**2)/n</span></pre><p id="f0b5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所有功能都准备好了。现在梯度下降函数。在每次迭代中，</p><ol class=""><li id="1924" class="om on jj la b lb lc le lf lh oo ll op lp oq lt or os ot ou bi translated">它将使用之前定义的layered _ spl函数对12个数据点进行采样。</li><li id="750a" class="om on jj la b lb ov le ow lh ox ll oy lp oz lt or os ot ou bi translated">然后将特征和目标分割为X和y。</li><li id="1477" class="om on jj la b lb ov le ow lh ox ll oy lp oz lt or os ot ou bi translated">使用Ws和Xs计算预测的y。</li><li id="06e8" class="om on jj la b lb ov le ow lh ox ll oy lp oz lt or os ot ou bi translated">使用梯度下降公式更新Ws。</li></ol><blockquote class="nm"><p id="e131" class="nn no jj bd np nq pa pb pc pd pe lt dk translated">在分类问题中，通常使用对数来正则化误差项。但是我这里用MSE。因为我猜更多的人熟悉MSE。这个数据集很容易使用，我们为这个项目设计了MSE。</p></blockquote><p id="adcb" class="pw-post-body-paragraph ky kz jj la b lb pf kk ld le pg kn lg lh ph lj lk ll pi ln lo lp pj lr ls lt im bi translated">请随意尝试用日志记录错误术语。这里有一个例子:</p><div class="is it gp gr iu lu"><a rel="noopener follow" target="_blank" href="/a-complete-logistic-regression-algorithm-from-scratch-in-python-step-by-step-ce33eae7d703"><div class="lv ab fo"><div class="lw ab lx cl cj ly"><h2 class="bd jk gy z fp lz fr fs ma fu fw ji bi translated">Python中从头开始的完整逻辑回归算法:一步一步</h2><div class="mb l"><h3 class="bd b gy z fp lz fr fs ma fu fw dk translated">使用真实世界的数据集开发算法</h3></div><div class="mc l"><p class="bd b dl z fp lz fr fs ma fu fw dk translated">towardsdatascience.com</p></div></div><div class="md l"><div class="pk l mf mg mh md mi ja lu"/></div></div></a></div><p id="165e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将收集每次迭代和Ws中的错误。</p><pre class="ni nj nk nl gt ob oc od oe aw of bi"><span id="4ba1" class="mj mk jj oc b gy og oh l oi oj">def grad_des(df, w, alpha, epoch):<br/>    j = []<br/>    w1 = []<br/>    w1.append(w)<br/>    for i in range(epoch):<br/>        d = stratified_spl(df)<br/>        X, y = xy(d)<br/>        n= len(X)<br/>        yp = hypothesis(X, w)<br/>        <br/>        for i in range(4):<br/>            w[i] -= (alpha/n) * np.sum(-2*X[i]*(y-yp))<br/>        w[4] -= (alpha/n) *np.sum(-2*(y-yp))<br/>        w1.append(list(w))<br/>        j.append(error(X, y, w))<br/>    return j, w1</span></pre><p id="7eab" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">分类器完成了！现在，它需要被测试。</p><h2 id="7f65" class="mj mk jj bd ml mm mn dn mo mp mq dp mr lh ms mt mu ll mv mw mx lp my mz na nb bi translated">测试分类器</h2><p id="f2a0" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">我会随机初始化Ws。包括偏差在内共有五个特征。所以，对于每个特征或X，我需要初始化一个W，就像你在开始看到的线性公式一样。</p><pre class="ni nj nk nl gt ob oc od oe aw of bi"><span id="99df" class="mj mk jj oc b gy og oh l oi oj">import numpy as np<br/>w = np.random.rand(5)<br/>w</span></pre><p id="8d00" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="ni nj nk nl gt ob oc od oe aw of bi"><span id="e2fd" class="mj mk jj oc b gy og oh l oi oj">array([0.05837806, 0.91017305, 0.71097702, 0.91990355, 0.71139191])</span></pre><p id="186a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，使用梯度下降函数，步长(alpha)为0.01，迭代100次:</p><pre class="ni nj nk nl gt ob oc od oe aw of bi"><span id="b059" class="mj mk jj oc b gy og oh l oi oj">j, w1 = grad_des(x_train, w, 0.01, 100)</span></pre><p id="78fd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于100次迭代，这将为我们提供100个w1。使用这100个w1，我们可以计算100个MSE，以观察每次迭代中MSE的变化。我需要一个函数，应该计算每个w1的预测y，然后误差。</p><pre class="ni nj nk nl gt ob oc od oe aw of bi"><span id="674b" class="mj mk jj oc b gy og oh l oi oj">def err_test(X, y, w):<br/>    er = []<br/>    for i in range(len(w1)):<br/>        er.append(error(X, y, w[i]))<br/>    return er</span></pre><p id="2f91" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在每次迭代中看到MSE图是很好的。</p><p id="275e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面是绘图函数:</p><pre class="ni nj nk nl gt ob oc od oe aw of bi"><span id="6e2b" class="mj mk jj oc b gy og oh l oi oj">def plot(X, y, w):<br/>    error = err_test(X, y, w)<br/>    return plt.scatter(range(len(error)), error)</span></pre><p id="dd8c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">记得我们在x_train中添加了目标。因此，将目标从x_train中分离出来，并添加偏差项:</p><pre class="ni nj nk nl gt ob oc od oe aw of bi"><span id="5e3d" class="mj mk jj oc b gy og oh l oi oj">X = x_train.drop(columns = 'Species')<br/>X['00'] = [1]*len(X)</span></pre><p id="ca3a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，绘制训练集的MSE:</p><pre class="ni nj nk nl gt ob oc od oe aw of bi"><span id="b9db" class="mj mk jj oc b gy og oh l oi oj">import matplotlib.pyplot as plt<br/>plot(X, y_train, w1)</span></pre><figure class="ni nj nk nl gt iv gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/d9982747c4d822d84d6a779be155a9f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*G6_252DiTqm2pdXPfkZjnw.png"/></div></figure><p id="dd37" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们在测试集中添加一个偏差项。</p><pre class="ni nj nk nl gt ob oc od oe aw of bi"><span id="71bb" class="mj mk jj oc b gy og oh l oi oj">X_t['00'] = [1]*len(x_test)</span></pre><p id="76fd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，绘制测试数据的MSE:</p><pre class="ni nj nk nl gt ob oc od oe aw of bi"><span id="5738" class="mj mk jj oc b gy og oh l oi oj">plot(X_t, y_test, w1)</span></pre><figure class="ni nj nk nl gt iv gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/5bedeb60cd3349215c60c41d6fb68732.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*R7A2taLCu2MOVxQFHRA6tg.png"/></div></figure><p id="5bdb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个数据中发生的一个不寻常的事情是第一个MSE太低。那通常不会发生。通常情况下，第一个MSE太高，在第一个MSE之后，它会逐渐下降，如图所示。我找不到原因。</p><p id="fbd8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">不看准确性，绩效评估是不完整的。让我们为精确度定义一个函数:</p><pre class="ni nj nk nl gt ob oc od oe aw of bi"><span id="4417" class="mj mk jj oc b gy og oh l oi oj">def accuracy(X, y, w):<br/>    yp = hypothesis(X, w)<br/>    for i in range(len(yp)):<br/>        if yp[i] &gt;=0.5:<br/>            yp[i] = 1<br/>        else:<br/>            yp[i] = 0<br/>    return sum(yp == y)/len(y)</span></pre><p id="17f9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们想看看精确度如何随着每次迭代而变化:</p><pre class="ni nj nk nl gt ob oc od oe aw of bi"><span id="2257" class="mj mk jj oc b gy og oh l oi oj">def accuracy_series(X, y, w1):<br/>    acc = []<br/>    for i in range(len(w1)):<br/>        acc.append(accuracy(X, y, w1[i]))  <br/>    return acc</span></pre><p id="87de" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用此函数查看训练集的精确度系列:</p><pre class="ni nj nk nl gt ob oc od oe aw of bi"><span id="6ef2" class="mj mk jj oc b gy og oh l oi oj">np.array(accuracy_series(X, y_train, w1))</span></pre><p id="2534" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="ni nj nk nl gt ob oc od oe aw of bi"><span id="413c" class="mj mk jj oc b gy og oh l oi oj">array([0.975, 0.34166667, 0.34166667, 0.34166667, 0.34166667,<br/>       0.34166667, 0.34166667, 0.34166667, 0.33333333, 0.23333333,<br/>       0.34166667, 0.5       , 0.6       , 0.63333333, 0.64166667,<br/>       0.65833333, 0.65833333, 0.65833333, 0.65833333, 0.65833333,<br/>       0.65833333, 0.65833333, 0.65833333, 0.65833333, 0.65833333,<br/>       0.65833333, 0.65833333, 0.65833333, 0.65833333, 0.65833333,<br/>       0.65833333, 0.65833333, 0.65833333, 0.65833333, 0.65833333,<br/>       0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.66666667,<br/>       0.66666667, 0.675     , 0.69166667, 0.69166667, 0.69166667,<br/>       0.69166667, 0.71666667, 0.74166667, 0.75      , 0.76666667,<br/>       0.76666667, 0.78333333, 0.78333333, 0.83333333, 0.83333333,<br/>       0.83333333, 0.83333333, 0.85833333, 0.86666667, 0.83333333,<br/>       0.85      , 0.88333333, 0.88333333, 0.88333333, 0.90833333,<br/>       0.90833333, 0.90833333, 0.91666667, 0.925     , 0.925     ,<br/>       0.925     , 0.925     , 0.925     , 0.94166667, 0.94166667,<br/>       0.94166667, 0.95833333, 0.95833333, 0.96666667, 0.98333333,<br/>       0.98333333, 0.98333333, 0.98333333, 0.98333333, 0.98333333,<br/>       0.98333333, 0.99166667, 0.99166667, 0.99166667, 0.99166667,<br/>       0.99166667, 0.99166667, 0.99166667, 0.99166667, 0.99166667,<br/>       0.99166667, 0.99166667, 0.99166667, 0.99166667, 0.99166667,<br/>       0.99166667])</span></pre><p id="6bea" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">又来了。用第一个w获得99%的准确性是不寻常的，因为第一个w是随机启动的w，这在现实生活的项目中不会发生。</p><p id="e610" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，测试集的精度系列:</p><pre class="ni nj nk nl gt ob oc od oe aw of bi"><span id="6dcc" class="mj mk jj oc b gy og oh l oi oj">np.array(accuracy_series(X_t, y_test, w1))</span></pre><p id="81f9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="ni nj nk nl gt ob oc od oe aw of bi"><span id="8786" class="mj mk jj oc b gy og oh l oi oj">array([.93333333        , 0.3       , 0.3       , 0.3       , 0.3       ,<br/>       0.3       , 0.3       , 0.3       , 0.3       , 0.16666667,<br/>       0.33333333, 0.53333333, 0.6       , 0.63333333, 0.63333333,<br/>       0.7       , 0.7       , 0.7       , 0.7       , 0.7       ,<br/>       0.7       , 0.7       , 0.7       , 0.7       , 0.7       ,<br/>       0.7       , 0.7       , 0.7       , 0.7       , 0.7       ,<br/>       0.7       , 0.7       , 0.7       , 0.7       , 0.7       ,<br/>       0.7       , 0.7       , 0.7       , 0.7       , 0.7       ,<br/>       0.7       , 0.7       , 0.7       , 0.7       , 0.7       ,<br/>       0.7       , 0.73333333, 0.73333333, 0.73333333, 0.83333333,<br/>       0.83333333, 0.83333333, 0.83333333, 0.83333333, 0.86666667,<br/>       0.86666667, 0.86666667, 0.86666667, 0.86666667, 0.86666667,<br/>       0.86666667, 0.86666667, 0.86666667, 0.86666667, 0.9       ,<br/>       0.9       , 0.9       , 0.9       , 0.93333333, 0.93333333,<br/>       0.93333333, 0.96666667, 0.96666667, 0.96666667, 0.96666667,<br/>       0.96666667, 0.96666667, 0.96666667, 0.96666667, 0.96666667,<br/>       0.96666667, 0.96666667, 0.96666667, 0.96666667, 0.96666667,<br/>       0.96666667, 0.96666667, 0.96666667, 0.96666667, 0.96666667,<br/>       0.96666667, 1.        , 1.        , 1.        , 1.        ,<br/>       1.        , 1.        , 1.        , 1.        , 1.        ,<br/>       1.        ])</span></pre><p id="8b55" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可以看到最后准确率变成了100%。</p><h2 id="4137" class="mj mk jj bd ml mm mn dn mo mp mq dp mr lh ms mt mu ll mv mw mx lp my mz na nb bi translated">结论</h2><p id="cd18" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">我的目标是演示随机梯度下降的实现。我希望那是成功的。如果你很难理解，我的建议是自己运行代码。会更清晰。你可以看到，为什么随机梯度下降如此受欢迎。因为我在每次迭代中使用了12个数据，而不是使用数据集的所有150个数据。当数据集中有100，000个数据点时，可以使用相同的技术。这将节省大量的时间和计算成本。</p><p id="6fe7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">欢迎在推特上关注我，喜欢我的T2脸书页面。</p><h2 id="7e97" class="mj mk jj bd ml mm mn dn mo mp mq dp mr lh ms mt mu ll mv mw mx lp my mz na nb bi translated">更多阅读</h2><div class="is it gp gr iu lu"><a rel="noopener follow" target="_blank" href="/a-complete-anomaly-detection-algorithm-from-scratch-in-python-step-by-step-guide-e1daf870336e"><div class="lv ab fo"><div class="lw ab lx cl cj ly"><h2 class="bd jk gy z fp lz fr fs ma fu fw ji bi translated">Python中从头开始的完整异常检测算法:分步指南</h2><div class="mb l"><h3 class="bd b gy z fp lz fr fs ma fu fw dk translated">基于概率的异常检测算法</h3></div><div class="mc l"><p class="bd b dl z fp lz fr fs ma fu fw dk translated">towardsdatascience.com</p></div></div><div class="md l"><div class="pn l mf mg mh md mi ja lu"/></div></div></a></div><div class="is it gp gr iu lu"><a rel="noopener follow" target="_blank" href="/a-complete-k-mean-clustering-algorithm-from-scratch-in-python-step-by-step-guide-1eb05cdcd461"><div class="lv ab fo"><div class="lw ab lx cl cj ly"><h2 class="bd jk gy z fp lz fr fs ma fu fw ji bi translated">Python中从头开始的完整K均值聚类算法:分步指南</h2><div class="mb l"><h3 class="bd b gy z fp lz fr fs ma fu fw dk translated">还有，如何使用K均值聚类算法对图像进行降维</h3></div><div class="mc l"><p class="bd b dl z fp lz fr fs ma fu fw dk translated">towardsdatascience.com</p></div></div><div class="md l"><div class="po l mf mg mh md mi ja lu"/></div></div></a></div><div class="is it gp gr iu lu"><a rel="noopener follow" target="_blank" href="/a-complete-recommender-system-from-scratch-in-python-step-by-step-6fc17a4da054"><div class="lv ab fo"><div class="lw ab lx cl cj ly"><h2 class="bd jk gy z fp lz fr fs ma fu fw ji bi translated">一个完整的推荐系统从零开始:一步一步</h2><div class="mb l"><h3 class="bd b gy z fp lz fr fs ma fu fw dk translated">基于用户评分的线性回归电影推荐系统</h3></div><div class="mc l"><p class="bd b dl z fp lz fr fs ma fu fw dk translated">towardsdatascience.com</p></div></div><div class="md l"><div class="pp l mf mg mh md mi ja lu"/></div></div></a></div><div class="is it gp gr iu lu"><a rel="noopener follow" target="_blank" href="/univariate-and-multivariate-gaussian-distribution-clear-understanding-with-visuals-5b85e53ea76"><div class="lv ab fo"><div class="lw ab lx cl cj ly"><h2 class="bd jk gy z fp lz fr fs ma fu fw ji bi translated">单变量和多变量高斯分布:直观清晰的理解</h2><div class="mb l"><h3 class="bd b gy z fp lz fr fs ma fu fw dk translated">详细的高斯分布及其与均值、标准差和方差的关系</h3></div><div class="mc l"><p class="bd b dl z fp lz fr fs ma fu fw dk translated">towardsdatascience.com</p></div></div><div class="md l"><div class="pq l mf mg mh md mi ja lu"/></div></div></a></div><div class="is it gp gr iu lu"><a rel="noopener follow" target="_blank" href="/exploratory-data-analysis-of-text-data-including-visualization-and-sentiment-analysis-e46dda3dd260"><div class="lv ab fo"><div class="lw ab lx cl cj ly"><h2 class="bd jk gy z fp lz fr fs ma fu fw ji bi translated">文本数据的探索性数据分析，包括可视化和情感分析</h2><div class="mc l"><p class="bd b dl z fp lz fr fs ma fu fw dk translated">towardsdatascience.com</p></div></div><div class="md l"><div class="pr l mf mg mh md mi ja lu"/></div></div></a></div><div class="is it gp gr iu lu"><a rel="noopener follow" target="_blank" href="/a-complete-free-course-on-inferential-statistics-for-data-scientists-in-r-129483f5f522"><div class="lv ab fo"><div class="lw ab lx cl cj ly"><h2 class="bd jk gy z fp lz fr fs ma fu fw ji bi translated">一个完全免费的R语言数据科学家推断统计学课程</h2><div class="mc l"><p class="bd b dl z fp lz fr fs ma fu fw dk translated">towardsdatascience.com</p></div></div><div class="md l"><div class="ps l mf mg mh md mi ja lu"/></div></div></a></div></div></div>    
</body>
</html>