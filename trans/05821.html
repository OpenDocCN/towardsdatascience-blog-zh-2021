<html>
<head>
<title>Neural Networks From the Ground Up (Part 1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始的神经网络(第一部分)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-networks-from-the-ground-up-part-1-880e6228d991?source=collection_archive---------37-----------------------#2021-05-24">https://towardsdatascience.com/neural-networks-from-the-ground-up-part-1-880e6228d991?source=collection_archive---------37-----------------------#2021-05-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a492" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解神经网络如何工作。不需要神经网络的先验知识。</h2></div><p id="fa13" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">大家都知道神经网络很神奇。在过去的二十年里，神经网络已经从一种实验方法发展成为我们今天使用最广泛的机器学习技术。现在，你自己也比以往任何时候都更容易获得神经网络的力量；像TensorFlow这样的框架使得构建神经网络变得轻而易举。然而，我认为它仍然是值得的，看看引擎盖下，了解一切是如何工作的。</p><p id="1de6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这将是一个多部分的系列，我们将详细研究神经网络，首先发展理论，然后从头开始编码。除了一些高中数学知识外，不需要任何先验知识，也不会使用任何框架——一切都将从头开始编码。在这第一部分中，我们着重于神经网络理论。</p><p id="c192" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">第一部分:神经网络力学</strong></p><p id="2114" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们从一个神经网络图开始:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/bf993ef6de4d41781efd93bd099074ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c-GAUGya27McMH2c0uT8vA.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">一个神经网络的例子。图片作者。</p></figure><p id="2544" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们网络的输入是向量<strong class="kk iu"> <em class="lu"> x </em> </strong> = (x₁，x₂).在机器学习行话中，我们将输入称为<strong class="kk iu">输入层</strong>。我们网络的目标是计算输出向量<strong class="kk iu"> <em class="lu"> o </em> </strong>，也称为<strong class="kk iu">输出层</strong>。从图中，我们注意到还有另一个向量，<strong class="kk iu"> <em class="lu"> h </em> </strong>，在我们可以计算<strong class="kk iu"> <em class="lu"> o </em> </strong>之前需要计算它。向量<strong class="kk iu"> <em class="lu"> h </em> </strong>被称为<strong class="kk iu">隐层</strong>，因为它夹在网络中间，不被只能看到输入输出层的外界观察者看到。在这个网络中只有一个隐藏层，实际上你可以有更多的隐藏层。</p><p id="0ad4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">各层的单个组件(图中的圆圈)被称为网络的<strong class="kk iu">节点</strong>。箭头上的数字称为网络的<strong class="kk iu">权重</strong>。每个节点的值是根据前一层的节点和来自这些节点的箭头上的权重来计算的。这听起来很混乱，所以我们来看一个具体的例子。隐藏层中的节点h₁有两个指向它的箭头，分别来自节点x₁和x₂.这两个箭头的权重分别为2和3。由于来自x₁的箭的重量是2，对h₁的贡献是2x₁.相应的，x₂对h₁的贡献是3x₂.因此h₁的价值是2x₁ + 3x₂.我们按照这种方法来计算隐藏层中所有节点的值:</p><blockquote class="lv"><p id="8073" class="lw lx it bd ly lz ma mb mc md me ld dk translated">h₁ = 2x₁ + 3x₂</p><p id="e780" class="lw lx it bd ly lz ma mb mc md me ld dk translated">h₂ = -2x₁ + 4x₂</p><p id="5ca8" class="lw lx it bd ly lz ma mb mc md me ld dk translated">h₃ = x₁ + 2x₂</p></blockquote><p id="2ac7" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">然而，我们还没有计算完<strong class="kk iu"><em class="lu"/></strong>。请注意隐藏层下橙色框中的sigma符号。这代表了一个<strong class="kk iu">激活函数</strong>，在我们从上一层计算出隐藏层的所有元素后，我们对它们进行元素化的使用。激活函数的存在是为了给我们的网络引入非线性，允许它学习更多类型的函数。在这篇<a class="ae mk" rel="noopener" target="_blank" href="/understanding-the-expressive-power-of-neural-networks-d4918c9e98da">文章</a>中，我会更详细地解释为什么会这样。一些常见的激活函数是sigmoid、tanh和ReLU函数。我们将在以后的文章中讨论这些常见的函数，因为现在我们假设σ是某个任意的激活函数。为了加入激活函数，我们需要将方程编辑为:</p><blockquote class="lv"><p id="71bf" class="lw lx it bd ly lz ma mb mc md me ld dk translated">h₁ = σ(2x₁ + 3x₂)</p><p id="6310" class="lw lx it bd ly lz ma mb mc md me ld dk translated">h₂ = σ(-2x₁ + 4x₂)</p><p id="b1d5" class="lw lx it bd ly lz ma mb mc md me ld dk translated">h₃ = σ(x₁ + 2x₂)</p></blockquote><p id="4a85" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">现在我们已经计算了<strong class="kk iu"> <em class="lu"> h </em> </strong>，我们可以计算<strong class="kk iu"> <em class="lu"> o </em> </strong>。再次跟随箭头，我们有:</p><blockquote class="lv"><p id="25a2" class="lw lx it bd ly lz ma mb mc md me ld dk translated">o₁=5h₁–2h₂–3h₃</p><p id="e1f0" class="lw lx it bd ly lz ma mb mc md me ld dk translated">o₂ = 4h₁ + 3h₂ + h₃</p></blockquote><p id="e8de" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">请注意，输出层上没有激活功能，这是由一个网络接一个网络做出的决定。有时输出层会有一个激活功能，在我们的例子中没有。</p><p id="5a9c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">第2部分:使用矩阵</strong></p><p id="4fea" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在已经看到了神经网络不同部分的概述，以及如何从输入层计算输出层。我们的下一个目标是使这种计算更有效。我们目前单独计算每一层的每个节点。如果有一种方法可以并行计算一层中的所有节点，我们可以节省很多时间。事实证明，矩阵乘法正是这样做的。再次考虑我们计算隐藏层的三个等式:</p><blockquote class="lv"><p id="5e89" class="lw lx it bd ly lz ma mb mc md me ld dk translated">h₁ = σ(2x₁ + 3x₂)</p><p id="1e65" class="lw lx it bd ly lz ma mb mc md me ld dk translated">h₂ = σ(-2x₁ + 4x₂)</p><p id="8844" class="lw lx it bd ly lz ma mb mc md me ld dk translated">h₃ = σ(x₁ + 2x₂)</p></blockquote><p id="a856" class="pw-post-body-paragraph ki kj it kk b kl mf ju kn ko mg jx kq kr mh kt ku kv mi kx ky kz mj lb lc ld im bi translated">我们可以用一个矩阵乘法方程来表示这三个方程:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/1df8bb9a4cc3bee9534950d9d9b8723f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*Ocy5tZrnqVnRzuyNqKpb6w.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">图片作者。</p></figure><p id="e0eb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以前σ是从一个标量到另一个标量的激活函数，现在σ是从一个矢量到另一个矢量的激活函数。你可以自己算出，这个矩阵方程确实给出了和上面一样的三个方程。我们也可以把这个矩阵方程写成矢量化的形式:<strong class="kk iu">h</strong>=σ(w₁*<strong class="kk iu"><em class="lu">x</em></strong>)。W₁被称为<strong class="kk iu">权重矩阵</strong>，因为它包含了从一层到另一层的权重。这里是W₁:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/c4a477d7930678b80647e89f32e7ad88.png" data-original-src="https://miro.medium.com/v2/resize:fit:320/format:webp/1*UkA7NB4D2QnOGf7CNbeUew.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">图片作者。</p></figure><p id="4922" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们还可以使用矩阵来计算输出层:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/7421cc5d8dcad7dbfc789ac35c14e125.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*4B1Ly7jGvTt6Bz5Z2ed49A.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">图片作者。</p></figure><p id="9102" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个方程的矢量化形式是<strong class="kk iu"><em class="lu">o</em></strong>=w₂*<strong class="kk iu"><em class="lu">h</em></strong>。W₂是另一个权重矩阵:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/d37b0adf84401b57e6ee8ae19bfb743b.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/format:webp/1*6_i-x8V7wd2Hw7Ff_PjxXQ.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">图片作者。</p></figure><p id="1dd4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在可以把整个神经网络写成矢量化的形式:<strong class="kk iu"><em class="lu">o</em></strong>=w₂*σ(w₁*<strong class="kk iu"><em class="lu">x</em></strong>)。当我们在以后的文章中为我们的网络编写代码时，我们会看到这种矢量化的形式更容易实现，并且运行速度比我们原来的方程快得多。我们的矢量化方程也可以重写为函数的组合。这看起来像是无用的记数工作，但是当我们推导出训练网络的规则时，另一种形式就会派上用场。</p><p id="c0d6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">考虑函数f(W，<strong class="kk iu"><em class="lu">x</em></strong>)= W *<strong class="kk iu"><em class="lu">x</em></strong>。该函数取一个矩阵W和一个向量<strong class="kk iu"> <em class="lu"> x </em> </strong>作为其输入，并返回矩阵乘法结果作为输出。我们可以把我们的矢量化网络改写为<strong class="kk iu"> <em class="lu"> o </em> </strong> = f(W₂，σ(f(W₁，<strong class="kk iu"> <em class="lu"> x </em> </strong>))。这将是我们在以后的文章中用来推导训练网络的规则的等式。</p><p id="c8ec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">到目前为止，我们已经检查了神经网络的机制，并找到了一种很好的计算输出层的矢量化方法。在下一篇文章中，我们将学习如何训练网络。敬请期待！</p></div></div>    
</body>
</html>