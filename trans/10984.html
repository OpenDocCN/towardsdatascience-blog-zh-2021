<html>
<head>
<title>Deep Learning Optimization Theory — Introduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习优化理论—简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-learning-optimization-theory-introduction-148b3504b20f?source=collection_archive---------16-----------------------#2021-10-26">https://towardsdatascience.com/deep-learning-optimization-theory-introduction-148b3504b20f?source=collection_archive---------16-----------------------#2021-10-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4fd8" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">理解深度学习中的优化理论对于实现进步至关重要。这篇文章介绍了研究它的实验和理论方法。</h2></div><p id="abe8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在过去十年中，深度学习在学术界和工业界都蓬勃发展。几十年来众所周知难以解决的现实世界和学术问题，如计算机视觉、自然语言处理和玩游戏，现在正通过深度学习方法得到高度成功的解决。</p><p id="9df5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，尽管深度学习取得了这些重大进步，但对其成功的理论理解仍然滞后。我们的理解范围从基于实验的传统智慧和直觉到对玩具问题的更深入分析，这些玩具问题并不类似于真实世界深度学习架构的真正复杂性。这篇博客旨在强调从挑战传统智慧的深度学习理论研究中获得的有趣经验。</p><p id="1daf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一般来说，深度学习由以下三个支柱组成:</p><ol class=""><li id="913b" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la lg lh li lj bi translated"><strong class="kh ir">表达能力</strong> —我们可以用神经网络表达什么功能？就一个神经网络所能代表的功能集而言，它对另一个神经网络的效率如何？现代神经网络存在哪些归纳偏差？</li><li id="9607" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated"><strong class="kh ir">优化</strong>——对于给定的任务，我们如何找到神经网络的最佳权重？能保证找到最好的吗？我们能多快做到？</li><li id="d5d2" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated"><strong class="kh ir">推广</strong> —为什么一个训练集的解决方案可以很好地推广到一个未知的测试集？我们能限制泛化误差吗？</li></ol><p id="2cea" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇文章将重点讨论优化。假设读者熟悉 SGD、深度学习和一些优化背景。</p></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="241c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">深度学习理论——优化</strong></p><p id="3c35" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">凸函数的最优化被认为是数学中一个成熟的领域。因此，可以使用成熟的工具和理论来回答上一段中描述的优化问题。然而，复杂的非凸函数的优化很难分析。既然深度神经网络(是的，线性网络也是)的优化是非凸的，我们如何尝试回答这些问题呢？人们可能会寻找广泛的经验证据来证明 SGD 在现实世界问题上收敛于全局极小值。另一个可能会寻找一个严格的理论解释其收敛和条件，它发生了。由于这两种方法，实验的和理论的帮助推动了我们的知识，在这篇博文中，我将展示这两种方法的代表性作品。我要介绍的第一种方法是实验方法。</p><p id="410e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">DL 中的优化—一种实验方法</strong></p><p id="fa52" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Goodfellow 等人的作品“<a class="ae lw" href="https://arxiv.org/pdf/1412.6544.pdf" rel="noopener ugc nofollow" target="_blank">定性表征神经网络优化问题</a>”。艾尔。提出了一种简单的工具来解决上述针对特定实际网络和问题的优化问题。让我们假设你训练你的网络收敛到全局最优(训练损失接近零)。现在，取初始化时的权重和收敛时的权重，并简单地在初始权重和收敛权重之间的参数空间中的<strong class="kh ir">线上的一系列点处评估您的损失函数。更正式地说:</strong></p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/64cbe8b09942f96adddb69f8f9b9573e.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*Lcxq_st-FDU_OKEtHJVW-w.png"/></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">沿着θi(初始化时的权重)到θc(收敛时的权重)之间的线采样θa。</p></figure><p id="bc65" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因为我们怀疑损失函数是高度非凸的，所以很难想象损失函数沿着这条线的行为。令人惊讶的是，在 MNIST 上训练的前馈神经网络上执行该实验揭示了以下行为:</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/1a1d38780a96787a045fc9af5e6d3b26.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*eKPrI_rrH1cdxslL3Fa5_A.png"/></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">在 MNIST 数据集上训练的具有 ReLU 激活的全连接网络上的线性插值实验。这表明目标函数在模型的初始参数和目标参数之间的线内是平滑的。图片<a class="ae lw" href="https://arxiv.org/pdf/1412.6544.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>。</p></figure><p id="ec40" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不直观地，我们观察到这条线上的损失函数是连续的单调递减函数。这表明，虽然损失函数本身是非凸的，但有一条“几乎凸的”路径通向收敛点。此外，其他架构的其他实验(CNN/RNN)揭示了类似的现象。请阅读<a class="ae lw" href="https://arxiv.org/pdf/1412.6544.pdf" rel="noopener ugc nofollow" target="_blank">论文</a>了解更多详情。如果你有兴趣自己尝试一下，可以用<strong class="kh ir"> </strong> <a class="ae lw" href="https://github.com/OmriKaduri/nn-linear-path-experiments/blob/main/MNIST-Linear-Experiments.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">我为它实现</strong> </a>。</p><p id="9c77" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">既然我们知道从初始权重到收敛存在一条平滑单调递减的路径，有人可能会问 SGD 遵循这条线性路径吗？为了回答这个问题，我们正在寻找一种方法来评估“如果 SGD 被投射到生产线上，它是如何沿着生产线前进的”。此外，在那些权重不在 1D 子空间上的情况下，我们想要评估“它们离线有多远”。我们用<strong class="kh ir"> α </strong>表示沿线的距离，用<strong class="kh ir"> β </strong>表示离该线的距离。在深入研究α和β的数学定义之前，请注意以下几点:</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi mk"><img src="../Images/07c01f29983d4131becb8522e20b5a32.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eb0m--oYUHNh70itE7VFEA.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">水平轴— α(沿线性路径的进度— 0 表示初始，900 表示最终模型)。垂直轴— β(时间<strong class="bd mp"> T </strong>时 SGD 重量与线性路径的距离)。图片<a class="ae lw" href="https://arxiv.org/pdf/1412.6544.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>。</p></figure><p id="d13c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有趣的是，似乎随着 SGD 的发展，它有一个对称的发散和收敛模式。起初，它从它那里发散，然后在几乎完全对称的情况下，收敛回它那里。</p><p id="db1b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们快速描述一下这些参数的数学。如果你只关心直觉，跳过下面的段落，或者如果你想自己运行这个实验，就参考<a class="ae lw" href="https://github.com/OmriKaduri/nn-linear-path-experiments/blob/main/MNIST-Linear-Experiments.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">我的实现</strong> </a>。</p><p id="62c1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们将<strong class="kh ir"> u </strong>定义为从θi 指向θc 的单位向量，我们可以将θi 到θc 之间的线性路径描述为:θi + α(t)*u，其中α(t)是一个标量，表示“线上已经取得了多少进展”，定义为:</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/f1092688ea3885401cb128a45dddd978.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*GlOUTGjjTsmR2dGtGOMrmA.png"/></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">对于时间步长 t，计算 SGD 从初始化到收敛在直线上的投影进度。</p></figure><p id="3c4e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们有了α，它指定了我们沿着这条线前进的幅度。缺少的分量是离它的距离。让我们定义另一个单位向量<strong class="kh ir"> v </strong>,它指向从θ_ t 到线上计划进度的方向。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/49f13271a43f3628b790eeca75c39383.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*nHRAaDpd-B17ORcDc5QXIA.png"/></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">对于时间戳 t，β是 SGD 的权重的范数(<strong class="bd mp">距离</strong>)，来自它们在从初始化到收敛的线路上的预计进度。</p></figure><p id="dc0b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就是重现上述实验所需的全部内容！</p><p id="9261" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">这种现象适用于现代神经网络吗？</strong></p><p id="a605" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">今天回顾 2014 年的那些结果，在深度学习领域取得巨大进展后，人们可能会要求重新审视原始论文中观察到的现象。事实上，<a class="ae lw" href="https://arxiv.org/abs/2012.06898" rel="noopener ugc nofollow" target="_blank"> Jonathan Frankle </a>在 2020 年对现代神经网络架构和数据集进行了实验，发现情况可能不如简单数据集上的简单架构好。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi ms"><img src="../Images/cf0db0b2ded8a6d77d8ecee5a1676c57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JCiqLJHxbIWz23QsDl6KwA.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">在 ImageNet 数据集上使用 ResNet-50 重新访问线性插值实验。从初始到收敛，损耗不再单调递减。图片<a class="ae lw" href="https://arxiv.org/abs/2012.06898" rel="noopener ugc nofollow" target="_blank">来源</a>。</p></figure><p id="a59c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">令人惊讶的是，在现代 NN 架构中，他没有发现这种现象发生。更奇怪的是，他发现在 ImageNet 上使用 ResNet-50 进行的这个实验导致了上面显示的奇特观察结果。沿着初始权重到收敛之间的线，损失保持几乎相同的值，然后少量增加并下降到收敛。</p><p id="dfc8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">一种可能的解释(固执己见)</strong></p><p id="631d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">郝力等人的作品<a class="ae lw" href="https://arxiv.org/abs/1712.09913" rel="noopener ugc nofollow" target="_blank">“可视化神经网络的损失景观</a>”。艾尔。提供了一种可视化损失函数曲率的方法。使用他们的可视化方法(我将在以后的帖子中使用！)他们探讨了不同的网络架构如何影响损失情况。令人惊讶的是，将跳跃连接添加到深度神经网络中，可以显著减少从混沌到“几乎凸起”的景观，如下图所示。</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi mt"><img src="../Images/4c4f43e31ffafcc7f38239b25aed5d06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2ljDG567CGfMrt5hA_R2Mw.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">对用 ResNet-50 观察到的现象的一种可能的(自以为是的)解释。如果我们假设 ResNet 的跳跃连接使损失函数“凸化”,它可以解释“在突然下降到全局最小值之前损失不变”的现象。<a class="ae lw" href="https://arxiv.org/pdf/1712.09913.pdf" rel="noopener ugc nofollow" target="_blank">图片来源</a>。</p></figure><p id="fb8e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这一变化可能会弥合 Goodfellow 等人的现象。艾尔。发现于 2014 年，弗兰克尔发现于 2020 年。在图像的右侧，很明显，在初始化权重(假设在平坦区域的某处)和收敛权重之间有一个大的平坦插值区域。此外，在整个插值过程中，损失值变化不大。</p><p id="5274" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">总结一下<strong class="kh ir">实验方法</strong>，可以说它们给了我们一种关于优化问题难度的乐观感。这表明问题没有我们想象的那么难。我们如何严格证明(或反驳)这个假设？</p></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="9707" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">DL 中的优化—一种理论方法</strong></p><p id="e3cc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然实验方法产生了有助于我们将深度学习推向有趣的应用用途的传统智慧，但它并没有为我们所观察到的现象提供合理的解释，正如上一节中对现代架构的重新实验所看到的那样。具体来说，我将介绍一种方法，旨在更好地理解损失情况，从而了解其优化特征。另一种方法是理解梯度下降本身的轨迹，我将在以后的文章中介绍。</p><p id="bb1b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">亏损景观有多好看？</strong></p><p id="51ac" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Itay Safran 和 Ohad Shamir 的工作“<a class="ae lw" href="https://arxiv.org/pdf/1511.04210.pdf" rel="noopener ugc nofollow" target="_blank">关于过度指定的神经网络</a>中初始盆地的质量”，试图通过模拟(非凸)损失函数的几何结构来解决这个问题。这种方法的动机是，希望我们会发现<strong class="kh ir">损失图</strong>便于优化(更具体地说，是本地搜索算法，如 SGD)。我们之前看到的关于经验上存在的单调递减路径的观察鼓励研究人员解释这一现象。</p><p id="9dc2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">具体来说，这项工作提供了以下定理:给定一个全连通网络，它在初始化和收敛时的权重，以及一个凸损失函数，存在一个从该初始化点开始并以相同损失值(作为收敛点)的<strong class="kh ir">结束的<strong class="kh ir">单调递减路径</strong>。为了证明该定理，主要的重要假设是<strong class="kh ir">全零预测器的损失小于初始化时的损失</strong>。</strong></p><p id="3858" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">尽管现代神经网络不支持这种假设(由于权重初始化技术的最新进展)，但他们的结果非常有趣，因为它与我们之前看到的经验结果一致。他们证明，在他们的假设下，有一个“优化的简单方法”。</p><p id="88a9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然为他们的定理提供充分的证明和微妙的细节肯定超出了一篇博客文章的范围，但让我们试着理解一下总体思路。他们证明中的主要方法是为从初始到收敛的连续路径中的每个点创建<strong class="kh ir">，在单调递减路径中创建相应的点。</strong>它们是如何构造对应点的？它们为平方损失的特殊情况提供了直觉:</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi mu"><img src="../Images/0271ac3f76dc758a7e3fd4fa0c603edb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*QvyX_kmJtCFXBWKCwfVA8Q.png"/></div></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">对具有<strong class="bd mp"> m </strong>个样本的数据集，在由权重<strong class="bd mp"> W </strong>参数化的神经网络<strong class="bd mp">N</strong>上评估的平方损失。</p></figure><p id="cc02" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于该目标下神经网络的最后一层是线性的，请注意，最后一层乘以某个标量<strong class="kh ir"> c </strong>，对应于乘以网络的输出。因此，给定一些权重<strong class="kh ir"> W(λ) </strong>(其中<strong class="kh ir"> </strong> 0≤ <strong class="kh ir"> λ </strong> ≤1 定义了初始化到收敛之间的路径)，我们可以将最后一层乘以标量 c≥0，并接收相应的权重(用 W’表示)，目的是:</p><figure class="ly lz ma mb gt mc gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/9cc3678ceea922369dcc3c4d67959139.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*An2fvFiALWYXPK_OhlsGpw.png"/></div><p class="mf mg gj gh gi mh mi bd b be z dk translated"><strong class="bd mp">的相应目标 W(λ) </strong>通过将其最后一层乘以标量<strong class="bd mp"> c. </strong></p></figure><p id="a1f6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，对于 c=0，这个目标是 Y 的平均值。对于 c=∞，它也是∞的。它基本上是说(利用<a class="ae lw" href="https://en.wikipedia.org/wiki/Intermediate_value_theorem" rel="noopener ugc nofollow" target="_blank">中值定理</a>)我们可以调 c 得到从 Y 的均值到∞的任意客观值。现在剩下的就是为每个 W(λ)找到它对应的 c，这样我们就得到期望的单调递减路径！</p><p id="d155" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">视觉回顾</strong></p><p id="82b0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">理解这个想法可能很有挑战性。为了让你更容易理解，我制作了(使用伟大的<a class="ae lw" href="https://github.com/ManimCommunity/manim" rel="noopener ugc nofollow" target="_blank"> manim </a>工具，源自<a class="ae lw" href="https://github.com/3b1b/manim" rel="noopener ugc nofollow" target="_blank"> 3B1B </a>)一个简短的 GIF 来说明这个证明的想法:</p><figure class="ly lz ma mb gt mc"><div class="bz fp l di"><div class="mw mx l"/></div><p class="mf mg gj gh gi mh mi bd b be z dk translated">从初始化到收敛，从连续路径构建相应的单调递减路径的可视化。视频来源:<a class="ae lw" href="https://omrikaduri.github.io/" rel="noopener ugc nofollow" target="_blank">我</a>。</p></figure><p id="2e88" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">可视化从蓝色曲线开始，表示从初始到收敛的连续但非凸的曲线。然后，它突出显示了“全零预测值假设”,并从中投射光线来构建单调递减路径。</p></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="43d0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">结论</strong></p><p id="f750" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们理解深度学习模型优化的第一步总体上是乐观的。以前的优化知识告诉我们，优化高维非线性和非凸问题是一个众所周知的难题。然而，在真实世界模型上的实验(FC/CNN/RNN)提供了乐观的结果。此外，对损失情况的理论分析也很乐观。在某种意义上，这两种方法(实验的和理论的)都表明，我们用 SGD 在深度学习中解决的问题可能比人们想象的要容易得多。</p><p id="3f5c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，跳到 2020 年，我们观察到用现代架构和数据集重新审视这些实验揭示了不同的现象。虽然从实践者的角度来看，这些早期的发现是令人鼓舞的，但是仍然需要 SGD 收敛背后的理论。幸运的是，研究人员近年来取得了显著的成果，其中一些采用了不同的景观方法，我将在未来的帖子中进行回顾。</p><p id="66b2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">确认</strong></p><p id="35d0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我要感谢<a class="ae lw" href="https://www.cohennadav.com/" rel="noopener ugc nofollow" target="_blank"> Nadav Cohen </a>在特拉维夫大学教授了一门关于深度学习理论的伟大课程。</p></div></div>    
</body>
</html>