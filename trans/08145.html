<html>
<head>
<title>Data to Model to API: An End-to-End Approach</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据到模型到API:一种端到端的方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/data-to-model-to-api-an-end-to-end-approach-9343f2dde848?source=collection_archive---------35-----------------------#2021-07-26">https://towardsdatascience.com/data-to-model-to-api-an-end-to-end-approach-9343f2dde848?source=collection_archive---------35-----------------------#2021-07-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="dcdf" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">了解如何将ML/DL生命周期阶段的整个结构缝合在一起。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/9eee33a1a19fd403a695f66dafdbc022.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p6kRKjD-LxY_Bn3ZkbvrOA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">ML/DL API/模型服务架构(图片由作者提供)</p></figure><p id="633e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这篇博客中，我们不打算讨论强大的擎天柱，而是要经历一个基于机器学习的用例的整个旅程。我们将从数据处理开始，继续构建数据管道，使用Tensorflow为训练模型提供数据，使用高级<strong class="kx ir"> tf.keras </strong> API构建模型架构，最后使用FastAPI为模型提供服务。此外，我们将使用另一种更易维护的方式来服务ML模型，TFX服务于Tensorflow APIs的扩展特性。本文不讨论优化ML模型，因为这里的目的是从头到尾学习ML模型的可操作性方面。</p><p id="f77f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">文章的流程如下:</p><ol class=""><li id="6ad2" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated"><strong class="kx ir">问题陈述</strong></li><li id="8d0e" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated"><strong class="kx ir">数据预处理</strong></li><li id="49d4" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated"><strong class="kx ir">构建数据管道</strong></li><li id="d2be" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated"><strong class="kx ir">模型设计、培训和评估</strong></li><li id="f22e" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated"><strong class="kx ir">使用TFX服务的模型部署(REST API)</strong></li></ol><h1 id="d562" class="mf mg iq bd mh mi mj mk ml mm mn mo mp jw mq jx mr jz ms ka mt kc mu kd mv mw bi translated">问题陈述</h1><p id="bd7b" class="pw-post-body-paragraph kv kw iq kx b ky mx jr la lb my ju ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">我们需要找到一种方法来评估给定推文的情绪，即推文是正面情绪还是负面情绪。</p><p id="25ab" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，在不使用任何ML技术的情况下解决这个问题的最基本的方法是，在使用可用的注释数据分别去除两个类别(正面和负面)的无关紧要的停用词之后，制作单词的频率字典。然后，对于每条推文，在删除停用词后，就像我们在构建频率字典时所做的那样，将推文中的词与它们在两个类中各自的频率进行映射，最后，我们可以对所有词的总频率进行求和，包括正面和负面，无论哪个类的值更高，我们都会将该类分配给该推文。尽管这种方法在可伸缩性方面确实是廉价且高效的，但是这种方法的性能不会很好，原因如下:</p><ul class=""><li id="9b31" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq nc lx ly lz bi translated">我们正在定义非常严格的界限，根据单词的频率总和给推文分配标签。大多数情况下，两个类别中单词的频率和之间的差异可以忽略不计，这种方法是多余的。</li><li id="355e" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq nc lx ly lz bi translated">如果对于一条推文，两个类别中单词的频率总和相等或为零，会发生什么？这是否意味着该推文具有中性情绪，如果是的话，与该情绪相关联的概率是多少？我们不能用这种方法来估计与任何推论相关的概率。</li></ul><p id="afbf" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">即使我们试图通过手工创建自己的特性来使用ML解决这个问题，我们也不知道哪些潜在的特性可能会真正破解这个问题。在为我们的模型创建特性时，我们只是基于我们自己的假设进行操作。而且，这种方法仍然没有考虑tweet中单词和句子之间的相对上下文。这种方法不能解决各种各样的问题，例如意义、上下文、词义消歧以及同音异义词等等。</p><p id="3b78" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">输入单词嵌入。最大似然算法只能理解数字，而对于这个问题，我们只有文本。因此，我们需要想出一种方法来将我们的文本转换成数字，同时保留文本的含义和上下文。现在，这就是单词嵌入的实际作用，单词嵌入是密集的向量，实际上有助于将我们文本的上下文表示到n维多维空间中，维数是在创建向量时选择的。足够的理论，让我们直接进入如何创建我们自己的模型，可以给我们的文本数据点向量。</p><h1 id="f152" class="mf mg iq bd mh mi mj mk ml mm mn mo mp jw mq jx mr jz ms ka mt kc mu kd mv mw bi translated">数据预处理和分析</h1><h2 id="e250" class="nd mg iq bd mh ne nf dn ml ng nh dp mp le ni nj mr li nk nl mt lm nm nn mv no bi translated"><strong class="ak">预处理</strong></h2><p id="e6d1" class="pw-post-body-paragraph kv kw iq kx b ky mx jr la lb my ju ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">在开始这个数据预处理的旅程之前，这里有几个我们将用来预处理我们的文本的特殊库的快速回顾，现在，我们可以编写我们自己的逻辑来做这个库做的事情，但是这里有一个黄金法则，永远不要浪费你的时间去编写一段别人已经通过投入时间和精力编写的代码。但是我建议您在实现任何随机库或StackOverflow的一段代码之前，应该完全理解它的作用。所以，我们开始吧:</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="45f8" class="nd mg iq nq b gy nu nv l nw nx">!pip install wordninja contractions emoji<br/>!wget <a class="ae ny" href="http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip" rel="noopener ugc nofollow" target="_blank">http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip</a><br/>!unzip trainingandtestdata.zip</span><span id="ae40" class="nd mg iq nq b gy nz nv l nw nx">import numpy as np<br/>import pandas as pd<br/>import re<br/>import os<br/>import string<br/>import tensorflow as tf<br/>from sklearn.model_selection import train_test_split<br/>import wordninja, contractions, emoji</span></pre><p id="1672" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在数据中，我们只对包含tweet文本和情感值的两列感兴趣。“0”代表消极情绪，“1”显然代表积极情绪，因为这是一个二元分类问题。让我们看看数据是什么样的:</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="64ca" class="nd mg iq nq b gy nu nv l nw nx"># Data IO and Check<br/>train = pd.read_csv("training.1600000.processed.noemoticon.csv", encoding='latin-1', usecols=[0,5], names=['sentiment','tweet'])<br/>test = pd.read_csv("testdata.manual.2009.06.14.csv", encoding='latin-1', usecols=[0,5], names=['sentiment','tweet'])<br/>print(train.sentiment.value_counts()/train.shape[0])<br/>print(test.sentiment.value_counts()/test.shape[0])</span><span id="00d1" class="nd mg iq nq b gy nz nv l nw nx">OUTPUT:</span><span id="7c35" class="nd mg iq nq b gy nz nv l nw nx">4    0.5<br/>0    0.5<br/>Name: sentiment, dtype: float64</span><span id="6da4" class="nd mg iq nq b gy nz nv l nw nx">4    0.365462<br/>0    0.355422<br/>2    0.279116<br/>Name: sentiment, dtype: float64</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/274327ddfd7000b406d0199d694453bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*XFvaBa4BzOy8GXs_W11MkA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">数据概述</p></figure><p id="f012" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">由于我们在测试数据中有额外的离散极性，我们将去除它们以保持数据集之间的非冗余性，并将我们的标签转换为“0”和“1”。一条推文可能包含表情符号、标签和个人资料标签，需要正确处理才能捕捉推文的含义。因此，下面是我们要做的几个预处理步骤:</p><ul class=""><li id="2ca7" class="lr ls iq kx b ky kz lb lc le lt li lu lm lv lq nc lx ly lz bi translated">移除所有人物标签、表情符号、网址，并将标签拆分成有意义的单词。比如# programmingismeditation会变成“编程即冥想”</li><li id="bb79" class="lr ls iq kx b ky ma lb mb le mc li md lm me lq nc lx ly lz bi translated">将所有的简写形式，如“I'd”和“we 're ”,分别转换成它们的词根形式，如“I would”或“I had”和“we are”。基本上，我们要扩大所有的收缩。</li></ul><p id="8283" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">和一些基本的文本清理步骤，这些步骤将在代码中变得显而易见:</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="91fc" class="nd mg iq nq b gy nu nv l nw nx">''' Since we are working on a binary classification problem, we would remove all intermediate polarities from test data <br/>and only work on highly polarized sentiment data points. '''</span><span id="d589" class="nd mg iq nq b gy nz nv l nw nx">test = test[test.sentiment!=2]<br/>test.sentiment.value_counts()/test.shape[0]</span><span id="b3c4" class="nd mg iq nq b gy nz nv l nw nx">def <strong class="nq ir">strip_emoji</strong>(tweet):<br/>  new_tweet = re.sub(emoji.get_emoji_regexp(), r"", tweet)<br/>  return new_tweet.strip()</span><span id="743a" class="nd mg iq nq b gy nz nv l nw nx">def <strong class="nq ir">strip_urls</strong>(tweet):<br/>  new_tweet = re.sub(r'(https|http)?:\/\/(\w|\.|\/|\?|\=|\&amp;|\%)*\b', '', tweet, flags=re.MULTILINE)<br/>  return new_tweet.strip()</span><span id="7cc2" class="nd mg iq nq b gy nz nv l nw nx">def <strong class="nq ir">remove_tags</strong>(tweet):<br/>  return " ".join([token for token in tweet.split() if not token.startswith("@")])</span><span id="a200" class="nd mg iq nq b gy nz nv l nw nx">def <strong class="nq ir">preprocess_tweet</strong>(tweet):<br/>  tweet = remove_tags(strip_emoji(strip_urls(tweet)))<br/>  tweet = contractions.fix(" ".join(wordninja.split(tweet)))<br/>  tweet = [token.lower() for token in tweet.split() if (len(set(token))&gt;1)]<br/>  return " ".join(tweet)</span><span id="2cfe" class="nd mg iq nq b gy nz nv l nw nx"># Preprocessing tweets data<br/>train.tweet = train.tweet.apply(preprocess_tweet)<br/>test.tweet = test.tweet.apply(preprocess_tweet)</span><span id="37c9" class="nd mg iq nq b gy nz nv l nw nx"># Preprocessing Labels<br/>train.sentiment = train.sentiment.apply(lambda value: 1 if value==4 else value)<br/>test.sentiment = test.sentiment.apply(lambda value: 1 if value==4 else value)</span></pre><h2 id="db2b" class="nd mg iq bd mh ne nf dn ml ng nh dp mp le ni nj mr li nk nl mt lm nm nn mv no bi translated"><strong class="ak">分析</strong></h2><p id="ace0" class="pw-post-body-paragraph kv kw iq kx b ky mx jr la lb my ju ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">现在我们已经完成了数据预处理，让我们进入下一个重要的事情，即对于一个好的模型来说非常重要，这比超参数调整更重要。这是估计每个实例中输入模型的词汇大小和向量序列长度。现在，我们通过分析手头的训练数据分布来做到这一点。我们将绘制训练数据中推文长度的分布图。Tweet length就是tweet post预处理中的字数。下面是这样做的代码:</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="98fc" class="nd mg iq nq b gy nu nv l nw nx"># Estimating vocab size and max sequence length to allow in vectorization layer.<br/>def <strong class="nq ir">tweet_length</strong>(tweet):<br/>  return len([token for token in tweet.split()])</span><span id="a29a" class="nd mg iq nq b gy nz nv l nw nx">import seaborn as sns<br/>tweet_lengths = [tweet_length(tweet) for tweet in train_tweets.tolist()]<br/>sns.distplot(tweet_lengths)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/8783bf6434753a3fe23256801f6bb658.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*mxt0VgTFr_Lz3BY8WOMVng.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">推文长度分布</p></figure><p id="77fb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">上面的viz将帮助我们决定实例的最大序列长度。接下来，我们需要编写代码来估计词汇量:</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="a87e" class="nd mg iq nq b gy nu nv l nw nx"># Unique words<br/>unique_words = set([token for tweet in train_tweets for token in tweet.split()])<br/>print("Total Unique Words:", len(unique_words))</span><span id="2bcf" class="nd mg iq nq b gy nz nv l nw nx"># Counting Total Words and Stop Words<br/>import nltk<br/>nltk.download("stopwords")<br/>from nltk.corpus import stopwords<br/>stop_words = stopwords.words("english")<br/>total_words = [token for tweet in train_tweets for token in tweet.split()]<br/>total_stop_words = [token for tweet in train_tweets for token in tweet.split() if token in stop_words]<br/>print('Total Stop Words', len(total_words))<br/>print('Total Stop Words', len(total_stop_words))<br/>print('Ratio of Total Words to Total Stop Words:', len(total_words)/len(total_stop_words))</span><span id="386b" class="nd mg iq nq b gy nz nv l nw nx">OUTPUT:</span><span id="860f" class="nd mg iq nq b gy nz nv l nw nx">Total Unique Words: 75553<br/>[nltk_data] Downloading package stopwords to /root/nltk_data...<br/>[nltk_data]   Unzipping corpora/stopwords.zip.<br/>Total Stop Words 17861874<br/>Total Stop Words 7690978<br/>Ratio of Total Words to Total Stop Words: 2.322445077856158</span></pre><p id="5f44" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">使用上面这段代码，我们将估计词汇量。看起来在平均10个单词的tweet中，我们几乎有4个停用词。而且，由于我们正在执行情感分析，删除一些停用词(如“not ”)会完全颠倒文本的意思，并且有许多这样的词(您可能想自己检查“nltk”包中的“停用词”)。因此，我们避免删除它们(除了单字单词，以避免词汇中有无用的单词)。</p><p id="f760" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">根据上面的分析，我们将把最大序列长度固定为50，词汇大小固定为75000。</p><h1 id="b273" class="mf mg iq bd mh mi mj mk ml mm mn mo mp jw mq jx mr jz ms ka mt kc mu kd mv mw bi translated">构建数据管道</h1><p id="edc4" class="pw-post-body-paragraph kv kw iq kx b ky mx jr la lb my ju ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">我们已经预处理了数据，估计了最大序列长度和词汇大小。现在，我们需要开发一个管道，将数据批量输入到模型中。当您的GPU数量有限，并且不想一次性将整个数据集加载到GPU中并训练模型时，这非常有用。利用这一点，我们将数据一批一批地加载到固定大小的GPU(模型也将在这里被训练)上来训练模型。我们使用TF数据集API来做到这一点:</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="0225" class="nd mg iq nq b gy nu nv l nw nx"># Data Pipeline Function using TF Dataset API<br/>def <strong class="nq ir">data_input_fn</strong>(texts, labels, batch_size=32, is_training=True):<br/>  # Convert the inputs to a Dataset.<br/>  dataset = tf.data.Dataset.from_tensor_slices((texts,labels))<br/>  # Shuffle, repeat, and batch the examples.<br/>  dataset = dataset.cache()<br/>  if is_training:<br/>    dataset = dataset.shuffle(1000, reshuffle_each_iteration=True)<br/>    dataset = dataset.repeat()<br/>  dataset = dataset.batch(batch_size, drop_remainder=True)<br/>  # Return the dataset.<br/>  return dataset</span><span id="8019" class="nd mg iq nq b gy nz nv l nw nx"># Data pipelines for 3 different datasets<br/>training_dataset = data_input_fn(train_tweets, train_labels, batch_size=1024)<br/>validation_dataset = data_input_fn(val_tweets, val_labels, batch_size=128, is_training=False)<br/>test_dataset = data_input_fn(test.tweet, test.sentiment, batch_size=8, is_training=False)</span></pre><h1 id="6927" class="mf mg iq bd mh mi mj mk ml mm mn mo mp jw mq jx mr jz ms ka mt kc mu kd mv mw bi translated">模型设计、培训和评估</h1><h2 id="7663" class="nd mg iq bd mh ne nf dn ml ng nh dp mp le ni nj mr li nk nl mt lm nm nn mv no bi translated"><strong class="ak">文本矢量化层</strong></h2><p id="6a6c" class="pw-post-body-paragraph kv kw iq kx b ky mx jr la lb my ju ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">通常，我们首先对文本进行矢量化，然后将其提供给模型进行训练或推理。但是，如果我告诉你，我们不必再单独完成这一步，会怎么样呢？因此，<strong class="kx ir"> tf.keras </strong>已经添加了文本矢量化API来帮你做这件事。我们是这样做的:</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="0741" class="nd mg iq nq b gy nu nv l nw nx"># Creating Vectorization Layer<br/>max_features = 75000<br/>max_len = 50</span><span id="4c82" class="nd mg iq nq b gy nz nv l nw nx">vectorization_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(<br/>    max_tokens=max_features, output_sequence_length=max_len)<br/>vectorization_layer.adapt(train_tweets.values)</span></pre><p id="d45d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们简单地创建一个TextVectorization层的对象，然后调用一个方法将该层调整到我们的训练数据(“adapt”方法)。接下来，我们在模型设计步骤中使用这一层。</p><h2 id="c286" class="nd mg iq bd mh ne nf dn ml ng nh dp mp le ni nj mr li nk nl mt lm nm nn mv no bi translated"><strong class="ak">设计</strong></h2><p id="74f5" class="pw-post-body-paragraph kv kw iq kx b ky mx jr la lb my ju ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">下面代码设计了一个使用“TF . keras”API的深度学习模型。代码完全是不言自明的。</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="c4ae" class="nd mg iq nq b gy nu nv l nw nx"># Create Model Func<br/>def <strong class="nq ir">create_model</strong>():<br/>  words = tf.keras.Input(shape=(1,), dtype=tf.string)<br/>  vectors = vectorization_layer(words)<br/>  embeddings = tf.keras.layers.Embedding(input_dim=max_features+1, output_dim=128)(vectors)<br/>  output = tf.keras.layers.LSTM(256, return_sequences=True, name='LSTM_1')(embeddings)<br/>  output = tf.keras.layers.LSTM(256, name='LSTM_2')(output)<br/>  output = tf.keras.layers.Dropout(0.3)(output)<br/>  output = tf.keras.layers.Dense(64, activation='relu', name='Dense_3')(output)<br/>  output = tf.keras.layers.Dense(1,activation='sigmoid', name='Output')(output)</span><span id="e0bd" class="nd mg iq nq b gy nz nv l nw nx">  model = tf.keras.models.Model(words,output)<br/>  return model</span></pre><p id="ae9b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这段代码中唯一的新东西是前两行函数。输入形状是一个秩为1的数组，因为我们将传递给模型的是一个句子/tweet，而不是一个向量(因此，形状是(1，)。接下来，我们之前开发的矢量化层在这里用于将单词映射到vocab，然后进一步用于训练嵌入层等等。</p><h2 id="9b42" class="nd mg iq bd mh ne nf dn ml ng nh dp mp le ni nj mr li nk nl mt lm nm nn mv no bi translated"><strong class="ak">训练</strong></h2><p id="a305" class="pw-post-body-paragraph kv kw iq kx b ky mx jr la lb my ju ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">现在我们训练模型。这里，模型只训练了3个时期，因为这篇文章不是关于模型性能的，我是在Colab上做的(顺便说一下，它总是崩溃！).但是，请随意根据您的选择进一步调整，并在评论中告诉我您的实验。</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="b5a0" class="nd mg iq nq b gy nu nv l nw nx">batch_size = 1024<br/>epochs = 3<br/>steps_per_epoch = train_tweets.shape[0] // batch_size<br/>model = create_model()<br/>model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])</span><span id="6c93" class="nd mg iq nq b gy nz nv l nw nx"># Fitting the model<br/>model.fit(training_dataset, epochs=epochs, batch_size=batch_size, <br/>          steps_per_epoch=steps_per_epoch, validation_data=validation_dataset)</span><span id="f0e5" class="nd mg iq nq b gy nz nv l nw nx">OUTPUT:<br/>Epoch 1/3<br/>1402/1402 [==============================] - 288s 199ms/step - loss: 0.4545 - accuracy: 0.7738 - val_loss: 0.3984 - val_accuracy: 0.8193<br/>Epoch 2/3<br/>1402/1402 [==============================] - 283s 202ms/step - loss: 0.3814 - accuracy: 0.8293 - val_loss: 0.3966 - val_accuracy: 0.8224<br/>Epoch 3/3<br/>1402/1402 [==============================] - 283s 202ms/step - loss: 0.3509 - accuracy: 0.8455 - val_loss: 0.4079 - val_accuracy: 0.8202<br/>&lt;tensorflow.python.keras.callbacks.History at 0x7fde2aae2c10&gt;</span></pre><p id="2544" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">您还可以添加模型检查点，并使用Tensorboard来监控模型性能。接下来，我们需要在磁盘上持久化模型:</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="e9ed" class="nd mg iq nq b gy nu nv l nw nx"># Saving Model<br/>import os<br/>MODEL_DIR = "/content/drive/MyDrive/TextSummarizerModel/SentimentAnalysis/Model"<br/>version = 1<br/>export_path = os.path.join(MODEL_DIR, str(version))<br/>print('export_path = {}\n'.format(export_path))</span><span id="8cf8" class="nd mg iq nq b gy nz nv l nw nx">tf.keras.models.save_model(<br/>    model,<br/>    export_path,<br/>    overwrite=True,<br/>    include_optimizer=True,<br/>    save_format=None,<br/>    signatures=None,<br/>    options=None<br/>)</span><span id="7fff" class="nd mg iq nq b gy nz nv l nw nx"># Check the path<br/>print('\nSaved model:')<br/>!ls -l {export_path}</span><span id="0ec3" class="nd mg iq nq b gy nz nv l nw nx"># Using SavedModelCLI to check if model is persisted properly<br/>!saved_model_cli show --dir {export_path} --all</span></pre><h2 id="506b" class="nd mg iq bd mh ne nf dn ml ng nh dp mp le ni nj mr li nk nl mt lm nm nn mv no bi translated"><strong class="ak">评估</strong></h2><p id="faca" class="pw-post-body-paragraph kv kw iq kx b ky mx jr la lb my ju ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">我们使用测试数据集，我们必须在看不见的数据上测试模型性能:</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="ffbb" class="nd mg iq nq b gy nu nv l nw nx"># Loading and Evaluation of Model<br/>model = tf.keras.models.load_model(export_path)<br/>model.evaluate(test_dataset)</span><span id="2e3a" class="nd mg iq nq b gy nz nv l nw nx">OUTPUT:<br/>44/44 [==============================] - 7s 5ms/step - loss: 0.4386 - accuracy: 0.8153<br/>[0.43862035870552063, 0.8153409361839294]</span></pre><p id="353b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">输出列表中的第一个值是测试数据的最终损失，第二个值是准确度。假设输出数据集是平衡，0.81是很高的精度，几乎不需要模型调整。由于就情绪分析而言，手头的数据完全是奶油，因此可能仍有巨大的改进潜力，但这是改天再喝的一杯茶。如果你想了解用于评估各种分类模型的各种度量标准(如本文)，这里的<a class="ae ny" rel="noopener" target="_blank" href="/building-and-evaluating-classification-ml-models-9c3f45038ef4"><strong class="kx ir"/></a>是我的另一篇文章可以提供帮助。</p><h1 id="bbce" class="mf mg iq bd mh mi mj mk ml mm mn mo mp jw mq jx mr jz ms ka mt kc mu kd mv mw bi translated">使用TFX服务的模型部署(REST API)</h1><h2 id="4bcd" class="nd mg iq bd mh ne nf dn ml ng nh dp mp le ni nj mr li nk nl mt lm nm nn mv no bi translated"><strong class="ak"> TFX发球设置</strong></h2><p id="e9be" class="pw-post-body-paragraph kv kw iq kx b ky mx jr la lb my ju ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">以下命令将帮助您在您的计算机上设置TFX服务。确定是Linux，因为会让你的生活更轻松。</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="7f3c" class="nd mg iq nq b gy nu nv l nw nx"># Updating apt repo for tensorflow-model-server<br/>!echo "deb <a class="ae ny" href="http://storage.googleapis.com/tensorflow-serving-apt" rel="noopener ugc nofollow" target="_blank">http://storage.googleapis.com/tensorflow-serving-apt</a> stable tensorflow-model-server tensorflow-model-server-universal" | tee /etc/apt/sources.list.d/tensorflow-serving.list &amp;&amp; \<br/>curl <a class="ae ny" href="https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg" rel="noopener ugc nofollow" target="_blank">https://storage.googleapis.com/tensorflow-serving-apt/tensorflow-serving.release.pub.gpg</a> | apt-key add -<br/>!apt-get update</span><span id="d4e7" class="nd mg iq nq b gy nz nv l nw nx"># Installing Tensorflow Model Server<br/>!apt-get update &amp;&amp; apt-get install tensorflow-model-server</span></pre><p id="52b5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">据我所知，在运行上面的命令时，您应该不会遇到任何错误，但是如果您遇到这种情况(可能性极小),请随意“StackOverflow”它，因为每个人都是这样做的。</p><h2 id="ecef" class="nd mg iq bd mh ne nf dn ml ng nh dp mp le ni nj mr li nk nl mt lm nm nn mv no bi translated"><strong class="ak"> TFX服务休息API </strong></h2><p id="10b1" class="pw-post-body-paragraph kv kw iq kx b ky mx jr la lb my ju ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">既然已经建立了TFX服务，那么是时候将我们的模型作为REST API来提供服务了。为此，我们需要使用以下命令:</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="8214" class="nd mg iq nq b gy nu nv l nw nx"># Setting Env Variable Model Path<br/>os.environ["MODEL_DIR"] = MODEL_DIR</span><span id="7e91" class="nd mg iq nq b gy nz nv l nw nx"># Model to Server RUN<br/>%%bash --bg<br/>nohup tensorflow_model_server \<br/>  --rest_api_port=8501 \<br/>  --model_name=sample_model \<br/>  --model_base_path="${MODEL_DIR}" &gt;server.log 2&gt;&amp;1</span><span id="3ad8" class="nd mg iq nq b gy nz nv l nw nx">OUTPUT:<br/>Starting job # 0 in a separate thread.</span><span id="c5ea" class="nd mg iq nq b gy nz nv l nw nx">!tail server.log # Check if server is up &amp; running</span><span id="75fc" class="nd mg iq nq b gy nz nv l nw nx">OUTPUT:<br/>2021-07-25 07:24:07.498478: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:190] Running initialization op on SavedModel bundle at path: /content/drive/MyDrive/TextSummarizerModel/SentimentAnalysis/Model/1<br/>2021-07-25 07:24:07.566753: I external/org_tensorflow/tensorflow/cc/saved_model/loader.cc:277] SavedModel load for tags { serve }; Status: success: OK. Took 479864 microseconds.<br/>2021-07-25 07:24:07.579310: I tensorflow_serving/servables/tensorflow/saved_model_warmup_util.cc:59] No warmup data file found at /content/drive/MyDrive/TextSummarizerModel/SentimentAnalysis/Model/1/assets.extra/tf_serving_warmup_requests<br/>2021-07-25 07:24:07.583893: I tensorflow_serving/core/loader_harness.cc:87] Successfully loaded servable version {name: sample_model version: 1}<br/>2021-07-25 07:24:07.585131: I tensorflow_serving/model_servers/server_core.cc:486] Finished adding/updating models<br/>2021-07-25 07:24:07.585235: I tensorflow_serving/model_servers/server.cc:367] Profiler service is enabled<br/>2021-07-25 07:24:07.585748: I tensorflow_serving/model_servers/server.cc:393] Running gRPC ModelServer at 0.0.0.0:8500 ...<br/>[warn] getaddrinfo: address family for nodename not supported<br/>2021-07-25 07:24:07.586243: I tensorflow_serving/model_servers/server.cc:414] Exporting HTTP/REST API at:localhost:8501 ...<br/>[evhttp_server.cc : 245] NET_LOG: Entering the event loop ...</span></pre><p id="4c0c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果一切顺利，你应该得到上面的输出，如果没有，在评论中写给我，或者更好；跳上StackOverflow。回到上面我们所做的，在运行TensorFlow模型服务器时，我们使用了一些参数，如模型名称、路径等。“model_base_path”是您将在其中部署模型的增量版本的目录的路径。现在我们在那个目录中只有我们模型的一个版本，但是在将来，可能会有更多的版本。其次，我们有“model_name ”,它是您想要在API中显示的模型的名称。其余的不言自明。默认情况下，它将在本地主机上运行(这是显而易见的！).您可以通过传递参数“rest_api_host”的值来类似地指定主机。</p><h2 id="b0d1" class="nd mg iq bd mh ne nf dn ml ng nh dp mp le ni nj mr li nk nl mt lm nm nn mv no bi translated"><strong class="ak">服务API </strong></h2><p id="9f9c" class="pw-post-body-paragraph kv kw iq kx b ky mx jr la lb my ju ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">现在，这是我们期待已久的时刻。我们提供了API，现在我们用它来做一些预测。</p><pre class="kg kh ki kj gt np nq nr ns aw nt bi"><span id="22bd" class="nd mg iq nq b gy nu nv l nw nx"># Testing the API<br/>import json<br/>instances = [<br/>            ['The dinner was ok.'],<br/>             ['I really hated the dinner.'],<br/>             ['That was a really great movie, we should definitely consider watching it again.'],<br/>             ["We've been using this for a long time and it's a really good one but the handle is not that great. Only use if you don't want to move it around much."],<br/>]<br/>data = json.dumps({"signature_name": "serving_default", "instances": instances})</span><span id="875f" class="nd mg iq nq b gy nz nv l nw nx"># Testing the API<br/>import requests<br/>headers = {"content-type": "application/json"}<br/>json_response = requests.post('<a class="ae ny" href="http://localhost:8501/v1/models/sample_model:predict'" rel="noopener ugc nofollow" target="_blank">http://localhost:8501/v1/models/sample_model:predict'</a>, data=data, headers=headers)<br/>predictions = json.loads(json_response.text)<br/>print(predictions)</span><span id="ea60" class="nd mg iq nq b gy nz nv l nw nx">OUTPUT:<br/>{'predictions': [[0.762461543], [0.0516885221], [0.976486802], [0.567632318]]}</span></pre><p id="a324" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">预测与实例的顺序相同。我们可以看到，根据实例的上下文，情绪预测几乎是正确的。当我们得到中性情绪时，它更接近于0.5，否则高度极性情绪对于消极和积极情绪分别接近于0或1。</p><h1 id="1dea" class="mf mg iq bd mh mi mj mk ml mm mn mo mp jw mq jx mr jz ms ka mt kc mu kd mv mw bi translated">结论</h1><p id="a07f" class="pw-post-body-paragraph kv kw iq kx b ky mx jr la lb my ju ld le mz lg lh li na lk ll lm nb lo lp lq ij bi translated">我们经历了ML/DL实验的整个生命周期。尽管这种循环会持续下去，因为我们并不是针对一个问题只训练一次模型。当数据分布变化、词汇变化等时，我们会不断地重新审视它。在生产中监控模型是我们在这里没有讨论的，它是整个MLOps生命周期的关键部分。请关注我未来的文章，因为它们肯定会在这里出现。整个实验都是在谷歌实验室完成的。你可以在这里访问这篇文章的jupyter笔记本<a class="ae ny" href="https://github.com/nishitjain/BloggingNotebooks/blob/main/E2E_Sentiment_Analysis_API.ipynb" rel="noopener ugc nofollow" target="_blank">。</a></p></div></div>    
</body>
</html>