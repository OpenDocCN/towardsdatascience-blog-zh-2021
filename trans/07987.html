<html>
<head>
<title>A Natural Language Processing (NLP) Primer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理初级读本</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-natural-language-processing-nlp-primer-6a82667e9aa5?source=collection_archive---------10-----------------------#2021-07-22">https://towardsdatascience.com/a-natural-language-processing-nlp-primer-6a82667e9aa5?source=collection_archive---------10-----------------------#2021-07-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e17f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用Python的常见NLP任务概述</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/01978fd3c6e0522c424cbd4dec76d202.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*63PIcqFntsKgyLF1ad5Czw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://www.pexels.com/@skylar-kang?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> Skylar Kang </a>从<a class="ae kv" href="https://www.pexels.com/photo/german-text-on-pieces-of-paper-6045366/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> Pexels </a>拍摄</p></figure><p id="fdc7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">自然语言处理<strong class="ky ir"> NLP </strong>用于分析文本数据。这可能是来自网站、扫描文档、书籍、期刊、推特、YouTube评论等来源的数据。</p><p id="e356" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这本初级读本介绍了一些可以使用Python执行的常见NLP任务。示例大多使用自然语言工具包(NLTK)和scikit学习包。假设您具备Python和数据科学原理的基本工作知识。</p><p id="28a6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">自然语言指的是像英语、法语、阿拉伯语和汉语这样的语言，而不是像Python、R和C++这样的计算机语言。NLP使文本数据的部分分析自动化，这在以前只有定性方法才有可能。这些定性方法，如框架/主题分析，不能扩展到大量的文本数据。这就是NLP的用武之地。它还被用于创建聊天机器人、数字助理(如Siri和Alexa)等其他用途。</p><p id="7a0d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本笔记本中使用的数据来自<a class="ae kv" href="https://www.english-corpora.org/corona/" rel="noopener ugc nofollow" target="_blank">https://www.english-corpora.org/corona/</a>【1】的数据子样本。这些数据是关于冠状病毒疫情的，代表了来自各种媒体来源(例如报纸、网站)的子集，并且是2020年1月至5月期间的数据。该数据包含大约320万个英语单词。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="037c" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">使用Python加载文本数据</h2><p id="61fa" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">让我们从查看文本数据开始，看看它是什么样子的。我们可以使用操作系统(<strong class="ky ir"> os </strong>)库来列出我们的文本文件夹中的所有文件。在这种情况下，文件位于名为<strong class="ky ir"> text </strong>的文件夹中，该文件夹位于名为<strong class="ky ir"> NLP </strong>的文件夹中，该文件夹与Python源代码文件相关(例如Jupyter notebook或。py文件)并用“.”表示。/".</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="35f2" class="lz ma iq my b gy nc nd l ne nf">import os<br/>os.listdir("./NLP/text")</span></pre><p id="c4bb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这将生成一个包含5个文本文件的列表:</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="4938" class="lz ma iq my b gy nc nd l ne nf">&gt;&gt;&gt; ['20–01.txt', '20–02.txt', '20–03.txt', '20–04.txt', '20–05.txt']</span></pre><p id="dfa7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如您所见，有5个文本文件(*。txt)。我们可以使用标准的Python文件处理函数来打开文件。在这种情况下，我们将把它限制在前10行，以了解文件的结构和它包含的信息。在下面的输出中，为了简洁起见，这被缩短了。<strong class="ky ir"> next </strong>函数用于遍历文件中的行。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="bd5f" class="lz ma iq my b gy nc nd l ne nf">with open("./NLP/text/20–01.txt") as txt_file:<br/>    head = [next(txt_file) for i in range(10)]<br/>print(head)</span><span id="7bcf" class="lz ma iq my b gy ng nd l ne nf">&gt;&gt;&gt; ['\n', '@@31553641 &lt;p&gt; The government last July called the energy sector debt situation a " state of emergency . " &lt;p&gt; This was during the mid-year budget review during which the Finance Minister Ken Ofori-Atta castigated the previous NDC government for entering into " obnoxious take-or-pay contracts signed by the NDC , which obligate us to pay for capacity we do not need . " &lt;p&gt; The government pays over GH ? 2.5 billion annually for some 2,300MW in installed capacity which the country does not consume . &lt;p&gt; He sounded alarmed that " from 2020 if nothing is done , we will be facing annual excess gas capacity charges of between $550 and $850 million every year . &lt;p&gt; JoyNews \' Business editor George Wiafe said the latest IMF Staff report expressing fears over a possible classification is " more of a warning " to government . &lt;p&gt; He said the latest assessment raises concerns about the purpose of government borrowings , whether it goes into consumption or into projects capable of generating revenue to pay back the loan . &lt;p&gt; The move could increase the country \'s risk profile and ability to borrow on the international market . &lt;p&gt; @ @ @ @ @ @ @ @ @ @ issue another Eurobond in 2020 . &lt;p&gt; The Finance Minister Ken Ofori-Atta wants to return to the Eurobond market to raise $3bn to pay for expenditure items the country can not fund from domestic sources . &lt;p&gt; The government wants to spend GH ? 86m in 2020 but is projecting to raise only GH ? 67bn . It leaves a deficit of GH ? 19bn , monies that the Eurobond could make available . &lt;p&gt; The planned return to the Eurobond market is the seventh time in the past eight years . &lt;p&gt; Ghana is already among 10 low-income countries ( LICs ) in Africa that were at high risk of debt distress . &lt;p&gt; The country in April 2019 , successfully completion an Extended Credit Facility ( ECF ) programme , or bailout , of the International Monetary fund ( IMF ) . \n']</span></pre></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="7fa9" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">文本表示</h2><p id="0bf4" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">为了以数字方式存储和使用字符，通常用编码系统来表示它们。有<strong class="ky ir"> ASCII </strong>(美国信息交换标准码)等不同的字符编码标准。例如，字母“a”由ASCII码097表示，在二进制中也是01100001。还有其他编码集，如<strong class="ky ir"> UTF-8 </strong> (Unicode(或通用编码字符集)转换格式，8位)支持可变宽度的字符。这个系统中的字母“a”是U+0061。在Python中，我们可以像这样直接使用UTF-8:</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="b535" class="lz ma iq my b gy nc nd l ne nf">u"\u0061"</span><span id="0c91" class="lz ma iq my b gy ng nd l ne nf">&gt;&gt;&gt; 'a'</span></pre><p id="c014" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可能需要转换导入的文本数据的字符编码，以执行进一步的处理并更好地表示某些符号(例如表情符号🌝).您可以使用<strong class="ky ir"> sys </strong>模块中的<strong class="ky ir"> getdefaultencoding </strong>函数检查您正在使用的编码类型。要更改编码，您可以使用<strong class="ky ir"> setdefaultencoding </strong>函数，例如<strong class="ky ir">sys . setdefaultencoding(" utf-8 ")</strong>。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="12ef" class="lz ma iq my b gy nc nd l ne nf">import sys<br/>sys.getdefaultencoding()</span><span id="d5e2" class="lz ma iq my b gy ng nd l ne nf">&gt;&gt;&gt; 'utf-8'</span></pre></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="e5aa" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">数据预处理</h2><p id="6980" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">应用NLP时，数据处理有几个阶段。这些根据确切的上下文而有所不同，但通常遵循类似于下图所示的路径。这通常包括访问文本数据，无论是网页、推文、评论、PDF文档还是原始文本格式。然后将其分解为算法可以轻松处理的表示形式(例如，表示单个单词或字母的标记)，移除常用单词(停用词)(例如，“and”、“or”、“the”)。进一步的标准化之后是诸如特征提取和/或去除噪声的任务。最后，各种模型和方法(例如，主题建模、情感分析、神经网络等。)都适用。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/a8765a2444b14ea5c8c9f528e99072df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r0pzLGVfIrB155vcO61ayA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">文本数据的通用预处理方法(图片由作者提供)</p></figure><p id="e7b7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因为我们已经有了一些要处理的文本，所以我们可以看看预处理数据的后续步骤。我们可以从标记化开始。很多Python库都可以完成标记化，包括机器学习库scikit learn。自然语言处理任务的一个流行库是自然语言工具包(NLTK):</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="0e46" class="lz ma iq my b gy nc nd l ne nf">import nltk</span></pre><p id="f421" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">注意:</strong>Python中另一个强大的NLP替代库是<strong class="ky ir"> spaCy </strong>。</p><h2 id="83be" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">符号化</h2><p id="8d82" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">即使在库中，也经常有不同的标记器可供选择。例如，NLTK也有一个RegexpTokenizer(使用正则表达式)。这里我们将使用TreebankWordTokenizer来过滤掉一些标点符号和空格。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="cb2c" class="lz ma iq my b gy nc nd l ne nf">from nltk.tokenize import TreebankWordTokenizer</span></pre><p id="a083" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将从文本数据中提取一小段文本来说明这是如何工作的。在这里，我们将它作为一个字符串(Python中用来存储文本数据的一种数据类型)存储在一个名为<strong class="ky ir"> txt </strong>的变量中。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="ecdd" class="lz ma iq my b gy nc nd l ne nf">txt = "The government last July called the energy sector debt situation a state of emergency. &lt;p&gt; This was during the mid-year budget review during which the Finance Minister Ken Ofori-Atta castigated the previous NDC government for entering into obnoxious take-or-pay contracts signed by the NDC , which obligate us to pay for capacity we do not need . &lt;p&gt; The government pays over GH ? 2.5 billion annually for some 2,300MW in installed capacity which the country does not consume ."</span></pre><p id="88e1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">简单的第一步可能是将所有文本转换成小写字母。我们可以通过<strong class="ky ir">下</strong>功能来实现。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="b749" class="lz ma iq my b gy nc nd l ne nf">txt = txt.lower()<br/>txt</span><span id="cd05" class="lz ma iq my b gy ng nd l ne nf">&gt;&gt;&gt; 'the government last july called the energy sector debt situation a state of emergency. &lt;p&gt; this was during the mid-year budget review during which the finance minister ken ofori-atta castigated the previous ndc government for entering into obnoxious take-or-pay contracts signed by the ndc , which obligate us to pay for capacity we do not need .  &lt;p&gt; the government pays over gh ? 2.5 billion annually for some 2,300mw in installed capacity which the country does not consume .'</span></pre><p id="950a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来我们可以创建一个<strong class="ky ir">treebankwodtokenizer</strong>类的实例，并使用<strong class="ky ir"> tokenize </strong>函数，传入我们的<strong class="ky ir"> txt </strong>变量。输出如下所示(显示前20个)。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="7f44" class="lz ma iq my b gy nc nd l ne nf">tk = TreebankWordTokenizer()<br/>tk_words = tk.tokenize(txt)<br/>tk_words[:20]</span><span id="8014" class="lz ma iq my b gy ng nd l ne nf">&gt;&gt;&gt; ['the',<br/>     'government',<br/>     'last',<br/>     'july',<br/>     'called',<br/>     'the',<br/>     'energy',<br/>     'sector',<br/>     'debt',<br/>     'situation',<br/>     'a',<br/>     'state',<br/>     'of',<br/>     'emergency.',<br/>     '&lt;',<br/>     'p',<br/>     '&gt;',<br/>     'this',<br/>     'was',<br/>     'during']</span></pre><p id="8d5b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个<strong class="ky ir"> casual_tokenize </strong>对于社交媒体符号化很有用，因为它可以很好地处理用户名和表情符号之类的事情。<strong class="ky ir"> TweetTokenizer </strong>也为Twitter分析保持了完整的散列标签。</p><h2 id="fafd" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">处理停用词和标点符号</h2><p id="0723" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">下一件常见的事情是删除停用词。这些是在句子结构中使用的常见高频词，但对分析来说没有太多意义。这些词包括“the”、“and”、“to”、“a”等。我们可以从NLTK库中下载这些单词的列表，并将它们存储在一个变量(<strong class="ky ir"> sw </strong>)中，如下所示:</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="b76f" class="lz ma iq my b gy nc nd l ne nf">nltk.download("stopwords", quiet=True)<br/>sw = nltk.corpus.stopwords.words("english")</span></pre><p id="b970" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以看看这些单词的前20个。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="480d" class="lz ma iq my b gy nc nd l ne nf">sw[:20]</span><span id="9634" class="lz ma iq my b gy ng nd l ne nf">&gt;&gt;&gt; ['i',<br/>     'me',<br/>     'my',<br/>     'myself',<br/>     'we',<br/>     'our',<br/>     'ours',<br/>     'ourselves',<br/>     'you',<br/>     "you're",<br/>     "you've",<br/>     "you'll",<br/>     "you'd",<br/>     'your',<br/>     'yours',<br/>     'yourself',<br/>     'yourselves',<br/>     'he',<br/>     'him',<br/>     'his']</span></pre><p id="ef97" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在写这篇文章的时候，列表中有超过100个停用词。如果你感兴趣，可以使用<strong class="ky ir"> len </strong>函数查看列表中有多少单词(例如<strong class="ky ir"> len(sw) </strong>)。</p><p id="ad9f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们把这些停用词从标记词中去掉。我们可以使用Python list comprehension来过滤<strong class="ky ir"> tk_words </strong>列表中不在停用字词(<strong class="ky ir"> sw </strong>)列表中的字词。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="01ac" class="lz ma iq my b gy nc nd l ne nf">tk_words_filtered_sw = [word for word in tk_words if word not in sw]</span></pre><p id="9d5b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果您不熟悉列表理解，它们本质上是一种创建列表和遍历列表数据结构的简洁方法。这通常避免了需要多几行单独的“for循环”代码。假设我想求0到5的平方。我们可以像这样使用for循环:</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="9e57" class="lz ma iq my b gy nc nd l ne nf">squared_nums = []<br/>for n in range(5):<br/>    squared_nums.append(n**2)</span></pre><p id="7330" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然这有预期的效果，但我们可以创建列表，并使用列表理解来遍历它，而不是将它组合成一行代码:</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="14ff" class="lz ma iq my b gy nc nd l ne nf">squared_nums = [n**2 for n in range(5)]</span></pre><p id="9b48" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们输出变量<strong class="ky ir"> tk_words_filtered_sw </strong>的内容，我们可以看到很多停用词现在都被删除了(为简洁起见，缩写为):</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="52b3" class="lz ma iq my b gy nc nd l ne nf">tk_words_filtered_sw</span><span id="a59d" class="lz ma iq my b gy ng nd l ne nf">&gt;&gt;&gt; ['government',<br/>     'last',<br/>     'july',<br/>     'called',<br/>     'energy',<br/>     'sector',<br/>     'debt',<br/>     'situation',<br/>     'state',<br/>     'emergency.',<br/>     '&lt;',<br/>     'p',<br/>     '&gt;',<br/>     'mid-year',<br/>     'budget',<br/>     'review',<br/>     'finance',<br/>     'minister',<br/>     'ken',<br/>     'ofori-atta',<br/>     'castigated',<br/>     'previous',<br/>     'ndc',<br/>     'government',<br/>     'entering',<br/>     'obnoxious',<br/>     'take-or-pay',<br/>     'contracts',<br/>     'signed',<br/>     'ndc',<br/>     ','<br/>...</span></pre><p id="de7b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">可以看到文本中仍然有句号(句号)、问号、逗号之类的标点符号。同样，我们通常会从列表中删除这些内容。这可以通过多种不同的方式来实现。这里，我们使用字符串库来删除存储结果的标点符号，这个结果存储在一个名为<strong class="ky ir"> no_punc </strong>的变量中。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="ac65" class="lz ma iq my b gy nc nd l ne nf">import string<br/>no_punc = ["".join( j for j in i if j not in string.punctuation) for i in  tk_words_filtered_sw]<br/>no_punc</span><span id="5383" class="lz ma iq my b gy ng nd l ne nf">&gt;&gt;&gt; ['government',<br/>     'last',<br/>     'july',<br/>     'called',<br/>     'energy',<br/>     'sector',<br/>     'debt',<br/>     'situation',<br/>     'state',<br/>     'emergency',<br/>     '',<br/>     'p',<br/>     '',<br/>     'midyear',<br/>     'budget'<br/>...</span></pre><p id="9b99" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后我们可以从列表中过滤掉这些空字符串(“”)。我们可以将结果存储在一个名为<strong class="ky ir"> filtered_punc </strong>的变量中。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="8866" class="lz ma iq my b gy nc nd l ne nf">filtered_punc = list(filter(None, no_punc))<br/>filtered_punc</span><span id="bba2" class="lz ma iq my b gy ng nd l ne nf">&gt;&gt;&gt; ['government',<br/>     'last',<br/>     'july',<br/>     'called',<br/>     'energy',<br/>     'sector',<br/>     'debt',<br/>     'situation',<br/>     'state',<br/>     'emergency',<br/>     'p',<br/>     'midyear',<br/>     'budget'<br/>...</span></pre><p id="a6d7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我们可能还想从列表中删除数字。为此，我们可以使用<strong class="ky ir"> isdigit </strong>函数检查字符串是否包含任何数字。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="cd4f" class="lz ma iq my b gy nc nd l ne nf">str_list = [i for i in filtered_punc if not any(j.isdigit() for j in i)]</span></pre><p id="5c5d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如您所见，文本数据可能非常混乱，需要在开始运行各种分析之前进行大量清理。另一种去除不必要单词的常用方法是使用词干法或词干法。</p><h2 id="97c6" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">词干化和词汇化</h2><p id="b2d7" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">词干化指的是将单词缩减到它们的词根(词干)形式。例如，单词“waited”、“waities”、“waiting”可以简化为“wait”。我们可以看一个叫做<strong class="ky ir">波特词干分析器</strong>的普通词干分析器的例子。首先，我们可以创建一个简短的单词列表来演示这是如何工作的。这一阶段通常跟随单词的标记化。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="a54a" class="lz ma iq my b gy nc nd l ne nf">word_list = ["flying", "flies", "waiting", "waits", "waited", "ball", "balls", "flyer"]</span></pre><p id="d3ab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们将从NLTK导入Porter词干分析器。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="b163" class="lz ma iq my b gy nc nd l ne nf">from nltk.stem.porter import PorterStemmer</span></pre><p id="7eb3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">制作一个名为<strong class="ky ir"> ps </strong>的<strong class="ky ir"> PorterStemmer </strong>类的实例。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="cdc6" class="lz ma iq my b gy nc nd l ne nf">ps = PorterStemmer()</span></pre><p id="dc40" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后用一个列表理解列表中的每个单词。其结果可以在下面看到。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="c75d" class="lz ma iq my b gy nc nd l ne nf">stem_words = [ps.stem(word) for word in word_list]<br/>stem_words</span><span id="189c" class="lz ma iq my b gy ng nd l ne nf">&gt;&gt;&gt; ['fli', 'fli', 'wait', 'wait', 'wait', 'ball', 'ball', 'flyer']</span></pre><p id="9405" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本应具有不同词干的单词被词干化为同一个词根的情况下，可能会发生词干过度。也有可能得到下面的词干。这在本质上是相反的(应该词根相同的单词却不是)。有各种不同的词干分析器可以使用，如波特，英语斯特梅尔，派斯和洛文斯仅举几例。波特梗是最广泛使用的一种。</p><p id="e498" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有些词干分析器比其他词干分析器更“苛刻”或更“温和”，因此您可能需要尝试不同的词干分析器来获得想要的结果。您也可以根据输出决定在停用词移除之前或之后应用词干。</p><p id="dcef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另一方面，词条释义是通过识别一个单词相对于它出现的句子的意义来工作的。本质上，上下文对于引理满足很重要。这种情况下的词根叫做引理。例如，单词“better”可以用单词“good”来表示，因为这个单词就是它的来源。这个过程在计算上比词干提取更昂贵。</p><p id="da45" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们需要首先从NLTK(一个包含英语名词、形容词、副词和动词的大型单词数据库)下载wordnet资源。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="6365" class="lz ma iq my b gy nc nd l ne nf">nltk.download('wordnet', quiet=True)</span></pre><p id="0542" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将导入并创建一个<strong class="ky ir"> WordNetLemmatizer </strong>的实例，并将其应用于我们之前使用的单词列表。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="8937" class="lz ma iq my b gy nc nd l ne nf">word_list</span><span id="d0af" class="lz ma iq my b gy ng nd l ne nf">&gt;&gt;&gt; ['flying', 'flies', 'waiting', 'waits', 'waited', 'ball', 'balls', 'flyer']</span><span id="d9f4" class="lz ma iq my b gy ng nd l ne nf">from nltk.stem import WordNetLemmatizer<br/>lm = WordNetLemmatizer()</span><span id="aa3a" class="lz ma iq my b gy ng nd l ne nf">lem_words = [lm.lemmatize(word) for word in word_list]<br/>lem_words</span><span id="6c0a" class="lz ma iq my b gy ng nd l ne nf">&gt;&gt;&gt; ['flying', 'fly', 'waiting', 'wait', 'waited', 'ball', 'ball', 'flyer']</span></pre><p id="45de" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们将<strong class="ky ir"> lem_words </strong>与<strong class="ky ir"> stem_words </strong>(如下)进行比较，您可以看到，尽管在某些情况下相似/相同，但对于一些单词，如“flying”和“flies ”,使用引理化保留了其含义，否则词干化将会丢失该含义。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="3510" class="lz ma iq my b gy nc nd l ne nf">stem_words</span><span id="9c31" class="lz ma iq my b gy ng nd l ne nf">&gt;&gt;&gt; ['fli', 'fli', 'wait', 'wait', 'wait', 'ball', 'ball', 'flyer']</span></pre><h2 id="216b" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">标准化/缩放</h2><p id="6885" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">移除特征(单词/术语)的另一种方法是使用代表术语频率的<strong class="ky ir"> tf-idf </strong>重新调整数据，逆文档频率。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/4c423847e3e0e4164bd96d7286503a9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*JR4L0JnqiNxuSP_5fbxywg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">tf-idf方程(图片由作者提供)</p></figure><p id="f285" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这用于查看一个术语(单词)对文档集合(语料库)中的单个文档有多重要。我们本质上是用这个作为权重。术语频率是该单词/术语在文档中出现的频率。例如，如果我们有两个文件𝑑1和𝑑2看起来像这样。</p><p id="ad36" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">𝑑1 =“那个小男孩在房子里。”𝑑2 =“那个小男孩不在房子里。”</p><p id="1e87" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果在文档1 ( 𝑑1)中感兴趣的术语是“男孩”，这在七个单词中出现一次1/7=0.1428。我们可以对每个文档中的每个术语做同样的事情。一旦我们计算出术语频率，我们就可以用语料库中文档总数的对数乘以包含该术语的文档数。这告诉我们一个特定的术语与一个文档是更相关、更不相关还是同等相关。在上面的示例中，文档二中的单词“not”对于区分这两个文档非常重要。高权重是由于文档中的高术语频率和语料库(文档集合)中的低术语频率。TF-IDF也可以用于文本摘要任务。</p><p id="f2c3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了在实践中使用TF-IDF，我们将需要使用我们前面看到的文本文件。为了做得更好，我保存了一个没有停用词和HTML段落标签的版本。这也被转换成小写。本质上，这是使用一个循环一个接一个地打开文件，一行一行地读取它们，分割成单词。如果这个词不是停用词，它将被写入一个新文件。最后，<strong class="ky ir"> BeautifulSoup </strong>库用于从文本数据中去除所有的HTML标签。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="6613" class="lz ma iq my b gy nc nd l ne nf"><strong class="my ir">from</strong> bs4 <strong class="my ir">import</strong> BeautifulSoup<br/><br/>file_path <strong class="my ir">=</strong> "./NLP/text/"<br/>file_list <strong class="my ir">=</strong> ['20-01', '20-02', '20-03', '20-04']<br/><br/><strong class="my ir">for</strong> file <strong class="my ir">in</strong> file_list:<br/>    current_file <strong class="my ir">=</strong> open(file_path <strong class="my ir">+</strong> file <strong class="my ir">+</strong> ".txt")<br/>    line <strong class="my ir">=</strong> current_file.read().lower()<br/>    soup <strong class="my ir">=</strong> BeautifulSoup(line)<br/>    words <strong class="my ir">=</strong> soup.get_text().split()<br/>    <strong class="my ir">for</strong> word <strong class="my ir">in</strong> words:<br/>        <strong class="my ir">if</strong> word <strong class="my ir">not</strong> <strong class="my ir">in</strong> sw:<br/>            formated_list <strong class="my ir">=</strong> open((file_path <strong class="my ir">+</strong> file <strong class="my ir">+</strong> '-f.txt'),'a')<br/>            formated_list.write(" "<strong class="my ir">+</strong>word)<br/>            formated_list.close()<br/><br/>    current_file.close()</span></pre><p id="e725" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们现在可以加载这些文件并将它们的内容存储在变量中。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="0ab1" class="lz ma iq my b gy nc nd l ne nf">path = "./NLP/text/processed/"</span><span id="1c66" class="lz ma iq my b gy ng nd l ne nf">txt_file_1 = open(path + "20–01-f.txt") <br/>file_1 = txt_file_1.read()</span><span id="4e6b" class="lz ma iq my b gy ng nd l ne nf">txt_file_2 = open(path + "20–02-f.txt") <br/>file_2 = txt_file_2.read()</span><span id="ec13" class="lz ma iq my b gy ng nd l ne nf">txt_file_3 = open(path + "20–03-f.txt")<br/>file_3 = txt_file_3.read()</span><span id="2147" class="lz ma iq my b gy ng nd l ne nf">txt_file_4 = open(path + "20–04-f.txt") <br/>file_4 = txt_file_4.read()</span></pre><p id="182e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以将从文件中提取的文本数据放在一个列表中，以便于使用。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="8377" class="lz ma iq my b gy nc nd l ne nf">data_files = [file_1, file_2, file_3, file_4]</span></pre><p id="143c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们将导入<strong class="ky ir"> pandas </strong>库，它在数据科学中经常使用，并提供以表格结构(数据框)表示数据的功能。接下来，我们将从<strong class="ky ir"> sklearn </strong>机器学习库中导入<strong class="ky ir">tfidf矢量器</strong>，它将标记文档并应用IDF权重。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="7875" class="lz ma iq my b gy nc nd l ne nf">import pandas as pd<br/>from sklearn.feature_extraction.text import TfidfVectorizer</span></pre><p id="b42c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我们可以创建该类的一个实例，并使其适合数据。我们可以在数据框中显示结果，其中每个要素和相关的TF-IDF权重按降序排列。最后，为了简洁起见，我们将只输出前30个术语。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="5369" class="lz ma iq my b gy nc nd l ne nf">tv = TfidfVectorizer(use_idf=True)<br/>tfIdf = tv.fit_transform(data_files)<br/>df = pd.DataFrame(tfIdf[0].T.todense(), index=tv.get_feature_names(), columns=["TF-IDF"])<br/>df = df.sort_values('TF-IDF', ascending=False)<br/>df.head(30)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/55c92196c3adb20a26abbafa9346b9b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:306/format:webp/1*o-t5lqorF5Uh0-xHBp7Q4g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">熊猫数据框的输出-为简洁起见而缩短(图片由作者提供)</p></figure><h2 id="95c3" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">词频</h2><p id="d27a" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">我们可以对单词数据做的最简单的事情之一是查看一个独特的单词在文档中出现的频率(和/或它在语料库中的累积频率)。我们可以使用<strong class="ky ir"> FreqDist </strong>函数来计算一个字典，它的键/值对包含每个术语(单词)，后跟它在文本中出现的次数。例如，单词“government”在我们之前使用的原始简短示例文本中出现了3次。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="292c" class="lz ma iq my b gy nc nd l ne nf">dist = nltk.FreqDist(str_list)<br/>dist</span><span id="773f" class="lz ma iq my b gy ng nd l ne nf">&gt;&gt;&gt; FreqDist({'government': 3, 'p': 2, 'ndc': 2, 'capacity': 2, 'last': 1, 'july': 1, 'called': 1, 'energy': 1, 'sector': 1, 'debt': 1, ...})</span></pre><p id="59b8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了使它更容易可视化，我们可以将它输出为一个图。正如您所看到的，单词government出现了3次，字母p(来自HTML段落标记)出现了两次，其他单词只出现了一次。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="4af2" class="lz ma iq my b gy nc nd l ne nf">dist.plot();</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/d64d358fa033e41d04b2c59b5927c19a.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*UMkUZrqk6TvmuOpXUn3WSg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">单词的情节和出现频率(图片由作者提供)</p></figure><p id="88be" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另一种显示单词频率的视觉方式是使用单词云，这里单词越大，出现的次数就越多。为此，我们可以使用<strong class="ky ir"> WordCloud </strong>库。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="87d0" class="lz ma iq my b gy nc nd l ne nf">from wordcloud import WordCloud</span></pre><p id="e42d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们还需要来自用于各种可视化的<strong class="ky ir"> matplotlib </strong>库中的绘图(<strong class="ky ir"> plt </strong>)。<strong class="ky ir"> %matplotlib inline </strong>设置绘图命令，以确保当与Jupyter笔记本等字体端一起使用时，绘图出现在代码单元格下方并存储在笔记本中。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="42bf" class="lz ma iq my b gy nc nd l ne nf">import matplotlib.pyplot as plt<br/>%matplotlib inline</span></pre><p id="8f6b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们需要将数据转换成word cloud函数的正确格式。它接受一个字符串，因此我们将把标记化的字符串列表折叠成一个单独的字符串，单词之间留有空格，使用内置的<strong class="ky ir"> join </strong>函数进行字符串连接(将字符串连接在一起)。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="273e" class="lz ma iq my b gy nc nd l ne nf">flattend_text = " ".join(str_list)</span></pre><p id="342f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们可以使用传入文本字符串的<strong class="ky ir"> generate </strong>函数创建单词云。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="00ab" class="lz ma iq my b gy nc nd l ne nf">wc = WordCloud().generate(flattend_text)</span></pre><p id="a2f5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我们将使用<strong class="ky ir">双线性</strong>插值选项输出关闭轴文本的单词云，以平滑图像的外观。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="2b00" class="lz ma iq my b gy nc nd l ne nf">plt.imshow(wc, interpolation='bilinear')<br/>plt.axis("off")<br/>plt.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/3d5ba905eb0c387851948ee544b23a8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*0CAlykI88-W4iCZRIFbv8g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">词云输出(图片由作者提供)</p></figure><p id="2533" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您还可以使用其他可选参数。一些流行的方法包括设置最大字体大小(max_font_size)和包含的字数(如果你有很多字的话会很有帮助)以及改变背景颜色。例如，我们可以将显示的最大字数设置为10，并将背景设置为白色。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="5723" class="lz ma iq my b gy nc nd l ne nf">wc_2 = WordCloud(max_words = 10, background_color = "white").generate(flattend_text)<br/>plt.imshow(wc_2, interpolation='bilinear')<br/>plt.axis("off")<br/>plt.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/e0d6f9740351644d2af0d7abfaa12cb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*xYpjcMyTtwkHx1vhFE60OA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">词云输出(图片由作者提供)</p></figure><h2 id="c171" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">n元语法分析</h2><p id="3eb7" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">当我们把单词符号化，并把它们表示成一个单词包时，我们就失去了一些上下文和意义。单个单词本身并不能说明太多，但是它们与其他单词一起出现的频率可能会说明很多。例如,“信息”和“治理”这两个词可能经常一起出现，并具有特定的含义。我们可以用n元语法来解释这一点。这是指一起出现的多个令牌。这些标记可以是单词或字母。这里我们将使用单词。两个词(𝑛=2)被称为双字，三个词被称为三字，等等。使用n元语法有助于我们保留文本中的一些含义/上下文。</p><p id="1738" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将使用前面相同的短文本:</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="6a0f" class="lz ma iq my b gy nc nd l ne nf">txt</span><span id="deff" class="lz ma iq my b gy ng nd l ne nf">&gt;&gt;&gt; 'the government last july called the energy sector debt situation a state of emergency. &lt;p&gt; this was during the mid-year budget review during which the finance minister ken ofori-atta castigated the previous ndc government for entering into obnoxious take-or-pay contracts signed by the ndc , which obligate us to pay for capacity we do not need .  &lt;p&gt; the government pays over gh ? 2.5 billion annually for some 2,300mw in installed capacity which the country does not consume .'</span></pre><p id="6765" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们可以从NLTK实用程序包中导入ngrams函数。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="a4b5" class="lz ma iq my b gy nc nd l ne nf">from nltk.util import ngrams</span></pre><p id="ba81" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将使用与之前相同的标记器再次标记文本。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="4087" class="lz ma iq my b gy nc nd l ne nf">tk = TreebankWordTokenizer()<br/>tk_words = tk.tokenize(txt)</span></pre><p id="2975" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我们将这个标记化的列表传递到<strong class="ky ir"> ngrams </strong>函数中，并指定𝑛(在本例中为二进制数的2)(为简洁起见而缩写)。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="f5aa" class="lz ma iq my b gy nc nd l ne nf">bigrams = list(ngrams(tk_words, 2))<br/>bigrams</span><span id="cd42" class="lz ma iq my b gy ng nd l ne nf">&gt;&gt;&gt; [('the', 'government'),<br/>     ('government', 'last'),<br/>     ('last', 'july'),<br/>     ('july', 'called'),<br/>     ('called', 'the'),<br/>     ('the', 'energy'),<br/>     ('energy', 'sector'),<br/>     ('sector', 'debt'),<br/>     ('debt', 'situation'),<br/>     ('situation', 'a'),<br/>     ('a', 'state'),<br/>     ('state', 'of'),<br/>     ('of', 'emergency.'),<br/>     ('emergency.', '&lt;'),<br/>     ('&lt;', 'p'),<br/>     ('p', '&gt;'),<br/>     ('&gt;', 'this'),<br/>     ('this', 'was'),<br/>     ('was', 'during'),<br/>     ('during', 'the'),<br/>     ('the', 'mid-year'),<br/>     ('mid-year', 'budget')<br/>...</span></pre><p id="baf3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这里，我们可以看到像“合同”和“签署”或“政府”和“支付”这样的术语，它们提供了比单个单词更多的上下文。作为预处理的一部分，您也可以从文本数据中过滤出n元语法。</p><p id="8c65" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们还可以使用<strong class="ky ir"> BigramCollocationFinder </strong>类来确定二元模型出现的次数。这里我们按降序对列表进行排序。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="0701" class="lz ma iq my b gy nc nd l ne nf">from nltk.collocations import BigramCollocationFinder<br/>finder = BigramCollocationFinder.from_words(tk_words, window_size=2)<br/>ngram = list(finder.ngram_fd.items())<br/>ngram.sort(key=lambda item: item[-1], reverse=True)<br/>ngram</span><span id="6469" class="lz ma iq my b gy ng nd l ne nf">&gt;&gt;&gt; [(('the', 'government'), 2),<br/>     (('&lt;', 'p'), 2),<br/>     (('p', '&gt;'), 2),<br/>     (('which', 'the'), 2),<br/>     (('government', 'last'), 1),<br/>     (('last', 'july'), 1),<br/>     (('july', 'called'), 1),<br/>     (('called', 'the'), 1),<br/>     (('the', 'energy'), 1),<br/>     (('energy', 'sector'), 1),<br/>     (('sector', 'debt'), 1),<br/>     (('debt', 'situation'), 1),<br/>     (('situation', 'a'), 1),<br/>     (('a', 'state'), 1),<br/>     (('state', 'of'), 1),<br/>     (('of', 'emergency.'), 1),<br/>     (('emergency.', '&lt;'), 1),<br/>     (('&gt;', 'this'), 1),<br/>     (('this', 'was'), 1),<br/>     (('was', 'during'), 1),<br/>     (('during', 'the'), 1),<br/>     (('the', 'mid-year'), 1),<br/>     (('mid-year', 'budget'), 1)<br/>...</span></pre><h2 id="df05" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">情感分析</h2><p id="8a16" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">这包括分析文本以确定文本的“正面”或“负面”程度。这可以给我们关于人们的观点/情绪的信息。这可以应用到一些事情上，比如浏览产品评论，以获得产品是否被看好的总体感觉。我们还可以将它用于研究目的——例如，人们在他们的推文中对戴口罩持肯定还是否定态度。为此，我们可以训练一个模型或使用现有的词典。VADER是用Python实现的。这会产生一个介于-1和+1之间的正面、负面和中性情绪得分。它还产生一个复合分数，即<strong class="ky ir">阳性+中性标准化</strong> (-1比1)。首先，我们将下载词典。词典包含与单个单词或文本串相关的信息(例如，语义或语法)。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="bf9f" class="lz ma iq my b gy nc nd l ne nf">nltk.download("vader_lexicon", quiet=True)</span></pre><p id="0af3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们导入了情感分析器类<strong class="ky ir">SentimentIntensityAnalyzer</strong>，并创建了一个名为<strong class="ky ir"> snt </strong>的实例(用于情感)。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="31b5" class="lz ma iq my b gy nc nd l ne nf">from nltk.sentiment.vader import SentimentIntensityAnalyzer<br/>snt = SentimentIntensityAnalyzer()</span></pre><p id="85a3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，我们可以向函数<strong class="ky ir"> polarity_scores </strong>传递一些文本数据(例如，我们的第二个文件)。您可以在下面的字典数据结构中看到作为键/值对返回的分数。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="7c37" class="lz ma iq my b gy nc nd l ne nf">snt.polarity_scores(data_files[1])</span><span id="2d60" class="lz ma iq my b gy ng nd l ne nf">&gt;&gt;&gt; {'neg': 0.101, 'neu': 0.782, 'pos': 0.117, 'compound': 1.0}</span></pre><p id="22c5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这个例子中，查看第二个文本文件，我们可以看到一个占主导地位的中性情绪得分(0.782)，后面是一个正得分(0.117)和一个负得分(0.101)。你可以在不同的时间点比较情绪，看看它是如何变化的，或者在不同的小组之间。有时使用的另一个指标是净情绪得分(NSS)，它是通过从积极得分中减去消极得分来计算的。首先，我们需要将分数存储在一个变量中来访问它们。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="1d41" class="lz ma iq my b gy nc nd l ne nf">sent_scores = snt.polarity_scores(data_files[1])</span></pre><p id="1293" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后我们可以从正数中减去负数。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="b03b" class="lz ma iq my b gy nc nd l ne nf">nss = sent_scores['pos'] — sent_scores['neg']<br/>print("NSS =", nss)</span><span id="f3cc" class="lz ma iq my b gy ng nd l ne nf">&gt;&gt;&gt; NSS = 0.016</span></pre><h2 id="0045" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">主题建模</h2><p id="66cc" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">主题建模通常用于文本挖掘，它允许我们创建一个统计模型来发现文本数据中的主题。有各种不同的方法/算法可以做到这一点。我们将在这里看一对夫妇。这是一种无人监管的方法。我们首先来看看潜在语义分析(LSA)，它与主成分分析(PCA)的工作原理相同。LSA是一个线性模型，并假设文档中的术语呈正态分布。它还使用SVD(奇异值分解),这在计算上是昂贵的。这种方法减少了数据中的噪声。</p><p id="bdb9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">注意:</strong> SVD的工作原理是将一个文档术语矩阵分成3个连续的方阵(其中一个是对角矩阵)，然后将它们转置并再次相乘。这个方法可以用来求矩阵的逆矩阵。</p><p id="a6df" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第一阶段涉及创建文档术语矩阵。这在行中表示文档，在列中表示术语，在单元格中表示TF-IDF分数。然后，我们将SVD应用于这个矩阵，以获得最终的主题列表。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/f97d5da8379f06f2550d93ac3cfd8921.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*4XpBbInqCWnHtHTP8iBBbA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">文档-术语矩阵，行中有文档，列中有术语。单元格包含TF-IDF分数(图片由作者提供)</p></figure><p id="68a0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以使用之前使用的相同的<strong class="ky ir">tfidf矢量器</strong>来计算TF-IDF分数。我们将限制术语(特征)的数量，以将所需的计算资源减少到800。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="b599" class="lz ma iq my b gy nc nd l ne nf">from sklearn.feature_extraction.text import TfidfVectorizer<br/>v = TfidfVectorizer(stop_words='english', max_features=800, max_df=0.5)<br/>X = v.fit_transform(data_files)</span></pre><p id="a72b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们查看文档-术语矩阵的维度，我们可以看到行中有4个文档，列中有800个术语。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="e589" class="lz ma iq my b gy nc nd l ne nf">X.shape</span><span id="2a6f" class="lz ma iq my b gy ng nd l ne nf">&gt;&gt;&gt; (4, 800)</span></pre><p id="21ad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们现在需要实现SVD，我们可以使用<strong class="ky ir">截断的SVD </strong>类来实现它。这将为我们做繁重的工作。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="2707" class="lz ma iq my b gy nc nd l ne nf">from sklearn.decomposition import TruncatedSVD</span></pre><p id="7d3c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以用参数<strong class="ky ir"> n_components </strong>指定主题的数量。在这种情况下，我们将它设置为4，假设每个文档有一个不同的主主题(<strong class="ky ir">注意:</strong>我们还可以使用像主题一致性这样的方法来确定主题的最佳数量<em class="no"> k </em>)。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="3e9c" class="lz ma iq my b gy nc nd l ne nf">svd = TruncatedSVD(n_components=4)</span></pre><p id="a8a9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们拟合模型并获得特征名称:</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="14a6" class="lz ma iq my b gy nc nd l ne nf">svd.fit(X)</span><span id="0593" class="lz ma iq my b gy ng nd l ne nf">doc_terms = v.get_feature_names()</span></pre><p id="b148" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们可以输出与每个主题相关的术语。在这种情况下，4个主题中的每一个都有前12个。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="3ddc" class="lz ma iq my b gy nc nd l ne nf">for i, component in enumerate(svd.components_):<br/>    terms_comp = zip(doc_terms, component)<br/>    sorted_terms = sorted(terms_comp, key=lambda x:x[1], reverse=True)[:12]<br/>    print("")<br/>    print("Topic "+str(i+1)+": ", end="")<br/>    for term in sorted_terms:<br/>        print(term[0], " ", end="")</span><span id="ef19" class="lz ma iq my b gy ng nd l ne nf">&gt;&gt;&gt; Topic 1: rsquo href ldquo rdquo ventilators easter keytruda quebec unincorporated ford books inmates <br/>Topic 2: davos wef sibley denly stamler comox nortje caf pd rsquo href ldquo <br/>Topic 3: hopland geely easyjet davos vanderbilt wef asbestos macy jamaat sibley denly stamler <br/>Topic 4: rsquo href ldquo rdquo quebec div eacute src noopener rel mdash rsv</span></pre><p id="ee69" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，由您根据这些主题所包含的单词来确定它们可能代表什么(主题标签)。</p><p id="fc32" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">主题建模的另一个流行选项是潜在狄利克雷分配(LDA ),不要与其他LDA(线性判别分析)混淆。LDA假设单词的狄利克雷分布，并创建语义向量空间模型。其实现的具体细节超出了本入门书的范围，但本质上，它将文档术语矩阵转换为两个矩阵。一个表示文档和主题，第二个表示主题和术语。然后，该算法尝试根据主题生成所讨论的单词的概率计算来调整每个文档中每个单词的主题。</p><p id="aa8d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们使用与前面LSA相同的概念。首先，我们用<strong class="ky ir">计数矢量器</strong>将数据标记化。然后，我们再次创建LDA类的实例，将主题数量设置为4，并输出前12个。</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="6d77" class="lz ma iq my b gy nc nd l ne nf">from sklearn.feature_extraction.text import CountVectorizer<br/>cv = CountVectorizer()<br/>fitted = cv.fit_transform(data_files)<br/>from sklearn.decomposition import LatentDirichletAllocation<br/>lda = LatentDirichletAllocation(n_components=4, random_state=42)<br/>lda.fit(fitted)</span></pre><p id="6849" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，我们可以像以前一样输出结果:</p><pre class="kg kh ki kj gt mx my mz na aw nb bi"><span id="9a1e" class="lz ma iq my b gy nc nd l ne nf">doc_terms = cv.get_feature_names()</span><span id="a322" class="lz ma iq my b gy ng nd l ne nf">for i, component in enumerate(lda.components_):<br/>   terms_comp = zip(doc_terms, component)<br/>   sorted_terms = sorted(terms_comp, key=lambda x:x[1], reverse=True)[:12]<br/>   print("")<br/>   print("Topic "+str(i+1)+": ", end="")<br/>   for term in sorted_terms:<br/>       print(term[0], " ", end="")</span><span id="d17e" class="lz ma iq my b gy ng nd l ne nf">&gt;&gt;&gt; Topic 1: said  19  covid  people  coronavirus  new  health  also  would  one  pandemic  time  <br/>Topic 2: 021  040  25000  421  4q  712  85th  885  accrues  accuser  acuity  afterthought  <br/>Topic 3: said  coronavirus  people  health  19  new  covid  also  cases  virus  one  would  <br/>Topic 4: 021  040  25000  421  4q  712  85th  885  accrues  accuser  acuity  afterthought</span></pre><p id="15d7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们认为这些数字不相关，我们可能还想回去过滤掉它们。您可以看到，这与我们之前看到的LSA产生了非常不同的结果。首先，LSA是主题建模的良好开端。如果需要，LDA提供不同的选项。</p><h2 id="2f1c" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">摘要</h2><p id="ebe5" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">这本初级读本概述了现代NLP任务中使用的一些常用方法，以及如何用Python实现这些方法。每种方法都有细微差别，需要根据手头的任务来考虑。还有像词性标注这样的方法，也称为语法标注，可以为算法提供额外的信息。例如，你可以在某些单词上标注该单词是名词、动词、形容词还是副词等信息。这通常被表示为具有<strong class="ky ir">单词、标签</strong>的元组列表，例如[('build '，' v ')，(' walk '，' v ')，(' mountain '，' n')]。还有许多方法可以表示单词和术语，用于后续处理，例如word2vec和单词包等。您选择的格式将再次取决于任务和算法要求。与所有机器学习和数据科学一样，数据预处理所花费的时间通常最长，并且对生成的输出有很大影响。读完这本初级读本后，你应该有希望开始从文本数据中获得一些有趣的见解，并对你可能用来分析这类数据的一些方法有所了解。</p><h2 id="7fbf" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">参考</h2><p id="0354" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">[1]戴维斯，马克。(2019-)冠状病毒语料库。可在https://www.english-corpora.org/corona/的<a class="ae kv" href="https://www.english-corpora.org/corona/" rel="noopener ugc nofollow" target="_blank">在线购买。</a></p></div></div>    
</body>
</html>