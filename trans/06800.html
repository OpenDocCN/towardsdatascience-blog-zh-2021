<html>
<head>
<title>K-Means tricks for fun and profit</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">意思是为了乐趣和利益的诡计</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/k-means-tricks-for-fun-and-profit-724996864274?source=collection_archive---------14-----------------------#2021-06-19">https://towardsdatascience.com/k-means-tricks-for-fun-and-profit-724996864274?source=collection_archive---------14-----------------------#2021-06-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="60a3" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">K-Means是一个有趣、简单、非常直观的算法。事实证明，它可以做的不仅仅是集群。</h2></div><h1 id="a600" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">序言</h1><p id="7c01" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这将是一个非常小的帖子，但仍然是一个有趣的帖子。</p><p id="0ba3" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">K-Means是一个优雅的算法。这很容易理解(制造随机点，反复移动它们以成为一些现有集群的中心)，并且在实践中工作得很好。当我第一次了解它的时候，我记得我很着迷。很优雅。但是，随着时间的推移，这种兴趣逐渐消失，我注意到了许多限制，其中之一是球形簇先验，它的线性性质，我发现在EDA场景中特别令人讨厌的是，它不会自己找到最佳的簇数，所以你也需要修改这个参数。然后，几年前，我发现了一些如何使用K-Means的巧妙技巧。所以开始了。</p><h1 id="4bcf" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated"><strong class="ak">第一招</strong></h1><p id="f4dc" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">首先，我们需要建立一个基线。我将主要使用乳腺癌数据集，但是您可以使用任何其他数据集。</p><pre class="ly lz ma mb gt mc md me mf aw mg bi"><span id="5ca6" class="mh kg iq md b gy mi mj l mk ml">from sklearn.cluster import KMeans<br/>from sklearn.svm import LinearSVC<br/>from sklearn.datasets import load_breast_cancer<br/>from sklearn.model_selection import train_test_split</span><span id="c83c" class="mh kg iq md b gy mm mj l mk ml">import numpy as np</span><span id="4b3c" class="mh kg iq md b gy mm mj l mk ml"><br/>X, y = load_breast_cancer(return_X_y=True)</span><span id="7260" class="mh kg iq md b gy mm mj l mk ml">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=17)</span><span id="b241" class="mh kg iq md b gy mm mj l mk ml">svm = LinearSVC(random_state=17)<br/>svm.fit(X_train, y_train)<br/>svm.score(X_test, y_test) <strong class="md ir"># should be ~0.93</strong></span></pre><p id="93a6" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">那么，是什么让我重新对K-Means产生了兴趣呢？</p><blockquote class="mn"><p id="f722" class="mo mp iq bd mq mr ms mt mu mv mw ls dk translated"><strong class="ak"> K-Means可以作为新特征的来源。</strong></p></blockquote><p id="a05e" class="pw-post-body-paragraph kx ky iq kz b la mx jr lc ld my ju lf lg mz li lj lk na lm ln lo nb lq lr ls ij bi translated">你可能会问，怎么做？K-Means是一种聚类算法，对吧？您可以添加推断聚类作为新的分类特征。</p><p id="d423" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">现在，让我们试试这个。</p><pre class="ly lz ma mb gt mc md me mf aw mg bi"><span id="b4e1" class="mh kg iq md b gy mi mj l mk ml"># imports from the example above</span><span id="70ea" class="mh kg iq md b gy mm mj l mk ml">svm = LinearSVC(random_state=17)<br/>kmeans = KMeans(n_clusters=3, random_state=17)</span><span id="54c2" class="mh kg iq md b gy mm mj l mk ml">X_clusters = kmeans.fit_predict(X_train).reshape(-1, 1)</span><span id="bfb4" class="mh kg iq md b gy mm mj l mk ml">svm.fit(np.hstack([X_train, X_clusters]), y_train)<br/>svm.score(np.hstack([X_test, kmeans.predict(X_test) \<br/>   .reshape(-1, 1)]), y_test) <strong class="md ir"># should be ~0.937</strong></span></pre><figure class="ly lz ma mb gt nd gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/7ebc929148753449280203361d26de46.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/0*8R4UbDkgouaFnb6K.png"/></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">【knowyourmeme.com】来源:T4</p></figure><p id="4450" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">这些特征是分类的，但是我们可以要求模型输出到所有质心的距离，从而获得(希望)更多的信息特征。</p><pre class="ly lz ma mb gt mc md me mf aw mg bi"><span id="5129" class="mh kg iq md b gy mi mj l mk ml"># imports from the example above</span><span id="ebe5" class="mh kg iq md b gy mm mj l mk ml">svm = LinearSVC(random_state=17)<br/>kmeans = KMeans(n_clusters=3, random_state=17)</span><span id="248e" class="mh kg iq md b gy mm mj l mk ml">X_clusters = kmeans.fit_transform(X_train)<br/>#                       ^^^^^^^^^<br/>#                       Notice the `transform` instead of `predict`<br/># Scikit-learn supports this method as early as version 0.15</span><span id="66fc" class="mh kg iq md b gy mm mj l mk ml">svm.fit(np.hstack([X_train, X_clusters]), y_train)<br/>svm.score(np.hstack([X_test, kmeans.transform(X_test)]), y_test)<br/><strong class="md ir"># should be ~0.727</strong></span></pre><p id="480c" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">等等，怎么了？现有的特征和到质心的距离之间有关联吗？</p><pre class="ly lz ma mb gt mc md me mf aw mg bi"><span id="7c39" class="mh kg iq md b gy mi mj l mk ml">import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>import pandas as pd</span><span id="99bc" class="mh kg iq md b gy mm mj l mk ml">columns = ['mean radius', 'mean texture', 'mean perimeter', 'mean area', 'mean smoothness', 'mean compactness', 'mean concavity', 'mean concave points', 'mean symmetry', 'mean fractal dimension', 'radius error', 'texture error', 'perimeter error', 'area error', 'smoothness error', 'compactness error', 'concavity error', 'concave points error', 'symmetry error', 'fractal dimension error', 'worst radius', 'worst texture', 'worst perimeter', 'worst area', 'worst smoothness', 'worst compactness', 'worst concavity', 'worst concave points', 'worst symmetry', 'worst fractal dimension', <strong class="md ir">'distance to cluster 1', 'distance to cluster 2', 'distance to cluster 3'</strong>]</span><span id="c19c" class="mh kg iq md b gy mm mj l mk ml">data = pd.DataFrame.from_records(np.hstack([X_train, X_clusters]), columns=columns)</span><span id="32b5" class="mh kg iq md b gy mm mj l mk ml">sns.heatmap(data.corr())<br/>plt.xticks(rotation=-45)<br/>plt.show()</span></pre><figure class="ly lz ma mb gt nd gh gi paragraph-image"><div role="button" tabindex="0" class="nm nn di no bf np"><div class="gh gi nl"><img src="../Images/24987eeda2daf5e748c57bb96e986846.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7MjOaze1lrLSSTEE.png"/></div></div><p class="ng nh gj gh gi ni nj bd b be z dk translated">注意最后3列，尤其是最后一列，以及它们在每一行的颜色。来源:帖子作者，见上面代码片段生成。</p></figure><p id="5cb6" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">您可能听说过我们希望数据集中的要素尽可能独立。原因是很多机器学习模型假设这种独立性是为了有更简单的算法。关于这个主题的更多信息可以在<a class="ae nq" href="https://datascience.stackexchange.com/questions/24452/in-supervised-learning-why-is-it-bad-to-have-correlated-features" rel="noopener ugc nofollow" target="_blank">这里</a>和<a class="ae nq" rel="noopener" target="_blank" href="/why-exclude-highly-correlated-features-when-building-regression-model-34d77a90ea8e">这里</a>找到，但它的要点是，在线性模型中有冗余信息会使模型不稳定，反过来，它更有可能搞砸。在许多情况下，我注意到了这个问题，有时甚至是非线性模型，清除数据集的相关特征通常会略微提高模型的性能特征。</p><p id="3efe" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">回到我们的主题。假设我们的新特征确实与一些现有的特征相关，那么如果我们只使用到聚类均值的距离作为特征，这是否可行呢？</p><pre class="ly lz ma mb gt mc md me mf aw mg bi"><span id="edfa" class="mh kg iq md b gy mi mj l mk ml"># imports from the example above</span><span id="0e69" class="mh kg iq md b gy mm mj l mk ml">svm = LinearSVC(random_state=17)<br/>kmeans = KMeans(n_clusters=3, random_state=17)<br/>X_clusters = kmeans.fit_transform(X_train)</span><span id="743c" class="mh kg iq md b gy mm mj l mk ml">svm.fit(X_clusters, y_train)<br/>svm.score(kmeans.transform(X_test), y_test) <strong class="md ir"># should be ~0.951</strong></span></pre><p id="602c" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">好多了。通过这个例子，你可以看到我们可以使用K-Means作为降维的方法。干净利落。</p><p id="eaf0" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">到目前为止一切顺利。但是抵抗的部分还没有出现。</p><h1 id="8948" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated"><strong class="ak">第二招</strong></h1><blockquote class="mn"><p id="1548" class="mo mp iq bd mq mr ms mt mu mv mw ls dk translated"><strong class="ak"> K-Means可以作为内核技巧的替代品</strong></p></blockquote><p id="927b" class="pw-post-body-paragraph kx ky iq kz b la mx jr lc ld my ju lf lg mz li lj lk na lm ln lo nb lq lr ls ij bi translated">你没听错。例如，你可以为K-Means算法定义比特征更多的质心。</p><pre class="ly lz ma mb gt mc md me mf aw mg bi"><span id="1c84" class="mh kg iq md b gy mi mj l mk ml"># imports from the example above</span><span id="62ca" class="mh kg iq md b gy mm mj l mk ml">svm = LinearSVC(random_state=17)<br/>kmeans = KMeans(n_clusters=250, random_state=17)<br/>X_clusters = kmeans.fit_transform(X_train)</span><span id="46f7" class="mh kg iq md b gy mm mj l mk ml">svm.fit(X_clusters, y_train)<br/>svm.score(kmeans.transform(X_test), y_test) <strong class="md ir"># should be ~0.944</strong></span></pre><p id="6789" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">嗯，没那么好，但是相当不错。在实践中，这种方法的最大好处是当您拥有大量数据时。此外，在预测性能方面，您的收获可能会有所不同，我曾经用<code class="fe ns nt nu md b">n_clusters=1000</code>运行过这种方法，它比只使用几个集群效果更好。</p><p id="7af9" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">众所周知，支持向量机在大数据集上训练很慢。极其缓慢。去过那里，做过那个。这就是为什么，举例来说，有许多技术可以用少得多的计算资源来近似内核技巧。</p><p id="e426" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">顺便说一下，让我们比较一下这个K-Means技巧与经典的SVM和一些替代的核近似方法的效果。</p><p id="165a" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">以下代码的灵感来自于<a class="ae nq" href="https://scikit-learn.org/stable/auto_examples/miscellaneous/plot_kernel_approximation.html" rel="noopener ugc nofollow" target="_blank">这</a> <a class="ae nq" href="https://scikit-learn.org/stable/auto_examples/kernel_approximation/plot_scalable_poly_kernels.html" rel="noopener ugc nofollow" target="_blank">两个</a> scikit-learn示例。</p><pre class="ly lz ma mb gt mc md me mf aw mg bi"><span id="1994" class="mh kg iq md b gy mi mj l mk ml">import matplotlib.pyplot as plt</span><span id="ff7c" class="mh kg iq md b gy mm mj l mk ml">import numpy as np</span><span id="2f51" class="mh kg iq md b gy mm mj l mk ml">from time import time</span><span id="742d" class="mh kg iq md b gy mm mj l mk ml">from sklearn.datasets import load_breast_cancer</span><span id="018c" class="mh kg iq md b gy mm mj l mk ml">from sklearn.svm import LinearSVC, SVC<br/>from sklearn import pipeline<br/>from sklearn.kernel_approximation import RBFSampler, Nystroem, PolynomialCountSketch<br/>from sklearn.preprocessing import MinMaxScaler, Normalizer<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.cluster import MiniBatchKMeans</span><span id="88f0" class="mh kg iq md b gy mm mj l mk ml"><br/>mm = pipeline.make_pipeline(MinMaxScaler(), Normalizer())</span><span id="9ea9" class="mh kg iq md b gy mm mj l mk ml">X, y = load_breast_cancer(return_X_y=True)<br/>X = mm.fit_transform(X)</span><span id="d896" class="mh kg iq md b gy mm mj l mk ml">data_train, data_test, targets_train, targets_test = train_test_split(X, y, random_state=17)</span></pre><p id="062f" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">我们将测试scikit-learn包中可用的3种内核近似方法，与K-Means技巧相对比，作为基线，我们将有一个线性SVM和一个使用内核技巧的SVM。</p><pre class="ly lz ma mb gt mc md me mf aw mg bi"><span id="50f7" class="mh kg iq md b gy mi mj l mk ml"># Create a classifier: a support vector classifier<br/>kernel_svm = SVC(gamma=.2, random_state=17)<br/>linear_svm = LinearSVC(random_state=17)</span><span id="4f40" class="mh kg iq md b gy mm mj l mk ml"># create pipeline from kernel approximation and linear svm<br/>feature_map_fourier = RBFSampler(gamma=.2, random_state=17)<br/>feature_map_nystroem = Nystroem(gamma=.2, random_state=17)<br/>feature_map_poly_cm = PolynomialCountSketch(degree=4, random_state=17)<br/>feature_map_kmeans = MiniBatchKMeans(random_state=17)</span><span id="be43" class="mh kg iq md b gy mm mj l mk ml">fourier_approx_svm = pipeline.Pipeline([("feature_map", feature_map_fourier), ("svm", LinearSVC(random_state=17))])</span><span id="4225" class="mh kg iq md b gy mm mj l mk ml">nystroem_approx_svm = pipeline.Pipeline([("feature_map", feature_map_nystroem), ("svm", LinearSVC(random_state=17))])</span><span id="d07c" class="mh kg iq md b gy mm mj l mk ml">poly_cm_approx_svm = pipeline.Pipeline([("feature_map", feature_map_poly_cm), ("svm", LinearSVC(random_state=17))])</span><span id="3d70" class="mh kg iq md b gy mm mj l mk ml">kmeans_approx_svm = pipeline.Pipeline([("feature_map", feature_map_kmeans), ("svm", LinearSVC(random_state=17))])</span></pre><p id="78c0" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">让我们收集每种配置的计时和评分结果。</p><pre class="ly lz ma mb gt mc md me mf aw mg bi"><span id="c7ae" class="mh kg iq md b gy mi mj l mk ml"># fit and predict using linear and kernel svm:<br/>kernel_svm_time = time()<br/>kernel_svm.fit(data_train, targets_train)<br/>kernel_svm_score = kernel_svm.score(data_test, targets_test)<br/>kernel_svm_time = time() - kernel_svm_time</span><span id="877f" class="mh kg iq md b gy mm mj l mk ml">linear_svm_time = time()<br/>linear_svm.fit(data_train, targets_train)<br/>linear_svm_score = linear_svm.score(data_test, targets_test)<br/>linear_svm_time = time() - linear_svm_time</span><span id="8da2" class="mh kg iq md b gy mm mj l mk ml">sample_sizes = 30 * np.arange(1, 10)<br/>fourier_scores = []<br/>nystroem_scores = []<br/>poly_cm_scores = []<br/>kmeans_scores = []</span><span id="5402" class="mh kg iq md b gy mm mj l mk ml">fourier_times = []<br/>nystroem_times = []<br/>poly_cm_times = []<br/>kmeans_times = []</span><span id="960e" class="mh kg iq md b gy mm mj l mk ml">for D in sample_sizes:<br/>    fourier_approx_svm.set_params(feature_map__n_components=D)<br/>    nystroem_approx_svm.set_params(feature_map__n_components=D)<br/>    poly_cm_approx_svm.set_params(feature_map__n_components=D)<br/>    kmeans_approx_svm.set_params(feature_map__n_clusters=D)</span><span id="2ef8" class="mh kg iq md b gy mm mj l mk ml">    start = time()<br/>    nystroem_approx_svm.fit(data_train, targets_train)<br/>    nystroem_times.append(time() - start)</span><span id="5869" class="mh kg iq md b gy mm mj l mk ml">    start = time()<br/>    fourier_approx_svm.fit(data_train, targets_train)<br/>    fourier_times.append(time() - start)</span><span id="6759" class="mh kg iq md b gy mm mj l mk ml">    start = time()<br/>    poly_cm_approx_svm.fit(data_train, targets_train)<br/>    poly_cm_times.append(time() - start)</span><span id="7f57" class="mh kg iq md b gy mm mj l mk ml">    start = time()<br/>    kmeans_approx_svm.fit(data_train, targets_train<br/>    kmeans_times.append(time() - start)</span><span id="c4ca" class="mh kg iq md b gy mm mj l mk ml">    fourier_score = fourier_approx_svm.score(data_test, targets_test)<br/>    fourier_scores.append(fourier_score)</span><span id="a03e" class="mh kg iq md b gy mm mj l mk ml">    nystroem_score = nystroem_approx_svm.score(data_test, targets_test)<br/>    nystroem_scores.append(nystroem_score)</span><span id="3ba5" class="mh kg iq md b gy mm mj l mk ml">    poly_cm_score = poly_cm_approx_svm.score(data_test, targets_test)<br/>    poly_cm_scores.append(poly_cm_score)</span><span id="3caf" class="mh kg iq md b gy mm mj l mk ml">    kmeans_score = kmeans_approx_svm.score(data_test, targets_test)<br/>    kmeans_scores.append(kmeans_score)</span></pre><p id="0315" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">现在让我们绘制所有收集的结果。</p><pre class="ly lz ma mb gt mc md me mf aw mg bi"><span id="7e6e" class="mh kg iq md b gy mi mj l mk ml">plt.figure(figsize=(16, 4))<br/>accuracy = plt.subplot(211)<br/>timescale = plt.subplot(212)</span><span id="082e" class="mh kg iq md b gy mm mj l mk ml">accuracy.plot(sample_sizes, nystroem_scores, label="Nystroem approx. kernel")<br/>timescale.plot(sample_sizes, nystroem_times, '--', label='Nystroem approx. kernel')<br/>accuracy.plot(sample_sizes, fourier_scores, label="Fourier approx. kernel")<br/>timescale.plot(sample_sizes, fourier_times, '--', label='Fourier approx. kernel')<br/>accuracy.plot(sample_sizes, poly_cm_scores, label="Polynomial Count-Min approx. kernel")<br/>timescale.plot(sample_sizes, poly_cm_times, '--', label='Polynomial Count-Min approx. kernel')<br/>accuracy.plot(sample_sizes, kmeans_scores, label="K-Means approx. kernel")<br/>timescale.plot(sample_sizes, kmeans_times, '--', label='K-Means approx. kernel')</span><span id="1bae" class="mh kg iq md b gy mm mj l mk ml"># horizontal lines for exact rbf and linear kernels:<br/>accuracy.plot([sample_sizes[0], sample_sizes[-1]],<br/>[linear_svm_score, linear_svm_score], label="linear svm")<br/>timescale.plot([sample_sizes[0], sample_sizes[-1]], [linear_svm_time, linear_svm_time], '--', label='linear svm')</span><span id="00d1" class="mh kg iq md b gy mm mj l mk ml">accuracy.plot([sample_sizes[0], sample_sizes[-1]],<br/>[kernel_svm_score, kernel_svm_score], label="rbf svm")<br/>timescale.plot([sample_sizes[0], sample_sizes[-1]], [kernel_svm_time, kernel_svm_time], '--', label='rbf svm')</span></pre><p id="431f" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">和一些更多的情节调整，使它漂亮。</p><pre class="ly lz ma mb gt mc md me mf aw mg bi"><span id="86ca" class="mh kg iq md b gy mi mj l mk ml"># legends and labels<br/>accuracy.set_title("Classification accuracy")<br/>timescale.set_title("Training times")<br/>accuracy.set_xlim(sample_sizes[0], sample_sizes[-1])<br/>accuracy.set_xticks(())<br/>accuracy.set_ylim(np.min(fourier_scores), 1)<br/>timescale.set_xlabel("Sampling steps = transformed feature dimension")<br/>accuracy.set_ylabel("Classification accuracy")<br/>timescale.set_ylabel("Training time in seconds")<br/>accuracy.legend(loc='best')<br/>timescale.legend(loc='best')<br/>plt.tight_layout()<br/>plt.show()</span></pre><figure class="ly lz ma mb gt nd gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/1bd0331cfc39f084b66801034896f835.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*ol2jC7fj0ggEAKxo.png"/></div><p class="ng nh gj gh gi ni nj bd b be z dk translated"><em class="nk">咩。这一切都是徒劳的吗？来源:</em>帖子作者，见上面代码片段生成。</p></figure><p id="cc9a" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">你猜怎么着一点也不。即使它是最慢的，K-Means作为RBF核的近似仍然是一个好的选择。我没开玩笑。你可以在scikit-learn中使用这种特殊的K-Means，称为<code class="fe ns nt nu md b">MiniBatchKMeans</code>，它是少数支持<code class="fe ns nt nu md b">.partial_fit</code>方法的算法之一。将这与也有<code class="fe ns nt nu md b">.partial_fit</code>的机器学习模型结合起来，就像<code class="fe ns nt nu md b">PassiveAggressiveClassifier</code>一样，可以创建一个非常有趣的解决方案。</p><p id="6b3b" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">注意，<code class="fe ns nt nu md b">.partial_fit</code>的妙处是双重的。首先，它使得以核外方式训练算法成为可能，也就是说，使用比RAM中所能容纳的更多的数据。其次，根据您的问题类型，如果您原则上(原则上非常非常)不需要切换模型，那么可以在部署模型的地方对其进行额外的培训。那叫在线学习，超级有意思。类似这样的事情是一些中国公司正在做的，一般来说对<a class="ae nq" href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41159.pdf" rel="noopener ugc nofollow" target="_blank">广告技术</a>非常有用，因为你可以在几秒钟内收到你的广告推荐是对还是错的信息。</p><p id="d109" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">你知道吗，这里有一个关于这种方法的小例子。</p><pre class="ly lz ma mb gt mc md me mf aw mg bi"><span id="4ff1" class="mh kg iq md b gy mi mj l mk ml">from sklearn.cluster import MiniBatchKMeans<br/>from sklearn.linear_model import PassiveAggressiveClassifier<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.datasets import load_breast_cancer</span><span id="4764" class="mh kg iq md b gy mm mj l mk ml">import numpy as np</span><span id="8ada" class="mh kg iq md b gy mm mj l mk ml"><br/>def batch(iterable, n=1):<br/>    # source: <a class="ae nq" href="https://stackoverflow.com/a/8290508/5428334" rel="noopener ugc nofollow" target="_blank">https://stackoverflow.com/a/8290508/5428334</a><br/>    l = len(iterable)<br/>    for ndx in range(0, l, n):<br/>        yield iterable[ndx:min(ndx + n, l)]</span><span id="68e8" class="mh kg iq md b gy mm mj l mk ml">X, y = load_breast_cancer(return_X_y=True)<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=17)</span><span id="3dc5" class="mh kg iq md b gy mm mj l mk ml">kmeans = MiniBatchKMeans(n_clusters=100, random_state=17)<br/># K-Means has a constraint, n_clusters &lt;= n_samples to fit<br/>pac = PassiveAggressiveClassifier(random_state=17)</span><span id="0af4" class="mh kg iq md b gy mm mj l mk ml">for x, y in zip(batch(X_train, n=100), batch(y_train, n=100)):<br/>    kmeans.partial_fit(x, y)       # fit K-Means a bit<br/>    x_dist = kmeans.transform(x)   # obtain distances<br/>    pac.partial_fit(x_dist, y, classes=[0, 1])<br/>    # learn a bit the classifier, we need to indicate the classes</span><span id="57dd" class="mh kg iq md b gy mm mj l mk ml">    print(pac.score(kmeans.transform(X_test), y_test))<br/><strong class="md ir"># 0.909 after 100 samples<br/># 0.951 after 200 samples<br/># 0.951 after 300 samples<br/># 0.944 after 400 samples<br/># 0.902 after 426 samples</strong></span><span id="ebb2" class="mh kg iq md b gy mm mj l mk ml"># VS</span><span id="26e2" class="mh kg iq md b gy mm mj l mk ml">kmeans = MiniBatchKMeans(n_clusters=100, random_state=17)<br/>pac = PassiveAggressiveClassifier(random_state=17)</span><span id="55a5" class="mh kg iq md b gy mm mj l mk ml">pac.fit(kmeans.fit_transform(X_train), y_train)<br/>pac.score(kmeans.transform(X_test), y_test) <strong class="md ir"># should be ~0.951</strong></span></pre><h1 id="6b54" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">收场白</h1><p id="7b80" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">所以你坚持到了最后。希望现在你的ML工具集更丰富了。也许你听说过所谓的“没有免费的午餐”定理；基本上，没有银弹，在这种情况下，对于ML问题。也许对于下一个项目，这篇文章中概述的方法不会起作用，但是对于接下来的一个项目，它们会起作用。所以你就自己去实验看看吧。如果你需要一个在线学习算法/方法，那么，K-Means作为一个内核近似，更有可能是你的正确工具。</p><p id="9017" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">顺便说一句，还有另一篇博客文章，也是关于ML的，正在撰写中。更好的是，在它的许多其他优点中，它描述了一种相当有趣的使用K-Means的方法。但是现在没有剧透。敬请关注。</p><p id="6785" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">最后，如果你正在读这篇文章，谢谢你！如果你想留下一些反馈或只是有一个问题，HMU在Twitter上，或只是留下评论。</p><h2 id="3e5c" class="mh kg iq bd kh nw nx dn kl ny nz dp kp lg oa ob kr lk oc od kt lo oe of kv og bi translated">一些你可能会感兴趣的链接</h2><ul class=""><li id="ed45" class="oh oi iq kz b la lb ld le lg oj lk ok lo ol ls om on oo op bi translated"><a class="ae nq" href="https://datascience.stackexchange.com/questions/24324/how-to-use-k-means-outputs-extracted-features-as-svm-inputs" rel="noopener ugc nofollow" target="_blank">关于使用K-Means作为特征工程工具的stackexchange讨论</a></li><li id="0668" class="oh oi iq kz b la oq ld or lg os lk ot lo ou ls om on oo op bi translated"><a class="ae nq" href="https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html" rel="noopener ugc nofollow" target="_blank">对K-Means的更深入解释</a></li><li id="2aa2" class="oh oi iq kz b la oq ld or lg os lk ot lo ou ls om on oo op bi translated"><a class="ae nq" href="http://www.jcomputers.us/vol8/jcp0810-25.pdf" rel="noopener ugc nofollow" target="_blank">使用K-Means进行高效SVM的研究论文</a></li></ul><h2 id="4d81" class="mh kg iq bd kh nw nx dn kl ny nz dp kp lg oa ob kr lk oc od kt lo oe of kv og bi translated">承认</h2><p id="08c0" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">特别感谢<a class="ae nq" href="https://twitter.com/dgaponcic" rel="noopener ugc nofollow" target="_blank"> @dgaponcic </a>的风格检查和内容审核，也感谢<a class="ae nq" href="https://twitter.com/anisoara_ionela" rel="noopener ugc nofollow" target="_blank">@ anisara _ ionela</a>比任何人工智能更彻底地检查了这篇文章的语法。你是最好的❤</p><p id="daa1" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated"><strong class="kz ir">附言</strong>我相信你已经注意到准则中的所有这些<code class="fe ns nt nu md b">random_state</code>。如果你想知道我为什么添加这些，那是为了使代码样本可重复。因为教程经常不这样做，这给作者留下了挑选的空间，作者只给出最好的结果，当试图复制这些结果时，读者要么不能，要么要花很多时间。但是要知道，你可以随意改变<code class="fe ns nt nu md b">random_state</code>的值，得到非常不同的结果。例如，当使用原始特征和到3个质心的距离运行代码片段时，得分为0.727，随机种子为41而不是17，您可以获得0.944的精度得分。所以，是的，<code class="fe ns nt nu md b">random_state</code>无论你选择的框架中随机种子是什么，这都是要记住的一个重要方面，尤其是在做研究的时候。</p></div><div class="ab cl ov ow hu ox" role="separator"><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa pb"/><span class="oy bw bk oz pa"/></div><div class="ij ik il im in"><p id="52e5" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated"><em class="nr">原载于2021年6月19日</em><a class="ae nq" href="https://alexandruburlacu.github.io/posts/2021-06-18-kmeans-trick" rel="noopener ugc nofollow" target="_blank"><em class="nr">https://alexandruburlacu . github . io</em></a><em class="nr">。</em></p></div></div>    
</body>
</html>