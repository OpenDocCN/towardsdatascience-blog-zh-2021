<html>
<head>
<title>Backpropagation: The Simple Proof</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">反向传播:简单的证明</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/backpropagation-the-natural-proof-946c5abf63b1?source=collection_archive---------7-----------------------#2021-09-21">https://towardsdatascience.com/backpropagation-the-natural-proof-946c5abf63b1?source=collection_archive---------7-----------------------#2021-09-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="c4cc" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">是时候真正理解算法是如何工作的了。</h2></div><p id="28be" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">人工神经网络与其他机器学习算法的区别在于它们如何有效地处理大数据，以及它们如何对你的数据集假设很少。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/c163bf9d8e64ea71d37fbddae08ffc7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U4vnXs0NR5GygJJ2oRrtvQ.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">内核诡计[1]</p></figure><p id="8760" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您的神经网络不关心您的分类数据是否不能通过核进行线性分离，或者您的回归数据所遵循的趋势是否是过山车。只要你的数据集是从一个有限空间(<em class="lr"> x </em>)到另一个有限空间(<em class="lr"> y </em>)的连续映射，那么你就可以根据你的架构以任何精度近似这个映射。这是因为它们是通用近似器，正如<a class="ae ls" href="https://en.wikipedia.org/wiki/Universal_approximation_theorem" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">通用近似理论</strong> </a>所证明的。关键是，当神经网络在40年代首次出现时，没有快速的方法来利用神经网络的这一方面。直到1970年，<strong class="kh ir">反向传播</strong>——一种神经网络的快速训练算法以其现代形式发表。</p><p id="637f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这个故事中，我们将花大部分时间从数学角度理解这种算法是如何工作的。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lt"><img src="../Images/1959f9a8f6f7287c2c9fa5a7047b9aa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XgfSBO3poUeMsPABFGD-Qg.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">一个全局最小值的例子[2]</p></figure><p id="76e2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们深入研究它的证明之前。我想确定你知道两件事。首先，假设给定一个多变量函数<strong class="kh ir"><em class="lr">【x，y】</em></strong>(例如上图所示的曲面)那么最小化<strong class="kh ir"> <em class="lr"> f(x，y) </em> </strong>就是找到<strong class="kh ir">∂<em class="lr">f</em>/∂<em class="lr">x</em></strong><em class="lr"/>和<strong class="kh ir">∂<em class="lr">f</em>/∂<em class="lr">y</em></strong><em class="lr">t22】</em>如果我们的多变量函数是数百万个变量(权重和偏差)中的某个成本函数，这同样适用。如果我们希望将其最小化，那么只需找到该函数相对于每个参数的<strong class="kh ir"> </strong>偏导数，然后使用梯度下降迭代更新其参数，以最小化损失。<br/>第二个<strong class="kh ir"> </strong>我想让你注意的是<strong class="kh ir">链条法则</strong>。考虑<strong class="kh ir"> <em class="lr"> f(x) </em> </strong>与<strong class="kh ir"> <em class="lr"> x </em> </strong>为某变量中的函数<strong class="kh ir"> <em class="lr"> w </em> </strong> <em class="lr">， </em>然后去找<strong class="kh ir">∂<em class="lr">f</em>/∂<em class="lr">w</em></strong><em class="lr"/>我们写<br/><strong class="kh ir">∂<em class="lr">f</em>/∂w<em class="lr">=(</em>∂<em class="lr">f</em>/∂<em class="lr">x</em>×<em class="lr">(</em>∂<em class="lr"> <em class="lr"> x </em> ₃) </em></strong>与每个<strong class="kh ir"><em class="lr">xᵢ</em></strong><em class="lr"/>是<em class="lr"/><strong class="kh ir"><em class="lr">w</em></strong><em class="lr"/>中的一个函数通过为每个写它然后添加:<br/><strong class="kh ir">∂<em class="lr">f</em>/∂w<em class="lr">=(</em>∂<em class="lr">f +(</em>∂<em class="lr">f</em>/∂<em class="lr">x</em>₂)×<em class="lr">(</em>∂<em class="lr">x</em>₂/∂<em class="lr">w)+(</em>∂<em class="lr">f</em>/∂<em class="lr">x</em>₃)×<em class="lr">(</em>∩<em class="lr">x</em></strong></p><p id="be40" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">现在</strong>我们的目标很明确，给我们一个数据集，一个任意的神经网络，一个成本函数<em class="lr"> J </em>，我们需要找到<em class="lr"> J </em>关于神经网络中每个参数(权重/偏差)的导数，反向传播允许我们这样做，一次一个观察值。这意味着如果我们的损失函数是</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lu"><img src="../Images/6ee3a053f04a0d6899c5f97321d9724f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K97PXJHxzkzp08stFQBjbw.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">均方误差:<strong class="bd lv"> yᵢ </strong> <em class="lw">为观测值</em><strong class="bd lv"><em class="lw">y(xᵢ)</em></strong><em class="lw">为由于馈网用</em><strong class="bd lv"><em class="lw">【xᵢ】</em></strong><em class="lw">。</em> <strong class="bd lv"> <em class="lw"> d </em> </strong> <em class="lw">是数据集的长度。</em></p></figure><p id="a3b6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了找到它对某个权重或偏差的导数，我们可以找到</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lu"><img src="../Images/13b7331f643c4dcb8983f1ea8b1cb6de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*abTJCbHwS5PfTQMsTL7Oeg.png"/></div></div></figure><p id="5bbe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于每个单独的训练示例<strong class="kh ir"> <em class="lr"> (xᵢ、yᵢ) </em> </strong> <em class="lr"> </em>然后将它们全部相加并除以<strong class="kh ir"> d </strong>。就像我们在和中引入导数一样。向前看，上面的公式就是我们在写<strong class="kh ir"> <em class="lr"> J </em> </strong>时所暗指的。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lx"><img src="../Images/858f186b5238c6796b5834d780adafa2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DFyphBszMjiukJJK1qVTzQ.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">艺术的例证[3]</p></figure><p id="1920" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，在我们开始证明之前剩下的是决定一个<strong class="kh ir">符号</strong>。</p><p id="fc87" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于权重，设<strong class="kh ir"> <em class="lr"> w </em> ᴸₘₙ </strong>为从<strong class="kh ir"> <em class="lr"> (L-1) </em> ₜₕ </strong>层的<strong class="kh ir"> mₜₕ </strong>神经元到<strong class="kh ir"> <em class="lr"> L </em> ₜₕ </strong>层的<strong class="kh ir"> nₜₕ </strong>神经元的权重。例如，从输入层的第三个神经元到隐藏层的第二个神经元的权重将是<strong class="kh ir"> <em class="lr"> w </em> ⁰₃₂ </strong>我们稍后会回头看，让第一个隐藏层的权重为<strong class="kh ir"> L = 0 </strong>，但现在请耐心听我说。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ly"><img src="../Images/ab87cbd2c5edf2e219a15c3ffc83e94a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vn2_K4CUTOTDCOOi.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">神经网络示例[4]</p></figure><p id="947f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">除了权重，我们还有神经元特有的激活和偏差。我们将通过<strong class="kh ir"> <em class="lr"> a </em> ᴸₘ </strong>来表示<strong class="kh ir"> mₜₕ </strong>神经元在<strong class="kh ir"> <em class="lr"> L </em> ₜₕ </strong>层的激活，对于偏差我们将做<strong class="kh ir"> <em class="lr"> b </em> ᴸₘ </strong>。</p><p id="7fd6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，如果我们调用激活函数<strong class="kh ir"> <em class="lr"> h </em> </strong> <em class="lr"> </em>，那么我们可以写</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lu"><img src="../Images/1503dd21141a1b5ae5397f329ac740b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hnEVwuvzzDrxYdSBXnOu4g.png"/></div></div></figure><p id="1f51" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这准确地捕捉到了任意神经元内部发生的情况。激活后的线性组合。现在，如果我们称线性组合为<strong class="kh ir"> z </strong>可能会对我们有好处，所以下面是真的</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lu"><img src="../Images/fd315581951f24c59b3077596905af31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l7ss93RJLeuGbyGiJAgiMg.png"/></div></div></figure><p id="e72e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们要开始证明了吗？好吧，既然我们要实现这个(也许，在<a class="ae ls" rel="noopener" target="_blank" href="/implementing-backpropagation-with-style-in-python-da4c2f49adb4">的另一个故事</a>中)，如果我们也讨论一下如何用向量/矩阵符号来写我们到目前为止所介绍的内容，那将会很有帮助。</p><p id="8e38" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于神经网络中不是输入层的每一层，我们可以定义如下:</p><p id="1e3d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> Wᴸ </strong>:包含从<strong class="kh ir"> (L-1)ₜₕ </strong>层到<strong class="kh ir"/>层权重的权重矩阵。如果<strong class="kh ir"> (L-1)ₜₕ </strong>层具有<strong class="kh ir"> M </strong>个神经元，而<strong class="kh ir"> Lₜₕ </strong>层具有<strong class="kh ir"> N </strong>个神经元，则这具有维度<strong class="kh ir"> M*N </strong>。正如你所想象的，元素<strong class="kh ir">wᴸ[m,n】</strong>就是我们之前定义的<strong class="kh ir"> <em class="lr"> w </em> ᴸₘₙ </strong>。</p><p id="275d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> bᴸ </strong> : <strong class="kh ir"> </strong>任何层的偏置向量都会有其每个神经元的偏置，<strong class="kh ir"> bᴸ[m] = bᴸₘ </strong></p><p id="03e6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> aᴸ </strong> : <strong class="kh ir"> </strong>任何层的激活向量将具有其每个神经元的激活，<strong class="kh ir"> aᴸ[m] = <em class="lr"> a </em> ᴸₘ </strong></p><p id="8d39" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> zᴸ </strong> : <strong class="kh ir"> </strong>任何层的预激活向量将具有其每个神经元的线性组合，<strong class="kh ir"> zᴸ[m] = zᴸₘ </strong></p><p id="33a9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用这个新的符号，如果我们让函数<em class="lr"> h被</em>应用于向量元素，那么我们可以写</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lu"><img src="../Images/dfe768e9c4049de17e85fbfa36544c0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2riRWOYh8nb3rLhBaa3gXA.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">请注意，<strong class="bd lv"> h </strong>应用于向量<strong class="bd lv"> z </strong>中的每个元素。</p></figure><p id="5bd1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请记住，由于我们的符号，<strong class="kh ir"> W </strong>中的每一列都代表<strong class="kh ir"> Lₜₕ </strong>层中特定神经元的权重。这一事实以及矩阵乘法的工作原理应该能让你理解上面的公式。</p><p id="fe9b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">还要注意，对于<strong class="kh ir"> L = 0 </strong>(第一个隐藏层)，当且仅当我们用<strong class="kh ir"> <em class="lr"> x </em> </strong> <em class="lr"> </em>代替激活时，上述关系成立。我们没有给输入层<strong class="kh ir"> L = 0 </strong>的唯一原因是，这将使<strong class="kh ir"> Wᴸ </strong>、<strong class="kh ir"> bᴸ </strong>等。对于<strong class="kh ir"> L = 0 </strong>未定义。我们不妨让最后一层有<strong class="kh ir"> L = H </strong>，这样我们就可以很容易地在证明中突出它。这也使得<strong class="kh ir"> H </strong>成为我们网络中的隐藏层数。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lz"><img src="../Images/a420e29d9247e7785edac6dc54304d14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1kIZ_jGjh3qwiTQ53VPEHw.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">正在进行的证明的说明。[5]</p></figure><p id="0438" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们从找到<strong class="kh ir"> ∂J/∂ <em class="lr"> w </em> ᴸₘₙ </strong>开始我们可以利用这样一个事实，即<strong class="kh ir">t5】zᴸₙ</strong>是<strong class="kh ir">T9】wᴸₘₙ</strong>中的一个函数，通过使用链式法则，如下所示</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ma"><img src="../Images/53de74c46c62339cb258d001457ab3ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gfSyG_fJrkoOfrtJdIwqag.png"/></div></div></figure><p id="e387" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过将<strong class="kh ir"> <em class="lr"> z </em> ᴸₙ </strong>代入其定义，并考虑到<strong class="kh ir"> <em class="lr"> b </em> ᴸₙ </strong>不是<strong class="kh ir"> <em class="lr"> w </em> ᴸₘₙ </strong>中的函数，我们得到</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mb"><img src="../Images/1737b1d6b26013e0ffceffc16e4adf3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KX_4iL3yEYolAYDG87WJ8Q.png"/></div></div></figure><p id="02da" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，因为在求和中唯一能经受住微分的权重是i = m，我们可以写为</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mb"><img src="../Images/be080c70b79a30bb899ca27012522193.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ypKyujxQ_mr7od1-XDio5A.png"/></div></div></figure><p id="8540" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们证明的大部分时间里，我们会试图找到红色的术语；因此，给它一个名字是值得的。姑且称之为δᴸₙ吧。注意，每个神经元都有自己的<strong class="kh ir"> δᴸₙ </strong>。对于整个图层，我们可以像写bᴸ的<strong class="kh ir">、<strong class="kh ir"> aᴸ的</strong>和zᴸ的<strong class="kh ir">一样写矢量<strong class="kh ir"> δᴸ的</strong>。</strong></strong></p><p id="6eca" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以我们在索引符号中的结果变成了</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ma"><img src="../Images/167806c0b02676a9208964cd52ef91f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WYPJcAmoIar6CdwCjrsCSQ.png"/></div></div></figure><p id="7eec" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在向量符号中</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ma"><img src="../Images/9de1fcd31c2955e9ac0079944b4c0724.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4xfS_5oUtmAbwex7Jn5adA.png"/></div></div></figure><p id="9cd5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这直接来源于外积的定义和我们的指数符号结果。</p><p id="61e1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们利用类似的事实来找到∂J/∂bᴸₙ，即zᴸₙ是bᴸₙ的一个函数</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ma"><img src="../Images/14b80d6d05b1e23ed25e31974b51b501.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fdx0aE0tSogYkXFu7WvNTg.png"/></div></div></figure><p id="3e9e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">将<strong class="kh ir"> zᴸₙ </strong>代入其定义，并考虑到总和<em class="lr">不是<strong class="kh ir"> bᴸₙ </strong>中的</em>函数，我们得到</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ma"><img src="../Images/8880406915d5cc00188da52f66c707b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iHo_3W8RjNprExo4-VMAeA.png"/></div></div></figure><p id="cd77" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这意味着</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ma"><img src="../Images/9ca7434a700ae3314be5569f9ba63c6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RQVzSFgoNjeoclE0vYfbug.png"/></div></div></figure><p id="85c7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">或者用向量符号表示</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ma"><img src="../Images/a0ea1130a507927504be15ffe6d64e3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2_7kd8ddf00CDDaUH_IJOg.png"/></div></div></figure><p id="a703" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以证明的结论可以归结为找到delta。还记得我们的成本函数是什么样的吗？</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ma"><img src="../Images/855a6c8adefe928b59c5a5da1bf1fa83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jm0mBmDxSvsnrz5Y7s5GFA.png"/></div></div></figure><p id="2f10" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"><em class="lr">【y(x)】</em></strong>其实只是最后一层的激活，所以我们可以这样写</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ma"><img src="../Images/c3fc154be5b9a895a0af07266b104ccd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KbSqm99Rn8KxfJqwsomTNg.png"/></div></div></figure><p id="7e8e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因为<strong class="kh ir"> <em class="lr"> J </em> </strong>在<strong class="kh ir"><em class="lr">【aᴴ</em></strong>中明确是一个函数，这应该会诱使你利用链式法则并涉及<strong class="kh ir"><em class="lr"/></strong><em class="lr">去寻找<strong class="kh ir">【δᴴₙ</strong>。</em></p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ma"><img src="../Images/05f856c2c1edbd5143eb044e02ab2c2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jXiv5fv_vaZ7xnn5a8wc2w.png"/></div></div></figure><p id="119c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这样，我们就隐含地假设<strong class="kh ir"><em class="lr">【aᴴₙ】</em></strong>只是该层中<strong class="kh ir">【zᴴₙ】</strong>的一个函数，这对于你遇到的大多数成本函数来说是正确的，但对于<strong class="kh ir"> soft-max </strong>来说不是，因为任何<strong class="kh ir"> <em class="lr"> aₙ </em> </strong>都是该层中所有<strong class="kh ir"> zₙ </strong>的函数。在这种情况下，我们需要用链式法则的多变量版本来考虑它们。但是我们下次再做吧。</p><p id="4f6d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这两个术语都很容易找到。第一次</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mc"><img src="../Images/541cea7a918cde0b01a388aced5da55e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2b6j6_AGwZVP5Vf7dHHmMQ.png"/></div></div></figure><p id="c248" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因为这适用于最后一层中的任何神经元，所以我们可以将向量形式写为</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi md"><img src="../Images/eee3ca63c5682e1404e4da8b50cd4e7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GBKbco1-ePvisTeGjn8VzA.png"/></div></div></figure><p id="69a0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在是第二学期，我们有</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi md"><img src="../Images/7515a4e5d1fd661c3ba2deea67231c91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HsfIBjbZVHsWGicqmZbWlQ.png"/></div></div></figure><p id="9744" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在矢量形式中，它只是简单的<strong class="kh ir"><em class="lr">h’(</em>z<em class="lr">ᴴ)</em></strong><em class="lr"/>其中<strong class="kh ir">h’</strong>将按元素应用于矢量<strong class="kh ir"> z <em class="lr"> ᴴ </em> </strong> <em class="lr">。</em></p><p id="e4a8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在索引符号中结合这两者产生</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi md"><img src="../Images/f779a2049d9a21c914b8d1e8326ae6fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0WBnAWNAc1IKVB5Jgs6QEg.png"/></div></div></figure><p id="aa51" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因为它在索引符号中的样子，如果我们用ⵙ来表示两个向量的元素乘积，那么我们可以通过写来实现向量符号中的公式</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi md"><img src="../Images/61634cfe2a588c0989f0534abbfd4363.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ytXy4hYnyAQ4SzXcRsCkhA.png"/></div></div></figure><p id="e1d5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是，当我们为网络中的任何神经元寻找<strong class="kh ir">δ</strong>(<strong class="kh ir">δᴸₙ</strong>)时，这只能说明最后一层。</p><p id="d52f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因为我们知道最后一层的δ，所以使用它来找出其它层的δ的一种方法是找出当前层L的δ与下一层的δ的关系。让我们试试看。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi md"><img src="../Images/5ff5e003566d3af2ec72c8d74f176c09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JkUwMZmYKsaKvTCFb6Z7jg.png"/></div></div></figure><p id="fbcd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里我们需要求和，因为下一层中的每一个<strong class="kh ir"> zₘ </strong>都是<strong class="kh ir">zₙ</strong>(<strong class="kh ir"><em class="lr">h</em>(zₙ)</strong>的函数，对吗？).<br/>对于下一个术语，将<strong class="kh ir"> z </strong>代入其定义，考虑到<strong class="kh ir"><em class="lr">a</em>ᴸ<em class="lr">ᵢ</em></strong>只是<strong class="kh ir"> zᴸₙ </strong>中的一个函数，当<strong class="kh ir"> i = n </strong>时，我们可以写成:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi md"><img src="../Images/8f3393ba297d5e89cb88b40f539b574e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6VJX3An5USiQ3Y6zv25OfQ.png"/></div></div></figure><p id="082d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在回想一下<em class="lr"/><strong class="kh ir"><em class="lr">aᴸₙ=h(</em>z<em class="lr">ᴸₙ)</em></strong><em class="lr"/>我们可以写</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi md"><img src="../Images/05d042b11965ffc3e5c4422ea80f958f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*incl2Z8M6T1ps_pWjynTSw.png"/></div></div></figure><p id="bd73" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过代入δᴸₙ，在索引符号中我们得到</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi me"><img src="../Images/0e6aa8381170ff5b2ade17f3875d1e42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*92Flc4Or3KzFvXBLvMwQNA.png"/></div></div></figure><p id="b0d1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，我们在sum中所做的相当于权重矩阵中对应于神经元的行与整个delta向量(列)之间的点积。如果我们将权重矩阵乘以增量向量，这正是幕后发生的情况，除了因为它发生在W中的所有行，所以我们得到一个考虑所有神经元数量的向量。因为对于每个我们简单地乘以<em class="lr"> h( </em> z <em class="lr"> ᴸₙ) </em>我们可以再次利用ⵙ作为元素式乘积，并写出:</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mf"><img src="../Images/8d9d730b7f2b94ab0b8861d17a62fb47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c48s1GqdqlPa5UazohhPSA.png"/></div></div></figure><p id="6e14" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">深呼吸。证明到此结束。</p><p id="685b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了编译我们用索引符号导出的所有方程，我们可以写</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mg"><img src="../Images/beb2a5555293821e4dfc09990b5e2ed0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dIqiKSX92OZCpqKYt8wvCw.png"/></div></div></figure><p id="1d5c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">用向量符号表示(一层一层)</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mg"><img src="../Images/673401c63a4f12b7bb6d2d50aec6dfd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F2iU1e1sx8ce7P4omQshGA.png"/></div></div></figure><p id="bad7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">证明只帮助我们得出方程；算法就是雇佣他们的东西。<strong class="kh ir">它</strong> <strong class="kh ir">一次考虑一个例子，如下所示:</strong></p><p id="a278" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 1- </strong>找<strong class="kh ir"> <em class="lr"> aᴸ </em> </strong>和<strong class="kh ir"> z <em class="lr"> ᴸ </em> </strong>找层<strong class="kh ir"> 0 </strong>到<strong class="kh ir"> H </strong>通过把一个例子馈入网络。(用前两个方程。)这就是所谓的“向前传球”。</p><p id="0bcc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 2- </strong>分别用<strong class="kh ir"> δᴴ </strong>、<strong class="kh ir"> δᴸ </strong>的公式计算<strong class="kh ir"> H </strong>到<strong class="kh ir"> 0 </strong>层的<strong class="kh ir"> δᴸ </strong>。(用第三个等式。)</p><p id="1e42" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 3- </strong>同时计算层<strong class="kh ir"> H </strong>到<strong class="kh ir"> 0 </strong>的<strong class="kh ir"> ∂J/∂Wᴸ </strong>和<strong class="kh ir"> ∂J/∂bᴸ </strong>因为一旦我们有了<strong class="kh ir"> δᴸ </strong>我们就可以找到这两个。(用最后两个等式。)这就是所谓的“倒传”。</p><p id="3664" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 4- </strong>重复更多示例，直到网络的权重和偏差可以通过梯度下降进行更新(取决于您的批量):</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mg"><img src="../Images/edbc2f08ba350c4824bfdc6c27c020a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8IHg6y2vJDHOrH2ZRxmudg.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">假设d是数据集的长度。</p></figure><p id="7de9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如你可能已经注意到的，步骤2和3是这个算法的名字；为了找到参数，我们必须反向传播。在下一个故事中，我们可能会考虑用Python来实现。</p><p id="ab0e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你喜欢阅读，并希望看到更多这样的故事，那么请考虑给帖子一些掌声，并跟我来。下次见，再见。</p><p id="0df6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">参考文献:</strong></p><p id="485e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[1]“文件:内核Machine.Png—维基共享”。<em class="lr">Commons.Wikimedia.Org</em>，2011，<a class="ae ls" href="https://commons.wikimedia.org/wiki/File:Kernel_Machine.png." rel="noopener ugc nofollow" target="_blank">https://commons.wikimedia.org/wiki/File:Kernel_Machine.png.</a></p><p id="1dd4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2] Pixabay，2015，<a class="ae ls" href="https://pixabay.com/de/vectors/paraboloid-mathematik-schale-fl%c3%a4che-696804/." rel="noopener ugc nofollow" target="_blank">https://pix abay . com/de/vectors/抛物面-mathematik-schale-fl % C3 % a4che-696804/。</a>于2021年9月21日访问。</p><p id="c946" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3]“文件:English.Png多边网络——维基共享”。<em class="lr">Commons.Wikimedia.Org</em>，2010年<a class="ae ls" href="https://commons.wikimedia.org/wiki/File:MultiLayerNeuralNetworkBigger_english.png." rel="noopener ugc nofollow" target="_blank">https://commons . wikimedia . org/wiki/File:MultiLayerNeuralNetworkBigger _ English . png .</a></p><p id="646e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[4]布特，威廉。“档案:美国旧金山旧金山现代艺术博物馆(Unsplash Rkjf2bmrljc)。Jpg —维基共享”。<em class="lr">Commons.Wikimedia.Org</em>，2017，<a class="ae ls" href="https://commons.wikimedia.org/wiki/File:San_Francisco_Museum_of_Modern_Art,_San_Francisco,_United_States_(Unsplash_RkJF2BMrLJc).jpg." rel="noopener ugc nofollow" target="_blank">https://commons . wikimedia . org/wiki/File:San _ Francisco _ Museum _ of _ Modern _ Art，_San_Francisco，_ United States _(Unsplash _ rkjf 2 bmrljc). jpg .</a></p><p id="7c39" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[5]“在白板上写字的女人”。<em class="lr"> Pexels </em>，2021，<a class="ae ls" href="https://www.pexels.com/photo/woman-writing-on-a-whiteboard-3862130/." rel="noopener ugc nofollow" target="_blank">https://www . Pexels . com/photo/woman-writing-on-a-white board-3862130/。</a>于2021年9月21日访问。</p><p id="73fa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">学习资源:</strong></p><p id="875c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">尼尔森迈克尔。<em class="lr">神经网络和深度学习</em>。2019，CHP。1,2.</p></div></div>    
</body>
</html>