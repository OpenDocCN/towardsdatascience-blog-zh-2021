<html>
<head>
<title>Encoding Categorical Variables: One-hot vs Dummy Encoding</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分类变量编码:一次性编码与虚拟编码</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/encoding-categorical-variables-one-hot-vs-dummy-encoding-6d5b9c46e2db?source=collection_archive---------0-----------------------#2021-12-16">https://towardsdatascience.com/encoding-categorical-variables-one-hot-vs-dummy-encoding-6d5b9c46e2db?source=collection_archive---------0-----------------------#2021-12-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5b4d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用Pandas和Scikit实现-学习</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/92222f211e0fd67380339299c18eaadd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d5-PQyRRjvzBZjI5f7X3hA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供，用draw.io制作)</p></figure><p id="0620" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您经常会在数据集中找到分类变量。分类变量的值有有限数量的类别或标签。例如，<strong class="la iu"> <em class="lu">性别</em> </strong>是一个分类变量，可以取“男性”和“女性”为其值。</p><h1 id="316c" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">分类变量的类型</h1><p id="d650" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">分类变量主要分为两种类型:</p><ul class=""><li id="3301" class="ms mt it la b lb lc le lf lh mu ll mv lp mw lt mx my mz na bi translated"><strong class="la iu">有序分类变量:</strong>这些分类变量的值遵循自然顺序。例如，在变量<strong class="la iu"><em class="lu"/></strong>中，其“小学”、“某学院”和“研究生学历”的值遵循一个顺序，即“研究生学历”是最高的学历，“小学”是最低的。</li><li id="562e" class="ms mt it la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated"><strong class="la iu">名义分类变量:</strong>这些分类变量的值<strong class="la iu"> <em class="lu">而非</em> </strong>遵循自然顺序。例如，<strong class="la iu"> <em class="lu">性别</em> </strong>是一个名词性分类变量，其“男性”和“女性”的值不遵循顺序。</li></ul><h1 id="f717" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">什么是分类变量编码，我们为什么需要它？</h1><p id="0235" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">分类变量通常有字符串值。许多机器学习算法不支持输入变量的字符串值。因此，我们需要用数字替换这些字符串值。这个过程被称为<strong class="la iu">分类变量编码</strong>。</p><h1 id="b2f3" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">编码类型</h1><p id="c49e" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">这里，我们将讨论两种不同类型的编码:</p><ul class=""><li id="1976" class="ms mt it la b lb lc le lf lh mu ll mv lp mw lt mx my mz na bi translated">一键编码</li><li id="35bc" class="ms mt it la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated">虚拟编码</li></ul><p id="bf62" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将从一次性编码开始。</p><h2 id="4054" class="ng lw it bd lx nh ni dn mb nj nk dp mf lh nl nm mh ll nn no mj lp np nq ml nr bi translated">一键编码</h2><p id="2cf0" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">在一次性编码中，我们创建一组新的虚拟(二进制)变量，它等于变量中类别的数量(k)。例如，假设我们有一个分类变量<strong class="la iu"> <em class="lu"> Color </em> </strong>，它有三个类别，称为“红色”、“绿色”和“蓝色”，我们需要使用三个虚拟变量，使用一键编码对这个变量进行编码。虚拟(二进制)变量只取值0或1来表示排除或包含某个类别。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/50cc82b63a30485556f8236ffcc1f09e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1022/format:webp/1*lS-1YL8UfhcSFnZ5weIMdg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一键编码(图片由作者提供)</p></figure><p id="3ac3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在独热编码中，</p><ul class=""><li id="ed55" class="ms mt it la b lb lc le lf lh mu ll mv lp mw lt mx my mz na bi translated">“红色”被编码为大小为3的[1 0 0]向量。</li><li id="7064" class="ms mt it la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated">“绿色”被编码为大小为3的[0 1 0]向量。</li><li id="cd6f" class="ms mt it la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated">“蓝色”被编码为大小为3的[0 0 1]矢量。</li></ul><h2 id="04d4" class="ng lw it bd lx nh ni dn mb nj nk dp mf lh nl nm mh ll nn no mj lp np nq ml nr bi translated">虚拟编码</h2><p id="7d9e" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">虚拟编码也使用虚拟(二进制)变量。哑元编码使用k-1个哑元变量，而不是创建与变量中类别数(k)相等的多个哑元变量。要使用哑编码对三个类别的相同<strong class="la iu"> <em class="lu">颜色</em> </strong>变量进行编码，我们只需要使用两个哑变量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/67f14bb4a1622bfb80138662a7435dca.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*GlKVmFiao1DUFaxxu6Zucw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">虚拟编码(图片由作者提供)</p></figure><p id="df4c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在虚拟编码中，</p><ul class=""><li id="9795" class="ms mt it la b lb lc le lf lh mu ll mv lp mw lt mx my mz na bi translated">“红色”被编码为大小为2的[1 0]向量。</li><li id="42dd" class="ms mt it la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated">“绿色”被编码为大小为2的[0-1]向量。</li><li id="c2ce" class="ms mt it la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated">“蓝色”被编码为大小为2的[0 0]向量。</li></ul><p id="4246" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">虚拟编码消除了一次性编码中存在的重复类别。</p><h1 id="427f" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">熊猫的实施</h1><p id="01fc" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">通过使用它的<strong class="la iu"> get_dummies </strong>函数，可以在Pandas中实现一位热编码和虚拟编码。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="1009" class="ng lw it nv b gy nz oa l ob oc">import pandas as pd</span><span id="f025" class="ng lw it nv b gy od oa l ob oc">pd.<strong class="nv iu">get_dummies</strong>(<em class="lu">data</em>, <em class="lu">prefix</em>, <em class="lu">dummy_na</em>,<strong class="nv iu"> </strong><em class="lu">columns</em>, <em class="lu">drop_first</em>)</span></pre><ul class=""><li id="9424" class="ms mt it la b lb lc le lf lh mu ll mv lp mw lt mx my mz na bi translated"><strong class="la iu"> <em class="lu">数据</em> </strong> —这里我们指定需要编码的数据。它可以是NumPy数组、Pandas系列或DataFrame。</li><li id="5e48" class="ms mt it la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated"><strong class="la iu"> <em class="lu">前缀</em> </strong> —如果我们指定一个前缀，它将添加到列名中，这样我们就可以很容易地识别列。前缀可以指定为单个列名的字符串。对于多个列名，它被定义为将列名映射到前缀的字典。有关更多详细信息，请参见下面的示例。</li><li id="8823" class="ms mt it la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated"><strong class="la iu"> <em class="lu"> dummy_na </em> </strong> —如果为False(默认)，则在对变量进行编码时忽略缺失值(nan)。如果为真，这将在单独的类别中返回缺失的数据。</li><li id="1765" class="ms mt it la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated"><strong class="la iu"> <em class="lu">列</em> </strong> —指定要编码的列名。如果无(默认)，将对<strong class="la iu"> <em class="lu">数据</em> </strong>参数中的所有分类列进行编码。如果将列名指定为列表，则只对指定的列进行编码。</li><li id="78fb" class="ms mt it la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated"><strong class="la iu"> <em class="lu"> drop_first </em> </strong> —这是最重要的参数。这需要一个布尔值，真或假。如果为False(默认值)，将执行一键编码。如果为真，这将删除每个分类变量的第一个类别，为每个分类变量创建k-1个虚拟变量，并执行虚拟编码。</li></ul><p id="0337" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们使用<a class="ae oe" href="https://www.kaggle.com/shivam2503/diamonds" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu"> <em class="lu">钻石</em> </strong> </a>数据集(参见底部的源代码和许可证信息)来查看这两种类型的编码。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="2178" class="ng lw it nv b gy nz oa l ob oc">import pandas as pd</span><span id="b568" class="ng lw it nv b gy od oa l ob oc">df = pd.read_csv("diamonds.csv")<br/>df.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/b52fe9179614eb20621d700883ee14a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*2aEugEE2KN3bbqNYits-og.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">钻石数据集的前几行(图片由作者提供)</p></figure><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="593a" class="ng lw it nv b gy nz oa l ob oc">df.shape</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/b080cdd6d54417264da862289b5da91a.png" data-original-src="https://miro.medium.com/v2/resize:fit:286/format:webp/1*RN1JMrnFNfBwxRwYr7selw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="97fc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">数据集包含53，940个实例和10个变量。</p><p id="f39f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们看看数据集中是否有丢失的值。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="8762" class="ng lw it nv b gy nz oa l ob oc">df.isnull().sum().sum()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/01cf44d3e85269391db324d1d6e9bd9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:102/format:webp/1*7bmTx0xSFE9rmz-eptenzg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="bd79" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因为这会返回0，所以数据集中没有丢失的值。</p><p id="cd4e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们看看数据集中有多少分类变量。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="cabf" class="ng lw it nv b gy nz oa l ob oc">df.info()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/341a84ef6f91ba4597f80362745a0887.png" data-original-src="https://miro.medium.com/v2/resize:fit:634/format:webp/1*KCCiinrihyNXJI9ccI6hvQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">钻石数据集变量信息(图片由作者提供)</p></figure><p id="9f34" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">分类变量有<strong class="la iu"> <em class="lu">对象</em> </strong>或<strong class="la iu"> <em class="lu">类别</em> </strong>数据类型。因此，数据集中有3个分类变量。它们是<strong class="la iu">切割</strong>、<strong class="la iu">颜色</strong>和<strong class="la iu">净度</strong>。</p><p id="3a4b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们看看<strong class="la iu">切割</strong>变量的独特类别或标签。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="cae5" class="ng lw it nv b gy nz oa l ob oc">df["cut"].unique()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/f544474962102bd308864112308988af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*wUe5COBB24d8GtQXuEM1Ew.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ok">切割</strong>变量的独特类别(图片由作者提供)</p></figure><p id="1c32" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在<strong class="la iu">切割</strong>变量中有5个独特的类别。为了对这个变量进行编码，我们需要在一键编码中创建5个伪变量，在伪编码中创建4个伪变量。</p><p id="6ecf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">同样地，<strong class="la iu">颜色</strong>和<strong class="la iu">清晰度</strong>变量也有独特的类别。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="4002" class="ng lw it nv b gy nz oa l ob oc">df["color"].unique()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/4d7706b8cf53c707afbcb3f09fe6bc2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*gkipu3XDUfyziR5e5Pvfeg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ok">颜色</strong>变量的独特类别(图片由作者提供)</p></figure><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="592a" class="ng lw it nv b gy nz oa l ob oc">df["clarity"].unique()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/51857d3fecad5fcba63f4411eeec77e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1102/format:webp/1*Xv07m8HP7VWAcD8aYS-5Ew.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ok">透明度</strong>变量的独特类别(图片由作者提供)</p></figure><h2 id="4ae7" class="ng lw it bd lx nh ni dn mb nj nk dp mf lh nl nm mh ll nn no mj lp np nq ml nr bi translated">用Pandas实现一键编码</h2><p id="6630" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">现在，我们单独对<strong class="la iu">颜色</strong>变量应用一键编码，看看结果。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="9e2f" class="ng lw it nv b gy nz oa l ob oc">one_hot = pd.get_dummies(df["color"],<br/>                         prefix="color",<br/>                         <strong class="nv iu">drop_first=False</strong>)</span><span id="f488" class="ng lw it nv b gy od oa l ob oc">one_hot</span></pre><p id="94a4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这将返回编码数据的熊猫数据帧。让我们看看它的前几行。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/ad4b8bfd214b47a016ae209e2640dab7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qgCEB9PJKEppMqv39cLvBA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="0fba" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">查看前缀参数中指定的文本是如何与<strong class="la iu">颜色</strong>的类别名称组合在一起的。</p><p id="dd12" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们将<strong class="la iu">颜色</strong>变量的一键编码添加到数据集中。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="bd0d" class="ng lw it nv b gy nz oa l ob oc">one_hot_df = pd.get_dummies(df, prefix="color", <br/>                            columns=["color"], <br/>                            <strong class="nv iu">drop_first=False</strong>)</span><span id="5cbc" class="ng lw it nv b gy od oa l ob oc">one_hot_df.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/c33a34126486c8ef454d0a4a1ee82bb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AbJtI2H57jtkSOzQzQIQUw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="9c4f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们对数据集中的所有分类变量应用一次性编码。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="407d" class="ng lw it nv b gy nz oa l ob oc">one_hot_df = pd.get_dummies(df, prefix={'color':'color',<br/>                                        'cut':'cut',<br/>                                        'clarity':'clarity'},<br/>                            <strong class="nv iu">drop_first=False</strong>)</span></pre><p id="c1a4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们看到了编码数据集的形状。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="18cb" class="ng lw it nv b gy nz oa l ob oc">one_hot_df.shape</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/25c36b1cc7ac7fbec7f998d5d96b0461.png" data-original-src="https://miro.medium.com/v2/resize:fit:242/format:webp/1*6xTuM0lmJR-9Q5bh4Lqgpw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="1c70" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">编码数据集有27个变量。这是因为在对分类变量进行编码时，一键编码增加了20个额外的虚拟变量。因此，一键编码扩展了数据集中的特征空间(维度)。</p><h2 id="809f" class="ng lw it bd lx nh ni dn mb nj nk dp mf lh nl nm mh ll nn no mj lp np nq ml nr bi translated">用Pandas实现虚拟编码</h2><p id="3e3c" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">要对数据实现虚拟编码，您可以遵循在一键编码中执行的相同步骤。唯一不同的是，你应该将<strong class="la iu"> <em class="lu"> drop_first </em> </strong>参数设置为True而不是False。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="d96a" class="ng lw it nv b gy nz oa l ob oc">dummy_df = pd.get_dummies(df, prefix={'color':'color', <br/>                                      'cut':'cut',<br/>                                      'clarity':'clarity'},<br/>                          <strong class="nv iu">drop_first=True</strong>)</span></pre><p id="845d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们看到了编码数据集的形状。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="256a" class="ng lw it nv b gy nz oa l ob oc">dummy_df.shape</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/bfe2ebb37f18b70454317324b194fa3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:252/format:webp/1*dE197Cz8eD9JH_8Bjvb-xA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="cb73" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">编码数据集有24个变量。这是因为哑编码在对分类变量进行编码时增加了17个额外的哑变量。因此，虚拟编码也扩展了数据集中的特征空间(维度)。</p><h1 id="90f4" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">用Scikit-learn实现</h1><p id="67fb" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">通过使用Scikit-learn的<strong class="la iu"> OneHotEncoder </strong>函数，可以在Scikit-learn中实现单热编码和伪编码。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="e4c3" class="ng lw it nv b gy nz oa l ob oc">from sklearn.preprocessing import OneHotEncoder</span><span id="aab0" class="ng lw it nv b gy od oa l ob oc">ohe = <strong class="nv iu">OneHotEncoder</strong>(<em class="lu">categories</em>, <em class="lu">drop</em>, <em class="lu">sparse</em>)</span><span id="8bf4" class="ng lw it nv b gy od oa l ob oc">encoded_data = ohe.fit_transform(original_data)<br/>#Returns a NumPy array of encoded data</span></pre><ul class=""><li id="9e43" class="ms mt it la b lb lc le lf lh mu ll mv lp mw lt mx my mz na bi translated"><strong class="la iu"> <em class="lu">类别</em> </strong> —默认为<strong class="la iu">【自动】<em class="lu"> </em> </strong>自动确定每个变量中的类别。</li><li id="5278" class="ms mt it la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated"><strong class="la iu"> <em class="lu">删除</em> </strong> —默认为无，执行一键编码。要执行虚拟编码，将该参数设置为<strong class="la iu">‘first’</strong>，这将删除每个变量的第一个类别。</li><li id="e57c" class="ms mt it la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated"><strong class="la iu"> <em class="lu">稀疏</em> </strong> —设置为False以NumPy数组的形式返回输出。默认值为True，这将返回一个稀疏矩阵。</li></ul><h2 id="ade6" class="ng lw it bd lx nh ni dn mb nj nk dp mf lh nl nm mh ll nn no mj lp np nq ml nr bi translated">用Scikit-learn实现一键编码</h2><p id="8a1b" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">这里，我们也使用相同的<strong class="la iu"> <em class="lu">钻石</em> </strong>数据集。我们对数据集中的所有分类变量应用一次性编码。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="d6a6" class="ng lw it nv b gy nz oa l ob oc">from sklearn.preprocessing import OneHotEncoder</span><span id="8c61" class="ng lw it nv b gy od oa l ob oc">ohe = OneHotEncoder(categories='auto', <br/>                    drop=None,sparse=False)</span><span id="d862" class="ng lw it nv b gy od oa l ob oc">ohe_df = pd.DataFrame(ohe.fit_transform(df)</span></pre><p id="0a94" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们看到了编码数据集的形状。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/f55e220b725ebd61b82c172d49b6ebc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:294/format:webp/1*qW4HXxqojc_MulenUzCMIQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="44f1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">哦。什么？编码数据集有13，687个变量。这可能发生吗？是的，这是因为Scikit-learn的<strong class="la iu"> OneHotEncoder() </strong>函数也为数字变量创建了虚拟变量。为了避免这种情况，我们应该在调用它的<strong class="la iu"> fit_transform </strong>方法时只传递分类数据。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="4a4a" class="ng lw it nv b gy nz oa l ob oc">ohe_df = pd.DataFrame(ohe.fit_transform(df[['cut', <br/>                                            'color', 'clarity']]))</span></pre><p id="f75d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是，这仅返回编码的分类数据，而不是包含数值变量的整个数据集。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/abf56f3f1cffdd420adc7cafd2b37118.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7h4KKQ_clkns02j4XBoksA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><h2 id="05cc" class="ng lw it bd lx nh ni dn mb nj nk dp mf lh nl nm mh ll nn no mj lp np nq ml nr bi translated">用Scikit-learn实现虚拟编码</h2><p id="29de" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">您可以简单地通过将<strong class="la iu"> <em class="lu">下降</em> </strong>参数指定为<strong class="la iu">‘第一’</strong>来实现。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="1406" class="ng lw it nv b gy nz oa l ob oc">ohe = OneHotEncoder(categories='auto', <br/>                    drop='first',sparse=False)</span></pre><h1 id="bdfc" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">何时使用独热编码和虚拟编码</h1><p id="b109" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">这两种类型的编码都可以用来编码序数和名义分类变量。然而，如果您严格地想要保持有序分类变量的自然顺序，您可以使用<strong class="la iu">标签编码</strong>来代替我们上面讨论的两种编码。</p><p id="f287" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">假设我们有一个分类变量<strong class="la iu"> <em class="lu">质量</em> </strong>，有三个类别，称为“一般”、“良好”和“优质”。这些类别的自然顺序是:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="4f2d" class="ng lw it nv b gy nz oa l ob oc">Fair &lt; Good &lt; Premium =&gt; 0 &lt; 1 &lt; 2</span></pre><p id="1573" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以这样编码。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/321b9f3aa1e0802d9925ba5abf10e908.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/1*pqcgdgmUBS5Dmdsg7OxMDg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">标签编码(图片由作者提供)</p></figure><p id="ba9e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">标签编码的一个优点是，它根本不扩展特征空间，因为我们只是用数字替换类别名称。这里，我们不使用虚拟变量。</p><p id="2d9a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">标签编码的主要缺点是机器学习算法可能会认为编码类别之间可能存在关系。例如，某个算法可能会将Premium (2)解释为Good (1)的两倍。实际上，类别之间没有这种关系。</p><p id="cd01" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了避免这种情况，标签编码应该只应用于目标(y)值，而不是输入(X)值。</p><p id="d4c5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">可以通过使用Scikit-learn的<strong class="la iu"> LabelEncoder </strong>函数来应用标签编码。现在，我们将它应用于钻石数据集中的<strong class="la iu"/><strong class="la iu">切割</strong>变量。这仅用于说明目的，因为我们不使用标签编码来编码输入(X)值。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="6c9f" class="ng lw it nv b gy nz oa l ob oc">from sklearn.preprocessing import LabelEncoder</span><span id="2bfc" class="ng lw it nv b gy od oa l ob oc">df['cut_enc'] = LabelEncoder().fit_transform(df['cut'])<br/>df.head(10)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/5887eae81a34fbc43d787e6843afda64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*mPOWSTQ08_IMaVnf1q-uMQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="1d34" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">右边添加了新的编码数据列(cut_enc)。我们现在可以移除<strong class="la iu">切割</strong>变量。</p><h1 id="7d2b" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">Pandas get_dummies()函数相对于Scikit-learn onehotencode()函数的优势</h1><ul class=""><li id="9216" class="ms mt it la b lb mn le mo lh ov ll ow lp ox lt mx my mz na bi translated"><strong class="la iu"> get_dummies() </strong>函数返回带有变量名的编码数据。我们还可以在每个分类变量名称中为虚拟变量添加前缀。</li><li id="07a0" class="ms mt it la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated"><strong class="la iu"> get_dummies() </strong>函数返回包含数字变量的整个数据集。</li></ul><h1 id="cd24" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">伪编码相对于一位热码编码的优势</h1><ul class=""><li id="3b7b" class="ms mt it la b lb mn le mo lh ov ll ow lp ox lt mx my mz na bi translated">两者都通过添加虚拟变量来扩展数据集中的特征空间(维度)。然而，哑元编码比一位热码编码添加更少的哑元变量。</li><li id="6d7e" class="ms mt it la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated">虚拟编码删除了每个分类变量中的重复类别。这避免了虚拟变量陷阱。</li></ul></div><div class="ab cl oy oz hx pa" role="separator"><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd pe"/><span class="pb bw bk pc pd"/></div><div class="im in io ip iq"><p id="8196" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">今天的帖子到此结束。</p><p id="fd9e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您有任何反馈，请告诉我。</p><p id="4b34" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">同时，你可以<a class="ae oe" href="https://rukshanpramoditha.medium.com/membership" rel="noopener"> <strong class="la iu">注册成为会员</strong> </a>来获得我写的每一个故事，我会收到你的一部分会员费。</p><p id="af4c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">非常感谢你一直以来的支持！下一个故事再见。祝大家学习愉快！</p><p id="9003" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">钻石数据集来源:</strong></p><p id="0f87" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> <em class="lu">钻石</em> </strong>数据集可在<a class="ae oe" href="https://www.kaggle.com/shivam2503/diamonds" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/shivam2503/diamonds</a>下载</p><p id="1422" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">钻石数据集许可证:</strong></p><p id="765f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> <em class="lu">钻石</em> </strong>数据集由公共领域许可证授权，如<a class="ae oe" href="https://www.kaggle.com/general/116302" rel="noopener ugc nofollow" target="_blank">此处</a>所定义。这意味着数据集已经捐赠给公众。</p><p id="f6ce" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="pf pg ep" href="https://medium.com/u/f90a3bb1d400?source=post_page-----6d5b9c46e2db--------------------------------" rel="noopener" target="_blank">鲁克山普拉莫迪塔</a><br/><strong class="la iu">2021–12–16</strong></p></div></div>    
</body>
</html>