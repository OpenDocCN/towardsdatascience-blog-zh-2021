<html>
<head>
<title>Comparing Keras and PyTorch on sentiment classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Keras和PyTorch在情感分类上的比较</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/comparing-keras-and-pytorch-on-sentiment-classification-20041734d131?source=collection_archive---------25-----------------------#2021-03-20">https://towardsdatascience.com/comparing-keras-and-pytorch-on-sentiment-classification-20041734d131?source=collection_archive---------25-----------------------#2021-03-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="64a9" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">IMDB电影评论的实用比较</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/8b6b04c8e2222e4e66972168ce4f291d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9cDVTrYkyDP2f_v2_XvA_Q.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://www.pexels.com/@karolina-grabowska?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> <strong class="bd kw">卡罗琳娜·格拉博斯卡</strong> </a>发自<a class="ae kv" href="https://www.pexels.com/photo/coffee-in-different-cups-on-small-table-4195598/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> <strong class="bd kw">佩克斯</strong> </a></p></figure><p id="6330" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在第一部分概述了Keras和PyTorch语法之后，这是我们比较Keras和PyTorch的第二部分！这一部分更实用，因为我们将实现一个神经网络来按情感对电影评论进行分类，并比较分类结果。</p><div class="lt lu gp gr lv lw"><a rel="noopener follow" target="_blank" href="/comparing-keras-and-pytorch-syntaxes-54b164268466"><div class="lx ab fo"><div class="ly ab lz cl cj ma"><h2 class="bd ir gy z fp mb fr fs mc fu fw ip bi translated">比较Keras和PyTorch语法</h2><div class="md l"><h3 class="bd b gy z fp mb fr fs mc fu fw dk translated">数据加载、模型定义和张量操作</h3></div><div class="me l"><p class="bd b dl z fp mb fr fs mc fu fw dk translated">towardsdatascience.com</p></div></div><div class="mf l"><div class="mg l mh mi mj mf mk kp lw"/></div></div></a></div><p id="4a20" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">正如我们在第一部分中看到的，Keras和PyTorch在设计上是不同的</p><ul class=""><li id="4621" class="ml mm iq kz b la lb ld le lg mn lk mo lo mp ls mq mr ms mt bi translated">Keras的目标是快速原型制作。它旨在编写更少的代码，让开发人员专注于其他任务，如数据准备、处理、清理等</li><li id="e075" class="ml mm iq kz b la mu ld mv lg mw lk mx lo my ls mq mr ms mt bi translated">PyTorch的目标是模块化和多功能性。对计算流程的细粒度控制</li></ul><p id="9901" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">然而，这两个框架的目的是一样的，那就是训练深度神经网络。“罗塞塔石碑”风格教程的资源在网上的数量非常有限，尤其是演示实际例子的资源，所以我希望这有助于揭开两个框架之间的区别和相似之处。现在让我们开始吧！</p><h1 id="daf0" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated">IMDB电影评论数据集</h1><p id="9298" class="pw-post-body-paragraph kx ky iq kz b la nr jr lc ld ns ju lf lg nt li lj lk nu lm ln lo nv lq lr ls ij bi translated">我们将使用IMDB数据集，这是机器学习中一个受欢迎的玩具数据集，由来自IMDB网站的电影评论组成，由正面或负面情绪注释。</p><p id="f0e2" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">Keras和PyTorch都有助手函数来下载和加载IMDB数据集。因此，我们将使用这段代码在Keras中导入IMDB数据集，而不是像我们在<a class="ae kv" rel="noopener" target="_blank" href="/comparing-keras-and-pytorch-syntaxes-54b164268466">第一部分</a>中看到的那样使用数据加载器。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="cc40" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这将创建一个文件夹<code class="fe ny nz oa ob b">data/</code>，并在其中下载数据集。它将数据分成训练集和验证集两部分。输出给出了序列的数量。</p><pre class="kg kh ki kj gt oc ob od oe aw of bi"><span id="e05a" class="og na iq ob b gy oh oi l oj ok">25000 Training sequences<br/>25000 Validation sequences</span></pre><p id="bbe8" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">使用<a class="ae kv" href="https://pytorch.org/text/_modules/torchtext/datasets/imdb.html" rel="noopener ugc nofollow" target="_blank"> torchtext，PyTorch也可以以同样的方式下载和加载IMDB数据集</a>。但是为了确保我们提供给Keras的数据和我们提供给PyTorch的数据是相同的，我将简单地修改从Keras下载到PyTorch的数据，这样我们就可以确保两个实现之间的一致性，特别是在数据清理和文本标记化方面。</p><p id="f1a1" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">此外，我们将填补电影评论有相同长度的序列。这对于批量训练很重要，因为不可能并行计算可变长度序列的误差梯度。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="338e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我们将评论的长度固定为200个单词，将词汇表固定为20k个不同的单词。数据集已经被清理和标记，所以我们可以直接把它输入神经网络！在继续之前，出于好奇，我们可以看一下数据集中的一些例子。我们需要使用<code class="fe ny nz oa ob b">imdb.get_word_index</code>获取单词词汇表，这将让我们把数据集中的整数解码成简单的英语。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="599e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">使用上面的代码，我们可以检查数据集中的序列“1，13，165，219，14，20，33，6，750，…”对应于</p><pre class="kg kh ki kj gt oc ob od oe aw of bi"><span id="5dcd" class="og na iq ob b gy oh oi l oj ok">&lt;START&gt; i actually saw this movie at a theater as soon as i handed the cashier my money she said two words i had never heard at a theater before or since no &lt;UNK&gt; as soon as i heard those words i should have just &lt;UNK&gt; bye bye to my cash and gone home but no foolishly i went in and watched the movie this movie didn't make anyone in the theater laugh not even once not even &lt;UNK&gt; mostly we sat there in stunned silence every ten minutes or so someone would yell this movie sucks the audience would applaud enthusiastically then sit there in stunned bored silence for another ten minutes &lt;PAD&gt; &lt;PAD&gt; &lt;PAD&gt; [...]</span></pre><p id="0486" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我们可以打印与此评论相关的情绪，即<code class="fe ny nz oa ob b">y_train[50]</code>，0表示“负面”。看起来没错。</p><p id="7e94" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">但是等等！我们需要修改PyTorch的数据。这段代码将完成这项工作。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="8e8f" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><code class="fe ny nz oa ob b">TensorDataset</code>类将把我们的数据转换成torch tensors，切片成批，然后洗牌。然后使用一个<code class="fe ny nz oa ob b">DataLoader</code>实例，我们将能够迭代这些批次。</p><p id="aba0" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">很好！数据准备好了，让我们定义我们的分类器。</p><h1 id="b8e7" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated">分类器模型</h1><p id="1ad5" class="pw-post-body-paragraph kx ky iq kz b la nr jr lc ld ns ju lf lg nt li lj lk nu lm ln lo nv lq lr ls ij bi translated">我们的分类器是一个在嵌入层之上的双向两层LSTM，后面是一个给出一个输出值的密集层。这个输出值给出了一个评论是正面的概率。越接近零，评论被预测为越负面，越接近一，评论被预测为越正面。为了获得这样的概率，我们使用sigmoid函数，该函数获取神经网络的最终输出，并将该值压缩在0和1之间。</p><p id="9277" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我们将定义以下两个超参数，以确保两个实现匹配。</p><pre class="kg kh ki kj gt oc ob od oe aw of bi"><span id="29d9" class="og na iq ob b gy oh oi l oj ok">embedding_dim = 128<br/>hidden_dim = 64</span></pre><p id="799b" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">让我们从Keras中的实现开始(<a class="ae kv" href="https://keras.io/examples/nlp/bidirectional_lstm_imdb/" rel="noopener ugc nofollow" target="_blank">归功于官方的Keras文档</a>)</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="5b55" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">现在，让我们在PyTorch中实现同样的功能。坚持住，因为有一些技术上的微妙之处需要注意。我想先向您展示什么是幼稚的实现</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="b832" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">如果你读了<a class="ae kv" rel="noopener" target="_blank" href="/comparing-keras-and-pytorch-syntaxes-54b164268466">第一部分，</a>这段代码就不会有什么惊喜了。在PyTorch中，我们简单地详述了一些计算，而Keras在我们堆叠层时推断出它们。</p><p id="a8b5" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">现在坏消息是这个实现与Keras的实现并不完全相同。而且主要区别是初始化！要让两个框架在初始化上达成一致，需要做大量的工作。例如，您可以看到Keras的<code class="fe ny nz oa ob b">Embedding</code>层<a class="ae kv" href="https://keras.io/api/layers/core_layers/embedding/" rel="noopener ugc nofollow" target="_blank">使用均匀分布</a>初始化其权重，而PyTorch 使用的是正态分布<a class="ae kv" href="https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html" rel="noopener ugc nofollow" target="_blank">。让我们开门见山，使用下面的方法使PyTorch初始化适应Keras。</a></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="4c86" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这是取自<a class="ae kv" href="https://medium.com/huggingface/understanding-emotions-from-keras-to-pytorch-3ccb61d5a983" rel="noopener"> torchMoji的一个很好的片段，它是HuggingFace </a>对DeepMoji的PyTorch实现。我们现在在PyTorch中有了更新的BiLSTM类。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="e50a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">嗯……正如你所看到的，即使对于非常基础的架构，转移代码也不是一件容易的工作。甚至细节对于实现类似的结果也很重要，但是请耐心等待，因为这是我能得到的最接近Keras实现的结果。</p><p id="ba44" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">一个快速的完整性检查是验证两个实现的参数数量是否匹配。<code class="fe ny nz oa ob b">model.summary()</code>在Keras中输出以下内容。</p><pre class="kg kh ki kj gt oc ob od oe aw of bi"><span id="217f" class="og na iq ob b gy oh oi l oj ok">Model: "sequential" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding (Embedding) (None, None, 128) 2560000 _________________________________________________________________ bidirectional (Bidirectional (None, None, 128) 98816 _________________________________________________________________ bidirectional_1 (Bidirection (None, 128) 98816 _________________________________________________________________ dense (Dense) (None, 1) 129 ================================================================= Total params: 2,757,761 Trainable params: 2,757,761 Non-trainable params: 0 _________________________________________________________________</span></pre><p id="da34" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">用PyTorch打印带有<code class="fe ny nz oa ob b">print(model)</code>的模型表明我们有相同的架构。</p><pre class="kg kh ki kj gt oc ob od oe aw of bi"><span id="fcd8" class="og na iq ob b gy oh oi l oj ok">BiLSTM(<br/>  (encoder): Embedding(20000, 128)<br/>  (lstm): LSTM(128, 64, num_layers=2, bidirectional=True)<br/>  (linear): Linear(in_features=128, out_features=1, bias=True)<br/>  (activation): Sigmoid()<br/>)</span></pre><p id="d131" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">但是这并没有告诉我们参数的数量。这一行给出了我们想要的(<a class="ae kv" href="https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325/8" rel="noopener ugc nofollow" target="_blank">感谢费德里科·巴尔达萨尔</a></p><pre class="kg kh ki kj gt oc ob od oe aw of bi"><span id="46cf" class="og na iq ob b gy oh oi l oj ok">sum(p.numel() for p in model.parameters() if p.requires_grad)</span></pre><p id="cdd9" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这给出了2758785个参数。哦！参数个数不一样！？在深入研究这个问题后，我发现PyTorch有第二个偏差张量<a class="ae kv" href="https://discuss.pytorch.org/t/why-lstm-has-two-bias-parameters/45025" rel="noopener ugc nofollow" target="_blank">，这是因为它在实现LSTM </a>时与CuDNN兼容。你可以有效地检查计数差是否等于<code class="fe ny nz oa ob b">2,758,785 - 2,757,761 = 1024</code>，也就是<code class="fe ny nz oa ob b">2 * 4 * 2 * 64 = 1024</code>。其中，我们将一个LSTM单元的偏置向量的大小64乘以2，因为我们有一个前向和一个后向LSTM，再次乘以2，因为我们使用了两个层，而4是LSTM定义中的门的数量。即使有这样的基础架构，也会出现微妙而重要的差异。</p><p id="9229" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">现在让我们来看看实现是如何执行的。</p><h1 id="7044" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated">培养</h1><p id="e31e" class="pw-post-body-paragraph kx ky iq kz b la nr jr lc ld ns ju lf lg nt li lj lk nu lm ln lo nv lq lr ls ij bi translated">我们将使用以下参数进行培训。</p><pre class="kg kh ki kj gt oc ob od oe aw of bi"><span id="d8ad" class="og na iq ob b gy oh oi l oj ok">batch_size=32<br/>epochs = 2</span></pre><p id="06f0" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在喀拉斯，这个一行程序将完成这项工作</p><pre class="kg kh ki kj gt oc ob od oe aw of bi"><span id="684b" class="og na iq ob b gy oh oi l oj ok">model.fit(x=x_train, y=y_train,<br/>          batch_size=batch_size, epochs=epochs,<br/>          validation_data=(x_val, y_val))</span></pre><p id="12b7" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">现在使用PyTorch，记住Keras在幕后做的每一件事都必须显式编写。首先是初始化日志、损失标准、模型、设备和优化器。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="77c0" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">接下来是训练循环。我使用我推荐的<code class="fe ny nz oa ob b"><a class="ae kv" rel="noopener" target="_blank" href="/training-models-with-a-progress-a-bar-2b664de3e13e">tqdm</a></code> <a class="ae kv" rel="noopener" target="_blank" href="/training-models-with-a-progress-a-bar-2b664de3e13e">库来打印一个漂亮的进度条，和Keras </a>一样。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="4112" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">注意<code class="fe ny nz oa ob b">model.train()</code>和<code class="fe ny nz oa ob b">model.eval()</code>方法的使用，它们将<code class="fe ny nz oa ob b">model.training</code>设置为<code class="fe ny nz oa ob b">True</code>或<code class="fe ny nz oa ob b">False</code>。它决定是否在正向过程中保持梯度，这将用于在反向过程中优化网络。</p><p id="19d0" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我用<code class="fe ny nz oa ob b">torch.nn.utils.clip_grad_norm_(model.parameters(), 5)</code>将权重渐变剪辑为5。我不确定Keras是如何处理渐变裁剪的，但是实现这种技术来避免渐变爆炸的问题通常是很好的实践。</p><p id="5b09" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">最后一点是关于维度的排序。正如我在<a class="ae kv" rel="noopener" target="_blank" href="/comparing-keras-and-pytorch-syntaxes-54b164268466">第一部分</a>中所解释的，Keras默认期望批处理维在第一位，而PyTorch期望它在第二位。因此，通过<code class="fe ny nz oa ob b">samples.transpose(0, 1)</code>，我们有效地置换了第一个和第二个维度，以适应PyTorch数据模型。</p><p id="91ef" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">现在，同样地，Keras，当我们到达一个时代的终点，我们想要评估这个模型。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nw nx l"/></div></figure><h1 id="9605" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated">结果</h1><p id="3aef" class="pw-post-body-paragraph kx ky iq kz b la nr jr lc ld ns ju lf lg nt li lj lk nu lm ln lo nv lq lr ls ij bi translated">在下面的表中，我报告了每个训练时期的结果，是批次数的平均值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/7e50354b0aa32b7b1bc8bc0edcb57f2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OBiXpAdyo0IaxtWqCM3-YA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">训练集上的历元结果。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/0aecc01867ed3931f4405abf3cf3ac45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X9ibwC6pcswmzxjQKo-mqw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">验证集上的纪元结果。</p></figure><p id="f7a0" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">下图显示了训练期间批次的损失和二进制精度。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/6ccfda17714227e78a9b556f2d79f249.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4BTAQUYIS93jtGYr7Mou_A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">批量训练损失</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/316fb5aea33bd3574fd582bd35853b59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uqpwfPiPCPUiClvUEKjlqg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">批量训练准确度</p></figure><p id="e021" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我们可以注意到Keras实现的准确性和损失的突然增加。如果我们优化学习速度，这可能会减少。</p><h1 id="81b9" class="mz na iq bd nb nc nd ne nf ng nh ni nj jw nk jx nl jz nm ka nn kc no kd np nq bi translated">临终遗言</h1><p id="2d4a" class="pw-post-body-paragraph kx ky iq kz b la nr jr lc ld ns ju lf lg nt li lj lk nu lm ln lo nv lq lr ls ij bi translated">即使非常小心，使用相同的架构、数据处理和训练也很难获得完全相同的结果，因为每个框架都有自己的内部机制、设计和特定的技巧来保证数值稳定性。尽管如此，我们得到的结果是接近的。</p><p id="f374" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">有一些雄心勃勃的项目，如<a class="ae kv" href="https://onnx.ai/" rel="noopener ugc nofollow" target="_blank"> ONNX </a>创建深度学习框架之间的标准，以实际解决我们在本文中看到的差异。</p><p id="d47b" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我希望你喜欢Keras和PyTorch的比较，我想看看你的想法和反馈！祝你在机器学习的旅途中一切顺利！</p></div><div class="ab cl on oo hu op" role="separator"><span class="oq bw bk or os ot"/><span class="oq bw bk or os ot"/><span class="oq bw bk or os"/></div><div class="ij ik il im in"><p id="e39d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><em class="ou">原载于2021年3月20日</em><a class="ae kv" href="https://adamoudad.github.io/posts/keras_torch_comparison/sentiment_classification/" rel="noopener ugc nofollow" target="_blank"><em class="ou">https://adamoudad . github . io</em></a><em class="ou">。</em></p></div></div>    
</body>
</html>