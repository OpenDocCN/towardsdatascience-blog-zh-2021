<html>
<head>
<title>Boosted Embeddings with Catboost</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Catboost增强嵌入</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/boosted-embeddings-with-catboost-8dc15e18fb9a?source=collection_archive---------11-----------------------#2021-08-26">https://towardsdatascience.com/boosted-embeddings-with-catboost-8dc15e18fb9a?source=collection_archive---------11-----------------------#2021-08-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="3093" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">本文深入探讨了Catboost，这是一种简单且鲜为人知的方法，可以将嵌入与梯度增强模型结合使用。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/daeaa6d5368c64620fad34c4cb986fb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d9E-PQN_iAYO7cYfRaEsXQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://unsplash.com/@rvignes?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Romain Vignes </a>在<a class="ae kv" href="https://unsplash.com/s/photos/text-book?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="aee0" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">介绍</h1><p id="a49c" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">当处理大数据时，有必要将具有特征的空间压缩成矢量。文本嵌入就是一个例子，它是几乎所有NLP模型创建过程中不可或缺的一部分。不幸的是，使用神经网络来处理这种类型的数据并不总是可行的，例如，原因可能是拟合率或推断率较低。</p><p id="b24f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我建议一个很有趣的方法来使用梯度增强，很少有人知道。</p><h1 id="2917" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">数据</h1><p id="1464" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">Kaggle的竞赛之一<a class="ae kv" href="https://www.kaggle.com/c/commonlitreadabilityprize/overview" rel="noopener ugc nofollow" target="_blank">最近结束了，一个带有文本数据的小数据集出现在那里。我决定将这些数据用于实验，因为竞赛表明数据集被很好地标记，并且我没有面临任何令人不愉快的意外。</a></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/7f326da6e4a9a73f1cea095edcf3e458.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bCSN8-8oQtbhgliXCSgcjA.png"/></div></div></figure><p id="ba50" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">列:</p><ul class=""><li id="62ea" class="mq mr iq lq b lr mk lu ml lx ms mb mt mf mu mj mv mw mx my bi translated"><code class="fe mz na nb nc b">id</code> -摘录的唯一ID</li><li id="4b5d" class="mq mr iq lq b lr nd lu ne lx nf mb ng mf nh mj mv mw mx my bi translated"><code class="fe mz na nb nc b">url_legal</code> -来源的URL</li><li id="46ba" class="mq mr iq lq b lr nd lu ne lx nf mb ng mf nh mj mv mw mx my bi translated"><code class="fe mz na nb nc b">license</code> -源材料许可证</li><li id="b35d" class="mq mr iq lq b lr nd lu ne lx nf mb ng mf nh mj mv mw mx my bi translated"><code class="fe mz na nb nc b">excerpt</code> -文本预测阅读的难易程度</li><li id="d208" class="mq mr iq lq b lr nd lu ne lx nf mb ng mf nh mj mv mw mx my bi translated"><code class="fe mz na nb nc b">target</code>——读书容易</li><li id="51ff" class="mq mr iq lq b lr nd lu ne lx nf mb ng mf nh mj mv mw mx my bi translated"><code class="fe mz na nb nc b">standard_error</code> -对每个摘录在多个评价人之间的分数分布的测量</li></ul><p id="c20a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">作为数据集中的一个目标，它是一个数值型变量，提出它是为了解决回归问题。但是，我决定换成一个分类问题。主要原因是我将使用的库不支持在回归问题中处理文本和嵌入。希望以后开发者消除这个不足。但是在任何情况下，回归和分类问题都是密切相关的，对于分析来说，解决哪一个问题都没有区别。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/35d314b457c86b056038b99d668c5060.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tTkhUs4-8yXvvR4AYyD4wQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">目标</p></figure><p id="243f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们用斯特奇法则来计算箱子的数量:</p><pre class="kg kh ki kj gt nj nc nk nl aw nm bi"><span id="ceb1" class="nn kx iq nc b gy no np l nq nr">num_bins = int(np.floor(1 + np.log2(len(train))))</span><span id="4f3c" class="nn kx iq nc b gy ns np l nq nr">train['target_q'], bin_edges = pd.qcut(train['target'],<br/>    q=num_bins, labels=False, retbins=True, precision=0)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/eeb1887ab312cd10ecd891958bddb282.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*roNOqP90d70G2obOOuA_QA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">目标_q</p></figure><p id="9482" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">但是，首先，我清理数据。</p><pre class="kg kh ki kj gt nj nc nk nl aw nm bi"><span id="348c" class="nn kx iq nc b gy no np l nq nr">train['license'] = train['license'].fillna('nan')<br/>train['license'] = train['license'].astype('category').cat.codes</span></pre><p id="c204" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">借助一个自写的小函数，我对文字进行清理和词条化。函数可以很复杂，但这对于我的实验来说已经足够了。</p><pre class="kg kh ki kj gt nj nc nk nl aw nm bi"><span id="356a" class="nn kx iq nc b gy no np l nq nr">def clean_text(text):<br/>    <br/>    table = text.maketrans(<br/>        dict.fromkeys(string.punctuation))<br/>    <br/>    words = word_tokenize(<br/>        text.lower().strip().translate(table))<br/>    words = [word for word in words if word not in stopwords.words('english')]<br/>    lemmed = [WordNetLemmatizer().lemmatize(word) for word in words]    <br/>    return " ".join(lemmed)</span></pre><p id="435f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我将清理后的文本保存为新特征。</p><pre class="kg kh ki kj gt nj nc nk nl aw nm bi"><span id="597d" class="nn kx iq nc b gy no np l nq nr">train['clean_excerpt'] = train['excerpt'].apply(clean_text)</span></pre><p id="1439" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">除了文本，我还可以选择URL中的单个单词，并将这些数据转换成新的文本特征。</p><pre class="kg kh ki kj gt nj nc nk nl aw nm bi"><span id="42ec" class="nn kx iq nc b gy no np l nq nr">def getWordsFromURL(url):<br/>    return re.compile(r'[\:/?=\-&amp;.]+',re.UNICODE).split(url)</span><span id="028b" class="nn kx iq nc b gy ns np l nq nr">train['url_legal'] = train['url_legal'].fillna("nan").apply(getWordsFromURL).apply(<br/>    lambda x: " ".join(x))</span></pre><p id="a1f9" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我从文本中创建了几个新特征——这些是各种各样的统计信息。还是那句话，创意的空间很大，但是这个数据对我们来说已经足够了。这些特性的主要目的是对基线模型有所帮助。</p><pre class="kg kh ki kj gt nj nc nk nl aw nm bi"><span id="9133" class="nn kx iq nc b gy no np l nq nr">def get_sentence_lengths(text):</span><span id="a89e" class="nn kx iq nc b gy ns np l nq nr">    tokened = sent_tokenize(text)<br/>    lengths = []<br/>    <br/>    for idx,i in enumerate(tokened):<br/>        splited = list(i.split(" "))<br/>        lengths.append(len(splited))</span><span id="7fdd" class="nn kx iq nc b gy ns np l nq nr">    return (max(lengths),<br/>            min(lengths),<br/>            round(mean(lengths), 3))</span><span id="f5b7" class="nn kx iq nc b gy ns np l nq nr">def create_features(df):<br/>    <br/>    df_f = pd.DataFrame(index=df.index)<br/>    df_f['text_len'] = df['excerpt'].apply(len)<br/>    df_f['text_clean_len' ]= df['clean_excerpt'].apply(len)<br/>    df_f['text_len_div'] = df_f['text_clean_len' ] / df_f['text_len']<br/>    df_f['text_word_count'] = df['clean_excerpt'].apply(<br/>        lambda x : len(x.split(' ')))<br/>    <br/>    df_f[['max_len_sent','min_len_sent','avg_len_sent']] = \<br/>        df_f.apply(<br/>            lambda x: get_sentence_lengths(x['excerpt']),<br/>            axis=1, result_type='expand')<br/>    <br/>    return df_f</span><span id="c156" class="nn kx iq nc b gy ns np l nq nr">train = pd.concat(<br/>    [train, create_features(train)], axis=1, copy=False, sort=False)</span><span id="4fba" class="nn kx iq nc b gy ns np l nq nr">basic_f_columns = [<br/>    'text_len', 'text_clean_len', 'text_len_div', 'text_word_count',<br/>    'max_len_sent', 'min_len_sent', 'avg_len_sent']</span></pre><p id="5303" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">当数据稀缺时，更容易检验假设，结果通常需要更加稳定。因此，为了对结果更有信心，我更喜欢在这种情况下使用OOF(Out-of-Fold)预测。</p><h1 id="b69d" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">基线</h1><p id="2973" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我选择了<a class="ae kv" href="https://catboost.ai/" rel="noopener ugc nofollow" target="_blank"> Catboost </a>作为模型的免费库。Catboost是一个高性能的开源库，用于决策树的梯度提升。从版本0.19.1开始，它支持在GPU上开箱即用的分类文本功能。主要优点是CatBoost可以在数据中包含分类和文本函数，而无需额外的预处理。</p><p id="cc13" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">在<a class="ae kv" rel="noopener" target="_blank" href="/unconventional-sentiment-analysis-bert-vs-catboost-90645f2437a9">非常规情感分析:BERT vs. Catboost </a>中，我详述了Catboost如何处理文本，并将其与BERT进行了比较。</p><p id="1b44" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这个库有一个杀手锏:它知道如何处理嵌入。不幸的是，目前文档中对此只字未提，只有少数人知道Catboost的这一优势。</p><pre class="kg kh ki kj gt nj nc nk nl aw nm bi"><span id="266b" class="nn kx iq nc b gy no np l nq nr">!pip install catboost</span></pre><p id="9f9e" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">当使用Catboost时，我建议使用池。它是一个方便的包装器，结合了特性、标签和元数据，比如分类和文本特性。</p><p id="19f4" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了比较实验，我创建了一个仅使用数字和分类特征的基线模型。</p><p id="9743" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我写了一个函数来初始化和训练模型。顺便说一下，我没有选择最佳参数。</p><pre class="kg kh ki kj gt nj nc nk nl aw nm bi"><span id="7a6d" class="nn kx iq nc b gy no np l nq nr">def fit_model_classifier(train_pool, test_pool, **kwargs):<br/>    model = CatBoostClassifier(<br/>        task_type='GPU',<br/>        iterations=5000,<br/>        eval_metric='AUC',<br/>        od_type='Iter',<br/>        od_wait=500,<br/>        l2_leaf_reg=10,<br/>        bootstrap_type='Bernoulli',<br/>        subsample=0.7,<br/>        **kwargs<br/>    )</span><span id="d44f" class="nn kx iq nc b gy ns np l nq nr">    return model.fit(<br/>        train_pool,<br/>        eval_set=test_pool,<br/>        verbose=100,<br/>        plot=False,<br/>        use_best_model=True)</span></pre><p id="c0b7" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了实现OOF，我编写了一个简单明了的小函数。</p><pre class="kg kh ki kj gt nj nc nk nl aw nm bi"><span id="34aa" class="nn kx iq nc b gy no np l nq nr">def get_oof_classifier(<br/>        n_folds, x_train, y, embedding_features,<br/>        cat_features, text_features, tpo, seeds,<br/>        num_bins, emb=None, tolist=True):<br/>    <br/>    ntrain = x_train.shape[0]<br/>        <br/>    oof_train = np.zeros((len(seeds), ntrain, num_bins))    <br/>    models = {}</span><span id="f038" class="nn kx iq nc b gy ns np l nq nr">    for iseed, seed in enumerate(seeds):<br/>        kf = StratifiedKFold(<br/>            n_splits=n_folds,<br/>            shuffle=True,<br/>            random_state=seed)    <br/>      <br/>        for i, (tr_i, t_i) in enumerate(kf.split(x_train, y)):<br/>            if emb and len(emb) &gt; 0:<br/>                x_tr = pd.concat(<br/>                    [x_train.iloc[tr_i, :],<br/>                     get_embeddings(<br/>                         x_train.iloc[tr_i, :], emb, tolist)],<br/>                    axis=1, copy=False, sort=False)<br/>                x_te = pd.concat(<br/>                    [x_train.iloc[t_i, :],<br/>                     get_embeddings(<br/>                         x_train.iloc[t_i, :], emb, tolist)],<br/>                    axis=1, copy=False, sort=False)<br/>                columns = [<br/>                    x for x in x_tr if (x not in ['excerpt'])]  <br/>                if not embedding_features:<br/>                    for c in emb:<br/>                        columns.remove(c)<br/>            else:<br/>                x_tr = x_train.iloc[tr_i, :]<br/>                x_te = x_train.iloc[t_i, :]<br/>                columns = [<br/>                    x for x in x_tr if (x not in ['excerpt'])] <br/>            x_tr = x_tr[columns]<br/>            x_te = x_te[columns]                <br/>            y_tr = y[tr_i]            <br/>            y_te = y[t_i]</span><span id="ec32" class="nn kx iq nc b gy ns np l nq nr">            train_pool = Pool(<br/>                data=x_tr,<br/>                label=y_tr,<br/>                cat_features=cat_features,<br/>                embedding_features=embedding_features,<br/>                text_features=text_features)</span><span id="b9cc" class="nn kx iq nc b gy ns np l nq nr">            valid_pool = Pool(<br/>                data=x_te,<br/>                label=y_te,<br/>                cat_features=cat_features,<br/>                embedding_features=embedding_features,<br/>                text_features=text_features)</span><span id="8aee" class="nn kx iq nc b gy ns np l nq nr">            model = fit_model_classifier(<br/>                train_pool, valid_pool,<br/>                random_seed=seed,<br/>                text_processing=tpo<br/>            )<br/>            oof_train[iseed, t_i, :] = \<br/>                model.predict_proba(valid_pool)<br/>            models[(seed, i)] = model<br/>            <br/>    oof_train = oof_train.mean(axis=0)<br/>    <br/>    return oof_train, models</span></pre><p id="7d90" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我将在下面写关于<em class="nu"> get_embeddings </em>函数，但是它不用于获取模型的基线。</p><p id="8eff" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我用以下参数训练了基线模型:</p><pre class="kg kh ki kj gt nj nc nk nl aw nm bi"><span id="f973" class="nn kx iq nc b gy no np l nq nr">columns = ['license', 'url_legal'] + basic_f_columns </span><span id="5be7" class="nn kx iq nc b gy ns np l nq nr">oof_train_cb, models_cb = get_oof_classifier(<br/>    n_folds=5,<br/>    x_train=train[columns],<br/>    y=train['target_q'].values,<br/>    embedding_features=None,<br/>    cat_features=['license'],<br/>    text_features=['url_legal'],<br/>    tpo=tpo,<br/>    seeds=[0, 42, 888],<br/>    num_bins=num_bins<br/>)</span></pre><p id="3027" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">训练模型的质量:</p><pre class="kg kh ki kj gt nj nc nk nl aw nm bi"><span id="de52" class="nn kx iq nc b gy no np l nq nr">roc_auc_score(train['target_q'], oof_train_cb, multi_class="ovo")</span><span id="bd28" class="nn kx iq nc b gy ns np l nq nr">AUC: 0.684407</span></pre><p id="f961" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在我有了模型质量的基准。从数字来看，这个模型很弱，我不会在生产中实现它。</p><h1 id="1f99" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">嵌入</h1><p id="c839" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">你可以把多维向量转化为嵌入，嵌入是一个相对低维的空间。因此，嵌入简化了大输入的机器学习，例如表示单词的稀疏向量。理想情况下，嵌入通过将语义相似的输入放在嵌入空间中彼此靠近来捕获一些输入语义。</p><p id="2cde" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">有许多方法可以获得这样的向量，我在本文中不考虑它们，因为这不是研究的目的。然而，对我来说，以任何方式获得嵌入就足够了；最重要的是他们保存了必要的信息。在大多数情况下，我使用目前流行的方法——预训练变形金刚。</p><pre class="kg kh ki kj gt nj nc nk nl aw nm bi"><span id="7242" class="nn kx iq nc b gy no np l nq nr">from sentence_transformers import SentenceTransformer</span><span id="0672" class="nn kx iq nc b gy ns np l nq nr">STRANSFORMERS = {<br/>    'sentence-transformers/paraphrase-mpnet-base-v2': ('mpnet', 768),<br/>    'sentence-transformers/bert-base-wikipedia-sections-mean-tokens': ('wikipedia', 768)<br/>}</span><span id="8f83" class="nn kx iq nc b gy ns np l nq nr">def get_encode(df, encoder, name):    <br/>    device = torch.device(<br/>        "cuda:0" if torch.cuda.is_available() else "cpu")<br/><br/>    model = SentenceTransformer(<br/>        encoder, <br/>        cache_folder=f'./hf_{name}/'<br/>    )<br/>    model.to(device)<br/>    model.eval()<br/>    return np.array(model.encode(df['excerpt']))</span><span id="a3e7" class="nn kx iq nc b gy ns np l nq nr">def get_embeddings(df, emb=None, tolist=True):<br/>    <br/>    ret = pd.DataFrame(index=df.index)<br/>    <br/>    for e, s in STRANSFORMERS.items():<br/>        if emb and s[0] not in emb:<br/>            continue<br/>        <br/>        ret[s[0]] = list(get_encode(df, e, s[0]))<br/>        if tolist:<br/>            ret = pd.concat(<br/>                [ret, pd.DataFrame(<br/>                    ret[s[0]].tolist(),<br/>                    columns=[f'{s[0]}_{x}' for x in range(s[1])],<br/>                    index=ret.index)],<br/>                axis=1, copy=False, sort=False)<br/>    <br/>    return ret</span></pre><p id="9dd1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在我已经准备好开始测试不同版本的模型了。</p><h1 id="1179" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">模型</h1><p id="2653" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我有几个拟合模型的选项:</p><ul class=""><li id="391c" class="mq mr iq lq b lr mk lu ml lx ms mb mt mf mu mj mv mw mx my bi translated">文本特征；</li><li id="5607" class="mq mr iq lq b lr nd lu ne lx nf mb ng mf nh mj mv mw mx my bi translated">嵌入特征；</li><li id="f073" class="mq mr iq lq b lr nd lu ne lx nf mb ng mf nh mj mv mw mx my bi translated">嵌入特征，如一列分离的数字特征。</li></ul><p id="fd4a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我一直在这些选项的各种组合中接受训练，这让我得出结论，嵌入可能是多么有用，或者，也许，它是一个过度工程。</p><p id="cc50" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">作为一个例子，我给出了一个使用所有三个选项的代码:</p><pre class="kg kh ki kj gt nj nc nk nl aw nm bi"><span id="14f5" class="nn kx iq nc b gy no np l nq nr">columns = ['license', 'url_legal', 'clean_excerpt', 'excerpt'] </span><span id="01b1" class="nn kx iq nc b gy ns np l nq nr">oof_train_cb, models_cb = get_oof_classifier(<br/>    n_folds=FOLDS,<br/>    x_train=train[columns],<br/>    y=train['target_q'].values,<br/>    embedding_features=['mpnet', 'wikipedia'],<br/>    cat_features=['license'],<br/>    text_features=['clean_excerpt','url_legal'],<br/>    tpo=tpo,<br/>    seeds=[0, 42, 888],<br/>    num_bins=num_bins,<br/>    emb=['mpnet', 'wikipedia'],<br/>    tolist=True<br/>)</span></pre><p id="2559" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了获得更多信息，我在GPU和CPU上训练了模型；并将结果汇总在一个表格中。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/1da293d12995774ea24fc7e622741679.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*uYlSg0ijypms4JVoTQEtZg.png"/></div></figure><p id="f5ce" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">首先让我震惊的是<strong class="lq ir">文字特征和嵌入的交互极差</strong>。不幸的是，我还没有对这个事实的任何逻辑解释——在这里，需要在其他数据集上对这个问题进行更详细的研究。同时，请注意，文本的组合使用和同一文本的嵌入会降低模型的质量。</p><p id="9571" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">更新</strong>:我收到开发者的评论:</p><blockquote class="nw nx ny"><p id="5394" class="lo lp nu lq b lr mk jr lt lu ml ju lw nz mm lz ma oa mn md me ob mo mh mi mj ij bi translated">“谢谢你的报告！这个错误已经在<a class="ae kv" href="https://github.com/catboost/catboost/commit/d76e62988dea0e2a0b3b2e56df979d75c944279e" rel="noopener ugc nofollow" target="_blank">提交</a>中修复，并将在下一个版本中发布”</p></blockquote><p id="975f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我的另一个发现是，当在CPU上训练模式时,<strong class="lq ir">嵌入不起作用。</strong></p><p id="30c2" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在，一件好事是，如果你有一个GPU并且可以获得嵌入，最好的质量将是当你同时使用嵌入作为一个功能和一系列单独的数字功能时。</p><h1 id="838f" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">摘要</h1><p id="a439" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在这篇文章中，我:</p><ul class=""><li id="ace7" class="mq mr iq lq b lr mk lu ml lx ms mb mt mf mu mj mv mw mx my bi translated">选择了一个小型免费数据集进行测试；</li><li id="d39b" class="mq mr iq lq b lr nd lu ne lx nf mb ng mf nh mj mv mw mx my bi translated">为文本数据创建了几个统计特征，用于制作基线模型；</li><li id="d091" class="mq mr iq lq b lr nd lu ne lx nf mb ng mf nh mj mv mw mx my bi translated">测试了嵌入、文本和简单特性的各种组合；</li><li id="68b6" class="mq mr iq lq b lr nd lu ne lx nf mb ng mf nh mj mv mw mx my bi translated">我得到了一些非显而易见的见解。</li></ul><p id="1805" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这些鲜为人知的信息将对社区有所帮助，并从您的项目中受益。不幸的是，Catboost处理嵌入和文本的功能仍然是原始的。但是，它正在积极改进，我希望很快会有一个稳定的版本，开发者会更新文档。复制本文结果的完整代码可从<a class="ae kv" href="https://github.com/sagol/boosted-embeddings/blob/main/boosted_embeddings.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>获得。</p></div></div>    
</body>
</html>