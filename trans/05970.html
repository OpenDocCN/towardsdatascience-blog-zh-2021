<html>
<head>
<title>Beginner’s Introduction to NLP — Building a Spam Classifier</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP入门——构建垃圾邮件分类器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-beginners-introduction-to-nlp-building-a-spam-classifier-cf0973c7f42c?source=collection_archive---------26-----------------------#2021-05-28">https://towardsdatascience.com/a-beginners-introduction-to-nlp-building-a-spam-classifier-cf0973c7f42c?source=collection_archive---------26-----------------------#2021-05-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3796" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如果您不熟悉自然语言处理这个激动人心的世界，请从这里开始</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/048a23752ad890381ddd8884186c5e35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*d4BljH8rZ6H-n0t3"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">内森·迪菲斯塔在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="9c1f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">单词、句子、段落和短文。我们成年后几乎每天都在使用它们。无论你是在发推文，给你的同事写电子邮件，还是像我现在这样写文章，作为人类，我们都用文字来交流我们的思想和想法。</p><p id="bc8c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，想象一个世界，在那里我们可以教计算机与文字互动，就像我们与另一个人互动一样。想象一下这样一个世界，计算机不仅能够检测人类的语言，更重要的是，能够学习语言的细微差别，从而得出给定信息的潜在含义或意图。</p><p id="06c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这正是自然语言处理(NLP)的全部内容。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="544c" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">介绍</h1><p id="9826" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">NLP指的是人工智能领域，通过自然语言处理计算机和人类之间的交互。这包括使计算机能够操纵、分析、解释以及生成人类语言。</p><p id="2d87" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我最近花了一些时间涉猎了一些NLP的基础知识，我想用这篇文章不仅分享我所学到的东西，而且进一步加强我对这个主题的理解。具体来说，这篇文章的灵感来源于<a class="ae ky" href="https://www.linkedin.com/learning/nlp-with-python-for-machine-learning-essential-training" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">NLP with Python for Machine Learning Essential Training</strong></a>，一个由<a class="ae ky" href="https://www.linkedin.com/in/derek-jedamski-8a887045/?trk=lil_course" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> Derek Jedamski </strong> </a>给出的LinkedIn课程。</p><p id="de41" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我想在此对Derek的工作给予充分的肯定和参考，并强烈推荐查看他在Python中关于机器学习的其他课程。</p><p id="938d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我写这篇文章的目的是重新创建在课程中介绍的主要项目，即使用垃圾短信收集数据集<a class="ae ky" href="https://www.kaggle.com/uciml/sms-spam-collection-dataset" rel="noopener ugc nofollow" target="_blank">构建的垃圾短信(二进制)分类器，该数据集包含一组5，572条英文短信。一旦准备就绪，分类器将能够读取给定的文本字符串，并随后将文本分类为<em class="mz">火腿</em>或<em class="mz">垃圾邮件</em>。</a></p><p id="530f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我将介绍构建垃圾邮件分类器的步骤，并重点介绍自然语言处理中的一些关键概念。如果你想继续下去，这个项目的代码可以在我的GitHub <a class="ae ky" href="https://github.com/chongjason914/spam-classification" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><p id="d30f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">事不宜迟，我们开始吧！</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="na nb l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">GIF by GIPHY</p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="3005" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">1.探索性数据分析</h1><p id="06fc" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">像任何专业的数据科学家一样，让我们从探索数据集开始。</p><p id="50d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">探索性数据分析是对数据进行初步调查的过程，以发现模式、发现异常、测试假设并借助汇总统计和图形表示检查假设(定义归功于Prasad Patil的“<a class="ae ky" rel="noopener" target="_blank" href="/exploratory-data-analysis-8fc1cb20fd15">什么是探索性数据分析</a>”)。</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="3eb8" class="nh md it nd b gy ni nj l nk nl">data = pd.read_csv("spam.csv", encoding = "latin-1")<br/>data = data.dropna(how = "any", axis = 1)<br/>data.columns = ['label', 'body_text']<br/>data.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/6d9cb0b405838a7089309064be2e1d7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*sF7htt1D7scXTA6YHTpTJg.png"/></div></figure><p id="2de1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输入数据有5572行(每行代表一个唯一的文本消息)和2列:<em class="mz">标签</em>和<em class="mz"> body_text。在5572行中，有747行是垃圾邮件，剩下的4825行是火腿。我们也没有丢失数据，耶！</em></p><p id="75d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们使用seaborn来更直观地表示我们的输入数据。</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="ca84" class="nh md it nd b gy ni nj l nk nl">total = len(data)<br/>plt.figure(figsize = (5, 5))<br/>plt.title("Number of spam vs ham messages")<br/>ax = sns.countplot(x = 'label', data = data)<br/>for p in ax.patches:<br/>    percentage = '{0:.0f}%'.format(p.get_height() / total * 100)<br/>    x = p.get_x() + p.get_width() / 2<br/>    y = p.get_height() + 20<br/>    ax.annotate(percentage, (x, y), ha = 'center')<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/466552cdb63596737b9efaa6ed66ffcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*MkQa5Ai_2-m9kjRdLShZng.png"/></div></figure><p id="a151" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如我们所见，垃圾邮件(87%)明显多于垃圾邮件(13%)。</p><p id="4b05" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从技术上讲，在这个阶段，我们应该关注与不平衡数据集相关的问题。然而，为了简单起见，我们暂时忽略这一点。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="b04a" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">2.特征工程</h1><p id="8e93" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">特征工程是创建新特征和/或改造现有特征以充分利用数据的过程。</p><p id="7a22" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本节中，我们将创建两个新功能:</p><ul class=""><li id="2175" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu nt nu nv nw bi translated"><strong class="lb iu"> body_len: </strong>正文的长度，不包括空格</li><li id="1fe8" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated"><strong class="lb iu">点%</strong>正文中标点符号的百分比</li></ul><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="5b9b" class="nh md it nd b gy ni nj l nk nl"># body_len<br/>data['body_len'] = data.body_text.apply(lambda x: len(x) - x.count(" "))</span><span id="36e4" class="nh md it nd b gy oc nj l nk nl"># punct%<br/>def count_punct(text):<br/>    count = sum([1 for char in text if char in string.punctuation])<br/>    return round(count/(len(text) - text.count(" ")), 3) * 100 data[‘punct%’] = data.body_text.apply(lambda x: count_punct(x))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/15793b03dd3ef1968bcb59b95b6dd14d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R0vP7iu-dalkjXu1dOw1RA.png"/></div></div></figure><p id="d76e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们可以继续使用新创建的要素来探索输入数据的分布。</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="343c" class="nh md it nd b gy ni nj l nk nl">bins = np.linspace(0, 200, 40)<br/>data.loc[data.label == 'spam', 'body_len'].plot(kind = 'hist', bins = bins, alpha = 0.5, density = True, label = 'spam')<br/>data.loc[data.label == 'ham', 'body_len'].plot(kind = 'hist', bins = bins, alpha = 0.5, density = True, label = 'ham')<br/>plt.legend(loc = 'best')<br/>plt.xlabel("body_len")<br/>plt.title("Body length ham vs spam")<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/4c3f4cd3862b2fc0bc4e251844955980.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*H9T8KRBbMn-YeGOsZdiCpA.png"/></div></figure><p id="1b1d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们所见，与垃圾邮件相比，垃圾邮件的正文长度更长，即包含更多的单词。</p><p id="3eb5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">那么，标点符号的百分比呢？</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="688e" class="nh md it nd b gy ni nj l nk nl">bins = np.linspace(0, 50, 40)<br/>data.loc[data.label == 'spam', 'punct%'].plot(kind = 'hist', bins = bins, alpha = 0.5, density = True, label = 'spam')<br/>data.loc[data.label == 'ham', 'punct%'].plot(kind = 'hist', bins = bins, alpha = 0.5, density = True, label = 'ham')<br/>plt.legend(loc = 'best')<br/>plt.xlabel("punct%")<br/>plt.title("Punctuation percentage ham vs spam")<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/c6c0ce043fe3de3557d3dc24846a8f02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1230/format:webp/1*6-DPhDxJ_PqnfJ8yhKhShg.png"/></div></figure><p id="1523" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，这两种分布看起来非常相似，尽管业余爱好者的消息似乎有一个更长的尾巴，即业余爱好者的消息往往有更高的标点符号百分比。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="691b" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">3.清理文本</h1><p id="7ab0" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">如果您有任何处理真实世界数据的经验，您会知道真实世界的数据通常非常混乱，并且不容易处理。这个也不例外。</p><p id="c92e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们能够实际建模和预测数据之前，还有很多工作要做。更具体地说，为了让我们更好地管理杂乱的文本消息，我们将需要执行以下预处理步骤(这些步骤非常特定于NLP管道):</p><ul class=""><li id="9b5b" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu nt nu nv nw bi translated">将单词转换成小写字母</li><li id="6a24" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">删除标点符号和停用词</li><li id="36c4" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">符号化</li><li id="5486" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">词干与词汇匹配(文本规范化)</li></ul><p id="afcf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们逐一调查每个步骤。</p><h2 id="8e5e" class="nh md it bd me of og dn mi oh oi dp mm li oj ok mo lm ol om mq lq on oo ms op bi translated">3.1将单词转换成小写字母</h2><p id="3d25" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">Python并不认为所有字符都是平等的。为了保持一致，我们需要将所有单词转换成小写字母。例如，Python将向下面的第一条语句返回False，但向第二条语句返回True。</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="77a2" class="nh md it nd b gy ni nj l nk nl"># False statement <br/>"NLP" == "nlp"</span><span id="d5e4" class="nh md it nd b gy oc nj l nk nl"># True statement <br/>"NLP".lower() == "nlp"</span></pre><h2 id="270f" class="nh md it bd me of og dn mi oh oi dp mm li oj ok mo lm ol om mq lq on oo ms op bi translated">3.2删除标点符号</h2><p id="0bbe" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">与上面的论点相似，去掉标点符号的理由是因为标点符号在文本中没有任何意义。因此，我们希望Python只关注给定文本中的单词，而不用担心涉及的标点符号。</p><p id="6c4c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面举个例子。</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="581a" class="nh md it nd b gy ni nj l nk nl"># False statement<br/>"I love NLP" == "I love NLP."</span></pre><p id="81ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为人类，我们可以立即看出上面的两段文字几乎完全相同，除了第二段在句尾有一个句号。</p><p id="0201" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是，Python将无法区分这两种文本。出于这个原因，为了让Python更清楚地解释文本，我们删除句子中的所有标点符号是很重要的。</p><p id="dd28" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以通过键入以下内容找到存储在Python中的<strong class="lb iu">字符串</strong>库中的标点符号列表:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="0241" class="nh md it nd b gy ni nj l nk nl">string.punctuation</span></pre><p id="24c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了去掉句子中的标点符号，我们可以如下使用列表理解:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="1d12" class="nh md it nd b gy ni nj l nk nl"># Original text <br/>text = 'OMG! Did you see what happened to her? I was so shocked when I heard the news. :('</span><span id="49a9" class="nh md it nd b gy oc nj l nk nl"># List comprehension to remove punctuation <br/>text = "".join([word for word in text if word not in string.punctuation])</span></pre><h2 id="ee89" class="nh md it bd me of og dn mi oh oi dp mm li oj ok mo lm ol om mq lq on oo ms op bi translated">3.3令牌化</h2><p id="ec86" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">在NLP的上下文中，标记化意味着将一个字符串或一个句子转换成一个字符列表，我们可以通过利用Python中的<strong class="lb iu">正则表达式(re) </strong>库来实现这一点。</p><p id="9c01" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">库中两个最简单的命令包括:</p><ul class=""><li id="0eb0" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu nt nu nv nw bi translated">重新拆分</li><li id="43f2" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">重新发现</li></ul><p id="f798" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你有兴趣知道它们是如何工作的，我鼓励你查看我的<a class="ae ky" href="https://github.com/chongjason914/spam-classification" rel="noopener ugc nofollow" target="_blank">笔记本</a>以了解更多细节。</p><h2 id="68ba" class="nh md it bd me of og dn mi oh oi dp mm li oj ok mo lm ol om mq lq on oo ms op bi translated">3.4删除停用词</h2><p id="3f5e" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">停用词是英语中常用的词，如<em class="mz">但是</em>、<em class="mz"> if </em>和<em class="mz"> the </em>对句子的整体意思没有太大贡献。因此，为了减少Python在构建我们的机器学习模型时需要存储和处理的令牌数量，通常会删除停用词。</p><p id="e693" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">停用词存储在<strong class="lb iu"> nltk.corpus.stopwords </strong>中，可通过以下方式访问:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="135f" class="nh md it nd b gy ni nj l nk nl">stopwords = nltk.corpus.stopwords.words('english')<br/>stopwords</span></pre><p id="1330" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要删除给定字符串中的停用词，我们可以再次应用列表理解。</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="fd09" class="nh md it nd b gy ni nj l nk nl"># Original text <br/>text = 'OMG Did you see what happened to her I was so shocked when I heard the news'<br/>print(text)</span><span id="8a2d" class="nh md it nd b gy oc nj l nk nl"># Convert text into list of words in lowercase letters<br/>print(text.lower().split())</span><span id="ffaa" class="nh md it nd b gy oc nj l nk nl"># List comprehension to remove stopwords <br/>print([word for word in text.lower().split() if word not in stopwords]</span></pre><p id="c95a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">运行上面的代码片段后，以下停用词将被删除。</p><ul class=""><li id="511e" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu nt nu nv nw bi translated">做</li><li id="f2b0" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">你们</li><li id="3c89" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">什么</li><li id="1b79" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">到</li><li id="cc62" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">她</li><li id="e8ae" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">我</li><li id="a507" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">是</li><li id="29c9" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">因此</li><li id="36ce" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">当...的时候</li><li id="c2fd" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">这</li></ul><p id="89e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，在运行列表理解之前，我们首先将原始文本转换为小写单词列表。这是因为单词在nltk库中是以小写字母存储的。</p><h2 id="b923" class="nh md it bd me of og dn mi oh oi dp mm li oj ok mo lm ol om mq lq on oo ms op bi translated">3.5词干化与词汇化(单词规范化)</h2><p id="12c6" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated"><strong class="lb iu">词干化:</strong>通过粗略地砍掉词尾，只留下词根，把词干或词根的词形变化或派生词减少到词干或词根的过程。<br/> <strong class="lb iu">词汇匹配:</strong>把一个词的屈折形式组合在一起，使它们可以作为一个词来分析的过程。</p><p id="8e7b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从广义上讲，词干分析和词目分析都是为了将同一个单词的变体浓缩到它们的词根形式。这是为了防止计算机存储它在单词语料库中看到的每一个独特的单词，而是只记录一个单词的最基本形式，并将其他具有类似含义的单词关联起来。</p><p id="d4e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">比如<em class="mz">成长</em>、<em class="mz">成长</em>、<em class="mz">成长</em>、<em class="mz">成长</em>都是<em class="mz">成长</em>一词的简单变体。在这种情况下，计算机只需要记住单词<em class="mz"> grow </em>而不需要记住其他的。</p><p id="2fb4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要访问词干分析器和lemmatiser:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="8a07" class="nh md it nd b gy ni nj l nk nl">ps = nltk.PorterStemmer()<br/>wn = nltk.WordNetLemmatizer()</span></pre><p id="5593" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">斯特梅尔采用了一种比lemmatiser更为粗糙的方法，即在不知道单词使用的上下文的情况下，使用启发式方法简单地切断单词的结尾。因此，词干分析器有时不能返回词典中的实际单词。</p><p id="b5d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一方面，Lemmatiser将总是返回一个字典单词。Lemmatiser在简化一个给定的单词之前考虑了多个因素，通常更准确。然而，这是以比词干分析器更慢和计算量更大为代价的。</p><h2 id="cbfe" class="nh md it bd me of og dn mi oh oi dp mm li oj ok mo lm ol om mq lq on oo ms op bi translated">3.6将所有内容放在一个clean_text函数中</h2><p id="31e4" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">太棒了，现在我们已经理解了文本清理的所有预处理步骤，我们想把所有东西总结成一个名为<em class="mz"> clean_text </em>的函数，然后我们可以将它应用到我们的输入数据中。</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="0f65" class="nh md it nd b gy ni nj l nk nl"># Create function for text cleaning <br/>def clean_text(text):<br/>    text = "".join([word.lower() for word in text if word not in string.punctuation])<br/>    tokens = re.findall('\S+', text)<br/>    text = [wn.lemmatize(word) for word in tokens if word not in stopwords]<br/>    return text</span><span id="fde0" class="nh md it nd b gy oc nj l nk nl"># Apply function to body_text <br/>data['cleaned_text'] = data['body_text'].apply(lambda x: clean_text(x))<br/>data[['body_text', 'cleaned_text']].head(10)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/05f149519a6284f0a81d522206df61b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l4sEztTTmvIMnLyfV3zaeg.png"/></div></div></figure><p id="5781" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从那里，我们可以计算出在垃圾邮件中最常见的单词。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/8ec2ed2fe05cbf042db275bdd603b7f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*84BiryzeN_Viuy4C4PWVEA.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/fceed780e76798f04c6c914455bf95d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aXp8r6Niw6x_ubYha_8UTg.png"/></div></div></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="d7eb" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">4.矢量化</h1><p id="b4b6" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">矢量化是将文本编码为整数以创建特征向量的过程。</p><p id="8927" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本节中，我们将在<strong class="lb iu"> scikit-learn </strong>中查看三个不同的文本矢量化函数:</p><ul class=""><li id="9eb1" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu nt nu nv nw bi translated">计数矢量器</li><li id="d84e" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">tfidfttransformer</li><li id="634a" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">tfidf矢量器</li></ul><h2 id="0baa" class="nh md it bd me of og dn mi oh oi dp mm li oj ok mo lm ol om mq lq on oo ms op bi translated">4.1(计数矢量器+ TfidfTransformer)的工作原理</h2><p id="3180" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated"><a class="ae ky" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" rel="noopener ugc nofollow" target="_blank"> CountVectorizer </a>创建一个文档术语矩阵，其中每个单元的条目将是该单词在该文档中出现的次数的计数。</p><p id="331d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html" rel="noopener ugc nofollow" target="_blank"> TfidfTransformer </a>类似于CountVectorizer，但单元格不是代表计数，而是代表一个权重，用于确定一个单词对单个文本消息的重要性。计算每个像元权重的公式如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/b89e4e2821c62eab4bebceb91db3e7f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*gAAIvJ6DMqpccQm-_bZEfA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">TF-IDF下的加权计算公式</p></figure><p id="5608" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了证明这一点，让我们看一个例子。</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="6515" class="nh md it nd b gy ni nj l nk nl"># CountVectorizer<br/>corpus = ['I love bananas', 'Bananas are so amazing!', 'Bananas go so well with pancakes']<br/>count_vect = CountVectorizer()<br/>corpus = count_vect.fit_transform(corpus)<br/>pd.DataFrame(corpus.toarray(), columns = count_vect.get_feature_names())</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/1a179304ea6bebe9d195c4f0a0216cc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IlMvnO-UGJqnH0k-pFmmXw.png"/></div></div></figure><p id="aa39" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据帧中的每一行代表一个句子(文档),每一列代表整个语料库中唯一的词，不包括停用词。例如，在<em class="mz">惊人的</em>、<em class="mz">是</em>、<em class="mz">香蕉</em>和<em class="mz">所以</em>列中，“香蕉如此神奇”的值为1(其他的为0)，因为这些单词中的每一个在特定的句子中都出现过一次。</p><p id="53ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一方面，TfidfTransformer的工作方式如下:</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="c44c" class="nh md it nd b gy ni nj l nk nl"># TfidfTransformer<br/>tfidf_transformer = TfidfTransformer()<br/>corpus = tfidf_transformer.fit_transform(corpus)<br/>pd.DataFrame(corpus.toarray(), columns = count_vect.get_feature_names())</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/b69f17cbe1ffa9b2873c41c80f3b9060.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zAQkW6UnPHkW0xy0OMNLzA.png"/></div></div></figure><p id="323d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">回想一下，TF-IDF中的单元格表示一个单词对一条单独的文本消息的重要性的权重。</p><p id="9096" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们以<em class="mz">香蕉</em>栏目为例。虽然单词<em class="mz"> bananas </em>在三个句子的每一个中只出现一次，但是与第二个和第三个相比，第一个句子被赋予更高的权重，因为第一个句子具有最短的长度。换句话说，单词<em class="mz">香蕉</em>在第一句中比在第二句和第三句中更重要。</p><p id="f684" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个单词在文档或语料库中出现得越少(越不频繁)，在TF-IDF下的权重就越高。</p><h2 id="a9c7" class="nh md it bd me of og dn mi oh oi dp mm li oj ok mo lm ol om mq lq on oo ms op bi translated">4.2 tfidf矢量器的工作原理</h2><p id="ff5e" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html" rel="noopener ugc nofollow" target="_blank"> TfidfVectorizer </a>相当于CountVectorizer后跟TfidfTransformer。</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="8f9f" class="nh md it nd b gy ni nj l nk nl"># TfidfVectorizer<br/>corpus = ['I love bananas', 'Bananas are so amazing!', 'Bananas go so well with pancakes']<br/>tfidf_vect = TfidfVectorizer()<br/>corpus = tfidf_vect.fit_transform(corpus)<br/>pd.DataFrame(corpus.toarray(), columns = tfidf_vect.get_feature_names())</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/b69f17cbe1ffa9b2873c41c80f3b9060.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zAQkW6UnPHkW0xy0OMNLzA.png"/></div></div></figure><p id="27f2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到，结果是完全一样的。因此，为了方便起见，我们将在项目中使用TfidfVectorizer。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="2e92" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">5.系统模型化</h1><p id="ca6b" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">最后，是时候找点乐子了！</p><p id="6b35" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们的数据已经准备好了，我们终于可以继续建模了，这实际上是建立我们的垃圾邮件过滤器，将一个给定的文本分类为垃圾邮件。</p><p id="0158" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，我们将考虑两种建模方法:<strong class="lb iu">训练-测试-分割</strong>和<strong class="lb iu">流水线</strong>以及两种机器学习模型，或者更具体地说，集成方法:<strong class="lb iu">随机森林</strong>和<strong class="lb iu">梯度推进</strong>。</p><p id="656d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你是机器学习的新手，集成方法本质上是一种技术，通过它可以创建多个模型并进行组合，目标是产生比单个模型更好的预测精度。</p><h2 id="4ed4" class="nh md it bd me of og dn mi oh oi dp mm li oj ok mo lm ol om mq lq on oo ms op bi translated">5.1列车-测试-分离</h2><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="7f42" class="nh md it nd b gy ni nj l nk nl"># Train test split<br/>X_train, X_test, Y_train, Y_test = train_test_split(data[['body_text', 'body_len', 'punct%']], data.label, random_state = 42, test_size = 0.2)</span><span id="d3d5" class="nh md it nd b gy oc nj l nk nl"># Instantiate and fit TfidfVectorizer<br/>tfidf_vect = TfidfVectorizer(analyzer = clean_text)<br/>tfidf_vect_fit = tfidf_vect.fit(X_train['body_text'])</span><span id="4a19" class="nh md it nd b gy oc nj l nk nl"># Use fitted TfidfVectorizer to transform body text in X_train and X_test<br/>tfidf_train = tfidf_vect.transform(X_train['body_text'])<br/>tfidf_test = tfidf_vect.transform(X_test['body_text'])</span><span id="60c5" class="nh md it nd b gy oc nj l nk nl"># Recombine transformed body text with body_len and punct% features<br/>X_train = pd.concat([X_train[['body_len', 'punct%']].reset_index(drop = True), pd.DataFrame(tfidf_train.toarray())], axis = 1)<br/>X_test = pd.concat([X_test[['body_len', 'punct%']].reset_index(drop = True), pd.DataFrame(tfidf_test.toarray())], axis = 1)</span></pre><h2 id="0209" class="nh md it bd me of og dn mi oh oi dp mm li oj ok mo lm ol om mq lq on oo ms op bi translated">5.1.1随机森林</h2><p id="1c53" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" rel="noopener ugc nofollow" target="_blank"> RandomForestClassifier </a>是一种集成学习方法，它利用bagging来构建决策树集合，然后聚合每棵树的预测来确定最终预测。</p><p id="5214" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">RandomForestClassifier的关键超参数包括:</p><ul class=""><li id="57f0" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu nt nu nv nw bi translated"><strong class="lb iu"> max_depth: </strong>每个决策树的最大深度</li><li id="90cb" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">n_estimators: 要构建多少个并行决策树</li><li id="e234" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated"><strong class="lb iu">随机状态:</strong>用于再现性目的</li><li id="cc15" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated"><strong class="lb iu"> n_jobs: </strong>并行运行的作业数量</li></ul><p id="113a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们开始在数据上训练我们的随机森林模型之前，让我们首先使用嵌套的for循环构建一个手动网格搜索，以找到最佳的超参数集。</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="f718" class="nh md it nd b gy ni nj l nk nl">def explore_rf_params(n_est, depth):<br/>    rf = RandomForestClassifier(n_estimators = n_est, max_depth = depth, n_jobs = -1, random_state = 42)<br/>    rf_model = rf.fit(X_train, Y_train)<br/>    Y_pred = rf_model.predict(X_test)<br/>    precision, recall, fscore, support = score(Y_test, Y_pred, pos_label = 'spam', average = 'binary')<br/>    print(f"Est: {n_est} / Depth: {depth} ---- Precision: {round(precision, 3)} / Recall: {round(recall, 3)} / Accuracy: {round((Y_pred==Y_test).sum() / len(Y_pred), 3)}")<br/>    <br/>for n_est in [50, 100, 150]:<br/>    for depth in [10, 20, 30, None]:<br/>        explore_rf_params(n_est, depth)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/4cc20976f276b857d6757c0effc4faa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*11M2yANtKcWWtamQ-TtG4g.png"/></div></div></figure><p id="2837" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上面的输出，我们可以得出结论:</p><ul class=""><li id="b02b" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu nt nu nv nw bi translated">在所有情况下，精度都恒定为1</li><li id="67de" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">随着max_depth的增加，查全率和查准率都有所提高，但没有一个给出最好的结果</li><li id="2fc1" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">在第100棵树之后添加更多的树几乎没有改善，所以我们将设置n_estimators = 100。</li></ul><p id="8872" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们已经有了超参数，我们可以继续使我们的模型符合数据。</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="9176" class="nh md it nd b gy ni nj l nk nl"># Instantiate RandomForestClassifier with optimal set of hyperparameters <br/>rf = RandomForestClassifier(n_estimators = 100, max_depth = None, random_state = 42, n_jobs = -1)</span><span id="6f79" class="nh md it nd b gy oc nj l nk nl"># Fit model<br/>start = time.time()<br/>rf_model = rf.fit(X_train, Y_train)<br/>end = time.time()<br/>fit_time = end - start</span><span id="ac61" class="nh md it nd b gy oc nj l nk nl"># Predict <br/>start = time.time()<br/>Y_pred = rf_model.predict(X_test)<br/>end = time.time()<br/>pred_time = end - start</span><span id="d203" class="nh md it nd b gy oc nj l nk nl"># Time and prediction results<br/>precision, recall, fscore, support = score(Y_test, Y_pred, pos_label = 'spam', average = 'binary')<br/>print(f"Fit time: {round(fit_time, 3)} / Predict time: {round(pred_time, 3)}")<br/>print(f"Precision: {round(precision, 3)} / Recall: {round(recall, 3)} / Accuracy: {round((Y_pred==Y_test).sum() / len(Y_pred), 3)}")</span></pre><ul class=""><li id="1d22" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu nt nu nv nw bi translated"><strong class="lb iu">适合时间:</strong> 15.684</li><li id="100b" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated"><strong class="lb iu">预测时间:</strong> 0.312</li><li id="4e13" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated"><strong class="lb iu">精度:</strong> 1.0</li><li id="a6dd" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated"><strong class="lb iu">召回:</strong> 0.833</li><li id="3c5d" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated"><strong class="lb iu">精度:</strong> 0.978</li></ul><p id="7c54" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">或者，我们也可以使用混淆矩阵来可视化二元分类的结果。</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="a84a" class="nh md it nd b gy ni nj l nk nl"># Confusion matrix for RandomForestClassifier<br/>matrix = confusion_matrix(Y_test, Y_pred)<br/>sns.heatmap(matrix, annot = True, fmt = 'd')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/c8a7583dc772022c32ffe035155df4eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*XiLQcpyEHQTBliFtpPqh_A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">随机森林模型的混淆矩阵</p></figure><h2 id="59f0" class="nh md it bd me of og dn mi oh oi dp mm li oj ok mo lm ol om mq lq on oo ms op bi translated">梯度增强</h2><p id="ed32" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html" rel="noopener ugc nofollow" target="_blank">另一方面，GradientBoostingClassifier </a>也是一种集成学习方法，它采用一种称为bagging的迭代方法，通过关注先前迭代的错误来组合弱学习器以创建强学习器。</p><p id="a0d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">GradientBoostingClassifier的关键超参数包括:</p><ul class=""><li id="f14a" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu nt nu nv nw bi translated"><strong class="lb iu"> learning_rate: </strong>每个顺序树在最终预测上的权重</li><li id="c81b" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated"><strong class="lb iu"> max_depth: </strong>每个决策树的最大深度</li><li id="a54b" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated"><strong class="lb iu"> n_estimators: </strong>顺序树的数量</li><li id="8122" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated"><strong class="lb iu">随机_状态:</strong>出于再现性目的</li></ul><p id="a0f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不幸的是，梯度推进的网格搜索需要很长时间，所以我决定现在坚持使用默认的超参数。</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="1b11" class="nh md it nd b gy ni nj l nk nl"># Instantiate GradientBoostingClassifier<br/>gb = GradientBoostingClassifier(random_state = 42)</span><span id="e41e" class="nh md it nd b gy oc nj l nk nl"># Fit model<br/>start = time.time()<br/>gb_model = gb.fit(X_train, Y_train)<br/>end = time.time()<br/>fit_time = end - start</span><span id="9f4e" class="nh md it nd b gy oc nj l nk nl"># Predict <br/>start = time.time()<br/>Y_pred = gb_model.predict(X_test)<br/>end = time.time()<br/>pred_time = end - start</span><span id="9a3c" class="nh md it nd b gy oc nj l nk nl"># Time and prediction results<br/>precision, recall, fscore, support = score(Y_test, Y_pred, pos_label = 'spam', average = 'binary')<br/>print(f"Fit time: {round(fit_time, 3)} / Predict time: {round(pred_time, 3)}")<br/>print(f"Precision: {round(precision, 3)} / Recall: {round(recall, 3)} / Accuracy: {round((Y_pred==Y_test).sum() / len(Y_pred), 3)}")</span></pre><ul class=""><li id="7a13" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu nt nu nv nw bi translated"><strong class="lb iu">拟合时间:</strong> 262.863</li><li id="5702" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">预测时间: 0.622</li><li id="0989" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated"><strong class="lb iu">精度:</strong> 0.953</li><li id="c07b" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated"><strong class="lb iu">召回:</strong> 0.813</li><li id="fa64" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated"><strong class="lb iu">精度:</strong> 0.97</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/e10a59de847fa3ba1584526684a04398.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*TgWK0wog1LO_rxz8b9aV2g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">梯度推进模型的混淆矩阵</p></figure><h2 id="2241" class="nh md it bd me of og dn mi oh oi dp mm li oj ok mo lm ol om mq lq on oo ms op bi translated">5.2管道</h2><p id="eb58" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">管道将机器学习工作流中的多个步骤链接在一起，其中每个步骤的输出被用作下一个步骤的输入。</p><pre class="kj kk kl km gt nc nd ne nf aw ng bi"><span id="7624" class="nh md it nd b gy ni nj l nk nl"># Instantiate TfidfVectorizer, RandomForestClassifier and GradientBoostingClassifier <br/>tfidf_vect = TfidfVectorizer(analyzer = clean_text)<br/>rf = RandomForestClassifier(random_state = 42, n_jobs = -1)<br/>gb = GradientBoostingClassifier(random_state = 42)</span><span id="6167" class="nh md it nd b gy oc nj l nk nl"># Make columns transformer<br/>transformer = make_column_transformer((tfidf_vect, 'body_text'), remainder = 'passthrough')</span><span id="2107" class="nh md it nd b gy oc nj l nk nl"># Build two separate pipelines for RandomForestClassifier and GradientBoostingClassifier <br/>rf_pipeline = make_pipeline(transformer, rf)<br/>gb_pipeline = make_pipeline(transformer, gb)</span><span id="e460" class="nh md it nd b gy oc nj l nk nl"># Perform 5-fold cross validation and compute mean score <br/>rf_score = cross_val_score(rf_pipeline, data[['body_text', 'body_len', 'punct%']], data.label, cv = 5, scoring = 'accuracy', n_jobs = -1)<br/>gb_score = cross_val_score(gb_pipeline, data[['body_text', 'body_len', 'punct%']], data.label, cv = 5, scoring = 'accuracy', n_jobs = -1)<br/>print(f"Random forest score: {round(mean(rf_score), 3)}")<br/>print(f"Gradient boosting score: {round(mean(gb_score), 3)}")</span></pre><ul class=""><li id="a0c8" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu nt nu nv nw bi translated"><strong class="lb iu">随机森林得分:</strong> 0.973</li><li id="61d2" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated"><strong class="lb iu">梯度推进得分:</strong> 0.962</li></ul><h2 id="4feb" class="nh md it bd me of og dn mi oh oi dp mm li oj ok mo lm ol om mq lq on oo ms op bi translated">5.3关于建模的总结想法</h2><p id="eb6e" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">虽然在这个特定示例中，两个模型都返回了非常相似的预测结果，但重要的是要记住在其他情况下可能会出现的权衡。更具体地说，值得考虑业务环境和构建模型的总体目的。</p><p id="01ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，在垃圾邮件分类中，最好优化精确度，因为我们可能会处理收件箱中的一些垃圾邮件，但我们绝对不希望我们的模型将重要的邮件分类为垃圾邮件。相比之下，在欺诈检测中，更好的做法是优化召回，因为如果我们的模型未能识别出真正的威胁(假阴性)，其成本将高于识别出假威胁(假阳性)的成本。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="03ff" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">结论</h1><p id="d2d4" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">总之，在本文中，我们研究了一个端到端的自然语言处理(NLP)项目，该项目涉及构建一个能够将给定文本消息分类为垃圾邮件或垃圾邮件的二元分类器。</p><p id="0b9d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们从探索数据集开始，接着是特征工程，我们创建了两个新特征:<em class="mz"> body_len </em>和<em class="mz"> punct% </em>。然后，我们继续进行特定于NLP工作流的几个预处理步骤，例如:</p><ul class=""><li id="bb04" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu nt nu nv nw bi translated">将单词转换成小写字母</li><li id="46e8" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">删除标点符号和停用词</li><li id="fff6" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">符号化</li><li id="f0c3" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">词干与词汇匹配(文本规范化)</li></ul><p id="c859" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">之后，我们使用执行矢量化，以便对文本进行编码，并将其转换为机器学习的特征向量。最后，我们构建了两个独立的预测模型，随机森林和梯度推进，并比较了它们各自的准确性和整体模型性能。</p><p id="567e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">也就是说，非常感谢您的阅读，我期待在我的下一篇文章中见到您！</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="1913" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">跟我来</h1><ul class=""><li id="9d7d" class="no np it lb b lc mu lf mv li oz lm pa lq pb lu nt nu nv nw bi translated"><a class="ae ky" href="http://www.twitter.com/chongjason914" rel="noopener ugc nofollow" target="_blank">推特</a></li><li id="09f8" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated"><a class="ae ky" href="http://www.linkedin.com/in/chongjason914" rel="noopener ugc nofollow" target="_blank">领英</a></li><li id="b3ac" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated"><a class="ae ky" href="http://www.medium.com/@chongjason" rel="noopener">中等</a></li><li id="38b2" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated"><a class="ae ky" href="http://www.youtube.com/jasonchong914" rel="noopener ugc nofollow" target="_blank"> YouTube </a></li></ul><div class="pc pd gp gr pe pf"><a rel="noopener follow" target="_blank" href="/i-finally-got-a-data-science-job-465ef457cab6"><div class="pg ab fo"><div class="ph ab pi cl cj pj"><h2 class="bd iu gy z fp pk fr fs pl fu fw is bi translated">我终于找到了一份数据科学的工作</h2><div class="pm l"><h3 class="bd b gy z fp pk fr fs pl fu fw dk translated">我在澳大利亚一家大型数据科学公司面试全职职位的经历</h3></div><div class="pn l"><p class="bd b dl z fp pk fr fs pl fu fw dk translated">towardsdatascience.com</p></div></div><div class="po l"><div class="pp l pq pr ps po pt ks pf"/></div></div></a></div><div class="pc pd gp gr pe pf"><a href="https://medium.com/geekculture/70-data-science-interview-questions-you-need-to-know-before-your-next-technical-interview-ccfbd37a37b3" rel="noopener follow" target="_blank"><div class="pg ab fo"><div class="ph ab pi cl cj pj"><h2 class="bd iu gy z fp pk fr fs pl fu fw is bi translated">下一次技术面试前你需要知道的70个数据科学面试问题</h2><div class="pm l"><h3 class="bd b gy z fp pk fr fs pl fu fw dk translated">概率和统计，机器学习算法，脑筋急转弯等等</h3></div><div class="pn l"><p class="bd b dl z fp pk fr fs pl fu fw dk translated">medium.com</p></div></div><div class="po l"><div class="pu l pq pr ps po pt ks pf"/></div></div></a></div></div></div>    
</body>
</html>