<html>
<head>
<title>Random Forests Algorithm explained with a real-life example and some Python code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用一个真实的例子和一些Python代码解释了随机森林算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/random-forests-algorithm-explained-with-a-real-life-example-and-some-python-code-affbfa5a942c?source=collection_archive---------2-----------------------#2021-07-12">https://towardsdatascience.com/random-forests-algorithm-explained-with-a-real-life-example-and-some-python-code-affbfa5a942c?source=collection_archive---------2-----------------------#2021-07-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6950" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">随机森林是一种机器学习算法，它解决了<a class="ae kf" rel="noopener" target="_blank" href="/decision-tree-classifier-explained-in-real-life-picking-a-vacation-destination-6226b2b60575">决策树</a> : <strong class="ak">方差</strong>的最大问题之一。</h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/3266274b0302aaa7905612ed33a58f29.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LW04M0kPmgtY0xbhOqRJZw.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">图片作者。</p></figure><p id="8323" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">这是致力于基于树的算法系列的第二篇文章，基于树的算法是一组广泛使用的监督机器学习算法。</em></p><p id="70f7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">第一篇是关于</em> <a class="ae kf" rel="noopener" target="_blank" href="/decision-tree-classifier-explained-in-real-life-picking-a-vacation-destination-6226b2b60575"> <em class="ls">决策树</em> </a> <em class="ls">。下一篇，也是本系列的最后一篇文章，探索梯度提升决策树。一切都用现实生活中的例子和一些Python代码来解释。</em></p><p id="3195" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">敬请期待！</em></p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><p id="ff97" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">随机森林是一种机器学习算法，它解决了<a class="ae kf" rel="noopener" target="_blank" href="/decision-tree-classifier-explained-in-real-life-picking-a-vacation-destination-6226b2b60575">决策树</a> : <strong class="ky ir">方差</strong>的最大问题之一。</p><p id="079b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">尽管决策树简单灵活，但它是<a class="ae kf" href="https://en.wikipedia.org/wiki/Greedy_algorithm" rel="noopener ugc nofollow" target="_blank">贪婪算法</a>。它侧重于优化手头的节点分裂，而不是考虑分裂如何影响整个树。贪婪的方法使决策树运行得更快，但也容易过度拟合。</p><p id="7057" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">过度拟合树被高度优化以预测训练数据集中的值，从而产生具有<strong class="ky ir">高方差</strong>的学习模型。</p><p id="bf10" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如何在决策树中计算方差取决于您正在解决的问题。</p><p id="1111" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在回归任务中，您可以计算预测与真实目标相比的实际<a class="ae kf" href="https://en.wikipedia.org/wiki/Variance" rel="noopener ugc nofollow" target="_blank">差异</a>。如果树产生的结果与它的真实目标相差太远，那么它具有高方差，因此它是过度拟合的。</p><blockquote class="ma mb mc"><p id="6433" class="kw kx ls ky b kz la jr lb lc ld ju le md lg lh li me lk ll lm mf lo lp lq lr ij bi translated">高度过度拟合的树具有高方差。这意味着它的预测与实际目标相去甚远。</p></blockquote><p id="a53b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是在分类任务中，你不能使用同样的方法。检测决策树是否过拟合的方法是查看测试错误。如果树有很高的测试误差，意味着它不擅长对没有被训练过的观察值进行分类，那么它是过度拟合的。</p><blockquote class="ma mb mc"><p id="1f00" class="kw kx ls ky b kz la jr lb lc ld ju le md lg lh li me lk ll lm mf lo lp lq lr ij bi translated">在回归任务中，通过高方差来检测过度拟合，而在分类任务中，通过高泛化误差来检测过度拟合。</p></blockquote><p id="9b7d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了解决过度拟合，并减少决策树的差异，<a class="ae kf" href="https://en.wikipedia.org/wiki/Leo_Breiman" rel="noopener ugc nofollow" target="_blank"> Leo Breiman </a>开发了<a class="ae kf" href="https://en.wikipedia.org/wiki/Random_forest" rel="noopener ugc nofollow" target="_blank">随机森林</a>算法【1】。这是一种创新的算法，因为它首次利用了<a class="ae kf" href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)" rel="noopener ugc nofollow" target="_blank">自举</a>的统计技术，并将多个模型的训练结果合并成一个更强大的学习模型。</p><p id="773e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是，在您看到随机森林的运行和代码之前，让我们绕道探索一下随机森林的独特之处。</p><h1 id="7d2f" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">Bagging:引导聚合</h1><p id="59b2" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated"><strong class="ky ir"> Bagging </strong>，是<a class="ae kf" href="https://en.wikipedia.org/wiki/Bootstrap_aggregating" rel="noopener ugc nofollow" target="_blank"> Bootstrap Aggregation </a>的简称，是Leo Breiman开发的一种技术，目标是减少一个学习模型的方差。Bagging也是模型不可知的，所以不管你使用什么类型的模型，过程都是一样的。</p><p id="6d6a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Bagging的<strong class="ky ir">引导</strong>部分指的是从数据集[3]中抽取几个随机样本进行替换的重采样方法。因此，Bootstrapping会从同一分布中创建多个更小的随机数据集。</p><p id="af86" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每个自举数据集用于训练一个模型，然后输出被<strong class="ky ir">聚集</strong>成一个最终结果。</p><p id="5332" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是聚合也意味着不同的东西，这取决于你正在解决的问题的类型。</p><p id="e9fa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当你在处理一个回归问题时，聚合意味着对所有模型的每个观察结果进行平均。而在分类中，聚合意味着为每个观察选择最常见的类，就像进行<em class="ls">多数投票</em>。</p><p id="5e97" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这令人印象深刻<em class="ls"> </em>但是，<em class="ls">这实际上如何帮助减少模型方差呢？</em></p><p id="f3b2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每个模型都在不同的数据集上训练，因为它们是引导的。所以不可避免地，每个模型都会犯<em class="ls">不同的</em>错误，并且有明显的误差和方差。误差和方差都在聚合步骤中减少，在回归的情况下，它们被平均化。</p><p id="cbf1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">而且因为你最终得到的是一个单一的模型，它结合了多个模型的输出，Bagging被称为<strong class="ky ir">集成技术</strong>。</p><h1 id="156c" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">拔靴带</h1><p id="8b80" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">有了自举，你就遵循了<em class="ls">少花钱多办事的座右铭</em>。你把一个数据集乘以几个更小的随机数据集。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nd"><img src="../Images/97a4799b552d448dce754d5e392e1007.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*freUFRJ68pfjQjkw40F1LQ.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">启动数据集的代码示例。</p></figure><p id="24c6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最初的数据集只有10个元素，但是最终产生了5个大小为4的采样数据集。</p><p id="6612" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在某种程度上，<strong class="ky ir">乘以了数据集</strong>，当你合计单个自举数据集时，从仅仅10个观测值增加到4x5=20个观测值。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/38195840c906ceded0c796d34bacd3a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/format:webp/1*bpikOxdjZAm_8qpEn5dd1Q.png"/></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><em class="nf">引导数据集的结果，生成5个样本，每个样本包含4个元素。</em></p></figure><p id="a960" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">自举之所以有效，是因为你在用 <strong class="ky ir">替换</strong><strong class="ky ir">采样</strong> <strong class="ky ir">。</strong></p><p id="13fb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可以<em class="ls">多次选择</em>相同的数据点，就像在最后两个样本中一样，但是每个采样数据集与前一个略有不同。</p><p id="c292" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果没有替换，您将很快用完数据点，并且只能生成有限数量的样本。</p><p id="5b36" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这次绕道的重点是随机森林的独特之处。</p><p id="dcb4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们回到主题，<em class="ls">随机森林如何减少模型方差。</em></p><h1 id="2ce7" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">随机森林</h1><p id="aec6" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">随机森林是专门为解决决策树中的高方差问题而开发的。顾名思义，你不是在训练一个单独的决策树，而是在训练整个森林！在这种情况下，一个袋装决策树的森林。</p><p id="8a61" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在高层次上，在<em class="ls">伪代码</em>中，随机森林算法遵循以下步骤:</p><ol class=""><li id="3052" class="ng nh iq ky b kz la lc ld lf ni lj nj ln nk lr nl nm nn no bi translated">获取原始数据集并创建大小为<em class="ls"> n </em>的<em class="ls"> N </em>个袋装样本，其中<em class="ls"> n </em>小于原始数据集。</li><li id="88b6" class="ng nh iq ky b kz np lc nq lf nr lj ns ln nt lr nl nm nn no bi translated">使用每个<em class="ls"> N个</em>袋装数据集作为输入来训练决策树。但是，在进行结点分割时，不要浏览数据集中的所有要素。从训练集中的所有特征中随机选择一个较小的数，<em class="ls"> M </em>个特征。然后使用杂质指标，比如基尼系数或熵，挑选出最佳分割。</li><li id="c84a" class="ng nh iq ky b kz np lc nq lf nr lj ns ln nt lr nl nm nn no bi translated">将各个决策树的结果汇总到一个输出中。</li><li id="2a87" class="ng nh iq ky b kz np lc nq lf nr lj ns ln nt lr nl nm nn no bi translated">如果您正在进行回归任务，请对每棵树生成的每个观察值进行平均。</li><li id="d373" class="ng nh iq ky b kz np lc nq lf nr lj ns ln nt lr nl nm nn no bi translated">如果你正在做一个分类任务，对每一个观察，在所有的树上进行多数投票。</li></ol><p id="908b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然随机森林的<em class="ls">森林</em>部分指的是训练多棵树，但是<em class="ls">随机</em>部分出现在算法中的两个不同点。</p><p id="2f7f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">装袋过程中存在随机性。但是，您也可以选择一个随机的要素子集来评估结点分裂。这保证了每棵树都是不同的，因此，确保了每个模型产生的结果略有不同。</p><p id="56d5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然您可能认为在每次分割时随机采样特性会引入另一个您可能需要调整的<a class="ae kf" href="https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)" rel="noopener ugc nofollow" target="_blank">超参数</a>，但事实并非如此。模特帮你打理！</p><p id="7c5c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您当然可以调整这个超参数。然而，对于随机选取等于数据集中可用要素总数的平方根的多个要素，存在数学共识[2]。</p><h1 id="373d" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">为什么使用随机森林？</h1><p id="d2a2" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">与训练单个决策树相比，随机森林有几个优点。</p><h2 id="012d" class="nu mh iq bd mi nv nw dn mm nx ny dp mq lf nz oa ms lj ob oc mu ln od oe mw of bi translated">决策树的所有优点，但更强大</h2><p id="58dd" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">这个算法的核心是一个决策树，所以随机森林分享了它的所有优点。</p><p id="f4cf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是一个数据健壮的算法，能够处理不同类型的数据，并且不需要任何数据预处理。</p><p id="2963" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">随机森林的真正潜力来自于组合不同决策树的结果。</p><h2 id="9080" class="nu mh iq bd mi nv nw dn mm nx ny dp mq lf nz oa ms lj ob oc mu ln od oe mw of bi translated">每个型号都不一样</h2><p id="7c59" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">像决策树一样，该算法针对局部分裂进行优化。但是，它不是探索数据集中每个要素的所有可能分割，而是随机选取这些要素的子集。</p><p id="9475" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这减少了算法在每次分割时需要评估的结果的数量，并且使得每个训练的树略有不同。</p><h2 id="990d" class="nu mh iq bd mi nv nw dn mm nx ny dp mq lf nz oa ms lj ob oc mu ln od oe mw of bi translated">不需要维持集</h2><p id="450c" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">在机器学习中，你通常将数据集分成训练集和测试集，努力用它从未见过的观察来评估模型性能。</p><p id="a21e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当数据集很小，或者收集更多数据的成本和工作量很高时，这就成了一个具有挑战性的问题。</p><p id="e262" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是对于随机森林，您可以使用整个数据集来训练和评估模型。装袋过程会帮你搞定的！由于您正在生成N个较小的随机数据集，这些数据集是通过替换选取的，因此总会有一组点没有用于创建树。</p><p id="302a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">看看前面的例子，它引导了数据集[1，2，3，4，5，6，7，8，9，10]。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/38195840c906ceded0c796d34bacd3a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/format:webp/1*bpikOxdjZAm_8qpEn5dd1Q.png"/></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><em class="nf">引导数据集的结果，生成5个样本，每个样本包含4个元素。</em></p></figure><p id="2403" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你要使用这些样本中的每一个来训练一个决策树，有一些观察结果这些树根本<em class="ls">不知道，</em>因为它们不是它们训练集的一部分。</p><p id="380d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于随机森林，您可以使用整个数据集来训练模型，并直接根据其结果计算测试误差。你不需要仅仅为了测试而留出数据集的一部分。</p><p id="cb78" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种新类型的误差直接从训练树的结果中计算出来，被称为<a class="ae kf" href="https://en.wikipedia.org/wiki/Out-of-bag_error" rel="noopener ugc nofollow" target="_blank">出袋误差</a>。</p><h1 id="b674" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">🛩🏝用随机森林减少模型方差</h1><p id="eb42" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">计划假期很有挑战性。这一次，你将把你的机器学习技能付诸实践，并获得关于你下一个度假目的地的算法意见。</p><p id="8953" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每当你开始计划假期时，你总是会考虑到:</p><ul class=""><li id="1bc6" class="ng nh iq ky b kz la lc ld lf ni lj nj ln nk lr og nm nn no bi translated">假期的持续时间，</li><li id="b9e7" class="ng nh iq ky b kz np lc nq lf nr lj ns ln nt lr og nm nn no bi translated">个人预算，</li><li id="4311" class="ng nh iq ky b kz np lc nq lf nr lj ns ln nt lr og nm nn no bi translated">天气预报，</li><li id="aba0" class="ng nh iq ky b kz np lc nq lf nr lj ns ln nt lr og nm nn no bi translated">如果你的大家庭要加入，</li><li id="5df1" class="ng nh iq ky b kz np lc nq lf nr lj ns ln nt lr og nm nn no bi translated">如果你喜欢冒险，想要探索新的地方。</li></ul><p id="3b7a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">据说决策树是模仿人类做决定的方式。但是正如你在本系列的前一篇文章<a class="ae kf" rel="noopener" target="_blank" href="/decision-tree-classifier-explained-in-real-life-picking-a-vacation-destination-6226b2b60575">中看到的，一个单独的决策树并不是一个非常强大的预测器。</a></p><p id="c7de" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">随机森林是对决策树的改进，但是它真的好很多吗？</p><p id="d52f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">只有一个办法可以知道！</p><p id="bd01" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是首先，再看一下单个决策树的性能。</p><h1 id="62a3" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">单个决策树的测试错误</h1><p id="0ac0" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">如果只训练一个决策树，测试误差大约为70%。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi oh"><img src="../Images/0d59ac7e8d4600ea7754b704222d39d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tc9muYY5AEDANN0G0Cu_NA.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">用于训练单个决策树来预测您的下一个度假目的地的代码示例。</p></figure><p id="2712" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">测试误差，也称为泛化误差，相当高。高方差的标志。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/e07ccea262016af5577c2f1e5de6c9d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*E8ADqor8csxvSPQ7r3EE0g.png"/></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">决策树模型的测试错误。</p></figure><p id="4318" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">好吧，你被说服了！决策树绝对不是帮助你选择下一个度假目的地的最佳算法。</p><p id="c0d6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们使用<a class="ae kf" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" rel="noopener ugc nofollow" target="_blank"> ScikitLearn </a>，完全<em class="ls">开箱即用</em>并且没有任何超参数调整来训练一个随机森林模型。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi oj"><img src="../Images/9a74a40f1e7edd15a895b8fe885b4957.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HiDDktOqf-mORomW8_SPCw.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">代码来训练一个随机的森林来预测你的下一个度假目的地。</p></figure><p id="8a29" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">随机森林立即减少了30%的测试误差！</p><p id="d17d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与单个决策树的性能相比，这是一个令人印象深刻的结果。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/63070f48fc9b91bfa7a371310fcebffa.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*KIcGngSsqEDnpBPxa0IuxQ.png"/></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">随机森林模型的出袋误差，即测试误差。</p></figure><p id="f1bf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你注意到的一件事是，默认情况下，该算法训练并结合了100袋树木的结果。这是您可以在<a class="ae kf" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" rel="noopener ugc nofollow" target="_blank">随机森林</a>中调整的众多超参数之一。</p><p id="890a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">好奇心刚刚爆发了！</p><p id="3824" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">为什么是100棵树？</em></p><p id="7e62" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">使用不同数量的树，算法的性能会更好还是更差？</em></p><p id="ff8f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">可以减少多少泛化误差？</em></p><h1 id="bfc5" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">找出树木的最佳数量</h1><p id="fca9" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">您没有通过试凑法来调整这个超参数，而是决定将<em class="ls">超出误差</em>绘制为随机森林中树木数量的函数。</p><p id="00ae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你决定训练任意数量的150棵树。但是你开始只有5个，慢慢地增加到150个，建立一个新的模型，每次增加一棵树。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ol"><img src="../Images/86f0303a114544653d857add75fe69fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JTu9Y7K9aszT3uWF3BNTpA.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">将袋外误差绘制为随机森林中的树木数量的代码示例。</p></figure><p id="4a37" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">训练一大片森林并不能改善模型。</p><p id="a1c8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这个过程中，你已经为你的任务找到了性能最佳点。你只需要随机森林中的8棵树就可以将泛化误差降低到33%。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi om"><img src="../Images/76c36df99324413cc32158e1e7057b83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TUFuc-2eZgUxNbS6N-Qbtw.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">绘制作为随机森林中树木数量的函数的<em class="nf">出袋误差</em>的输出。</p></figure><p id="9c5c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">看一下这个图，很容易看出超出8棵树的<em class="ls">最佳点</em>后，这个误差变得越来越大。然后，当<em class="ls">森林</em>大于50棵树时，模型达到了<em class="ls">性能稳定期</em>。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi on"><img src="../Images/fd68f3d636fa894bacf97b3ae2cffe84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tb_bMkLktXeMl87b_kdRDA.png"/></div></div></figure><h1 id="f838" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">结论</h1><p id="18ca" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">您成功降低了模型的方差！</p><p id="45c2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，您可以更加自信地使用随机森林来预测您的下一个度假目的地。而不是训练一个单独的决策树。</p><p id="4265" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是一个简单的例子，但是你可以看到泛化误差显著下降，从67%下降到33%。</p><p id="3453" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">随机森林是一个强大的算法，但它有一个明显的缺点。你无法<em class="ls">看到</em>模型如何决策。</p><p id="e1ae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">决策树的一个很酷的方面是，在模型被训练之后，你实际上可以可视化模型如何做出决策。有了一棵足够小的树，你可以追踪整个<em class="ls">决策</em>过程，以获得新的观察结果。</p><blockquote class="ma mb mc"><p id="4f96" class="kw kx ls ky b kz la jr lb lc ld ju le md lg lh li me lk ll lm mf lo lp lq lr ij bi translated">但是对于随机森林，您需要牺牲可解释性来换取性能。</p></blockquote><p id="a9ae" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一旦你开始组合多棵树的决策，决策过程<em class="ls">就会变得更加复杂，并且难以想象。</em></p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><p id="b8bb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">希望你喜欢学习随机森林，以及为什么它比<a class="ae kf" rel="noopener" target="_blank" href="/decision-tree-classifier-explained-in-real-life-picking-a-vacation-destination-6226b2b60575">决策树</a>更强大。</p><p id="9888" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">敬请期待本系列的下一篇也是最后一篇文章！这是关于梯度增强决策树的。用一个真实的例子和一些Python代码来解释。</em></p><h1 id="01cc" class="mg mh iq bd mi mj mk ml mm mn mo mp mq jw mr jx ms jz mt ka mu kc mv kd mw mx bi translated">参考</h1><ol class=""><li id="57f5" class="ng nh iq ky b kz my lc mz lf oo lj op ln oq lr nl nm nn no bi translated">随机森林。<em class="ls">机器学习</em> <strong class="ky ir"> 45，</strong>5–32(2001)</li><li id="163b" class="ng nh iq ky b kz np lc nq lf nr lj ns ln nt lr nl nm nn no bi translated">加雷斯·詹姆斯，丹妮拉·威滕，特雷弗·哈斯蒂，罗伯特·蒂布拉尼。(2013).统计学习导论:在r .纽约的应用</li><li id="4fae" class="ng nh iq ky b kz np lc nq lf nr lj ns ln nt lr nl nm nn no bi translated">Breiman，L. <a class="ae kf" href="https://link.springer.com/article/10.1007/BF00058655" rel="noopener ugc nofollow" target="_blank">装袋预测器</a>。<em class="ls">马赫学</em>24、123–140(1996)</li></ol></div></div>    
</body>
</html>