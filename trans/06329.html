<html>
<head>
<title>Node2vec explained graphically</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Node2vec用图形解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/node2vec-explained-graphically-749e49b7eb6b?source=collection_archive---------10-----------------------#2021-06-07">https://towardsdatascience.com/node2vec-explained-graphically-749e49b7eb6b?source=collection_archive---------10-----------------------#2021-06-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="5764" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/getting-started" rel="noopener" target="_blank">入门</a></h2><div class=""/><div class=""><h2 id="7875" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">图形上的二阶随机游走是如何工作的，通过动画解释</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/61cf927da7747e620bff2b1e14d47fcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*afSKTD2f-z3ElUelMCvzkA.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">来自<a class="ae le" href="https://pixabay.com/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>的<a class="ae le" href="https://pixabay.com/users/thedigitalartist-202249/" rel="noopener ugc nofollow" target="_blank">数码师</a>的照片</p></figure><p id="cef2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">Node2vec是一种嵌入方法，将图形(或网络)转换成数字表示[1]。例如，给定一个社交网络，其中人(节点)通过关系(边)进行交互，node2vec生成数字表示，即数字列表，来表示每个人。这种表示在某种意义上保留了原始网络的结构，因此关系密切的人具有相似的表示，反之亦然。在本文中，我们将通过一系列动画来直观地了解<em class="mb"> node2vec </em>方法，特别是二阶随机游走在图上是如何工作的。</p><h1 id="a8dc" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">TL；速度三角形定位法(dead reckoning)</h1><ul class=""><li id="f3a3" class="mu mv iq lh b li mw ll mx lo my ls mz lw na ma nb nc nd ne bi translated"><em class="mb"> Node2vec </em>通过<strong class="lh ja">二阶(有偏)随机游走</strong>生成图中节点的数字表示。</li><li id="ddc4" class="mu mv iq lh b li nf ll ng lo nh ls ni lw nj ma nb nc nd ne bi translated">一阶随机游走是通过沿着图的边对图上的节点进行采样来完成的，并且每一步仅<em class="mb">依赖于<strong class="lh ja">当前状态</strong>。</em></li><li id="561b" class="mu mv iq lh b li nf ll ng lo nh ls ni lw nj ma nb nc nd ne bi translated">二阶随机游走是一阶随机游走的修改版本，它不仅依赖于<strong class="lh ja">当前状态</strong>，还依赖于<strong class="lh ja">先前状态</strong>。</li><li id="abe8" class="mu mv iq lh b li nf ll ng lo nh ls ni lw nj ma nb nc nd ne bi translated">使用网络中的每个节点作为起点来生成随机行走的语料库。这个语料库然后通过<em class="mb"> word2vec </em>生成最终的节点嵌入。</li></ul><h1 id="ec25" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">一阶随机行走</h1><p id="787a" class="pw-post-body-paragraph lf lg iq lh b li mw ka lk ll mx kd ln lo nk lq lr ls nl lu lv lw nm ly lz ma ij bi translated">图上的随机行走可以被认为是沿着图的边遍历图的“行走者”。在每一步，步行者需要决定下一步去哪里，然后移动到下一个州。这个过程被称为<strong class="lh ja">一跳转换</strong>。让我们看一个简单的例子。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nn"><img src="../Images/d38b126565bdfb798bea1c523160cdd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s-qiljoVUUXrC02fl-sP-A.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">一阶转移概率的例子。(图片由作者提供)</p></figure><p id="6994" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这个例子中，让我们假设walker当前在v上，它有三个相邻的节点:u1、u2和u3。每个邻居以不同的权重连接到v，如图所示。这些权重用于确定选择下一步的概率。具体地，通过归一化边缘权重来计算<strong class="lh ja">转移概率</strong> <em class="mb"> </em>:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi no"><img src="../Images/0af3c703cfd9fc4cf24d18c9a8c626bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*a_EnpHkFeEmCWZLebZLzCg.png"/></div></figure><p id="ed41" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">其中N_v是v的相邻节点的集合，在本例中是u1、u2和u3，d(v)是节点v的度。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi np"><img src="../Images/bf0da46d36883c88f48370e9cf48a487.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/1*tILPBHrXvuR2iLUO2lEBgA.gif"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="22dc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">快速计算后，我们看到从v到u1、u2和u3的转移概率分别为0.4、0.5和0.1。根据大数定律，我们知道，如果我们使用指定的转移概率抽取足够多的样本(即，在给定当前状态v的情况下选择下一个状态)，平均转移应该接近期望值。条形图动画显示了一百次在给定v处的当前状态的情况下选取下一个状态的示例。可以看出，每个相邻节点被选取的次数百分比分别收敛到期望的概率0.4、0.5和0.1。</p><p id="dc84" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">既然我们已经了解了如何通过转移概率实现一跳转移，我们可以继续进行<strong class="lh ja">随机漫步</strong>生成。简而言之，通过执行多个1跳转换来生成随机游走，其中前一个1跳转换确定当前状态。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nq"><img src="../Images/581a2d6414fbea793ac0ef3d876a46d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*qKUw21HrfmQf5F8FcJgFMQ.gif"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图中从节点“a”开始的随机游走(图片由作者提供)</p></figure><p id="8200" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">上面的动画显示了一个随机漫步的例子，它在一个有12个节点(节点“a”到节点“l”)的图上进行，从节点“a”开始。在每次迭代中，遍历器通过一跳转换前进到下一个节点。随机行走过程继续，直到它达到预定义的“walk_length ”,或者行走者到达有向图的“死胡同”,即，节点没有外出边。在这个例子中，生成的随机游走是节点序列:[a，b，e，h，e，c，d，I，…]</p><p id="a24b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">到目前为止，我们所看到的被称为<strong class="lh ja">一阶</strong>随机行走，这意味着1跳跃迁只取决于当前状态。然而，<em class="mb"> node2vec </em>执行<strong class="lh ja">二阶</strong>随机漫步，这是一个稍微修改的版本，包含了来自上一步的信息。</p><h1 id="38a1" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">二阶有偏行走</h1><p id="c447" class="pw-post-body-paragraph lf lg iq lh b li mw ka lk ll mx kd ln lo nk lq lr ls nl lu lv lw nm ly lz ma ij bi translated">二阶转换不是查看当前状态的直接邻居，而是根据前一状态应用偏差因子α来重新加权边权重。特别地，alpha是一个将当前状态和潜在的下一个状态作为输入的函数。在第一种情况下，如果两个状态(节点)没有连接，那么alpha设置为1/q，其中q是<strong class="lh ja">输入输出参数</strong>。因此，可以通过指定小的q值来增加“向外”行走的概率(意味着行走不被限制在局部邻域中)，或者相反地通过设置大的q值来将行走限制在局部邻域中。在第二种情况下，如果两个状态相同，意味着行走“返回”到先前的状态，那么alpha被设置为1/p，其中p是<strong class="lh ja">返回</strong>参数。最后，如果两个状态不相同并且是连接的，那么alpha被设置为1。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nr"><img src="../Images/fb2e19496344e2e93b7513b57d12996d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bb4yGU1NdM_uw2NvVsSgkg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">偏差因素。(<a class="ae le" href="https://arxiv.org/abs/1607.00653" rel="noopener ugc nofollow" target="_blank">格罗弗和莱斯科维奇，2016 </a>)</p></figure><p id="824a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">回到我们之前关于1跳转换的例子，假设u2是之前的状态，下图用红色突出显示了<em class="mb">返回</em>边沿，用黄色突出显示了<em class="mb">进出</em>边沿。偏置因子α被相应地分配:1/q用于<em class="mb">输入输出</em>(黄色)，1/p用于<em class="mb">返回</em>(红色)，否则为1。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ns"><img src="../Images/e18152d901a5c205ad56bf5451b44fd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hdhB2HLkkA9toVjRVbScBw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">返回(红色)和输入输出(黄色)边沿的偏置系数示例。(图片由作者提供)</p></figure><p id="8bf7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">有了这些偏差因子，我们可以继续计算<strong class="lh ja">二阶跃迁概率</strong>，如下所示，这与我们在上面看到的一阶跃迁非常相似，但现在每个权重都有α偏差。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/d468f5548a8aca9e51afac0717ed6386.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*YUm_yYKYHA9oOlCg9Zj-OA.png"/></div></figure><p id="7238" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如前所述，可以使用二阶转移概率生成<strong class="lh ja">二阶随机游走</strong>。在下面的动画中，<em class="mb">返回</em>和<em class="mb">进出</em>边缘分别用红色和黄色突出显示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nq"><img src="../Images/d15b06f9bf17422fb363d07b25dbc986.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*GbZk_M_HqCu8Y99J_FzhQw.gif"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">二阶随机游走、返回和进出边的示例分别用红色和黄色突出显示(图片由作者提供)</p></figure><h1 id="f71b" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">来自随机漫步的节点嵌入</h1><p id="7b67" class="pw-post-body-paragraph lf lg iq lh b li mw ka lk ll mx kd ln lo nk lq lr ls nl lu lv lw nm ly lz ma ij bi translated">到目前为止，我们看到的随机行走都是从单个节点开始的。想象一下，使用图中的每个节点作为初始节点，这个随机漫步生成过程被重复多次。然后，作为结果，我们有一个节点序列的大“语料库”。然后，这个“语料库”可以直接输入到<em class="mb"> word2vec </em>算法中，以生成节点嵌入。特别是<em class="mb"> node2vec </em>使用<em class="mb"> skipgram </em>配合<em class="mb">负采样</em>(简称SGNS)。这里省略了SGNS的细节，但是我强烈建议读者查看两篇伟大的博文(<a class="ae le" href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/" rel="noopener ugc nofollow" target="_blank">【2】</a>和<a class="ae le" href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/" rel="noopener ugc nofollow" target="_blank">【3】</a>)，这两篇博文分别由<a class="ae le" href="http://mccormickml.com/" rel="noopener ugc nofollow" target="_blank"> Chris McCormic </a>撰写，解释了SG和NS。</p><p id="bd31" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">下面的动画给出了使用图上生成的随机游走训练<em class="mb"> skipgram </em>的简要思路。主要思想是最大化预测给定中心节点的正确上下文节点的概率(通过softmax计算)。另一方面，<em class="mb">负采样</em>用于通过仅计算来自随机抽取的“负样本”的几个激活来提高计算效率，而不是softmax中的全部归一化因子。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nq"><img src="../Images/49df8ebdd80918a2437849945785e518.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*EXl6qtSqb1AsTRZO5ckhqQ.gif"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">使用随机行走的skipgram训练动画。(图片由作者提供)</p></figure><h1 id="9d5a" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">为什么走偏了？</h1><p id="0140" class="pw-post-body-paragraph lf lg iq lh b li mw ka lk ll mx kd ln lo nk lq lr ls nl lu lv lw nm ly lz ma ij bi translated">现在我们知道了node2vec如何使用二阶随机漫步在图上生成嵌入，您可能会想，为什么不直接使用一阶随机漫步呢？主要原因是搜索策略的<strong class="lh ja">灵活性。一方面，当返回参数(p)和输入输出参数(q)设置为1时，它精确地恢复了一阶随机游走。然而，另一方面，当p和q被设置为除1之外的正值时，随机行走可能被偏置为或者局限于网络模块中，或者相反，在整个网络中。</strong></p><p id="ad11" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">尽管如此，你可能想知道，为什么你要关心在这种意义上的行走是否灵活？</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/e16d3306132dd45d3cd88628b722f659.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*nCyF5jFSU5uJVdAPdf-0HA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(<a class="ae le" href="https://arxiv.org/abs/1607.00653" rel="noopener ugc nofollow" target="_blank">格罗弗和莱斯科维奇，2016 </a></p></figure><p id="5fce" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="mb"> node2vec </em>论文给出了以下具有不同p和q的悲惨世界网络嵌入的示例。所得到的嵌入用于执行聚类分析以将节点分组在一起。然后根据节点所属的集群对节点进行着色。顶部和底部面板对应于使用q = 0.5和q = 2生成的<em class="mb"> node2vec </em>嵌入。可以看到，在顶部面板中，属于同一<strong class="lh ja">本地网络邻居</strong>(即<strong class="lh ja">同向</strong>)的节点颜色相同。另一方面，在底部面板中，<strong class="lh ja">结构相同的</strong>节点颜色相同。例如，蓝色节点对应于充当社区之间桥梁的角色。</p><h1 id="b65e" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">结束语</h1><p id="5702" class="pw-post-body-paragraph lf lg iq lh b li mw ka lk ll mx kd ln lo nk lq lr ls nl lu lv lw nm ly lz ma ij bi translated"><em class="mb"> Node2vec </em>在计算生物学中特别流行，主要用于使用蛋白质-蛋白质相互作用网络嵌入的基因分类。例如，已经表明，对于基因功能预测，PPI <em class="mb"> node2vec </em>嵌入后进行逻辑回归可以产生与最先进的网络传播方法相同或更好的性能[4]。类似地，另一种称为GeneWalk的方法使用<em class="mb"> node2vec </em>来生成基因和基因本体术语的生物上下文特定嵌入[5]。</p><p id="1112" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">最后，尽管<em class="mb"> node2vec </em>嵌入有很好的性能，但是由于内存问题，最初的实现不太具有可伸缩性。特别是，正如我们之前看到的，<em class="mb"> node2vec </em>设计了二阶跃迁随机游走，它依赖于当前状态和先前状态。这导致转移概率的总数与图中边的数量成二次方比例(巨大……)。例如，嵌入一个具有17k个节点和3.6M条边的相当稀疏的网络需要超过100GB的内存。这个内存问题被一个叫做<em class="mb"> PecanPy </em>的新实现有效地解决了，它只使用不到1GB的内存来嵌入相同的网络【6】。你也可以看看我的另一篇<a class="ae le" rel="noopener" target="_blank" href="/run-node2vec-faster-with-less-memory-using-pecanpy-1bdf31f136de">博文</a>，了解更多关于<em class="mb"> PecanPy </em>的信息。</p><p id="ab44" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">感谢您的阅读，我希望这篇文章能让您更好地了解<em class="mb"> node2vec </em>是如何工作的！</p></div><div class="ab cl nv nw hu nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="ij ik il im in"><h1 id="b44c" class="mc md iq bd me mf oc mh mi mj od ml mm kf oe kg mo ki of kj mq kl og km ms mt bi translated">参考</h1><p id="8cd5" class="pw-post-body-paragraph lf lg iq lh b li mw ka lk ll mx kd ln lo nk lq lr ls nl lu lv lw nm ly lz ma ij bi translated">[1] A. Grover，J. Leskovec，<a class="ae le" href="http://arxiv.org/abs/1607.00653" rel="noopener ugc nofollow" target="_blank"> node2vec:面向网络的可扩展特征学习</a> (2016)，<em class="mb"> ACM SIGKDD知识发现与数据挖掘国际会议(KDD) </em></p><p id="71be" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[2] C. McCormick，<a class="ae le" href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/" rel="noopener ugc nofollow" target="_blank"> Word2Vec教程——跳格模型</a> (2016)</p><p id="4f25" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[3] C. McCormick，<a class="ae le" href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/" rel="noopener ugc nofollow" target="_blank"> Word2Vec教程第2部分—阴性取样</a> (2017)</p><p id="5a59" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[4]刘，Christopher A Mancuso，Anna Yannakopoulos，Kayla A Johnson，Arjun Krishnan，<a class="ae le" href="https://doi.org/10.1093/bioinformatics/btaa150" rel="noopener ugc nofollow" target="_blank">监督学习是基于网络的基因分类的精确方法</a> (2020)，<em class="mb">生物信息学</em></p><p id="ae78" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[5] Ietswaart，r .，Gyori，B.M .，Bachman，J.A .，<em class="mb">等</em> <a class="ae le" href="https://doi.org/10.1186/s13059-021-02264-8" rel="noopener ugc nofollow" target="_blank"> GeneWalk使用网络表示学习</a>(2021)<em class="mb">Genome Biol</em></p><p id="93c9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[6] R. Liu，A. Krishnan，<a class="ae le" href="https://doi.org/10.1093/bioinformatics/btab202" rel="noopener ugc nofollow" target="_blank">pecan py:<em class="mb">node 2 vec</em></a>(2021)，<em class="mb">生物信息学</em>的快速高效并行Python实现</p></div></div>    
</body>
</html>