<html>
<head>
<title>Linear Regression: Gradient Descent Vs Analytical Solution</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归:梯度下降与解析解</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-gradient-descent-is-so-common-in-data-science-def3e6515c5c?source=collection_archive---------6-----------------------#2021-12-23">https://towardsdatascience.com/why-gradient-descent-is-so-common-in-data-science-def3e6515c5c?source=collection_archive---------6-----------------------#2021-12-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a66c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">解释为什么梯度下降在数据科学中经常使用，并用 C 语言实现</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a00ffaa47796df5227381dde0d92ea9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_01JZYlRgSbmns1K"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@aaaaaaaaaaaaaaaa?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">trần ngọc·万</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="a103" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="2942" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">梯度下降</strong> </a>是一种无处不在的优化算法，在数据科学中广泛应用于<strong class="lt iu">神经网络、线性回归和梯度推进机</strong>等算法中。但是，为什么用的这么频繁呢？</p><h1 id="b15e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">梯度下降直觉</h1><p id="d06c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">让我们从解释梯度下降开始。这将是一个简短的描述，因为这个主题已经被彻底覆盖，所以如果你想要一个更密集的解释，请参考其他博客或教程。</p><p id="d5e9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">梯度下降是一种<strong class="lt iu">一阶迭代法</strong>求一个可微函数的<strong class="lt iu">最小值。我们从初始猜测开始，并在当前猜测的计算梯度的<strong class="lt iu">相反方向</strong>上<strong class="lt iu">缓慢下降</strong>。然后更新初始猜测，产生新的改进值。这个过程依次执行，直到我们<strong class="lt iu">收敛到最小值</strong>。</strong></p><p id="faa8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在机器学习中，这个微分函数就是<strong class="lt iu">损失函数</strong>，它告诉我们当前模型与数据的拟合程度。然后使用梯度下降来更新模型的当前参数，以使损失函数最小化。</p><h1 id="f4de" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">简单线性回归的梯度下降</h1><p id="e619" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">也许证明梯度下降最简单的例子是一个简单的线性回归模型。在这种情况下，我们的<strong class="lt iu">假设函数</strong>、<strong class="lt iu">、<em class="ms">、</em>、</strong>，依赖于一个<strong class="lt iu">单一特征变量、<em class="ms">、</em>、</strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/23874ede19fa0959e9756c93a187abe8.png" data-original-src="https://miro.medium.com/v2/resize:fit:406/format:webp/1*3ZXPNZisQE49qAb9F-yQ0A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者对我们模型的假设</p></figure><p id="a846" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">其中<strong class="lt iu"> <em class="ms"> θ_0 </em> </strong>和<em class="ms"> θ_1 </em> 为模型的<strong class="lt iu">参数。这个问题的损失函数是<strong class="lt iu">误差平方和(SSE) </strong>:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/7de453d726b4efbb6727a9d2c2436a8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*BPr-Wygh1DGnqqoSXNNNwg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者的和或平方损失函数</p></figure><p id="93f6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">因此，我们将使用梯度下降法来寻找使上述损失函数最小的参数值。</p><p id="e9c7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">正如你所看到的，损失函数是可微的，并且有一个抛物线形状，因此它有一个最小值。如前所述，梯度下降通过<strong class="lt iu">在梯度的相反方向迈出小步来更新模型的参数。</strong>因此，我们需要计算损失函数相对于两个参数的梯度:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/ebd9c9637abbb8e670fa8b036e96f4fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:906/format:webp/1*flM5W1RAxovXhYL8iJ4c9Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者截取的渐变</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/864542828fc39bd7b7539ea52c18aa95.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*EyEjhV7YAjaFV--hHan3qA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者绘制的坡度</p></figure><p id="fe4e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这些参数随后更新为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/b004bf6414d879f5c845c2357f52795d.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/1*J1F6fhxGn8_Hn6oQCuHZyQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者截取更新</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/1c8b11bbc6d960d675f6d13bd7882e83.png" data-original-src="https://miro.medium.com/v2/resize:fit:554/format:webp/1*vdJ1WQo5JxXHNOED7dtUEw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者更新坡度</p></figure><p id="335d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">其中<strong class="lt iu"> <em class="ms"> η </em> </strong>为<strong class="lt iu">学习率</strong>，决定了每个参数更新的<strong class="lt iu">步长</strong>。学习率在<strong class="lt iu">零和一</strong>之间，指定我们多快收敛到最小值。如果太大，我们可能<strong class="lt iu">超过最小值，</strong>但是太小会导致<strong class="lt iu">更长的计算时间</strong>。因此，需要找到一个快乐的媒介。这就是<strong class="lt iu">超参数调整</strong>通过<strong class="lt iu">网格和随机搜索</strong>甚至<strong class="lt iu">贝叶斯</strong>方法使用的地方。</p><p id="89fb" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">给定足够的迭代，在数据科学中这被称为<strong class="lt iu">训练时期</strong>，梯度将<strong class="lt iu">趋向于零。因此，参数的当前值已经最小化损失函数并且已经收敛。</strong></p><h1 id="bb26" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">解析</h1><p id="3f20" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">然而，简单线性回归模型确实存在<strong class="lt iu">解析解</strong>。不使用数值技术，我们可以简单地将偏导数方程设置为零:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi my"><img src="../Images/4d4ef33bf47be0a5a3c88890cb065096.png" data-original-src="https://miro.medium.com/v2/resize:fit:364/format:webp/1*9q3H1iy5Qdzvm-llWD6tFQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者方程式</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi my"><img src="../Images/5fa63916d92fb02a7b99c24a9c01b662.png" data-original-src="https://miro.medium.com/v2/resize:fit:364/format:webp/1*NCjyUz8mOc1qK0sj-r0mIg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者方程式</p></figure><p id="b8e6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这是一个由两个线性方程组成的<strong class="lt iu">系统，其中两个未知数可以解析求解。通过数学推导和重新排列，满足上述方程的参数值为:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/32f6767ce4e947e648d8b5d8104efc4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:312/format:webp/1*Q2WnuvRbXh3jBCNdcaswqg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者截距公式</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi na"><img src="../Images/fdd81a9ca2f1e499f999fb8004e6af71.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*bfPP8HOzGSHLyAD92GiZQA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者的斜率方程</p></figure><p id="ab9a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">其中<strong class="lt iu"> x̅ </strong>和<strong class="lt iu"> ȳ </strong>分别是数据的<strong class="lt iu">平均值和目标变量</strong>的平均值。因此，通过计算这些平均值，我们可以找到使损失函数最小化的参数，而无需使用迭代方法！</p><p id="a49f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">以上方程是<strong class="lt iu">简单线性回归模型</strong>的<strong class="lt iu">解析解</strong>。这只是针对<strong class="lt iu">线性回归模型</strong>的<strong class="lt iu">一般解决方案</strong>的简化版本，其中我们可以有<strong class="lt iu">两个以上的未知参数:</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/16190c3db5fbc7de303a0b775f83cd60.png" data-original-src="https://miro.medium.com/v2/resize:fit:454/format:webp/1*IDPJdH2UUbUY3DbU4l98Qw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一般解决方案，作者方程</p></figure><p id="96d1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">其中<strong class="lt iu"> <em class="ms"> X </em> </strong>为数据矩阵，<strong class="lt iu"> <em class="ms"> Y </em> </strong> <em class="ms">，</em>为目标变量矩阵，<strong class="lt iu"><em class="ms">θ</em></strong>为参数矩阵。</p><h1 id="9435" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">那为什么是梯度下降呢？</h1><p id="0e4f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">那么，当存在解析解时，我们为什么要使用梯度下降法呢？这个答案完全基于<strong class="lt iu">计算时间和空间成本。</strong></p><p id="051d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">梯度下降的时间复杂度为<strong class="lt iu"> <em class="ms"> O(kn ) </em> </strong>其中<strong class="lt iu"> <em class="ms"> k </em> </strong>为特征个数，<strong class="lt iu"> <em class="ms"> n </em> </strong>为数据点总数。这种复杂性可以通过<strong class="lt iu">矢量化实现进一步提高。这就是今天大多数机器学习算法的实现方式。</strong></p><p id="9679" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然而，线性回归的一般解析解的时间复杂度为<strong class="lt iu"> <em class="ms"> O </em> (𝑛)。</strong>因此，对于小数据集，差异可以忽略不计，但计算时间差会随着数据大小的增加而呈指数增长。实际上，大多数数据集大约有 100 个要素，100 万行。因此，对于这些情况，解析解是不可行的。</p><p id="b283" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">此外，对于某些模型，如<strong class="lt iu">泊松回归</strong>和<strong class="lt iu">逻辑回归</strong>，将导数设置为零会导致一组<strong class="lt iu">非线性方程</strong> <strong class="lt iu">没有闭合形式的解析解</strong>，因此，我们被迫使用数值方法，如梯度下降。</p><h1 id="1d03" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="560f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">梯度下降优于解析解，因为它的计算速度和一些回归模型缺乏封闭形式的解决方案。这使得迭代数值方法的实现成为必要。</p><p id="df97" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我希望你们喜欢这篇文章，并学到了一些新东西！有很多其他的文章对我在这篇文章中浓缩的一些推导进行了更深入的探讨，所以我建议大家去看看！</p><h1 id="ac2e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">和我联系！</h1><ul class=""><li id="ca40" class="nc nd it lt b lu lv lx ly ma ne me nf mi ng mm nh ni nj nk bi translated"><a class="ae ky" href="/@egorhowell/membership" rel="noopener ugc nofollow" target="_blank"> <em class="ms">要想在媒体上阅读无限的故事，请务必在这里报名！</em>T45<em class="ms">T47】💜</em></a></li><li id="9786" class="nc nd it lt b lu nl lx nm ma nn me no mi np mm nh ni nj nk bi translated"><a class="ae ky" href="/subscribe/@egorhowell" rel="noopener ugc nofollow" target="_blank"> <em class="ms">在我发布注册邮件通知时获得更新！</em>T51<em class="ms">T53】😀</em></a></li><li id="b8dc" class="nc nd it lt b lu nl lx nm ma nn me no mi np mm nh ni nj nk bi translated"><a class="ae ky" href="https://www.linkedin.com/in/egor-howell-092a721b3/" rel="noopener ugc nofollow" target="_blank"> <em class="ms">领英</em> </a> <em class="ms"> </em>👔</li><li id="a7a0" class="nc nd it lt b lu nl lx nm ma nn me no mi np mm nh ni nj nk bi translated"><a class="ae ky" href="https://twitter.com/EgorHowell" rel="noopener ugc nofollow" target="_blank"> <em class="ms">碎碎念</em> </a> <em class="ms"> </em> 🖊</li><li id="fb18" class="nc nd it lt b lu nl lx nm ma nn me no mi np mm nh ni nj nk bi translated"><a class="ae ky" href="https://github.com/egorhowell" rel="noopener ugc nofollow" target="_blank"> <em class="ms"> GitHub </em> </a> <em class="ms"> </em> 🖥</li><li id="87b0" class="nc nd it lt b lu nl lx nm ma nn me no mi np mm nh ni nj nk bi translated"><a class="ae ky" href="https://www.kaggle.com/egorphysics" rel="noopener ugc nofollow" target="_blank"><em class="ms"/></a><em class="ms"/>🏅</li></ul><blockquote class="nq nr ns"><p id="72f1" class="lr ls ms lt b lu mn ju lw lx mo jx lz nt mp mc md nu mq mg mh nv mr mk ml mm im bi translated">(所有表情符号都是由<a class="ae ky" href="https://openmoji.org/" rel="noopener ugc nofollow" target="_blank"> OpenMoji </a>设计的——开源的表情符号和图标项目。许可证:<a class="ae ky" href="https://creativecommons.org/licenses/by-sa/4.0/#" rel="noopener ugc nofollow" target="_blank"> CC BY-SA 4.0 </a></p></blockquote><h1 id="dd1d" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">额外的东西！</h1><p id="3bb3" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">下面显示的是我用<strong class="lt iu"> C </strong>编写的示例代码，展示了如何对梯度下降进行编程！</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="0ce8" class="ob la it nx b gy oc od l oe of">#include &lt;stdio.h&gt;<br/>#include &lt;stdlib.h&gt;<br/>#include &lt;math.h&gt;</span><span id="6be8" class="ob la it nx b gy og od l oe of">double dydx(double x);</span><span id="e67f" class="ob la it nx b gy og od l oe of">int main(){</span><span id="bce0" class="ob la it nx b gy og od l oe of">  int epochs, i; <br/>  double learning_rate, x, x_new;</span><span id="1717" class="ob la it nx b gy og od l oe of">  printf("Enter your intial guess integer: ");<br/>  scanf("%lf", &amp;x);      <br/> <br/>  printf("Enter how many epochs: ");<br/>  scanf("%d", &amp;epochs);</span><span id="3fe3" class="ob la it nx b gy og od l oe of">  printf("Enter your learning rate: ");<br/>  scanf("%lf", &amp;learning_rate);</span><span id="63eb" class="ob la it nx b gy og od l oe of">  for (i=1;i&lt;epochs+1;++i){<br/>   <br/>    x_new = x;<br/>    x_new = x_new - learning_rate*dydx(x_new);</span><span id="72f9" class="ob la it nx b gy og od l oe of">    if ((x - x_new) &lt; 0.000001){<br/>      printf("number of epochs to coverge %d\n", i);<br/>      break;  <br/>   <br/>    }</span><span id="4f93" class="ob la it nx b gy og od l oe of">    x = x_new;</span><span id="456d" class="ob la it nx b gy og od l oe of">  }</span><span id="b7d0" class="ob la it nx b gy og od l oe of">  printf("The value of x that minimises is %lf", x);</span><span id="8c35" class="ob la it nx b gy og od l oe of">}</span><span id="009f" class="ob la it nx b gy og od l oe of">double dydx(double x){<br/>  return 2*x - 5;<br/>}</span></pre><p id="9f27" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">完整代码可以在我的 GitHub 上找到:</p><div class="oh oi gp gr oj ok"><a href="https://github.com/egorhowell/Medium-Articles/blob/main/Data%20Science%20Basics/gradient_descent.c" rel="noopener  ugc nofollow" target="_blank"><div class="ol ab fo"><div class="om ab on cl cj oo"><h2 class="bd iu gy z fp op fr fs oq fu fw is bi translated">Medium-Articles/gradient _ descent . c at main egorhowell/Medium-Articles</h2><div class="or l"><h3 class="bd b gy z fp op fr fs oq fu fw dk translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="os l"><p class="bd b dl z fp op fr fs oq fu fw dk translated">github.com</p></div></div><div class="ot l"><div class="ou l ov ow ox ot oy ks ok"/></div></div></a></div></div></div>    
</body>
</html>