<html>
<head>
<title>A Complete Step by Step Tutorial on Sentiment Analysis in Keras and Tensorflow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于Keras和Tensorflow中情感分析的一步一步的完整教程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-complete-step-by-step-tutorial-on-sentiment-analysis-in-keras-and-tensorflow-ea420cc8913f?source=collection_archive---------5-----------------------#2021-07-08">https://towardsdatascience.com/a-complete-step-by-step-tutorial-on-sentiment-analysis-in-keras-and-tensorflow-ea420cc8913f?source=collection_archive---------5-----------------------#2021-07-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/e6cd9410f6454d17dfea54b73af86401.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*P8PwsZYlOrwVZKcC"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">马克·戴恩斯在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class=""/><div class=""><h2 id="493f" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">用于数据准备、深度学习模型开发和训练网络的完整工作代码</h2></div><p id="c5ca" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">情感分析是非常常见的自然语言处理任务之一。企业使用情感分析来有效地理解社交媒体评论、产品评论和其他文本数据。Tensorflow和Keras是解决这个问题的绝佳工具。</p><p id="a09a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Tensorflow可以说是最受欢迎的深度学习库。它在幕后使用一个神经网络。它如此受欢迎的原因是，它真的很容易使用和工作相当快。甚至不知道神经网络如何工作，你就可以运行神经网络。尽管这很有帮助，如果你知道一些关于神经网络的基础知识。</p><p id="eb79" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Tensorflow也有非常好的文档。所以，也很好学。如今，我们中的许多人都希望从事机器学习和深度学习，但害怕这可能需要非常高水平的编程解决问题技能。</p><p id="c9a7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">是的，肯定需要有编程知识。但是因为有了Tensorflow这种很酷的库，如果你有中级的编程技能，就有可能致力于机器学习和深度学习问题。大多数专业数据科学家不会从头开始开发他们的算法。在行业层面上，使用这些库非常普遍。</p><p id="f802" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">本文结合一个项目重点介绍TensorFlow中一个简单的情感分析项目。请随意从以下链接下载数据集:</p><div class="is it gp gr iu lu"><a href="https://www.kaggle.com/sameersmahajan/reviews-of-amazon-baby-products" rel="noopener  ugc nofollow" target="_blank"><div class="lv ab fo"><div class="lw ab lx cl cj ly"><h2 class="bd jk gy z fp lz fr fs ma fu fw ji bi translated">亚马逊婴儿产品评论</h2><div class="mb l"><h3 class="bd b gy z fp lz fr fs ma fu fw dk translated">Kaggle是世界上最大的数据科学社区，拥有强大的工具和资源来帮助您实现您的数据…</h3></div><div class="mc l"><p class="bd b dl z fp lz fr fs ma fu fw dk translated">www.kaggle.com</p></div></div><div class="md l"><div class="me l mf mg mh md mi ja lu"/></div></div></a></div><p id="f226" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个数据集有三列。产品名称、评论和评级。review列是包含客户评论的文本列，rating列具有从1到5的数字评级。1表示最差，5表示最好。</p><h2 id="c370" class="mj mk jj bd ml mm mn dn mo mp mq dp mr lh ms mt mu ll mv mw mx lp my mz na nb bi translated">从这篇文章中可以期待什么？</h2><p id="bb33" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">本文将提供:</p><ol class=""><li id="8a64" class="nh ni jj la b lb lc le lf lh nj ll nk lp nl lt nm nn no np bi translated">如何使用TensorFlow和Keras进行情感分析的分步说明</li><li id="88c0" class="nh ni jj la b lb nq le nr lh ns ll nt lp nu lt nm nn no np bi translated">完整的工作代码将在一些现实生活中的项目工作。</li><li id="584d" class="nh ni jj la b lb nq le nr lh ns ll nt lp nu lt nm nn no np bi translated">参数说明</li><li id="c483" class="nh ni jj la b lb nq le nr lh ns ll nt lp nu lt nm nn no np bi translated">一些资源来更好地理解参数</li></ol><h2 id="6877" class="mj mk jj bd ml mm mn dn mo mp mq dp mr lh ms mt mu ll mv mw mx lp my mz na nb bi translated">数据准备</h2><p id="d5a0" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">在自然语言处理项目中，数据预处理是工作的一半。因为算法不理解文本。所以，我们需要将文本转换成算法可以理解的数字。</p><p id="7e63" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在处理文本之前，定义积极情绪和消极情绪也很重要。在这个数据集中，我们可以使用评级列来了解情绪。我们有从1到5的等级。因此，当评级为1或2时，该评论可被视为负面评论，而当评级为3、4或5时，则该评论可被视为正面评论。我们可以为负面情绪设置0，为正面情绪设置1。</p><pre class="nv nw nx ny gt nz oa ob oc aw od bi"><span id="5851" class="mj mk jj oa b gy oe of l og oh">df['sentiments'] = df.rating.apply(lambda x: 0 if x in [1, 2] else 1)</span></pre><p id="cbf0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用数据集上方的代码行添加“情绪”列后，如下所示:</p><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oi"><img src="../Images/40cc8f1a81ed00ab82e8f7179259bfe0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dgqvLK688Oad_HGZgWZxIQ.png"/></div></div></figure><p id="aa24" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下一步是像我们之前提到的那样对文本进行标记。为此将使用记号赋予器函数。默认情况下，它会删除所有标点符号，并将文本设置为以空格分隔的有序形式。每个单词通过tokenizer函数变成一个整数。让我们设置记号赋予器函数:</p><pre class="nv nw nx ny gt nz oa ob oc aw od bi"><span id="f5bc" class="mj mk jj oa b gy oe of l og oh">from tensorflow.keras.preprocessing.text import Tokenizer</span><span id="0272" class="mj mk jj oa b gy oj of l og oh">from tensorflow.keras.preprocessing.sequence import pad_sequences</span><span id="3bca" class="mj mk jj oa b gy oj of l og oh">tokenizer = Tokenizer(oov_token="&lt;OOV&gt;")</span></pre><p id="f585" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里，oov_token的值被设置为“oov”。这意味着任何未知单词都将被oov_token替换。这是一个更好的选择，而不是抛出未知的单词。我们稍后将讨论“填充序列”。</p><blockquote class="ok ol om"><p id="4f80" class="ky kz on la b lb lc kk ld le lf kn lg oo li lj lk op lm ln lo oq lq lr ls lt im bi translated">分割数据集</p></blockquote><p id="eb8e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将保留80%的数据用于培训，20%用于测试目的。</p><pre class="nv nw nx ny gt nz oa ob oc aw od bi"><span id="7af9" class="mj mk jj oa b gy oe of l og oh">split = round(len(df)*0.8)</span><span id="d889" class="mj mk jj oa b gy oj of l og oh">train_reviews = df['review'][:split]</span><span id="32cc" class="mj mk jj oa b gy oj of l og oh">train_label = df['sentiments'][:split]</span><span id="3c6e" class="mj mk jj oa b gy oj of l og oh">test_reviews = df['review'][split:]</span><span id="810e" class="mj mk jj oa b gy oj of l og oh">test_label = df['sentiments'][split:]</span></pre><p id="8189" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我会格外小心地做一步。也就是把每个评论转换成一个字符串。因为万一有不是字符串格式的数据，我们稍后会得到一个错误。所以，我想多走一步。</p><pre class="nv nw nx ny gt nz oa ob oc aw od bi"><span id="fd99" class="mj mk jj oa b gy oe of l og oh">import numpy as np</span><span id="3a3e" class="mj mk jj oa b gy oj of l og oh">training_sentences = []</span><span id="edfc" class="mj mk jj oa b gy oj of l og oh">training_labels = []</span><span id="8b4f" class="mj mk jj oa b gy oj of l og oh">testing_sentences = []</span><span id="83ba" class="mj mk jj oa b gy oj of l og oh">testing_labels = []</span><span id="c5cf" class="mj mk jj oa b gy oj of l og oh">for row in train_reviews:<br/>    training_sentences.append(str(row))</span><span id="82e7" class="mj mk jj oa b gy oj of l og oh">for row in train_label:<br/>    training_labels.append(row)</span><span id="d218" class="mj mk jj oa b gy oj of l og oh">for row in test_reviews:<br/>    testing_sentences.append(str(row))</span><span id="bd7d" class="mj mk jj oa b gy oj of l og oh">for row in test_label:<br/>    testing_labels.append(row)</span></pre><p id="f94a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">训练和测试集已经准备好了。一些重要的术语需要修正。我将在这段代码之后解释它们:</p><pre class="nv nw nx ny gt nz oa ob oc aw od bi"><span id="8484" class="mj mk jj oa b gy oe of l og oh">vocab_size = 40000</span><span id="090c" class="mj mk jj oa b gy oj of l og oh">embedding_dim = 16</span><span id="5dea" class="mj mk jj oa b gy oj of l og oh">max_length = 120</span><span id="bb2b" class="mj mk jj oa b gy oj of l og oh">trunc_type = 'post'</span><span id="28fe" class="mj mk jj oa b gy oj of l og oh">oov_tok = '&lt;OOV&gt;'</span><span id="73e0" class="mj mk jj oa b gy oj of l og oh">padding_type = 'post'</span></pre><p id="3a26" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这里，vocab_size 40，000意味着我们将采用40，000个唯一的单词来训练网络。嵌入维数16意味着每个单词将由16维向量表示。Max_length 120表示每个评论的长度。我们将保留每个评论的120个单词。如果最初的评论超过120个单词，它将被截断。术语trunc_type设置为‘post’。因此，当评论超过120个单词时，评论将在最后被截断。另一方面，如果评论少于120个单词，它将被填充为120个单词。最后，padding_type 'post '意味着填充将应用于结尾，而不是开头。</p><p id="39d3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，让我们从词汇化开始:</p><pre class="nv nw nx ny gt nz oa ob oc aw od bi"><span id="e1fc" class="mj mk jj oa b gy oe of l og oh">tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)</span><span id="ffc2" class="mj mk jj oa b gy oj of l og oh">tokenizer.fit_on_texts(training_sentences)</span><span id="66da" class="mj mk jj oa b gy oj of l og oh">word_index = tokenizer.word_index</span></pre><p id="fdc1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下是word_index值的一部分:</p><pre class="nv nw nx ny gt nz oa ob oc aw od bi"><span id="39fb" class="mj mk jj oa b gy oe of l og oh">{'&lt;OOV&gt;': 1,  <br/>'the': 2,  <br/>'it': 3,  <br/>'i': 4,  <br/>'and': 5,  <br/>'to': 6,  <br/>'a': 7,  <br/>'is': 8,  <br/>'this': 9,  <br/>'for': 10,</span></pre><p id="1aab" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">看，每个单词都有一个整数值。现在，复习句子可以表示为一系列单词。下一个代码块将句子转换成单词序列，然后根据需要进行填充:</p><pre class="nv nw nx ny gt nz oa ob oc aw od bi"><span id="23b9" class="mj mk jj oa b gy oe of l og oh">sequences = tokenizer.texts_to_sequences(training_sentences)</span><span id="3c93" class="mj mk jj oa b gy oj of l og oh">padded = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)</span><span id="e96c" class="mj mk jj oa b gy oj of l og oh">testing_sentences = tokenizer.texts_to_sequences(testing_sentences)</span><span id="2a99" class="mj mk jj oa b gy oj of l og oh">testing_padded = pad_sequences(testing_sentences, maxlen=max_length)</span></pre><p id="e518" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">数据处理在这里完成。现在，模型开发可以非常容易地完成。</p><h2 id="94b3" class="mj mk jj bd ml mm mn dn mo mp mq dp mr lh ms mt mu ll mv mw mx lp my mz na nb bi translated">模型开发</h2><p id="d2a2" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">对于这个项目，'<a class="ae jg" href="https://keras.io/guides/sequential_model/" rel="noopener ugc nofollow" target="_blank"> keras。将使用顺序</a>模式。请查看附加链接，了解有关“keras”的详细信息。连续的。我将解释如何在这类项目中使用它。模型如下:</p><pre class="nv nw nx ny gt nz oa ob oc aw od bi"><span id="6292" class="mj mk jj oa b gy oe of l og oh">model = tf.keras.Sequential([<br/>    tf.keras.layers.Embedding(vocab_size, embedding_dim,       input_length=max_length),</span><span id="f82f" class="mj mk jj oa b gy oj of l og oh">    tf.keras.layers.GlobalAveragePooling1D(),</span><span id="5bf5" class="mj mk jj oa b gy oj of l og oh">    tf.keras.layers.Dense(6, activation='relu'),</span><span id="c5fe" class="mj mk jj oa b gy oj of l og oh">    tf.keras.layers.Dense(1, activation='sigmoid')</span><span id="bafe" class="mj mk jj oa b gy oj of l og oh">])</span></pre><p id="ab0b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">第一层是嵌入层，其中所有的参数都已经在前面定义和解释过了。第二层是' GlobalAveragePooling1D()'展平矢量。最初，数据是三维的(批量大小x步骤x特征)。GlobalAveragePooling1D制作(batch_size x features)。</p><p id="7af5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">第三层是密集层，其中使用了“relu”激活功能。您可以尝试“tanh”或您选择的任何其他激活功能。这一层称为隐藏层。我只用了一个隐藏层。随意尝试多个隐藏层。更复杂的问题可能需要更多的隐藏层。此外，我在隐藏层中使用了6个神经元。你可能想知道如何选择神经元的数量。</p><p id="837e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有很多关于这方面的文章。<a class="ae jg" href="https://www.linkedin.com/pulse/choosing-number-hidden-layers-neurons-neural-networks-sachdev/" rel="noopener ugc nofollow" target="_blank">这里有一篇简短的文章，简要地提供了一些见解</a>。</p><p id="e0bc" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后一层使用sigmoid激活函数或逻辑函数。</p><blockquote class="or"><p id="1b13" class="os ot jj bd ou ov ow ox oy oz pa lt dk translated">在隐藏层中，您可以使用“relu”或“tanh”激活函数，但分类问题中的最后一层总是sigmoid或softmax激活函数。</p></blockquote><p id="c44d" class="pw-post-body-paragraph ky kz jj la b lb pb kk ld le pc kn lg lh pd lj lk ll pe ln lo lp pf lr ls lt im bi translated">现在，编译模型:</p><pre class="nv nw nx ny gt nz oa ob oc aw od bi"><span id="f919" class="mj mk jj oa b gy oe of l og oh">model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])</span></pre><p id="d04f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我选择二元交叉熵作为损失函数，因为这是一个概率损失。这里还描述了其他的<a class="ae jg" href="https://keras.io/api/losses/" rel="noopener ugc nofollow" target="_blank">损失函数</a>。我用优化器作为“亚当”。还有其他几个优化器函数，如RMSProp、adadelta、adagrad、adamax等等。以下是模型总结:</p><pre class="nv nw nx ny gt nz oa ob oc aw od bi"><span id="ff3b" class="mj mk jj oa b gy oe of l og oh">model.summary()</span></pre><p id="121f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="nv nw nx ny gt nz oa ob oc aw od bi"><span id="cae5" class="mj mk jj oa b gy oe of l og oh">Model: "sequential"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>embedding (Embedding)        (None, 120, 16)           640000    <br/>_________________________________________________________________<br/>global_average_pooling1d (Gl (None, 16)                0         <br/>_________________________________________________________________<br/>dense (Dense)                (None, 6)                 102       <br/>_________________________________________________________________<br/>dense_1 (Dense)              (None, 1)                 7         <br/>=================================================================<br/>Total params: 640,109<br/>Trainable params: 640,109<br/>Non-trainable params: 0<br/>_________________________________________________________________</span></pre><p id="8f7f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您查看模型摘要，我们之前讨论的许多术语现在都有意义了。</p><p id="a6d8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我解释一下。嵌入层显示三维。我们没有提到任何批量大小。这里，我们使用每个时期的整个数据集进行训练。每个评论中的120个单词，每个单词都表示为一个16(我们之前选择的是这个16)的元素向量。这意味着我们有16个特征。在全局平均池化之后，它变平了，我们只有批量大小(没有)和特性的数量。</p><h2 id="901a" class="mj mk jj bd ml mm mn dn mo mp mq dp mr lh ms mt mu ll mv mw mx lp my mz na nb bi translated">训练模型</h2><p id="b1c4" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">在训练模型之前，我们只需要将标签转换为数组。如果你注意到，它们是列表形式的:</p><pre class="nv nw nx ny gt nz oa ob oc aw od bi"><span id="5165" class="mj mk jj oa b gy oe of l og oh">training_labels_final = np.array(training_labels)</span><span id="80c3" class="mj mk jj oa b gy oj of l og oh">testing_labels_final = np.array(testing_labels)</span></pre><p id="eb54" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们开始训练“模型”。我将训练20个纪元的模型。</p><pre class="nv nw nx ny gt nz oa ob oc aw od bi"><span id="9bb2" class="mj mk jj oa b gy oe of l og oh">num_epochs = 20</span><span id="126f" class="mj mk jj oa b gy oj of l og oh">history = model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))</span></pre><p id="9831" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="nv nw nx ny gt nz oa ob oc aw od bi"><span id="5d63" class="mj mk jj oa b gy oe of l og oh">Epoch 1/20 <br/>4589/4589 [==============================] - 40s 8ms/step - loss: 0.2849 - accuracy: 0.8831 - val_loss: 0.2209 - val_accuracy: 0.9094 Epoch 2/20 <br/>4589/4589 [==============================] - 35s 8ms/step - loss: 0.2098 - accuracy: 0.9127 - val_loss: 0.1990 - val_accuracy: 0.9186 Epoch 3/20 <br/>4589/4589 [==============================] - 36s 8ms/step - loss: 0.1931 - accuracy: 0.9195 - val_loss: 0.2000 - val_accuracy: 0.9177 Epoch 4/20 <br/>4589/4589 [==============================] - 35s 8ms/step - loss: 0.1837 - accuracy: 0.9234 - val_loss: 0.1993 - val_accuracy: 0.9168 Epoch 5/20 <br/>4589/4589 [==============================] - 35s 8ms/step - loss: 0.1766 - accuracy: 0.9264 - val_loss: 0.2013 - val_accuracy: 0.9163 Epoch 6/20 <br/>4589/4589 [==============================] - 35s 8ms/step - loss: 0.1708 - accuracy: 0.9287 - val_loss: 0.2044 - val_accuracy: 0.9174 Epoch 7/20 <br/>4589/4589 [==============================] - 36s 8ms/step - loss: 0.1656 - accuracy: 0.9309 - val_loss: 0.2164 - val_accuracy: 0.9166 Epoch 8/20 <br/>4589/4589 [==============================] - 35s 8ms/step - loss: 0.1606 - accuracy: 0.9332 - val_loss: 0.2122 - val_accuracy: 0.9155 Epoch 9/20 4589/4589 [==============================] - 35s 8ms/step - loss: 0.1560 - accuracy: 0.9354 - val_loss: 0.2203 - val_accuracy: 0.9170 <br/>Epoch 10/20 <br/>4589/4589 [==============================] - 36s 8ms/step - loss: 0.1515 - accuracy: 0.9373 - val_loss: 0.2222 - val_accuracy: 0.9161 Epoch 11/20 <br/>4589/4589 [==============================] - 35s 8ms/step - loss: 0.1468 - accuracy: 0.9396 - val_loss: 0.2225 - val_accuracy: 0.9143 Epoch 12/20 <br/>4589/4589 [==============================] - 37s 8ms/step - loss: 0.1427 - accuracy: 0.9413 - val_loss: 0.2330 - val_accuracy: 0.9120 Epoch 13/20 <br/>4589/4589 [==============================] - 36s 8ms/step - loss: 0.1386 - accuracy: 0.9432 - val_loss: 0.2369 - val_accuracy: 0.9131 Epoch 14/20 <br/>4589/4589 [==============================] - 34s 7ms/step - loss: 0.1344 - accuracy: 0.9455 - val_loss: 0.2418 - val_accuracy: 0.9102 Epoch 15/20 <br/>4589/4589 [==============================] - 36s 8ms/step - loss: 0.1307 - accuracy: 0.9470 - val_loss: 0.2487 - val_accuracy: 0.9073 Epoch 16/20 <br/>4589/4589 [==============================] - 37s 8ms/step - loss: 0.1272 - accuracy: 0.9490 - val_loss: 0.2574 - val_accuracy: 0.9058 Epoch 17/20 <br/>4589/4589 [==============================] - 36s 8ms/step - loss: 0.1237 - accuracy: 0.9502 - val_loss: 0.2663 - val_accuracy: 0.9009 Epoch 18/20 <br/>4589/4589 [==============================] - 36s 8ms/step - loss: 0.1202 - accuracy: 0.9519 - val_loss: 0.2734 - val_accuracy: 0.9028 Epoch 19/20 <br/>4589/4589 [==============================] - 36s 8ms/step - loss: 0.1173 - accuracy: 0.9536 - val_loss: 0.2810 - val_accuracy: 0.8978 Epoch 20/20 <br/>4589/4589 [==============================] - 36s 8ms/step - loss: 0.1144 - accuracy: 0.9550 - val_loss: 0.2959 - val_accuracy: 0.9058</span></pre><p id="55b7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从上一个历元的结果可以看出，训练准确率为95.5%，验证准确率为90.58%。看起来有点太合身了。您可能想尝试更多的历元，看看精度是否会进一步提高。</p><p id="cb13" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以绘制训练和验证的精确度，以及训练和验证的损失:</p><pre class="nv nw nx ny gt nz oa ob oc aw od bi"><span id="118f" class="mj mk jj oa b gy oe of l og oh">%matplotlib inline</span><span id="e3b2" class="mj mk jj oa b gy oj of l og oh">import matplotlib.pyplot as plt</span><span id="2186" class="mj mk jj oa b gy oj of l og oh">import matplotlib.image as mpimg</span><span id="2377" class="mj mk jj oa b gy oj of l og oh">acc = history.history['accuracy']</span><span id="8624" class="mj mk jj oa b gy oj of l og oh">val_acc = history.history['val_accuracy']</span><span id="825a" class="mj mk jj oa b gy oj of l og oh">loss = history.history['loss']</span><span id="e616" class="mj mk jj oa b gy oj of l og oh">val_loss = history.history['val_loss']</span><span id="312a" class="mj mk jj oa b gy oj of l og oh">epochs=range(len(acc))</span><span id="3d0d" class="mj mk jj oa b gy oj of l og oh">plt.plot(epochs, acc, 'r', 'Training Accuracy')</span><span id="b306" class="mj mk jj oa b gy oj of l og oh">plt.plot(epochs, val_acc, 'b', 'Validation Accuracy')</span><span id="20e1" class="mj mk jj oa b gy oj of l og oh">plt.title('Training and validation accuracy')</span><span id="8d3c" class="mj mk jj oa b gy oj of l og oh">plt.figure()</span><span id="4e3f" class="mj mk jj oa b gy oj of l og oh">plt.plot(epochs, loss, 'r', 'Training Loss')</span><span id="7505" class="mj mk jj oa b gy oj of l og oh">plt.plot(epochs, val_loss, 'b', 'Validation Loss')</span><span id="3229" class="mj mk jj oa b gy oj of l og oh">plt.title('Training and validation loss')</span><span id="2dfb" class="mj mk jj oa b gy oj of l og oh">plt.figure()</span></pre><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/938685485a4a842b41245316dd09da96.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*WkPv3j2gd0D1z5XroUWSDA.png"/></div></figure><p id="5e92" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">模型做好了。主要部分完成了。现在，如果您愿意，可以使用不同的其他绩效评估指标。</p><h2 id="3355" class="mj mk jj bd ml mm mn dn mo mp mq dp mr lh ms mt mu ll mv mw mx lp my mz na nb bi translated">结论</h2><p id="b782" class="pw-post-body-paragraph ky kz jj la b lb nc kk ld le nd kn lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">这几乎是一个基本但有用的模型。你可以使用这个具有不同隐藏层和神经元的相同模型来解决自然语言处理中的相当多的问题。我还在文章中提供了更多的资源，可以用来改进或更改模型，并尝试不同的模型。随意用不同数量的隐藏层、神经元、激活函数、度量或优化器来改变模型，并进行尝试。那会给你很多学习经验。</p><p id="28b3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请随时在Twitter上关注我。</p><h2 id="e2f0" class="mj mk jj bd ml mm mn dn mo mp mq dp mr lh ms mt mu ll mv mw mx lp my mz na nb bi translated">更多阅读</h2><div class="is it gp gr iu lu"><a rel="noopener follow" target="_blank" href="/exploratory-data-analysis-of-text-data-including-visualization-and-sentiment-analysis-e46dda3dd260"><div class="lv ab fo"><div class="lw ab lx cl cj ly"><h2 class="bd jk gy z fp lz fr fs ma fu fw ji bi translated">文本数据的探索性数据分析，包括可视化和情感分析</h2><div class="mc l"><p class="bd b dl z fp lz fr fs ma fu fw dk translated">towardsdatascience.com</p></div></div><div class="md l"><div class="ph l mf mg mh md mi ja lu"/></div></div></a></div><div class="is it gp gr iu lu"><a rel="noopener follow" target="_blank" href="/four-popular-feature-selection-methods-for-efficient-machine-learning-in-python-fdd34762efdb"><div class="lv ab fo"><div class="lw ab lx cl cj ly"><h2 class="bd jk gy z fp lz fr fs ma fu fw ji bi translated">Python中高效机器学习的四种流行特征选择方法</h2><div class="mb l"><h3 class="bd b gy z fp lz fr fs ma fu fw dk translated">使用真实数据集执行特征选择方法，并在每个方法后检索所选特征</h3></div><div class="mc l"><p class="bd b dl z fp lz fr fs ma fu fw dk translated">towardsdatascience.com</p></div></div><div class="md l"><div class="pi l mf mg mh md mi ja lu"/></div></div></a></div><div class="is it gp gr iu lu"><a rel="noopener follow" target="_blank" href="/an-overview-of-performance-evaluation-metrics-of-machine-learning-classification-algorithms-7a95783a762f"><div class="lv ab fo"><div class="lw ab lx cl cj ly"><h2 class="bd jk gy z fp lz fr fs ma fu fw ji bi translated">机器学习(分类)算法性能评价指标综述</h2><div class="mb l"><h3 class="bd b gy z fp lz fr fs ma fu fw dk translated">开发一个分类模型和计算所有流行的性能评估指标使用…</h3></div><div class="mc l"><p class="bd b dl z fp lz fr fs ma fu fw dk translated">towardsdatascience.com</p></div></div><div class="md l"><div class="pj l mf mg mh md mi ja lu"/></div></div></a></div><div class="is it gp gr iu lu"><a rel="noopener follow" target="_blank" href="/exploratory-data-analysis-visualization-and-prediction-model-in-python-241b954e1731"><div class="lv ab fo"><div class="lw ab lx cl cj ly"><h2 class="bd jk gy z fp lz fr fs ma fu fw ji bi translated">Python中的探索性数据分析、可视化和预测模型</h2><div class="mb l"><h3 class="bd b gy z fp lz fr fs ma fu fw dk translated">在Python中使用Pandas、Matplotlib、Seaborn和Scikit_learn库</h3></div><div class="mc l"><p class="bd b dl z fp lz fr fs ma fu fw dk translated">towardsdatascience.com</p></div></div><div class="md l"><div class="pk l mf mg mh md mi ja lu"/></div></div></a></div><div class="is it gp gr iu lu"><a rel="noopener follow" target="_blank" href="/stochastic-gradient-descent-explanation-and-complete-implementation-from-scratch-a2c6a02f28bd"><div class="lv ab fo"><div class="lw ab lx cl cj ly"><h2 class="bd jk gy z fp lz fr fs ma fu fw ji bi translated">随机梯度下降:从头开始的解释和完整实现</h2><div class="mb l"><h3 class="bd b gy z fp lz fr fs ma fu fw dk translated">使用单个感知器</h3></div><div class="mc l"><p class="bd b dl z fp lz fr fs ma fu fw dk translated">towardsdatascience.com</p></div></div><div class="md l"><div class="pl l mf mg mh md mi ja lu"/></div></div></a></div><div class="is it gp gr iu lu"><a rel="noopener follow" target="_blank" href="/an-ultimate-cheatsheet-of-data-visualization-in-seaborn-be8ed13a3697"><div class="lv ab fo"><div class="lw ab lx cl cj ly"><h2 class="bd jk gy z fp lz fr fs ma fu fw ji bi translated">用Python的Seaborn库实现数据可视化的终极指南</h2><div class="mb l"><h3 class="bd b gy z fp lz fr fs ma fu fw dk translated">对学习者来说也是一个很好的资源</h3></div><div class="mc l"><p class="bd b dl z fp lz fr fs ma fu fw dk translated">towardsdatascience.com</p></div></div><div class="md l"><div class="pm l mf mg mh md mi ja lu"/></div></div></a></div></div></div>    
</body>
</html>