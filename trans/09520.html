<html>
<head>
<title>Feature Selection using Logistic Regression Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用逻辑回归模型的特征选择</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-selection-using-logistic-regression-model-efc949569f58?source=collection_archive---------3-----------------------#2021-09-04">https://towardsdatascience.com/feature-selection-using-logistic-regression-model-efc949569f58?source=collection_archive---------3-----------------------#2021-09-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6384" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用正则化移除冗余要素</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a4f47a37529ef130dcd1ae382eb781a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w3oISeIk3DlDrcCT8zy_xQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://pixabay.com/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=1767563" rel="noopener ugc nofollow" target="_blank">皮克斯拜</a></p></figure><p id="61d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">特征工程是数据科学模型开发管道的重要组成部分。<em class="lv">‘更多的数据导致更好的机器学习模型’</em>，对实例的数量成立，但对特征的数量不成立。</p><p id="2376" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据科学家将大部分工作时间用于准备相关特征，以训练一个健壮的机器学习模型。原始数据集包含大量可能影响模型性能的冗余要素。特征选择是一个特征工程组件，它涉及删除不相关的特征，并挑选最佳的特征集来训练健壮的机器学习模型。</p><p id="a9f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">特征选择方法降低了数据的维数，避免了维数灾难的问题。在我以前的一篇文章中，我已经广泛地讨论了<a class="ae ky" rel="noopener" target="_blank" href="/top-7-feature-selection-techniques-in-machine-learning-94e08730cd09"> 7特性选择技术</a>:</p><div class="lw lx gp gr ly lz"><a rel="noopener follow" target="_blank" href="/top-7-feature-selection-techniques-in-machine-learning-94e08730cd09"><div class="ma ab fo"><div class="mb ab mc cl cj md"><h2 class="bd iu gy z fp me fr fs mf fu fw is bi translated">机器学习中的7大特征选择技术</h2><div class="mg l"><h3 class="bd b gy z fp me fr fs mf fu fw dk translated">选择最佳功能的流行策略</h3></div><div class="mh l"><p class="bd b dl z fp me fr fs mf fu fw dk translated">towardsdatascience.com</p></div></div><div class="mi l"><div class="mj l mk ml mm mi mn ks lz"/></div></div></a></div><p id="8180" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将讨论如何使用带有L1正则化的逻辑回归模型来移除数据中的冗余特征。</p><h1 id="3874" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">想法:</h1><p id="7498" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">正则化是一种通过向误差函数添加惩罚来调整模型的技术。正则化可以用于训练模型，这些模型可以更好地概括测试数据或看不见的数据，并防止算法过度拟合训练数据集。</p><blockquote class="nl nm nn"><p id="e923" class="kz la lv lb b lc ld ju le lf lg jx lh no lj lk ll np ln lo lp nq lr ls lt lu im bi translated">L2正则化指的是等价于系数大小的平方的惩罚，而L1正则化引入了等价于系数绝对值之和的惩罚(收缩量)。</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/f903b041f887615d7f95dad48187c722.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*pbNkP16hZBxc3GU9wZ6MBw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)，L1和L2正则项</p></figure><p id="5947" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">L1正则化在数据集中引入了稀疏性，它可以通过消除不重要的特征来执行特征选择。Lasso或L1正则化将冗余特征的系数缩小到0，因此这些特征可以从训练样本中移除。</p><h1 id="8f57" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">实施:</h1><blockquote class="nl nm nn"><p id="7aca" class="kz la lv lb b lc ld ju le lf lg jx lh no lj lk ll np ln lo lp nq lr ls lt lu im bi translated">从Kaggle下载的<a class="ae ky" href="https://www.kaggle.com/mlg-ulb/creditcardfraud" rel="noopener ugc nofollow" target="_blank">信用卡欺诈检测数据集</a>用于演示使用Lasso回归模型的特征选择实现</p></blockquote><ul class=""><li id="0c4c" class="ns nt it lb b lc ld lf lg li nu lm nv lq nw lu nx ny nz oa bi translated">读取数据集并执行要素工程(标准化)以使其适合训练逻辑回归模型。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><ul class=""><li id="c1f5" class="ns nt it lb b lc ld lf lg li nu lm nv lq nw lu nx ny nz oa bi translated">在标准化的训练样本上训练最适合的逻辑回归模型。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><ul class=""><li id="4285" class="ns nt it lb b lc ld lf lg li nu lm nv lq nw lu nx ny nz oa bi translated">使用<code class="fe od oe of og b"><strong class="lb iu">model.coef_</strong></code> <strong class="lb iu"> </strong>函数计算逻辑回归模型的系数，该函数返回逻辑回归划分平面的权重向量。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/be6ff7cac0292301d69088e2b2404046.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uV5kIf-E9xbS1fzmCm4qjg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)，逻辑回归模型的系数值</p></figure><p id="9214" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">系数向量的维数与训练数据集中的特征数量相同。</p><ul class=""><li id="419c" class="ns nt it lb b lc ld lf lg li nu lm nv lq nw lu nx ny nz oa bi translated">等于0的系数值是冗余特征，可以从训练样本中去除。从上面的系数向量快照可以看出，我们有系数为0的<strong class="lb iu"> 7特征</strong>。</li></ul><pre class="kj kk kl km gt oi og oj ok aw ol bi"><span id="5fb3" class="om mp it og b gy on oo l op oq"><strong class="og iu">coef = model.coef_[0]<br/>imp_features = pd.Series(X_std.columns)[list(coef!=0)]</strong></span><span id="5932" class="om mp it og b gy or oo l op oq"><strong class="og iu">X_train = X_train[imp_features]<br/>X_test = X_test[imp_features]</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/db36efeb1bb2c7b4289037bbd5154c01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XIkSU5rXts_kzb25FMRgzQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)，从信用卡数据集中删除的7个冗余特征的列表</p></figure><p id="8268" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在特征选择实现之前，训练样本具有29个特征，在去除7个冗余特征之后，训练样本减少到22个特征。</p><h2 id="540a" class="om mp it bd mq ot ou dn mu ov ow dp my li ox oy na lm oz pa nc lq pb pc ne pd bi translated">注意:</h2><p id="e3dd" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">逻辑回归模型的参数<code class="fe od oe of og b">‘C’</code>影响系数项。当正则化变得越来越松散或者<code class="fe od oe of og b">‘C’</code>的值减小时，我们得到更多的系数值为0。必须记住保持<code class="fe od oe of og b"> ‘C’</code>的正确值，以获得所需数量的冗余特征。</p><p id="6af5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">较高的“C”值可能认为重要的特征是多余的，而较低的“C”值可能不排除多余的特征。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/664036cdbe9f79e2e9d2f4dc431223c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*DeIaxFb19ibrxWnKdfoi1Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(<a class="ae ky" href="https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_path.html" rel="noopener ugc nofollow" target="_blank">来源</a>)，C与非零系数项的关系图</p></figure><h1 id="828e" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">结论:</h1><p id="e3d2" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">套索回归(带L1正则化的逻辑回归)可用于移除数据集中的冗余要素。L1正则化在数据集中引入了稀疏性，并将冗余特征的系数的值缩小到0。</p><p id="fbbe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过移除不相关的特征来降低数据集的维度是一种非常有用的技术或技巧。</p><p id="0aec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还有各种其他的特征选择技术。我在以前的一篇文章中已经讨论过<a class="ae ky" rel="noopener" target="_blank" href="/top-7-feature-selection-techniques-in-machine-learning-94e08730cd09"> 7这样的特征选择技术</a>:</p><div class="lw lx gp gr ly lz"><a rel="noopener follow" target="_blank" href="/top-7-feature-selection-techniques-in-machine-learning-94e08730cd09"><div class="ma ab fo"><div class="mb ab mc cl cj md"><h2 class="bd iu gy z fp me fr fs mf fu fw is bi translated">机器学习中的7大特征选择技术</h2><div class="mg l"><h3 class="bd b gy z fp me fr fs mf fu fw dk translated">选择最佳功能的流行策略</h3></div><div class="mh l"><p class="bd b dl z fp me fr fs mf fu fw dk translated">towardsdatascience.com</p></div></div><div class="mi l"><div class="mj l mk ml mm mi mn ks lz"/></div></div></a></div><h1 id="7506" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">参考资料:</h1><p id="a064" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">[1] Scikit-learn文档:<a class="ae ky" href="https://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_path.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/auto _ examples/linear _ model/plot _ logistic _ path . html</a></p><blockquote class="pf"><p id="9896" class="pg ph it bd pi pj pk pl pm pn po lu dk translated">感谢您的阅读</p></blockquote></div></div>    
</body>
</html>