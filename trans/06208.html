<html>
<head>
<title>Aspect-based Document Similarity using Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于特征的文档相似度的变换器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/aspect-based-document-similarity-using-transformers-8a2f820e5692?source=collection_archive---------22-----------------------#2021-06-03">https://towardsdatascience.com/aspect-based-document-similarity-using-transformers-8a2f820e5692?source=collection_archive---------22-----------------------#2021-06-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b4b4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">NLP研究论文解释</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/dfe27e979c51d94f54b5556b07f4b3f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cgOR0XEaQy7ffLETMXn19Q.png"/></div></div></figure><p id="f239" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="lq">在这篇博客中，我试图根据我的理解，使用变形金刚</em>  <em class="lq">来总结论文</em> <a class="ae lr" href="https://www.aclweb.org/anthology/2020.coling-main.545.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lq">中基于方面的文档相似度。请随时评论你的想法！</em></a></p><h1 id="adfd" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz md ka me kc mf kd mg kf mh kg mi mj bi translated">问题陈述</h1><p id="6b5f" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">当前的<a class="ae lr" rel="noopener" target="_blank" href="/a-complete-beginners-guide-to-document-similarity-algorithms-75c44035df90">文档相似性</a>技术主要集中在比较文档，而没有考虑它们固有的文档结构。这种相似性技术被称为<strong class="kw iu">方面无关的相似性</strong>。这里，similarity函数只返回一个介于0–1之间的值，用来衡量两个文档之间的相似程度，这多少有点像一个无法解释的相似性黑盒。此外，这可能会限制像<a class="ae lr" href="https://en.wikipedia.org/wiki/Recommender_system" rel="noopener ugc nofollow" target="_blank">推荐系统</a>这样的应用程序的性能，这些应用程序主要依赖于文档相似性作为基础。例如，您可能希望设计一个推荐系统，仅基于“方法论”或“数据集”部分返回与现有论文相似的论文，这里的相似性通常被认为是多方面的，而当前的相似性系统无法捕捉这种粒度。</p><p id="d3a6" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">本文正是以此为目标，将与方面无关的相似性扩展到基于方面的相似性。为了结合方面的概念，他们将测量相似性的任务建模为<strong class="kw iu">成对的文档分类任务</strong>。此外，他们使用<a class="ae lr" href="https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/" rel="noopener ugc nofollow" target="_blank"> Transformer </a>模型，如<a class="ae lr" href="https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/" rel="noopener ugc nofollow" target="_blank"> RoBERTa </a>和<a class="ae lr" href="https://en.wikipedia.org/wiki/BERT_(language_model)" rel="noopener ugc nofollow" target="_blank"> BERT </a>变体，在来自<a class="ae lr" href="https://acl2020.org/" rel="noopener ugc nofollow" target="_blank"> ACL </a>和<a class="ae lr" href="https://allenai.org/data/cord-19" rel="noopener ugc nofollow" target="_blank"> CORD-19 </a>的研究论文数据集上执行和评估他们的方法。</p><p id="e0cb" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="lq">下图显示了与方面无关和基于方面的相似性的图示视图— </em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mp"><img src="../Images/d4507f73a8afe8a7dd4c227aad2eead9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-Xt3XKRQwvm65hU7fJhV9A.png"/></div></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">图片来自<a class="ae lr" href="https://www.aclweb.org/anthology/2020.coling-main.545.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="b6dc" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在上图中，给定一个<strong class="kw iu">种子文档</strong>，基于某些<strong class="kw iu">距离度量，如欧几里德距离、余弦距离</strong>等，无特征相似性返回<a class="ae lr" href="https://www.geeksforgeeks.org/k-nearest-neighbours/" rel="noopener ugc nofollow" target="_blank"> k近邻</a>。然而，基于方面的相似性方法将只返回那些对于某些方面<em class="lq"> (a1) </em>相似的文档，导致低<a class="ae lr" href="https://en.wikipedia.org/wiki/False_positives_and_false_negatives#:~:text=A%20false%20positive%20is%20an,a%20condition%20when%20it%20is" rel="noopener ugc nofollow" target="_blank">假阳性</a>。</p><blockquote class="mu"><p id="761c" class="mv mw it bd mx my mz na nb nc nd lp dk translated">如果你想继续享受阅读与数据科学和机器学习相关的精彩文章的乐趣，你可以通过我的<a class="ae lr" href="https://prakhar-mishra.medium.com/membership" rel="noopener">推荐链接</a>:)购买中级会员资格</p></blockquote><h1 id="ff28" class="ls lt it bd lu lv lw lx ly lz ma mb mc jz ne ka me kc nf kd mg kf ng kg mi mj bi translated">提议的方法</h1><p id="849b" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">他们将整个问题建模为<strong class="kw iu">多类多标签分类</strong>问题。在本节中，我们将介绍数据集准备和模型训练方面——</p><h2 id="b133" class="nh lt it bd lu ni nj dn ly nk nl dp mc ld nm nn me lh no np mg ll nq nr mi ns bi translated">数据集准备</h2><p id="b696" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">在这项任务中，从研究论文中获取人工注释的数据成本很高，如果可能的话，可能仅限于少量数据。因此，作者通过将<strong class="kw iu">引用视为训练信号</strong>来继续并自动化这一过程，即如果任何两篇论文之间存在引用，我们认为这些论文是相似的。此外，为了合并节级别信息<em class="lq">(方面)，</em>他们选择引用其他文档的种子文档的节的标题作为类标签。例如，在下图中，种子文档引用了介绍和讨论部分下的目标文档。因此，我们将种子和目标作为带有标签介绍和讨论的文档对。因此，分类类型在本质上将是<strong class="kw iu">多类<em class="lq">(因为多个标题)</em>以及多标签<em class="lq">(同一篇论文在不同章节下的多次引用)</em> </strong> <em class="lq"> </em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/f4341a866a068fb95400c0a74edb5fa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*JHEGiagYMKWlTl4GKgGmOw.png"/></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">使用引用信号的数据注释|图片来自<a class="ae lr" href="https://www.aclweb.org/anthology/2020.coling-main.545.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="1ae3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">现在，由于对一个章节可以采用什么样的标题没有特定的标准，作者有意识地对某些章节标题进行标准化、分组和划分，以获得所有论文的固定标题集，他们还将离群值或无法识别的章节放在“其他”类别中。下图显示了<strong class="kw iu"> ACL </strong> <em class="lq">(左)</em><strong class="kw iu">CORD</strong>-<strong class="kw iu">19</strong><em class="lq">(右)</em>纸张的标签分布-</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/2682b4699ba38814ac108eb1f313999b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GQECqTUTA6z-G-K0oV-YPA.png"/></div></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">ACL和CORD-19数据集的标签分布|图片来自<a class="ae lr" href="https://www.aclweb.org/anthology/2020.coling-main.545.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="9c49" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">此外，除了只有正信号样本之外，他们还引入了一个名为“<strong class="kw iu"> None </strong>的新类，它在相同的比例下充当我们正样本的<a class="ae lr" href="http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/" rel="noopener ugc nofollow" target="_blank">负对应</a> <strong class="kw iu"> </strong>。他们将一组论文归入这一类别的一般经验法则是，论文<strong class="kw iu">首先不应该是正对的，不应该一起被共同引用，不应该共享任何作者，并且不应该在同一地点发表</strong>。</p><p id="7d26" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在所有这些步骤和转换之后，我们已经准备好了数据集！</p><h2 id="2fe4" class="nh lt it bd lu ni nj dn ly nk nl dp mc ld nm nn me lh no np mg ll nq nr mi ns bi translated">模特培训</h2><p id="f4b9" class="pw-post-body-paragraph ku kv it kw b kx mk ju kz la ml jx lc ld mm lf lg lh mn lj lk ll mo ln lo lp im bi translated">为了训练相似性模型，作者从seed和其他研究论文中提取论文标题和摘要，并将其视为整个文档的代理。提取后，它们用<strong class="kw iu"> cls </strong>和<strong class="kw iu">分隔符</strong>标记连接这些片段，形成一个大序列。这然后被馈送到变换器模型，并且在输出端，它们在CLS表示法上堆叠一个<strong class="kw iu">分类层，并且用<strong class="kw iu"> </strong> <a class="ae lr" href="https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html" rel="noopener ugc nofollow" target="_blank"> <strong class="kw iu">交叉熵损失</strong> </a> <strong class="kw iu">作为罚信号</strong>来对照地面真相中存在的标签训练模型。<em class="lq">下图显示的是同一— </em>的图示</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/ea5f42864556287820bb7d54c55a7538.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*GVDzX1EkGr9P-DUZUtj43w.png"/></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">模型训练|图片来自<a class="ae lr" href="https://www.aclweb.org/anthology/2020.coling-main.545.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><blockquote class="mu"><p id="4fe9" class="mv mw it bd mx my nw nx ny nz oa lp dk translated">如果你愿意，你也可以查看我写的其他研究论文摘要。</p></blockquote><p id="30b9" class="pw-post-body-paragraph ku kv it kw b kx oc ju kz la od jx lc ld oe lf lg lh of lj lk ll og ln lo lp im bi translated">所以，是的，这就是我的博客。我有一个同样的多语种字幕视频漫游，如果你喜欢消费视频内容而不是文本(就像我一样:D)，一定要看看—</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oh oi l"/></div><p class="mq mr gj gh gi ms mt bd b be z dk translated"><a class="ae lr" href="https://www.youtube.com/channel/UCoz8NrwgL7U9535VNc0mRPA" rel="noopener ugc nofollow" target="_blank">多看看这样的视频</a></p></figure><p id="155f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">请随意阅读整篇论文，并向作者问好，感谢他们的贡献。</p><blockquote class="oj ok ol"><p id="a94c" class="ku kv lq kw b kx ky ju kz la lb jx lc om le lf lg on li lj lk oo lm ln lo lp im bi translated"><strong class="kw iu"> <em class="it">论文标题:</em> </strong> <em class="it">基于方面的文档相似度使用变形金刚</em></p><p id="6e07" class="ku kv lq kw b kx ky ju kz la lb jx lc om le lf lg on li lj lk oo lm ln lo lp im bi translated"><strong class="kw iu"> <em class="it">论文链接:</em></strong><a class="ae lr" href="https://www.aclweb.org/anthology/2020.coling-main.545.pdf" rel="noopener ugc nofollow" target="_blank"><em class="it">https://www.aclweb.org/anthology/2020.coling-main.545.pdf</em></a></p><p id="39db" class="ku kv lq kw b kx ky ju kz la lb jx lc om le lf lg on li lj lk oo lm ln lo lp im bi translated"><strong class="kw iu"> <em class="it">作者:</em> </strong> <em class="it">马尔特·奥斯坦多夫、特里·鲁斯、蒂尔·布卢姆、贝拉·吉普、格奥尔格·雷姆</em></p></blockquote><p id="55b1" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我希望你喜欢读这篇文章。如果你愿意支持我成为一名作家，可以考虑注册<a class="ae lr" href="https://prakhar-mishra.medium.com/membership" rel="noopener">成为一名媒体成员</a>。每月只需5美元，你就可以无限制地使用Medium。</p><p id="d178" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><em class="lq">感谢您的宝贵时间！❤ </em></p></div></div>    
</body>
</html>