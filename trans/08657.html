<html>
<head>
<title>Linear regression: Comparing pythons sklearn with ‘writing it from scratch’</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归:pythons sklearn与‘从头开始写’的比较</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/challenge-can-i-do-linear-regression-without-sklearn-3151d8fc8aa?source=collection_archive---------9-----------------------#2021-08-10">https://towardsdatascience.com/challenge-can-i-do-linear-regression-without-sklearn-3151d8fc8aa?source=collection_archive---------9-----------------------#2021-08-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="41d8" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">曾经想知道python的sklearn中多元线性回归/梯度下降的幕后发生了什么吗？你会惊讶它是多么容易。让我们从头开始写，并对两者应用相同的评估方法，看看我们做得如何。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/58d1550e810fbac4aa34655218713fa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A_vuKQQp8YT4XV6EQVojSQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者Sani2C线性回归能让我骑得更快吗？</p></figure><h1 id="f438" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">介绍</h1><p id="a901" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">所以，从理解的角度来看，理解线性回归中发生的事情是很好的。这里是一个没有使用python库的的<strong class="lp ir">的深度探索。这里有一个<a class="ae mj" href="https://github.com/shaunenslin/machinelearning/blob/master/python/linearregression/lifeexpectency.ipynb" rel="noopener ugc nofollow" target="_blank">链接</a>指向github中这篇文章的源代码。如果你确实需要梯度下降的介绍，先看看我的5集YouTube系列。</strong></p><blockquote class="mk ml mm"><p id="d229" class="ln lo mn lp b lq mo jr ls lt mp ju lv mq mr ly lz ms mt mc md mu mv mg mh mi ij bi translated">在第一步中，我们将从头开始编写梯度下降，而在第二步中，我们将使用sklearn的线性回归。</p></blockquote><h1 id="73ff" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">我们的数据集</h1><p id="90a4" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">让我们从<a class="ae mj" href="https://www.kaggle.com/kumarajarshi/life-expectancy-who" rel="noopener ugc nofollow" target="_blank"> kaggle </a>下载我们的数据集。世界卫生组织(世卫组织)下属的全球卫生观察站(GHO)数据库记录所有国家的健康状况以及许多其他相关因素。这些数据集向公众开放，用于卫生数据分析。与193个国家的预期寿命、健康因素相关的数据集从同一个世卫组织数据库网站收集，其相应的经济数据从联合国网站收集。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mw"><img src="../Images/72886e3835c33cf11fdae808f2f349b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c_evikGDd9-Le4Bm2dzM9A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图kaggle的屏幕截图</p></figure></div><div class="ab cl mx my hu mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="ij ik il im in"><h1 id="4b55" class="kv kw iq bd kx ky ne la lb lc nf le lf jw ng jx lh jz nh ka lj kc ni kd ll lm bi translated">步骤1:线性回归/梯度下降从零开始</h1><p id="cc9c" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">让我们从导入我们的库开始，看看前几行。</p><pre class="kg kh ki kj gt nj nk nl nm aw nn bi"><span id="4ae1" class="no kw iq nk b gy np nq l nr ns">import pandas as pd<br/>import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>import numpy as np<br/>from sklearn.preprocessing import LabelEncoder<br/>from sklearn import metrics</span><span id="82c2" class="no kw iq nk b gy nt nq l nr ns">df = pd.read_csv(‘Life Expectancy Data.csv’)<br/>df.head()</span></pre><p id="cf45" class="pw-post-body-paragraph ln lo iq lp b lq mo jr ls lt mp ju lv lw mr ly lz ma mt mc md me mv mg mh mi ij bi translated">您将看到一个很好的数据视图，可以看到我们有国家和状态文本字段，而“预期寿命”是我们想要预测的字段。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/5c43f79a03198ddb55afaa094eae5086.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MAukNT6kBqcDPp3qUTjAmQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图jupyter的屏幕截图</p></figure><p id="8969" class="pw-post-body-paragraph ln lo iq lp b lq mo jr ls lt mp ju lv lw mr ly lz ma mt mc md me mv mg mh mi ij bi translated">为了获得更多的洞察力，让我们运行一个info，我们将在图3中得到下面的info。</p><ul class=""><li id="0ad9" class="nv nw iq lp b lq mo lt mp lw nx ma ny me nz mi oa ob oc od bi translated">我们有2个文本字段，即。国家和地位</li><li id="e140" class="nv nw iq lp b lq oe lt of lw og ma oh me oi mi oa ob oc od bi translated">酒精、乙肝等领域。有我们需要解决的空值</li><li id="dea2" class="nv nw iq lp b lq oe lt of lw og ma oh me oi mi oa ob oc od bi translated">列名需要一些工作</li></ul><pre class="kg kh ki kj gt nj nk nl nm aw nn bi"><span id="697f" class="no kw iq nk b gy np nq l nr ns">df.info()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/e1a1f3dde4f1250a53d1d1745492bdfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*K3J0yPXfA9AQeXXncTyjwQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图jupyter的屏幕截图</p></figure><h1 id="aaf0" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">准备数据</h1><p id="f83c" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">这些名称不适合使用，所以让我们重命名一些列。然后将对象字段转换为数字，因为我们不能处理文本。最后，让我们将Y移动到它自己的数组中，并将其从“df”中删除。这种陡峭的结果是,‘df’是我们的特征集并且只包含数字，而‘y’是我们的结果集。</p><pre class="kg kh ki kj gt nj nk nl nm aw nn bi"><span id="8daf" class="no kw iq nk b gy np nq l nr ns">df.rename(columns = {“ BMI “ :”BMI”,<br/>  “Life expectancy “: “Life_expectancy”,<br/>  “Adult Mortality”:”Adult_mortality”,<br/>  “infant deaths”:”Infant_deaths”,<br/>  “percentage expenditure”:”Percentage_expenditure”,<br/>  “Hepatitis B”:”HepatitisB”,<br/>  “Measles “:”Measles”,<br/>  “under-five deaths “: “Under_five_deaths”,<br/>  “Total expenditure”:”Total_expenditure”,<br/>  “Diphtheria “: “Diphtheria”,<br/>  “ thinness 1–19 years”:”Thinness_1–19_years”,<br/>  “ thinness 5–9 years”:”Thinness_5–9_years”,<br/>  “ HIV/AIDS”:”HIV/AIDS”,<br/>  “Income composition of resources”:<br/>  ”Income_composition_of_resources”}, inplace = True)</span><span id="0e27" class="no kw iq nk b gy nt nq l nr ns"># convert labels to numbers<br/>columns = [“Status”,”Country”]<br/>for feature in columns:<br/>  le = LabelEncoder()<br/>  df[feature] = le.fit_transform(df[feature])</span><span id="6ac8" class="no kw iq nk b gy nt nq l nr ns"># extract Y and drop from data frame<br/>Y = df[“Life_expectancy”]<br/>df = df.drop([“Life_expectancy”], axis=1)</span></pre><h1 id="82e2" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">绘图相关矩阵</h1><p id="8920" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">如果我们有一个包含许多列的数据集，快速检查列间相关性的一个好方法是将相关性矩阵可视化为热图。查看该矩阵，您可以看到9列具有高于0.38的最高相关性。我们只对“预期寿命”感兴趣，所以请看最下面一行的结果。</p><pre class="kg kh ki kj gt nj nk nl nm aw nn bi"><span id="5520" class="no kw iq nk b gy np nq l nr ns">plt.figure(figsize = (24,16))<br/>sns.heatmap(pd.concat([df,Y], axis=1).corr(), annot=True, cmap=”coolwarm”)</span></pre><blockquote class="mk ml mm"><p id="68f4" class="ln lo mn lp b lq mo jr ls lt mp ju lv mq mr ly lz ms mt mc md mu mv mg mh mi ij bi translated">为了获得更好的结果，我们可以选择在相关矩阵中仅使用高于0.3的特征。</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ok"><img src="../Images/f1c17acb2d44bdde8bb8a0f1eeadecb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Hn1kAG6_VrkPDFgP6tCVA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图jupyter的屏幕截图</p></figure><h1 id="76ba" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">为我们缺失的特征填入数值</h1><p id="8c71" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">回归的一个重要部分是理解哪些特征缺失了。我们可以选择忽略所有缺少值的行，或者用mode、median或mode填充它们。下面是一个手动函数，它基于以下三种方法之一来填充缺失值:</p><ul class=""><li id="79ef" class="nv nw iq lp b lq mo lt mp lw nx ma ny me nz mi oa ob oc od bi translated">模式=最常见的值</li><li id="cb59" class="nv nw iq lp b lq oe lt of lw og ma oh me oi mi oa ob oc od bi translated">中位数=中间值</li><li id="859c" class="nv nw iq lp b lq oe lt of lw og ma oh me oi mi oa ob oc od bi translated">平均值=平均</li></ul><pre class="kg kh ki kj gt nj nk nl nm aw nn bi"><span id="97ab" class="no kw iq nk b gy np nq l nr ns">def fillmissing(df, feature, method):<br/>  if method == “mode”:<br/>    df[feature] = df[feature].fillna(df[feature].mode()[0])<br/>  elif method == “median”:<br/>    df[feature] = df[feature].fillna(df[feature].median())<br/>  else:<br/>    df[feature] = df[feature].fillna(df[feature].mean())</span></pre><p id="1ee4" class="pw-post-body-paragraph ln lo iq lp b lq mo jr ls lt mp ju lv lw mr ly lz ma mt mc md me mv mg mh mi ij bi translated">现在，查找缺少值的列，并使用我们方便的函数填充它们。让我们也填写y中任何缺失的值。</p><pre class="kg kh ki kj gt nj nk nl nm aw nn bi"><span id="d147" class="no kw iq nk b gy np nq l nr ns">features_missing= df.columns[df.isna().any()]<br/>for feature in features_missing:<br/>  fillmissing(df, feature= feature, method= “mean”)</span><span id="ad92" class="no kw iq nk b gy nt nq l nr ns">Y.fillna(Y.median(), inplace=True)</span></pre><h1 id="5e85" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">获取X/Y数组</h1><p id="d219" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">让我们将数据帧放入易于操作的数组中。</p><pre class="kg kh ki kj gt nj nk nl nm aw nn bi"><span id="7739" class="no kw iq nk b gy np nq l nr ns">X = df.to_numpy() <br/>y = Y.to_numpy().transpose() <br/>m,n = X.shape</span></pre><h1 id="8216" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">标准化X</h1><p id="6fa5" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">现在，让我们将X归一化，使其值介于-1和1之间。我们这样做是为了让所有的特征都在一个相似的范围内。如果我们需要绘制数据，这很有帮助，但也能给出更好的线性回归结果。我们使用下面的等式，你应该看到你的特征现在归一化为类似于图5的值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/38451487e98fc930d7468fe31c7a1f76.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*tNK2lSggnggPuL56zlI6qw.png"/></div></figure><pre class="kg kh ki kj gt nj nk nl nm aw nn bi"><span id="5401" class="no kw iq nk b gy np nq l nr ns">mu = X.mean(0) <br/>sigma = X.std(0) # standard deviation: max(x)-min(x)<br/>xn = (X — mu) / sigma</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/4e26960dcd43284bde1d3d18f11f6f21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*UFZhctJVeUMXv2X5JdfEQA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图jupyter的屏幕截图</p></figure><h1 id="400b" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">添加一列1</h1><p id="57af" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">现在在X上增加一列1，以便稍后对我们的假设和成本函数进行更简单的矩阵操作。您的数据现在应该如图6所示，带有一列1。</p><pre class="kg kh ki kj gt nj nk nl nm aw nn bi"><span id="8310" class="no kw iq nk b gy np nq l nr ns">xo = np.hstack((np.ones((m, 1)), xn))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/aa8a1d0dacfdd36314b9bedf020ea195.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/1*FeSZoe_tfrWBoGjnjRM5vQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图jupyter的屏幕截图</p></figure><h1 id="cb32" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">梯度下降</h1><p id="4aaa" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">创建梯度下降所需的变量。我们需要以下变量:</p><ul class=""><li id="4dbb" class="nv nw iq lp b lq mo lt mp lw nx ma ny me nz mi oa ob oc od bi translated">重复=重复梯度下降的次数</li><li id="4943" class="nv nw iq lp b lq oe lt of lw og ma oh me oi mi oa ob oc od bi translated">θ=对于X的每个特征，增加一列θ0</li><li id="2e01" class="nv nw iq lp b lq oe lt of lw og ma oh me oi mi oa ob oc od bi translated">成本历史=保持梯度下降的每次迭代的成本</li></ul><pre class="kg kh ki kj gt nj nk nl nm aw nn bi"><span id="0eed" class="no kw iq nk b gy np nq l nr ns">repeat = 1000<br/>lrate = 0.01<br/>theta = np.zeros((n+1))</span></pre><p id="e8d8" class="pw-post-body-paragraph ln lo iq lp b lq mo jr ls lt mp ju lv lw mr ly lz ma mt mc md me mv mg mh mi ij bi translated">让我们定义一个<strong class="lp ir">成本函数</strong>，梯度下降将使用它来确定每个θ的成本。成本函数将实现以下成本等式。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oo"><img src="../Images/a120ba59756012ba0d1f8e2eeb5aadf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OoGwLKqIvBl3JhJaTQUJhQ.png"/></div></div></figure><pre class="kg kh ki kj gt nj nk nl nm aw nn bi"><span id="bcdb" class="no kw iq nk b gy np nq l nr ns">def computeCost(X, y, theta):<br/>  m = len(y) # number of training examples<br/>  diff = np.matmul(X, theta) — y<br/>  J = 1 / (2 * m) * np.matmul(diff, diff)</span><span id="57de" class="no kw iq nk b gy nt nq l nr ns">  return J</span></pre><p id="b2ff" class="pw-post-body-paragraph ln lo iq lp b lq mo jr ls lt mp ju lv lw mr ly lz ma mt mc md me mv mg mh mi ij bi translated">我们现在进入梯度下降循环，在这里我们在每个循环上计算一个新的θ，并跟踪它的成本。参见下面的等式:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/1fe3638107e6b10a70d52f2e2f03d7d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8asi8XzkoEQcT64tc5YjDQ.png"/></div></div></figure><p id="497b" class="pw-post-body-paragraph ln lo iq lp b lq mo jr ls lt mp ju lv lw mr ly lz ma mt mc md me mv mg mh mi ij bi translated">现在我们看到了这个方程，让我们把它放入一个方便的函数中</p><pre class="kg kh ki kj gt nj nk nl nm aw nn bi"><span id="0903" class="no kw iq nk b gy np nq l nr ns">def gradientDescent(X, y, theta, alpha, num_iters):<br/>  # Initialize some useful values<br/>  m = len(y) # number of training examples<br/>  J_history = []</span><span id="5a75" class="no kw iq nk b gy nt nq l nr ns">  # repeat until convergance<br/>  for i in range(num_iters):<br/>    hc = np.matmul(X, theta) — y<br/>    theta -= alpha / m * np.matmul(X.transpose(), hc)<br/>    # Save the cost J in every iteration<br/>    J_history.append(computeCost(X, y, theta))</span><span id="aa33" class="no kw iq nk b gy nt nq l nr ns">  return theta, J_history</span></pre><p id="b794" class="pw-post-body-paragraph ln lo iq lp b lq mo jr ls lt mp ju lv lw mr ly lz ma mt mc md me mv mg mh mi ij bi translated">让我们运行梯度下降并打印结果</p><pre class="kg kh ki kj gt nj nk nl nm aw nn bi"><span id="1a23" class="no kw iq nk b gy np nq l nr ns">theta, J_history = gradientDescent(xo, y, theta, lrate, repeat)</span><span id="3d11" class="no kw iq nk b gy nt nq l nr ns"># Display gradient descent's result<br/>print('Best theta computed from gradient descent: ')<br/>print(f' {theta} ')</span></pre><h1 id="54ef" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated"><strong class="ak">绘制梯度下降的成本</strong></h1><p id="d66d" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">绘制成本历史以确保成本随着迭代次数的增加而减少。在绘制之后，您应该看到成本随着每次迭代而降低，如图7所示。</p><pre class="kg kh ki kj gt nj nk nl nm aw nn bi"><span id="0230" class="no kw iq nk b gy np nq l nr ns"># Plot the convergence graph<br/>plt.plot(np.arange(repeat), J_history, ‘-b’, LineWidth=2)<br/>plt.xlabel(‘Number of iterations’)<br/>plt.ylabel(‘Cost J’)<br/>plt.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oq"><img src="../Images/f7632f792926b71dc3de99b4ba24dde9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y-GzrkkpmkbcjJiCQNHAjg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图jupyter的屏幕截图</p></figure><h1 id="7677" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">预言；预测；预告</h1><p id="9d74" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">让我们用下面的等式来进行预测。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi or"><img src="../Images/0b41c45361d5c170fee36a0fd7d448d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*63kn-jJHiA9bR-c-kNq_bw.png"/></div></div></figure><p id="c968" class="pw-post-body-paragraph ln lo iq lp b lq mo jr ls lt mp ju lv lw mr ly lz ma mt mc md me mv mg mh mi ij bi translated"><strong class="lp ir">注意</strong>将一列1加到X上，然后使用矩阵乘法，一步就可以完成上面的等式。</p><pre class="kg kh ki kj gt nj nk nl nm aw nn bi"><span id="ecb3" class="no kw iq nk b gy np nq l nr ns">y_pred = np.matmul(xo, theta)</span></pre><h1 id="9702" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated"><strong class="ak">评估预测</strong></h1><p id="0b2f" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">让我们使用均方根误差(RMSE ),它是平方误差平均值的平方根。下面是应用的等式，其结果将在以后用于比较。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi os"><img src="../Images/5c6b50e6132522155af440fc16557f81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WAL1EAwEakhfrqr8JSjCSA.png"/></div></div></figure><p id="5c0f" class="pw-post-body-paragraph ln lo iq lp b lq mo jr ls lt mp ju lv lw mr ly lz ma mt mc md me mv mg mh mi ij bi translated">让我们也算出每个预测占真实结果的百分比。然后用它来找出实际年龄的90%到110%之间的预测年龄。我们认为这对于计算最终精度是可以接受的。</p><pre class="kg kh ki kj gt nj nk nl nm aw nn bi"><span id="1ed5" class="no kw iq nk b gy np nq l nr ns"># get RMSE error rate<br/>print('RMSE: ',np.sqrt(metrics.mean_squared_error(y, y_pred)))</span><span id="4bf4" class="no kw iq nk b gy nt nq l nr ns"># calculate our own accuracy where prediction within 10% is o<br/>diff = (y_pred / y * 100)</span><span id="3694" class="no kw iq nk b gy nt nq l nr ns">print('Mean of results: ',diff.mean())<br/>print('Deviation of results: ',diff.std())<br/>print('Results within 10% support/resistance: ', len(np.where(np.logical_and(diff&gt;=90, diff&lt;=110))[0]) / m * 100)</span></pre><p id="b83f" class="pw-post-body-paragraph ln lo iq lp b lq mo jr ls lt mp ju lv lw mr ly lz ma mt mc md me mv mg mh mi ij bi translated">您现在将看到如下结果。如果我们对每一行的预测都在实际年龄的10%以内，那么我们决定称之为成功。因此，我们最终达到了90%的准确率。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ot"><img src="../Images/ab29c314194f8ff797032d7bd386e632.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ou8A-T0NL_54CMEjCu7K7g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图jupyter的屏幕截图</p></figure><p id="cc3e" class="pw-post-body-paragraph ln lo iq lp b lq mo jr ls lt mp ju lv lw mr ly lz ma mt mc md me mv mg mh mi ij bi translated">最后，让我们想象一下每个预测的准确性。自然，<strong class="lp ir"> 100%是完美的预测</strong>。</p><pre class="kg kh ki kj gt nj nk nl nm aw nn bi"><span id="e9e4" class="no kw iq nk b gy np nq l nr ns">plt.plot(np.arange(m), diff, '-b', LineWidth=1)<br/>plt.xlabel('Number')<br/>plt.ylabel('Accuracy %')<br/>plt.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ou"><img src="../Images/bce96677ea1c09283a92e748430775cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XTZEOSSc_oi567AhbnKx7A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图jupyter的屏幕截图</p></figure><p id="92e0" class="pw-post-body-paragraph ln lo iq lp b lq mo jr ls lt mp ju lv lw mr ly lz ma mt mc md me mv mg mh mi ij bi translated">所以，现在我们已经看到了使用矩阵操作的线性回归，让我们看看结果如何与使用sklearn进行比较。</p></div><div class="ab cl mx my hu mz" role="separator"><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc nd"/><span class="na bw bk nb nc"/></div><div class="ij ik il im in"><h1 id="06fc" class="kv kw iq bd kx ky ne la lb lc nf le lf jw ng jx lh jz nh ka lj kc ni kd ll lm bi translated"><strong class="ak">第二步:使用sklearn的线性回归</strong></h1><p id="937a" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">让我们使用sklearn来执行线性回归。这次你可以看到它的代码少了很多。一旦我们有了一个预测，我们将使用RMSE和我们的支持/阻力计算来看看我们的手动计算如何与一个经过验证的sklearn函数进行比较。</p><pre class="kg kh ki kj gt nj nk nl nm aw nn bi"><span id="c0e8" class="no kw iq nk b gy np nq l nr ns">from sklearn import metrics<br/>from sklearn.linear_model import LinearRegression<br/>from sklearn.model_selection import train_test_split</span><span id="1309" class="no kw iq nk b gy nt nq l nr ns"># Split data<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)</span><span id="8687" class="no kw iq nk b gy nt nq l nr ns"># Instantiate model<br/>lm2 = LinearRegression()</span><span id="511a" class="no kw iq nk b gy nt nq l nr ns"># Fit Model<br/>lm2.fit(X_train, y_train)</span><span id="0bcf" class="no kw iq nk b gy nt nq l nr ns"># Predict<br/>y_pred2 = lm2.predict(X_test)</span><span id="9e0b" class="no kw iq nk b gy nt nq l nr ns"># RMSE<br/>print('RMSE: ',np.sqrt(metrics.mean_squared_error(y_test, y_pred2)))</span><span id="206f" class="no kw iq nk b gy nt nq l nr ns"># calculate our own accuracy where prediction within 10% is ok<br/>diff2 = (y_pred2 / y_test * 100)</span><span id="22bf" class="no kw iq nk b gy nt nq l nr ns">print('Mean of results: ',diff2.mean())<br/>print('Deviation of results: ',diff2.std())<br/>print('Results within 10% support/resistance: ', len(np.where(np.logical_and(diff2&gt;=90, diff2&lt;=110))[0]) / len(y_pred2) * 100)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ov"><img src="../Images/ac616b0c999caa80a9ec73602e11f198.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v0Nq_qsQrO1w8vMDbEM8Ww.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图jupyter的屏幕截图</p></figure><h1 id="c33d" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">结论</h1><p id="af02" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">你可以看到我们的RMSE和支持/抵制百分比在两种方法中是相似的。很自然，你可能会使用sklearn，因为它的代码少得多，但是希望这个例子向你展示了这些等式是如何工作的。</p></div></div>    
</body>
</html>