<html>
<head>
<title>Activate Early Stopping in Boosting Algorithms to Mitigate Overfitting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在增强算法中激活早期停止以减轻过度拟合</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/activate-early-stopping-in-boosting-algorithms-to-mitigate-overfitting-9c1b12cc6729?source=collection_archive---------16-----------------------#2021-11-08">https://towardsdatascience.com/activate-early-stopping-in-boosting-algorithms-to-mitigate-overfitting-9c1b12cc6729?source=collection_archive---------16-----------------------#2021-11-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="735b" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">助推技术</h2><div class=""/><div class=""><h2 id="6802" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">机器学习中的助推算法——第八部分</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/91f21a311240d25acb99a984086008aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2b5puGfzc065FY0dEz8Ozw.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片来自<a class="ae lh" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=4964455" rel="noopener ugc nofollow" target="_blank">皮克斯拜</a>的<a class="ae lh" href="https://pixabay.com/users/schraubgut-15720492/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=4964455" rel="noopener ugc nofollow" target="_blank">克里斯蒂安·里德</a></p></figure><p id="434d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在<a class="ae lh" rel="noopener" target="_blank" href="/performance-comparison-catboost-vs-xgboost-and-catboost-vs-lightgbm-886c1c96db64">第七部分</a>中，我已经提到过在提升算法中很容易发生过度拟合。过拟合是 boosting 技术的主要缺点之一。</p><p id="56f7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> <em class="me">提前停止</em> </strong>是一种特殊的技术，可用于减轻 boosting 算法中的过拟合。它在算法的训练阶段使用。</p><h1 id="8b66" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">提前停止在 boosting 算法中有多有效？</h1><p id="c2e6" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">在 boosting 中，在每个 boosting 轮(迭代)中添加一个新树。新树试图纠正树在前几轮中所犯的错误。</p><p id="275e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">顾名思义，提前停止在由<strong class="lk jd"> n_estimators </strong>给出的指定提升回合之前的某处停止算法的训练过程。可以通过绘制学习曲线来监控停止点。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/58c55a6d864dbdf6537d2833c7fce910.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*zOwEV4ppAboKWxbyZVr8vw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">(图片由作者提供)</p></figure><p id="7c02" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们通过提供训练和验证数据来计算训练和验证分数(通常，RMSE 用于回归，对数损失用于分类),然后将它们与增强回合数进行绘图。有一个点，验证分数没有进一步提高，并开始变得更差，而训练分数仍然继续提高。这是过度拟合开始和模型训练应该停止的地方。</p><p id="e53c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">根据迭代或推进轮次来确定该点的价值是非常重要的。在此之前，验证分数在每次迭代中都会提高。在这一点上，增强的模型在看不见的数据上概括得很好。在这一点之后，模型往往会过度拟合。</p><p id="1f8e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">默认情况下，提前停止不是由升压算法本身激活的。要激活 XGBoost、LightGBM 和 CatBoost 等 boosting 算法中的提前停止，我们应该在 boosting 模型的<strong class="lk jd"> fit() </strong>方法或<strong class="lk jd"> train() </strong>函数中的参数<strong class="lk jd"><em class="me">early _ stopping _ rounds</em></strong>中指定一个整数值。</p><pre class="ks kt ku kv gt nd ne nf ng aw nh bi"><span id="16a9" class="ni mg it ne b gy nj nk l nl nm">.fit(<strong class="ne jd"><em class="me">early_stopping_rounds=int</em></strong>)<br/>#OR<br/>.train(<strong class="ne jd"><em class="me">early_stopping_rounds=int</em></strong>)</span></pre><p id="9aa0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">整数值表示模型对看不见的数据进行良好概括的提升回合。如前所述，这可以通过监控学习曲线来发现。</p><h1 id="7dea" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">使用 CatBoost 执行提前停止</h1><p id="ce5f" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">CatBoost 是一个强大的升压算法。它有两个很好的特性，是其他 boosting 算法所没有的。它可以直接处理分类特征。另一个是我们可以通过在<strong class="lk jd"> fit() </strong>方法或者<strong class="lk jd"> train() </strong>函数中将<strong class="lk jd"> <em class="me"> plot </em> </strong>自变量设置为<code class="fe nn no np ne b">True</code>来轻松生成学习曲线。这就是我们在这里使用 CatBoost 实现提前停止的原因。</p><p id="7da2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，我们在<a class="ae lh" href="https://drive.google.com/file/d/19s5qMRjssBoohFb2NY4FFYQ3YW2eCxP4/view?usp=sharing" rel="noopener ugc nofollow" target="_blank"><strong class="lk jd">heart _ disease</strong></a><strong class="lk jd"/>数据集上构建 CatBoost 分类模型，并通过创建学习曲线来监控学习过程。除了训练数据，我们还应该为<strong class="lk jd"> fit() </strong>方法中的<strong class="lk jd"> <em class="me"> eval_set </em> </strong>参数提供验证(测试)数据。这里，我们使用<strong class="lk jd">“对数损失”</strong>作为我们模型的评估指标。对数损失值越低，预测概率与实际值的偏差就越小。因此，日志损失值越低越好。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nq nr l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">监控 CatBoost 的学习过程</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ns"><img src="../Images/74e490d9cc3d1723a444df244b097152.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*405maQT8CtQWkVLbavc2Fw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">(图片由作者提供)</p></figure><p id="7410" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">模型在看不见的数据上概括得很好的地方用绿点标出。可以通过运行以下命令获得 X 坐标:</p><pre class="ks kt ku kv gt nd ne nf ng aw nh bi"><span id="d840" class="ni mg it ne b gy nj nk l nl nm"><strong class="ne jd">cb.get_best_iteration()</strong></span></pre><p id="1a03" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这将返回 42，这意味着模型在第 42 次迭代时对看不见的数据进行了很好的概括。之后，模型开始过度拟合。我们可以为<strong class="lk jd"><em class="me">early _ stopping _ rounds</em></strong>argument<strong class="lk jd"><em class="me"/></strong>提供这个值来激活提前停止。</p><pre class="ks kt ku kv gt nd ne nf ng aw nh bi"><span id="30c0" class="ni mg it ne b gy nj nk l nl nm">cb.fit(X_test, y_test,<br/>       <strong class="ne jd"><em class="me">early_stopping_rounds=42</em></strong>)</span></pre><p id="6cc6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当您再次运行模型时，它将在第 42 轮提升(迭代)时停止训练，尽管我们已经在<strong class="lk jd"> <em class="me"> n_estimators </em> </strong>中指定了 550 轮提升！</p><h1 id="3999" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">关键要点</h1><p id="9555" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">我们已经讨论了如何实现针对 CatBoost 算法的早期停止，该算法具有内置功能来监控学习过程。但是，我们也可以将早期停止与 AdaBoost、梯度增强、XGBoost 和 LightGBM 等其他增强算法结合使用。下面是一些将早期停止应用于这些算法的具体想法。</p><h2 id="3d10" class="ni mg it bd mh nt nu dn ml nv nw dp mp lr nx ny mr lv nz oa mt lz ob oc mv iz bi translated">想法 1:创建学习曲线来监控学习过程</h2><p id="0d7e" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">使用其内置的<strong class="lk jd"> <em class="me">图</em> </strong>参数，很容易为 CatBoost 创建针对增强回合的学习曲线。对于其他算法，您需要手动创建学习曲线。</p><h2 id="911c" class="ni mg it bd mh nt nu dn ml nv nw dp mp lr nx ny mr lv nz oa mt lz ob oc mv iz bi translated">想法二:激活提前停车</h2><p id="afd1" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">XGBoost、LightGBM 和 CatBoost 在<strong class="lk jd"> fit() </strong>方法或<strong class="lk jd"> train() </strong>函数中有一个参数叫做<strong class="lk jd"><em class="me">early _ stopping _ rounds</em></strong>。您可以指定一个小于<strong class="lk jd"> <em class="me"> n_estimators </em> </strong>中的值的整数值来激活提前停止。<strong class="lk jd"><em class="me"/></strong>AdaBoost 和 Gradient Boosting 中没有这样的内置参数。要激活 AdaBoost 和梯度增强的提前停止，您可以监控学习过程，找到模型开始过度拟合的点(如果有)，并在<strong class="lk jd"> <em class="me"> n_estimators </em> </strong>参数中设置该值。然后，需要用新的<strong class="lk jd"> <em class="me"> n_estimators </em> </strong>值重新执行算法。</p><h2 id="d14f" class="ni mg it bd mh nt nu dn ml nv nw dp mp lr nx ny mr lv nz oa mt lz ob oc mv iz bi translated">想法三:寻找是否有机会提前停止</h2><p id="fbb8" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">不是所有的增压模型都有机会提前停止。通过查看学习曲线，您可以很容易地找到这一点。具有以下学习曲线的模型即使在 5000 次助推轮后也没有提前停止的机会！</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi od"><img src="../Images/16a1069c3be02faf2a1a9939167e44d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mWGYeplSgw5bnltRL7H3yg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">没有机会提前停止(图片由作者提供)</p></figure><h2 id="cb74" class="ni mg it bd mh nt nu dn ml nv nw dp mp lr nx ny mr lv nz oa mt lz ob oc mv iz bi translated">想法 4:选择正确的 API</h2><p id="90c1" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">对于 XGBoost 和 LightGBM 这样的 boosting 算法，有两个 API:Scikit-learn API 和 Learning API。Scikit-learn API 使用通用的<strong class="lk jd"> fit() </strong>方法进行训练，而 Learning API 使用自己的<strong class="lk jd"> train() </strong>函数进行训练。举个例子，</p><pre class="ks kt ku kv gt nd ne nf ng aw nh bi"><span id="45fa" class="ni mg it ne b gy nj nk l nl nm">import lightgbm</span><span id="e2f7" class="ni mg it ne b gy oe nk l nl nm">lgbm = lightgbm.LGBMRegressor()</span><span id="b4bb" class="ni mg it ne b gy oe nk l nl nm">#Training: Scikit-learn API<br/>lgbm.fit(X_train, y_train)</span><span id="9a68" class="ni mg it ne b gy oe nk l nl nm">#Training: Learning API<br/>lightgbm.train(<strong class="ne jd">params=...,<br/>               train_set=...,<br/>               valid_sets=...</strong>)</span></pre><p id="40bc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最简单和推荐的是 Scikit-learn API。在这些 API 和算法中，同一个参数可能有不同的名称。</p><h2 id="a431" class="ni mg it bd mh nt nu dn ml nv nw dp mp lr nx ny mr lv nz oa mt lz ob oc mv iz bi translated">想法 5:随机化你的数据</h2><p id="7409" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">由于训练/测试数据分割过程的随机性，输出可能会有很大变化。我建议您在拆分之前对数据进行洗牌，并为训练集使用尽可能多的数据。我还建议您在不同的<strong class="lk jd"> random_state </strong>值下多次运行该算法，并查看输出。</p></div><div class="ab cl of og hx oh" role="separator"><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok ol"/><span class="oi bw bk oj ok"/></div><div class="im in io ip iq"><p id="1eea" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">今天的帖子到此结束。我总是尽我最大的努力来写和组织我的内容，以一种你可以从中获得最大收益的方式。应该有很多方法来进一步改善我的内容，你可能有很好的想法。所以，</p><p id="8af3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你有任何反馈，请告诉我。</p><p id="414b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">与此同时，你可以通过下面的链接注册成为会员，以获得我写的每一个故事的全部信息，我会收到你的一部分会员费。</p><div class="om on gp gr oo op"><a href="https://rukshanpramoditha.medium.com/membership" rel="noopener follow" target="_blank"><div class="oq ab fo"><div class="or ab os cl cj ot"><h2 class="bd jd gy z fp ou fr fs ov fu fw jc bi translated">通过我的推荐链接加入 Medium</h2><div class="ow l"><h3 class="bd b gy z fp ou fr fs ov fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="ox l"><p class="bd b dl z fp ou fr fs ov fu fw dk translated">rukshanpramoditha.medium.com</p></div></div><div class="oy l"><div class="oz l pa pb pc oy pd lb op"/></div></div></a></div><p id="16f2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">还有，再也不要错过我的一个故事。订阅获取这样的故事:</p><p id="fb4e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://rukshanpramoditha.medium.com/subscribe" rel="noopener">https://rukshanpramoditha.medium.com/subscribe</a></p><p id="9b73" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">非常感谢你一直以来的支持！下一个故事再见。祝大家学习愉快！</p><p id="4026" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">特别感谢 Pixabay 上的<strong class="lk jd"> Christian Riedl </strong>，<strong class="lk jd"> </strong>为我提供了这篇文章的封面图片。</p><p id="c277" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="pe pf ep" href="https://medium.com/u/f90a3bb1d400?source=post_page-----9c1b12cc6729--------------------------------" rel="noopener" target="_blank">鲁克山·普拉莫迪塔</a><br/><strong class="lk jd">2021–11–08</strong></p></div></div>    
</body>
</html>