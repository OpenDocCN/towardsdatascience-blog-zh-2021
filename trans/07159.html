<html>
<head>
<title>Modeling Product Search Relevance in e-Commerce: Home Depot Case Study</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">电子商务中产品搜索相关性建模:家得宝案例研究</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/modeling-product-search-relevance-in-e-commerce-home-depot-case-study-8ccb56fbc5ab?source=collection_archive---------4-----------------------#2021-06-29">https://towardsdatascience.com/modeling-product-search-relevance-in-e-commerce-home-depot-case-study-8ccb56fbc5ab?source=collection_archive---------4-----------------------#2021-06-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="daa2" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="8b12" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">利用机器学习预测homedepot.com搜索结果的相关性</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/db5aa50b399a15c2db0c88dd8b47d4b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n8uaFZduAPr3Yq3whEmKQw.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">照片由来自<a class="ae le" href="https://www.pexels.com/photo/crop-buyer-with-smartphone-and-coffee-to-go-6331238/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> Pexels </a>的<a class="ae le" href="https://www.pexels.com/@anete-lusina?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> Anete Lusina </a>拍摄</p></figure><h1 id="c102" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">目录</h1><ol class=""><li id="e0ac" class="lx ly iq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated"><strong class="lz ja">简介<br/> </strong>摘要<br/>问题陈述<br/>商业问题的ML公式化<br/>数据概述<br/>ML问题的目标和指标<br/>现实世界的约束</li><li id="8515" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><strong class="lz ja">文献综述</strong></li><li id="d21b" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><strong class="lz ja">探索性数据分析:第1部分<br/></strong>·相关性得分<br/>·Product _ uid<br/>·属性<br/>·描述<br/>·合并产品文本数据<br/>·填充空值</li><li id="8f26" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><strong class="lz ja">数据清理<br/>T26】基本预处理<br/>标准化单元<br/>纠正搜索中的错别字<br/>进一步预处理</strong></li><li id="59ac" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><strong class="lz ja">探索性数据分析:第二部分<br/> </strong>【基本文本统计】<br/>【单词和双词分布】</li><li id="59e7" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><strong class="lz ja">特征工程<br/></strong>·集合论特征<br/>·基于VSM的特征<br/>·概率特征<br/>·使用Word2Vec的查询扩展</li><li id="cd7b" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><strong class="lz ja">特性集的性能图</strong></li><li id="c2cc" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><strong class="lz ja">最终建模和评估<br/> </strong>【堆叠回归器】<br/>基础模型<br/>元模型<br/>ka ggle上的性能(前10%)</li><li id="3ff1" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><strong class="lz ja">扩展成现实世界的搜索引擎</strong></li><li id="ebd1" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><strong class="lz ja">全管道</strong></li><li id="e760" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><strong class="lz ja">最终测试和结果<br/> </strong>测试<br/>运行时间<br/>网络应用程序演示</li><li id="3a92" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><strong class="lz ja">结论和未来工作</strong></li><li id="aa40" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><strong class="lz ja">致谢</strong></li><li id="3d6f" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><strong class="lz ja">参考文献</strong></li></ol><h1 id="a6ed" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">介绍</h1><h2 id="abb6" class="mu lg iq bd lh mv mw dn ll mx my dp lp me mz na lr mg nb nc lt mi nd ne lv iw bi translated">摘要</h2><p id="21ea" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated">一切都在向数字化转变，购物体验也是如此。电子商务在过去几年中发展迅速，因此，在线产品搜索已成为为客户提供满意购物体验的最重要因素之一。在这篇博客中，我提出了一种预测给定搜索查询所需产品的可靠方法，使用了涉及机器学习、自然语言处理和信息检索的技术。</p><h2 id="5d9e" class="mu lg iq bd lh mv mw dn ll mx my dp lp me mz na lr mg nb nc lt mi nd ne lv iw bi translated">问题陈述</h2><p id="83b6" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated">这项任务很容易理解。对于客户输入的任何搜索查询，我需要找到最相关的产品，并按照相关性的顺序显示给用户。从商业的角度来看，有几点需要考虑。首先，需要对产品进行排序，因此，即使在最相关的产品中，我们也需要能够分辨出哪一个更相关。第二，有一个时间限制，即结果需要在几秒钟内显示。</p><h2 id="4617" class="mu lg iq bd lh mv mw dn ll mx my dp lp me mz na lr mg nb nc lt mi nd ne lv iw bi translated">商业问题的机器学习公式</h2><p id="c0ec" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated">该任务可以表述如下:给定一个搜索和一个产品，找出它们之间的相关性分数，即该产品与手边的搜索查询有多相关。假设我的机器已经学会了如何预测(搜索-查询，产品)对的相关性分数。现在，对于用户输入的任何搜索，我可以计算该搜索与数据库中所有产品匹配的相关性分数，并向客户显示(比方说)前10个结果。</p><p id="19ad" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">因此，如果我有很多带标签的数据，即很多(搜索-查询，产品)对及其相关性分数，那么我可以将此作为监督ML问题。这正是我在这个案例研究中所做的。我使用的数据是家得宝为Kaggle比赛提供的<a class="ae le" href="https://www.kaggle.com/c/home-depot-product-search-relevance" rel="noopener ugc nofollow" target="_blank">家得宝产品搜索相关性</a></p><p id="907f" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">现在，在现实世界的电子商务搜索引擎中，计算给定搜索的每个产品的相关性分数是不可能的，因为，在任何典型的电子商务网站中，产品的数量都非常大，因此计算成本很高，非常耗时。</p><p id="4538" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">✦因此，首先，我们使用一个允许快速查询评估的更简单的检索模型来检索一些候选产品。在第二阶段，使用更精确但计算量大的机器学习模型对这些产品进行重新排序。</p><p id="0062" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">为了解释这一点，假设我们有一组100，000个产品。而搜索查询是“太阳能灯”。第一个更简单的检索模型将检索几个候选产品。这个模型可以简单到像搜索单词和产品文本之间的AND操作符。所以在这里，一个由AND operator管理的模型将得到所有带有“solar”和“lamp”字样的产品。假设它检索了大约500种产品。现在，在这500种产品的基础上，我们可以运行复杂的机器学习算法，并计算每个(搜索，产品)对的相关性分数，其中我们的搜索词“太阳能灯”保持不变，而产品有所不同。现在，我们根据它们的相关性分数对它们重新排序，并向用户显示排名靠前的产品。这叫做<a class="ae le" href="https://en.wikipedia.org/wiki/Learning_to_rank" rel="noopener ugc nofollow" target="_blank">学习排名</a> (LTOR)。在这篇博客中，重点将放在系统的学习排名部分。最后，我还解释了我是如何扩展它，使之成为一个成熟的搜索引擎的。</p><h2 id="36f7" class="mu lg iq bd lh mv mw dn ll mx my dp lp me mz na lr mg nb nc lt mi nd ne lv iw bi translated">数据概述</h2><p id="67a6" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated">数据是四个CSV文件的形式；train.csv，test.csv，attributes.csv，product_description.csv，你可以从<a class="ae le" href="https://www.kaggle.com/c/home-depot-product-search-relevance/data" rel="noopener ugc nofollow" target="_blank">这里</a>下载。</p><p id="66e3" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated"><strong class="lz ja">训练数据:</strong>我们在train.csv文件中总共有74067行用于训练。一部分数据如下所示，每行对应一对</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nx"><img src="../Images/56ad84be19ddb4a1c139e5bbbccd9a05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kZYYt-v5i02iw1qHCfRbwg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">train.csv文件</p></figure><p id="7e27" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">(search_term，product_title)以及说明该产品与搜索相关程度的相关性分数。相关性分数是一个从1到3的实数(1表示不相关，3表示完全匹配)。对于每个产品，我们通常有多个行，即多个搜索查询及其相关性得分。product_uid是每个产品的唯一标识符。</p><p id="d9f6" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">product_description.csv包含每个产品的文本描述。attributes.csv文件包含一些关于产品子集的附加信息。</p><p id="a6fb" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">搜索查询是字符串形式的，产品由文本数据表示，这些文本数据以标题、描述和一些属性的形式提供给我们。在现实世界中，我们还可能有产品的其他特征，如图像、评级等。但是这里我们只有文本可以使用。</p><p id="1501" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">数据的相关性分数是手动标记的。每一对(搜索、产品)由至少三名人类评价者进行评估，然后将最终的相关性分数作为平均值。</p><p id="53ef" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated"><strong class="lz ja">测试数据:</strong>由于这是Kaggle竞赛的一部分，我们还得到一个test.csv文件，其中包含大约166，000行，我们需要预测这些行的相关性分数。训练集中大约一半的产品出现在测试集中。这两组的文氏图如下所示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/be6b0a3679612369f3084c3cac9d001b.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*Z6YThksszjA_iPw5XNmW3Q.png"/></div></figure><h2 id="0afc" class="mu lg iq bd lh mv mw dn ll mx my dp lp me mz na lr mg nb nc lt mi nd ne lv iw bi translated">ML问题的目标和度量</h2><p id="2961" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated"><strong class="lz ja">目标</strong>:对于每一个给定的(搜索-查询，产品)，预测相关性得分。这将被视为一个回归问题，因为我们可以更容易地对产品进行排序。</p><p id="3a5f" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated"><strong class="lz ja">使用的度量:<br/> </strong>比赛中给出的这个度量是<strong class="lz ja">均方根误差(RMSE) </strong>。这是回归问题中最广泛使用的度量标准，在大多数情况下效果很好。它不同于MAE(平均绝对误差),因为它更多地惩罚大误差。RMSE的一个问题是它的价值很难解释；它的范围可以从零到无穷大。因此，一般来说，基线得分是使用一个简单的回归模型建立的，然后我们可以使用它作为其他模型的参考。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/71cccb7ee0fc112da8930576ff6ef078.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*SOhwB6kDlgIng4FkoYDKmA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">来源:<a class="ae le" href="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/05/rmse.png" rel="noopener ugc nofollow" target="_blank">https://S3-AP-south-1 . Amazon AWS . com/av-blog-media/WP-content/uploads/2018/05/RMSE . png</a></p></figure><h2 id="be21" class="mu lg iq bd lh mv mw dn ll mx my dp lp me mz na lr mg nb nc lt mi nd ne lv iw bi translated">现实世界的约束</h2><ul class=""><li id="f274" class="lx ly iq lz b ma mb mc md me mf mg mh mi mj mk oa mm mn mo bi translated">延迟限制:产品需要在几秒钟内展示。</li><li id="ee58" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk oa mm mn mo bi translated">产品排名是必要的。</li><li id="f00e" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk oa mm mn mo bi translated">可解释性是部分重要的:我们想知道为什么我们的模型向客户建议了一个特定的产品。</li></ul><p id="7399" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated"><strong class="lz ja"> <em class="ob">注:</em> </strong> <em class="ob">在博客中，我曾交替使用过‘搜索查询’，‘查询’，‘搜索词’和‘搜索’这几个词。</em></p><h1 id="ef47" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">文献评论</h1><p id="ffdf" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated">眼前的问题是搜索相关性，它大致属于信息检索领域。关于自由文本查询技术的大量研究已经发表。</p><p id="b6e4" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">传统的非机器学习模型集中于探索各种建模语言的方法。像<a class="ae le" href="https://www.researchgate.net/profile/Arash-Habibi-Lashkari/publication/232614940_A_Boolean_Model_in_Information_Retrieval_for_Search_Engines_PDF/links/5756c52808aef6cbe35f0f5a/A-Boolean-Model-in-Information-Retrieval-for-Search-Engines-PDF.pdf" rel="noopener ugc nofollow" target="_blank">布尔模型</a>和<a class="ae le" href="https://www.sciencedirect.com/science/article/abs/pii/S0165011409001080?via%3Dihub" rel="noopener ugc nofollow" target="_blank">模糊检索</a>这样的集合论模型是基于表示查询并记录为单词集，然后应用一些基本的集合运算来获得相关性。像<a class="ae le" href="https://en.wikipedia.org/wiki/Vector_space_model" rel="noopener ugc nofollow" target="_blank"> VSM </a>这样的代数模型将文档和查询表示为向量(例如<a class="ae le" href="https://en.wikipedia.org/wiki/Bag-of-words_model" rel="noopener ugc nofollow" target="_blank">单词袋</a>)，然后将它们之间的相似度计算为标量。更受欢迎的模型如<a class="ae le" href="https://www.researchgate.net/profile/Hugo-Zaragoza/publication/220613776_The_Probabilistic_Relevance_Framework_BM25_and_Beyond/links/53d9ce800cf2a19eee8807e8/The-Probabilistic-Relevance-Framework-BM25-and-Beyond.pdf" rel="noopener ugc nofollow" target="_blank"> Okapi BM25 </a>、<a class="ae le" href="http://ciir.cs.umass.edu/pubfiles/ir-407.pdf" rel="noopener ugc nofollow" target="_blank"> Indri </a>和<a class="ae le" href="https://en.wikipedia.org/wiki/Language_model" rel="noopener ugc nofollow" target="_blank">语言模型</a>都是基于<a class="ae le" href="https://en.wikipedia.org/wiki/Probabilistic_relevance_model" rel="noopener ugc nofollow" target="_blank">概率论</a>。尽管所有这些模型都来自不同的底层理论，但它们中的大多数都使用了来自自然语言处理领域的非常相似的概念，如单词袋、余弦相似度等。</p><p id="4305" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">最近的方法包括<a class="ae le" href="https://en.wikipedia.org/wiki/Learning_to_rank" rel="noopener ugc nofollow" target="_blank">学习排序</a>，其中通常查询-文档对由其特征向量表示，然后在其上训练监督或半监督机器学习模型以获得排序。这里，通常直接使用传统的检索模型作为特征。<a class="ae le" href="https://opensourceconnections.com/blog/2016/03/29/semantic-search-with-latent-semantic-analysis/" rel="noopener ugc nofollow" target="_blank"> LSA </a>(或LSI)将术语-文档矩阵映射到一个“概念”空间，从而得到一个密集且更好的文本表示。<a class="ae le" href="https://en.wikipedia.org/wiki/Word2vec" rel="noopener ugc nofollow" target="_blank"> Word2Vec </a>算法使用神经网络从大型文本语料库中学习单词关联，并将单词映射到高维向量，这些向量可以用作特征。<a class="ae le" href="https://www.researchgate.net/profile/Saar-Kuzi/publication/310823543_Query_Expansion_Using_Word_Embeddings/links/5a94aacf0f7e9ba42970d144/Query-Expansion-Using-Word-Embeddings.pdf" rel="noopener ugc nofollow" target="_blank">查询扩展</a>是用语料库中存在的相似单词扩展查询的另一个有效想法。为了测量相似性，通常使用来自Word2Vec模型的单词向量。</p><h1 id="0563" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">探索性数据分析:第1部分</h1><p id="3d86" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated">在这一节中，我将对手头的数据进行分析。我已经将train.csv文件保存为train_df变量中的数据帧。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oc od l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oe"><img src="../Images/33493d95c63b77fbe0fcbde8dd1857cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t06WUkj_gdz36YIyZcreBw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">火车_df</p></figure><p id="8cd9" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">id列没有用。product_uid是每个产品的唯一id，在合并来自其他文本文件的数据时非常重要。训练数据中没有空值。</p><h2 id="79e7" class="mu lg iq bd lh mv mw dn ll mx my dp lp me mz na lr mg nb nc lt mi nd ne lv iw bi translated">相关性分数</h2><p id="e11e" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated">发现相关性分数本质上是分类的。这是因为大部分的分数是3个评分者的平均分数，每个评分者只能给出1或2或3的整数分。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oc od l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi of"><img src="../Images/b76bb41ee7219d06a5dc3dc9f6c28e94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2I2FFS4cY7obaXWc8thnKw.png"/></div></div></figure><p id="ac25" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">大多数产品具有高相关性分数，即大多数产品与查询相关。由于相关性分数的分类性质，如果分类模型有助于最终排名，也可以在此基础上建立分类模型。得分{1.25，1.5，1.75，2.25，2.5，2.75}出现的频率非常低。</p><h2 id="d52e" class="mu lg iq bd lh mv mw dn ll mx my dp lp me mz na lr mg nb nc lt mi nd ne lv iw bi translated">产品_uid</h2><p id="d59b" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated">许多产品在训练数据中出现多次，也就是说，我们有多个搜索术语和相同产品的相应相关性。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi og"><img src="../Images/b3e4411c59d6724dfc4ddf4e65e64424.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l-EXNARi-DjqU1UJOZWUfQ.png"/></div></div></figure><ul class=""><li id="0f7a" class="lx ly iq lz b ma ns mc nt me oh mg oi mi oj mk oa mm mn mo bi translated">超过10，000种产品多次出现</li><li id="09dd" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk oa mm mn mo bi translated">74067种独特产品的总数是54667种</li><li id="c139" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk oa mm mn mo bi translated">产品出现的最大次数是21次</li></ul><h2 id="e6ed" class="mu lg iq bd lh mv mw dn ll mx my dp lp me mz na lr mg nb nc lt mi nd ne lv iw bi translated">属性</h2><p id="667b" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated">对于产品，我们有产品标题、产品描述和属性形式的文本信息。产品标题在train.csv文件中。描述和属性分别从product_description.csv和attributes.csv文件中提取。</p><p id="628c" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">attributes.csv文件的一个实例如下所示。字段“name”表示属性的名称，“value”表示属性文本。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oc od l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/515d724db40b010adeee411265771acf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*b51BpY1R400UeSmLd5Mq2w.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">属性. csv</p></figure><p id="43dc" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">attributes.csv文件中的每个产品都有多个属性。产品的最小属性数为5，最大属性数为88。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ol"><img src="../Images/a93e3d9b790d3b9ae893ed52f488bccc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MTwaidBw39Y2OjjCeFQhrQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">属性数量与产品指数</p></figure><p id="cd5e" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">但是在attributes.csv文件中并没有找到train.csv文件中的所有产品。总共54667个产品中有16263个没有属性。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi om"><img src="../Images/0af71ade46a8ab1623c15ce2a02cea2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*nDAvx7VGqdBKwY8s7vTGNw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">train.csv和attributes.csv文件中的产品集</p></figure><p id="e6f0" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">然后，我研究了最常出现的属性，在attributes.csv文件的86，250个实例中，Brand是最常见的属性。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oc od l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi on"><img src="../Images/14b2469d0293fe748891f01dd479a869.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*js33cQvQKBnNtZIkov8MyQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">最常见的属性</p></figure><p id="3791" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">不仅如此，我们还观察到，大多数情况下，product_title包含品牌信息。因此，我在我的train_df中添加了brand作为一个单独的特性。</p><h2 id="11b1" class="mu lg iq bd lh mv mw dn ll mx my dp lp me mz na lr mg nb nc lt mi nd ne lv iw bi translated">描述</h2><p id="d91b" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated">product_description.csv包含产品的描述。与属性不同，我们在train.csv文件中有对所有产品的描述。</p><h2 id="fe74" class="mu lg iq bd lh mv mw dn ll mx my dp lp me mz na lr mg nb nc lt mi nd ne lv iw bi translated">合并属性、描述和品牌特征</h2><p id="3d5a" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated">最后，我用train_df合并了所有属性、描述和品牌。<em class="ob">产品的所有属性首先被连接起来，然后存储为“组合属性”特征</em>。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oc od l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oo"><img src="../Images/51887a6dbfa018a2d3417317810fd77f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SeOJRTLOTXXKAJDO8yEMqA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">合并后的train_df</p></figure><p id="8912" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated"><em class="ob">注意:从现在开始，代表产品的文本字段，即产品标题、品牌、组合属性和产品描述，统称为“产品-文本”或“文档-字段”或“文档”。</em></p><h2 id="5be8" class="mu lg iq bd lh mv mw dn ll mx my dp lp me mz na lr mg nb nc lt mi nd ne lv iw bi translated">填充空值</h2><p id="5598" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated">在开始清理文本之前，我发现我们的train_df有很多空值。这些是在我合并属性时发生的，因为训练数据中的许多产品没有属性。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oc od l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi op"><img src="../Images/8030a74167cea0aca244fb5b6b5fa6e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*nix1U00-7YwPI78KpCZipg.png"/></div></figure><p id="5214" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">我们可以看到，空值只出现在brand和combined_attr特性中。有大量的空值，因此正确填充它们是清理过程中的重要一步。</p><p id="ee89" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated"><strong class="lz ja">品牌:</strong>据观察，产品的品牌一般出现在标题的前几个字里。例如，产品名称为“Simpson Strong-Tie 12-Gauge Angle”的产品的品牌是“Simpson Strong-Tie”。“Delta Vero 1手柄淋浴专用水龙头”是“Delta”。此外，99%的品牌长度不超过四个单词。所以大多数情况下，品牌应该出现在标题的前四个字里。</p><p id="a484" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">所以，我首先将所有唯一的品牌存储在一个列表“unique_brands”中。现在，对于任何品牌为空的行，我检查该产品标题的前四个单词是否出现在unique_brands列表中。如果没有，那么我检查前三个单词，以此类推，直到第一个单词。这种方法非常有效，我能够填充15003个无效品牌。对于剩下的2625个值，我用标题的第一个字填充。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="e4d5" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated"><strong class="lz ja">描述:</strong>为描述填充空值并不困难，因为我们发现描述文本与组合属性文本非常相似。因此，combined_attr字段的所有空值都用相应的product_description值填充。</p><h1 id="4704" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">数据清理</h1><p id="49bc" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated">这是整个案例研究中最重要的部分之一。大量的时间花在了观察文本和进行必要的预处理步骤以使其适合建模上。使用了许多自然语言处理技术。</p><p id="8ae2" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated"><em class="ob">为什么文本预处理在这个项目中如此重要，一个非常简单的动机是因为我们在某种程度上试图找到搜索查询和产品文本之间的相似性。考虑搜索为“5加仑桶”，产品名称为“Leaklite 5加仑黑桶”。如果我们使用一个像普通单词数这样的衡量标准，那么在这种情况下，相似度将为零，因为没有共同的单词。现在说一点预处理，我们的搜索和标题分别成为“5加仑桶”和“leaklite 5加仑黑桶”。现在我们的相似性得分为3，因为现在我们有3个共同的单词。因此，很少的预处理就能产生巨大的差异。</em></p><h2 id="85ff" class="mu lg iq bd lh mv mw dn ll mx my dp lp me mz na lr mg nb nc lt mi nd ne lv iw bi translated">基本预处理</h2><p id="c0d7" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated">基本的预处理步骤包括小写、删除特殊字符、删除停用词和词干。</p><h2 id="182a" class="mu lg iq bd lh mv mw dn ll mx my dp lp me mz na lr mg nb nc lt mi nd ne lv iw bi translated">标准化单位</h2><ol class=""><li id="6e2d" class="lx ly iq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">许多产品名称包含产品的尺寸。一种非常常见的表示方式是“8英尺”。x 5英尺。x 8英尺。”。在产品描述中也可以观察到同样的情况。</li><li id="82da" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">在product_title中，单位通常表示为{in。磅。sq。磅。奥兹。加仑mph}，但是在product_description和search_term中，单位以多种方式表示。例如，英寸等于英寸。英寸}和加仑{gal。加仑}。因此，这些单元需要标准化。</li><li id="e77e" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">在搜索词中，指定测量值时有一些常见错误。例如“4个架子”、“9x12”、“g135”、“5加仑”、“1/2英寸乘12英寸”。因此，需要把数字和单词分开。</li></ol><p id="6560" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated"><strong class="lz ja">预处理步骤:</strong></p><ol class=""><li id="2c20" class="lx ly iq lz b ma ns mc nt me oh mg oi mi oj mk ml mm mn mo bi translated">将数字和单词分开:“9x 12”→“9x 12”，“5加仑”→“5加仑”…</li><li id="9147" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">将所有测量单位统一表示。对女孩来说。gals gallon}全部转换为“加仑”。类似地，{ ft fts feets foot也转换为“英尺”。</li></ol><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oc od l"/></div></figure><h2 id="069c" class="mu lg iq bd lh mv mw dn ll mx my dp lp me mz na lr mg nb nc lt mi nd ne lv iw bi translated">纠正搜索词中的拼写错误</h2><p id="736b" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated">在浏览搜索词时，发现它包含许多拼写错误，如“割草机”被拼写为“mowe”，“厕所”被拼写为“toiled”等。这是可以理解的，因为这些搜索是由用户输入的，在在线搜索中，这样的拼写错误是常见的。但这是一个大问题，因为拼写错误的单词可能会使常用单词计数等功能变得非常无效。因此，为了纠正这一点，我使用了一个自定义的拼写纠正。结果非常积极。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oc od l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oq"><img src="../Images/55a198f15b61d816fd2f1a5e87dcb5fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*on-Z2OFzxGlNO_pLuai7wg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">不正确的搜索→正确的搜索</p></figure><p id="baca" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">我使用的拼写校正器是由彼得·诺威格建造的。它基于概率理论，但是非常容易理解并且非常有效。最棒的是，你可以根据自己的数据进行调整。这非常重要，因为通用拼写校正器可能无法识别仅限于我们数据的品牌名称。我鼓励你参考这个链接并阅读更多相关内容。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="7b20" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">我更正了所有的搜索词，出于实验目的，我还保留了原始/未更正的搜索。</p><h2 id="bdc8" class="mu lg iq bd lh mv mw dn ll mx my dp lp me mz na lr mg nb nc lt mi nd ne lv iw bi translated">进一步预处理</h2><p id="0ce4" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated">在基本的预处理中，我留下了停用词移除和词干部分，因为我想先修复不正确的搜索词。在这里，这两个步骤都被执行，清理后的文本被存储在cleaned_df中。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="989d" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">我们最终清理后的数据帧如下所示</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi or"><img src="../Images/8e673c9fdd6c72f01ce05598633fab5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cXpNHI0ywS7o0qgkIB8kwA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">已清理_df</p></figure><p id="fddb" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated"><em class="ob">我还创建了一个dataframe cleaned_df2，在那里存储没有词干的文本数据。这样做是为了以后能够使用像Word2Vec这样预先训练好的编码。</em></p><h1 id="9ea4" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">探索性数据分析:第2部分</h1><p id="7b60" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated">清理之后，我能够从文本数据分析中做出更准确的发现。已经做了很多分析，我将在这一部分回顾最重要的发现/结论。</p><p id="040b" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">为了便于分析，我将相关性分数分为两类:分数&gt; 2的“高”和分数≤ 2的“低”。它们也分别被称为正类和负类。虽然，这是一个回归问题，但是把分数分成两类在分析中帮助很大；一个有助于区分高等级和低等级的特征在最终的回归问题中也会很有帮助。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="4437" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated"><em class="ob">我们总共有6个文本字段，对每个字段分别进行了分析。</em></p><p id="f331" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated"><strong class="lz ja"> <em class="ob">注意:<br/> </em> </strong> <em class="ob">经过仔细检查，我发现描述和属性文本非常相似，因此，我没有在任何进一步的分析中考虑属性文本。</em></p><h2 id="a1e2" class="mu lg iq bd lh mv mw dn ll mx my dp lp me mz na lr mg nb nc lt mi nd ne lv iw bi translated">基本文本统计</h2><p id="05b7" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated">对文本的一些基本数据进行了分析。即文本串的长度、字数和平均单词长度。对于corrected_search，这些统计信息的分布如下所示。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oc od l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi os"><img src="../Images/5afaa228e61b46d7b1963ed97193a7a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dsOD-blkH8kf_KQdBD0CWA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">基本统计分布</p></figure><p id="c50b" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated"><strong class="lz ja">观察:</strong></p><ol class=""><li id="c192" class="lx ly iq lz b ma ns mc nt me oh mg oi mi oj mk ml mm mn mo bi translated">大多数搜索查询的字数在2到5个词之间。</li><li id="be0e" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">这些统计数据的分布对于正类和负类都非常相似。</li><li id="c0e4" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">上面提到的最重要的统计数据是字数。</li></ol><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/abde925c7e9fa1848f4bffc951d3e1a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*G68UpeQCU5IUwYcmEAdTBA.png"/></div></figure><p id="52b4" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">请注意，当我们向右移动时，正类与负类的高度/密度比率开始下降，或者我们可以说负类开始变得更加突出。这意味着随着字数的增加，相关性高的概率降低。因此，字数给出了关于相关性分数的一些信息。</p><p id="f34a" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">4.通过观察字数与相关性的相关性，可以证明上述说法是正确的。有一点相关性，本质上是负面的。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oc od l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ou"><img src="../Images/bcb9412245c40f4c1e99fc053ac53f9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tSYOAtDyr0pLC3HfXGKfDg.png"/></div></div></figure><p id="2c30" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">5.所以字数统计可能不是一个坏的特性。</p><p id="0e10" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">6.字符数和平均单词长度与相关性的相关性<br/>分数非常低，分别为-0.092和0.084，因此可能没有帮助。</p><p id="e916" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated"><em class="ob">对所有这些统计数据进行了标题、描述和品牌文本分析。发现了同样的模式。唯一与相关性有显著相关性的统计数据是字数。</em></p><p id="f21d" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">除此之外，我还试验了另外两个统计数据:可读性指数和情感分数。两者的相关分数分别为-0.035和0.019，非常低。他们没有帮助区分这两个阶级。</p><h2 id="7a10" class="mu lg iq bd lh mv mw dn ll mx my dp lp me mz na lr mg nb nc lt mi nd ne lv iw bi translated">最常见的单字和双字</h2><p id="abc0" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated"><strong class="lz ja">搜索文本</strong>:corrected _ Search最常见的单字如下:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oc od l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ov"><img src="../Images/1e1d10f097c9288cdc697596ba2579d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jfGTn0DgolRNXICcnE6YFQ.png"/></div></div></figure><p id="739d" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">inch和x是最常用的术语。还有，很多数字是非常频繁出现的。这意味着大量的搜索词包含了所需产品的维度。这些数字和单位很重要，因为它们可以很好地表明用户想要什么样的产品。因此，我没有删除数据清理时的数字，并将保留它们作为最终的特征部分。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/81d4e1912c040a9f47d03161f9600ab6.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*C_SnRHoQoqrE2GA747zYpA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">每类最常见的单字</p></figure><p id="3454" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">然而，这两个类别中最常见的单词非常相似。在左侧，您可以看到每个类别中搜索最频繁的单词及其频率。</p><p id="f4bb" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">甚至两个阶层最常见的二元模型也非常相似。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ox"><img src="../Images/942829174a9692f3c466a6e01489cd2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uC59eiZ9w01MFYmuLiuOGw.png"/></div></div></figure><p id="8f99" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">因此，从上面的观察中，对于搜索文本，我们看到两个类中最常见的单词和双词非常相似。因此，像simple BOW或TF-IDF BOW这样的矢量化方法可能效果不太好。</p><p id="f5b1" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated"><strong class="lz ja">描述文字</strong>:即使在描述文字中，也发现最常见的词是inch，数字比较突出。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oy"><img src="../Images/700e0aad5026a6c9f987a5e919265f55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*stZTThIXmOZy7yghNgSSng.png"/></div></div></figure><p id="eb9e" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">两个类别中最常见的单字和双字被发现非常相似。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oz"><img src="../Images/24561b3afe5349742fd50373dc77dfbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aaSpH2IK0U1-lAeJngraxQ.png"/></div></div></figure><p id="a100" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">对品牌和标题文本进行了同样的分析，发现了类似的趋势。文本字段本身并没有给出很多关于相关性分数的信息。</p><h2 id="377d" class="mu lg iq bd lh mv mw dn ll mx my dp lp me mz na lr mg nb nc lt mi nd ne lv iw bi translated">来自EDA2的结论</h2><ol class=""><li id="43f5" class="lx ly iq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">在所有关于文本的统计数据中，字数统计是区分这两类数据最有效的方法。我们无法从其他统计数据中获得很多信息，如平均单词长度和字符串长度，这两个数据显示的相关值非常低。因此，在最后的特征化，我将只包括字数统计功能。</li><li id="4a7e" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">对于这两个类，所有文本字段(搜索、标题、描述、品牌)的unigrams和bigrams分布非常相似。因此，像BOW和TF-IDF BOW这样的简单矢量化方法可能无法单独使用。</li></ol><h1 id="1572" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">特征工程</h1><p id="b401" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated">这个问题的特征主要来源于<a class="ae le" href="https://en.wikipedia.org/wiki/Information_retrieval" rel="noopener ugc nofollow" target="_blank">信息检索</a>领域。注意，这里的特征需要是搜索和产品组合的任意函数，因为我们试图找到(搜索，产品)对的相关性。一些基本的检索模型被直接用作特征。我将这些特性分为四组:</p><ul class=""><li id="c7f6" class="lx ly iq lz b ma ns mc nt me oh mg oi mi oj mk oa mm mn mo bi translated">集合1:集合论特征</li><li id="cb2b" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk oa mm mn mo bi translated">集合2:基于向量空间模型(VSM)的特征</li><li id="25d5" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk oa mm mn mo bi translated">集合3:概率特征</li><li id="90b4" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk oa mm mn mo bi translated">集合4:使用Word2Vec进行查询扩展</li></ul><h2 id="ff46" class="mu lg iq bd lh mv mw dn ll mx my dp lp me mz na lr mg nb nc lt mi nd ne lv iw bi translated">特征集1:集合论特征</h2><p id="3f40" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated">这些特征来源于简单的集合论检索模型，如<a class="ae le" href="https://en.wikipedia.org/wiki/Boolean_model_of_information_retrieval" rel="noopener ugc nofollow" target="_blank">布尔模型</a>、<a class="ae le" href="https://en.wikipedia.org/wiki/Fuzzy_retrieval" rel="noopener ugc nofollow" target="_blank">模糊检索</a>等。其基于将查询和文档表示为单词集，然后对它们应用某种操作来获得相关性分数。这些功能包括:</p><p id="b529" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">➤搜索和文档字段(标题、品牌、描述)之间的常用词计数</p><p id="fa5a" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">搜索和文档字段之间的➤余弦系数和雅克卡系数</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pa"><img src="../Images/c82af09aba66c80951fb49c5a2d6e8c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zj5JNCiF3H8r-FiY-L8dwA.png"/></div></div></figure><p id="6db2" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">➤搜索长度和文档字段</p><p id="03dc" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">➤搜索中的最后一个单词是否在文档字段中</p><p id="821e" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated"><em class="ob">最后一个特征很直观，因为在多词搜索查询中，最初的词通常描述了你需要的产品的属性，但最后一个词代表了产品的名词。就像在“黑色夹克”中，第一个词描述了夹克的颜色，但最后一个词是你需要的产品的名词。</em></p><p id="1a81" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">以上特征是为corrected_search和raw_search创建的。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oc od l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pb"><img src="../Images/d30170843e35d99ed59e6b89a0e459c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BRir7srES7s07-R3uFcxUA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">corrected_search和raw_search的比较</p></figure><ul class=""><li id="657b" class="lx ly iq lz b ma ns mc nt me oh mg oi mi oj mk oa mm mn mo bi translated">可以观察到，在相当多的行中，num-common-words特征更多地用于corrected_search而不是raw_search。因此，更正搜索词确实有所帮助。</li></ul><h2 id="9f0f" class="mu lg iq bd lh mv mw dn ll mx my dp lp me mz na lr mg nb nc lt mi nd ne lv iw bi translated">功能集1上的EDA</h2><p id="ab33" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated">这里总共创建了29个特征。下面给出了一些的PDF(概率密度函数)图。</p><p id="2ee3" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated"><em class="ob">注:以“ST”为后缀的特征表示校正搜索与标题之间的特征，“SB”表示品牌，“SD”表示描述。对于raw_search，我使用了“r_ST”形式的后缀。例如，“余弦_ST”表示校正搜索和标题之间的余弦系数，而“余弦_r_ST”表示原始搜索和标题之间的余弦系数。</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pc"><img src="../Images/95715306eebda3ea130716f8764cb91f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l_w25kUj6CnupFxmUrzL0Q.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">集合1特征的PDF图</p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/f74d7be63f2a11f0cd9e50ec3ac41862.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*RFgQtHd_5TR0E9_LmAhGQg.png"/></div></figure><p id="5594" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">除非你仔细看，否则很难理解它们。以余弦_SD为例。一般来说，在所有的pdf中，我们看到正或“高”类的曲线高于负或“低”类。这显然是由于数据不平衡，因为属于正类的点比负类的多。你可以看到在0标记处，正类的曲线略高于负类。当您向右移动时，正曲线仍保持在上方，但正曲线与负曲线的高度比会增加。也就是说，随着cosine_SD增加，正类中的点与负类中的点的比率增加。因此，该点属于正类的概率增加。现在，如果你给我两个点，那么我可以说，具有较大余弦_SD值的点应该具有较高的相关性分数。</p><p id="b78a" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">因此，从上面的观察，我们可以得出结论，余弦_SD确实提供了一些关于(搜索，产品-文本)对的相关性分数的信息。</p><p id="dcdd" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">对该集合中的所有特征进行了相同的分析，包括raw_search和<strong class="lz ja">的特征。主要观察结果</strong>是:</p><ol class=""><li id="2d72" class="lx ly iq lz b ma ns mc nt me oh mg oi mi oj mk ml mm mn mo bi translated">特征num_common、余弦、Jaccard在正负类分布上表现出相当大的差异。其中，余弦_ST是最有效的。</li><li id="3c11" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">以ST为后缀的特征，即搜索和标题之间的特征，与SD和SB相比，在正类和负类的分布中显示出更多的区别。</li><li id="9c71" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">长度特征在分布上没有显示出很大的差异。</li><li id="8519" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">最有趣的特色之一是islast特色。观察到分布有相当大的差异。例如，在islast_ST中，负类的密度在0和1值处似乎是相似的，但是正类的密度从0到1显著增加。</li></ol><p id="5699" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">✦:根据观察，余弦和伊斯特特征是最重要的特征，其次是雅克卡。其中，ST特征应该显示最好的结果。</p><p id="1ca0" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">还构建了这些特征的相关矩阵:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pe"><img src="../Images/43d17789ad169bd8efa37317b58e6e4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DEph445178KiCNVaS73hsw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">集合1特征的相关矩阵</p></figure><p id="e262" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">并且找到它们与相关性的相关分数</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oc od l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pf"><img src="../Images/4d534f589046caa52e505bb268c156fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aXrAKcwTBFL70F-Ih0sPQQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">与相关性分数最相关的集合1特征</p></figure><p id="b4ee" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated"><strong class="lz ja"> <em class="ob">注:</em> </strong> <em class="ob">期望的特征集合将是这样的集合，其中所有特征具有高相关性分数和彼此低相关性分数。</em></p><p id="437d" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated"><strong class="lz ja">观察和结论:</strong></p><ol class=""><li id="ef67" class="lx ly iq lz b ma ns mc nt me oh mg oi mi oj mk ml mm mn mo bi translated">有几个特征彼此具有高度的相关性。这些相关特征可能是多余的，因此可以在建模时移除。</li><li id="1491" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">相关分数wrt。相关性不是很高，但是确实存在一些相关性。因此，这些功能可能是有用的。</li><li id="2212" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">我们可以看到，cosine_ST、jaccard_ST和islast_ST的相关性最高，与我们预测的完全一致。</li><li id="d187" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">这些值也是为raw_search计算的，一般来说，可以看出，修正的_search特征与相关性的相关性大于raw_search特征与相关性的相关性。</li><li id="a250" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">我还在这些特征上应用了T-SNE，使它们在二维空间中可视化。我将嵌入的特征投影到二维空间中，并用它们各自的类给它们着色。我在之前的两个班级和七个班级中尝试过。但是它根本没有给出任何见解。没有一个类显示任何聚类。</li></ol><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pg"><img src="../Images/7bca373b2002c4144f1c2033aeec4dd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vcNlZ8EnYAQSd0094wxkjQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">分别用七个类和两个类可视化TSNE要素。</p></figure><h2 id="ed5a" class="mu lg iq bd lh mv mw dn ll mx my dp lp me mz na lr mg nb nc lt mi nd ne lv iw bi translated">特征集2:基于VSM的特征</h2><p id="ae09" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated"><a class="ae le" href="https://en.wikipedia.org/wiki/Vector_space_model" rel="noopener ugc nofollow" target="_blank">基于VSM </a>的特征通常基于将文档和查询表示为向量，然后将它们之间的相似度计算为标量。</p><p id="5fed" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">这里，为了将文本转换成向量，我使用了<a class="ae le" href="https://en.wikipedia.org/wiki/Latent_semantic_analysis" rel="noopener ugc nofollow" target="_blank">潜在语义索引</a>，其中<a class="ae le" href="https://en.wikipedia.org/wiki/Singular_value_decomposition" rel="noopener ugc nofollow" target="_blank">截尾SVD </a>被应用于术语-文档矩阵，这样查询和文档被转换成“概念空间”中的向量。基本上，这里我首先使用TF-IDF BOW对文本进行矢量化，然后使用Truncated-SVD来降低向量的维数，并创建一个更加密集和有意义的表示。这些功能包括:</p><p id="7743" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">➤分别对搜索、标题和描述的术语文档矩阵应用LSI，并直接使用低秩近似作为特征。</p><p id="4699" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">➤在全文(标题和描述的组合)上应用LSI。然后，将搜索查询转换到“概念”空间，并计算它们之间的余弦相似度。</p><h2 id="e46b" class="mu lg iq bd lh mv mw dn ll mx my dp lp me mz na lr mg nb nc lt mi nd ne lv iw bi translated">功能集2上的EDA</h2><p id="fd17" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated">余弦相似性特征的pdf如下所示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ph"><img src="../Images/a71af162bd92327e07841bd923bbfc10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JEjj3HVXoVd3GEIarxNBsg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">集合2余弦相似特征的pdf</p></figure><p id="bebb" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">正如您所看到的，相似性特征在两个类之间提供了相当大的区别，非常有用。发现这三个中的每一个与相关性类的相关值是0.32，这是相当好的。</p><h2 id="e83f" class="mu lg iq bd lh mv mw dn ll mx my dp lp me mz na lr mg nb nc lt mi nd ne lv iw bi translated">特征集3:概率特征</h2><p id="3ffd" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated">这些特征基于<a class="ae le" href="https://en.wikipedia.org/wiki/Probabilistic_relevance_model" rel="noopener ugc nofollow" target="_blank">概率论</a>中的核心概念，如贝叶斯定理。使用的功能有:</p><p id="ebf5" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">➤ <a class="ae le" href="https://en.wikipedia.org/wiki/Language_model" rel="noopener ugc nofollow" target="_blank">语言模型</a>具有狄利克雷、绝对和杰利内克米勒平滑</p><p id="1c9e" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">➤ <a class="ae le" href="https://en.wikipedia.org/wiki/Okapi_BM25" rel="noopener ugc nofollow" target="_blank"> Okapi BM25 </a>排名功能</p><p id="eb4e" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">➤查询和表示为TF-IDF Word2Vec的字段</p><p id="7a29" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">每个文本字段中搜索查询的(tf，idf，tf-idf)的➤ (sum，min，max)</p><p id="e274" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated"><strong class="lz ja">BM25排名函数:<br/> </strong>除了这些特性，我想重点介绍BM25排名函数，因为它本身在传统搜索引擎中用于文本查询，并且在简单的自由文本查询中仍然非常流行。对于给定的(查询，文档)对，它计算相似性得分，如下所示:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pi"><img src="../Images/3cec21d82beb77fd8f18a953423bb607.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*h-AFYojKv8DmHQlL"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">来源:<a class="ae le" href="https://en.wikipedia.org/wiki/Okapi_BM25" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Okapi_BM25</a></p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pj"><img src="../Images/2acdbb40b5c27acf132b6db976d1d652.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W43hvVXlY0zVrpj2nY6UiQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图片由作者提供，灵感来自Victor Lavrenko 关于IR概率模型的<a class="ae le" href="https://www.youtube.com/playlist?list=PLBv09BD7ez_56YwzSyQOOsrAE4Js7XuAB" rel="noopener ugc nofollow" target="_blank">讲座</a></p></figure><p id="e842" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">以上是解释整个等式的一个尝试。它基本上是TF-IDF加权的修改形式，其中TF值已经被修改以结合文档的变化长度，并在评分wrt项-频率的增加中带来非线性。为了更好地理解它，我鼓励你们去听一听维克多·拉夫连科的讲座。</p><h2 id="4201" class="mu lg iq bd lh mv mw dn ll mx my dp lp me mz na lr mg nb nc lt mi nd ne lv iw bi translated">功能集3上的EDA</h2><p id="1864" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated">进行了类似的分析，并且所有特征都具有与相关性分数相当的相关值，但是与集合1和集合2相比稍低。有趣的是，具有最高相关性的特征被发现是min_tf_ST，即标题文本中搜索词的最小词频。直觉上，这应该是一个好的特性；min_tf_ST = 0意味着在搜索中有没有在标题中出现的单词，min_tf_ST ≠ 0意味着在标题中找到了搜索中的所有单词，因此表示高度相关。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/d5287180a87ad988fb497a428aa04d9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*FE7bJJNHZ67XdXRgfHDm3w.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">相关性最高的第3组特征</p></figure><p id="64a7" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">ST功能的PDF图，即搜索和标题之间的PDF图如下所示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pl"><img src="../Images/7b37157f304db6b9323216d0b5f95e75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8FdAQRNt6kkxt9c-6Ruk2Q.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pm"><img src="../Images/c353930044ad320bef3662f34ffc8f15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iKAEhumTrqJr_n_3t2JBAQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">第三组功能的pdf</p></figure><p id="2905" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated"><strong class="lz ja">观察和结论:</strong></p><ol class=""><li id="e201" class="lx ly iq lz b ma ns mc nt me oh mg oi mi oj mk ml mm mn mo bi translated">在tf、idf、tf-idf特征中，tf特征即min_tf、max_tf和sum_tf是最有效的。而idf特征不能区分这两个类别，也没有显示出与相关性分数的高度相关性。</li><li id="990b" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">bm25和语言模型特性(JM，AD，Dir)显示了这两个类之间的区别。请注意，我设计的语言模型与相关性呈负相关，因此较低的值意味着高相关性</li><li id="ac94" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">所有这些特征的唯一警告是，它们彼此具有非常高的相关值，因此存在大量冗余，这可能会影响线性模型，如线性回归和SVM。为了解决这个问题，我们可以删除这些高度相关的特征中的一个。</li></ol><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oc od l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/002dd19e35321bf9787fcfae48a21801.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*vkPB5V_L43vzefJQoP5aJw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">AD_SB和Dir_SB特征高度相关</p></figure><h2 id="ff43" class="mu lg iq bd lh mv mw dn ll mx my dp lp me mz na lr mg nb nc lt mi nd ne lv iw bi translated">功能集4:查询扩展</h2><p id="8abd" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated"><a class="ae le" href="https://en.wikipedia.org/wiki/Query_expansion" rel="noopener ugc nofollow" target="_blank">查询扩展</a>是一个很容易理解的概念。</p><ul class=""><li id="4d2b" class="lx ly iq lz b ma ns mc nt me oh mg oi mi oj mk oa mm mn mo bi translated">首先，使用Word2Vec将搜索查询中的所有单词转换成单词向量。</li><li id="82e2" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk oa mm mn mo bi translated">然后，设q是一个由一包术语表示的用户查询，q = (t1，t2，.。。，t|q|)</li><li id="1cb7" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk oa mm mn mo bi translated">为了扩展查询q，我们遵循对于每个t ∈ q，使用w2v收集与t最相似的k个术语。将新术语包含到查询集中，给出q。</li></ul><p id="7dd5" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">在查询扩展之后，我们可以使用扩展的搜索查询来重新计算来自特征集1的所有基本特征。</p><p id="eda4" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">我确实在这个集合上执行了一点EDA，结果观察非常类似于没有扩展的搜索，即扩展术语没有被证明是非常有帮助的。这可能是因为我在我的语料库上训练了一个定制的word2vec模型，该模型可能没有训练好。有了训练有素的模型，我相信查询扩展会非常有帮助。</p><h1 id="0c4c" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">功能集的性能图表</h1><p id="ca4a" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated">在这些特性集上，我用一些超参数调整训练了一个<a class="ae le" href="https://xgboost.readthedocs.io/en/latest/python/python_api.html" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>模型。我不断添加特性集，检查是否有任何改进，结果非常好。训练是在具有74067个数据点的整个训练数据上进行的。为了评估，所用的指标是具有5重交叉验证的RMSE。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pn"><img src="../Images/105a80ee17ff05e8428febf2fff5344b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0uAEBSEzDTX2m2RvaBn1Zw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">功能集的性能图表</p></figure><h2 id="6788" class="mu lg iq bd lh mv mw dn ll mx my dp lp me mz na lr mg nb nc lt mi nd ne lv iw bi translated">意见和结论:</h2><ol class=""><li id="37e7" class="lx ly iq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated">随着我们添加更多功能，错误不断减少。</li><li id="9a28" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">f1_raw的误差为0.4805，在f1_corrected后略有改善。当我们结合使用这两个特性集时，它显著提高到0.4763。因此，原始特征集和校正后的特征集确实提高了整体性能。</li><li id="854c" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">添加f2_lsi集后，误差也大幅下降至0.471。</li><li id="2d02" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">当我们添加特征集3时，观察到最大的改进，其中误差从0.471下降到0.4547。不仅如此，特征集3单独表现得非常好，误差为0.4569。因此这是最重要的一套。</li><li id="e9c5" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">f4的查询扩展特性并没有带来很大的改进。</li><li id="3c15" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">在来自所有集合的组合的特征上观察到的最好分数是0.4541。</li></ol><h1 id="548c" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">最终建模和评估</h1><h2 id="d10c" class="mu lg iq bd lh mv mw dn ll mx my dp lp me mz na lr mg nb nc lt mi nd ne lv iw bi translated">堆积回归器</h2><p id="3fd2" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated">前一节中的XGBoost模型主要是为了观察和比较不同特性集的性能而构建的。对于最后的建模部分，我用17个基础模型和一个岭回归模型作为元模型构建了一个定制的堆叠回归器。为此，我进行了如下操作:</p><ol class=""><li id="ebb4" class="lx ly iq lz b ma ns mc nt me oh mg oi mi oj mk ml mm mn mo bi translated">将train.csv文件中的全部数据(74067点)分成训练集和测试集(80–20)</li><li id="8a99" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">现在，我把80%的火车分成D1和D2。(50–50).所以D1和D2各有大约29k点。</li><li id="e7f4" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">在d1，我做了替换取样，为我的17个基本模型创建了17个样品D1、d2、D3…d17。所用的采样比在0.85和0.9之间。因此，为了训练每个基本模型，使用了大约25k个数据点。</li><li id="8c50" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">然后，我创建了17个模型，并用这17个样本中的每一个来训练这些模型。</li><li id="4c47" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">我将D2集传递给这17个模型中的每一个，并从每个模型中得到17个对D2的预测。</li><li id="3c19" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">现在，使用这17个预测，我创建了一个新的数据集，对于D2，我已经知道其相应的目标值，所以我用这17个预测作为特征来训练我的元模型。因此，用29k个点训练元模型，每个点具有17个特征。</li><li id="3415" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">对于模型评估，我使用我在开始时保留的20%的数据作为测试集。我将该测试集传递给每个基础模型，获得17个预测，用这17个预测创建一个新的数据集，最后，将它传递给元模型以获得最终预测。使用这个最终预测和测试集的目标，我计算了模型的性能分数。</li></ol><h2 id="d0c6" class="mu lg iq bd lh mv mw dn ll mx my dp lp me mz na lr mg nb nc lt mi nd ne lv iw bi translated">基础模型</h2><p id="a88d" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated">我把基本模型分成三组M1、M2和M3。主要使用基于树的模型和线性模型。</p><ul class=""><li id="3eb8" class="lx ly iq lz b ma ns mc nt me oh mg oi mi oj mk oa mm mn mo bi translated">M1模型在总共71个基本特征上被训练。它们是f1_comb(29) + f2_lsi(3) + f3(39)。各种模型(总共7个)在这个集合上被训练。</li><li id="2e2a" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk oa mm mn mo bi translated">M2模型在总共971个特征上被训练。其中71个是基本特征。其他900个特性来自搜索、标题和描述的Word2Vec嵌入。</li><li id="4681" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk oa mm mn mo bi translated">M3模型在总共4071个特征上被训练:71个基本特征，其余4000个来自特征集3中的截断SVD。对于这个集合，由于它的高维数，我只坚持线性模型。</li></ul><p id="611c" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">对于交叉验证，我通常使用5重交叉验证。基本型号的性能报告如下所示:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi po"><img src="../Images/ff9c2621ac4c3448042f5e5233da60d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*sExlXLEcOPn4zNA1-NLUVg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">基本型号的性能</p></figure><h2 id="a67e" class="mu lg iq bd lh mv mw dn ll mx my dp lp me mz na lr mg nb nc lt mi nd ne lv iw bi translated">元模型</h2><p id="00d0" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated">对于元模型，我使用了一个简单的岭回归模型。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="oc od l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pp"><img src="../Images/b430535302eef7ba087d1947ff47506f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ktOVvz-Maect_jDSwNlInQ.png"/></div></div></figure><p id="9a03" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">经过超参数调优，我在测试集上得到了最终的RMSE<strong class="lz ja">0.4539</strong>。在火车上，我得到了0.4531 <strong class="lz ja"> </strong>的RMSE，因此模型也没有过度拟合。</p><h2 id="ee62" class="mu lg iq bd lh mv mw dn ll mx my dp lp me mz na lr mg nb nc lt mi nd ne lv iw bi translated">Kaggle上的性能</h2><p id="0c1f" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated">我在Kaggle提供的包含超过160，000个数据点的测试集上测试了我的最终模型，它得到了0.46525的私人分数<strong class="lz ja">。在两千多支参赛队伍中，它名列前10%</strong><strong class="lz ja">。</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pq"><img src="../Images/604aae84126114b3b95dff850422085b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UJgG-rSmNs0qTq_Ua7uS4Q.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">前10%</p></figure><h1 id="0f9e" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">延伸到现实世界的搜索引擎</h1><p id="6041" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated">正如本文前面提到的，在现实世界的搜索引擎中，对于任何搜索查询，我们首先会使用一个更简单的检索模型来检索一些候选产品。然后，使用机器学习模型对这些产品进行重新排序。</p><p id="b37f" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">我们已经以堆叠回归器的形式建立了机器学习模型。对于最初的检索部分，我首先用train.csv和test.csv文件中的所有产品创建了一个数据库。该数据库包含124428个独特的产品，其字段为product_uid、标题、描述和属性。对于模型，我使用<a class="ae le" href="https://en.wikipedia.org/wiki/Okapi_BM25" rel="noopener ugc nofollow" target="_blank"> BM25 </a>，这也是我们机器学习模型的一个特征。给定一个搜索查询，BM25模型根据它的自定义排名函数检索前100个产品。</p><p id="e5af" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">现在，在这100个产品之上，我运行我的堆叠回归器，作为输入，我给出100对(搜索，产品-文本),其中搜索保持不变，但产品不同。因此，我的模型会对它们进行排名，然后我们可以向客户展示相关性得分最高的产品。</p><h1 id="6125" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated"><strong class="ak">全面铺开</strong></h1><p id="77e1" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated">该模型的完整管道如下所示:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pr"><img src="../Images/040269ec96d3a44a6d84f27635247920.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*baCt3v3AVPS67Mlkg_TbIg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">完整管道</p></figure><h1 id="711a" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">最终测试和结果</h1><p id="2e3e" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated">所以现在整个搜索引擎完成了。我对它进行了一系列查询测试。结果非常有趣。我在下面附上一些结果的截图。</p><p id="d088" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">首先，一个简单的搜索“潘”和结果足够好。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ps"><img src="../Images/5bb718ee7fc75a18ca844a41a540cc68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dxbW8M4FjJKeVmfTIuF20g.png"/></div></div></figure><p id="ff7c" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">然后我搜索“空调”，然后是一个拼写错误“AOR conditioner”。两者的结果完全相同。这意味着我们的拼写校正器工作正常！</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pt"><img src="../Images/d5706f7822e2fa861a069a535f930cee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CeL-VxQCacphEKqNlLSRdQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">'空调'</p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oq"><img src="../Images/25d9dca381fba21ca236098068f5defe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sw8adQ6YVuvLjitFsqjT4Q.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">' aor condiotioner '</p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pu"><img src="../Images/756d42d3e631e1b2a19b2929e479a91d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FENXT_5q8KADhkoHMD0_3A.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">“温水”</p></figure><p id="14a6" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">对于“温水”，结果很有意思。没有一款产品的名称同时包含“温暖”和“水”两个词。在最上面的结果中，你可以看到所有的标题都有两个单词。那么第一个标题被赋予第一个排名的推理是什么？这是因为“Icicle”这个词的出现，它的定义是“由滴水冻结形成的一块悬挂的、逐渐变细的冰”。我们使用的单词embeddings一定是以类似于单词water的编码方式对其进行了编码。因此，我们的模型发现了“冰柱”与“水”的联系，并以这样的方式显示结果。另请注意，在接下来的三个结果中，我们得到了更多热水器方面的预期结果。在这里，单词embeddings much使模型知道单词“warm”与单词“heat”相关联，从而得出结果。</p><p id="0865" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">更多搜索的结果如下所示。</p><div class="kp kq kr ks gt ab cb"><figure class="pv kt pw px py pz qa paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/f4f46fbd9f12df75a803bd5eabd820eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*FUEzTtAk9G86Ad06MtQCLA.png"/></div></figure><figure class="pv kt qb px py pz qa paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/ec57656dc38820ed632fa9ea03c3a9b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1076/format:webp/1*KmbpDQ8ihnU-YZYLIeVWtw.png"/></div></figure></div><div class="ab cb"><figure class="pv kt qc px py pz qa paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/dacc2f7fc6db14d52beeedb04d6a1a15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*A_eM15IijjmcYsak63Dc9w.png"/></div></figure><figure class="pv kt qd px py pz qa paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/ba43b7d1b7eeb08e4eaad0d2789d59f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*Nwi0f3L-icrJhSwHXJxPAA.png"/></div></figure></div><p id="d2a0" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated"><strong class="lz ja">运行时间<br/> </strong>不同的搜索运行时间不同。对于像“pan”这样的简单单词搜索，平均运行时间约为5.09秒，但对于长搜索和需要拼写纠正的搜索，运行时间在5.5秒和<strong class="lz ja"> </strong> 5.7秒之间。</p><p id="eee7" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated"><strong class="lz ja">注意:</strong>本案例研究的主要目的是对产品搜索进行建模，主要关注于提出我们可以应用ML算法的有用特征。我们不太关注设计和优化部分。对于现实世界的使用，这个搜索引擎可以像疯了一样进行优化，并且运行时间不需要很长时间就可以减少到一秒钟以下。</p><p id="4b4e" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated"><strong class="lz ja">网络应用演示</strong></p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="qe od l"/></div></figure><h1 id="ba1e" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">结论和未来工作</h1><p id="d5eb" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated">在这个案例研究中，我探索了信息检索领域的各种技术，以便从提供的数据中提取特征。这些技术从简单的集合操作符到潜在的语义索引。基于LSI的相似性和传统的检索模型如BM25和语言模型被证明是非常有效的。对于建模，我建立了一个具有17个基础模型的堆叠回归器，并将岭回归器作为元模型，在Kaggle上获得了0.4652的最终分数。此外，我通过添加BM25形式的初始检索模型，将其扩展为一个成熟的搜索引擎。</p><p id="c9c2" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">我的分数和排名第一的队伍的分数(0.4319)还是有很大差距的。因此，很明显，需要做更多的工作来改进模型。以下是一些建议:</p><ol class=""><li id="0b2d" class="lx ly iq lz b ma ns mc nt me oh mg oi mi oj mk ml mm mn mo bi translated">我们拥有的文本数据与互联网上找到的一般文本非常不同。因此，只要有可能根据我们自己的客户数据训练一个模型，我们就应该探索这种可能性。比如我们可以训练自己的Word2Vec模型。有了这个，也可以探索Para2Vec和Sent2Vec模型。</li><li id="7343" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">可以实现更好的拼写校正器，因为这可以使模型更加健壮。</li><li id="c54f" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">属性文本未在任何特征中使用。可以收录。</li><li id="b5e7" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">在预处理部分，可以进行词汇化。</li><li id="f295" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">一定有品牌具有相似类型的产品，因此品牌可以被聚类，然后聚类数本身可以被用作分类特征。</li><li id="ea3d" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">只有常用词的“计数”被用作特征。我们还可以尝试找到一种方法来嵌入这些常用词，以告诉我们的模型存在哪种常用词。</li><li id="661c" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">在查询扩展中，为了计算相似的单词，可以探索更好的矢量化技术</li><li id="a253" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">对于建模部分，可以探索LSTMs、GRUs、BERT、CNN等深度学习模型。</li></ol><h1 id="451d" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">感谢</h1><p id="a58a" class="pw-post-body-paragraph nf ng iq lz b ma mb ka nh mc md kd ni me nj nk nl mg nm nn no mi np nq nr mk ij bi translated">我要感谢整个<a class="ae le" href="https://www.appliedaicourse.com/" rel="noopener ugc nofollow" target="_blank">应用课程</a>团队在整个案例研究中对我的指导。</p><h1 id="b227" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">参考</h1><ol class=""><li id="3813" class="lx ly iq lz b ma mb mc md me mf mg mh mi mj mk ml mm mn mo bi translated"><a class="ae le" href="https://www.kaggle.com/c/home-depot-product-search-relevance" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/c/home-depot-product-search-relevance</a></li><li id="3107" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><a class="ae le" href="https://en.wikipedia.org/wiki/Information_retrieval" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Information_retrieval</a></li><li id="e23b" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><a class="ae le" href="https://www.researchgate.net/publication/232614940_A_Boolean_Model_in_Information_Retrieval_for_Search_Engines_PDF" rel="noopener ugc nofollow" target="_blank">搜索引擎信息检索中的布尔模型</a></li><li id="6cb2" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><a class="ae le" href="https://www.sciencedirect.com/science/article/abs/pii/S0165011409001080?via%3Dihub" rel="noopener ugc nofollow" target="_blank">sławomirzadrożny和KatarzynaNowacka重新研究了模糊信息检索模型</a></li><li id="b0f5" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><a class="ae le" href="https://arxiv.org/abs/0911.5046" rel="noopener ugc nofollow" target="_blank">将概率模型BM25/BM25F集成到Lucene中</a></li><li id="4c21" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">由华金·佩雷斯-伊格莱西亚斯、何塞·佩雷斯-阿圭拉等人创作的《https://en.wikipedia.org/wiki/Learning_to_rank》。</li><li id="07a3" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">Indri:一个基于语言模型的复杂查询搜索引擎，作者Trevor Strohman，Donald Metzler，Howard Turtle和W. Bruce Croft。</li><li id="1d25" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><a class="ae le" href="http://delab.csd.auth.gr/~dimitris/courses/ir_spring06/page_rank_computing/01cc99333c00501ddab030.pdf" rel="noopener ugc nofollow" target="_blank">使用线性代数进行信息检索</a>由Michael W. Berry等人完成。</li><li id="a723" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><a class="ae le" href="https://www.researchgate.net/publication/310823543_Query_Expansion_Using_Word_Embeddings" rel="noopener ugc nofollow" target="_blank">Saar Kuzi等人的使用单词嵌入的查询扩展</a>。</li><li id="f166" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><a class="ae le" href="https://arxiv.org/pdf/1803.05127.pdf" rel="noopener ugc nofollow" target="_blank">在微软学习排序数据集上的特征选择和模型比较</a>森磊等。</li><li id="85f7" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><a class="ae le" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.104.3479&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">信息检索中的向量空间模型:术语加权问题</a></li><li id="96ad" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><a class="ae le" href="https://opensourceconnections.com/blog/2016/03/29/semantic-search-with-latent-semantic-analysis/" rel="noopener ugc nofollow" target="_blank">https://opensourceconnections . com/blog/2016/03/29/semantic-search-with-latent-semantic-analysis/</a></li><li id="7ea8" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><a class="ae le" href="http://www.ccs.neu.edu/home/jaa/CSG339.06F/Lectures/vector.pdf" rel="noopener ugc nofollow" target="_blank">http://www . CCS . neu . edu/home/jaa/CSG 339.06 f/Lectures/vector . pdf</a></li><li id="8c36" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">Victor Lavrenko关于信息检索语言模型的视频讲座</li><li id="9115" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><a class="ae le" href="http://datasciencebar.github.io/blog/home-depot-produce-relevance-review/" rel="noopener ugc nofollow" target="_blank">http://datasciencebar . github . io/blog/home-depot-produce-relevance-review/</a></li><li id="1066" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><a class="ae le" href="https://medium.com/kaggle-blog/home-depot-product-search-relevance-winners-interview-2nd-place-thomas-sean-qingchen-nima-68068f9f9ffd" rel="noopener">https://medium . com/ka ggle-blog/home-depot-product-search-relevance-winders-interview-2nd-place-Thomas-Sean-陈清-nima-68068 f 9 ffd</a></li><li id="c4b0" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">https://arxiv.org/pdf/1803.05127.pdf<a class="ae le" href="https://arxiv.org/pdf/1803.05127.pdf" rel="noopener ugc nofollow" target="_blank"/></li><li id="aa59" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated">【http://billy-inn.github.io/papers/cmput690.pdf T4】</li><li id="f334" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><a class="ae le" href="https://arxiv.org/pdf/2001.04980.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2001.04980.pdf</a></li><li id="4987" class="lx ly iq lz b ma mp mc mq me mr mg ms mi mt mk ml mm mn mo bi translated"><a class="ae le" href="https://www.appliedaicourse.com/" rel="noopener ugc nofollow" target="_blank">https://www.appliedaicourse.com</a></li></ol></div><div class="ab cl qf qg hu qh" role="separator"><span class="qi bw bk qj qk ql"/><span class="qi bw bk qj qk ql"/><span class="qi bw bk qj qk"/></div><div class="ij ik il im in"><p id="002d" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">完整的代码可以在我的Github档案中找到:</p><div class="qm qn gp gr qo qp"><a href="https://github.com/kriz17/Home-Depot-Product-Search-Relevance" rel="noopener  ugc nofollow" target="_blank"><div class="qq ab fo"><div class="qr ab qs cl cj qt"><h2 class="bd ja gy z fp qu fr fs qv fu fw iz bi translated">kriz 17/Home-Depot-产品-搜索-相关性</h2><div class="qw l"><h3 class="bd b gy z fp qu fr fs qv fu fw dk translated">通过在GitHub上创建一个帐户，为kriz 17/Home-Depot-Product-Search-Relevance开发做出贡献。</h3></div><div class="qx l"><p class="bd b dl z fp qu fr fs qv fu fw dk translated">github.com</p></div></div><div class="qy l"><div class="qz l ra rb rc qy rd ky qp"/></div></div></a></div><p id="429c" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated">在LinkedIn上与我联系:</p><div class="qm qn gp gr qo qp"><a href="https://www.linkedin.com/in/kriz-moses/" rel="noopener  ugc nofollow" target="_blank"><div class="qq ab fo"><div class="qr ab qs cl cj qt"><h2 class="bd ja gy z fp qu fr fs qv fu fw iz bi translated">印度中央邦克里兹·摩西-IIT·印多尔-印多尔| LinkedIn</h2><div class="qw l"><h3 class="bd b gy z fp qu fr fs qv fu fw dk translated">在全球最大的职业社区LinkedIn上查看Kriz Moses的个人资料。Kriz的教育列在他们的…</h3></div><div class="qx l"><p class="bd b dl z fp qu fr fs qv fu fw dk translated">www.linkedin.com</p></div></div><div class="qy l"><div class="re l ra rb rc qy rd ky qp"/></div></div></a></div><p id="04fc" class="pw-post-body-paragraph nf ng iq lz b ma ns ka nh mc nt kd ni me nu nk nl mg nv nn no mi nw nq nr mk ij bi translated"><strong class="lz ja"> PS </strong>:如果你认为他们可以改进这个博客，请随时提供意见/批评，我一定会努力做出必要的修改。</p></div></div>    
</body>
</html>