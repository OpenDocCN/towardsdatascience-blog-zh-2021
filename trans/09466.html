<html>
<head>
<title>A beginner’s guide to OCTIS vol. 1: Optimizing and Comparing Topic Models Is Simple</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">OCTIS入门指南第1卷:优化和比较主题模型很简单</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-beginners-guide-to-octis-optimizing-and-comparing-topic-models-is-simple-590554ec9ba6?source=collection_archive---------18-----------------------#2021-09-02">https://towardsdatascience.com/a-beginners-guide-to-octis-optimizing-and-comparing-topic-models-is-simple-590554ec9ba6?source=collection_archive---------18-----------------------#2021-09-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="e37a" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="b92e" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">拥有最高数量的集成的最先进的主题模型的Python包。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/9e149479559964c9f973793c6625789e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Ty8KPxzjhHKWbW9V"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae le" href="https://unsplash.com/@universaleye?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">环球之眼</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><p id="70d4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">主题模型是一种有前途的生成统计方法，旨在提取文档集合中隐藏的主题。通常，主题模型有两个矩阵作为输出。</p><p id="88ff" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">1.主题单词矩阵(<em class="mb">词汇</em> x <em class="mb">数字_主题</em>)，表示单词<em class="mb"> i </em>出现在主题<em class="mb"> k. </em>中的概率</p><p id="45cb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">2.主题-文档-矩阵(<em class="mb">数量_主题</em> x <em class="mb">数量_文档</em>)。</p><p id="0e5f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">然后，来自该矩阵的具有最高概率的前n个单词被用于表示主题。</p><p id="6c43" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">最流行的主题建模方法是潜在的Dirichlet分配，许多文章都是关于它的工作和实现的。然而，只关注LDA是有限制的，并且对于给定的语料库可能是次优的。对于给定的语料库，其他主题模型可能更好。</p><p id="a525" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">最近发布了新的Python包，OCTIS(优化和比较主题模型很简单！).根据它的名字，OCTIS允许在主题模型之间进行简单的比较，这不会让你感到惊讶。该软件包包含多个主题模型、数据集、评估指标和优化选项。</p><p id="2c46" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这篇博客中，我演示了如何开始使用这个包。在以后的博客中，我将更详细地讨论主题模型优化和比较。</p><p id="afb9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">首先安装软件包:</p><pre class="kp kq kr ks gt mc md me mf aw mg bi"><span id="b9e1" class="mh mi iq md b gy mj mk l ml mm">pip install octis</span></pre><p id="42d1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们需要一个数据集。</p><pre class="kp kq kr ks gt mc md me mf aw mg bi"><span id="802d" class="mh mi iq md b gy mj mk l ml mm">from octis.dataset.dataset import Dataset</span></pre><p id="404f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">OCTIS有四个内置数据集:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/5582881b7aa5585b893c1311f72e1d1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*BpAuLmcNbWdPchOGPmuY3w.png"/></div></figure><p id="b116" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们从最小的开始:</p><pre class="kp kq kr ks gt mc md me mf aw mg bi"><span id="ca7b" class="mh mi iq md b gy mj mk l ml mm">dataset = Dataset()</span><span id="12d8" class="mh mi iq md b gy mo mk l ml mm">dataset.fetch_dataset('BBC_news')</span></pre><p id="23f6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">要选择其他数据集，请从表中选择一个名称(注意代码中“BBC_news”的拼写不同于表中的拼写。其他名称相同)。</p><p id="fe91" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">现在，让我们看看数据集是什么样子的:</p><pre class="kp kq kr ks gt mc md me mf aw mg bi"><span id="45e7" class="mh mi iq md b gy mj mk l ml mm">len(dataset._Dataset__corpus)<br/>&gt;&gt;&gt; 2225</span><span id="025b" class="mh mi iq md b gy mo mk l ml mm">len(dataset._Dataset__vocabulary)<br/>&gt;&gt;&gt; 2949</span><span id="c4d9" class="mh mi iq md b gy mo mk l ml mm">print(dataset._Dataset__corpus[0:5])<br/>&gt;&gt;&gt; [[‘broadband’, ‘ahead’, ‘join’, ‘internet’, ‘fast’, ‘accord’, ‘official’, ‘figure’, ‘number’, ‘business’, ‘connect’, ‘jump’, ‘report’, ‘broadband’, ‘connection’, ‘end’, ‘compare’, ‘nation’, ‘rank’, ‘world’, ‘telecom’, ‘body’, ‘election’, ‘campaign’, ‘ensure’, ‘affordable’, ‘high’, ‘speed’, ‘net’, ‘access’, ‘american’, ‘accord’, ‘report’, ‘broadband’, ‘increasingly’, ‘popular’, ‘research’, ‘shopping’, ‘download’, ‘music’, ‘watch’, ‘video’, ‘total’, ‘number’, ‘business’, ‘broadband’, ‘rise’, ‘end’, ‘compare’, ‘hook’, ‘broadband’, ‘subscriber’, ‘line’, ‘technology’, ‘ordinary’, ‘phone’, ‘line’, ‘support’, ‘high’, ‘data’, ‘speed’, ‘cable’, ‘lead’, ‘account’, ‘line’, ‘broadband’, ‘phone’, ‘line’, ‘connection’, ‘accord’, ‘figure’],</span><span id="9fe7" class="mh mi iq md b gy mo mk l ml mm">[‘plan’, ‘share’, ‘sale’, ‘owner’, ‘technology’, ‘dominate’, ‘index’, ‘plan’, ‘sell’, ‘share’, ‘public’, ‘list’, ‘market’, ‘operate’, ‘accord’, ‘document’, ‘file’, ‘stock’, ‘market’, ‘plan’, ‘raise’, ‘sale’, ‘observer’, ‘step’, ‘close’, ‘full’, ‘public’, ‘icon’, ‘technology’, ‘boom’, ‘recently’, ‘pour’, ‘cold’, ‘water’, ‘suggestion’, ‘company’, ‘sell’, ‘share’, ‘private’, ‘technically’, ‘public’, ‘stock’, ‘start’, ‘trade’, ‘list’, ‘equity’, ‘trade’, ‘money’, ‘sale’, ‘investor’, ‘buy’, ‘share’, ‘private’, ‘filing’, ‘document’, ‘share’, ‘technology’, ‘firm’, ‘company’, ‘high’, ‘growth’, ‘potential’, ‘symbol’, ‘internet’, ‘telecom’, ‘boom’, ‘bubble’, ‘burst’, ‘recovery’, ‘fortune’, ‘tech’, ‘giant’, ‘dot’, ‘revive’, ‘fortune’],</span><span id="67b3" class="mh mi iq md b gy mo mk l ml mm">[‘mobile’, ‘rack’, ‘mobile’, ‘phone’, ‘celebrate’, ‘anniversary’, ‘weekend’, ‘mobile’, ‘phone’, ‘call’, ‘vodafone’, ‘network’, ‘veteran’, ‘day’, ‘mobile’, ‘phone’, ‘integral’, ‘part’, ‘modern’, ‘life’, ‘briton’, ‘handset’, ‘mobile’, ‘popular’, ‘handset’, ‘phone’, ‘rarely’, ‘call’, ‘portable’, ‘phone’, ‘commercial’, ‘mobile’, ‘service’, ‘launch’, ‘rest’, ‘world’, ‘set’, ‘network’, ‘call’, ‘walk’, ‘call’, ‘office’, ‘house’, ‘day’, ‘vodafone’, ‘firm’, ‘mobile’, ‘network’, ‘launch’, ‘service’, ‘spokesman’, ‘phone’, ‘launch’, ‘size’, ‘cost’, ‘battery’, ‘life’, ‘minute’, ‘hugely’, ‘popular’, ‘mid’, ‘status’, ‘symbol’, ‘young’, ‘business’, ‘fact’, ‘phone’, ‘radio’, ‘signal’, ‘communicate’, ‘easy’, ‘rack’, ‘customer’, ‘month’, ‘easy’, ‘forget’, ‘put’, ‘bid’, ‘document’, ‘forecast’, ‘total’, ‘market’, ‘forecast’, ‘vodafone’, ‘customer’, ‘vodafone’, ‘mobile’, ‘phone’, ‘operator’, ‘launch’, ‘launch’, ‘newcomer’, ‘operate’, ‘mobile’, ‘network’, ‘operator’, ‘technology’, ‘spectrum’, ‘phone’, ‘retire’, ‘call’, ‘global’, ‘system’, ‘mobile’, ‘widely’, ‘phone’, ‘technology’, ‘planet’, ‘call’, ‘digital’, ‘technology’, ‘introduce’, ‘thing’, ‘text’, ‘mobile’, ‘popular’],</span><span id="a8bb" class="mh mi iq md b gy mo mk l ml mm">[‘launch’, ‘reconstruction’, ‘drive’, ‘appeal’, ‘peace’, ‘national’, ‘unity’, ‘important’, ‘find’, ‘solution’, ‘internal’, ‘conflict’, ‘damage’, ‘tsunami’, ‘cut’, ‘percentage’, ‘point’, ‘economic’, ‘growth’, ‘estimate’, ‘wave’, ‘leave’, ‘physical’, ‘damage’, ‘equal’, ‘economy’, ‘separately’, ‘lose’, ‘call’, ‘action’, ‘create’, ‘job’, ‘attend’, ‘ceremony’, ‘southern’, ‘town’, ‘join’, ‘government’, ‘opposition’, ‘politician’, ‘lay’, ‘foundation’, ‘stone’, ‘housing’, ‘project’, ‘intend’, ‘provide’, ‘home’, ‘tsunami’, ‘call’, ‘tragedy’, ‘start’, ‘beginning’, ‘rebuild’, ‘nation’, ‘country’, ‘natural’, ‘resource’, ‘fully’, ‘fight’, ‘add’, ‘due’, ‘arrive’, ‘revive’, ‘peace’, ‘talk’, ‘decade’, ‘long’, ‘conflict’, ‘government’, ‘force’, ‘tiger’, ‘separate’, ‘state’, ‘country’, ‘reconstruction’, ‘effort’, ‘hamper’, ‘tension’, ‘side’, ‘authority’, ‘initial’, ‘estimate’, ‘put’, ‘physical’, ‘damage’, ‘add’, ‘implication’, ‘economy’, ‘wide’, ‘broad’, ‘impact’, ‘substantial’, ‘detail’, ‘difficult’, ‘assess’, ‘early’, ‘stage’, ‘growth’, ‘inflation’, ‘balance’, ‘payment’, ‘foreign’, ‘exchange’, ‘reserve’, ‘expect’, ‘show’, ‘effect’, ‘lose’, ‘business’, ‘reconstruction’, ‘cost’, ‘industry’, ‘agricultural’, ‘production’, ‘affect’, ‘tourism’, ‘suffer’, ‘short’, ‘term’, ‘report’, ‘estimate’, ‘lose’, ‘job’, ‘industry’, ‘earning’, ‘tourism’, ‘expect’, ‘low’, ‘economic’, ‘growth’, ‘expect’, ‘previously’, ‘forecast’, ‘inflation’, ‘climb’, ‘compare’, ‘previous’, ‘estimate’, ‘major’, ‘export’, ‘suffer’, ‘expect’, ‘reconstruction’, ‘effort’, ‘require’, ‘high’, ‘import’, ‘damage’, ‘balance’, ‘payment’, ‘foreign’, ‘exchange’, ‘reserve’, ‘hard’, ‘press’, ‘international’, ‘reserve’, ‘pre’, ‘tsunami’, ‘level’, ‘total’, ‘month’, ‘worth’, ‘import’, ‘week’, ‘approve’, ‘request’, ‘freeze’, ‘loan’, ‘repayment’],</span><span id="a4b0" class="mh mi iq md b gy mo mk l ml mm">[‘buy’, ‘giant’, ‘profit’, ‘soar’, ‘acquisition’, ‘big’, ‘firm’, ‘tax’, ‘profit’, ‘rise’, ‘expect’, ‘solid’, ‘growth’, ‘performance’, ‘sale’, ‘firm’, ‘world’, ‘big’, ‘volume’, ‘buy’, ‘acquisition’, ‘sale’, ‘volume’, ‘grow’, ‘month’, ‘sale’, ‘account’, ‘increase’, ‘sell’, ‘volume’, ‘big’, ‘term’, ‘sale’, ‘continue’, ‘demand’, ‘product’, ‘south’, ‘american’, ‘market’, ‘brazilian’, ‘arm’, ‘popular’, ‘expect’, ‘boost’, ‘turnover’, ‘business’, ‘analyst’, ‘strong’, ‘performance’, ‘boost’, ‘share’, ‘market’, ‘end’, ‘report’, ‘contrast’, ‘volume’, ‘sale’, ‘fall’, ‘central’, ‘european’, ‘sale’, ‘rise’, ‘net’, ‘profit’]]</span></pre><p id="71a0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">那很好；每个文档都是预处理过的标记列表，所以这里不需要进行任何预处理。</p><p id="6c78" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">OCTIS有八个内置主题模型:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/480aa1a342374d7d50eb06b99e58f1a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*70Gbjr7mNsnpzMAKdhwNfw.png"/></div></figure><p id="915d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">(如果您实现了其中一个模型，请确保引用正确的论文)</p><p id="dc63" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">现在，让我们用这些数据训练一个NeuralLDA模型:</p><pre class="kp kq kr ks gt mc md me mf aw mg bi"><span id="b27c" class="mh mi iq md b gy mj mk l ml mm">from octis.models.NeuralLDA import NeuralLDA</span></pre><p id="c0b5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">通过用上表中的名称替换“NeuralLDA”，可以类似地导入其他主题模型(注意，“ProdLda”需要是“ProdLDA”)。</p><p id="b62a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">初始化模型:</p><pre class="kp kq kr ks gt mc md me mf aw mg bi"><span id="c78a" class="mh mi iq md b gy mj mk l ml mm">model = NeuralLDA(num_topics=20)</span></pre><p id="2426" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">所有不同的主题模型都包含“num_topics”。其他参数因型号而异，可以在实现页面上找到。但是，检查源代码可以更快地找到参数:</p><pre class="kp kq kr ks gt mc md me mf aw mg bi"><span id="d3f9" class="mh mi iq md b gy mj mk l ml mm">import inspect</span><span id="8fcd" class="mh mi iq md b gy mo mk l ml mm">print(inspect.getsource(NeuralLDA))</span><span id="2562" class="mh mi iq md b gy mo mk l ml mm">&gt;&gt;&gt; class NeuralLDA(AVITM):</span><span id="ed3b" class="mh mi iq md b gy mo mk l ml mm">def __init__(self, num_topics=10, activation=’softplus’, dropout=0.2, learn_priors=True, batch_size=64, lr=2e-3,</span><span id="fb0c" class="mh mi iq md b gy mo mk l ml mm">momentum=0.99, solver=’adam’, num_epochs=100, reduce_on_plateau=False, prior_mean=0.0,</span><span id="e78f" class="mh mi iq md b gy mo mk l ml mm">prior_variance=None, num_layers=2, num_neurons=100, num_samples=10, use_partitions=True):</span><span id="332c" class="mh mi iq md b gy mo mk l ml mm">…</span></pre><p id="0eb3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">构造函数中的参数可以手动设置。在这种情况下，它们是:</p><ul class=""><li id="cc1b" class="mq mr iq lh b li lj ll lm lo ms ls mt lw mu ma mv mw mx my bi translated"><code class="fe mz na nb md b">num_topics</code></li><li id="cd1f" class="mq mr iq lh b li nc ll nd lo ne ls nf lw ng ma mv mw mx my bi translated"><code class="fe mz na nb md b">activation</code></li><li id="0e24" class="mq mr iq lh b li nc ll nd lo ne ls nf lw ng ma mv mw mx my bi translated"><code class="fe mz na nb md b">dropout</code></li><li id="ae82" class="mq mr iq lh b li nc ll nd lo ne ls nf lw ng ma mv mw mx my bi translated"><code class="fe mz na nb md b">learn_priors</code></li><li id="cd74" class="mq mr iq lh b li nc ll nd lo ne ls nf lw ng ma mv mw mx my bi translated"><code class="fe mz na nb md b">batch_size</code></li><li id="7185" class="mq mr iq lh b li nc ll nd lo ne ls nf lw ng ma mv mw mx my bi translated"><code class="fe mz na nb md b">lr</code></li><li id="1ff4" class="mq mr iq lh b li nc ll nd lo ne ls nf lw ng ma mv mw mx my bi translated"><code class="fe mz na nb md b">momentum</code></li><li id="13d3" class="mq mr iq lh b li nc ll nd lo ne ls nf lw ng ma mv mw mx my bi translated"><code class="fe mz na nb md b">solver</code></li><li id="66b7" class="mq mr iq lh b li nc ll nd lo ne ls nf lw ng ma mv mw mx my bi translated"><code class="fe mz na nb md b">num_epochs</code></li><li id="a52a" class="mq mr iq lh b li nc ll nd lo ne ls nf lw ng ma mv mw mx my bi translated"><code class="fe mz na nb md b">reduce_on_plateau</code></li><li id="7c0c" class="mq mr iq lh b li nc ll nd lo ne ls nf lw ng ma mv mw mx my bi translated"><code class="fe mz na nb md b">prior_mean</code></li><li id="5351" class="mq mr iq lh b li nc ll nd lo ne ls nf lw ng ma mv mw mx my bi translated"><code class="fe mz na nb md b">prior_variance</code></li><li id="67c2" class="mq mr iq lh b li nc ll nd lo ne ls nf lw ng ma mv mw mx my bi translated"><code class="fe mz na nb md b">num_layers</code></li><li id="15a5" class="mq mr iq lh b li nc ll nd lo ne ls nf lw ng ma mv mw mx my bi translated"><code class="fe mz na nb md b">num_neurons</code></li><li id="2bdc" class="mq mr iq lh b li nc ll nd lo ne ls nf lw ng ma mv mw mx my bi translated"><code class="fe mz na nb md b">num_samples</code></li><li id="2223" class="mq mr iq lh b li nc ll nd lo ne ls nf lw ng ma mv mw mx my bi translated"><code class="fe mz na nb md b">use_partition</code></li></ul><p id="0a91" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">现在，我们将简单地使用默认设置，并将<code class="fe mz na nb md b">num_topics</code> = 20:</p><pre class="kp kq kr ks gt mc md me mf aw mg bi"><span id="89a7" class="mh mi iq md b gy mj mk l ml mm">model = NeuralLDA(num_topics=20)</span><span id="4580" class="mh mi iq md b gy mo mk l ml mm">trained_model = model.train_model(dataset)</span><span id="b6f0" class="mh mi iq md b gy mo mk l ml mm">#Now, you should see something like:</span><span id="f4de" class="mh mi iq md b gy mo mk l ml mm">Epoch: [1/100] Samples: [1557/155700] Train Loss: 987.1886277195729 Time: 0:00:00.352704</span><span id="aa4f" class="mh mi iq md b gy mo mk l ml mm">Epoch: [1/100] Samples: [334/33400] Validation Loss: 982.9130479275823 Time: 0:00:00.020085</span><span id="3b94" class="mh mi iq md b gy mo mk l ml mm">Epoch: [2/100] Samples: [3114/155700] Train Loss: 990.8275228805395 Time: 0:00:00.362417</span><span id="85cd" class="mh mi iq md b gy mo mk l ml mm">Epoch: [2/100] Samples: [334/33400] Validation Loss: 982.0897677301647 Time: 0:00:00.010114</span><span id="c080" class="mh mi iq md b gy mo mk l ml mm">Epoch: [3/100] Samples: [4671/155700] Train Loss: 981.4996826328677 Time: 0:00:00.362483</span><span id="0a53" class="mh mi iq md b gy mo mk l ml mm">Epoch: [3/100] Samples: [334/33400] Validation Loss: 965.4536162050898 Time: 0:00:00.010038</span></pre><p id="68a9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">trained_model是一个字典，具有以下键:</p><pre class="kp kq kr ks gt mc md me mf aw mg bi"><span id="3f9e" class="mh mi iq md b gy mj mk l ml mm">print(trained_model.keys())</span><span id="05b2" class="mh mi iq md b gy mo mk l ml mm">&gt;&gt;&gt; dict_keys([‘topics’, ‘topic-document-matrix’, ‘topic-word-matrix’, ‘test-topic-document-matrix’])</span></pre><p id="3504" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这些键如下:</p><ol class=""><li id="aed3" class="mq mr iq lh b li lj ll lm lo ms ls mt lw mu ma nh mw mx my bi translated"><em class="mb">主题</em>:word主题列表</li><li id="7fc2" class="mq mr iq lh b li nc ll nd lo ne ls nf lw ng ma nh mw mx my bi translated"><em class="mb"> topic-word-matrix </em>:每个主题的词汇的单词分布(维度:|num topics| x |vocabulary|)</li><li id="5fc4" class="mq mr iq lh b li nc ll nd lo ne ls nf lw ng ma nh mw mx my bi translated"><em class="mb">主题-文档-矩阵</em>:训练集每个文档的主题分布(维度:|主题数| x |训练文档|)</li><li id="961d" class="mq mr iq lh b li nc ll nd lo ne ls nf lw ng ma nh mw mx my bi translated"><em class="mb">test-document-topic-matrix</em>:测试集每个文档的主题分布(维度:|num topics| x |test documents|)</li></ol><p id="99de" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">第一个关键字<code class="fe mz na nb md b">topics</code>由来自主题单词矩阵的每个主题具有最高概率的<em class="mb"> n </em>个单词的列表组成。</p><p id="4a19" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们看看这些是什么:</p><pre class="kp kq kr ks gt mc md me mf aw mg bi"><span id="a83e" class="mh mi iq md b gy mj mk l ml mm">for topic in trained_model[‘topics’]:<br/>    print(“ “.join(topic))</span><span id="0d45" class="mh mi iq md b gy mo mk l ml mm">&gt;&gt;&gt; claim chief meet budget official talk cut raise future economic</span><span id="e938" class="mh mi iq md b gy mo mk l ml mm">win side match final chance team season half great goal</span><span id="654a" class="mh mi iq md b gy mo mk l ml mm">company market sale month share expect rise fall growth report</span><span id="9b2f" class="mh mi iq md b gy mo mk l ml mm">charge competition link victim ago injury anti tragedy decision miss</span><span id="9ec9" class="mh mi iq md b gy mo mk l ml mm">government plan party add election public give claim week tory</span><span id="4cf8" class="mh mi iq md b gy mo mk l ml mm">game show give big play add back week put find</span><span id="2cbe" class="mh mi iq md b gy mo mk l ml mm">company find user firm technology mobile service phone accord information</span><span id="d71d" class="mh mi iq md b gy mo mk l ml mm">case evidence order trial concern court common law clear power</span><span id="c3fb" class="mh mi iq md b gy mo mk l ml mm">enjoy tragedy rest carry navigate motion admit date opener technological</span><span id="90b3" class="mh mi iq md b gy mo mk l ml mm">choose tragedy due motion navigate involve manage suffer stiff technological</span><span id="2110" class="mh mi iq md b gy mo mk l ml mm">film good top award star director movie chance performance career</span><span id="434e" class="mh mi iq md b gy mo mk l ml mm">rival reveal drug short range break challenge battle focus fourth</span><span id="222b" class="mh mi iq md b gy mo mk l ml mm">tragedy position seller navigate squeeze motion rare flanker overwhelming technological</span><span id="54b8" class="mh mi iq md b gy mo mk l ml mm">include follow number hit man release hold place record band</span><span id="df12" class="mh mi iq md b gy mo mk l ml mm">tragedy navigate seller earn join escape squeeze opener adoption technological</span></pre><p id="966e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">通常，连贯性被用来评估主题的质量。让我们也检查一下这个模型的一致性:</p><pre class="kp kq kr ks gt mc md me mf aw mg bi"><span id="d2b0" class="mh mi iq md b gy mj mk l ml mm">from octis.evaluation_metrics.coherence_metrics import Coherence</span></pre><p id="90c9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在不同的一致性度量中，<code class="fe mz na nb md b">c_v</code>与人工翻译的相关性最高。</p><pre class="kp kq kr ks gt mc md me mf aw mg bi"><span id="ef18" class="mh mi iq md b gy mj mk l ml mm">cv = Coherence(texts=dataset.get_corpus(),topk=10, measure=’c_v’)<br/>print(‘Coherence: ‘ + str(cv.score(trained_model)))<br/>&gt;&gt;&gt; Coherence: 0.4963712149432245</span></pre><p id="804b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">(与典型的主题建模符号相反，‘topk’表示主题中包含的单词数，而不是主题数。由于我们的主题模型每个主题有十个单词，‘topk’不能超过10个)。</p><p id="841d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">此外，OCTIS还可以选择计算所谓的“多样性得分”，即主题中独特词的百分比。这个分数是制作主题质量的另一个指标。如果没有很多独特的话，那么很多话题都是大同小异，意义不是很大。</p><pre class="kp kq kr ks gt mc md me mf aw mg bi"><span id="8346" class="mh mi iq md b gy mj mk l ml mm">from octis.evaluation_metrics.diversity_metrics import TopicDiversity</span><span id="2775" class="mh mi iq md b gy mo mk l ml mm">diversity = TopicDiversity(topk=10)<br/>print('Diversity score: '+str(diversity.score(trained_model)))<br/>&gt;&gt;&gt; Diversity score: 0.8533333333333334</span></pre><p id="e841" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">连贯性和多样性主要用于比较不同主题模型的主题。在下一篇博客中，我将展示如何优化主题模型。</p></div><div class="ab cl ni nj hu nk" role="separator"><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn no"/><span class="nl bw bk nm nn"/></div><div class="ij ik il im in"><p id="75a5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果你喜欢这项工作，你可能会对主题建模感兴趣。在这种情况下，您可能也会对以下内容感兴趣。</p><p id="7f60" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">我们创建了一个新的话题建模算法，叫做FLSA-W </strong>(官方页面是<a class="ae le" href="https://ieeexplore.ieee.org/abstract/document/9660139" rel="noopener ugc nofollow" target="_blank">这里</a>，但是你可以看到论文<a class="ae le" href="https://pure.tue.nl/ws/portalfiles/portal/243684581/A_Comparative_Study_of_Fuzzy_Topic_Models_and_LDA_in_terms_of_Interpretability.pdf" rel="noopener ugc nofollow" target="_blank">这里</a>)。</p><p id="2df8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在几个公开数据集上，FLSA-W算法的性能优于其他一流算法(如LDA、ProdLDA、NMF、CTM等)。 <a class="ae le" href="https://pure.tue.nl/ws/files/222725628/Pure_ExperimentalStudyOfFlsa_wForTopicModeling.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lh ja">该作品</strong> </a> <strong class="lh ja">已提交，但尚未通过同行评审。</strong></p><p id="aad5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">如果想用FLSA-W，可以下载Gensim中的</strong><a class="ae le" href="https://pypi.org/project/FuzzyTM/" rel="noopener ugc nofollow" target="_blank"><strong class="lh ja">FuzzyTM包</strong> </a> <strong class="lh ja">或者flsamodel。</strong>如需引用，<a class="ae le" href="https://ieeexplore.ieee.org/abstract/document/9882661?casa_token=UsYg7SvoSioAAAAA:3ltCVZexA9-lPveuGVeRDh5VQW6rw0pVRDxmYk39tXbx13u4OuB2sTEFZzIGJCkdRiZBg0eJ" rel="noopener ugc nofollow" target="_blank">请用本文</a>。</p></div></div>    
</body>
</html>