<html>
<head>
<title>How to Optimize a Deep Learning Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何优化深度学习模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-optimize-a-deep-learning-model-2e9ff8a4c5de?source=collection_archive---------10-----------------------#2021-07-28">https://towardsdatascience.com/how-to-optimize-a-deep-learning-model-2e9ff8a4c5de?source=collection_archive---------10-----------------------#2021-07-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7e33" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过有效的超参数优化，让您的模型更加出色</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6eae7184c0c50053aa9c0493c8fbdb3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AKsqeAgY0_mMPTgT"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">约翰·巴克利在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="336d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">超参数优化是任何机器学习管道的关键部分。仅仅选择一个型号不足以实现卓越的性能。您还需要调整您的模型，以便更好地解决问题。</p><p id="4f6d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇文章将讨论深度学习架构的超参数调整。有代码块来重新创建所示的示例。</p><blockquote class="lv"><p id="3707" class="lw lx it bd ly lz ma mb mc md me lu dk translated"><strong class="ak">欢迎将这篇文章加入书签，复制代码来快速优化你的深度学习模型。</strong></p></blockquote><p id="af01" class="pw-post-body-paragraph kz la it lb b lc mf ju le lf mg jx lh li mh lk ll lm mi lo lp lq mj ls lt lu im bi translated"><em class="mk">如果您正在开发不同的深度学习模型，请用您的特定模型替换函数“create_model”中的代码，并更新每个函数的相关超参数。其余的优化代码是独立于模型的。</em></p><p id="5f7e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用scikit-learn开发模型时，超参数调整是一个相对简单的过程。随机搜索和网格搜索各有利弊。然而，被称为贝叶斯优化的第三种选择为超参数优化提供了一种实用、平衡的方法，可以产生更稳健的模型。</p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><h1 id="ab5a" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated"><strong class="ak">黑盒模型</strong></h1><p id="2ef2" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">机器学习中的优化通常遵循相同的格式。首先，定义一个代表损失的函数。然后，通过最小化这种损失，该模型被迫产生日益改善的性能。<br/>选择损失函数有两个主要原因。首先，它们很好地代表了问题。</p><p id="be46" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">标准损失函数的第二个方面是梯度被很好地定义。通过定义明确的梯度，许多不同的优化技术变得可用。最常见的是依赖于函数梯度的梯度下降法。</p><p id="00cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是当函数没有梯度时会发生什么呢？当您有一个黑盒模型时，这种情况正是如此。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/b1b637098c1f4ee973833c1070a59ec4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*BPv_WNTZrpZBp5CL"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">萨姆·穆卡达姆在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="4ed6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">黑盒模型有一个输入和一个输出。但是盒子里发生了什么是未知的。该属性使模型成为黑盒模型。</p><p id="73c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当模型是黑箱时，优化问题就更加困难。</p><p id="ecec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不幸的是，在建立深度学习模型时，黑盒是标准的，因此标准的优化方法性能不佳。</p><p id="5dd4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简要考虑为什么梯度下降等优化方法无法优化超参数。如果你想优化一个超参数，你需要确定这个超参数的导数。例如，想象一下对随机森林的估计数进行求导。不可能的。</p><p id="33a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，黑盒优化并不关心确定函数的导数。相反，目标是基于来自超参数空间的多个样本来选择函数。那么一旦这个函数紧密表示超参数空间，你就可以优化这个函数了。这种优化的结果产生了最佳的超参数配置，其应该在原始黑盒函数上表现良好。</p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><h1 id="dbef" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated"><strong class="ak">贝叶斯优化</strong></h1><p id="5f8c" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">在上一篇文章中，我讨论了不同的超参数调优方法，并介绍了贝叶斯优化。有关其他可用调优方法的详细信息、它们的缺陷和优点，请参考我之前的帖子:</p><div class="nq nr gp gr ns nt"><a rel="noopener follow" target="_blank" href="/hyperparameter-tuning-always-tune-your-models-7db7aeaf47e9"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd iu gy z fp ny fr fs nz fu fw is bi translated">超参数调整—始终调整您的模型</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">不要放弃免费的性能提升。</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">towardsdatascience.com</p></div></div><div class="oc l"><div class="od l oe of og oc oh ks nt"/></div></div></a></div><blockquote class="lv"><p id="2b1b" class="lw lx it bd ly lz oi oj ok ol om lu dk translated">贝叶斯优化是从可能的超参数空间中采样，基于这些样本建模函数，然后优化该模型的过程</p></blockquote><p id="6965" class="pw-post-body-paragraph kz la it lb b lc mf ju le lf mg jx lh li mh lk ll lm mi lo lp lq mj ls lt lu im bi translated">贝叶斯优化是从可能的超参数空间重复采样、基于这些样本建模函数、然后优化该模型的过程</p><p id="9465" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与需要使用导数进行优化的梯度下降相反，贝叶斯优化创建了通过从超参数空间多次采样而生成的代表性函数。</p><p id="8581" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种多次采样和生成空间近似值的过程就是该方法是贝叶斯方法的原因。最后，使用来自重复采样的先验信息来调整代表性函数。</p><p id="1f9f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个过程需要两件事，一个是从超参数空间采样的方法，另一个是优化产生的函数的方法。</p><p id="cc6f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从黑盒采样的方法需要对超参数空间进行采样，基于多个样本创建代表性函数，然后优化该函数。因此，输入成为超参数，输出是模型的性能。</p><p id="f473" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">利用来自超参数空间的足够数量的样本，生成该函数的良好近似。由此，该模型然后被优化用于超参数的最佳配置。</p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><h1 id="5c12" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated"><strong class="ak">深度学习架构</strong></h1><p id="4fed" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">深度学习模型需要大量的调整。当你手动调整你的深度学习模型时，这是非常耗时的。</p><p id="ef9b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">用于定义深度学习模型的超参数数量众多。</p><p id="9ebd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过反复手动更改网络中的参数，您可以有效地执行实际的网格搜索。</p><p id="c549" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以定义想要分析的每个超参数的分布，而不是手动检查不同的网络配置。然后有了很多样本，你就能找到一个更优的模型，你就能清楚地了解每个超参数对你的模型整体的影响。</p><p id="4e39" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种架构调整方法至少需要两个函数。首先，评估函数并存储最佳结果和函数以基于一组超参数生成深度学习模型的过程。</p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><h1 id="84d5" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">优化深度学习模型</h1><p id="3674" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">在这篇文章中，我将重点关注优化一个具有脱落层的神经网络的架构。我将在TensorFlow中使用Keras，因为它可以直接快速地生成不同的模型。</p><blockquote class="on oo op"><p id="50d8" class="kz la mk lb b lc ld ju le lf lg jx lh oq lj lk ll or ln lo lp os lr ls lt lu im bi translated">这里稍作停顿，向Keras和TensorFlow的负责人说声谢谢。如果你曾经从零开始创建过神经网络、CNN或LSTM，你就会知道这是极其乏味和复杂的。因此，我非常感激这些图书馆的存在，并向公众开放。</p></blockquote><p id="af37" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用从每个定义的分布中采样的超参数来生成模型。这些值控制层的数量、每个层上的节点数量、脱层概率、学习速率、优化器、优化方法的学习衰减以及激活函数。这个模型是为分类而建立的。</p><p id="681b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以将这些函数应用于不同的网络，例如卷积神经网络。然而，由于卷积层的性质，每个后续层都依赖于前一层。这一方面增加了模型创建功能的复杂性。CNN优化将可能是未来文章的重点。然而，CNN的一个简单解决方法是固定层数并优化其余参数。</p><p id="8202" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一步是导入一些包并初始化一些参数。在这里，我还初始化了我要测试的不同超参数的分布。您会注意到，我没有将所有的超参数都设置为分布，所以我不会测试所有可能的组合。</p><h2 id="1981" class="ot mt it bd mu ou ov dn my ow ox dp nc li oy oz ne lm pa pb ng lq pc pd ni pe bi translated">导入和数据加载</h2><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="de1c" class="ot mt it pg b gy pk pl l pm pn">import pandas as pd<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn.datasets import load_breast_cancer<br/>from sklearn.model_selection import train_test_split<br/>import tensorflow as tf<br/>from tensorflow.keras import backend as K<br/>from tensorflow.keras.models import Sequential<br/>from tensorflow.keras.layers import Dense, Dropout<br/>from tensorflow.keras.callbacks import TensorBoard<br/>from tensorflow.keras.optimizers import Adam, SGD<br/>from tensorflow.keras.models import load_model<br/>from tensorflow.keras.callbacks import EarlyStopping<br/>from tensorflow.keras.optimizers.schedules import ExponentialDecay<br/>import skopt<br/>from skopt.space import Real, Categorical, Integer<br/>from skopt import gp_minimize, forest_minimize<br/>from skopt.plots import plot_convergence, plot_objective, plot_evaluations<br/>from skopt.utils import use_named_args<br/>print( "Tensorflow version: ", tf.__version__)</span><span id="e3ef" class="ot mt it pg b gy po pl l pm pn">data = load_breast_cancer()<br/>y = data.target.astype(float)<br/>X = data.data<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=2)<br/>X_validation, X_test, y_validation, y_test = train_test_split(X_test, y_test, test_size=0.4, random_state=2)</span></pre><p id="c904" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，定义超参数的分布。这些分布可以是实连续值、整数值或分类值。此外，还为连续分布添加了分布类型。</p><p id="20af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，当您要测试的大部分值都落在对数均匀分布的子集中时，对数均匀分布可能更适合学习率衰减。超参数也需要默认值。这些默认值用于优化过程中构建的第一个模型。</p><h2 id="2206" class="ot mt it bd mu ou ov dn my ow ox dp nc li oy oz ne lm pa pb ng lq pc pd ni pe bi translated">超参数初始化</h2><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="dd57" class="ot mt it pg b gy pk pl l pm pn">dim_optimization = Categorical(categories=['adam', 'SGD'],<br/>name='optimization')<br/>dim_learning_rate = Real(low=1e-3, high=1e-1, prior='log-uniform',<br/>name='learning_rate')<br/>dim_learning_decay = Real(low=0.9, high=0.999, prior='uniform',<br/>name='learning_decay')<br/>dim_num_layers = Integer(low=1, high=5, name='num_layers')<br/>dim_num_dense_nodes = Integer(low=5, high=50, name='num_dense_nodes')<br/>dim_dropout_prob = Real(low=0.5, high=0.99, prior='uniform',<br/>name='dropout_prob')<br/>dim_activation = Categorical(categories=['sigmoid', 'softmax', 'relu'],name='activation')</span><span id="d6c0" class="ot mt it pg b gy po pl l pm pn">dimensions = [<br/>    dim_optimization, dim_learning_rate, dim_learning_decay,<br/>    dim_num_layers, dim_num_dense_nodes, dim_dropout_prob, dim_activation<br/>]</span><span id="f89d" class="ot mt it pg b gy po pl l pm pn">NUM_LAYERS = 1<br/>BATCH_SIZE = 128<br/>LEARNING_RATE = 0.001<br/>DECAY_STEPS = 1000<br/>DENSE_UNITS = 20<br/>DROPOUT_PROB = 0.8<br/>ACTIVATION_FUNC = 'relu'<br/>LOSS_FUNC = 'binary_crossentropy'<br/>METRIC = 'accuracy'<br/>LEARNING_DECAY = 0.9<br/>OTIMIZATION_FUNC = 'adam'<br/>ACQ_FUNC = 'EI' # Expected Improvement<br/>best_score = 0<br/>EPOCHS = 50<br/>BATCH_SIZE = 256<br/>N_CALLS = 1000</span><span id="a5a4" class="ot mt it pg b gy po pl l pm pn">default_parameters = [OTIMIZATION_FUNC, LEARNING_RATE, LEARNING_DECAY, NUM_LAYERS, DENSE_UNITS, DROPOUT_PROB, ACTIVATION_FUNC]</span></pre><p id="e716" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想改变底层模型，这个函数就需要改变。根据超参数输入，一些模型参数可能以其他参数为条件。例如，在CNN中，过滤器尺寸影响下一层的输出形状。目前skopt不支持条件特性。</p><p id="cb88" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于这个例子，我控制了每个密集层中的单元数量，然后添加了一个下降层。这个结构是一个相当简单的网络，但是这个代码可以很容易地修改，以包含您选择的不同模型。</p><p id="ad4a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下一个代码块基于来自样本的超参数生成深度神经网络。</p><h2 id="3f20" class="ot mt it bd mu ou ov dn my ow ox dp nc li oy oz ne lm pa pb ng lq pc pd ni pe bi translated">深度学习模型生成</h2><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="dc20" class="ot mt it pg b gy pk pl l pm pn">def create_model(optimization, learning_rate, learning_decay, num_layers, num_dense_nodes, dropout_prob, activation):<br/>    model = Sequential()<br/>    name = 'layer_{0}_dense_units'.format(0)<br/>    model.add( Dense( num_dense_nodes, input_dim=X_train.shape[1],<br/>        activation=activation, name=name))</span><span id="ab26" class="ot mt it pg b gy po pl l pm pn">    for i in range(num_layers-1):<br/>        name = 'layer_{0}_dense_units'.format(i+1) <br/>        model.add( Dense( num_dense_nodes, activation=activation, name=name))<br/>        model.add( Dropout(dropout_prob))</span><span id="2e6b" class="ot mt it pg b gy po pl l pm pn">    model.add( Dense(1, activation=activation))<br/>    lr_schedule = ExponentialDecay( initial_learning_rate=learning_rate,<br/>        decay_steps=DECAY_STEPS, decay_rate=learning_decay)<br/>    <br/>    if optimization == 'adam':<br/>        optimizer = Adam(learning_rate=lr_schedule)<br/>    if optimization == 'SGD':<br/>        optimizer = SGD(learning_rate=lr_schedule)<br/>    model.compile(<br/>        optimizer=optimizer, loss=LOSS_FUNC, metrics=[METRIC])<br/>    return model</span></pre><p id="6082" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">适应度是skopt将使用‘gp _ minimize’进行优化的函数，它将生成的高斯过程最小化以对超参数空间进行建模。该函数通过构造最小化。因此，最大化的分数变成了负分数的最小化。</p><h2 id="1da2" class="ot mt it bd mu ou ov dn my ow ox dp nc li oy oz ne lm pa pb ng lq pc pd ni pe bi translated">适合的功能</h2><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="9b45" class="ot mt it pg b gy pk pl l pm pn">@use_named_args(dimensions=dimensions)<br/>def fitness(optimization, learning_rate, learning_decay, num_layers, num_dense_nodes, dropout_prob, activation):<br/>    # Create the neural network<br/>    model = create_model( optimization=optimization,<br/>        learning_rate=learning_rate, learning_decay=learning_decay,<br/>        num_layers=num_layers, num_dense_nodes=num_dense_nodes,<br/>        dropout_prob=dropout_prob, activation=activation)<br/>    # Save log for tensorboard<br/>    callback_log = TensorBoard(<br/>        log_dir =<br/>"./21_logs/opt_{0}_lr_{1:.0e}_lr_decay_{2:.0e}_layers_{3}_nodes_{4}_dropout_{5}_activation_{5}/".format(<br/>        optimization, learning_rate, learning_decay, num_layers,         num_dense_nodes, dropout_prob, activation),<br/>        histogram_freq=0, write_graph=True, write_grads=False, write_images=False)</span><span id="3b6c" class="ot mt it pg b gy po pl l pm pn">    # Train the model.<br/>    history = model.fit(<br/>        x= X_train,y= y_train,<br/>        epochs=EPOCHS,batch_size=BATCH_SIZE,<br/>        validation_data=(X_validation, y_validation),<br/>        callbacks=[callback_log],verbose=0)<br/>    # Get the final model performance.<br/>    col = [x for x in list(history.history.keys()) if 'val_'+METRIC in x]<br/>    score = history.history[col[0]][-1]<br/>    print("--&gt; Validation {0}: {1:.2%}".format(METRIC, score))<br/>    global best_score<br/>    # Track scores and save best model<br/>    if score &gt; best_score:<br/>        model.save('Optimal-NN')<br/>    best_score = score<br/>    # Clear model to save space<br/>    del model<br/>    K.clear_session()<br/>    <br/>    # Skopt minimizes black-box functions, return the negative<br/>    return -score</span></pre><p id="847a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">skopt函数‘gp _ minimize’对黑盒模型运行贝叶斯优化，以确定最佳超参数配置。用于优化高斯过程的“acq_func”是优化的几个选项之一。</p><h2 id="379b" class="ot mt it bd mu ou ov dn my ow ox dp nc li oy oz ne lm pa pb ng lq pc pd ni pe bi translated">高斯过程最小化</h2><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="dbd8" class="ot mt it pg b gy pk pl l pm pn">search_result = gp_minimize(func=fitness,<br/>    dimensions=dimensions,<br/>    acq_func=ACQ_FUNC,<br/>    n_calls=N_CALLS,<br/>    x0=default_parameters)</span></pre></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><h1 id="855f" class="ms mt it bd mu mv mw mx my mz na nb nc jz nd ka ne kc nf kd ng kf nh kg ni nj bi translated">模型解释</h1><p id="b65d" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">搜索完成后，找到的最佳模型将保存在本地。可以从这一点加载和使用这个模型，如果需要，还可以进一步训练。</p><p id="c141" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">搜索结果包含每次评估的超参数以及每个测试模型的性能。</p><p id="026f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要查看搜索如何随着时间的推移提高模型性能，请运行以下代码块:</p><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="357f" class="ot mt it pg b gy pk pl l pm pn">plot_convergence(search_result)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/02ea4f8f458a08dac17e6bc85bb08d79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*fyOOt1GC51atCSfEogGf2w.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">优化期间最佳模型性能的收敛(作者提供照片)</p></figure><p id="f11c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，仅仅看到不同测试模型的改进并不是贝叶斯优化的最大好处。独立分析每个超参数的行为，以及它们如何与其他超参数相互作用。</p><pre class="kj kk kl km gt pf pg ph pi aw pj bi"><span id="a000" class="ot mt it pg b gy pk pl l pm pn">_ = plot_objective(result=search_result)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pq"><img src="../Images/46039f3f18f45da3b42c39243c77cfdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uTwHGE99xzSdMMt6BOCfIA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">贝叶斯优化的部分相关性和参数交互(作者提供图片)</p></figure><p id="2890" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上图中可以看出，一些参数仍然存在一些不确定性。改变密集节点数量的结果差异很大。然而，学习率似乎被有效地建模了。您可以看到优化选择了大约10e-2的学习速率。类似地，sigmoid激活和adam优化在所有生成的模型中表现最佳。</p><h1 id="041e" class="ms mt it bd mu mv pr mx my mz ps nb nc jz pt ka ne kc pu kd ng kf pv kg ni nj bi translated">结论</h1><p id="5b07" class="pw-post-body-paragraph kz la it lb b lc nk ju le lf nl jx lh li nm lk ll lm nn lo lp lq no ls lt lu im bi translated">当试图微调和优化计算量大的模型时，贝叶斯优化非常有用。此外，贝叶斯优化需要很少的样本，并迅速接近更优化的超参数配置，因此它是深度学习模型的理想选择。</p><p id="7a86" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于神经网络和网络变体等深度学习模型，微调架构至关重要。但是，不要浪费时间手动调优您的模型。相反，使用贝叶斯优化来确定每个超参数对模型的作用。</p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><p id="80ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你有兴趣阅读关于新颖的数据科学工具和理解机器学习算法的文章，可以考虑在Medium上关注我。</p><p id="4f22" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="mk">如果你对我的写作感兴趣，想直接支持我，请通过以下链接订阅。这个链接确保我会收到你的会员费的一部分。</em></p><div class="nq nr gp gr ns nt"><a href="https://zjwarnes.medium.com/membership" rel="noopener follow" target="_blank"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd iu gy z fp ny fr fs nz fu fw is bi translated">通过我的推荐链接加入Medium-Zachary Warnes</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">阅读扎卡里·沃恩斯(以及媒体上成千上万的其他作家)的每一个故事。您的会员费直接支持…</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">zjwarnes.medium.com</p></div></div><div class="oc l"><div class="pw l oe of og oc oh ks nt"/></div></div></a></div></div></div>    
</body>
</html>