<html>
<head>
<title>Bayesian Optimization with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Python 实现贝叶斯优化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bayesian-optimization-with-python-85c66df711ec?source=collection_archive---------1-----------------------#2021-12-25">https://towardsdatascience.com/bayesian-optimization-with-python-85c66df711ec?source=collection_archive---------1-----------------------#2021-12-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="bae4" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">优化评估成本高昂的黑盒函数</h2></div><p id="be11" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你在数据科学或机器学习领域，很可能你已经在做<a class="ae lb" href="https://en.wikipedia.org/wiki/Mathematical_optimization" rel="noopener ugc nofollow" target="_blank">优化</a>！例如，训练一个<a class="ae lb" href="https://en.wikipedia.org/wiki/Neural_network" rel="noopener ugc nofollow" target="_blank">神经网络</a>是一个优化问题，因为我们希望找到一组模型权重，使<a class="ae lb" href="https://en.wikipedia.org/wiki/Loss_function" rel="noopener ugc nofollow" target="_blank">损失函数</a>最小化。<a class="ae lb" href="https://en.wikipedia.org/wiki/Hyperparameter_optimization" rel="noopener ugc nofollow" target="_blank">找到产生最佳性能模型的超参数组</a>是另一个优化问题。</p><p id="85e3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">优化算法有多种形式，每一种都是为了解决特定类型的问题而创建的。特别是，学术界和工业界的科学家共同面临的一类问题是评估昂贵的<a class="ae lb" href="https://en.wikipedia.org/wiki/Black_box" rel="noopener ugc nofollow" target="_blank">黑盒函数</a>的优化。在今天的帖子中，我们将探讨如何用 Python 优化评估代价高昂的黑盒函数！</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/1a85d3d3737eef2b5a17586fe303daf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sG-LxxkiAGaWRC7nrgee6Q.jpeg"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">最优化问题是科学和工程中经常遇到的问题。Shane Rounce 在<a class="ae lb" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片。</p></figure><h1 id="e4eb" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">评估黑盒功能的成本很高</h1><p id="cee6" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">“评估昂贵的黑盒”意味着所涉及的功能或操作需要花费大量的金钱或资源来执行，并且其内部工作方式无法理解。一个评估代价昂贵的黑盒函数的好例子是优化深度神经网络的超参数。每个训练迭代可能需要几天的时间来完成，我们不能预先分析超参数值，这将导致最佳性能的模型。</p><p id="3e26" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当然，我们对每一个可能的超参数值进行交叉验证网格搜索，但是重复这么多训练迭代会导致计算成本激增！需要一种更有效的方法来使用最少的迭代次数找到最佳的超参数集。好在那种方法已经存在:<a class="ae lb" href="https://en.wikipedia.org/wiki/Bayesian_optimization" rel="noopener ugc nofollow" target="_blank">贝叶斯优化</a>！</p><h1 id="3e16" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">贝叶斯优化算法</h1><p id="e192" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated"><a class="ae lb" href="https://en.wikipedia.org/wiki/Bayesian_optimization" rel="noopener ugc nofollow" target="_blank">贝叶斯优化</a>是一种基于机器学习的优化算法，用于寻找参数，以全局优化给定的黑盒函数。该算法包含两个重要部分:</p><ol class=""><li id="7eb5" class="mp mq iq kh b ki kj kl km ko mr ks ms kw mt la mu mv mw mx bi translated">要优化的黑盒函数:<em class="my"> f </em> ( <em class="my"> x </em>)。我们要找到全局优化<em class="my"> f </em> ( <em class="my"> x </em>)的<em class="my"> x </em>的值。根据问题的不同，<em class="my"> f </em> ( <em class="my"> x </em>)有时也被称为目标函数、目标函数或损失函数。一般来说，我们只知道<em class="my"> f </em> ( <em class="my"> x </em>)的输入和输出。</li><li id="ff05" class="mp mq iq kh b ki mz kl na ko nb ks nc kw nd la mu mv mw mx bi translated">采集函数:<em class="my"> a </em> ( <em class="my"> x </em>，用于生成<em class="my"> x </em>的新值，以供<em class="my"> f </em> ( <em class="my"> x </em>)评估。<em class="my"> a </em> ( <em class="my"> x </em>)内部依靠一个<a class="ae lb" href="https://medium.com/@natsunoyuki/gaussian-process-models-7ebce1feb83d" rel="noopener">高斯过程模型</a> <em class="my"> m </em> ( <em class="my"> X </em>，<em class="my"> y </em>)生成新的<em class="my"> x </em>值。</li></ol><p id="8be3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">优化过程本身如下:</p><ol class=""><li id="6eb2" class="mp mq iq kh b ki kj kl km ko mr ks ms kw mt la mu mv mw mx bi translated">定义黑盒函数<em class="my"> f </em> ( <em class="my"> x </em>)、采集函数<em class="my"> a </em> ( <em class="my"> x </em>)以及参数<em class="my"> x </em>的搜索空间。</li><li id="512a" class="mp mq iq kh b ki mz kl na ko nb ks nc kw nd la mu mv mw mx bi translated">随机产生 x 的一些初始值<em class="my">，从<em class="my"> f </em> ( <em class="my"> x </em>)测量相应的输出。</em></li><li id="7211" class="mp mq iq kh b ki mz kl na ko nb ks nc kw nd la mu mv mw mx bi translated">将一个<a class="ae lb" href="https://medium.com/@natsunoyuki/gaussian-process-models-7ebce1feb83d" rel="noopener">高斯过程模型</a> <em class="my"> m </em> ( <em class="my"> X </em>，<em class="my"> y </em>)装配到<em class="my"> X </em> = <em class="my"> x </em>和<em class="my">y</em>=<em class="my">f</em>(<em class="my">X</em>)上。换句话说，<em class="my"> m </em> ( <em class="my"> X </em>，<em class="my"> y </em>)作为<em class="my"> f </em> ( <em class="my"> x </em>)的代理模型！</li><li id="664f" class="mp mq iq kh b ki mz kl na ko nb ks nc kw nd la mu mv mw mx bi translated">采集函数<em class="my"> a </em> ( <em class="my"> x </em>)然后使用<em class="my"> m </em> ( <em class="my"> X </em>，<em class="my"> y </em>)生成新的<em class="my"> x </em>值，如下所示。用<em class="my"> m </em> ( <em class="my"> X </em>，<em class="my"> y </em>)预测<em class="my"/>f(<em class="my">X</em>)如何随<em class="my"> x </em>变化。导致<em class="my"> m </em> ( <em class="my"> X </em>，<em class="my"> y </em>)中预测值最大的<em class="my"> x </em>的值，则建议作为<em class="my"> x </em>的下一个样本，用<em class="my"> f </em> ( <em class="my"> x </em>)进行评估。</li><li id="337f" class="mp mq iq kh b ki mz kl na ko nb ks nc kw nd la mu mv mw mx bi translated">重复步骤 3 和 4 中的优化过程，直到我们最终得到一个导致全局最优值<em class="my"> f </em> ( <em class="my"> x </em>)的值<em class="my"> x </em>。请注意，<em class="my"> x </em>和<em class="my"> f </em> ( <em class="my"> x </em>)的所有历史值都应该用于在下一次迭代中训练高斯过程模型<em class="my"> m </em> ( <em class="my"> X </em>，<em class="my">y</em>)——随着数据点数量的增加，<em class="my"> m </em> ( <em class="my"> X </em>，<em class="my"> y </em>)在预测最优值方面变得更好</li></ol><h1 id="776a" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">用<code class="fe ne nf ng nh b">bayes_opt</code>库进行贝叶斯优化</h1><p id="0a22" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">作为演示的一部分，我们使用<code class="fe ne nf ng nh b"><a class="ae lb" href="https://github.com/fmfn/BayesianOptimization" rel="noopener ugc nofollow" target="_blank">bayes_opt</a></code>库来执行对基于<code class="fe ne nf ng nh b">sklearn</code>乳腺癌数据训练的<code class="fe ne nf ng nh b">SVC</code>模型的超参数<code class="fe ne nf ng nh b">C</code>的搜索。</p><p id="85d2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">优化器的组件包括:</p><ol class=""><li id="ef71" class="mp mq iq kh b ki kj kl km ko mr ks ms kw mt la mu mv mw mx bi translated">黑盒函数<em class="my"> f </em> ( <em class="my"> x </em>)是我们希望最大化的 ROC AUC 分数，以便获得最佳表现模型。</li><li id="3b58" class="mp mq iq kh b ki mz kl na ko nb ks nc kw nd la mu mv mw mx bi translated">所使用的采集函数<em class="my"> a </em> ( <em class="my"> x </em>)是置信上限(<code class="fe ne nf ng nh b">"ucb"</code>)函数，其形式为:<code class="fe ne nf ng nh b">a = mean + kappa * std</code>。<code class="fe ne nf ng nh b">mean</code>和<code class="fe ne nf ng nh b">std</code>都是高斯过程模型<em class="my"> m </em> ( <em class="my"> X </em>，<em class="my"> y </em>)的输出。<code class="fe ne nf ng nh b">kappa</code>是优化器的一个超级参数，用于平衡对<em class="my"> x </em>搜索的探索和利用。对于<code class="fe ne nf ng nh b">kappa</code>的小值，<code class="fe ne nf ng nh b">std</code>没有<code class="fe ne nf ng nh b">mean</code>和<em class="my">那么强调，a </em> ( <em class="my"> x </em>)侧重于在发现的局部极小值周围搜索。对于<code class="fe ne nf ng nh b">kappa</code>的大值，<code class="fe ne nf ng nh b">std</code>更为重要，<em class="my"> a </em> ( <em class="my"> x </em>)侧重于在<em class="my"> x </em>的未搜索区域进行搜索。</li></ol><p id="3fab" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">执行上述优化步骤的“现成”Python 代码如下。</p><pre class="ld le lf lg gt ni nh nj nk aw nl bi"><span id="8376" class="nm lt iq nh b gy nn no l np nq">import numpy as np<br/>import matplotlib.pyplot as plt<br/>from sklearn.svm import SVC<br/>from sklearn.datasets import load_breast_cancer<br/>from sklearn.preprocessing import MinMaxScaler<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import roc_auc_score<br/>from bayes_opt import BayesianOptimization, UtilityFunction<br/>import warnings<br/>warnings.filterwarnings("ignore")</span><span id="8dcc" class="nm lt iq nh b gy nr no l np nq"><br/># Prepare the data.<br/>cancer = load_breast_cancer()<br/>X = cancer["data"]<br/>y = cancer["target"]</span><span id="0e70" class="nm lt iq nh b gy nr no l np nq">X_train, X_test, y_train, y_test = train_test_split(X, y,<br/>                                            stratify = y,<br/>                                        random_state = 42)</span><span id="600f" class="nm lt iq nh b gy nr no l np nq">scaler = MinMaxScaler()<br/>X_train_scaled = scaler.fit_transform(X_train)<br/>X_test_scaled = scaler.transform(X_test)</span><span id="ffbf" class="nm lt iq nh b gy nr no l np nq"># Define the black box function to optimize.<br/>def black_box_function(C):<br/>    # C: SVC hyper parameter to optimize for.<br/>    model = SVC(C = C)<br/>    model.fit(X_train_scaled, y_train)<br/>    y_score = model.decision_function(X_test_scaled)<br/>    f = roc_auc_score(y_test, y_score)<br/>    return f</span><span id="e73e" class="nm lt iq nh b gy nr no l np nq"># Set range of C to optimize for.<br/># bayes_opt requires this to be a dictionary.<br/>pbounds = {"C": [0.1, 10]}</span><span id="8d76" class="nm lt iq nh b gy nr no l np nq"># Create a BayesianOptimization optimizer,<br/># and optimize the given black_box_function.<br/>optimizer = BayesianOptimization(f = black_box_function,<br/>                                 pbounds = pbounds, verbose = 2,<br/>                                 random_state = 4)</span><span id="3daa" class="nm lt iq nh b gy nr no l np nq">optimizer.maximize(init_points = 5, n_iter = 10)</span><span id="4cd2" class="nm lt iq nh b gy nr no l np nq">print("Best result: {}; f(x) = {}.".format(optimizer.max["params"], optimizer.max["target"]))</span></pre><p id="cca0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">运行上面的 Python 代码会打印出以下输出:</p><pre class="ld le lf lg gt ni nh nj nk aw nl bi"><span id="c67a" class="nm lt iq nh b gy nn no l np nq">|   iter    |  target   |     C     | ------------------------------------- <br/>|  1        |  0.9979   |  9.674    | <br/>|  2        |  0.9975   |  5.518    | <br/>|  3        |  0.9979   |  9.73     | <br/>|  4        |  0.9979   |  7.177    | <br/>|  5        |  0.9979   |  7.008    | <br/>|  6        |  0.9914   |  0.1023   | <br/>|  7        |  0.9981   |  8.505    | <br/>|  8        |  0.9981   |  8.15     | <br/>|  9        |  0.9981   |  8.327    | <br/>|  10       |  0.9981   |  8.8      | <br/>|  11       |  0.9981   |  8.67     | <br/>|  12       |  0.9981   |  7.974    | <br/>|  13       |  0.9979   |  6.273    | <br/>|  14       |  0.9981   |  8.064    | <br/>|  15       |  0.9981   |  8.911    | ===================================== </span><span id="145e" class="nm lt iq nh b gy nr no l np nq">Best result: {'C': 8.505474666113539}; f(x) = 0.9981132075471698.</span></pre><p id="92c3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从上面的结果来看，优化器设法确定使用超级参数值<code class="fe ne nf ng nh b">C = 8.505</code>会产生性能最佳的模型！</p><h1 id="8e65" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">对优化过程的更多控制</h1><p id="74a5" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">您可能已经意识到优化器将搜索参数作为连续变量输出。如果参数必须是离散的，这将导致一个问题。作为一个例子，让我们假设我们也想搜索模型<code class="fe ne nf ng nh b">SVC</code>的最佳<code class="fe ne nf ng nh b">degree</code>值，然而<code class="fe ne nf ng nh b">degree</code>必须是一个整数。在这种情况下，我们需要对优化过程进行更多的控制。</p><p id="edc0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，我们没有指定上面采集函数<em class="my"> a </em> ( <em class="my"> x </em>)的超参数<code class="fe ne nf ng nh b">kappa</code>，也没有指定使用哪种类型的采集函数。一般来说，缺省设置在大多数情况下应该是有效的，但是在某些情况下，对优化器有更多的控制会更好。</p><p id="c34b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">幸运的是，<code class="fe ne nf ng nh b">bayes_opt</code>并没有使用上面显示的简单工作流程，而是提供了一个更加可控的优化过程。在这种情况下，我们必须在一个<code class="fe ne nf ng nh b">for</code>循环中手动执行每个优化步骤。在这个<code class="fe ne nf ng nh b">for</code>循环中，如果需要，我们可以添加额外的代码来执行其他计算，比如强制搜索参数是离散的。</p><pre class="ld le lf lg gt ni nh nj nk aw nl bi"><span id="01e4" class="nm lt iq nh b gy nn no l np nq"># Create the optimizer. The black box function to optimize is not<br/># specified here, as we will call that function directly later on.<br/>optimizer = BayesianOptimization(f = None, <br/>                                 pbounds = {"C": [0.01, 10], <br/>                                            "degree": [1, 5]}, <br/>                                 verbose = 2, random_state = 1234)</span><span id="2b1f" class="nm lt iq nh b gy nr no l np nq"># Specify the acquisition function (bayes_opt uses the term<br/># utility function) to be the upper confidence bounds "ucb".<br/># We set kappa = 1.96 to balance exploration vs exploitation.<br/># xi = 0.01 is another hyper parameter which is required in the<br/># arguments, but is not used by "ucb". Other acquisition functions<br/># such as the expected improvement "ei" will be affected by xi.<br/>utility = UtilityFunction(kind = "ucb", kappa = 1.96, xi = 0.01)</span><span id="2171" class="nm lt iq nh b gy nr no l np nq"># We want to optimize both C and degree simultaneously.<br/>def black_box_function(C, degree):<br/>    model = SVC(C = C, degree = degree)<br/>    model.fit(X_train_scaled, y_train)<br/>    y_score = model.decision_function(X_test_scaled)<br/>    f = roc_auc_score(y_test, y_score)<br/>    return f</span><span id="c96a" class="nm lt iq nh b gy nr no l np nq"># Optimization for loop.<br/>for i in range(25):<br/>    # Get optimizer to suggest new parameter values to try using the<br/>    # specified acquisition function.<br/>    next_point = optimizer.suggest(utility)</span><span id="8d83" class="nm lt iq nh b gy nr no l np nq">    # Force degree from float to int.<br/>    next_point["degree"] = int(next_point["degree"])</span><span id="a16f" class="nm lt iq nh b gy nr no l np nq">    # Evaluate the output of the black_box_function using <br/>    # the new parameter values.<br/>    target = black_box_function(**next_point)</span><span id="4596" class="nm lt iq nh b gy nr no l np nq">    try:<br/>        # Update the optimizer with the evaluation results. <br/>        # This should be in try-except to catch any errors!<br/>        optimizer.register(params = next_point, target = target)<br/>    except:<br/>        pass</span><span id="ae1d" class="nm lt iq nh b gy nr no l np nq">print("Best result: {}; f(x) = {:.3f}.".format(optimizer.max["params"], optimizer.max["target"]))</span><span id="bdf9" class="nm lt iq nh b gy nr no l np nq">plt.figure(figsize = (15, 5))<br/>plt.plot(range(1, 1 + len(optimizer.space.target)), optimizer.space.target, "-o")<br/>plt.grid(True)<br/>plt.xlabel("Iteration", fontsize = 14)<br/>plt.ylabel("Black box function f(x)", fontsize = 14)<br/>plt.xticks(fontsize = 14)<br/>plt.yticks(fontsize = 14)<br/>plt.show()</span></pre><p id="3f2d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此代码导致输出:<br/> <code class="fe ne nf ng nh b">Best result: {‘C’: 9.984215837074222, ‘degree’: 4.0}; f(x) = 0.998.</code></p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi ns"><img src="../Images/b6f5f7707ac094a07a50f7e72e66b737.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yvy1LbbUnR3eP3_LHnPp-w.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">超过 25 次迭代的 SVC 模型的 C 和度的贝叶斯优化。</p></figure><p id="fa6c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">根据上面的结果，优化器设法确定使用超级参数值<code class="fe ne nf ng nh b">C = 9.984</code>和<code class="fe ne nf ng nh b">degree = 4</code>会产生性能最佳的<code class="fe ne nf ng nh b">SVC</code>模型！</p><h1 id="706e" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">摘要</h1><p id="6e89" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">今天，我们探讨了贝叶斯优化的工作原理，并使用贝叶斯优化器来优化机器学习模型的超参数。对于小数据集或简单模型，与执行网格搜索相比，超参数搜索的速度可能并不显著。然而，对于非常大的数据集或深度神经网络，测试网格中的每个样本在经济上可能变得不可行，使用贝叶斯优化将提高超参数搜索过程的效率！</p><p id="64ef" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在以后的帖子中，我将演示如何使用贝叶斯优化来优化激光器的功率输出！在此之前，感谢您的阅读！</p><h1 id="ab3b" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">参考</h1><p id="856d" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">[1] C. M. Bishop (2006)，<a class="ae lb" href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf" rel="noopener ugc nofollow" target="_blank"><em class="my"/></a>【斯普林格】。【https://github.com/fmfn/BayesianOptimization】<a class="ae lb" href="https://github.com/fmfn/BayesianOptimization" rel="noopener ugc nofollow" target="_blank"><br/></a>。<br/>【3】<a class="ae lb" href="https://scikit-optimize.github.io/stable/" rel="noopener ugc nofollow" target="_blank">https://scikit-optimize.github.io/stable/</a>。</p></div></div>    
</body>
</html>