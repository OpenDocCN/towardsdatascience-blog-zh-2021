<html>
<head>
<title>Making MLOps easy for End-Users</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">让最终用户轻松使用MLOps</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/making-mlops-easy-for-end-users-a3c22491e5e0?source=collection_archive---------20-----------------------#2021-06-21">https://towardsdatascience.com/making-mlops-easy-for-end-users-a3c22491e5e0?source=collection_archive---------20-----------------------#2021-06-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c47c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用开源工具简化MLOps的教程</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/de51c3d928e556e09ade60d35c490368.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jmBnEExhWVBzqR4sh0e1_A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由pch.vector &amp; pikisuperstar创建的人物向量—<a class="ae ky" href="https://www.freepik.com/free-photos-vectors/people" rel="noopener ugc nofollow" target="_blank">www.freepik.com</a></p></figure><p id="372c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当人们说“MLOps”时，很难弄清楚T2指的是什么。即使对于一个技术人员来说，弄清楚如何正确地<em class="lv">做</em> MLOps，可能更加困难。那么，对于一个对web技术、Kubernetes、监控、云基础设施等一无所知的公民数据科学家来说，做MLOps会有多难呢？？在这里，我将继续探索如何为此目的建立一个开源的MLOps框架:具体来说，我将概述并展示Databricks、mlflow和BentoML的组合如何为最终用户提供一个引人注目、可扩展且易于使用的MLOps工作流。</p><p id="62ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我之前已经讨论过MLOps框架必须包含电池并支持广泛的功能列表；模型生命周期管理和治理、监控、A/B测试、可解释性、漂移/异常值检测，等等。但是，作为最终用户:</p><blockquote class="lw lx ly"><p id="2fff" class="kz la lv lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">我只想定义一个给定的python接口，按下一个大的绿色按钮，并获得一个REST端点URL，在那里我可以与我部署的模型进行交互。</p></blockquote><p id="7b5b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这一系列博客文章的第一部分中，我探讨了Databricks如何与Seldon-core相结合来检查我所看到的部署和运行MLOps的大部分需求；然而，Seldon-core的开源产品对最终用户来说相当麻烦，而且不符合简单性的愿景；写一个python类，按下按钮，得到REST端点。这篇文章是我上一篇文章的延伸，所以我推荐看看:</p><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/get-started-with-mlops-fd7062cab018"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">MLOps入门</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">包含开源工具的综合MLOps教程</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="mp l mq mr ms mo mt ks mf"/></div></div></a></div></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h2 id="a86f" class="nb nc it bd nd ne nf dn ng nh ni dp nj li nk nl nm lm nn no np lq nq nr ns nt bi translated">我们在看什么工具</h2><p id="8517" class="pw-post-body-paragraph kz la it lb b lc nu ju le lf nv jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">Databricks涵盖了ML模型生命周期管理的从实验、跟踪、版本化、注册和治理的所有内容，同时是按需付费的，没有前期成本，对普通数据科学家来说入门的障碍最小。在这篇文章中，我们将关注BentoML将帮助我们部署任何注册到mlflow模型注册表中的REST端点的下一步——我们将不得不做一些黑客工作来让这些工具很好地配合使用😏。再次检查<a class="ae ky" rel="noopener" target="_blank" href="/get-started-with-mlops-fd7062cab018">第一部分</a>，了解数据块中MLOps的详细信息。</p><h1 id="b241" class="nz nc it bd nd oa ob oc ng od oe of nj jz og ka nm kc oh kd np kf oi kg ns oj bi translated">步骤1:最终用户看到的内容</h1><p id="2b46" class="pw-post-body-paragraph kz la it lb b lc nu ju le lf nv jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">从第一篇博文开始，我们可以说我们已经训练了:</p><ul class=""><li id="150f" class="ok ol it lb b lc ld lf lg li om lm on lq oo lu op oq or os bi translated">MNIST分类的标准Keras模式</li><li id="f664" class="ok ol it lb b lc ot lf ou li ov lm ow lq ox lu op oq or os bi translated">一种检测输入数据中特征漂移的算法</li><li id="51ca" class="ok ol it lb b lc ot lf ou li ov lm ow lq ox lu op oq or os bi translated">一种检测输入数据中异常值的算法</li><li id="0887" class="ok ol it lb b lc ot lf ou li ov lm ow lq ox lu op oq or os bi translated">解释我们预测的综合梯度算法。</li></ul><p id="c6dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">大多数数据科学家至少会熟悉以上几点中的一点。让我们将所有这些模型保存到本地磁盘的<code class="fe oy oz pa pb b">artifacts/mnist-keras</code>文件夹中:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pc pd l"/></div></figure><p id="6e25" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们希望将所有这些模型放入一个简单的python类中，并将其注册到mlflow模型注册表中。本质上我们希望这个类做所有的事情；返回预测、返回解释、定义要监控的指标等。因此，我们告诉最终用户将模型放入如下界面:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pc pd l"/></div></figure><p id="7a22" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里有几点需要注意:</p><ul class=""><li id="b42a" class="ok ol it lb b lc ld lf lg li om lm on lq oo lu op oq or os bi translated">该类继承自<code class="fe oy oz pa pb b">mlflow.pyfunc.PythonModel</code>，因此可以登录到mlflow模型注册中心，在那里我们可以控制治理，拥有多个版本，将版本转换到不同的阶段，等等。</li><li id="c3f0" class="ok ol it lb b lc ot lf ou li ov lm ow lq ox lu op oq or os bi translated"><code class="fe oy oz pa pb b">predict</code>、<code class="fe oy oz pa pb b">explain</code>和<code class="fe oy oz pa pb b">reward</code>方法用BentoML端点定义修饰，以表明这些是我们希望公开的API端点。当然，可以添加额外的端点。注意mlflow不知道这些BentoML装饰器是做什么的。</li><li id="8994" class="ok ol it lb b lc ot lf ou li ov lm ow lq ox lu op oq or os bi translated">方法<code class="fe oy oz pa pb b">prepare_input</code>定义了如何处理输入数据；这是必要的，因为BentoML支持微批处理；也就是说，如果一百万个单独的请求同时到来，那么为了提高效率，它会创建批处理来同时通过模型。</li><li id="c8e5" class="ok ol it lb b lc ot lf ou li ov lm ow lq ox lu op oq or os bi translated">每次调用<code class="fe oy oz pa pb b">predict</code>时，该方法都会自动计算并记录这些样本的漂移和异常值结果到<a class="ae ky" href="https://github.com/prometheus/client_python" rel="noopener ugc nofollow" target="_blank"> Prometheus metrics </a>中——当然，如果我们可以异步计算并记录这些结果，那将是理想的，但是在大多数用例中，这些计算并不昂贵，并且我们通过将它们保留在<code class="fe oy oz pa pb b">predict</code>方法中，赢得了很多简单性。</li></ul><p id="4f30" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">既然我们已经定义了模型，我们希望将它记录到mlflow模型注册表中，与保存所有模型的本地目录(<code class="fe oy oz pa pb b">artifacts/mnist-keras</code>)和描述python环境&amp;包的字典放在一起。mlflow UI可以帮助完成部分工作，但我们也可以完全用代码来完成:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pc pd l"/></div></figure><p id="aa1e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就这样，现在我们已经将与MNIST模型相关的所有内容(预测器、解释器、漂移和异常值检测器)放入一个mlflow模型中，并将该模型注册到我们的注册表中，并将其过渡到“生产”阶段。</p><p id="a3f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是还没有部署任何东西。因此，在下一节中，我们将看看是否可以使用BentoML在mlflow的“生产”阶段自动部署所有内容的一些代码原型。</p><h1 id="22ee" class="nz nc it bd nd oa ob oc ng od oe of nj jz og ka nm kc oh kd np kf oi kg ns oj bi translated">第二步:幕后</h1><p id="06b6" class="pw-post-body-paragraph kz la it lb b lc nu ju le lf nv jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">在幕后，我们希望运行一个服务，该服务不断检查新模型是否已经进入模型注册中心的“生产”阶段，并确保所有这些模型都被实际部署。为了执行部署，我们将使用BentoML，它允许我们包装mlflow模型对象，并将它们封装到docker映像中。因此，本节中的所有内容应该是完全通用的，并且发生在后端，没有任何用户交互。</p><h2 id="2627" class="nb nc it bd nd ne nf dn ng nh ni dp nj li nk nl nm lm nn no np lq nq nr ns nt bi translated">步骤2.1:用BentoML包装mlflow</h2><p id="f00a" class="pw-post-body-paragraph kz la it lb b lc nu ju le lf nv jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">首先，我们必须为BentoML定义一个“工件”,在本例中，这个工件就是我们的mlflow模型。假设我们已经从mlflow模型注册中心下载了一个模型，我们可以创建一个工件，让BentoML将本地目录的内容复制到BentoML容器中(<code class="fe oy oz pa pb b">save</code>)，并指示BentoML如何加载模型(<code class="fe oy oz pa pb b">load</code>):</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pc pd l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用于加载mlflow模型工件的BentoML工件。作者代码。</p></figure><p id="a7d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们有了一个工件，接下来，我们需要定义一个BentoML“服务”，它将我们在mlflow模型上公开的所有端点也在BentoML上公开。为此，我们侵入<code class="fe oy oz pa pb b">_config_artifacts</code>方法，该方法在BentoML服务的每个实例化时被调用，然后在BentoML服务上动态添加来自mlflow模型的任何API方法。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pc pd l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用于将mlflow端点公开为BentoML端点的BentoML服务。作者代码。</p></figure><p id="8d1a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本课程的注意事项:</p><ul class=""><li id="5c5f" class="ok ol it lb b lc ld lf lg li om lm on lq oo lu op oq or os bi translated">它有一个装饰器，详细说明了它如何依赖于一个<code class="fe oy oz pa pb b">MlflowArtifact</code></li><li id="aaa2" class="ok ol it lb b lc ot lf ou li ov lm ow lq ox lu op oq or os bi translated">它也依靠一个<code class="fe oy oz pa pb b">environment.yml</code>来描述康达环境</li><li id="8be8" class="ok ol it lb b lc ot lf ou li ov lm ow lq ox lu op oq or os bi translated">它调用<code class="fe oy oz pa pb b">_config_inference_apis</code>将mlflow方法添加到API中</li></ul><p id="2fc3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这样，我们现在可以创建一个简单的函数，该函数获取模型注册中心中的任何mlflow模型，下载它，并将其打包到BentoML服务中，即类似于以下内容的东西:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pc pd l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">从模型注册表下载mlflow模型</p></figure><p id="8e9c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这将在本地保存一个<code class="fe oy oz pa pb b">MlflowBentoService:latest</code> BentoML服务。然后，我们可以运行以下命令，将BentoML服务放入docker映像中:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pc pd l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">将BentoML服务放入docker映像的Bash命令。作者代码。</p></figure><p id="62c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在有了一个<code class="fe oy oz pa pb b">bentoml-mlflow-mnist-keras</code> docker映像，它包含了我们的模型，并准备好进行部署。我们可以通过在本地运行docker映像来测试它是否工作正常:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pc pd l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用于本地测试BentoML docker映像的Bash命令。</p></figure><p id="f348" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">转到<code class="fe oy oz pa pb b">localhost:5000</code>,我们现在应该会看到一个swagger API，它暴露了我们所有的端点，准备好接受请求。目前为止一切正常。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/391f2463346529d076a15dbe3f08eab6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AoK3DEZT5hULcMET2tIc9A.png"/></div></div></figure><h2 id="ba17" class="nb nc it bd nd ne nf dn ng nh ni dp nj li nk nl nm lm nn no np lq nq nr ns nt bi translated">步骤2.2:自动部署</h2><p id="39ba" class="pw-post-body-paragraph kz la it lb b lc nu ju le lf nv jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">下一步是将注册中心中注册的任何mlflow模型自动部署到一些云基础设施中。BentoML允许您部署在许多不同的基础设施上，但是我们将假设我们已经建立了一个Kubernetes集群；参见<a class="ae ky" rel="noopener" target="_blank" href="/get-started-with-mlops-fd7062cab018">第一篇博文</a>中的例子。</p><p id="9be5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本质上，我们需要一个服务来保持mlflow模型注册与部署到Kubernetes的同步。我们可以把它想象成:</p><ul class=""><li id="1c8b" class="ok ol it lb b lc ld lf lg li om lm on lq oo lu op oq or os bi translated">每隔<em class="lv"> X </em>分钟运行一次API请求以在mlflow注册表中列出模型并部署它们的服务。</li><li id="f445" class="ok ol it lb b lc ot lf ou li ov lm ow lq ox lu op oq or os bi translated">使用Databricks模型注册中心(目前为私有预览版)中的CI/CD webhooks，在注册模型后立即部署模型。</li></ul><p id="7826" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦在注册表中找到一个新模型，就需要进行以下操作:</p><ul class=""><li id="09a7" class="ok ol it lb b lc ld lf lg li om lm on lq oo lu op oq or os bi translated">下载mlflow模型，并将其打包成一个BentoML docker映像，类似于上一节所示。</li><li id="d85a" class="ok ol it lb b lc ot lf ou li ov lm ow lq ox lu op oq or os bi translated">将docker映像推送到存储库(例如，DockerHub、ACR等)。)</li><li id="43b8" class="ok ol it lb b lc ot lf ou li ov lm ow lq ox lu op oq or os bi translated">创建一个Kubernetes部署<code class="fe oy oz pa pb b">yaml</code>并应用</li></ul><p id="0bd9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种实现的细节取决于我们将部署到的特定基础设施。尽管如此，我们仍然可以快速编写一个过于简化的python函数示例，该函数获取Kubernetes部署的模板，填写模型细节，然后应用部署。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pc pd l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用于读取Kubernetes模板文件的过于简化的python方法&amp;应用它。作者代码。</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="pc pd l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用于部署BentoML映像的过于简化的Kubernetes模板。作者代码</p></figure><p id="2ee9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">适当的实现还可以添加一个特定的入口路由，这样模型就可以在给定的自定义域路由(例如<code class="fe oy oz pa pb b">www.myexample.com/model-name/</code>)上提供Swagger UI API</p><h2 id="b8e0" class="nb nc it bd nd ne nf dn ng nh ni dp nj li nk nl nm lm nn no np lq nq nr ns nt bi translated">结果</h2><p id="b022" class="pw-post-body-paragraph kz la it lb b lc nu ju le lf nv jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated"><a class="ae ky" href="https://docs.bentoml.org/en/latest/guides/monitoring.html" rel="noopener ugc nofollow" target="_blank"> BentoML文档</a>展示了我们如何将Prometheus安装到我们的集群中，并为我们所有部署的模型自动抓取<code class="fe oy oz pa pb b">metrics/</code>端点。基于此，如果我们还在集群上安装了Grafana，我们可以让我们的自动部署创建如下所示的控制面板:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pf"><img src="../Images/7b0f0b5a3d21c1a9079c1f3457c0adc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jeDgvcqC95m2ZC5vlLe7vg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Grafana dashboard，最终用户可以对其进行定制，以包含我们记录的任何指标。作者截图</p></figure><h2 id="318d" class="nb nc it bd nd ne nf dn ng nh ni dp nj li nk nl nm lm nn no np lq nq nr ns nt bi translated">最后的想法</h2><p id="70dd" class="pw-post-body-paragraph kz la it lb b lc nu ju le lf nv jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">在这篇文章中，我们使用mlflow和BentoML拼凑了一个简单的python类。该类提供了实现定制模型的充分灵活性，定制模型具有要监控的漂移和异常值的定制度量，以及用于解释或奖励模型预测的定制端点。这个类的美妙之处在于，不管模型细节如何，我们都可以将它注册到mlflow模型注册表中，然后创建一个服务，自动将其部署到生产中。</p><p id="0587" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">概述的解决方案很容易扩展到包括附加功能。例如，我们可以在mlflow模型上创建额外的“配置”选项，这些选项将确定使用哪个基础docker映像进行部署、在哪个基础架构上进行部署(在多个集群的情况下)、向数据块发送重新训练请求(例如，当漂移或回报降低到阈值以下时)，或者甚至如何部署A/B测试。</p><p id="accb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个设置中必须实现的一个重要组件是从mlflow到Kubernetes集群的权限同步。也就是说，只有对mlflow注册表中的模型具有读取权限的用户(可以通过UI来控制)才应该具有调用最终端点的权限。</p><p id="debf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">像这样的框架是引人注目的。它允许数据科学家专注于模型开发，然后通过监控、治理等快速将模型投入生产。，在此过程中无需咨询数据/云工程师。</p></div></div>    
</body>
</html>