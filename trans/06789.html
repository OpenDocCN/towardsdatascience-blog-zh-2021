<html>
<head>
<title>Q-Learning and SARSA, with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Q-Learning和SARSA，使用Python</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/q-learning-and-sasar-with-python-3775f86bd178?source=collection_archive---------3-----------------------#2021-06-19">https://towardsdatascience.com/q-learning-and-sasar-with-python-3775f86bd178?source=collection_archive---------3-----------------------#2021-06-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0a78" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">解释无模型RL算法的基础:Q-Learning和SARSA(带代码！)</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a6d0f66106383bd589f6979d79147df7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*BODLYZQ1YLg2xHUt"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">法比奥·巴拉西纳在<a class="ae ky" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="cca2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">强化学习(RL)是机器学习中的一种学习范式，它通过与环境交互来学习将状态映射到动作的最优策略，以实现目标。在本文中，我将介绍两种最常用的RL算法:Q-Learning和SARSA。</p><p id="dbea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">类似于蒙特卡罗算法(MC)，Q-Learning和SARSA算法也是<strong class="lb iu">无模型RL算法</strong>，即<strong class="lb iu">不使用与马尔可夫决策过程(MDP)相关的转移概率分布</strong>。相反，他们从经验中学习最优政策。MC与Q-Learning或SARSA算法的主要区别在于<strong class="lb iu"> MC需要采样整个轨迹</strong>来学习价值函数，寻找最优策略。然而，对于一些问题来说，获得一个完整的轨迹可能是耗时的。因此，如果算法能够在每个动作之后更新策略，而不是在获得整个轨迹之后，这可能是好的。</p><p id="60a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在Q-Learning和SARSA中，我们只需要<strong class="lb iu">一步轨迹</strong> (𝑠,𝑎,𝑟,𝑠')而不是整个轨迹。此外，从这两个算法中，我们还可以突出显示出<strong class="lb iu">符合策略的</strong>和<strong class="lb iu">不符合策略的</strong>学习之间的差异，我将在本文稍后讨论这一点。</p><h1 id="52c6" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">q学习</h1><p id="9103" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">Q-Learning中的更新规则如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/cfb1d1476f6cfe78782bbd0bd7c60ff0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i0Go3BmlLAUC1dmqycIYnw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="b4d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">新样本和旧估计之间的差异用于更新旧估计。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/a91c5bfc48d3bff5339368e3ad9f5fdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*AligqHeKpZQfeks7KErqIA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1:Q-Learning——一种非策略学习算法[1]</p></figure><h2 id="0e61" class="mu lw it bd lx mv mw dn mb mx my dp mf li mz na mh lm nb nc mj lq nd ne ml nf bi translated">一步一步的例子</h2><p id="12b1" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">假设一个MDP的6室环境。我们将每个房间编号为0到6，房间由门/箭头连接，如下图所示。目标是让代理移动到房间5。注意，只有2号房、4号房、6号房可以通往5号房(目的地)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/b32fdaba39ff1bb78e06b8afbb03ad47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*G_m6Du6GWL9_KAvyCEVKIw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2: MDP 6室环境。作者图片</p></figure><p id="0951" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">目标</strong>:把一个代理放在任何一个房间里，从那个房间到5号房间。<br/> <strong class="lb iu">奖励</strong>:直接通往目标的门有100的即时奖励。其他不直接与目标房间相连的门奖励为0。</p><p id="a699" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本教程将通过简单易懂的例子介绍Q-learning的概念性知识。它描述了智能体如何通过强化学习方法从未知环境中学习。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/c91370c2f08bea19691cfd84936b85bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1028/format:webp/1*U5XAsJ4S7Kd1tNYAa4FxdQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">从上面的代码收敛后输出Q值表。作者图片</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/e491f3c517822a6f6397afd80e35ab84.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*mZknTuK6n8OIKKAJ1HlWog.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3:根据上述Q值矩阵，5个房间环境的最佳策略。。作者图片</p></figure><p id="f30e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据上面的Q表，我们可以根据最大Q值选择行动。例如，如果代理在房间1，它将有两条不同的路线可以通向房间5。代理可以从房间1移动到房间4或房间6，这两个房间的最大Q值都是80。之后，在两种情况下，代理将移动到房间5，这给出了100的最大Q值。类似地，如果代理在房间3，它可以移动到房间1或房间4，这给出了相同的最大Q值。</p></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><h1 id="181a" class="lv lw it bd lx ly ns ma mb mc nt me mf jz nu ka mh kc nv kd mj kf nw kg ml mm bi translated">萨尔萨</h1><p id="8875" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">SARSA中的更新规则如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/d3f470fa72ed7d53d54235915cdf0491.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WsA2q9zNVD1T-U_qEp3gSw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="9af5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">更新规则类似于Q-Learning，但有一些不同。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/0c2dad6de24f0580b71c862f717d0b5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*mHNdrdmeMe_EUVALDTr3aw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3: SARSA —一种基于策略的学习算法[1]</p></figure><p id="1945" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">ε-贪婪</strong>算法中的探索是指以ε概率，智能体随机采取行动。这种方法用于增加探索，因为没有它，代理可能会陷入局部最优。</p><p id="b4b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> SARSA </strong>是<strong class="lb iu"> on-policy </strong>，它用当前策略生成的(S，A，R，S’)样本更新Q表。(S’，A’)是转换样本中的下一个状态和下一个动作。在到达S '之后，它将采取动作A '并使用Q(S '，A ')来更新Q值。而<strong class="lb iu"> Q学习</strong>是<strong class="lb iu">非策略</strong>，其使用状态S’中的最大Q可能值来更新未来的Q值。然而，具有最大Q可能值的动作可能不是代理在未来将采取的实际动作，因为以ε概率代理将采取随机动作。换句话说，用于在Q-Learning中更新策略的动作不同于代理将采取的真实动作。</p><p id="0290" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用这个工具包来解决<a class="ae ky" href="https://gym.openai.com/envs/FrozenLake-v0" rel="noopener ugc nofollow" target="_blank"> FrozenLake </a>环境。有各种各样的游戏，如雅达利2600游戏，基于文本的游戏等。点击查看全部<a class="ae ky" href="https://gym.openai.com/envs/" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="22a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面的教程将使用SARSA算法来解决来自<a class="ae ky" href="https://gym.openai.com/" rel="noopener ugc nofollow" target="_blank">健身房</a>环境的<a class="ae ky" href="https://gym.openai.com/envs/FrozenLake-v0" rel="noopener ugc nofollow" target="_blank"> FrozenLake </a>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure></div><div class="ab cl nl nm hx nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="im in io ip iq"><h1 id="235e" class="lv lw it bd lx ly ns ma mb mc nt me mf jz nu ka mh kc nv kd mj kf nw kg ml mm bi translated">参考</h1><p id="d36a" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">[1]萨顿和巴尔托(2017年)。<a class="ae ky" href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf" rel="noopener ugc nofollow" target="_blank">强化学习:简介</a>。麻省理工学院出版社</p></div></div>    
</body>
</html>