<html>
<head>
<title>Feature Selection: Where Science Meets Art</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">专题选择:科学与艺术相遇的地方</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-selection-where-science-meets-art-639757d91293?source=collection_archive---------26-----------------------#2021-10-05">https://towardsdatascience.com/feature-selection-where-science-meets-art-639757d91293?source=collection_archive---------26-----------------------#2021-10-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/808d174ae3c45d977eb5b814480745eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wAAeymss3OHUSdqr"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">戴维·克洛德在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class=""/><div class=""><h2 id="e91c" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">从启发式到算法的数据科学项目特征选择技术</h2></div></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><p id="3561" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">有人说特征选择和工程是数据科学项目中最重要的部分。在许多情况下，并不是复杂的算法，而是特征选择决定了模型性能的差异。</p><p id="df04" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">太少的特征会使模型不适合。例如，如果你想预测房价，仅仅知道卧室的数量和建筑面积是不够的。你忽略了许多买家关心的重要变量，如位置、学区、房龄等。</p><p id="3251" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">你也可以从另一个方向来，选择100个不同的特征来描述每一个微小的细节，如财产上的树木名称。这些特性没有增加更多的信息，反而增加了噪音和复杂性。许多选择的功能可能完全不相关。最重要的是，太多的特征增加了训练模型的计算成本。</p><p id="887d" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">因此，要建立一个好的预测模型，特征的正确数量是多少，以及如何选择保留哪些特征、删除哪些特征以及添加哪些新特征？这是机器学习项目中管理所谓的<a class="ae jg" rel="noopener" target="_blank" href="/bias-variance-tradeoff-in-machine-learning-an-intuition-da85228c5074">偏差-方差权衡</a>的一个重要考虑因素。</p><p id="7888" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">这也是“科学”与“艺术”相遇的地方</p><p id="6da2" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">本文的目的是通过一些简单的实现来揭开特性选择技术的神秘面纱。我下面描述的技术应该同样适用于回归和分类问题。无监督分类(例如聚类)可能有点棘手，所以我将单独讨论它。</p></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><h1 id="32b9" class="mb mc jj bd md me mf mg mh mi mj mk ml kp mm kq mn ks mo kt mp kv mq kw mr ms bi translated">启发式方法</h1><p id="821c" class="pw-post-body-paragraph lf lg jj lh b li mt kk lk ll mu kn ln lo mv lq lr ls mw lu lv lw mx ly lz ma im bi translated">我们不太谈论数据科学中的试探法，但它非常相关。我们来看看定义(来源:<a class="ae jg" href="https://en.wikipedia.org/wiki/Heuristic" rel="noopener ugc nofollow" target="_blank">维基百科</a>):</p><blockquote class="my"><p id="287b" class="mz na jj bd nb nc nd ne nf ng nh ma dk translated">启发式或启发式技术<strong class="ak"> …</strong>采用一种实用的方法，这种方法不能保证是最佳的、完美的或合理的，但足以达到一个直接的、短期的目标或近似值。</p></blockquote><p id="7605" class="pw-post-body-paragraph lf lg jj lh b li ni kk lk ll nj kn ln lo nk lq lr ls nl lu lv lw nm ly lz ma im bi translated">如果是基于直觉，这个定义同样适用于特征选择。仅仅通过查看一个数据集，你就会直觉地感觉到这样那样的特征是强有力的预测器，而其他一些特征与因变量无关，你会觉得消除它们是安全的。</p><p id="6347" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">如果不确定，可以进一步检查特征和因变量之间的相关性。</p><p id="a95e" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">在数据集中有太多要素的情况下，仅这些启发式方法(直觉和相关性)就可以完成选择正确要素的大部分工作。</p><p id="fe14" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">举个例子，假设你的公司正在分配不同渠道(电视、广播、报纸)的广告预算。你想预测哪个渠道作为广告平台最有效，预期回报是多少。</p><p id="3071" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">在建立模型之前，您查看历史数据，发现不同平台上的广告费用与相应的销售收入之间存在以下关系。</p><figure class="no np nq nr gt iv gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/2db39d9b61e64a04e52df65c2594b85f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*kFKOiS1fRrkhZtVgqoSD5g.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">显示不同平台上销售额和广告支出之间关系的二元散点图(图来源:作者；数据来源:<a class="ae jg" href="https://rdrr.io/cran/ISLR/" rel="noopener ugc nofollow" target="_blank"> ISLR </a>，许可证:GLP 2，公共领域)。</p></figure><p id="f70c" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">根据散点图，你认为解释广告收入的最佳特征是什么？显然，报纸广告对结果没有任何重大影响，所以你可能想把它从模型中去掉。</p><h1 id="5181" class="mb mc jj bd md me ns mg mh mi nt mk ml kp nu kq mn ks nv kt mp kv nw kw mr ms bi translated">自动特征选择</h1><p id="4dfe" class="pw-post-body-paragraph lf lg jj lh b li mt kk lk ll mu kn ln lo mv lq lr ls mw lu lv lw mx ly lz ma im bi translated">我们现在将进入自动特征选择技术。它们中的大部分都集成在<code class="fe nx ny nz oa b">sklearn</code>模块中，所以您只需用标准格式的几行代码就可以实现特性选择。</p><p id="3528" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">为了演示，我将使用‘iris’数据集(来源:<a class="ae jg" href="https://www.kaggle.com/uciml/iris/metadata" rel="noopener ugc nofollow" target="_blank"> Kaggle/UCI机器学习</a>，许可证:CC0公共域)。这是一个简单的数据集，只有5列，然而，你会得到关键点。</p><p id="7fb9" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">让我们从<code class="fe nx ny nz oa b">seaborn</code>库中加载数据集。</p><pre class="no np nq nr gt ob oa oc od aw oe bi"><span id="376c" class="of mc jj oa b gy og oh l oi oj"># import seaborn library<br/><strong class="oa jk">import seaborn as sns</strong></span><span id="115c" class="of mc jj oa b gy ok oh l oi oj"># load iris dataset<br/><strong class="oa jk">iris = sns.load_dataset('iris')<br/>iris.head(5)</strong></span></pre><figure class="no np nq nr gt iv gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/f7b304b59ba0f0586c14084f74eefe59.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*F3IlYmtTmLecJv4ZBDuqPQ.png"/></div></figure><pre class="no np nq nr gt ob oa oc od aw oe bi"><span id="cc9c" class="of mc jj oa b gy og oh l oi oj"># separate features (X) from the target (y) variable<br/><strong class="oa jk">X = iris.drop('species', axis=1)<br/>y = iris['species']</strong></span></pre><p id="d4cb" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">在数据集中,“物种”是我们想要预测的，其余4列是预测值。让我们以编程方式确认特性的数量:</p><pre class="no np nq nr gt ob oa oc od aw oe bi"><span id="fe0f" class="of mc jj oa b gy og oh l oi oj"># number of predictors in the current dataset<br/><strong class="oa jk">X.shape[1]</strong></span><span id="83e4" class="of mc jj oa b gy ok oh l oi oj">&gt;&gt; 4</span></pre><p id="b0ba" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">现在让我们来实现一些特性选择技术。</p><h2 id="9181" class="of mc jj bd md om on dn mh oo op dp ml lo oq or mn ls os ot mp lw ou ov mr ow bi translated">1)基于卡方的技术</h2><p id="8996" class="pw-post-body-paragraph lf lg jj lh b li mt kk lk ll mu kn ln lo mv lq lr ls mw lu lv lw mx ly lz ma im bi translated">基于卡方的技术基于一些分数选择特定数量的用户定义的特征(k)。这些分数是通过计算X(自变量)和y(因变量)之间的卡方统计来确定的。</p><p id="23cd" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated"><code class="fe nx ny nz oa b">sklearn</code>内置了基于卡方的特征选择方法。您所要做的就是确定想要保留多少个特征(假设虹膜数据集的k=3)。</p><pre class="no np nq nr gt ob oa oc od aw oe bi"><span id="8c49" class="of mc jj oa b gy og oh l oi oj"># import modules<strong class="oa jk"><br/>from sklearn.feature_selection import SelectKBest, chi2</strong></span><span id="bc44" class="of mc jj oa b gy ok oh l oi oj"># select K best features<br/><strong class="oa jk">X_best = SelectKBest(chi2, k=3).fit_transform(X,y) </strong></span></pre><p id="7560" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">现在让我们确认一下，我们已经获得了4个特性中的3个最佳特性。</p><pre class="no np nq nr gt ob oa oc od aw oe bi"><span id="ca62" class="of mc jj oa b gy og oh l oi oj"># number of best features<br/><strong class="oa jk">X_best.shape[1]</strong></span><span id="cdf8" class="of mc jj oa b gy ok oh l oi oj">&gt;&gt; 3</span></pre><p id="ea43" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">对于大量的特性，您可以指定要保留或删除的特性的某个百分比。它的工作方式与上面类似。假设我们想保留75%的特性，去掉剩下的25%。</p><pre class="no np nq nr gt ob oa oc od aw oe bi"><span id="fdf8" class="of mc jj oa b gy og oh l oi oj"># keep 75% top features <br/><strong class="oa jk">X_top = SelectPercentile(chi2, percentile = 75).fit_transform(X,y)</strong></span><span id="e5e2" class="of mc jj oa b gy ok oh l oi oj"># number of best features<br/><strong class="oa jk">X_top.shape[1]</strong></span><span id="06cc" class="of mc jj oa b gy ok oh l oi oj">&gt;&gt; 3</span></pre><h2 id="ec96" class="of mc jj bd md om on dn mh oo op dp ml lo oq or mn ls os ot mp lw ou ov mr ow bi translated">2)基于杂质的特征选择</h2><p id="f8c7" class="pw-post-body-paragraph lf lg jj lh b li mt kk lk ll mu kn ln lo mv lq lr ls mw lu lv lw mx ly lz ma im bi translated">基于树的算法(如随机森林分类器)具有内置的<code class="fe nx ny nz oa b">feature_importances_</code>属性。</p><p id="5231" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">决策树将使用减少杂质的特征来分割数据(根据<a class="ae jg" href="https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity" rel="noopener ugc nofollow" target="_blank">基尼杂质</a>或<a class="ae jg" href="https://en.wikipedia.org/wiki/Decision_tree_learning#Information_gain" rel="noopener ugc nofollow" target="_blank">信息增益</a>来测量)。这意味着，找到最佳特征是算法如何解决分类问题的关键部分。然后，我们可以通过<code class="fe nx ny nz oa b">feature_importances_</code>属性访问最佳特性。</p><p id="b0ce" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">让我们首先将“iris”数据集与具有200个评估者的随机森林分类器相匹配。</p><pre class="no np nq nr gt ob oa oc od aw oe bi"><span id="a9e2" class="of mc jj oa b gy og oh l oi oj"># import model<br/><strong class="oa jk">from sklearn.ensemble import RandomForestClassifier</strong></span><span id="a069" class="of mc jj oa b gy ok oh l oi oj"># instantiate model<br/><strong class="oa jk">model = RandomForestClassifier(n_estimators=200, random_state=0)</strong></span><span id="1221" class="of mc jj oa b gy ok oh l oi oj"># fit model<br/><strong class="oa jk">model.fit(X,y)</strong></span></pre><p id="e8c0" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">现在让我们通过属性调用来访问特性重要性。</p><pre class="no np nq nr gt ob oa oc od aw oe bi"><span id="5345" class="of mc jj oa b gy og oh l oi oj"># importance of features in the model<br/><strong class="oa jk">importances = model.feature_importances_</strong></span><span id="e4d2" class="of mc jj oa b gy ok oh l oi oj"><strong class="oa jk">print(importances)</strong></span><span id="25bf" class="of mc jj oa b gy ok oh l oi oj">&gt;&gt; array([0.0975945 , 0.02960937, 0.43589795, 0.43689817])</span></pre><p id="ef96" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">上面的输出显示了4个特征中的每一个在减少每个节点/分裂处的杂质方面的重要性。</p><p id="2b47" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">由于随机森林分类器有许多估计器(例如，上面的200个决策树)，我们可以用置信区间计算相对重要性的估计。让我们想象一下。</p><pre class="no np nq nr gt ob oa oc od aw oe bi"><span id="9c8b" class="of mc jj oa b gy og oh l oi oj"># calculate standard deviation of feature importances <br/><strong class="oa jk">std = np.std([i.feature_importances_ for i in model.estimators_], axis=0)</strong></span><span id="5c2b" class="of mc jj oa b gy ok oh l oi oj"># visualization</span><span id="c915" class="of mc jj oa b gy ok oh l oi oj"><strong class="oa jk">feat_with_importance  = pd.Series(importances, X.columns)</strong></span><span id="cf45" class="of mc jj oa b gy ok oh l oi oj"><strong class="oa jk">fig, ax = plt.subplots()<br/>feat_with_importance.plot.bar(yerr=std, ax=ax)<br/>ax.set_title("Feature importances")<br/>ax.set_ylabel("Mean decrease in impurity")<br/>fig.tight_layout()</strong></span></pre><figure class="no np nq nr gt iv gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/e240e0d8b1b38149f7f0de42da34b0ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*QyJtSKuXuVoRVwFR9YvUUA.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图:杂质测量中特征的重要性(来源:作者)</p></figure><p id="e256" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">既然我们知道了每个特性的重要性，我们就可以手动(或直观地)决定保留哪些特性，放弃哪些特性。</p><p id="c723" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">或者，我们可以利用Scikit-Learn的元转换器<code class="fe nx ny nz oa b">SelectFromModel</code>来完成这项工作。</p><pre class="no np nq nr gt ob oa oc od aw oe bi"><span id="406a" class="of mc jj oa b gy og oh l oi oj"># import the transformer<br/><strong class="oa jk">from sklearn.feature_selection import SelectFromModel</strong></span><span id="59fd" class="of mc jj oa b gy ok oh l oi oj"># instantiate and select features<br/><strong class="oa jk">selector = SelectFromModel(estimator = model, prefit=True)<br/>X_new = selector.transform(X)<br/>X_new.shape[1]</strong></span><span id="2cfb" class="of mc jj oa b gy ok oh l oi oj">&gt;&gt; 2</span></pre><h2 id="697d" class="of mc jj bd md om on dn mh oo op dp ml lo oq or mn ls os ot mp lw ou ov mr ow bi translated">3)正规化</h2><p id="621f" class="pw-post-body-paragraph lf lg jj lh b li mt kk lk ll mu kn ln lo mv lq lr ls mw lu lv lw mx ly lz ma im bi translated">正则化是机器学习中减少过拟合的一个重要概念(阅读:<a class="ae jg" rel="noopener" target="_blank" href="/avoid-overfitting-with-regularization-6d459c13a61f">用正则化避免过拟合</a>)。如果有太多的要素，正则化通过收缩要素系数(称为L2正则化/岭回归)或通过将一些要素系数设置为零(称为L1正则化/LASSO回归)来控制其效果。</p><p id="9cfd" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">一些线性模型内置L1正则化作为超参数来惩罚特征。这些特征可以用元转换器<code class="fe nx ny nz oa b">SelectFromModel</code>消除。</p><p id="01a2" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">让我们用超参数<em class="oy">惩罚=‘L1’</em>来实现<code class="fe nx ny nz oa b">LinearSVC</code>算法。然后，我们将使用<code class="fe nx ny nz oa b">SelectFromModel</code>删除一些功能。</p><pre class="no np nq nr gt ob oa oc od aw oe bi"><span id="fda3" class="of mc jj oa b gy og oh l oi oj"># implement algorithm<br/><strong class="oa jk">from sklearn.svm import LinearSVC<br/>model = LinearSVC(penalty= 'l1', C = 0.002, dual=False)<br/>model.fit(X,y)</strong></span><span id="7ddf" class="of mc jj oa b gy ok oh l oi oj"># select features using the meta transformer<br/><strong class="oa jk">selector = SelectFromModel(estimator = model, prefit=True)<br/>X_new = selector.transform(X)<br/>X_new.shape[1]</strong></span><span id="3c8b" class="of mc jj oa b gy ok oh l oi oj">&gt;&gt; 2</span><span id="377c" class="of mc jj oa b gy ok oh l oi oj"># names of selected features<br/><strong class="oa jk">feature_names = np.array(X.columns)<br/>feature_names[selector.get_support()]</strong></span><span id="aa84" class="of mc jj oa b gy ok oh l oi oj">&gt;&gt; array(['sepal_length', 'petal_length'], dtype=object)</span></pre><h2 id="fade" class="of mc jj bd md om on dn mh oo op dp ml lo oq or mn ls os ot mp lw ou ov mr ow bi translated">4)顺序选择</h2><p id="3df0" class="pw-post-body-paragraph lf lg jj lh b li mt kk lk ll mu kn ln lo mv lq lr ls mw lu lv lw mx ly lz ma im bi translated">顺序特征选择是一种古老的统计技术。在这种情况下，您将逐个向模型添加(或从中移除)特征，并检查模型性能，然后试探性地选择要保留的特征。</p><p id="976d" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">顺序选择有两种变体。<em class="oy">前向选择</em>技术从零个特征开始，然后增加一个最大程度地减小误差的特征；然后添加另一个特征，等等。<em class="oy">反向选择</em>反方向工作。该模型从所有特征开始并计算误差；然后，它消除一个特征，使误差进一步最小化，以此类推，直到剩下所需数量的特征。</p><p id="9f1e" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">Scikit-Learn模块有<code class="fe nx ny nz oa b">SequentialFeatureSelector </code>元转换器，让生活更轻松。注意，它适用于<code class="fe nx ny nz oa b">sklearn</code> v0.24或更高版本。</p><pre class="no np nq nr gt ob oa oc od aw oe bi"><span id="bc61" class="of mc jj oa b gy og oh l oi oj"># import transformer class<br/><strong class="oa jk">from sklearn.feature_selection import SequentialFeatureSelector</strong></span><span id="7185" class="of mc jj oa b gy ok oh l oi oj"># instantiate model<br/><strong class="oa jk">model = RandomForestClassifier(n_estimators=200, random_state=0)</strong></span><span id="bd9e" class="of mc jj oa b gy ok oh l oi oj"># select features<br/><strong class="oa jk">selector = SequentialFeatureSelector(estimator=model, n_features_to_select=3, direction='backward')<br/>selector.fit_transform(X,y).shape[1]</strong></span><span id="e352" class="of mc jj oa b gy ok oh l oi oj">&gt;&gt; 3</span><span id="6138" class="of mc jj oa b gy ok oh l oi oj"># names of features selected<br/><strong class="oa jk">feature_names = np.array(X.columns)<br/>feature_names[selector.get_support()]</strong></span><span id="030d" class="of mc jj oa b gy ok oh l oi oj">&gt;&gt; array(['sepal_width', 'petal_length', 'petal_width'], dtype=object)</span></pre><h1 id="7954" class="mb mc jj bd md me ns mg mh mi nt mk ml kp nu kq mn ks nv kt mp kv nw kw mr ms bi translated"><strong class="ak">替代技术……</strong></h1><p id="48c3" class="pw-post-body-paragraph lf lg jj lh b li mt kk lk ll mu kn ln lo mv lq lr ls mw lu lv lw mx ly lz ma im bi translated">除了我刚刚描述的技术，还有一些其他的方法可以尝试。它们中的一些并不完全是为特征选择而设计的，但是如果你深入一点，你会发现它们是如何创造性地应用于特征选择的。</p><ul class=""><li id="7e5a" class="oz pa jj lh b li lj ll lm lo pb ls pc lw pd ma pe pf pg ph bi translated"><strong class="lh jk"> Beta系数</strong>:运行线性回归后得到的系数(Beta系数)显示了因变量对每个特征的相对敏感度。从这里您可以选择具有高系数值的特征。</li><li id="033d" class="oz pa jj lh b li pi ll pj lo pk ls pl lw pm ma pe pf pg ph bi translated"><strong class="lh jk"> p值</strong>:如果你在一个经典的统计软件包(如<code class="fe nx ny nz oa b">statsmodels</code>)中实现回归，你会注意到模型输出包括每个特征的p值(<a class="ae jg" href="https://www.statsmodels.org/stable/regression.html" rel="noopener ugc nofollow" target="_blank">看看这个</a>)。p值检验系数正好为零的零假设。因此，您可以消除与高p值相关的特征。</li><li id="9eb5" class="oz pa jj lh b li pi ll pj lo pk ls pl lw pm ma pe pf pg ph bi translated"><a class="ae jg" href="https://etav.github.io/python/vif_factor_python.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lh jk">方差膨胀因子(VIF) </strong> </a>:通常VIF用于检测数据集中的多重共线性。统计学家通常移除具有高VIF的变量，以满足线性回归的关键假设。</li><li id="cf6d" class="oz pa jj lh b li pi ll pj lo pk ls pl lw pm ma pe pf pg ph bi translated"><a class="ae jg" href="https://en.wikipedia.org/wiki/Akaike_information_criterion" rel="noopener ugc nofollow" target="_blank"><strong class="lh jk">【AIC/BIC】</strong></a>:通常用AIC和BIC来比较两个模型的性能。但是，您可以利用它来进行特征选择，例如，通过选择某些特征来获得以AIC/BIC衡量的更好的模型质量。</li><li id="671f" class="oz pa jj lh b li pi ll pj lo pk ls pl lw pm ma pe pf pg ph bi translated"><strong class="lh jk">主成分分析(PCA) </strong>:如果你知道什么是PCA，你就猜对了。这不完全是一种特征选择技术，但是PCA的降维特性可以用于这种效果，而不完全消除特征。</li><li id="a425" class="oz pa jj lh b li pi ll pj lo pk ls pl lw pm ma pe pf pg ph bi translated"><strong class="lh jk">和许多其他的:</strong>有很多其他的特性选择类和<code class="fe nx ny nz oa b">sklearn</code>模块一起提供，<a class="ae jg" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection" rel="noopener ugc nofollow" target="_blank">查看文档</a>。最近在一篇科学论文<a class="ae jg" href="https://www.sciencedirect.com/science/article/pii/S2314717218300059" rel="noopener ugc nofollow" target="_blank">中也提出了一种基于聚类的算法。费希尔评分是另一种可用的技术。</a></li></ul><h1 id="bcb7" class="mb mc jj bd md me ns mg mh mi nt mk ml kp nu kq mn ks nv kt mp kv nw kw mr ms bi translated">集群怎么样？</h1><p id="4a4d" class="pw-post-body-paragraph lf lg jj lh b li mt kk lk ll mu kn ln lo mv lq lr ls mw lu lv lw mx ly lz ma im bi translated">聚类是一种无监督的机器学习算法，这意味着你将数据输入到聚类算法中，该算法将根据一些“属性”计算出如何将数据分成不同的聚类。这些属性实际上来自于特性。</p><p id="fcd6" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">聚类需要特征选择吗？当然了。没有适当的特征，集群可能是无用的。假设您想要细分客户，以便销售高端、中档和低端产品。这意味着你含蓄地使用<em class="oy">客户收入</em>作为一个因素。你也可以通过<em class="oy">教育</em>进入混。他们的<em class="oy">年龄</em>和<em class="oy">经验年限</em>？当然可以。但是，当您增加功能的数量时，算法会对您想要实现的目标感到困惑，因此输出可能并不完全是您想要的。</p><p id="1e41" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">也就是说，数据科学家并不是在真空中运行聚类算法，他们头脑中通常有一个假设或问题。因此，功能必须符合这种需求。</p></div><div class="ab cl ky kz hx la" role="separator"><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld le"/><span class="lb bw bk lc ld"/></div><div class="im in io ip iq"><h1 id="c49c" class="mb mc jj bd md me mf mg mh mi mj mk ml kp mm kq mn ks mo kt mp kv mq kw mr ms bi translated">摘要</h1><p id="73f7" class="pw-post-body-paragraph lf lg jj lh b li mt kk lk ll mu kn ln lo mv lq lr ls mw lu lv lw mx ly lz ma im bi translated">数据科学家非常重视特征选择，因为它对模型性能有影响。对于低维数据集，试探法和直觉完美地工作，然而，对于高维数据，有自动化技术来完成这项工作。最有用的技术包括卡方和基于杂质的算法以及正则化和顺序特征选择。此外，还有其他有用的替代技术，如回归中的β系数、p值、VIF、AIC/BIC和维度缩减。</p><p id="1a89" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">在这篇文章的标题里，我说的是“科学遇见艺术”。这是因为在功能选择上没有对错之分。我们可以使用科学工具，但最终，这可能是数据科学家做出的主观决定。</p><p id="fd13" class="pw-post-body-paragraph lf lg jj lh b li lj kk lk ll lm kn ln lo lp lq lr ls lt lu lv lw lx ly lz ma im bi translated">感谢阅读。请随意<a class="ae jg" href="https://mab-datasc.medium.com/subscribe" rel="noopener">订阅</a>以获得我即将发布的文章的通知，或者通过<a class="ae jg" href="https://twitter.com/DataEnthus" rel="noopener ugc nofollow" target="_blank"> Twitter </a>或<a class="ae jg" href="https://www.linkedin.com/in/mab-alam/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>与我联系。</p></div></div>    
</body>
</html>