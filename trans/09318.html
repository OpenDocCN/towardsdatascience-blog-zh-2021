<html>
<head>
<title>Deep Reinforcement Learning: From SARSA to DDPG and beyond</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度强化学习:从萨莎到DDPG及其他</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-reinforcement-learning-from-sarsa-to-ddpg-and-beyond-458100c2fda8?source=collection_archive---------12-----------------------#2021-08-29">https://towardsdatascience.com/deep-reinforcement-learning-from-sarsa-to-ddpg-and-beyond-458100c2fda8?source=collection_archive---------12-----------------------#2021-08-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="8a6b" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="6cce" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">抓住使RL成功的基本要素</h2></div><p id="5c06" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">让机器学习的能力是过去几十年的一项令人着迷的成就。许多新的商业机会已经开放，公司每天都在使用机器学习。</p><p id="374e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">就在几年前，DeepMind的AlphaGo算法打败了围棋世界冠军Lee Sedol。这一惊人的壮举是在强化学习的帮助下实现的，但无疑不是一夜之间实现的。这个雄心勃勃的项目的研究始于21世纪初。从那以后，这个领域本身已经有了很大的发展。本文旨在涵盖向现代方法转变的一部分。</p><h1 id="f0c5" class="ln lo it bd lp lq lr ls lt lu lv lw lx ki ly kj lz kl ma km mb ko mc kp md me bi translated">基础</h1><p id="7d5e" class="pw-post-body-paragraph kr ks it kt b ku mf kd kw kx mg kg kz la mh lc ld le mi lg lh li mj lk ll lm im bi translated">在我们开始之前，我们必须缩小强化学习的含义:强化学习的目标是尽可能好地行动。这个定义是的简化版本</p><p id="7f2c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">目标是找到一个使预期收益最大化的政策。</p><p id="bd69" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">为了实现这一点，我们有几个组件协同工作，如图所示:</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi mk"><img src="../Images/bdbfb60a7ebd39807ba50ed97acd0991.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b0pZNQ-CCtDkaeyFeLw8-A.png"/></div></div><p class="mw mx gj gh gi my mz bd b be z dk translated">强化学习的循环。图片由作者提供，来自一个封闭的大学讲座的例子。</p></figure><p id="e157" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">代理与环境交互，环境改变它的状态，并为动作产生奖励。然后，又一轮开始了。在数学上，这个周期是基于马尔可夫决策过程(MDP)。这种市场发展计划包括五个组成部分:南非、阿尔及利亚、菲律宾和p₀.</p><p id="9214" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">s是状态空间，它包含环境可能处于的所有可能状态<em class="na"> s </em>。例如，如果我们要为一辆汽车建模，它可能有以下状态:前进、后退、转弯、刹车等等。</p><p id="34dc" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">第二个组件A是动作空间，它定义了代理可以执行的所有动作<em class="na"> a </em>。对于汽车设定，这可能是转动方向盘，加速，刹车。这样的行动导致了新的国家。</p><p id="7dbc" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">第三个分量R是奖励函数。该功能负责根据代理的动作和结果状态对其进行奖励。例如，如果汽车成功地在交通灯前停下来，代理人会得到一个积极的奖励。然而，如果它没有停止，代理人会得到一个负的奖励。</p><p id="bd05" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">第四个分量P是转移概率函数。通常情况下，一个动作并不能保证会达到期望的状态。因此，该属性被建模为概率函数。例如，它可能会说:“有90%的可能性，我们会到达理想的状态，但有10%，可能会发生不同的事情。”</p><p id="8508" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">最后，我们有p₀，它是一组初始状态。每当一个代理被训练，它开始在这些状态之一。</p><p id="bd18" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在我们已经介绍了RL设置的要素，我们如何教代理学习期望的行为呢？这是在π策略的帮助下完成的，我之前简单地提到过。形式上，策略从状态空间S映射到所有可能动作的概率。听起来很复杂？考虑一下:你站在路边，想要过马路。这种情况就是你的起始状态。一个策略现在将列出所有可能的动作:向左看、向右看、直线向前跑、等待等等。所有这些选择都有被选中的概率。例如，你可能有20%的机会选择向左看，有50%的机会，你可能选择跑。</p><p id="54e6" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">随着代理被训练，它优化策略。一开始，π可能会经常选择跑过街道。然而，由于代理人多次被车撞，他获得了许多负面奖励，这是他不想要的。随着时间的推移，代理人学会做得更好，导致越来越多的积极回报。最终，这将导致找到一个最大化总回报的政策。</p><p id="4c04" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">但是，我们怎么知道我们现在的状态是好的呢？对于我们可以采取的行动，我们可能会问:我们如何知道它们是好的？这种评估借助于价值函数来实现。我们有两个，一个是国家的，一个是行动的。</p><p id="9d9d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">第一个函数称为状态值函数，缩写为v。对于每个状态，它“知道”这个状态有多好。状态的好与坏是由我们在这种状态下开始时期望得到的回报决定的。比如赛车中的杆位更容易赢得比赛；这位司机获得了积极的奖励。这一事实影响了杆位的价值:随着越来越多的人从这里开始并赢得比赛，从这种状态中获得的积极奖励的数量也在增加。反过来，极点位置的值增加。</p><p id="39cf" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">第二个函数是动作值函数，缩写为q。对于我们在给定状态下可以做的所有动作，它“知道”它们有多好。这个知识的获得与上面类似:导致好的回报的行为比导致负面回报的行为有更高的价值。</p><h1 id="d697" class="ln lo it bd lp lq lr ls lt lu lv lw lx ki ly kj lz kl ma km mb ko mc kp md me bi translated">萨尔萨</h1><p id="bf42" class="pw-post-body-paragraph kr ks it kt b ku mf kd kw kx mg kg kz la mh lc ld le mi lg lh li mj lk ll lm im bi translated">现在我们已经讨论了基础知识，我们可以检查一个经典的RL算法，称为SARSA [1]。这个名字是州行动奖励州行动的缩写，它巧妙地抓住了功能。</p><p id="854a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">首先，我们从一个状态(S)开始，采取行动(A)，得到回报(R)。现在我们处于后继状态，并选择另一个动作(A)。我们这样做是为了更新我们的Q函数。如前所述，行动的价值是当我们从一个状态开始并选择这个行动时，我们期望的总回报。同样的属性适用于下一个状态、下一个状态和下一个下一个状态。我们可以用动作值函数的更新规则来表达这一点:</p><p id="a038" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">Q(s，a) ← Q(s，a) + α(r + γ Q(s '，a') — Q(s，a))</p><p id="bda5" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">上面写了什么？我们的状态-动作对的新值是旧值加上，这是括号中的部分，与我们从这里得到的不同:r + γ Q(s '，a ')。所以随着时间的推移，我们的Q函数越来越接近我们得到的回报，这是由这个更新规则模拟的。</p><p id="23c7" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在我们有了Q函数(或者有时称为Q值)，我们能用它做什么呢？首先，这些值存储在一个表中。每当我们的策略必须选择一个操作时，它会检查它处于哪个状态，然后查看所有可能的操作:</p><figure class="ml mm mn mo gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mq mr di ms bf mt"><div class="gh gi nb"><img src="../Images/f8c264f2128a7e92b92024fdc8466ab1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WgHwajahFap1PXWxzxWrTA.png"/></div></div></figure><p id="f4a1" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">从所有这些动作中，选择具有最高值的动作。这个动作导致了一个新的状态，在这个状态中，再次选择了最有价值的动作。在训练期间，Q值按照先前引入的更新规则进行更新。一旦完成，我们就找到了一个合适的好策略，并准备在我们的环境中使用代理。</p><p id="32a8" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">但是至少有一点我们可以改进算法。这就是Q-Learning发挥作用的地方。</p><h1 id="e5c1" class="ln lo it bd lp lq lr ls lt lu lv lw lx ki ly kj lz kl ma km mb ko mc kp md me bi translated">q学习</h1><p id="1e03" class="pw-post-body-paragraph kr ks it kt b ku mf kd kw kx mg kg kz la mh lc ld le mi lg lh li mj lk ll lm im bi translated">Q学习算法[2，3]可以概括为非策略SARSA。为了解释这一点，让我们再来看看SARSA的更新规则:</p><p id="5e52" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">Q(s，a) ← Q(s，a) + α(r + γ Q(s '，a') — Q(s，a))</p><p id="36cc" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们的策略π选择一个初始动作，这是Q(s，a)部分。然后，我们使用相同的策略来选择后续动作，也就是说Q(s '，a ')的部分。使用相同策略来确定两个动作的过程由关键字<em class="na"> on-policy </em>表示。代理基于当前使用的策略所选择的动作来学习Q值。</p><p id="c9fa" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">相比之下，在<em class="na">非策略</em>算法中，我们有不止一个策略。这可以从Q-learning的更新规则中看出:</p><p id="b223" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">Q(s，a) ← Q(s，a) + α(r + γ maxₐ' (s '，a') — Q(s，a))</p><p id="2b06" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">关键的区别是选择<em class="na">最佳</em>后续行动，这是<em class="na">而不是</em>遵循政策完成的。相反，我们总是选择具有最高Q值的动作。这个选择过程可以理解为另一个用于确定后续行动的策略。好处是我们可以遵循不同的策略来选择后续操作。这看起来没什么大不了的，但是在Q-learning中，我们总是利用这个事实来选择一个最优的后续行动。</p><p id="0c0f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">此外，它允许我们很容易地加入额外的影响。例如，考虑我们可能拥有的第二个代理。这个代理有一个独特的功能，我们希望我们的主要代理也能学习它。我们采用二级代理的策略，并在我们选择新的后续动作时使用它。</p><p id="bdf8" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">到目前为止，我们不需要任何神经网络。但是表格方法不适合大的状态和动作空间:首先，它们的内存消耗很大。其次，我们甚至可能不会在训练过程中访问所有的状态或动作，所以有些条目可能没有初始化。第三，查找时间过长。然而，作为第一个措施，我们可以使用神经网络来代替Q表。</p><h1 id="ea38" class="ln lo it bd lp lq lr ls lt lu lv lw lx ki ly kj lz kl ma km mb ko mc kp md me bi translated">深度Q学习</h1><p id="f2f4" class="pw-post-body-paragraph kr ks it kt b ku mf kd kw kx mg kg kz la mh lc ld le mi lg lh li mj lk ll lm im bi translated">神经网络已经被证明具有良好的泛化能力。我们也可以利用它们从数据中学习特征的能力来进行强化学习。在深度强化学习中，我们用神经网络来逼近我们的函数，比如Q[4]。为了突出这种修改，Q或V函数通常标有θ: Q_θ和V_θ。除了神经网络之外，还有其他方法，但这些不是本文的范围。</p><p id="9372" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在默认的Q学习中，更新规则帮助我们找到最优的Q值。最优意味着它们会带来最高的回报；我们可以用它们来选择最佳行动(在给定状态下具有最高Q值的行动)。并且，通过使用神经网络，我们直接逼近这个最佳Q或V函数。我们从少量的训练数据中学习，然后可以推广到新的情况。</p><p id="0b48" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在深度Q学习算法中，我们使用两种技术，称为<em class="na">经验重放</em>和<em class="na">目标网络</em>。</p><p id="dd3b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">第一个修改将旧的转换(通过选择一个动作从一个状态转移到下一个状态)存储在重放缓冲区中。在训练期间，我们从这个缓冲区取样。这项技术提高了效率，并使数据分布更加稳定。这是怎么回事？现在，想想如果我们有几个动作，并且在每个训练步骤中，我们选择不同的一个，会发生什么。我们总是在各种可能性之间跳跃。有了重放缓冲区，我们可以检测分布中的趋势，使动作的选择更加稳定。</p><p id="b0ea" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">第二个修改，<em class="na">目标网络</em>，用于计算我们的目标。一般来说，我们的目标是最大化预期报酬。问题是，我们如何知道我们的奖励是否已经是最好的了？目标网络帮助我们解决这个问题。我们使用旧更新步骤中的网络，并将其Q值作为目标。这听起来可能很难，因此我们来看看目标网络的更新规则:</p><p id="c179" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">Q_θ(s，a) ← Q_θ(s，a) + α((r + γ maxₐ' Qₜₐᵣ(s'，a′))—q _θ(s，a))</p><p id="7b4c" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">不同之处在于更新目标的计算。通过最小化我们的当前值Q_θ(s，a)和Qₜₐᵣ(s'，a’)之间的差，我们接近目标，这里缩写为“tar”。而这个目标就是目标网络的价值。</p><p id="1f83" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">现在，如果我们不使用这样的目标网络，而只使用当前网络Q_θ，我们的目标将不断变化:随着网络参数的每次更新，我们将得到不同的Q值。这就是为什么我们使用旧网络(意味着在我们当前网络之前的一些更新步骤)作为固定目标。它的参数更新得更慢，使目标更稳定。最终，我们降低了我们的主网络可能会被自己的尾巴所困扰的风险。</p><p id="6297" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">换句话说，想想跑步比赛。每当你接近终点线时，它就会后退。这样，你永远不知道你离目标有多近。</p><p id="a46e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这两个提出的修改，使用一个重放缓冲器和目标网络，被用来帮助RL代理玩Atari游戏，有时甚至在超人的水平[5]。</p><h1 id="369b" class="ln lo it bd lp lq lr ls lt lu lv lw lx ki ly kj lz kl ma km mb ko mc kp md me bi translated">深度确定性政策梯度</h1><p id="025c" class="pw-post-body-paragraph kr ks it kt b ku mf kd kw kx mg kg kz la mh lc ld le mi lg lh li mj lk ll lm im bi translated">到目前为止，我们只考虑了离散动作空间。在离散的动作空间中，我们有固定数量的动作，只有一个单一的粒度级别。相比之下，想想门和墙之间的角度。如果我们只有，比方说，两个位置，我们有一个离散的空间。但是我们可以把门放在任意的位置:全开、全关、45度、30度、30.1度、30.01度等等。这个空间是连续的，用表格方法和深度Q学习方法都难以覆盖。</p><p id="f5e5" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">连续动作的问题是到目前为止我们对Q函数的更新步骤:</p><p id="85b3" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">Q_θ(s，a) ← Q_θ(s，a) + α((r + γ maxₐ' Qₜₐᵣ(s'，a′))—q _θ(s，a))</p><p id="4385" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">上面写着maxₐ' Qₜₐᵣ(s'的那一部分才是问题所在。在离散的行动空间中，很容易找到导致最高回报的行动。然而，在具有任意细粒度动作的连续动作空间中，这是非常昂贵的，如果不是不可能的话。</p><p id="d0d8" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">一个简单的解决方案是将空间离散化。但是，挑战在于如何离散化。然而，我们可以完全跳过这个问题，修改深度Q学习算法以支持连续动作。这导致了深度确定性策略梯度(DDPG)算法[6，7]，它本质上是针对连续动作的深度Q学习。它也使用重放缓冲区和目标网络，但采用最大运算。</p><p id="9412" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们引入另一个神经网络，而不是“手动”执行对最佳状态-动作对(最佳Q值)的搜索。这个神经网络学习逼近最大化器。然后，每次我们用一个状态查询它，它都返回最佳的对应动作——这正是我们需要的。</p><p id="9737" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">取代最大化器的新网络然后被用于目标的计算。因此，前面的等式可以改写为</p><p id="e43d" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">Q_θ(s，a) ← Q_θ(s，a) + α((r + γ Qₜₐᵣ(s'，μₜₐᵣ(s'))—q _θ(s，a))</p><p id="e485" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">正如我提到的，我们使用网络，μₜₐᵣ，来决定最佳行动。和以前一样，我们使用稍微旧一点的网络版本，因此有“tar”部分。即使μₜₐᵣ和Qₜₐᵣ每一步都被更新，它们也永远不会被更新到最新的参数。相反，他们通常保留90%的参数值，剩下的10%来自当前网络。这样，目标网络既不会太旧，也不会太新。</p><p id="3950" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">双延迟DDPG算法[8]引入了三种改进，以提高默认版本的性能。首先，TD3，也是缩写，学习两个Q函数并使用较小的值来构建目标。此外，策略(负责选择初始动作)更新不太频繁，并且添加噪声以平滑Q函数。</p><h1 id="3d0e" class="ln lo it bd lp lq lr ls lt lu lv lw lx ki ly kj lz kl ma km mb ko mc kp md me bi translated">熵正则化强化学习</h1><p id="26ef" class="pw-post-body-paragraph kr ks it kt b ku mf kd kw kx mg kg kz la mh lc ld le mi lg lh li mj lk ll lm im bi translated">所有算法面临的挑战是过早收敛。这种不受欢迎的行为是探索不足的结果。例如，如果代理已经确定了一个好的动作序列，他可能会关注这些特定的动作。在这个过程中，他没有探索其他州，而这些州可能会给他带来更好的回报。这种现象也被称为探索与开发的权衡:探索旨在探索动作和状态空间，而开发则寻求开发已经探索过的区域。</p><p id="5e5f" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">熵正则化强化学习方法是应对这一挑战的一种方式:我们训练策略来最大化如上所述的权衡。为了做到这一点，我们使用一个叫做熵的概念。简而言之，熵给出了关于分布“混沌”的信息。混沌程度越高(“分布越不均匀”)，熵值越低。因此，所有值都有相同的机会被抽取的均匀概率产生最大熵值。</p><p id="4db1" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我们利用这个事实，在我们的价值函数中加入一个熵项。详细解释它们超出了本文的范围，但要点如下:附加术语强制执行策略，以在高回报(探索)和从各种行动中选择(探索)之间保持平衡。</p><p id="ed98" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在软演员-评论家算法[9，10]中，这个概念尤其用于训练机器人行走。虽然机器人只在平坦的地形上训练，但由此产生的策略足以应对训练期间看不到的环境。</p><h1 id="69b9" class="ln lo it bd lp lq lr ls lt lu lv lw lx ki ly kj lz kl ma km mb ko mc kp md me bi translated">强化学习库</h1><p id="d233" class="pw-post-body-paragraph kr ks it kt b ku mf kd kw kx mg kg kz la mh lc ld le mi lg lh li mj lk ll lm im bi translated">上面所有的算法都已经在各种python包中实现了。OpenAI <a class="ae nc" href="https://gym.openai.com" rel="noopener ugc nofollow" target="_blank"> Gym </a>和<a class="ae nc" href="https://github.com/openai/safety-gym" rel="noopener ugc nofollow" target="_blank"> Safety Gym </a>框架提供了构建代理培训环境的代码。来自DeepMind的研究人员提供了<a class="ae nc" href="https://github.com/deepmind/dm_control" rel="noopener ugc nofollow" target="_blank">控制套件</a>包，用于基于物理的RL模拟。</p><p id="0212" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">要构建算法，可以从各种选项中进行选择。稳定基线3库是在PyTorch中实现的，它提供了比较算法和创建新算法的工具。类似地，ACME库提供了RL代理和构建块，并且足够灵活，可以进行自己的研究。作为第三种选择，你可以考虑<a class="ae nc" href="https://google.github.io/dopamine/" rel="noopener ugc nofollow" target="_blank">谷歌的多巴胺框架</a>，它专注于投机性研究的快速原型制作。它支持JAX和张量流。</p><h1 id="bd88" class="ln lo it bd lp lq lr ls lt lu lv lw lx ki ly kj lz kl ma km mb ko mc kp md me bi translated">摘要</h1><p id="5067" class="pw-post-body-paragraph kr ks it kt b ku mf kd kw kx mg kg kz la mh lc ld le mi lg lh li mj lk ll lm im bi translated">在过去的几十年里，强化学习领域经历了许多批判性的观点。该基金会是基于学习功能的理念建立的，这种学习功能可以量化处于特定情况下的优势。研究人员随后提议使用不同的政策来有效地探索环境。当在神经网络的帮助下学习函数时，向前迈出了重要的一步，这引入了算法设计的许多新的可能性。随着时间的推移，其他想法被纳入其中，如逐步更新作为训练目标的其他网络。然而，开发并没有完成。仍然有许多挑战需要克服，例如处理约束，弥合训练和现实生活环境之间的差距，或者可解释性。</p><h1 id="ce00" class="ln lo it bd lp lq lr ls lt lu lv lw lx ki ly kj lz kl ma km mb ko mc kp md me bi translated">文学</h1><p id="d8ba" class="pw-post-body-paragraph kr ks it kt b ku mf kd kw kx mg kg kz la mh lc ld le mi lg lh li mj lk ll lm im bi translated">[1] Gavin Rummery和Mahesan Niranjan，<a class="ae nc" href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.17.2539&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">使用连接主义系统的在线Q-learning</a>，1994年，Citeseer</p><p id="0ca5" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">[2] C.J.C.H .沃特金斯，<a class="ae nc" href="https://d1wqtxts1xzle7.cloudfront.net/50360235/Learning_from_delayed_rewards_20161116-28282-v2pwvq-with-cover-page-v2.pdf?Expires=1628665119&amp;Signature=ebJKaUbnN54qOUvLFCwoDoTLGfTA6GuCDLttjqa5QEwoIT7zeUhoYsM3WM59MIznGKFkTrTw18uYnUw2cwQGG85Us-5~gPohm4usSSrjhKWMTWxgnsEWtGv4ZrhQckBJt~6Ticmx3JpRWZSrD3ewcUHU8CpRyiI4P4RsONv6KuJUqvfEFq1dK1juQqibx0AaXhUIuWxNg9hidTan3aeijoxB1AeZta2oCD7s8ALMq3~m4ih5Rn~XYF16Gq2fY4i-FQSsBgv9zz6OXnkkpyp7CZasb-WGFQwQmGRdbJdcw~1vozPqVv3yuG~0d2Owm1nbm06qJbnVG7VOjHyodiYrxg__&amp;Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA" rel="noopener ugc nofollow" target="_blank">从延迟回报中学习</a>，博士论文，1989年，剑桥大学国王学院</p><p id="0317" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">[3] C.J.C.H .沃特金斯和彼得·达扬，<a class="ae nc" href="https://link.springer.com/content/pdf/10.1007/BF00992698.pdf" rel="noopener ugc nofollow" target="_blank"> Q-learning </a>，1992，机器学习8</p><p id="541e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">[4] Kurt Hornik，<a class="ae nc" href="http://www.vision.jhu.edu/teaching/learning/deeplearning18/assets/Hornik-91.pdf" rel="noopener ugc nofollow" target="_blank">多层前馈网络的逼近能力</a>，1991，神经网络4</p><p id="e6d1" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">[5] Mnih等人，<a class="ae nc" href="https://arxiv.org/pdf/1312.5602.pdf" rel="noopener ugc nofollow" target="_blank">用深度强化学习玩雅达利</a>，2013，arXiv</p><p id="8431" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">[6] Silver等人，<a class="ae nc" href="http://proceedings.mlr.press/v32/silver14.pdf" rel="noopener ugc nofollow" target="_blank">确定性策略梯度算法</a>，2014，机器学习国际会议</p><p id="1033" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">[7] Lillicrap等，<a class="ae nc" href="https://arxiv.org/pdf/1509.02971.pdf" rel="noopener ugc nofollow" target="_blank">深度强化学习的连续控制</a>，2015，arXiv</p><p id="c4fe" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">[8] Stephen Dankwa和Wenfeng Zheng，<a class="ae nc" href="https://dl.acm.org/doi/pdf/10.1145/3387168.3387199?casa_token=pHHjm6A89ckAAAAA:jOlVaEVQgU8g2tZ79UwdKGxdZCEpTUE5OGNcIgn_86LsDhCCFkxlzkCKX-dPVmfETvem_cVgeg" rel="noopener ugc nofollow" target="_blank">双延迟:一种深度强化学习技术，用于模拟智能机器人智能体的连续运动</a>，2019，ACM</p><p id="b54a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">[9]哈尔诺贾等，<a class="ae nc" href="http://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b.pdf" rel="noopener ugc nofollow" target="_blank">软行动者-批评家:带随机行动者的离策最大熵深度强化学习</a>，2018，ICML</p><p id="5b86" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">[10]哈尔诺贾等，软演员-评论家算法与应用，2018，arXiv</p><p id="5452" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">[11]哈尔诺贾等，<a class="ae nc" href="https://arxiv.org/pdf/1812.11103" rel="noopener ugc nofollow" target="_blank">通过深度强化学习学习走路</a>，2018，arXiv</p></div></div>    
</body>
</html>