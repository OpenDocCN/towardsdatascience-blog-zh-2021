<html>
<head>
<title>How to Train BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何训练伯特</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-train-bert-aaad00533168?source=collection_archive---------0-----------------------#2021-06-15">https://towardsdatascience.com/how-to-train-bert-aaad00533168?source=collection_archive---------0-----------------------#2021-06-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="729e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">训练变形金刚的速射指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/cc27dac600ad2b9fc95bc42c5ea9dd04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mq1r6J18FX6Nbl3zx4PbIA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">像这样的形式需要预先训练——由作者生成图像。</p></figure><p id="655e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi lu translated">transformer模型的成功在很大程度上要归功于它能够采用Google和OpenAI等公司在巨大数据集上预先训练的模型，并将它们应用到我们自己的用例中。</p><p id="1c98" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有时，这就是我们所需要的——我们采用模型并按原样滚动。</p><p id="23be" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但在其他时候，我们发现我们确实需要对模型进行微调。我们需要在我们的特定用例上对它进行更多的<em class="md">培训。</em></p><p id="fd3c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">每个变压器模型都是不同的，针对不同用例的微调也是不同的——因此我们将重点关注核心BERT模型的微调。并附有一些脚注，说明我们如何为几个最常见的应用程序修改它。</p><p id="3327" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可以在这里观看文章的视频版本:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="me mf l"/></div></figure></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><h1 id="ac6e" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">它是如何工作的</h1><p id="b6f8" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">首先，这些是如何工作的？我假设你对变形金刚和伯特有一定的了解，如果你不知道我在说什么——先看看<a class="ae nk" rel="noopener" target="_blank" href="/how-transformers-work-6cb4629506df">这篇文章</a>。</p><p id="1c3a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">《变形金刚》的威力源自谷歌和OpenAI等大公司将变形金刚模型预先训练到非常高的标准。</p><p id="e74d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，当我说按照非常高的标准进行预培训时，OpenAI的GPT-3的估计培训成本在460万美元到1200万美元之间[1][2]。</p><p id="630f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我没有多余的1200万美元来训练模特，你呢？</p><p id="1da3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通常，原始的预训练模型对于我们的需求来说已经足够了，我们不需要担心进一步的训练。</p><p id="ad39" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但有时，我们可能需要——幸运的是，变形金刚在制造时就考虑到了这一点。对于BERT，我们可以将进一步培训的可能性分为两类。</p><p id="0195" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，我们对核心BERT模型本身进行了微调。这种方法包括使用Google在训练原始模型时使用的相同训练方法——我们稍后将更深入地讨论这一点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/cdc50e02aa4d0469a92d87ff1d741741.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zo8WBswvHj1WnHvHElin4w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(左)带分类头的BERT，(右)带问答头的BERT。</p></figure><p id="7471" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">第二，我们可以添加不同的<em class="md">头</em>到我们的模型中，这给了伯特新的能力。这些是我们模型末尾的额外层，为不同的用例修改输出。例如，我们会使用不同的标题来回答问题或进行分类。</p><p id="4d7c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我们将重点关注对核心BERT模型的微调——这允许我们对BERT进行微调，以更好地理解我们的用例中特定的语言风格。</p><h2 id="8fca" class="nm mo it bd mp nn no dn mt np nq dp mx lh nr ns mz ll nt nu nb lp nv nw nd nx bi translated">微调内核</h2><p id="5618" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">使用两种方法训练BERT的核心，下一句预测(NSP)和屏蔽语言建模(MLM)。</p><p id="59f8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi lu translated"><span class="l lv lw lx bm ly lz ma mb mc di"> 1。</span> <strong class="la iu">下一个句子预测</strong>包括将句子对作为模型的输入，这些对中的一些对将是真的<em class="md">对</em>，其他的将不是。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/8fa3ba1cef4bd4379d855000cdb6f5da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3r3ZPTfugznRLYbGfwq-Lg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">两个连续的句子形成一个“真对”，其他的都不是真对。</p></figure><p id="979d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">BERTs在这里的任务是准确地识别哪些对是真正的对，哪些不是。</p><p id="c195" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">还记得我说过我们可以用不同的头来训练伯特吗？NSP(和MLM)也用特殊的头。这里使用的head将来自分类器令牌的输出处理到一个密集的NN中，输出两个<em class="md">类</em>。</p><p id="cbab" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们的分类头密集层消耗来自分类任务中使用的<code class="fe nz oa ob oc b">[CLS]</code>(分类器)标记位置的输出。</p><p id="927e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个<code class="fe nz oa ob oc b">[CLS]</code>令牌的输出是一个768维的向量，它被传递给我们具有两个节点的密集NN层——我们的<code class="fe nz oa ob oc b">IsNextSentence</code>和<code class="fe nz oa ob oc b">NotNextSentence</code>类。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/bc4b6467b99315f4bcde5af05890500d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fu0TmlpjtFlQmacDzQbSvw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">BERT中NSP任务的高级视图。</p></figure><p id="ae85" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这两个输出是我们对BERT是否相信句子B在句子a之后的真/假预测。索引<strong class="la iu"> 0 </strong>告诉我们BERT相信句子B <em class="md">在句子a之后。</em></p><p id="c2a8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">训练后，NSP头被丢弃——我们保留的只是许多BERT层中经过微调的<em class="md">权重。</em></p><p id="fdb4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi lu translated"><span class="l lv lw lx bm ly lz ma mb mc di"> 2。</span> <strong class="la iu">屏蔽语言建模</strong>包括获取一大块文本，屏蔽给定数量的标记，并要求BERT预测被屏蔽的单词是什么。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/8f3c18c15771ff64cc5badabe52b130b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z4t68mbWdkDsbZpy6MV59A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过屏蔽操作处理原始文本，用<strong class="bd oe">【屏蔽】</strong>标记替换随机标记。</p></figure><p id="63a8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">每个序列中有15%的单词被<code class="fe nz oa ob oc b">[MASK]</code>标记屏蔽。</p><p id="05b0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一个分类头被附加到该模型上，并且每个标记将被馈送到一个前馈神经网络中，随后是一个softmax函数。每个令牌的输出维数等于vocab的大小。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4ec8df26ee2d93d78c2d8eb13bed54ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*phTLnQ8itb3ZX5_h9BWjWw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">MLM进程的高级视图。</p></figure><p id="7cb0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这意味着从每个记号位置，我们将得到最高概率记号的输出预测。我们用我们的词汇把它翻译成一个特定的单词。</p><p id="6e55" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在训练期间，当计算损失函数时，对未被屏蔽的记号的预测被忽略。</p><p id="2005" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">同样，与NSP一样，MLM <em class="md">头部</em>在训练后被丢弃——留给我们优化的模型权重。</p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><h1 id="eb80" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">用代码</h1><p id="918d" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">我们知道NSP和MLM的微调是如何工作的，但是我们如何在代码中应用它呢？</p><p id="b823" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">好了，我们可以从导入<em class="md">变形金刚</em>、<em class="md"> PyTorch </em>，以及我们的训练数据— <em class="md">冥想</em>(在这里找到训练数据<a class="ae nk" href="https://github.com/jamescalam/transformers/blob/main/data/text/meditations/clean.txt" rel="noopener ugc nofollow" target="_blank">的副本)开始。</a></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of mf l"/></div></figure><p id="77b2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们在<code class="fe nz oa ob oc b">text</code>中有一个段落列表——一些，但不是全部，包含多个句子。这是我们在构建NSP培训数据时需要的。</p><h2 id="ae94" class="nm mo it bd mp nn no dn mt np nq dp mx lh nr ns mz ll nt nu nb lp nv nw nd nx bi translated">为NSP做准备</h2><p id="8802" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">为了准备NSP的数据，我们需要创建一个非随机句子(两个句子最初在一起)和随机句子的混合。</p><p id="1270" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为此，我们将创建一个从<code class="fe nz oa ob oc b">text</code>中提取的<em class="md">句子包</em>，然后我们可以在创建随机<code class="fe nz oa ob oc b">NotNextSentence</code>对时从中随机选择一个句子。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of mf l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">我们的<strong class="ak">包</strong>包含与<strong class="ak">文本</strong>相同的数据，但是按句子分割——通过使用句点字符来标识。</p></figure><p id="e805" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在创建我们的<code class="fe nz oa ob oc b">bag</code>之后，我们可以继续创建我们的50/50随机/非随机NSP训练数据。为此，我们将创建一个句子As、句子Bs以及它们各自的<code class="fe nz oa ob oc b">IsNextSentence</code>或<code class="fe nz oa ob oc b">NotNextSentence</code>标签的列表。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of mf l"/></div></figure><p id="0ee7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以在控制台输出中看到，标签<em class="md"> 1 </em>表示随机句子(<code class="fe nz oa ob oc b">NotNextSentence</code>)，标签<em class="md"> 0 </em>表示非随机句子(<code class="fe nz oa ob oc b">IsNextSentence</code>)。</p><h2 id="0328" class="nm mo it bd mp nn no dn mt np nq dp mx lh nr ns mz ll nt nu nb lp nv nw nd nx bi translated">标记化</h2><p id="ec89" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">我们现在可以标记我们的数据。如同典型的BERT模型一样，我们将序列截断/填充到长度为<em class="md"> 512 </em>的记号。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of mf l"/></div></figure><p id="dcb7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里有几件事我们应该注意。因为我们标记了两个句子，所以我们的标记器在<em class="md"> token_type_ids </em>张量中自动将<em class="md"> 0 </em>值应用于句子A，将<em class="md"> 1 </em>值应用于句子B。尾随零与填充标记对齐。</p><p id="9198" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其次，在<em class="md"> input_ids </em>张量中，记号赋予器自动在这两个句子之间放置一个<em class="md"> SEP </em>记号(102)——标记两者之间的边界。</p><p id="45e1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">伯特在表演NSP的时候需要看到这两者。</p><h2 id="8b2c" class="nm mo it bd mp nn no dn mt np nq dp mx lh nr ns mz ll nt nu nb lp nv nw nd nx bi translated">NSP标签</h2><p id="f976" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">我们的NSP标签必须放在一个叫做<em class="md"> next_sentence_label </em>的张量中。我们通过获取我们的<code class="fe nz oa ob oc b">label</code>变量，并将其转换为<code class="fe nz oa ob oc b">torch.LongTensor</code>——也必须使用<code class="fe nz oa ob oc b">.T</code>进行转置，从而轻松地创建它:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of mf l"/></div></figure><h2 id="3aea" class="nm mo it bd mp nn no dn mt np nq dp mx lh nr ns mz ll nt nu nb lp nv nw nd nx bi translated">MLM的屏蔽</h2><p id="c0c0" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">对于MLM，我们需要<code class="fe nz oa ob oc b">clone</code>我们当前的<em class="md"> input_ids </em>张量来创建一个MLM <em class="md">标签</em>张量——然后我们移动到屏蔽<em class="md"> input_ids </em>张量中大约15%的记号。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of mf l"/></div></figure><p id="2bdd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们克隆了我们的<em class="md">标签</em>，我们屏蔽了<em class="md"> input_ids </em>中的令牌。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of mf l"/></div></figure><p id="e8b0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请注意，我们在这里添加了一些规则，通过在创建<code class="fe nz oa ob oc b">mask_arr</code>时添加额外的逻辑，我们确保不屏蔽任何特殊的令牌，例如<em class="md"> CLS </em> (101)、<em class="md"> SEP </em> (102)和<em class="md"> PAD </em> (0)令牌。</p><h2 id="fed8" class="nm mo it bd mp nn no dn mt np nq dp mx lh nr ns mz ll nt nu nb lp nv nw nd nx bi translated">数据加载器</h2><p id="1953" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">我们所有的输入和标签张量都准备好了——我们现在需要做的就是将它们格式化为PyTorch dataset对象，以便可以将它们加载到PyTorch Dataloader中——这将在训练期间向我们的模型提供批量数据。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of mf l"/></div></figure><p id="7877" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">数据加载器期望使用<code class="fe nz oa ob oc b">__len__</code>方法检查数据集中的样本总数，使用<code class="fe nz oa ob oc b">__getitem__</code>方法提取样本。</p><h2 id="fce0" class="nm mo it bd mp nn no dn mt np nq dp mx lh nr ns mz ll nt nu nb lp nv nw nd nx bi translated">培训设置</h2><p id="d361" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">进入训练循环之前的最后一步是准备模型训练设置。</p><p id="f400" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们首先检查我们是否有可用的GPU，如果有，我们将模型移到它上面进行训练。然后，我们激活模型中的训练参数，并用加权衰减初始化Adam优化器。</p><p id="5c65" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">培养</p><p id="cfd7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，我们开始训练我们的模型。我们训练两个纪元，并使用<code class="fe nz oa ob oc b">tqdm</code>为我们的训练循环创建一个进度条。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="of mf l"/></div></figure><p id="3b9f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在循环中，我们:</p><ul class=""><li id="aa13" class="og oh it la b lb lc le lf lh oi ll oj lp ok lt ol om on oo bi translated">初始化渐变，这样我们就不会从上一步计算的渐变开始。</li><li id="3a04" class="og oh it la b lb op le oq lh or ll os lp ot lt ol om on oo bi translated">将所有批量张量移动到选中的<code class="fe nz oa ob oc b">device</code> (GPU或CPU)。</li><li id="eecf" class="og oh it la b lb op le oq lh or ll os lp ot lt ol om on oo bi translated">将一切输入模型，提取损失。</li><li id="eb91" class="og oh it la b lb op le oq lh or ll os lp ot lt ol om on oo bi translated">使用<code class="fe nz oa ob oc b">loss.backward()</code>计算每个参数的损失。</li><li id="a0ad" class="og oh it la b lb op le oq lh or ll os lp ot lt ol om on oo bi translated">基于计算的损失更新参数权重。</li><li id="2744" class="og oh it la b lb op le oq lh or ll os lp ot lt ol om on oo bi translated">将相关信息打印到进度条(<code class="fe nz oa ob oc b">loop</code>)。</li></ul><p id="e226" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">就这样，我们用MLM和NSP对我们的模型进行了微调！</p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><p id="5801" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">关于使用屏蔽语言建模和下一个句子预测微调BERT的文章到此结束。我们已经介绍了什么是MLM和NSP，它们是如何工作的，以及我们如何用它们来微调我们的模型。</p><p id="d193" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有很多地方可以对BERT进行微调，但是概念和实现并不太复杂——同时功能非常强大。</p><p id="f3fa" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用我们在这里学到的知识，我们可以采用NLP中最好的模型，并对它们进行微调以适应我们更特定于领域的语言用例——只需要未标记的文本——通常是很容易找到的数据源。</p><p id="20e5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我希望你喜欢这篇文章！如果你有任何问题，请通过<a class="ae nk" href="https://twitter.com/jamescalam" rel="noopener ugc nofollow" target="_blank"> Twitter </a>或在下面的评论中告诉我。如果你想要更多这样的内容，我也会在YouTube上发布。</p><p id="434f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢阅读！</p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><h1 id="5a9a" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">参考</h1><p id="a9dd" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">[1] B. Dickson，<a class="ae nk" href="https://bdtechtalks.com/2020/08/17/openai-gpt-3-commercial-ai/" rel="noopener ugc nofollow" target="_blank">《GPT 3号未披露的故事》是OpenAI的转变</a> (2020)，TechTalks</p><p id="adb5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[2] K. Wiggers，<a class="ae nk" href="https://venturebeat.com/2020/06/01/ai-machine-learning-openai-gpt-3-size-isnt-everything/" rel="noopener ugc nofollow" target="_blank"> OpenAI巨大的GPT-3模型令人印象深刻，但大小并不代表一切</a> (2020)，VentureBeat</p><p id="8177" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae nk" href="https://github.com/jamescalam/transformers/blob/main/course/training/08_mlm_and_nsp_training.ipynb" rel="noopener ugc nofollow" target="_blank">朱庇特笔记本</a></p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><p id="f32d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你有兴趣了解更多关于MLM和NSP背后的逻辑，以及一般的变形金刚，请查看我的NLP变形金刚课程:</p><p id="7998" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae nk" href="https://bit.ly/nlp-transformers" rel="noopener ugc nofollow" target="_blank">🤖《变形金刚》NLP课程70%的折扣</a></p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><p id="4b39" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="md">*所有图片均由作者提供，除非另有说明</em></p></div></div>    
</body>
</html>