<html>
<head>
<title>Popular Downstream Tasks for Video Representation Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">视频表征学习的流行下游任务</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/popular-downstream-tasks-for-video-representation-learning-8edbd8dc19c1?source=collection_archive---------31-----------------------#2021-06-09">https://towardsdatascience.com/popular-downstream-tasks-for-video-representation-learning-8edbd8dc19c1?source=collection_archive---------31-----------------------#2021-06-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="dd3f" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><em class="ki">用于评估长教学视频的视频表示学习的常见下游任务的总结。</em></h2></div><h1 id="6a6a" class="kj kk it bd kl km kn ko kp kq kr ks kt jz ku ka kv kc kw kd kx kf ky kg kz la bi translated"><strong class="ak">什么是表征学习？</strong></h1><p id="9edc" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">表征学习是一个研究领域，其重点是如何学习不同信号源的紧凑的数字表征。这些信号通常是视频、文本、音频和图像。这项研究的目标是将这些表征用于其他任务，如查询信息。一个众所周知的例子是在YouTube上搜索视频时:用户提供文本关键词，导致YouTube返回一组与这些词最相似的视频。</p><p id="4b7e" class="pw-post-body-paragraph lb lc it ld b le lx ju lg lh ly jx lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">在计算机视觉文献中，通过训练深度学习模型将原始输入嵌入(或转换)到数字向量来学习表示。当嵌入视频、音频或文本时，数字向量通常是多维的，以保持时间关系。研究人员训练这些模型的方式千差万别。下游任务是如何评估这些方法，也是本文的重点。</p><h1 id="c20b" class="kj kk it bd kl km kn ko kp kq kr ks kt jz ku ka kv kc kw kd kx kf ky kg kz la bi translated">数据集</h1><p id="7c9a" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">我看过的很多论文都是用数据集<a class="ae mc" href="https://www.di.ens.fr/willow/research/howto100m/" rel="noopener ugc nofollow" target="_blank"><strong class="ld iu">how to 100m</strong></a><strong class="ld iu"/>来训练模型的。这个语料库总共包含<em class="md">120万个视频</em>，活动范围23k！活动的类型多种多样，从烹饪到手工制作到健身到园艺等等。这个数据集非常庞大，普通研究人员需要很长时间来训练一个模型。然而，如果你有计算能力，它是一个用于表示学习的很好的数据集。每个视频都包含任务的简短描述和一组自动生成的字幕。根据我的经验，以及最初的研究人员的经验，字幕噪音很大，存在对齐问题，音频到文本的翻译也不准确。简短的任务描述并不总是准确或者非常笼统。然而，这是因为YouTube的提取，而不是研究人员的错。</p><p id="4dc9" class="pw-post-body-paragraph lb lc it ld b le lx ju lg lh ly jx lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">对于每个下游任务，都有数据集，这些数据集具有特定于任务评估的注释。这些数据集有一个较小的视频集，集中于一个较小的活动集。</p><h2 id="ae9f" class="me kk it bd kl mf mg dn kp mh mi dp kt lk mj mk kv lo ml mm kx ls mn mo kz mp bi translated">数据集</h2><p id="f415" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">适用于<em class="md">文本相关视频任务</em>的数据集有:</p><p id="ef01" class="pw-post-body-paragraph lb lc it ld b le lx ju lg lh ly jx lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated"><a class="ae mc" href="http://youcook2.eecs.umich.edu/" rel="noopener ugc nofollow" target="_blank"><strong class="ld iu">【you cook 2】</strong></a>是一个基于烹饪的数据集，包含89个烹饪食谱的2K个未经剪辑的视频，并带有逐步注释。该数据集还包括对时间目标任务有用的时间边界。<a class="ae mc" href="https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-description-dataset-for-bridging-video-and-language/" rel="noopener ugc nofollow" target="_blank"> <strong class="ld iu"> MSR-VTT </strong> </a>是一个更通用的10K视频剪辑数据集，具有257个不同主题的20万个人类注释剪辑-字幕对。虽然没有具体的指导性，但<a class="ae mc" href="https://sites.google.com/site/describingmovies/" rel="noopener ugc nofollow" target="_blank"> <strong class="ld iu"> LSMDC </strong> </a>数据集包含101k个唯一的视频剪辑-字幕对，其描述来自电影脚本或音频描述。</p><p id="c393" class="pw-post-body-paragraph lb lc it ld b le lx ju lg lh ly jx lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">更常用于<em class="md">动作相关视频任务</em>的数据集有:</p><p id="1655" class="pw-post-body-paragraph lb lc it ld b le lx ju lg lh ly jx lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated"><a class="ae mc" href="https://github.com/DmZhukov/CrossTask" rel="noopener ugc nofollow" target="_blank"><strong class="ld iu">CrossTask</strong></a><strong class="ld iu"/>包含2.7k的教学视频，每一帧都有分步动作注释。<a class="ae mc" href="https://coin-dataset.github.io/" rel="noopener ugc nofollow" target="_blank"> <strong class="ld iu">硬币</strong> </a> <strong class="ld iu"> </strong>也是一个通用的教学视频数据集，具有11，827个视频，显示了180个任务，带有与视频中发生的动作相关联的注释。</p><h1 id="2ba5" class="kj kk it bd kl km kn ko kp kq kr ks kt jz ku ka kv kc kw kd kx kf ky kg kz la bi translated">下游任务</h1><h2 id="6381" class="me kk it bd kl mf mg dn kp mh mi dp kt lk mj mk kv lo ml mm kx ls mn mo kz mp bi translated">视频/动作分类</h2><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mq"><img src="../Images/87296d0a752fb47f9ca2881796cbd8da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4iKhI-lWYxW09cwdjt4KhQ.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图一。特征提取表示通过原始表示学习模型传递视频。为了预测活性，增加了一个小的前馈网络。</p></figure><p id="3367" class="pw-post-body-paragraph lb lc it ld b le lx ju lg lh ly jx lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">要设置的最快的下游任务是对整个视频或修剪后的版本进行分类。这项任务的目标是对给定的视频进行高精度的分类，使其显示复杂的动作。目标是所有类别的平均准确度都很高。一些有助于展示这一空间的可视化方法是聚类。理想情况下，来自同一个类的视频应该在同一个集群附近。为了修改这个下游任务的原始模型，您添加了一个输出<em class="md">d =类的数量的小型前馈网络。</em>一个<strong class="ld iu">零触发</strong>实验意味着你不在下游数据集上训练这个修改过的网络，而只是测试。然而，更常见的是，训练一些时期来更新新层的随机参数。测量精度基于视频总数中准确分类的视频数量。</p><h2 id="518e" class="me kk it bd kl mf mg dn kp mh mi dp kt lk mj mk kv lo ml mm kx ls mn mo kz mp bi translated">文本到视频检索</h2><p id="e3e8" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">该任务的目标是使用文本查询来检索匹配的视频。将文本查询的特征向量与视频池的学习特征进行比较，并且地面实况匹配应该是最相似的视频。另一个相关的任务是<strong class="ld iu">文本到完整视频检索</strong>。该任务使用<em class="md">多个</em>标题来描述视频的多个部分，从而检索视频。通过将视频池传递通过训练的模型来提取特征。这项任务需要一个具有文本-视频配对的多模态数据集，以便将文本和视频投影到同一个“空间”中。<em class="md">回忆</em>准确性是这项任务最常见的表现衡量标准。R@1测量最相似的视频的平均准确度，R@5测量最相似的前5个视频之一的平均准确度，最后R@10是前10个视频之一。另外两个常用的指标是<em class="md"> median-R </em>和<em class="md"> mean-R </em>，用于衡量视频池中检索到的真实结果的中值和平均排名，越小越好。</p><figure class="mr ms mt mu gt mv gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi ng"><img src="../Images/b4d88f5b49f453d0743a401ca74a1866.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RSubpHsUgo7H2FL5NaF2Hw.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">图二。使用训练好的模型，提取所有视频的验证数据集的特征。为文本查询和视觉特征添加一组编码层(如果原始模型仅使用视频),以将两者嵌入同一空间。这通常是为少数时代训练的。完成后，将使用新图层嵌入文本查询和视频。基于文本查询和所有视频表示之间的相似性对视频进行排序。</p></figure><h2 id="16fe" class="me kk it bd kl mf mg dn kp mh mi dp kt lk mj mk kv lo ml mm kx ls mn mo kz mp bi translated">时间动作定位</h2><p id="67d9" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">此任务的目标是在视频剪辑中找到一个动作。给定一个视频片段和一个无序的动作列表，任务是在视频流中找到这些动作。这种设置非常类似于帧级动作分类，只是增加了一个前馈网络。这与图1所示的过程相同，但是视频片段更小。它也可以是基于相似性的，比如在检索中，将活动的嵌入与片段级的嵌入进行比较。这与图2所示的过程相同，但是查询是用动作而不是关键字。用于该任务的度量可以是召回率、准确度、交叉检测和交叉合并(参见<a class="ae mc" href="https://arxiv.org/pdf/1906.01012.pdf" rel="noopener ugc nofollow" target="_blank"> Kouhne等人</a> 2019和<a class="ae mc" href="https://arxiv.org/abs/2104.12671" rel="noopener ugc nofollow" target="_blank"> Chen等人</a>2021)。</p><h2 id="6728" class="me kk it bd kl mf mg dn kp mh mi dp kt lk mj mk kv lo ml mm kx ls mn mo kz mp bi translated">视频字幕</h2><p id="d8ae" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">此任务的目标是将原始视频转换为一个简短的文本描述，或一组简短的句子描述。这对于出于查询或基于知识的原因总结视频很有用。这项任务通常使用变压器解码器将视频解码成字幕(见<a class="ae mc" href="https://arxiv.org/abs/2011.07231" rel="noopener ugc nofollow" target="_blank">朱和杨2020 </a>)。关于变形金刚的更多细节，请看<a class="ae mc" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank"> <em class="md">图解变形金刚</em> </a> <em class="md">。</em>这类似于图1，但用变压器代替了前馈网络。像PyTorch这样的编程框架有<a class="ae mc" href="https://pytorch.org/docs/master/nn.html?highlight=nn%20transformer#transformer-layers" rel="noopener ugc nofollow" target="_blank">层</a>和<a class="ae mc" href="https://pytorch.org/tutorials/beginner/transformer_tutorial.html" rel="noopener ugc nofollow" target="_blank">教程</a>可以用于这个下游任务。基于NLP的指标用于评估性能，包括BLEU、METEOR、ROUGE和CIDEr。这些指标使用n元语法重叠、最佳匹配长度和词序来比较模型的输出标题和真实标题。</p><h2 id="2791" class="me kk it bd kl mf mg dn kp mh mi dp kt lk mj mk kv lo ml mm kx ls mn mo kz mp bi translated">视频问答</h2><p id="0804" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">此任务的目标是选择多项选择题的正确答案。这是通过将视频与每个可能答案的组合分别输入模型来实现的。附加层是线性的，并且使用视频和文本表示来预测候选对是正确选择的可能性。最常用的衡量标准是准确性。此任务测量您学习的功能完成基于推理的任务的能力。</p><h1 id="cf51" class="kj kk it bd kl km kn ko kp kq kr ks kt jz ku ka kv kc kw kd kx kf ky kg kz la bi translated">摘要</h1><p id="714b" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><a class="ae mc" href="https://paperswithcode.com/task/action-classification" rel="noopener ugc nofollow" target="_blank"> <strong class="ld iu">动作分类</strong> </a> <strong class="ld iu"> : </strong>将整个视频分类为一个复杂的动作。</p><ul class=""><li id="32ab" class="nh ni it ld b le lx lh ly lk nj lo nk ls nl lw nm nn no np bi translated"><em class="md">方法</em>:基于特征的线性分类器。</li><li id="0fd8" class="nh ni it ld b le nq lh nr lk ns lo nt ls nu lw nm nn no np bi translated"><em class="md">数据集</em> : <a class="ae mc" href="http://youcook2.eecs.umich.edu/" rel="noopener ugc nofollow" target="_blank"> YouCook2 </a>，<a class="ae mc" href="https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-description-dataset-for-bridging-video-and-language/" rel="noopener ugc nofollow" target="_blank"> MSRVTT </a>，<a class="ae mc" href="https://coin-dataset.github.io/" rel="noopener ugc nofollow" target="_blank">硬币</a>，<a class="ae mc" href="http://activity-net.org/" rel="noopener ugc nofollow" target="_blank">活动网</a></li><li id="18b1" class="nh ni it ld b le nq lh nr lk ns lo nt ls nu lw nm nn no np bi translated"><em class="md">度量</em>:平均精度(mAP)和准确度</li><li id="482a" class="nh ni it ld b le nq lh nr lk ns lo nt ls nu lw nm nn no np bi translated"><em class="md">示例</em> : W-TALC:弱监督时间活动定位和分类，VideoBERT:视频和语言表征学习的联合模型</li></ul><p id="f605" class="pw-post-body-paragraph lb lc it ld b le lx ju lg lh ly jx lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated"><a class="ae mc" href="https://paperswithcode.com/task/video-retrieval" rel="noopener ugc nofollow" target="_blank"> <strong class="ld iu">文本到视频检索</strong> </a>:给定真实文本描述，从视频池中检索匹配视频。</p><ul class=""><li id="486a" class="nh ni it ld b le lx lh ly lk nj lo nk ls nl lw nm nn no np bi translated"><em class="md">方法</em>:测量文本查询和视频特征数据库之间的相似度。</li><li id="1db9" class="nh ni it ld b le nq lh nr lk ns lo nt ls nu lw nm nn no np bi translated"><em class="md">数据集</em> : <a class="ae mc" href="http://youcook2.eecs.umich.edu/" rel="noopener ugc nofollow" target="_blank"> YouCook2 </a>，<a class="ae mc" href="https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-description-dataset-for-bridging-video-and-language/" rel="noopener ugc nofollow" target="_blank"> MSRVTT </a>，<a class="ae mc" href="https://coin-dataset.github.io/" rel="noopener ugc nofollow" target="_blank">硬币</a>，<a class="ae mc" href="http://activity-net.org/" rel="noopener ugc nofollow" target="_blank">活动网</a>，<a class="ae mc" href="https://sites.google.com/site/describingmovies/" rel="noopener ugc nofollow" target="_blank"> LSMDC </a></li><li id="eb68" class="nh ni it ld b le nq lh nr lk ns lo nt ls nu lw nm nn no np bi translated"><em class="md">指标</em> : R@1，R@5，R@10，中值-R</li><li id="2c4b" class="nh ni it ld b le nq lh nr lk ns lo nt ls nu lw nm nn no np bi translated"><em class="md">示例</em>:从无标签视频进行自我监督学习的多模态聚类网络(2021)，ActBERT:学习全局-局部视频-文本表示(2020)，HowTo100M:通过观看一亿个带叙述的视频剪辑学习文本-视频嵌入(2019)</li></ul><p id="018b" class="pw-post-body-paragraph lb lc it ld b le lx ju lg lh ly jx lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated"><a class="ae mc" href="https://paperswithcode.com/task/action-recognition" rel="noopener ugc nofollow" target="_blank"> <strong class="ld iu">时间动作定位</strong> </a>:给定一段视频和片段中出现的动作步骤的无序列表，在视频流中找到那些动作。<strong class="ld iu">行动步骤本地化</strong>使用有序的活动列表。</p><ul class=""><li id="5dce" class="nh ni it ld b le lx lh ly lk nj lo nk ls nl lw nm nn no np bi translated"><em class="md">方法</em>:动作嵌入和视频嵌入之间的片段级分类或相似度。</li><li id="24d3" class="nh ni it ld b le nq lh nr lk ns lo nt ls nu lw nm nn no np bi translated"><em class="md">数据集</em> : <a class="ae mc" href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhukov_Cross-Task_Weakly_Supervised_Learning_From_Instructional_Videos_CVPR_2019_paper.pdf" rel="noopener ugc nofollow" target="_blank">交叉任务</a>，<a class="ae mc" href="http://activity-net.org/" rel="noopener ugc nofollow" target="_blank">活动网</a>，<a class="ae mc" href="https://arxiv.org/abs/1906.01012" rel="noopener ugc nofollow" target="_blank">挖掘YouTube </a></li><li id="710f" class="nh ni it ld b le nq lh nr lk ns lo nt ls nu lw nm nn no np bi translated"><em class="md">指标</em>:召回</li><li id="2c50" class="nh ni it ld b le nq lh nr lk ns lo nt ls nu lw nm nn no np bi translated"><em class="md">示例</em> : HowTo100M:通过观看一亿个叙述视频剪辑学习文本-视频嵌入(2019)和ActBERT:学习全球-本地视频-文本表示(2020)</li></ul><p id="091c" class="pw-post-body-paragraph lb lc it ld b le lx ju lg lh ly jx lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated"><a class="ae mc" href="https://paperswithcode.com/task/action-segmentation" rel="noopener ugc nofollow" target="_blank"> <strong class="ld iu">动作分割</strong> </a>:在帧或片段级别对子动作进行分类。</p><ul class=""><li id="ce03" class="nh ni it ld b le lx lh ly lk nj lo nk ls nl lw nm nn no np bi translated"><em class="md">方法</em>:帧级/片段级分类。</li><li id="ec12" class="nh ni it ld b le nq lh nr lk ns lo nt ls nu lw nm nn no np bi translated"><em class="md">数据集</em> : <a class="ae mc" href="https://coin-dataset.github.io/" rel="noopener ugc nofollow" target="_blank">硬币</a>，<a class="ae mc" href="http://activity-net.org/" rel="noopener ugc nofollow" target="_blank">活动网</a></li><li id="141b" class="nh ni it ld b le nq lh nr lk ns lo nt ls nu lw nm nn no np bi translated"><em class="md">指标</em>:准确性</li><li id="45ce" class="nh ni it ld b le nq lh nr lk ns lo nt ls nu lw nm nn no np bi translated"><em class="md">示例</em> : ActBERT:学习全球-本地视频-文本表示(2020)</li></ul><p id="eadc" class="pw-post-body-paragraph lb lc it ld b le lx ju lg lh ly jx lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated"><a class="ae mc" href="https://paperswithcode.com/task/video-captioning" rel="noopener ugc nofollow" target="_blank"> <strong class="ld iu">视频字幕</strong> </a>:将视频转换成一段简短的文字描述或一组字幕。</p><ul class=""><li id="c067" class="nh ni it ld b le lx lh ly lk nj lo nk ls nl lw nm nn no np bi translated"><em class="md">方法</em>:变压器解码器</li><li id="4631" class="nh ni it ld b le nq lh nr lk ns lo nt ls nu lw nm nn no np bi translated"><em class="md">数据集</em> : <a class="ae mc" href="https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-description-dataset-for-bridging-video-and-language/" rel="noopener ugc nofollow" target="_blank"> MSRVTT </a>，<a class="ae mc" href="http://youcook2.eecs.umich.edu/" rel="noopener ugc nofollow" target="_blank"> YouCook2 </a></li><li id="eac8" class="nh ni it ld b le nq lh nr lk ns lo nt ls nu lw nm nn no np bi translated"><em class="md">指标</em>:蓝色、流星、苹果酒、胭脂</li><li id="cd4e" class="nh ni it ld b le nq lh nr lk ns lo nt ls nu lw nm nn no np bi translated"><em class="md">例子</em> : ActBERT:学习全球-本地视频-文本表示(2020)</li></ul><p id="fb77" class="pw-post-body-paragraph lb lc it ld b le lx ju lg lh ly jx lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated"><a class="ae mc" href="https://paperswithcode.com/task/video-question-answering" rel="noopener ugc nofollow" target="_blank"> <strong class="ld iu">视频问答</strong> </a> : <strong class="ld iu"> </strong>根据一个问题和一组选择题答案，能够选出正确答案。</p><ul class=""><li id="0d20" class="nh ni it ld b le lx lh ly lk nj lo nk ls nl lw nm nn no np bi translated"><em class="md">方法</em>:将每个带视频的选择题答案候选人送入线性分类器，对问题的正确答案进行分类。</li><li id="e287" class="nh ni it ld b le nq lh nr lk ns lo nt ls nu lw nm nn no np bi translated"><em class="md">数据集</em> : <a class="ae mc" href="https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-description-dataset-for-bridging-video-and-language/" rel="noopener ugc nofollow" target="_blank"> MSRVTT </a></li><li id="ef6d" class="nh ni it ld b le nq lh nr lk ns lo nt ls nu lw nm nn no np bi translated"><em class="md">指标</em>:准确性</li><li id="ec34" class="nh ni it ld b le nq lh nr lk ns lo nt ls nu lw nm nn no np bi translated"><em class="md">示例</em> : ActBERT:学习全球-本地视频-文本表示(2020)</li></ul><p id="dd6f" class="pw-post-body-paragraph lb lc it ld b le lx ju lg lh ly jx lj lk lz lm ln lo ma lq lr ls mb lu lv lw im bi translated">更多的例子和信息，请查看代码为的<a class="ae mc" href="https://paperswithcode.com/" rel="noopener ugc nofollow" target="_blank">论文。</a></p></div></div>    
</body>
</html>