<html>
<head>
<title>PCA and OLS in matrix form with R</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">矩阵形式的主成分分析和OLS</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/statistical-analysis-1a291f4bff3a?source=collection_archive---------19-----------------------#2021-04-11">https://towardsdatascience.com/statistical-analysis-1a291f4bff3a?source=collection_archive---------19-----------------------#2021-04-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0766" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用R的线性代数运算:主成分分析和普通最小二乘法</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/49ee8871f1fb4d6736a3681ed6832cbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AJpeleur-0rbjnlg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">弗拉多·帕诺维奇在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="903e" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">介绍</h1><p id="dce8" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">主成分分析(PCA)和普通最小二乘法(OLS)是两种重要的统计方法。他们在一起表演时甚至更好。我们将使用R中的矩阵运算来探索这些方法，并介绍一种基本的主成分回归(PCR)技术。</p><h1 id="c090" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">数据生成</h1><p id="6495" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们将从高斯分布中生成一个简单的数据集，该数据集包含四个高度相关的探索性变量，以及一个响应变量，该响应变量将是它们与添加的随机噪声的线性组合。</p><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="0003" class="mp kx iq ml b gy mq mr l ms mt">&gt; library(‘MASS’)<br/><br/>&gt; mu=rep(3,4)<br/>&gt; sigma=matrix(.9, nrow=4, ncol=4) + diag(4)*0.1</span><span id="8675" class="mp kx iq ml b gy mu mr l ms mt">&gt; set.seed(2021)<br/>&gt; data &lt;- as.data.frame(mvrnorm(20, mu = mu, Sigma = sigma), <br/>+ empirical = T)</span><span id="02af" class="mp kx iq ml b gy mu mr l ms mt">&gt; y &lt;- apply(data, 1, sum)+rnorm(20, 1, 1)</span></pre><p id="386d" class="pw-post-body-paragraph lo lp iq lq b lr mv jr lt lu mw ju lw lx mx lz ma mb my md me mf mz mh mi mj ij bi translated">我们可以观察相关矩阵，并确认探索变量是高度相关的。在这种情况下，回归系数可能会有偏差。</p><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="b9cd" class="mp kx iq ml b gy mq mr l ms mt">&gt; cor(data)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi na"><img src="../Images/083a7a94ba2d03615ac500eef74d8533.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wH0l4F31HqRutcLthtlQLw.png"/></div></div></figure><h1 id="da24" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">主成分分析</h1><p id="71f5" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">这种统计方法有助于我们处理多重共线性和高维度。使用R，我们可能只需要4个步骤就可以获得PC。</p><p id="3a57" class="pw-post-body-paragraph lo lp iq lq b lr mv jr lt lu mw ju lw lx mx lz ma mb my md me mf mz mh mi mj ij bi translated"><strong class="lq ir">首先</strong>，我们需要对数据进行缩放。在我们的例子中，我们知道数据具有相同的数量级；因此，我们只将其居中。</p><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="6552" class="mp kx iq ml b gy mq mr l ms mt">data.scaled &lt;-  scale(data, scale=F)</span></pre><p id="ad41" class="pw-post-body-paragraph lo lp iq lq b lr mv jr lt lu mw ju lw lx mx lz ma mb my md me mf mz mh mi mj ij bi translated"><strong class="lq ir">第二个</strong>，我们需要计算协方差矩阵:</p><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="de71" class="mp kx iq ml b gy mq mr l ms mt">data.cov &lt;- cov(data.scaled)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nb"><img src="../Images/e38132ecd8fba4e14342468b488b806f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gmbk9t5-ov_jIOvTv-g1Og.png"/></div></div></figure><p id="2342" class="pw-post-body-paragraph lo lp iq lq b lr mv jr lt lu mw ju lw lx mx lz ma mb my md me mf mz mh mi mj ij bi translated"><strong class="lq ir">第三个</strong>，我们需要计算特征值来获得特征向量。在R中，函数eigen()返回两个结果。特征值是标度参数；特征向量是旋转参数。</p><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="87c9" class="mp kx iq ml b gy mq mr l ms mt">data.eigen &lt;- eigen(data.cov)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi na"><img src="../Images/88a09334196262932df74b5ad56ad9be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eQnPCtxoVzZwpkKE33HjiQ.png"/></div></div></figure><p id="bf5a" class="pw-post-body-paragraph lo lp iq lq b lr mv jr lt lu mw ju lw lx mx lz ma mb my md me mf mz mh mi mj ij bi translated"><strong class="lq ir">最后</strong>，我们需要执行数据和特征向量的矩阵乘法。结果将产生主成分:这些是新轴上原始数据的坐标。</p><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="07bb" class="mp kx iq ml b gy mq mr l ms mt">&gt; data.custom.pca &lt;- data.scaled%*%(data.eigen$vectors)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/ac68ededb3375bd347e45d535d468100.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wuvFA_KI_CszAkULSGnnIw.png"/></div></div></figure><p id="3091" class="pw-post-body-paragraph lo lp iq lq b lr mv jr lt lu mw ju lw lx mx lz ma mb my md me mf mz mh mi mj ij bi translated">这些分量是正交的，我们可以通过观察相关矩阵来证实这一点:</p><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="3778" class="mp kx iq ml b gy mq mr l ms mt">&gt; round(cor(data.custom.pca),5)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/e9310d2f85e6cd879a34a4368fa71257.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L81nLXSlQDJ0WtR6GsjTgw.png"/></div></div></figure><p id="b3ac" class="pw-post-body-paragraph lo lp iq lq b lr mv jr lt lu mw ju lw lx mx lz ma mb my md me mf mz mh mi mj ij bi translated">我们还可以计算出每个成分所占的解释变量的比例。第一个成分获得了93.88%的变异；第一+第二分量捕获97.06%，以此类推。</p><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="8dc5" class="mp kx iq ml b gy mq mr l ms mt">&gt; cumsum(data.eigen$values)/sum(data.eigen$values)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/c2af56e59020be65fce8440be1b53621.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*39jwO1yCIcKWjbM8CSxplQ.png"/></div></div></figure><h2 id="72ee" class="mp kx iq bd ky nf ng dn lc nh ni dp lg lx nj nk li mb nl nm lk mf nn no lm np bi translated">现在，让我们检查内置函数prcomp()。</h2><p id="3240" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">该功能自动化了我们之前执行的四步方法。从下面的结果中，我们可以看到，这些组件与我们刚刚获得的组件是等效的:</p><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="1af9" class="mp kx iq ml b gy mq mr l ms mt">&gt; data.pca &lt;- prcomp(data)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/d460dfaec17e3e8c1d3305b41be5194e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SVk-eQIRoq37LwGaQXsubg.png"/></div></div></figure><p id="0d6e" class="pw-post-body-paragraph lo lp iq lq b lr mv jr lt lu mw ju lw lx mx lz ma mb my md me mf mz mh mi mj ij bi translated">一些组件具有相反的符号，但是如果我们检查相应的特征向量(在这个包中称为旋转)，我们会注意到它们的符号也是相反的，当我们将它们相乘以获得原始数据时，一切都会很好。</p><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="d456" class="mp kx iq ml b gy mq mr l ms mt">&gt; data.pca$rotation</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/d78ce62b5391a658187d757abb5fb790.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SpRJ3jt6bDy-bihJIs47bQ.png"/></div></div></figure><p id="036e" class="pw-post-body-paragraph lo lp iq lq b lr mv jr lt lu mw ju lw lx mx lz ma mb my md me mf mz mh mi mj ij bi translated">特征值可以通过平方该包的标准偏差来获得。</p><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="d3f2" class="mp kx iq ml b gy mq mr l ms mt">&gt; data.pca$sdev^2</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nb"><img src="../Images/45601c2c2c38ed171164c76316a272f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ub2Nk0oCmjlHxfrH1UhOEA.png"/></div></div></figure><p id="ecf1" class="pw-post-body-paragraph lo lp iq lq b lr mv jr lt lu mw ju lw lx mx lz ma mb my md me mf mz mh mi mj ij bi translated">最后，summary函数将返回组件的重要性和每个组件捕获的方差的比例。我们可以观察到结果与我们以前的估计是一致的。</p><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="0c97" class="mp kx iq ml b gy mq mr l ms mt">&gt; summary(data.pca)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/eb0a8c4fde500008d401210fd1270eb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jpan7z5JQ0hhlbaA23k1Ew.png"/></div></div></figure><h1 id="445e" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">普通最小二乘法</h1><p id="dae5" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在没有进一步细节的情况下，回归方程的系数可以使用以下公式获得:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/eecc6eae3c40c30860490a2125f5c29c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kawFWjB8Ddf0Hpv2mTXL_Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://en.wikipedia.org/wiki/Ordinary_least_squares" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Ordinary_least_squares</a></p></figure><p id="c26a" class="pw-post-body-paragraph lo lp iq lq b lr mv jr lt lu mw ju lw lx mx lz ma mb my md me mf mz mh mi mj ij bi translated">要计算它，我们需要知道R中的三个运算:如何求逆矩阵，如何转置矩阵，如何乘矩阵:</p><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="b72c" class="mp kx iq ml b gy mq mr l ms mt"># Matrix multiplication:  <!-- -->%*% <br/># Transpose matrix:       t(x)<br/># Invert matrix:          solve(x)</span></pre><p id="d10a" class="pw-post-body-paragraph lo lp iq lq b lr mv jr lt lu mw ju lw lx mx lz ma mb my md me mf mz mh mi mj ij bi translated">但在我们最终确定我们的方程之前，我们需要记住，这个公式假设x0 = 1，因此我们要在数据中添加一个单位列。然后，我们以矩阵形式运行该方程，并获得系数:</p><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="2d97" class="mp kx iq ml b gy mq mr l ms mt">&gt; x &lt;- as.matrix(cbind(1,data))</span><span id="ec4f" class="mp kx iq ml b gy mu mr l ms mt">&gt; solve(t(x)%*%x) %*% t(x) %*% y</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/a0f088c89819a72c0509f3aca4066fed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a6UqjvL43XpKPlBIfieVMA.png"/></div></div></figure><p id="1876" class="pw-post-body-paragraph lo lp iq lq b lr mv jr lt lu mw ju lw lx mx lz ma mb my md me mf mz mh mi mj ij bi translated">我们可以看到，R中的这个方程直接重复了我们在题目开始时提到的方程。让我们将它与R中的内置函数进行比较:</p><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="18f0" class="mp kx iq ml b gy mq mr l ms mt">&gt; model &lt;- lm(y~x[,2:5])</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/4f4aa79dcd3769ebee26a0698f7c597e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SjtSjstwheK9BNDqq1f-6A.png"/></div></div></figure><p id="6605" class="pw-post-body-paragraph lo lp iq lq b lr mv jr lt lu mw ju lw lx mx lz ma mb my md me mf mz mh mi mj ij bi translated">这两种方法的系数是相同的，因为它本质上是相同的方法。</p><p id="0722" class="pw-post-body-paragraph lo lp iq lq b lr mv jr lt lu mw ju lw lx mx lz ma mb my md me mf mz mh mi mj ij bi translated">现在我们知道了PCA和OLS方法是如何在简单数据集上执行的。这就完成了我们下一个主题的先决条件。</p><h1 id="83a7" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">主成分回归</h1><p id="c6b1" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们将使用我们之前获得的PC，因为它们对于某些成分具有不同的符号，我们将显示，当我们将数据转换为原始形式时，系数将是相等的。</p><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="bd2e" class="mp kx iq ml b gy mq mr l ms mt"># Adding components from both methods to the data set</span><span id="e3d2" class="mp kx iq ml b gy mu mr l ms mt">data.new &lt;-  cbind(data, data.custom.pca, data.pca$x, y)<br/>colnames(data.new)[5:8] &lt;- c('cust1', 'cust2', 'cust3', 'cust4')</span></pre><p id="79c5" class="pw-post-body-paragraph lo lp iq lq b lr mv jr lt lu mw ju lw lx mx lz ma mb my md me mf mz mh mi mj ij bi translated">出于演示目的，将使用所有四种成分来估计模型，即使通常在PCA之后我们减少预测因子的数量:</p><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="ba12" class="mp kx iq ml b gy mq mr l ms mt">model.cust &lt;- lm(y~cust1+cust2+cust3+cust4, data.new)</span><span id="5e2f" class="mp kx iq ml b gy mu mr l ms mt">model.pca &lt;- lm(y~PC1+PC2+PC3+PC4, data.new)</span></pre><p id="2146" class="pw-post-body-paragraph lo lp iq lq b lr mv jr lt lu mw ju lw lx mx lz ma mb my md me mf mz mh mi mj ij bi translated">现在我们需要将分量旋转的系数转换到原始空间。这可以通过将它们乘以相应的特征向量来实现:</p><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="a208" class="mp kx iq ml b gy mq mr l ms mt">beta.pca &lt;- as.matrix(model.pca$coefficients[2:5])<br/>beta.original.pca &lt;- as.matrix(data.pca$rotation) %*% beta.pca</span><span id="8977" class="mp kx iq ml b gy mu mr l ms mt">beta.cust.pca &lt;- as.matrix(model.cust$coefficients[2:5])<br/>beta.original.cust &lt;- as.matrix(data.custom.eigenvector) %*% beta.cust.pca</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/835b97a1d6f7e6d2da8d15d3c9a0f250.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M29XYWU-c170vMrVuOASUQ.png"/></div></div></figure><p id="1e9f" class="pw-post-body-paragraph lo lp iq lq b lr mv jr lt lu mw ju lw lx mx lz ma mb my md me mf mz mh mi mj ij bi translated">这证实了当我们将点变换到原始系统时，PCs的相反符号将由特征向量的符号补偿。</p><h2 id="d05a" class="mp kx iq bd ky nf ng dn lc nh ni dp lg lx nj nk li mb nl nm lk mf nn no lm np bi translated">现在，让我们检查来自库(pls)的内置函数pcr()</h2><p id="f7d5" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">函数pcr()有几个参数需要调整，但是我们将构造最基本的版本，并获得系数。</p><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="da9e" class="mp kx iq ml b gy mq mr l ms mt">library(pls)</span><span id="e032" class="mp kx iq ml b gy mu mr l ms mt">fit &lt;- pcr(y ~.,  data = as.data.frame(data.scaled), center=F)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/16675995e97d80fc93204aa9e6ca52ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UjzVQQ0eg7FtYBf0202_cw.png"/></div></div></figure><p id="e3b4" class="pw-post-body-paragraph lo lp iq lq b lr mv jr lt lu mw ju lw lx mx lz ma mb my md me mf mz mh mi mj ij bi translated">该函数计算所有PC、特征值等，以及不同数量组件的系数。上面我们对所有四个分量进行了切片，我们可以看到，它们与我们先前得到的没有一点不同。</p><p id="e4b5" class="pw-post-body-paragraph lo lp iq lq b lr mv jr lt lu mw ju lw lx mx lz ma mb my md me mf mz mh mi mj ij bi translated">但是需要多少组件呢？我们可以绘制估计的PCs或PCR模型，以在图上搜索“肘”，或者尝试预测测试集/交叉验证，或者使用一些更正式的方法。</p><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="85f2" class="mp kx iq ml b gy mq mr l ms mt">plot(fit, "validation", val.type = "MSEP")</span><span id="fe1a" class="mp kx iq ml b gy mu mr l ms mt"># or</span><span id="0dd6" class="mp kx iq ml b gy mu mr l ms mt">plot(data.pca, type='l')</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/4040e40648f0b9783fd8256da4041275.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*crBV-3vrPpz86tuwIoCX8A.png"/></div></div></figure><p id="c891" class="pw-post-body-paragraph lo lp iq lq b lr mv jr lt lu mw ju lw lx mx lz ma mb my md me mf mz mh mi mj ij bi translated">从图中，我们可以看到，一台电脑足以捕捉数据的变化。从前面的观察中我们还知道，第一个分量捕获了约93.88%的变化。因此，如果我们想对真实数据集进行回归并获得相似的结果，我们将只使用第一个分量来避免数据集中的多重共线性。</p><h1 id="b908" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">结论</h1><p id="bd3d" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在本文中，我们探索了一种计算PCs和OLS系数的矩阵形式方法，将这种自定义方法与R中的内置函数进行了比较，并通过执行PCR对其进行了总结。我们发现这两种技术没有区别。出于实用目的，使用PLS库很方便，但是出于理论目的，了解幕后的数学是很有用的。</p><p id="1db1" class="pw-post-body-paragraph lo lp iq lq b lr mv jr lt lu mw ju lw lx mx lz ma mb my md me mf mz mh mi mj ij bi translated">完整代码可以在<a class="ae kv" href="https://github.com/serafimpetrov1/PCA_OLS_PCR" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到。</p></div><div class="ab cl nu nv hu nw" role="separator"><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz oa"/><span class="nx bw bk ny nz"/></div><div class="ij ik il im in"><h1 id="543a" class="kw kx iq bd ky kz ob lb lc ld oc lf lg jw od jx li jz oe ka lk kc of kd lm ln bi translated">感谢您的阅读！</h1><p id="2a89" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">如果你有任何问题或发现了错误或错别字，请在下面留下评论。</p><p id="5462" class="pw-post-body-paragraph lo lp iq lq b lr mv jr lt lu mw ju lw lx mx lz ma mb my md me mf mz mh mi mj ij bi translated">在<a class="ae kv" href="http://www.linkedin.com/in/serafimpetrov" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上连接</p></div></div>    
</body>
</html>