<html>
<head>
<title>Neural Networks for Real-Time Audio: WaveNet</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">实时音频的神经网络:WaveNet</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-networks-for-real-time-audio-wavenet-2b5cdf791c4f?source=collection_archive---------6-----------------------#2021-05-03">https://towardsdatascience.com/neural-networks-for-real-time-audio-wavenet-2b5cdf791c4f?source=collection_archive---------6-----------------------#2021-05-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/bc96cb7fbaad04cf422664e655fbdaee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4-CIGWNOpBxPBBdrrdzZsQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">作者图片</p></figure><div class=""/><p id="e494" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这是关于使用神经网络进行实时音频的五部分系列的第二部分。对于之前的介绍文章，点击 <a class="ae lb" href="https://medium.com/nerd-for-tech/neural-networks-for-real-time-audio-introduction-ed5d575dc341" rel="noopener"> <em class="la">这里</em> </a> <em class="la">。</em></p><p id="7990" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在本文中，我们将使用WaveNet实时模拟吉他放大器。</p><p id="447f" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg"> WaveNet </strong>由DeepMind公司开发，并在2016年的论文<em class="la">wave net:Raw Audio的生成模型</em>中介绍。<em class="la"> </em>它解释了该模型如何用于生成音频，如逼真的人类语音。它是一个前馈神经网络，意味着信息只通过网络向前移动，而不像RNNs(递归神经网络)那样循环。</p><h1 id="40b9" class="lc ld jf bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">概观</h1><p id="a68f" class="pw-post-body-paragraph kc kd jf ke b kf ma kh ki kj mb kl km kn mc kp kq kr md kt ku kv me kx ky kz ij bi translated">这里使用的WaveNet的具体实现在论文<a class="ae lb" href="https://www.mdpi.com/2076-3417/10/3/766/htm" rel="noopener ugc nofollow" target="_blank"> <em class="la">带深度学习的实时吉他放大器仿真</em> </a>中有定义。本文使用扩展卷积层来捕捉吉他放大器的动态响应。扩张的层用于增加网络的感受域，允许它及时到达更远的时间来预测当前的音频样本。网络对过去的信号了解得越多，就能越好地预测下一个样本的值。</p><p id="0e34" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">音频数据是一维的。它是一个随时间变化的数值(振幅)。一个40秒声音文件的可视示例(。wav格式)显示在这里:</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mf"><img src="../Images/5af9058f38d407001808aaee89dabbf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vpL4qcgHALy-ijujvpGypg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图1:来自电吉他的40秒音频图</p></figure><p id="0d91" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如果放大上面的图，可以清楚地看到信号是如何随时间变化的。下面显示的是大约1/125秒(8毫秒)的相同音频。</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mk"><img src="../Images/f9910d869020f19027a0db8fbbfbaba2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wN-iioHxtfSrVKUI3ILVZA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图2:电吉他8毫秒音频的曲线图</p></figure><p id="fa07" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这里是上面吉他录音的音频:</p><figure class="mg mh mi mj gt is"><div class="bz fp l di"><div class="ml mm l"/></div></figure><p id="c3c7" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这是直接从挡泥板电吉他远程音频信号。不太令人兴奋，是吗？但是，如果我们通过一个Blackstar HT40电子管放大器发送同样的信号，情况会变得更加有趣:</p><figure class="mg mh mi mj gt is"><div class="bz fp l di"><div class="ml mm l"/></div></figure><p id="c1da" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在下一节中，我们将尝试使用WaveNet在特定设置下创建HT40放大器的实时模型。使用过驱通道和中性均衡器将放大器设置为25%的增益。录音是使用SM57麦克风进行的，距离扬声器格栅约1厘米，位于锥体中部。</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/9146ee5d54d5ff5ea13bb0e080815aa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*ykqKLQVO5XLJ3ep3etWpJg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">Blackstar HT40 amp和SM57麦克风(图片由作者提供)</p></figure><p id="5811" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">请注意，麦克风和扬声器/音箱都会修改来自放大器电子设备的音频信号，这种设置可能不太适合尽可能精确地模拟放大器。但是考虑到这个放大器是租来的，我想他们更希望我不要打开它(你也不应该打开，除非你知道你在做什么！真空管电子设备包含非常高的电流和电压)。</p><h1 id="4717" class="lc ld jf bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">PyTorch培训</h1><p id="99fc" class="pw-post-body-paragraph kc kd jf ke b kf ma kh ki kj mb kl km kn mc kp kq kr md kt ku kv me kx ky kz ij bi translated">PyTorch训练的示例代码来自Github上的<a class="ae lb" href="https://github.com/GuitarML/PedalNetRT" rel="noopener ugc nofollow" target="_blank"> <strong class="ke jg"> PedalNetRT </strong> </a>，用Python编写。注意，PedalNetRT使用了<a class="ae lb" href="https://www.pytorchlightning.ai/" rel="noopener ugc nofollow" target="_blank"><strong class="ke jg">Pytorch-lightning</strong></a>，这是一个有用的py torch包装器。</p><p id="e656" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">为了训练神经网络，我们将检查来自"<a class="ae lb" href="https://github.com/GuitarML/PedalNetRT/blob/master/model.py" rel="noopener ugc nofollow" target="_blank"> model.py </a> " python文件的代码，该文件定义了WaveNet类的实现。整个结构由一维卷积层组成。继承PyTorch的<code class="fe mo mp mq mr b">nn.Module</code>的基本WaveNet类如下所示:</p><figure class="mg mh mi mj gt is"><div class="bz fp l di"><div class="ms mm l"/></div></figure><p id="a9fc" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><code class="fe mo mp mq mr b">forward()</code>功能是进行音频处理的地方。此处使用了<em class="la">门控激活</em>，如吉他放大器仿真论文中所定义。<em class="la">门控激活</em>的输出被馈入<code class="fe mo mp mq mr b">self.residual</code>层堆栈。输入样本<code class="fe mo mp mq mr b">x</code>被添加回输出，该输出通过<code class="fe mo mp mq mr b">self.linear_mix</code>层馈送，以将输出减少为单个音频样本。</p><p id="8622" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">网络接收输入音频样本，然后提取内部层中的声音特征，并输出单个预测音频样本。请记住，这种情况每秒钟发生44，100次！预测样本与已知音频(来自HT40 amp录音)进行比较，PyTorch确定如何调整网络值以更接近目标。每次通过4分钟的训练数据是一个“时期”。在达到可接受的损失之前，典型的训练期可以持续1500个时期或更多。</p><p id="b36b" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">WaveNet类用定义神经网络的大小和结构的几个参数初始化。基本结构由<code class="fe mo mp mq mr b">self.input_layer</code>、<code class="fe mo mp mq mr b">self.hidden_layer</code> / <code class="fe mo mp mq mr b">self.residuals</code>(都是卷积层的堆叠)和<code class="fe mo mp mq mr b">self.linear_mix</code>(输出层)组成。你可能已经注意到使用了一个定制的<code class="fe mo mp mq mr b">CausalConv1d</code>类，而不是PyTorch内置的<code class="fe mo mp mq mr b">nn.Conv1d</code>。这是因为PyTorch中的因果卷积必须手动完成(在撰写本文时)。因果卷积仅在数据的一侧使用零填充，而不是“相同的”填充，它会均等地填充两侧。</p><p id="8d67" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><em class="la">注意:通过向特定维度添加额外的零，使用零填充数据来控制卷积层的输出大小。</em></p><p id="cf8c" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">自定义的<code class="fe mo mp mq mr b">CausalConv1d</code>类本质上是<code class="fe mo mp mq mr b">nn.Conv1d</code>的包装器，它只在左侧填充输入数据。这里显示了<code class="fe mo mp mq mr b">CausalConv1d</code>类的实现:</p><figure class="mg mh mi mj gt is"><div class="bz fp l di"><div class="ms mm l"/></div></figure><p id="32bf" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">设置基本WaveNet类的最后一部分是<code class="fe mo mp mq mr b">_conv_stack</code>函数，它堆叠所需数量的<code class="fe mo mp mq mr b">CausalConv1d</code>层。堆栈中的层数由整数<code class="fe mo mp mq mr b">dilations</code>定义。对于<code class="fe mo mp mq mr b">dilations=4</code>，你得到一个四层的堆栈，每个层的膨胀率为“1，2，4，8”。对于<code class="fe mo mp mq mr b">dilations=8</code>，你得到一个8层的堆叠，具有“1，2，4，8，16，32，64，128”的膨胀率。参数<code class="fe mo mp mq mr b">num_repeat</code>重复膨胀层。例如，对于<code class="fe mo mp mq mr b">dilations=8</code>和<code class="fe mo mp mq mr b">num_repeat=2</code>，您得到</p><p id="e104" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi">“1, 2, 4, 8, 16, 32, 64, 128, 1, 2, 4, 8, 16, 32, 64, 128”</p><p id="ee02" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">总共16个隐藏卷积层。<code class="fe mo mp mq mr b">_conv_stack</code>功能如下所示:</p><figure class="mg mh mi mj gt is"><div class="bz fp l di"><div class="ms mm l"/></div></figure><p id="a02d" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如前所述，在我们很好地匹配目标之前，WaveNet的典型训练会话可以持续1500个时期。根据目标信号的复杂性，更多的历元可以导致更高的精度。一般规律是信号越失真(例如高增益)，训练就越困难。通过选择更大的模型参数，可以以处理时间为代价来提高训练性能。训练速度和实时性能之间的权衡在这里很重要，并且对于每个目标硬件是不同的。</p><p id="e8c4" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">还需要注意的是，在吉他踏板的情况下，这种神经网络方法不适用于基于时间的效果，如延迟/混响/合唱/凸缘。它对失真/过驱动有效(即动态响应小于约50毫秒的效果)。</p><h1 id="505d" class="lc ld jf bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">培训结果</h1><p id="c613" class="pw-post-body-paragraph kc kd jf ke b kf ma kh ki kj mb kl km kn mc kp kq kr md kt ku kv me kx ky kz ij bi translated">以下是使用PedalNetRT对HT40进行1500多次训练的结果。损失值下降到大约0.02。这里使用的损失函数是amp仿真论文中定义的MSE(均方误差)的变体。这里显示了8毫秒数据的对比图:</p><figure class="mg mh mi mj gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mk"><img src="../Images/5b8469d1554c31d8ee402170322ec70d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XXe4EVqO9R3wdc8yuaIj8Q.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图HT40放大器的实际信号和预测信号之间的比较</p></figure><p id="41c6" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这是预测的音频(与之前的实际HT40音频相比):</p><figure class="mg mh mi mj gt is"><div class="bz fp l di"><div class="ml mm l"/></div></figure><p id="58a4" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">由此产生的0.02的损失非常接近原始记录。从SoundCloud mp3样本中，你很难分辨出哪个是哪个。训练有素的耳朵，高品质的录音室监听系统，以及原声。wav文件，人们可能会告诉区别，但我认为0.02损失是一个成功的捕捉HT40放大器。根据其他测试，0.05损失听起来不错，但感觉不同，0.10损失明显不同，但可以通过额外的均衡器处理听起来接近。0.2或更高的损失仍然是一个有趣的声音，但根据经验，我认为这是一个不成功的捕捉。</p><p id="5d78" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">训练时间取决于硬件、型号大小和录音长度。在Nvidia Quadro 2000 GPU上，运行1500个epochs需要大约4个小时。在内置Nvidia显卡的中等价位笔记本电脑上，同样的培训大约需要8个小时。对于纯CPU培训，您将需要24小时以上的培训时间。</p><h1 id="414f" class="lc ld jf bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">模型转换</h1><p id="d9bc" class="pw-post-body-paragraph kc kd jf ke b kf ma kh ki kj mb kl km kn mc kp kq kr md kt ku kv me kx ky kz ij bi translated">在实时使用经过训练的模型之前，必须将模型转换成合适的格式，以便加载到插件中。这里选择的格式是“json ”,因为它的可读性和在计算世界中的普遍接受性。PyTorch使用它自己的”。pt“格式”。ckpt "代表py torch-闪电。PedalNetRT中使用一个名为“<a class="ae lb" href="https://github.com/GuitarML/PedalNetRT/blob/master/export.py" rel="noopener ugc nofollow" target="_blank"> export.py </a>的脚本来执行这种转换，并以WaveNetVA理解的方式排列数据。</p><h1 id="e53a" class="lc ld jf bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">实时实现</h1><p id="cae3" class="pw-post-body-paragraph kc kd jf ke b kf ma kh ki kj mb kl km kn mc kp kq kr md kt ku kv me kx ky kz ij bi translated">实时c++的示例代码来自Github上的<a class="ae lb" href="https://github.com/damskaggep/WaveNetVA" rel="noopener ugc nofollow" target="_blank"> <strong class="ke jg"> WaveNetVA </strong> </a>(也在<a class="ae lb" href="https://github.com/GuitarML/SmartGuitarAmp" rel="noopener ugc nofollow" target="_blank"><strong class="ke jg">smart guitar amp</strong></a><strong class="ke jg"/>和<strong class="ke jg"/><a class="ae lb" href="https://github.com/GuitarML/SmartGuitarPedal" rel="noopener ugc nofollow" target="_blank"><strong class="ke jg">smart guitar pedal</strong></a>中实现)。代码使用<a class="ae lb" href="https://eigen.tuxfamily.org/dox/" rel="noopener ugc nofollow" target="_blank"> <strong class="ke jg">特征值</strong> </a>进行矩阵计算。</p><p id="6550" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">实时插件使用<a class="ae lb" href="https://juce.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="ke jg"> JUCE </strong> </a>框架，这是一个跨平台的c++框架，用于创建音频应用。基本目标是用高性能c++代码从PyTorch WaveNet类中重新创建<code class="fe mo mp mq mr b">forward()</code>函数。我们不会在这里涵盖所有的代码，但我会触及关键点。</p><p id="17b9" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">来自转换后的json模型的模型数据(状态参数)在WaveNet类中加载和设置。下面显示了<code class="fe mo mp mq mr b">setParams</code>方法，它基于json数据建立了一个WaveNet实例。一个经过训练的json模型的例子可以在<a class="ae lb" href="https://github.com/GuitarML/PedalNetRT/blob/master/models/pedalnet/pedalnet.json" rel="noopener ugc nofollow" target="_blank"> Github </a>上查看。</p><figure class="mg mh mi mj gt is"><div class="bz fp l di"><div class="ms mm l"/></div></figure><p id="5606" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这里定义了<code class="fe mo mp mq mr b">inputLayer</code>、<code class="fe mo mp mq mr b">outputLayer</code>、<code class="fe mo mp mq mr b">convStack</code>(内层)。每层都有通道数、滤波器宽度、扩张和激活功能的设置。</p><p id="6fec" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这里显示了WaveNet类的主要处理方法，它获取输入音频缓冲区<code class="fe mo mp mq mr b">inputData</code>，通过每一层(<code class="fe mo mp mq mr b">inputLayer</code>、<code class="fe mo mp mq mr b">convStack</code>、<code class="fe mo mp mq mr b">outputLayer</code>)对其进行处理，然后将处理后的音频复制到输出缓冲区<code class="fe mo mp mq mr b">outputData</code>。</p><figure class="mg mh mi mj gt is"><div class="bz fp l di"><div class="ms mm l"/></div></figure><p id="8fd3" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">音频数据实时流经每一层(通过音频缓冲区或“块”)，但实际的卷积计算是在“<a class="ae lb" href="https://github.com/damskaggep/WaveNetVA/blob/master/Source/Convolution.cpp" rel="noopener ugc nofollow" target="_blank">convolution . CPP</a>”<code class="fe mo mp mq mr b">processSingleSample</code>方法中执行的，该方法处理单个音频样本:</p><figure class="mg mh mi mj gt is"><div class="bz fp l di"><div class="ms mm l"/></div></figure><p id="bbc1" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">该方法使用json模型中的层设置来确定数据如何卷积，或者数据如何在层中流动。在最基本的操作中，这只是与内核(训练的神经网络值)相乘:</p><p id="eaf0" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><code class="fe mo mp mq mr b">outVec = outVec + *(fifo+readPos) * (*it);</code></p><p id="362b" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">以及偏置向量的加法:</p><p id="dad0" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><code class="fe mo mp mq mr b">outVec = outVec + bias</code></p><p id="3a09" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">复杂的部分是确定如何索引每个数组，以便正确计算卷积层。我会留到下一天再说！</p><p id="f899" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">一旦当前音频缓冲区被处理(从16到4096个2的倍数的样本，取决于音频设备设置)，它将被转换回模拟，并从扬声器或耳机输出。幸运的是，JUCE框架处理了所有这些问题。</p><h1 id="55f2" class="lc ld jf bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">实时性能</h1><p id="e385" class="pw-post-body-paragraph kc kd jf ke b kf ma kh ki kj mb kl km kn mc kp kq kr md kt ku kv me kx ky kz ij bi translated">这个WaveNet实现完全能够在任何现代计算机上实时运行。然而，与使用传统建模的音频DSP软件相比，它的CPU使用率要高得多。使用电路分析可以以更快的处理速度实现相同质量的建模(以吉他放大器为例)。可以说，只用音频样本而没有领域专业知识来模拟这些复杂系统的能力是一个很好的权衡。</p><p id="3b04" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这是一个SmartAmp插件的实时演示视频，它使用WaveNet来模拟小管放大器(Fender Blues Jr .)和其他放大器。</p><figure class="mg mh mi mj gt is"><div class="bz fp l di"><div class="mt mm l"/></div></figure><p id="7afb" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">作为一名吉他手，我对WaveNet的看法是，与目标放大器/踏板相比，它的声音非常自然。它似乎在高增益方面有问题(如在金属音乐中)，但可以准确处理中增益和干净的音调。在模型中也观察到了低音的衰减。改变模型的大小会对声音产生很大的影响，也会影响它实时平稳运行的能力。</p><p id="56af" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在下一篇文章中，我们将研究使用无状态LSTM，看看我们是否可以在保持高质量声音的同时提高CPU使用率和训练时间。在此继续阅读:</p><div class="ip iq gp gr ir mu"><a href="https://link.medium.com/Jg5ft4WM0fb" rel="noopener follow" target="_blank"><div class="mv ab fo"><div class="mw ab mx cl cj my"><h2 class="bd jg gy z fp mz fr fs na fu fw je bi translated">实时音频的神经网络:无状态的LSTM</h2><div class="nb l"><h3 class="bd b gy z fp mz fr fs na fu fw dk translated">这是关于使用神经网络进行实时音频的四部分系列的第三部分。对于上一篇关于…的文章</h3></div><div class="nc l"><p class="bd b dl z fp mz fr fs na fu fw dk translated">link.medium.com</p></div></div><div class="nd l"><div class="ne l nf ng nh nd ni ix mu"/></div></div></a></div><p id="5050" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">感谢您的阅读！</p><ol class=""><li id="c464" class="nj nk jf ke b kf kg kj kk kn nl kr nm kv nn kz no np nq nr bi translated">Aaron van den Oord等人，“wave net:Raw Audio的生成模型”<em class="la"> arXiv预印本arXiv:1609.03499 </em>，2016年。</li><li id="73ca" class="nj nk jf ke b kf ns kj nt kn nu kr nv kv nw kz no np nq nr bi translated">Alec Wright等人，“深度学习的实时吉他放大器仿真”<em class="la">应用科学</em> 10期，第3期(2020): 766。</li></ol></div></div>    
</body>
</html>