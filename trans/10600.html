<html>
<head>
<title>A Better mAP for Object Detection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于对象检测的更好的地图</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-better-map-for-object-detection-32662767d424?source=collection_archive---------7-----------------------#2021-10-11">https://towardsdatascience.com/a-better-map-for-object-detection-32662767d424?source=collection_archive---------7-----------------------#2021-10-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="7a0d" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">ML提示和技巧/潮流</h2><div class=""/><div class=""><h2 id="69df" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">为什么你的物体检测模型很烂？让我们改进它！</h2></div><blockquote class="ko kp kq"><p id="db50" class="kr ks kt ku b kv kw ka kx ky kz kd la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">TL；DR:你的对象检测模型很糟糕，你想通过利用更好的评估指标来改进它……跳过介绍，找出方法或者直接去<a class="ae lo" href="https://medium.com/r?url=https%3A%2F%2Fcolab.research.google.com%2Fdrive%2F1kji2JjD_N2Yq0gF4ddbuFcOa45yq0o8G%3Fusp%3Dsharing" rel="noopener"> Google Colab笔记本</a>看代码或者<a class="ae lo" href="https://t.clear.ml/tide_task" rel="noopener ugc nofollow" target="_blank">这里</a>看ClearML实验日志！</p></blockquote><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi lp"><img src="../Images/e0cc33edd31476e5c12eefaa9814d113.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q7OD4Osw_l7vS0RsiBAi0g.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">误报和漏报是物体检测中的主要问题(来源:图片6喷气飞机向云行进，作者是<a class="ae lo" href="https://www.pexels.com/@sajid-ali-35945?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">赛义德·阿里</a>和<a class="ae lo" href="https://www.pexels.com/photo/6-jets-parading-toward-clouds-129627/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">像素</a></p></figure><h1 id="4ae1" class="mf mg iq bd mh mi mj mk ml mm mn mo mp kf mq kg mr ki ms kj mt kl mu km mv mw bi translated">介绍</h1><p id="ce0d" class="pw-post-body-paragraph kr ks iq ku b kv mx ka kx ky my kd la mz na ld le nb nc lh li nd ne ll lm ln ij bi nf translated">O  <strong class="ku ja">物体检测容易</strong>。你需要做的只是获得一个训练数据集，从开源库中下载一个预训练的模型，如<a class="ae lo" href="https://github.com/tensorflow/models/tree/master/research/object_detection" rel="noopener ugc nofollow" target="_blank"> Tensorflow对象检测API </a>、<a class="ae lo" href="https://github.com/facebookresearch/detectron2" rel="noopener ugc nofollow" target="_blank"> Detectron2 </a>和<a class="ae lo" href="https://github.com/open-mmlab/mmdetection" rel="noopener ugc nofollow" target="_blank"> mmdetection </a>，并(重新)训练它。</p><p id="2d9b" class="pw-post-body-paragraph kr ks iq ku b kv kw ka kx ky kz kd la mz lc ld le nb lg lh li nd lk ll lm ln ij bi translated">实际上，在训练你的模型几个小时后，你<a class="ae lo" rel="noopener" target="_blank" href="/new-tf2-object-detection-api-5c6ea8362a8c">评估</a>它，检查度量标准…你最终对性能完全失望，因为它与你的预期相差甚远！</p><p id="29a5" class="pw-post-body-paragraph kr ks iq ku b kv kw ka kx ky kz kd la mz lc ld le nb lg lh li nd lk ll lm ln ij bi translated">现在你有麻烦了…关于如何调试模型的性能，以及如何优化模型以获得更好的性能，没有好的教程、菜谱或StackOverflow主题。在<a class="ae lo" href="http://forsight.ai" rel="noopener ugc nofollow" target="_blank"> Forsight </a>，我们使用潮汐来实现精确的物体检测。这个博客是关于分享小技巧的，这些小技巧对于提高你的物体探测模型的准确性是非常宝贵的。</p></div><div class="ab cl no np hu nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="ij ik il im in"><h1 id="1f12" class="mf mg iq bd mh mi nv mk ml mm nw mo mp kf nx kg mr ki ny kj mt kl nz km mv mw bi translated">模型性能评估—基础</h1><p id="5d49" class="pw-post-body-paragraph kr ks iq ku b kv mx ka kx ky my kd la mz na ld le nb nc lh li nd ne ll lm ln ij bi translated">构成物体检测模型性能的关键有两个方面:<strong class="ku ja">速度</strong>和<strong class="ku ja">精度</strong>。从下表中可以看出，这两者之间总是有一个权衡。在这篇博客中，我们将专注于分析模型的准确性。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/3d6e09bedf59b601880a25a060f5eaed.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*eHGrMOEs3MSrfCnW-9qV-A.png"/></div><p class="mb mc gj gh gi md me bd b be z dk translated">EfficientDet模型中速度与准确性的权衡(来源:图片由作者提供)</p></figure><p id="c787" class="pw-post-body-paragraph kr ks iq ku b kv kw ka kx ky kz kd la mz lc ld le nb lg lh li nd lk ll lm ln ij bi translated">简而言之，<strong class="ku ja">物体检测器以一定的置信度得分</strong>预测给定类别的物体在图像中的位置。通过在对象周围放置边界框来识别它们的位置，来定义对象的位置。</p><p id="1715" class="pw-post-body-paragraph kr ks iq ku b kv kw ka kx ky kz kd la mz lc ld le nb lg lh li nd lk ll lm ln ij bi translated">因此，检测由一组三个属性表示:</p><ol class=""><li id="45a2" class="ob oc iq ku b kv kw ky kz mz od nb oe nd of ln og oh oi oj bi translated"><strong class="ku ja">对象类</strong>(即人)</li><li id="9ffe" class="ob oc iq ku b kv ok ky ol mz om nb on nd oo ln og oh oi oj bi translated"><strong class="ku ja">对应的包围盒</strong>(即【63，52，150，50】)</li><li id="1acb" class="ob oc iq ku b kv ok ky ol mz om nb on nd oo ln og oh oi oj bi translated"><strong class="ku ja">置信度得分</strong>(即0.583或58.3%)</li></ol><p id="b5c7" class="pw-post-body-paragraph kr ks iq ku b kv kw ka kx ky kz kd la mz lc ld le nb lg lh li nd lk ll lm ln ij bi translated">类似地，对象检测模型的性能评估是基于:</p><ul class=""><li id="cd74" class="ob oc iq ku b kv kw ky kz mz od nb oe nd of ln op oh oi oj bi translated">一组<strong class="ku ja">基本事实边界框</strong>，表示包含待检测类别的对象的图像的矩形区域，</li><li id="31b6" class="ob oc iq ku b kv ok ky ol mz om nb on nd oo ln op oh oi oj bi translated">由模型预测的一组<strong class="ku ja">检测，每个检测由一个边界框、一个类别和一个置信度值组成。</strong></li></ul><h2 id="8aa2" class="oq mg iq bd mh or os dn ml ot ou dp mp mz ov ow mr nb ox oy mt nd oz pa mv iw bi translated">借据和信心得分</h2><p id="a685" class="pw-post-body-paragraph kr ks iq ku b kv mx ka kx ky my kd la mz na ld le nb nc lh li nd ne ll lm ln ij bi translated">考虑下图中由真实边界框(绿色)表示的感兴趣对象(人)和由预测边界框(红色)表示的检测区域。在不考虑置信度得分的情况下，<strong class="ku ja">当预测框和真实框的面积和位置相同时</strong>，即完美地包围了人，则出现完美匹配。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi pb"><img src="../Images/e2b7dc0be1db1c04326ae5616ad61226.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0Ayl9VRflD66U131wkKuQQ.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">预测与现实(来源:照片中的男子站在灰色金属设备附近，作者是<a class="ae lo" href="https://www.pexels.com/photo/man-standing-near-gray-metal-equipment-2760243/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> Pexels </a>的<a class="ae lo" href="https://www.pexels.com/@kateryna-babaieva-1423213?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> Kateryna Babaieva </a>)</p></figure><p id="a433" class="pw-post-body-paragraph kr ks iq ku b kv kw ka kx ky kz kd la mz lc ld le nb lg lh li nd lk ll lm ln ij bi translated">这两个条件通过<strong class="ku ja">交集除以并集(IoU) </strong>进行评估，这是一种基于<a class="ae lo" href="https://en.wikipedia.org/wiki/Jaccard_index" rel="noopener ugc nofollow" target="_blank"> Jaccard指数</a>的测量，Jaccard指数是两组数据的相似系数。在对象检测范围中，IoU等于预测边界框(红色)和地面真实边界框(绿色)之间的重叠(相交)面积除以它们的并集面积。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/087d1fd46b809115f194f4dd2e07f7fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*pFFavB5HFMkmsSI1d6oqBA.png"/></div><p class="mb mc gj gh gi md me bd b be z dk translated">IoU等于重叠面积除以边界框的并集面积。(来源:图片由作者提供)</p></figure><p id="1107" class="pw-post-body-paragraph kr ks iq ku b kv kw ka kx ky kz kd la mz lc ld le nb lg lh li nd lk ll lm ln ij bi translated">下图中可以看到一些不同IoU分数的例子。<strong class="ku ja">通过设置IoU阈值，一个指标可以或多或少地限制检测是正确还是不正确。</strong>更接近1的IoU阈值更具限制性，因为它需要几乎完美的检测，而更接近但不同于0的IoU阈值更灵活，即使很小的重叠也被视为有效检测。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi pd"><img src="../Images/da9315bd4ed54a03c296a965df2a8f00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*efXb_Nzxx3DudldWZeeTkw.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">边框的不同借据(来源:作者图片)</p></figure><p id="50cd" class="pw-post-body-paragraph kr ks iq ku b kv kw ka kx ky kz kd la mz lc ld le nb lg lh li nd lk ll lm ln ij bi translated"><strong class="ku ja">置信度得分</strong>反映了盒子包含感兴趣对象的可能性以及分类器对它的置信度。如果该框中不存在任何对象，则置信度得分理想情况下应该为零。一般来说，越紧密的边界框(严格的IoU)的置信度得分往往越高。</p><h2 id="3811" class="oq mg iq bd mh or os dn ml ot ou dp mp mz ov ow mr nb ox oy mt nd oz pa mv iw bi translated">精确度和召回率</h2><p id="ef3b" class="pw-post-body-paragraph kr ks iq ku b kv mx ka kx ky my kd la mz na ld le nb nc lh li nd ne ll lm ln ij bi translated"><strong class="ku ja">精度(Pr) </strong>是模型仅识别相关对象的能力，计算为正确正面预测的百分比。</p><p id="5d66" class="pw-post-body-paragraph kr ks iq ku b kv kw ka kx ky kz kd la mz lc ld le nb lg lh li nd lk ll lm ln ij bi translated"><strong class="ku ja"> Recall (Rc) </strong>是一个模型找到所有相关案例(所有真实边界框)的能力。它是所有已知事实中正确的正面预测的百分比。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/082b5695080c29a233527db536df9fd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*2ofLvLaVHiGDIkQoL0Ng0g.png"/></div><p class="mb mc gj gh gi md me bd b be z dk translated">精确与召回(来源:<a class="ae lo" href="https://en.wikipedia.org/wiki/File:Precisionrecall.svg" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/File:Precisionrecall.svg</a>有CC许可)</p></figure><p id="7972" class="pw-post-body-paragraph kr ks iq ku b kv kw ka kx ky kz kd la mz lc ld le nb lg lh li nd lk ll lm ln ij bi translated">为了计算精度和召回值，每个检测到的边界框必须首先被分类为:</p><ul class=""><li id="0140" class="ob oc iq ku b kv kw ky kz mz od nb oe nd of ln op oh oi oj bi translated"><strong class="ku ja">真阳性(TP) </strong> —地面真实边界框的正确检测；</li><li id="5cbd" class="ob oc iq ku b kv ok ky ol mz om nb on nd oo ln op oh oi oj bi translated"><strong class="ku ja">假阳性(FP) </strong> —不存在物体的错误检测或存在物体的误检测；</li><li id="f82e" class="ob oc iq ku b kv ok ky ol mz om nb on nd oo ln op oh oi oj bi translated"><strong class="ku ja">假阴性(FN) </strong> —未被检测到的真实边界框；</li><li id="cb05" class="ob oc iq ku b kv ok ky ol mz om nb on nd oo ln op oh oi oj bi translated"><strong class="ku ja">真否定(TN) </strong> — <strong class="ku ja"> </strong>不适用于对象检测，因为有无限多的实例不应被检测为对象。</li></ul><p id="738e" class="pw-post-body-paragraph kr ks iq ku b kv kw ka kx ky kz kd la mz lc ld le nb lg lh li nd lk ll lm ln ij bi translated">概括地说，对象检测器的输出由一个<strong class="ku ja">边界框、一个类和一个置信度得分</strong>来表征。通过仅将置信度大于置信度阈值τ的那些检测视为阳性检测，可以在精确度和召回率计算中考虑置信度得分。置信水平小于τ的检测被认为是否定的。</p><p id="043f" class="pw-post-body-paragraph kr ks iq ku b kv kw ka kx ky kz kd la mz lc ld le nb lg lh li nd lk ll lm ln ij bi translated">TP(τ)和FP(τ)都是τ的减函数，因为较大的τ会减少阳性检测的数量。相反，FN(τ)是τ的增函数，因为较少的阳性检测意味着较多的阴性检测。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/a7924642fc14a0457bf5944ef64ac49a.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/format:webp/1*T2elg-Dur53WuXkpTVqE8w.png"/></div></figure><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/d804d169032fdd7fa88a36a18bab2827.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/1*r2s5h_65z2qdprqCumvelQ.png"/></div></figure><h2 id="d4da" class="oq mg iq bd mh or os dn ml ot ou dp mp mz ov ow mr nb ox oy mt nd oz pa mv iw bi translated">平均精度</h2><p id="c554" class="pw-post-body-paragraph kr ks iq ku b kv mx ka kx ky my kd la mz na ld le nb nc lh li nd ne ll lm ln ij bi translated">精确度-召回率(PR)曲线是精确度作为召回率的函数的图。它显示了模型检测的不同置信度值的两个度量之间的权衡。<strong class="ku ja"> AP@α是精确召回曲线下的面积(AUC-PR)。</strong>数学上，AP定义为:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/307b6c281dad75e510c517610252970c.png" data-original-src="https://miro.medium.com/v2/resize:fit:308/format:webp/1*O3tg-xJC0W7lL365BeozVA.png"/></div></figure><p id="ef26" class="pw-post-body-paragraph kr ks iq ku b kv kw ka kx ky kz kd la mz lc ld le nb lg lh li nd lk ll lm ln ij bi translated"><strong class="ku ja">符号:</strong> AP@α表示IoU阈值为α时的平均精度(AP)。因此，AP@0.50和AP@0.75分别表示IoU阈值为50%和75%时的AP。</p><p id="38cf" class="pw-post-body-paragraph kr ks iq ku b kv kw ka kx ky kz kd la mz lc ld le nb lg lh li nd lk ll lm ln ij bi translated"><strong class="ku ja">高AUC-PR意味着高精度和高召回率</strong>。自然地，PR曲线具有之字形行为(不是单调递减的)。在计算AP之前，我们使用以下插值公式使PR曲线单调递减:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/08591778af5c7ee93b6db5ffa31f71d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:332/format:webp/1*pxwNQPluKcuHoR9JKHb-sQ.png"/></div></figure><p id="3cb9" class="pw-post-body-paragraph kr ks iq ku b kv kw ka kx ky kz kd la mz lc ld le nb lg lh li nd lk ll lm ln ij bi translated">在我们为每个单独的类计算AP之后，我们计算平均AP作为所有类的所有AP的平均值:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi pj"><img src="../Images/eec712bdd06bc5e45fb853fb96629cad.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*pUZu_YK3b-JVl86GoVINNw.png"/></div></div></figure><p id="c45d" class="pw-post-body-paragraph kr ks iq ku b kv kw ka kx ky kz kd la mz lc ld le nb lg lh li nd lk ll lm ln ij bi translated">关于精确召回曲线计算的例子，请查看Harshit Kumar的博客文章。</p><p id="06be" class="pw-post-body-paragraph kr ks iq ku b kv kw ka kx ky kz kd la mz lc ld le nb lg lh li nd lk ll lm ln ij bi translated">在下图中，我们可以看到三条不同的PR曲线，它们的不同之处在于精度/召回计算中使用的IoU阈值。由于AP对应于曲线下的面积，显然<strong class="ku ja">宽松的IoU阈值会导致比严格的IoU阈值更高的AP分数。</strong></p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi pk"><img src="../Images/91536acbd38bcd4b960351ec0725667b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*o2xnEgSagR569HZr.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">(图片来源:Harshit Kumar<a class="ae lo" href="https://kharshit.github.io/blog/2019/09/20/evaluation-metrics-for-object-detection-and-segmentation" rel="noopener ugc nofollow" target="_blank">https://kharshit . github . io/blog/2019/09/20/evaluation-metrics-for-object-detection-and-segmentation</a>)</p></figure><p id="618f" class="pw-post-body-paragraph kr ks iq ku b kv kw ka kx ky kz kd la mz lc ld le nb lg lh li nd lk ll lm ln ij bi translated">更多关于可可地图的具体细节，请访问:<a class="ae lo" href="https://cocodataset.org/#detection-eval" rel="noopener ugc nofollow" target="_blank">https://cocodataset.org/#detection-eval</a>！</p></div><div class="ab cl no np hu nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="ij ik il im in"><h1 id="e5c1" class="mf mg iq bd mh mi nv mk ml mm nw mo mp kf nx kg mr ki ny kj mt kl nz km mv mw bi translated">潮汐——更好的地图</h1><p id="de08" class="pw-post-body-paragraph kr ks iq ku b kv mx ka kx ky my kd la mz na ld le nb nc lh li nd ne ll lm ln ij bi translated">如前几节所示，对象检测和实例分割任务通常只使用一个指标来衡量模型性能:平均精度(mAP)。虽然mAP用一个数字简洁地总结了模型的性能，但是<strong class="ku ja">从mAP中理清对象检测和实例分割中的错误是困难的</strong>:假阳性可能是重复检测、错误分类、错误定位、与背景混淆，或者甚至是错误分类和错误定位。</p><p id="ae17" class="pw-post-body-paragraph kr ks iq ku b kv kw ka kx ky kz kd la mz lc ld le nb lg lh li nd lk ll lm ln ij bi translated"><a class="ae lo" href="https://dbolya.github.io/tide/paper.pdf" rel="noopener ugc nofollow" target="_blank"> TIDE(用于识别检测和分割错误的工具包)</a>被引入来解决上述挑战。它将目标检测误差分为六种类型，并引入一种技术来测量每种误差的贡献，以隔离其对整体性能的影响。<strong class="ku ja">潮基本上是田蜜地图上的类固醇！</strong></p><p id="4659" class="pw-post-body-paragraph kr ks iq ku b kv kw ka kx ky kz kd la mz lc ld le nb lg lh li nd lk ll lm ln ij bi translated">如果您的对象检测模型在自定义数据集上训练后有一个低mAP，您肯定会想知道我们可以在哪里进一步改进它。这是潮汐闪耀光芒的地方。<strong class="ku ja">为了让你的模型运行得更好，TIDE将帮助你精确定位你应该重点改进的错误类型。</strong></p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi pl"><img src="../Images/be6fdf44d545fca7459dae67f1064bea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*B4SrqaikbqLvyuGzONWhAA.gif"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">(作者提供gif图片——丹尼尔·博尔亚等人提供图片来源——<a class="ae lo" href="https://www.youtube.com/watch?v=McYFYU3PXcU" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=McYFYU3PXcU</a>)</p></figure><h2 id="33b1" class="oq mg iq bd mh or os dn ml ot ou dp mp mz ov ow mr nb ox oy mt nd oz pa mv iw bi translated">潮汐误差类型</h2><p id="493f" class="pw-post-body-paragraph kr ks iq ku b kv mx ka kx ky my kd la mz na ld le nb nc lh li nd ne ll lm ln ij bi translated">在本文中，有对错误类型以及如何定义它们的详细解释。</p><blockquote class="ko kp kq"><p id="7c1d" class="kr ks kt ku b kv kw ka kx ky kz kd la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">为了创建一个有意义的误差分布来捕捉mAP的组成部分，我们将模型中的所有假阳性和假阴性分为6种类型，如下图所示。注意，对于某些错误类型(分类和定位)，假阳性可能与假阴性成对出现。我们将使用IoUᵐᵃˣ来表示假阳性的最大IoU重叠与给定类别的真实情况。</p></blockquote><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi pm"><img src="../Images/f70399e6e2804382b0adca9ecc9793dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9pA316vp4SXp5SCLAjUUig.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">(来源:图片来自[1])</p></figure><blockquote class="ko kp kq"><p id="27e5" class="kr ks kt ku b kv kw ka kx ky kz kd la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">前台IoU阈值表示为tᶠ，后台阈值表示为tᵇ，除非另有说明，否则均设为0.5和0.1。</p><p id="2503" class="kr ks kt ku b kv kw ka kx ky kz kd la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ja"> 1。分类错误</strong> : IoUᵐᵃˣ ≥ tᶠ的燃气轮机分类不正确(即正确定位但分类不正确)。</p><p id="d4b0" class="kr ks kt ku b kv kw ka kx ky kz kd la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ja"> 2。本地化错误</strong> : tᵇ ≤ IoUᵐᵃˣ ≤ tᶠ为正确级别的GT(即分类正确但本地化不正确)。</p><p id="d53f" class="kr ks kt ku b kv kw ka kx ky kz kd la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ja"> 3。Cls和Loc错误</strong> : tᵇ ≤ IoUᵐᵃˣ ≤ tᶠ的GT分类不正确(即分类不正确和本地化不正确)。</p><p id="fbdc" class="kr ks kt ku b kv kw ka kx ky kz kd la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ja"> 4。重复检测错误</strong>:正确类别的燃气轮机的IoUᵐᵃˣ ≥ tᶠ，但另一个更高得分的检测已经匹配该燃气轮机(即，如果不是更高得分的检测，将是正确的)。</p><p id="8a04" class="kr ks kt ku b kv kw ka kx ky kz kd la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ja"> 5。背景错误</strong>:所有GT的IoUᵐᵃˣ ≤ tᵇ(即检测到的背景为前景)。</p><p id="94c2" class="kr ks kt ku b kv kw ka kx ky kz kd la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated"><strong class="ku ja"> 6。遗漏的GT错误</strong>:所有未检测到的地面真相(假阴性)尚未被分类或定位错误覆盖。</p></blockquote><p id="936b" class="pw-post-body-paragraph kr ks iq ku b kv kw ka kx ky kz kd la mz lc ld le nb lg lh li nd lk ll lm ln ij bi translated">TIDE旨在替代<a class="ae lo" href="https://github.com/cocodataset/cocoapi" rel="noopener ugc nofollow" target="_blank"> COCO评估工具包</a>，入门非常简单:</p><pre class="lq lr ls lt gt pn po pp pq aw pr bi"><span id="ea4e" class="oq mg iq po b gy ps pt l pu pv">from tidecv import TIDE, datasets<br/><br/>tide = TIDE()<br/>tide.evaluate(datasets.COCO(), datasets.COCOResult('path/to/your/results/file'), mode=TIDE.BOX)<br/>tide.summarize()  # Summarize the results as tables in the console<br/>tide.plot()       # Show a summary figure</span></pre><h2 id="4829" class="oq mg iq bd mh or os dn ml ot ou dp mp mz ov ow mr nb ox oy mt nd oz pa mv iw bi translated">潮汐实例</h2><p id="cd32" class="pw-post-body-paragraph kr ks iq ku b kv mx ka kx ky my kd la mz na ld le nb nc lh li nd ne ll lm ln ij bi translated">我们已经准备了一个<a class="ae lo" href="https://colab.research.google.com/drive/1kji2JjD_N2Yq0gF4ddbuFcOa45yq0o8G?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Google Colab笔记本</a>，里面有一个代码样本，展示了<strong class="ku ja">如何使用带有Tensorflow对象检测API </strong>的TIDE。</p><p id="51c7" class="pw-post-body-paragraph kr ks iq ku b kv kw ka kx ky kz kd la mz lc ld le nb lg lh li nd lk ll lm ln ij bi translated">如果您不想运行笔记本并等待评估完成，您可以<strong class="ku ja">在我们用于实验管理的</strong> <a class="ae lo" href="https://t.clear.ml/tide_task" rel="noopener ugc nofollow" target="_blank"> <strong class="ku ja"> Allegro ClearML仪表盘</strong> </a> <strong class="ku ja"> </strong>上查看结果。通过使用<a class="ae lo" href="https://clear.ml/" rel="noopener ugc nofollow" target="_blank"> ClearML </a>，我们可以<strong class="ku ja">以有组织的方式持久存储所有实验数据</strong>，这使我们能够轻松地执行比较并上传<a class="ae lo" href="https://clear.ml/docs/latest/docs/fundamentals/artifacts/" rel="noopener ugc nofollow" target="_blank">自定义工件</a>，如COCO格式的<a class="ae lo" href="https://demoapp.demo.clear.ml/projects/1f4d0d46cb4443fdbfda6511fbea1cc5/experiments/84b743d1a38a454e9f9dda0fb47fa5a0/artifacts/other/cocoDt/output?columns=selected&amp;columns=type&amp;columns=name&amp;columns=tags&amp;columns=status&amp;columns=project.name&amp;columns=users&amp;columns=started&amp;columns=last_update&amp;columns=last_iteration&amp;columns=parent.name&amp;order=-last_update" rel="noopener ugc nofollow" target="_blank">地面实况和检测元数据</a>，以及<a class="ae lo" href="https://demoapp.demo.clear.ml/projects/1f4d0d46cb4443fdbfda6511fbea1cc5/experiments/84b743d1a38a454e9f9dda0fb47fa5a0/info-output/metrics/plots?columns=selected&amp;columns=type&amp;columns=name&amp;columns=tags&amp;columns=status&amp;columns=project.name&amp;columns=users&amp;columns=started&amp;columns=last_update&amp;columns=last_iteration&amp;columns=parent.name&amp;order=-last_update" rel="noopener ugc nofollow" target="_blank">自定义图</a>到实验。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi pl"><img src="../Images/9a7370ffd4ff931ea41cbee42785a549.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*1UH9-G7xFyhxCBhgx38ddg.gif"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">潮汐评估— ClearML实验日志(来源:作者提供的gif)</p></figure><div class="lq lr ls lt gt ab cb"><figure class="pw lu px py pz qa qb paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><img src="../Images/23a98aed2e0cac56004cd5529d00deae.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*8-FC4bPftd1MMCVApo52-g.png"/></div></figure><figure class="pw lu qc py pz qa qb paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><img src="../Images/04b14760c7198b96d538b079b79687b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*_U24BtlL9p7DGXH-KYvx4g.png"/></div><p class="mb mc gj gh gi md me bd b be z dk qd di qe qf translated">EfficientDet-D0与EfficientDet-D7的TIDE和COCO评估结果(来源:作者提供的图片)</p></figure></div><p id="04d3" class="pw-post-body-paragraph kr ks iq ku b kv kw ka kx ky kz kd la mz lc ld le nb lg lh li nd lk ll lm ln ij bi translated">通过比较EfficientDet-D0和EfficientDet-D7型号的COCO评估结果，我们看到EfficientDet-D7的mAP比T22高出约15%。这是因为该型号的容量更大——与EfficentDet-D0相比，efficent det-D7中的参数多了<strong class="ku ja">到13倍。</strong></p><blockquote class="ko kp kq"><p id="84bb" class="kr ks kt ku b kv kw ka kx ky kz kd la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">注意:TIDE和pycocotools中的地图计算有一些已知的<a class="ae lo" href="https://github.com/dbolya/tide/issues/28" rel="noopener ugc nofollow" target="_blank">差异</a>。这就是可可地图和潮汐地图在某些情况下不相等的原因。</p></blockquote><p id="b916" class="pw-post-body-paragraph kr ks iq ku b kv kw ka kx ky kz kd la mz lc ld le nb lg lh li nd lk ll lm ln ij bi translated">当比较TIDE评估结果时，我们可以看到两个模型的<strong class="ku ja">分类和重复检测误差类似于</strong>，而D7模型的<strong class="ku ja">定位误差低2%。</strong></p><p id="c052" class="pw-post-body-paragraph kr ks iq ku b kv kw ka kx ky kz kd la mz lc ld le nb lg lh li nd lk ll lm ln ij bi translated"><strong class="ku ja">与D0相比，D7车型检测背景作为前景的误差高出2%，而D7车型遗漏的GT误差则小3.5%。这可以通过以下事实来解释，即D7模型具有较高的召回率，这降低了遗漏的GT误差，但同时增加了背景分类误差。</strong></p><p id="ac2e" class="pw-post-body-paragraph kr ks iq ku b kv kw ka kx ky kz kd la mz lc ld le nb lg lh li nd lk ll lm ln ij bi translated">比较的有趣部分是假阳性和假阴性的映射。我们可以看到两个模型的<strong class="ku ja">误报率非常相似。这可能是数据集</strong>中有一些未标记数据的标志，并且模型正在以高置信度检测它。另一方面，D7型号的<strong class="ku ja">假阴性率明显较低。很明显，较大的模型可以检测到较小容量模型难以处理的对象实例。</strong></p><p id="e69f" class="pw-post-body-paragraph kr ks iq ku b kv kw ka kx ky kz kd la mz lc ld le nb lg lh li nd lk ll lm ln ij bi translated">在这次比较中，我们比较了相同的模型架构(EfficientDet)和不同数量的参数(D0和D7)。<strong class="ku ja">当我们想要比较不同的模型架构的时候，TIDE模型评估就更加有用了</strong>，大家可以随意自己尝试一下。TIDE论文[1]包含对不同对象检测和分割模型的附加分析。</p><p id="1ef9" class="pw-post-body-paragraph kr ks iq ku b kv kw ka kx ky kz kd la mz lc ld le nb lg lh li nd lk ll lm ln ij bi translated">前面分析的潮汐图如下所示。</p><div class="lq lr ls lt gt ab cb"><figure class="pw lu qg py pz qa qb paragraph-image"><img src="../Images/368187a539f29f98045680372fc457c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*kxsFOYcfZbp7WHK5SuDbhw.png"/></figure><figure class="pw lu qg py pz qa qb paragraph-image"><img src="../Images/72c901542413ceb82dc868fa084c76d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*lTuS8nN2b6lxihJdScPy9Q.png"/><p class="mb mc gj gh gi md me bd b be z dk qh di qi qf translated">EfficientDet-D0与EfficientDet-D7的潮汐图(来源:作者提供的图片)</p></figure></div></div><div class="ab cl no np hu nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="ij ik il im in"><h1 id="bcb9" class="mf mg iq bd mh mi nv mk ml mm nw mo mp kf nx kg mr ki ny kj mt kl nz km mv mw bi translated">结论</h1><p id="4b98" class="pw-post-body-paragraph kr ks iq ku b kv mx ka kx ky my kd la mz na ld le nb nc lh li nd ne ll lm ln ij bi translated">最近，人工智能先驱吴恩达为以数据为中心的人工智能发起了一场<a class="ae lo" href="https://www.forbes.com/sites/gilpress/2021/06/16/andrew-ng-launches-a-campaign-for-data-centric-ai/" rel="noopener ugc nofollow" target="_blank">活动</a>，他的主要目标是将人工智能从业者的关注点从模型/算法开发转移到他们用来训练模型的数据质量上。虽然高质量的数据确实是实现模型高性能的关键，但是模型也起着关键作用:<strong class="ku ja">模型和数据不能完全解耦。</strong>在开发过程中，根据<strong class="ku ja">深入分析</strong>确定的瓶颈，只能将重点从一个切换到另一个。</p><p id="9074" class="pw-post-body-paragraph kr ks iq ku b kv kw ka kx ky kz kd la mz lc ld le nb lg lh li nd lk ll lm ln ij bi translated">在<a class="ae lo" href="http://forsight.ai" rel="noopener ugc nofollow" target="_blank"> Forsight </a>上，我们发现理解物体检测模型的性能必须超越普通的每秒帧数(FPS)和基于地图的精度指标。非常重要的是，不仅要天真地对模型进行基准测试，而且要明白你应该做些什么来进一步提高它们的性能。</p><p id="a486" class="pw-post-body-paragraph kr ks iq ku b kv kw ka kx ky kz kd la mz lc ld le nb lg lh li nd lk ll lm ln ij bi translated">使用像TIDE提供的更有洞察力的度量标准可以更容易地识别数据集中的具体问题，如未标记的图像、松散的边界框等。它还可以帮助您识别模型容量对于手头的特定任务来说不够大的情况。逐一解决这些问题最终会让您获得更好的机器算法，并帮助您为现实世界的应用程序创建生产就绪的模型！</p><blockquote class="ko kp kq"><p id="6319" class="kr ks kt ku b kv kw ka kx ky kz kd la lb lc ld le lf lg lh li lj lk ll lm ln ij bi translated">我们希望这篇博文对你有用，请看看我们团队在Forsight上写的其他一些博客，如果你有任何问题，请随时通过<strong class="ku ja"> info@forsight.ai </strong>联系我们！</p></blockquote><div class="qj qk gp gr ql qm"><a rel="noopener follow" target="_blank" href="/new-tf2-object-detection-api-5c6ea8362a8c"><div class="qn ab fo"><div class="qo ab qp cl cj qq"><h2 class="bd ja gy z fp qr fr fs qs fu fw iz bi translated">新TF2对象检测API</h2><div class="qt l"><h3 class="bd b gy z fp qr fr fs qs fu fw dk translated">欢迎新动物进入动物园——模型评估</h3></div><div class="qu l"><p class="bd b dl z fp qr fr fs qs fu fw dk translated">towardsdatascience.com</p></div></div><div class="qv l"><div class="qw l qx qy qz qv ra lz qm"/></div></div></a></div><div class="qj qk gp gr ql qm"><a href="https://medium.com/swlh/construction-feat-tf2-object-detection-api-4465a3937c87" rel="noopener follow" target="_blank"><div class="qn ab fo"><div class="qo ab qp cl cj qq"><h2 class="bd ja gy z fp qr fr fs qs fu fw iz bi translated">建筑壮举。TF2对象检测API</h2><div class="qt l"><h3 class="bd b gy z fp qr fr fs qs fu fw dk translated">你受到保护了吗？</h3></div><div class="qu l"><p class="bd b dl z fp qr fr fs qs fu fw dk translated">medium.com</p></div></div><div class="qv l"><div class="rb l qx qy qz qv ra lz qm"/></div></div></a></div><div class="qj qk gp gr ql qm"><a rel="noopener follow" target="_blank" href="/understand-your-algorithm-with-grad-cam-d3b62fce353"><div class="qn ab fo"><div class="qo ab qp cl cj qq"><h2 class="bd ja gy z fp qr fr fs qs fu fw iz bi translated">使用Grad-CAM了解您的算法</h2><div class="qt l"><h3 class="bd b gy z fp qr fr fs qs fu fw dk translated">当AI是一个黑匣子时，我们为什么要足够信任它来驾驶汽车，检测疾病，识别嫌疑人？</h3></div><div class="qu l"><p class="bd b dl z fp qr fr fs qs fu fw dk translated">towardsdatascience.com</p></div></div><div class="qv l"><div class="rc l qx qy qz qv ra lz qm"/></div></div></a></div></div><div class="ab cl no np hu nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="ij ik il im in"><h1 id="4986" class="mf mg iq bd mh mi nv mk ml mm nw mo mp kf nx kg mr ki ny kj mt kl nz km mv mw bi translated">参考</h1><ol class=""><li id="4f08" class="ob oc iq ku b kv mx ky my mz rd nb re nd rf ln og oh oi oj bi translated">丹尼尔·博尔亚、西恩·佛利、詹姆斯·海斯和朱迪·霍夫曼；<strong class="ku ja"> TIDE:识别物体检测错误的通用工具箱</strong>，<em class="kt"> ECCV 2020。</em></li><li id="2e7c" class="ob oc iq ku b kv ok ky ol mz om nb on nd oo ln og oh oi oj bi translated">帕迪拉、拉斐尔、帕索斯、韦斯利、迪亚斯、塔迪乌、内托、塞尔吉奥和达席尔瓦、爱德华多。<strong class="ku ja">与配套开源工具包</strong>、<em class="kt">电子2021的对象检测指标对比分析。，</em><a class="ae lo" href="https://doi.org/10.3390/electronics10030279" rel="noopener ugc nofollow" target="_blank"><strong class="ku ja">https://doi.org/10.3390/electronics10030279</strong></a></li><li id="f0aa" class="ob oc iq ku b kv ok ky ol mz om nb on nd oo ln og oh oi oj bi translated">图沙尔·科尔赫；<strong class="ku ja">如何通过理解数据提升物体检测精度</strong>，<a class="ae lo" href="https://blog.gofynd.com/boost-object-detection-model-accuracy-552586d698c" rel="noopener ugc nofollow" target="_blank">https://blog . gofynd . com/boost-object-detection-model-accuracy-552586 d698 c</a></li><li id="bbd2" class="ob oc iq ku b kv ok ky ol mz om nb on nd oo ln og oh oi oj bi translated">亚当·凯利；<strong class="ku ja">从头开始创建COCO注解</strong>，<a class="ae lo" href="https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch" rel="noopener ugc nofollow" target="_blank">https://www . immersivelmit . com/tutorials/Create-COCO-Annotations-从头开始</a></li><li id="27c2" class="ob oc iq ku b kv ok ky ol mz om nb on nd oo ln og oh oi oj bi translated">基普洛诺·伊利亚·克希；<strong class="ku ja">对象检测度量与工作示例</strong><a class="ae lo" rel="noopener" target="_blank" href="/on-object-detection-metrics-with-worked-example-216f173ed31e">https://towards data science . com/on-Object-Detection-Metrics-With-Worked-Example-216 f 173 ed 31 e</a></li><li id="7d38" class="ob oc iq ku b kv ok ky ol mz om nb on nd oo ln og oh oi oj bi translated">哈什特·库马尔；<strong class="ku ja">目标检测和分割的评估度量-图，</strong><a class="ae lo" href="https://kharshit.github.io/blog/2019/09/20/evaluation-metrics-for-object-detection-and-segmentation" rel="noopener ugc nofollow" target="_blank">https://kharshit . github . io/blog/2019/09/20/Evaluation-metrics-for-object-detection-and-segmentation</a></li></ol></div></div>    
</body>
</html>