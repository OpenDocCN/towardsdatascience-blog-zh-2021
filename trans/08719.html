<html>
<head>
<title>A Quick Guide to Decision Trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">决策树快速指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-quick-guide-to-decision-trees-bbd2f22f7f18?source=collection_archive---------34-----------------------#2021-08-11">https://towardsdatascience.com/a-quick-guide-to-decision-trees-bbd2f22f7f18?source=collection_archive---------34-----------------------#2021-08-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4103" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">很容易迷茫，只见树木不见森林。让我们仔细看看机器学习中的一些最常见的方法，以分析分类和回归任务的异构数据。</h2></div><p id="8d2e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">决策树是一种非常常见、直观且易于解释的数据分析方法，其目标是将实体分成不同的组(分类)或预测数量(回归)。DT在处理异质的非标准化输入特征方面也很出色(<em class="le">例如</em>分类的和连续的)。</p><p id="8c89" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有什么不喜欢的？</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/d4f71ce2e75de25aea5e6ce5a1310bd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4SwM9OdSzWInX_rOK28L7w.jpeg"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">由<a class="ae lv" href="https://unsplash.com/@tomzzlee?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">汤姆·帕森斯</a>在<a class="ae lv" href="https://unsplash.com/s/photos/forest-fork?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="415d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，一个简单的<strong class="kk iu">决策树</strong> (DT)是一系列的二元决策，接受一组输入特征，并基于这些特征分割输入数据，其方式类似于流行的游戏，如<em class="le"> 10个问题</em>或<em class="le">猜猜我是什么动物</em>。二元问题/答案对的路径可以存储为图形或一组规则。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi md"><img src="../Images/bf99d3e981bed9ab25b4f8ab2368e063.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*ag9_9AcbNR6kVaxycWpXEw.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">基本决策树的例子。图片作者。</p></figure><p id="d9e9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">可以创建具有多个最大可能节点的单个决策树，然后对其进行训练以优化目标函数。有许多常见的算法，如<a class="ae lv" href="https://en.wikipedia.org/wiki/C4.5_algorithm" rel="noopener ugc nofollow" target="_blank">c 4.5</a>(ID3的继承者)和<a class="ae lv" href="https://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/" rel="noopener ugc nofollow" target="_blank"> CART </a>(分类和回归树)。例如，在CART中,<a class="ae lv" href="https://www.learnbymarketing.com/481/decision-tree-flavors-gini-info-gain/#:~:text=Summary%3A%20The%20Gini%20Index%20is,2)%20of%20that%20class%20probability." rel="noopener ugc nofollow" target="_blank">基尼指数</a>被用作成本函数来测量分裂样本的“纯度”,而C4.5利用<a class="ae lv" rel="noopener" target="_blank" href="/entropy-how-decision-trees-make-decisions-2946b9c18c8">信息熵</a>来丰富任一类的叶子。所有算法都实现了一个停止条件，例如每个叶子中元素的最小数量(例如上图中的<em class="le"/>【昆虫】)或者是否达到了节点的最大数量(例如上图中的<em class="le"/>【鱼】和【蟹】)。在一个树被训练后，一个<a class="ae lv" href="https://en.wikipedia.org/wiki/Decision_tree_pruning" rel="noopener ugc nofollow" target="_blank">修剪程序</a>通常被应用来减少节点的数量:更简单的树是优选的，因为它们不太可能过度拟合数据，并且可以说花费更少的时间来执行。</p><p id="f319" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">虽然决策树简单且易于解释，但它们往往会过度拟合数据。然而，优势在于数量:boosting和ensemble方法可以有效地克服这个问题。</p></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="39b3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">决策树</strong>顾名思义，是从一组简单的决策树中构建的，这些决策树被用作“弱”分类器。根据树的组合方式定义了不同的方法。</p><p id="ae5f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae lv" href="https://en.wikipedia.org/wiki/Random_forest" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">随机森林</strong></a><strong class="kk iu">【RF’s】</strong>，这大概是最直观的，在分类中利用多数票或者在回归中平均。在这种情况下，每棵树输出<em class="le">，例如</em>类别索引(0或1)。最常见的答案作为最终结果。为了避免过度拟合，每个弱分类器在数据的子样本(具有替换)上被训练，并且仅利用输入特征的子集。这种双重欠采样过程简称为<a class="ae lv" href="https://en.wikipedia.org/wiki/Bootstrap_aggregating" rel="noopener ugc nofollow" target="_blank"> <em class="le">自举聚合</em> </a> <em class="le"> </em>或<em class="le">打包</em>。在不平衡数据集的情况下，应用类权重是一个常见的过程。有趣的是，随机森林也用于创建输入要素的稀疏<a class="ae lv" href="https://www.pinecone.io/learn/vector-embeddings/" rel="noopener ugc nofollow" target="_blank">矢量嵌入</a>。<a class="ae lv" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomTreesEmbedding.html" rel="noopener ugc nofollow" target="_blank">的基本思想</a>是对节点序列进行矢量化，并将它们保存在二进制元素的向量中。根据停止标准，向量的大小为<code class="fe me mf mg mh b">n_estimators * max_nodes</code>或<code class="fe me mf mg mh b">n_estimators * max_depth</code>。这种向量可以通过非常高效的线性分类器进行分析<em class="le">，例如</em>(例如参见scikit-learn网站上的<a class="ae lv" href="https://scikit-learn.org/stable/auto_examples/ensemble/plot_random_forest_embedding.html" rel="noopener ugc nofollow" target="_blank">本教程</a>)。</p><p id="b5ac" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">另一方面，<a class="ae lv" href="https://en.wikipedia.org/wiki/Gradient_boosting#Gradient_tree_boosting" rel="noopener ugc nofollow" target="_blank"> <strong class="kk iu">梯度提升树</strong></a><strong class="kk iu">【GBDT】</strong>为每个“弱分类器”<em class="le">【h(x)</em>分配不同的非负权重<em class="le"> w </em> ( <em class="le">即</em>一个相对重要性)，最终判决为<em class="le"> w_i * h_i(x) </em>之和。<em class="le"> boosting </em>其实就是把很多顺序连接的弱学习者组合起来，达到更好的效果。在这种方法中，通过添加越来越好的树来提高每一步的分类精度，从而最小化目标函数。为此，第<em class="le"> i </em>第<em class="le"> F_i </em>在第<em class="le"> (i-1) </em>第<em class="le">树F _(I-1)<em class="le">y—F _(I-1)</em>的残差上被训练。权重序列(<em class="le">w1</em>、<em class="le">w2</em>、…、<em class="le"> w_N </em>)结果是递减的，<em class="le">即</em>每棵树都是泰勒级数展开中的一种高阶修正。有关更多详细信息，请参考<a class="ae lv" href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html" rel="noopener ugc nofollow" target="_blank"> xgboost </a>库的文档，这可能是该算法最常见的实现。有趣的是，增强决策树在实验物理学中扮演着重要的角色，例如在欧洲核子研究中心对基本粒子属性的测量。</em></p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/539bd2e0c4473808e6635e8e99ddd9c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*bmWs662EvTqhR5KdfUX9vA.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">两种最流行的集成方法的视觉总结:随机森林(上)和梯度增强树(下)。图片作者。</p></figure></div><div class="ab cl lw lx hx ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="im in io ip iq"><p id="6ae7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们结束这场讨论之前，值得一提的是，自2021年5月下旬以来，谷歌的机器学习库<a class="ae lv" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>也提供了一个决策森林和梯度提升的实现，被恰当地称为<a class="ae lv" href="https://blog.tensorflow.org/2021/05/introducing-tensorflow-decision-forests.html" rel="noopener ugc nofollow" target="_blank"> TensorFlow决策森林(TF-DF) </a>。按照网站上的例子和<a class="ae lv" href="https://www.tensorflow.org/decision_forests/tutorials/beginner_colab" rel="noopener ugc nofollow" target="_blank"> Colab笔记本</a>，人们可以创建一个决策森林，并非常容易地适应<a class="ae lv" href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html" rel="noopener ugc nofollow" target="_blank">熊猫数据框架</a>，<em class="le">例如</em>:</p><pre class="lg lh li lj gt mj mh mk ml aw mm bi"><span id="0a48" class="mn mo it mh b gy mp mq l mr ms"># Install TensorFlow Decision Forests<br/>!pip install tensorflow_decision_forests<br/><br/># Load TensorFlow Decision Forests<br/>import tensorflow_decision_forests as tfdf<br/><br/># Load the training dataset using pandas<br/>import pandas<br/>train_df = pandas.read_csv("dataset.csv")<br/><br/># Convert the pandas dataframe into a TensorFlow dataset<br/>train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(train_df, label="category")<br/><br/># Train the model<br/>model = tfdf.keras.RandomForestModel()<br/>model.fit(train_ds)</span></pre><p id="d443" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">有趣的是，该库还提供了一些可视化工具来检查模型，<em class="le">例如</em> <code class="fe me mf mg mh b">tfdf.model_plotter.plot_model_in_colab(model, tree_idx=0)</code>。</p></div></div>    
</body>
</html>