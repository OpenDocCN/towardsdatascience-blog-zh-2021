<html>
<head>
<title>The Ultimate Guide to Bayesian Statistics</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">贝叶斯统计终极指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-ultimate-guide-to-bayesian-statistics-ed2940aa2bd2?source=collection_archive---------22-----------------------#2021-06-06">https://towardsdatascience.com/the-ultimate-guide-to-bayesian-statistics-ed2940aa2bd2?source=collection_archive---------22-----------------------#2021-06-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/555b843889531ee43234c43274422faa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ozW1-fKI4Jky7mW352tEkA.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">安妮·尼加德在<a class="ae jg" href="https://unsplash.com/s/photos/guide?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><div class=""/><div class=""><h2 id="eb74" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">像贝叶斯一样思考</h2></div><p id="716a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可能在统计课上听说过贝叶斯定理。基于条件概率的神奇公式开启了一种解释事件概率的新方法。在这篇文章中，我将讨论贝叶斯统计的基础知识，以及在统计推断方面Frequentist和贝叶斯之间的区别。此外，贝叶斯统计的一些应用。</p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h2 id="3985" class="mb mc jj bd md me mf dn mg mh mi dp mj lh mk ml mm ll mn mo mp lp mq mr ms mt bi translated">什么是贝叶斯统计</h2><p id="768c" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated"><strong class="la jk">贝叶斯统计</strong>是基于概率的贝叶斯解释的统计理论。要理解贝叶斯统计，首先需要理解条件概率和贝叶斯定理。</p><ol class=""><li id="2c60" class="mz na jj la b lb lc le lf lh nb ll nc lp nd lt ne nf ng nh bi translated"><strong class="la jk">条件概率</strong></li></ol><p id="faf1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">条件概率</strong>基于一个事件已经发生的事实，测量另一个事件发生的概率。正如下面的公式所示，假设事件B发生，事件A发生，是事件A和B的联合概率与事件B发生的概率的除法。</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/339987a8f4de1bd276bdfe40b462dfd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:286/0*QTZoLckOIF7dhcw7"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">条件概率</p></figure><p id="1c61" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">关于这个公式有两点需要注意。首先，不要被P(A|B)和P(AB)搞混了。前者是条件概率，后者是联合概率。即使我们观察到事件A和事件B以两种概率发生，事件发生的时间线是区分它们的关键。思考一个投掷两个公平骰子的例子。设事件A是第一个骰子是偶数，事件B是两个骰子之和是6。联合概率是事件A和事件B同时发生的几率，除以所有情况。当投掷两个骰子时，有36 (6*6)种结果，当事件A和事件B一起发生时，只有两种情况:我们投掷(2，4)或(4，2)。因此，联合概率P(AB)是2/36。条件概率P(A|B)的计算方式不同。当B发生时，我们只有3种情况:(2，4)，(3，3)，(4，2)。在这三个结果中，有两个结果满足事件A的条件，在这种情况下，条件概率P(A|B)是2/3。如果用条件概率公式计算条件概率，需要知道P(B)的概率，P(B)如上所述是3/36。P(A|B)将是2/36除以3/36，我们也得到2/3。</p><p id="a5ba" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">第二，如果事件A和事件B是独立的，则条件概率P(A|B)与P(A)相同。这是因为事件B的发生不会为我们了解事件A带来任何有用的信息。当我们说事件A和事件B是独立的，这意味着事件A的发生不会影响事件B，反之亦然。事件A和事件B的联合概率将等于P(A)*P(B)。因此，两个独立事件的条件概率将是:</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/f64c277ddc3c3e356858fa1a508569f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/0*Fyg1Omima0PK3BCq"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">两个独立事件的条件概率</p></figure><p id="fce6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">上面的公式表明，知道事件B发生对计算事件A发生的概率没有帮助。</p><p id="3c1f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> 2。链式法则和求和法则</strong></p><p id="7c63" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">根据条件概率，我们可以推导出两个事件的<strong class="la jk">链规则</strong>:</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div class="gh gi no"><img src="../Images/3c3273e0fea4211980cddadbe84ab2ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:618/0*_HDReLG_cTz6fj9T"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">链式法则</p></figure><p id="d845" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">链式法则也适用于大量的事件。对于具有以下相关性的事件X、Y、Z:</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div class="gh gi np"><img src="../Images/45915a45f2f70ea37545365d10dd4fa1.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*UnYYsfRsZ3fKe3uH93hY8g.png"/></div></figure><p id="89f3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">链式规则指定:</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/3ce78b105dd9cf350c0eb70fa715e44d.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/0*lRDpufHTpk1hWDvi"/></div></figure><p id="d8be" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以得出N个事件的联合概率如下:</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/867aa9a086fb708ef1b80c56f286e3f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/0*F0NZQyL7g4vMCbtd"/></div></figure><p id="3732" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">除了链式法则，概率论中的另一个重要法则是<strong class="la jk">求和法则</strong>，它通过整合联合概率来帮助推导出完整的概率:</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/fd7df86aad556158667c91b38634242f.png" data-original-src="https://miro.medium.com/v2/resize:fit:348/0*MqFqHel9ExBM_0fX"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">求和规则</p></figure><p id="548e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">概率论的这两条规则是贝叶斯定理的基础，我将在下一节详细说明。</p><p id="619b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> 3。贝叶斯定理</strong></p><p id="ba63" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">离链式法则只有一步之遥，我们将得到贝叶斯定理:</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/cbdf0a23c0dbd509824fcea8e3b56c75.png" data-original-src="https://miro.medium.com/v2/resize:fit:380/0*0uussBWETX2KObIR"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">贝叶斯定理</p></figure><p id="e03b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">贝叶斯定理是贝叶斯统计的基础。它帮助贝叶斯以不同的方式解释统计推断。贝叶斯声称，随着更多的证据或信息变得可用，假设的概率可以更新。在统计推断过程中，我们试图使用我们拥有的数据来估计概率模型的参数。如果我们将P(θ)视为我们试图从模型中估计的参数的概率，将P(X)视为我们观察到的数据的概率，我们可以应用贝叶斯定理:</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/fa90ecbc24a2e0cf51daa9e873535d00.png" data-original-src="https://miro.medium.com/v2/resize:fit:358/0*lPcMwqKi9RdIkrha"/></div></figure><p id="b4d5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里P(θ|X)称为<strong class="la jk">后验概率</strong>，是给定数据的观测参数θ的条件概率。另一方面，P(X|θ)被称为<strong class="la jk">似然函数</strong>，在给定参数的情况下观察数据的概率。似然函数显示了参数对数据的解释程度。我们也称P(θ)为<strong class="la jk">先验</strong>概率，P(X)为<strong class="la jk">证据</strong>。</p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h2 id="2569" class="mb mc jj bd md me mf dn mg mh mi dp mj lh mk ml mm ll mn mo mp lp mq mr ms mt bi translated"><strong class="ak">频率主义者vs贝叶斯</strong></h2><p id="f652" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">不知道频率主义者和贝叶斯的区别，我们永远无法完全理解贝叶斯。在下表中，我总结了它们的主要区别。</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nv"><img src="../Images/1a56ac7b99495cd98d2ace92f25036cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3Kz6uMu6aHXe8_qgfiJ_ig.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">常客vs贝叶斯</p></figure><p id="b181" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们举一个估计下雨概率P(θ)的例子来更好地说明差异。常客对于不确定性是客观的。对他们来说，下雨的概率P(θ)是固定的，先验知识或辅助信息不会帮助他们估计它。相反，他们有一些关于θ的假设，并持续记录长时间的天气状况(X: x1，x2…xn)来帮助他们学习P(θ)。常客把任何一天的天气状况视为随机变量。通过估计似然函数P(X|θ)，即在给定下雨概率的情况下观察这些天天气状况的概率，他们可以使用P值或置信区间拒绝或接受他们的假设。或者，他们可以计算argmax P(X|θ)相对于θ的值，以获得使观测数据X的似然性最大化的θ，这就是我们所说的最大似然估计(MLE)。对于像分类问题这样的机器学习问题，frequentist ists首先选择一个模型，如逻辑回归，然后使用他们拥有的数据(X和相应的标签y)来使用MLE估计模型参数θs，最后使用训练的模型来预测新数据的标签。</p><p id="34ba" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">另一方面，贝叶斯通过概率分布对不确定性建模。他们认为他们观察到的数据X是固定的，而θ是一个随机变量，并受先验信息的影响。因此，可以利用附加信息来估计概率P(θ)。例如，不同月份下雨的概率是不同的，因此知道现在是几月将有助于估计下雨的可能性。他们使用贝叶斯定理作为P(θ)来估计后验概率P(θ|X)。对于分类问题，他们首先通过估计后验概率来训练模型:</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/4c2837477da52d72a2216876169830a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:514/0*5xyVKweXxZn_qREC"/></div></figure><p id="9934" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后应用求和规则用新数据X_ts进行标签预测y_ts:</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/687cf52b2ebc1d73a98b18286c01a54c.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/0*HU2pqN5vhe40bnqN"/></div></figure><p id="9b9b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">总之，频率主义者和贝叶斯在是否将参数θ视为随机变量的观点上分歧最大。贝叶斯认为参数是随机变量。他们根据数据和先验分布赋予不同的概率，即后验概率:</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/a4ea74a5cd9efa12f64fac3f14ac7612.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/0*ibAS_Kv3EfHfM8Pn"/></div></figure><p id="af0d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">虽然常客认为他们应该将参数固定在一个值，但使用概率观点，我们可以将他们的心态写成如下:</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/75e4c9e4a87817046f1f967d8c3f2dcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:362/0*LybrF0Q7rbEzT_ef"/></div></figure><p id="965f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后他们进行实验收集数据，或者用观测数据X检验他们的假设θ是否为θ*，或者用MLE求θ*。</p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h2 id="462b" class="mb mc jj bd md me mf dn mg mh mi dp mj lh mk ml mm ll mn mo mp lp mq mr ms mt bi translated">贝叶斯统计的应用</h2><p id="18d5" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">贝叶斯统计在进行推理时使用先验知识，并随着获取更多信息而更新后验信念。因此，它被用于机器学习和统计推断的各种应用中，给出可靠的结果。</p><ol class=""><li id="f87a" class="mz na jj la b lb lc le lf lh nb ll nc lp nd lt ne nf ng nh bi translated"><strong class="la jk">线性回归正则化的贝叶斯解释</strong>:</li></ol><p id="4a7a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">线性回归模型是用于解决机器学习中的回归问题的简单模型。我们通常在线性模型中使用正则化来处理过拟合。正则化是在拟合模型时将参数的大小添加到成本函数中。因此，它迫使模型选择较少的特征或减少特征参数的大小，并减少过度拟合的机会，尤其是当有大量特征时。根据惩罚的形式，我们有L1(拉索)和L2(岭)正规化。L1正则化将参数的绝对值添加到成本函数，而L2正则化添加参数的平方。我们可以用贝叶斯定理来解释正则化。从贝叶斯的角度来看，参数由以下因素决定:</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oa"><img src="../Images/a32e767b2dd84865d5f48a42d410fb94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*vcXCgC80ZVIVNeMx.png"/></div></div></figure><p id="3f8c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当忽略先验分布，只最大化似然函数来估计β时，我们将得到正则线性回归模型。然而，当添加关于先验分布的假设时，我们将正则化添加到模型中。加上拉普拉斯分布的先验，我们得到L1正则化。而对于L2正则化，我们增加了β服从均值等于零的正态分布的假设。更多细节和数学证明，可以参考<a class="ae jg" href="https://bjlkeng.github.io/posts/probabilistic-interpretation-of-regularization/" rel="noopener ugc nofollow" target="_blank">这篇文章</a>。</p><p id="87cc" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">2.<strong class="la jk">潜在变量模型</strong></p><p id="1384" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">潜变量也叫隐藏变量，是我们不直接从数据中观察到的变量。然而，我们认为数据中存在一些隐藏的结构，可以提取出来用于更好的预测。</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/0b275e8e7d73bf0878a0d6fff9eb1095.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*EjDFZNEWBE00noYZuPT9tw.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">潜在变量t，数据X</p></figure><p id="5f56" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">条件概率和贝叶斯定理是构造隐变量模型的基础。对于被设置成聚类成N个组的数据，我们可以将似然函数写成如下:</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/a77cdfb92ee21a989b59e6d5905fc3e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/0*RpeVoAL68qRJse2I"/></div></figure><p id="5e83" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以使用潜在变量来处理数据中的缺失值，并且我们可以构建潜在变量模型来进行聚类，例如<a class="ae jg" rel="noopener" target="_blank" href="/gaussian-mixture-models-explained-6986aaf5a95"> GMM </a>用于客户聚类，或者<a class="ae jg" href="https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf" rel="noopener ugc nofollow" target="_blank"> LDA </a>用于文本主题建模。</p><p id="40fa" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">3 <strong class="la jk">。在线学习</strong></p><p id="c8b2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于贝叶斯统计推断使用新的证据来不断更新后验概率，因此它对于使用以下公式的在线学习也非常有用:</p><figure class="nj nk nl nm gt iv gh gi paragraph-image"><div class="gh gi od"><img src="../Images/f72d05ac8f10d8d14f71523c8cdb5a5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/0*7F9NtjL6_eSujGJi"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">在线学习</p></figure><p id="5412" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">随着新数据X_t的到来，后验概率P(θ)也从P_t-1(θ)更新到P_t(θ)。</p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><p id="e4de" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">贝叶斯统计不仅仅是关于贝叶斯定理或为不同条件概率指定的花哨术语，更重要的是，贝叶斯思维模式，我们可以使用先验知识进行初步猜测，并不断用新信息更新推论。希望这篇文章能帮助你更好地理解这种心态以及它与常客的区别。</p><p id="48f8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢您的阅读！这是我所有博客帖子的列表。如果你感兴趣的话，可以去看看！</p><div class="is it gp gr iu oe"><a href="https://zzhu17.medium.com/my-blog-posts-gallery-ac6e01fe5cc3" rel="noopener follow" target="_blank"><div class="of ab fo"><div class="og ab oh cl cj oi"><h2 class="bd jk gy z fp oj fr fs ok fu fw ji bi translated">我的博客文章库</h2><div class="ol l"><h3 class="bd b gy z fp oj fr fs ok fu fw dk translated">我快乐的地方</h3></div><div class="om l"><p class="bd b dl z fp oj fr fs ok fu fw dk translated">zzhu17.medium.com</p></div></div><div class="on l"><div class="oo l op oq or on os ja oe"/></div></div></a></div><div class="is it gp gr iu oe"><a href="https://zzhu17.medium.com/membership" rel="noopener follow" target="_blank"><div class="of ab fo"><div class="og ab oh cl cj oi"><h2 class="bd jk gy z fp oj fr fs ok fu fw ji bi translated">阅读朱(以及媒体上成千上万的其他作家)的每一个故事</h2><div class="ol l"><h3 class="bd b gy z fp oj fr fs ok fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="om l"><p class="bd b dl z fp oj fr fs ok fu fw dk translated">zzhu17.medium.com</p></div></div><div class="on l"><div class="ot l op oq or on os ja oe"/></div></div></a></div></div></div>    
</body>
</html>