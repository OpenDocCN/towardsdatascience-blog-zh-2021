<html>
<head>
<title>Understanding PCA</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解PCA</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-pca-90f7b1961fa4?source=collection_archive---------14-----------------------#2021-04-27">https://towardsdatascience.com/understanding-pca-90f7b1961fa4?source=collection_archive---------14-----------------------#2021-04-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/3a04265ca872ad52529846f815e54a50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2irBt7dlU8xk8e3CKbbMRw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">插图:寻找主成分。所有图片均由作者提供。</p></figure><div class=""/><div class=""><h2 id="89b4" class="pw-subtitle-paragraph kf jh ji bd b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw dk translated">主成分分析的深入解释</h2></div><p id="ba92" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">用眼睛看数据的能力是机器学习中最不受重视的事情之一。在进行任何机器学习之前，可视化数据可以揭示许多模式。探索它们可以在以后获得巨大的回报，因为我们可以获得关于使用什么算法、忽略什么特性等的良好直觉。</p><p id="5aad" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">然而，有一个大问题:人类无法看到大于三维的空间。因此，对于现实生活中的数据集，我们必须执行某些技巧，以受益于可视化可以提供的洞察力。</p><p id="97e0" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">主成分分析(PCA)是实现这一目的的最基本和最有用的方法之一。主成分分析将数据线性转换到一个空间，突出每个新特征的重要性，从而使我们能够修剪那些没有揭示太多的特征。</p><p id="b2c4" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">首先，我们将对PCA有一个直观的了解，然后我们进行更详细的数学处理。</p><h1 id="459b" class="lt lu ji bd lv lw lx ly lz ma mb mc md ko me kp mf kr mg ks mh ku mi kv mj mk bi translated">PCA背后的主要思想</h1><p id="602e" class="pw-post-body-paragraph kx ky ji kz b la ml kj lc ld mm km lf lg mn li lj lk mo lm ln lo mp lq lr ls im bi translated">为了更好地理解PCA的作用，我们将看看它在一个简单数据集上的表现。</p><p id="59dc" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">假设我们的数据存储在矩阵中</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/6a64d3881595dee0476093f8019f9f9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:464/format:webp/1*oVr1lsd7B5hSHFlTEjtYsw@2x.png"/></div></figure><p id="48cd" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">其中<em class="mv"> n </em>代表特征，<em class="mv"> d </em>代表样本数。为了简单起见，我们假设<em class="mv"> n = 2 </em>，这样我们就可以想象正在发生什么。然而，所有这些都适用于一般情况。</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/5ecdab956bd088c9f3c86534e7ab04d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BSqBACgT24ZWGXjZDzG8dQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">简单的数据集。</p></figure><p id="cff2" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">我们可以立即观察到的是，x₁的特征<em class="mv">和x₂的特征</em>可能不是我们数据的最佳描述符。这种分布似乎在两个变量之间延伸，尽管形状表明数据可以从特定的角度简化。这些特征似乎是相互关联的:如果x₁小，那么x₂大。如果x₁<em class="mv">大，x₂<em class="mv">小。这意味着变量包含冗余信息。当我们有数百个特性时，冗余是次优的。如果我们能够将数据重塑为变量不相关并根据重要性排序的形式，我们就能够丢弃那些不太有表现力的变量。</em></em></p><p id="b49f" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">我们可以用线性代数的语言来描述这个过程。为了使数据去相关，我们希望<em class="mv">对角化</em>X的协方差矩阵。因为经验协方差矩阵</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mw"><img src="../Images/be9d6012ff7ad27611fdcab37f6a5457.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xXZQjD-oJZl1neYnpWeJTA@2x.png"/></div></div></figure><p id="7aa5" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">是对称的，<a class="ae mx" href="https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix#Real_symmetric_matrices" rel="noopener ugc nofollow" target="_blank">频谱分解定理</a>保证作为正交矩阵<em class="mv"> U </em>使得</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi my"><img src="../Images/a2f3ccc244f21e72c20f818c2d2b34f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jhuD7xsbC7lyaDcVvx8thg@2x.png"/></div></div></figure><p id="bfe2" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">(注意，在不失一般性的情况下，我们可以假设对角线中的元素是递减的。)</p><p id="b8ef" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">所以，我们有</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi mz"><img src="../Images/63d56eab7dcea44f6f74a26b4040542c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5o2PBBc08TqpQbjVl2qSNg@2x.png"/></div></div></figure><p id="b721" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">如果我们定义变换</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="gh gi na"><img src="../Images/a373c4c389c896b70c46b372f28cdf78.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*OL_pY4S_3RcK0J02bHIa2A@2x.png"/></div></figure><p id="7931" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><em class="mv"> Y </em>表示具有不相关特征的数据视图。这就是<em class="mv">主成分分析</em>的精髓。<em class="mv"> Y </em>的每个特征都是<em class="mv"> X </em>特征的线性组合。<em class="mv"> Y </em>称为<em class="mv"> X </em>的<em class="mv">主成分向量</em>，而其特征称为<em class="mv">主成分</em>。</p><p id="b61e" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">应用到我们的简单数据集，这是结果。</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/2e8fb0ff019cb425714557912fd313c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zYuvlDCWUdOCsbqYs2uSSg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">简单玩具数据集的PCA变换。</p></figure><h1 id="bf66" class="lt lu ji bd lv lw lx ly lz ma mb mc md ko me kp mf kr mg ks mh ku mi kv mj mk bi translated">主成分使方差最大化</h1><p id="5f1b" class="pw-post-body-paragraph kx ky ji kz b la ml kj lc ld mm km lf lg mn li lj lk mo lm ln lo mp lq lr ls im bi translated">除了不相关的性质<em class="mv"> Y </em>之外，还有一点使其特征独一无二:每个主成分在投影到数据上时都使数据的方差最大化。</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/3a04265ca872ad52529846f815e54a50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2irBt7dlU8xk8e3CKbbMRw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">寻找一个方向，使投射到它上面的数据的方差最大化。</p></figure><p id="760b" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">PCA可以被认为是寻找投影数据的方差最大的方向的迭代过程。</p><p id="ea48" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">一般来说，第<em class="mv"> k </em>个主分量可以通过寻找与第一个<em class="mv"> k-1个</em>主分量正交的单位向量，并使投影到其上的数据的方差最大化来获得。</p><h1 id="8a35" class="lt lu ji bd lv lw lx ly lz ma mb mc md ko me kp mf kr mg ks mh ku mi kv mj mk bi translated">主成分分析降维</h1><p id="7ccd" class="pw-post-body-paragraph kx ky ji kz b la ml kj lc ld mm km lf lg mn li lj lk mo lm ln lo mp lq lr ls im bi translated">既然我们已经了解了PCA是如何工作的，我们应该研究它在真实数据集上是如何工作的。除了消除特征中的冗余，它还可以用来修剪那些没有传达很多信息的特征。回想一下，协方差矩阵</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="gh gi na"><img src="../Images/a373c4c389c896b70c46b372f28cdf78.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*OL_pY4S_3RcK0J02bHIa2A@2x.png"/></div></figure><p id="f6d0" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">是对角线，特征的方差按降序排列:</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nb"><img src="../Images/48d8d782363d84ff736ffcc08a7e38c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mQ6X6ULP_ggE8f3KWQCPxg@2x.png"/></div></div></figure><p id="0af4" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><em class="mv">解释了方差</em>对<strong class="kz jj"><em class="mv"/></strong><em class="mv"/>的比值定义</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/29da933958acf8fd3a8cb10e7c498e67.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*Xuda4Eytu_a6p8Eld_zVPg@2x.png"/></div></figure><p id="f704" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">可以认为是数据中“所有方差”的百分比。因为方差在减小，所以最有意义的特征出现在最前面。这意味着如果累计解释方差</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/6d6fae0e4699e61da2bb71e806515320.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*uYSo-xBlTV3c9QZQlw1ghQ@2x.png"/></div></figure><p id="8548" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">对于一些<em class="mv"> k </em>来说足够大，比如说在<em class="mv"> 95% </em>左右，在<em class="mv"> k </em>之后的主分量可以被丢弃，而没有显著的信息损失。</p><p id="c74e" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">为了了解它在真实数据上的表现，我们将看看PCA在著名的<a class="ae mx" href="https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html" rel="noopener ugc nofollow" target="_blank"> Iris数据集</a>上的表现！该数据集由来自三种不同种类鸢尾的四个特征(萼片长度、萼片宽度、花瓣长度、花瓣宽度)组成。这是数据集的外观。</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ne"><img src="../Images/ac03500bb79a95cd7bd95393a1c34e5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LIjySiVTG_Y0FcB2BH15EA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">虹膜数据集。</p></figure><p id="f8ae" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">通过检查，我们可以看到一些特性的组合很好地分离了数据，一些则没有。他们的顺序绝对不意味着他们的重要性。通过计算协方差矩阵，我们还可以注意到它们似乎是相关的:</p><pre class="mr ms mt mu gt nf ng nh ni aw nj bi"><span id="4bab" class="nk lu ji ng b gy nl nm l nn no">[[ 0.68112222 -0.04215111 1.26582 0.51282889]<br/> [-0.04215111 0.18871289 -0.32745867 -0.12082844]<br/> [ 1.26582 -0.32745867 3.09550267 1.286972 ]<br/> [ 0.51282889 -0.12082844 1.286972 0.57713289]]</span></pre><p id="87b2" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">让我们应用PCA来看看它对我们的数据集做了什么！</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ne"><img src="../Images/3002145ede8b9bcd946dda5c1ebdc3f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3T82Qv_O3KY64Po3fJc9oA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">虹膜数据集的PCA变换。</p></figure><p id="f27a" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">前两个主成分几乎完全描述了数据集，而第三个和第四个主成分可以省略而不会明显丢失信息。协方差矩阵看起来也更好。它本质上是对角的，所以特征彼此不相关。</p><pre class="mr ms mt mu gt nf ng nh ni aw nj bi"><span id="5851" class="nk lu ji ng b gy nl nm l nn no">[[ 1.57502004e+02 -3.33667355e-15 2.52281147e-15 5.42949940e-16]<br/> [-3.33667355e-15 9.03948536e+00 1.46458764e-15 1.37986097e-16]<br/> [ 2.52281147e-15 1.46458764e-15 2.91330388e+00 1.97218052e-17]<br/> [ 5.42949940e-16 1.37986097e-16 1.97218052e-17 8.87857213e-01]]</span></pre><h1 id="5767" class="lt lu ji bd lv lw lx ly lz ma mb mc md ko me kp mf kr mg ks mh ku mi kv mj mk bi translated">PCA不做的是</h1><p id="39dc" class="pw-post-body-paragraph kx ky ji kz b la ml kj lc ld mm km lf lg mn li lj lk mo lm ln lo mp lq lr ls im bi translated">尽管PCA经常用于特征工程，但它的功能是有限的。例如，如果数据集被拉伸得很薄，并且它们之间的分隔边距很小(如下例所示)，则PCA不会提供类之间差异较大的表示。</p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi np"><img src="../Images/6a61fb0648a7c5dade55f78970be8178.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EQ6Qyl79LGHb2eODyZ-G_Q.png"/></div></div></figure><p id="3ecb" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated">原因是正交变换给出主分量向量，并且正交变换保持距离。因此，这不会沿任何特征拉伸空间。这是优点，同时也是缺点。在二维空间中，正交变换只是旋转和反射的组合。</p></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><p id="583c" class="pw-post-body-paragraph kx ky ji kz b la lb kj lc ld le km lf lg lh li lj lk ll lm ln lo lp lq lr ls im bi translated"><a class="ae mx" href="https://www.tivadardanka.com/blog" rel="noopener ugc nofollow" target="_blank"> <strong class="kz jj"> <em class="mv">如果你喜欢把机器学习概念拆开，理解是什么让它们运转，我们有很多共同点。看看我的博客，我经常在那里发表这样的技术文章！</em> </strong> </a></p><figure class="mr ms mt mu gt iv gh gi paragraph-image"><a href="http://twitter.com/TivadarDanka"><div class="gh gi nx"><img src="../Images/ecb92c175d3ce7f69abfe2f3e8b6d357.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*NMbszMf8CZmSDH5rcALlBw.png"/></div></a></figure></div></div>    
</body>
</html>