<html>
<head>
<title>4 Useful techniques that can mitigate overfitting in decision trees</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">4种可以减轻决策树过度拟合的有用技术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/4-useful-techniques-that-can-mitigate-overfitting-in-decision-trees-87380098bd3c?source=collection_archive---------12-----------------------#2021-05-16">https://towardsdatascience.com/4-useful-techniques-that-can-mitigate-overfitting-in-decision-trees-87380098bd3c?source=collection_archive---------12-----------------------#2021-05-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1dc9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">预修剪、后修剪和创建系综</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/061b875fdff65057ed482a5b8727fd0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dt011jNJKdhBZTheYsPkcQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@emtm17?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">玛丽莎&amp;埃里克</a>在<a class="ae ky" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="8c68" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="lv">过拟合</em> </strong>对于数据科学家来说，这是继缺失值和离群值之后的第二大问题。与其他机器学习模型相比，决策树中的过度拟合很容易发生，因为它们的学习算法可以产生大型复杂的决策树，这些决策树完全适合训练实例。一般来说，过度拟合是当学习算法完美地拟合训练数据但未能对新的看不见的数据(测试/验证数据)进行概括时发生的问题。在过度拟合中，模型正在<em class="lv">记忆</em>数据集中的噪声，并且未能<em class="lv">学习</em>数据中的重要模式。过度拟合会影响由不属于训练集的样本做出的预测的准确性(或任何其他模型评估矩阵)。</p><p id="7073" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">人们可以在训练数据上以100%的准确度建立完美的决策树模型，但是在测试数据上的准确度非常低。看看下面的Python代码，它在<a class="ae ky" href="https://drive.google.com/file/d/1zQ3GzQrhtR4_cwlSbs7pAA34ekGkwz_c/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">“heart _ disease”数据集</a>上构建了这样一个决策树模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段1:决策树基础模型(等到加载！)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/5571d91e9c3ae6cb35458c2b30b4dd17.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*HyY9Zm81uZfkVKBrQ0onvA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">输出1(图片由作者提供)</p></figure><p id="8342" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如您在输出中看到的，训练准确率为100%，测试准确率为71%，远低于训练准确率。所以，很明显我们的决策树模型是过拟合的。在本文中，我们将使用这个模型作为基础模型<em class="lv">以便与其他模型进行比较。</em></p><p id="5227" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有很多可用的<em class="lv">通用</em>技术可以减轻或防止机器学习模型中的过拟合问题。然而，在本文中，我们将讨论4种特定于决策树的技术。</p><p id="4731" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注:</strong>如果你想了解更多关于使用决策树进行回归任务的知识，请阅读我写的这篇文章。但是，这并不是今天内容的前提。</p><h1 id="2bbb" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">技术概述</h1><p id="bfd6" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated">这里是我们将在本帖中讨论的技术的概述。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/694804003a7b25ff48a5e2bc48f5fa68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*enuV1O5klkcAmLo5O6KVIA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><h1 id="cbe1" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">决策树剪枝的一般思想</h1><p id="88e8" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated">在一般情况下，<em class="lv">修剪</em>是指移除树木或植物的特定部分以促进其生长的过程。同样的事情也适用于决策树，它不是真正的树，而是机器学习模型。在决策树中，修剪是用于控制或限制树的深度(大小)的过程。默认情况下，创建决策树模型超参数是为了将树扩展到其最大深度。这些树被称为<strong class="lb iu"> <em class="lv">成年树</em> </strong>，它们总是过度生长。然而，我们可以调整这些超参数来生成没有完全生长和没有过度拟合的树！</p><p id="292a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有两种类型的修剪技术:<strong class="lb iu">预修剪</strong>和<strong class="lb iu">后修剪</strong></p><h2 id="17ce" class="mx ma it bd mb my mz dn mf na nb dp mj li nc nd ml lm ne nf mn lq ng nh mp ni bi translated">预修剪</h2><p id="c6b5" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated"><em class="lv">预剪枝</em>是一个早期停止规则，在早期停止决策树的生长。它会生成一棵分枝较少的树。可以通过调整Scikit-learn<strong class="lb iu">decision tree classifier()</strong>或<strong class="lb iu">decision tree regressor()</strong>类中的以下超参数来应用预修剪。这些超参数指定了停止条件的阈值。</p><ul class=""><li id="2f2a" class="nj nk it lb b lc ld lf lg li nl lm nm lq nn lu no np nq nr bi translated"><strong class="lb iu"> max_depth: </strong>指定树的最大深度。这控制了分支的复杂性(即进行分割的次数)。如果<em class="lv"> None </em>(默认)，则节点被扩展，直到所有叶子都是纯的(即以100%的准确度拟合模型)。减小该值可以防止过度拟合。</li><li id="df17" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated"><strong class="lb iu"> min_samples_leaf: </strong>决定<strong class="lb iu"> </strong>一个叶节点需要的最小样本数。默认值为1。增加该值可以防止过度拟合。</li><li id="3be3" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated"><strong class="lb iu"> min_samples_split: </strong>指定分割内部节点所需的最小样本数。默认值为2。如果我们将它的值设置为5，那么不允许对具有5个或更少样本的节点进行进一步的分割。</li></ul><p id="164b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些超参数中，<strong class="lb iu"> max_depth </strong>是最重要的一个。我们可以通过单独调整<strong class="lb iu"> max_depth </strong>超参数轻松地对决策树进行预修剪。现在，我们将看到单独使用<strong class="lb iu"> max_depth </strong>的影响，同时保持其他预修剪超参数的默认值。</p><h2 id="d80d" class="mx ma it bd mb my mz dn mf na nb dp mj li nc nd ml lm ne nf mn lq ng nh mp ni bi translated"><strong class="ak">技术1:单独调整max_depth，观察单个超参数的影响</strong></h2><p id="e47c" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated">在不进行网格搜索或随机搜索的情况下，找出单个<strong class="lb iu"> max_depth </strong>超参数对训练和测试数据的影响有时是有用的。验证曲线是一种非常有用的技术，可用于测量单个超参数的影响。以下代码使用上述决策树基本模型创建验证曲线。请注意，下面的代码是从代码片段1(见上文)开始的。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段2:绘制max_depth的影响(等到加载！)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/5b49bb6d4e65ba47d518a829d87ef177.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*pW-ILYPlHCwW8bahS13F7g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">输出2(图片由作者提供)</p></figure><p id="9153" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，在图中，训练集的准确度分数被标记为“训练分数”，测试集的准确度分数被标记为“交叉验证分数”。在<strong class="lb iu"> max_depth </strong>值为3时，模型正好合适(不会过拟合或欠拟合)。该模型非常适合训练数据，并且还可以推广到新的未见过的数据。让我们再次构建决策树，但现在使用<strong class="lb iu"> max_depth=3 </strong>，并将预测与基础模型的预测进行比较。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段3:创建max_depth=3的决策树(等到加载！)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/e4845cfc2edb623a8e40d7630ccb1b49.png" data-original-src="https://miro.medium.com/v2/resize:fit:948/format:webp/1*p65mOsi191VmQ6UvyzHfgw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">输出3(图片由作者提供)</p></figure><p id="53f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与基本模型相比，测试精度从0.71提高到0.74。除此之外，假阴性的数量(将真正的心脏病患者归类为非心脏病患者——这要危险得多！)已显著减少。因此，模型按照我们想要的方式运行。</p><p id="0c33" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注:</strong>如果你想了解更多关于绘制和解释验证曲线的知识，请阅读<a class="ae ky" rel="noopener" target="_blank" href="/validation-curve-explained-plot-the-influence-of-a-single-hyperparameter-1ac4864deaf8">我写的这篇文章</a>。</p><h2 id="1c06" class="mx ma it bd mb my mz dn mf na nb dp mj li nc nd ml lm ne nf mn lq ng nh mp ni bi translated">技术2:一次调优多个超参数</h2><p id="83d7" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated">有时，单独调整单个超参数可能效率不高。另一种预修剪技术是寻找多个超参数值的新组合，以减轻过拟合。在这种情况下，不能使用验证曲线，因为它被设计用于绘制单个超参数的影响。这里，我们可以使用网格搜索结合交叉验证来一次调优多个超参数。下面的代码使用上面的基本模型执行交叉验证的超参数调整。请注意，下面的代码是从代码片段1(见上文)开始的。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段4:一次调优多个超参数(等到加载！)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/6a78de2d112e95259e2d4177923f0e9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6SL80dq1C6f0xyB8M34f4Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">输出4(图片由作者提供)</p></figure><p id="13f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与基础模型和之前的<strong class="lb iu"> max_depth=3 </strong>模型相比，测试精度有了显著提高。除此之外，假阳性和假阴性都大大减少了。现在，我们的模型没有过度拟合，测试数据的性能也得到了提高。</p><p id="9196" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注:</strong>如果你想了解更多关于k-fold交叉验证和超参数调优技术的知识，请阅读我写的<a class="ae ky" rel="noopener" target="_blank" href="/k-fold-cross-validation-explained-in-plain-english-659e33c0bc0">这篇帖子</a>。</p><h2 id="d3a3" class="mx ma it bd mb my mz dn mf na nb dp mj li nc nd ml lm ne nf mn lq ng nh mp ni bi translated">修剪后</h2><p id="7c58" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated">后剪枝技术允许完整地增长决策树，然后删除它的一部分。</p><h2 id="a413" class="mx ma it bd mb my mz dn mf na nb dp mj li nc nd ml lm ne nf mn lq ng nh mp ni bi translated">技巧3: <strong class="ak">成本复杂度剪枝</strong></h2><p id="d6f8" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated"><em class="lv">成本复杂度剪枝(ccp) </em>是后剪枝技术的一种。它提供了另一个控制树大小的选项。这可以通过为<strong class="lb iu"> alpha </strong>找到正确的值来实现，该值在Scikit-learn决策树类中通常被称为<strong class="lb iu"> ccp_alpha </strong>。<strong class="lb iu">CCP _阿尔法</strong>的值越大，修剪的节点数量越多。默认情况下，该值为零，表示不执行修剪。</p><p id="daed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">sci kit-learn<strong class="lb iu">decision tree classifier</strong>类提供了一个名为<strong class="lb iu">cost _ complexity _ pruning _ path</strong>的方法，该方法返回一个类似字典的对象，该对象包含alpha的有效值(通过<strong class="lb iu">CCP _阿尔法</strong>属性访问)以及在修剪过程的每一步子树的相应杂质(通过<strong class="lb iu">杂质</strong>属性访问)。我们可以通过执行以下命令来获得基础决策树模型的alpha值:</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="9c37" class="mx ma it ob b gy of og l oh oi">path = dtclf.cost_complexity_pruning_path(X_train, y_train)<br/>ccp_alphas = path.ccp_alphas<br/>ccp_alphas</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/49fe5c506fecb3e7ce1a871a48311c88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*Q9FtFKQkFJqmRVfkoxganQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">输出5(图片由作者提供)</p></figure><p id="0b35" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们可以使用这些阿尔法值来修剪我们的决策树。通过循环遍历<strong class="lb iu">CCP _阿尔法值</strong>数组，我们将每个阿尔法值传递给<strong class="lb iu">决策树分类器</strong>的<strong class="lb iu">CCP _阿尔法值</strong>超参数，然后计算每次迭代的训练和测试准确度分数，并将它们存储在两个列表中。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段5(等待加载！)</p></figure><p id="68c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们绘制训练和测试准确度分数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段6:绘制ccp阿尔法值与精确度的关系图(等待加载！)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/6dd1fdde5308961c21f8032ea9c0ca10.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*yQxgwALA6akaNy4T71n0Gg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">输出6(图片由作者提供)</p></figure><p id="2f4a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过查看该图，我们可以决定减轻决策树模型中过度拟合的最佳alpha值是<strong class="lb iu"> 0.06 </strong>。</p><p id="00ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们通过设置<strong class="lb iu">CCP _阿尔法=0.06 </strong>再次创建决策树。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段7:创建一个ccp阿尔法=0.06的决策树(等到加载！)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/0bd2a01390cea016bdd407837d435386.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*dTuIPi_2JIcJz-0VL5UhHw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">输出7(图片由作者提供)</p></figure><p id="b58d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与基本模型相比，测试精度从0.71提高到0.77。除此之外，假阳性和假阴性都减少了！现在，我们的模型没有过度拟合，测试数据的性能也有所提高。</p><p id="70d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以想象修剪后的决策树。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段8:可视化决策树(等到加载！)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/edcf4d8d08cf91df254fe180910558e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*QnLF0sajg_oYJxkWeFiHzA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">输出8(图片由作者提供)</p></figure><p id="89c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们意识到后期修剪是在幕后进行的，因此如果我们选择第二个大的alpha值(或更接近的值)，树会以自下而上的方式进行修剪，直到最大深度为2。<strong class="lb iu">CCP _阿尔法</strong>值越大，<strong class="lb iu">修剪的节点数越多！</strong></p><p id="b68e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注:</strong>如果你想了解更多可视化决策树的不同方法，请阅读我写的这篇文章<a class="ae ky" rel="noopener" target="_blank" href="/4-ways-to-visualize-individual-decision-trees-in-a-random-forest-7a9beda1d1b7">。</a></p><h1 id="a107" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">创建合奏</h1><p id="63ec" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated">另一种减少决策树过度拟合的技术是从训练数据和特征的子集创建多个决策树。这些树的集合称为系综(群)。</p><h2 id="e0fb" class="mx ma it bd mb my mz dn mf na nb dp mj li nc nd ml lm ne nf mn lq ng nh mp ni bi translated">技术4:创建随机森林</h2><p id="72b0" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated"><em class="lv">随机森林</em>顾名思义，就是决策树的集合体。一个随机森林中的<em class="lv">不相关</em>决策树的大集合可以产生比任何单个决策树更准确和稳定的结果。在正常的决策树中，当算法想要分割一个节点时，它从所有特征中搜索最佳特征<em class="lv">。相反，随机森林中的每棵树都从<em class="lv">特征的随机子集</em>中搜索最佳特征。当在随机森林中种植树木时，这会产生额外的随机性。</em></p><p id="846c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注:</strong>如果你想了解更多关于随机森林的知识，请看<a class="ae ky" rel="noopener" target="_blank" href="/random-forests-an-ensemble-of-decision-trees-37a003084c6c">我写的这个帖子</a>。</p><p id="75a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面的代码创建了一个包含100棵决策树的随机森林。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段9:创建一个有100棵树的随机森林(等待加载！)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/4905620aba92a2dff59370fb23c09281.png" data-original-src="https://miro.medium.com/v2/resize:fit:978/format:webp/1*RbqMPX51UvsotJDNsx-o0w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">输出9(图片由作者提供)</p></figure><p id="cbdb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">即使不调整任何超参数，随机森林模型也比基本模型表现得好！但是，这个模型仍然是过度拟合的。建议应用一些超参数调整技术来进一步减轻过拟合。</p><h1 id="cf37" class="lz ma it bd mb mc md me mf mg mh mi mj jz mk ka ml kc mm kd mn kf mo kg mp mq bi translated">摘要</h1><p id="4c38" class="pw-post-body-paragraph kz la it lb b lc mr ju le lf ms jx lh li mt lk ll lm mu lo lp lq mv ls lt lu im bi translated">我们今天讨论的所有技术只能<em class="lv">减轻</em>决策树中的过度拟合，但是它们不能完全<em class="lv">防止</em>过度拟合的问题。决策树是其他基于树的学习器的基础学习器，例如Random Forest，XGBoost。因此，我们今天讨论的技术几乎也可以应用于那些基于树的学习者。</p><p id="7768" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策树中的过度拟合很容易发生。正因为如此，决策树很少在建模任务中单独使用。但是，如果您应用适当的超参数调整技术或使用多棵树进行集成，您可以减轻决策树中的过度拟合，甚至在模型构建任务中单独使用它们。</p><p id="0252" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在这里使用的模型评估指标是<strong class="lb iu">“准确性”</strong>指标，当目标变量具有<strong class="lb iu"> <em class="lv">类别不平衡</em> </strong>时，这不是最佳选择。我们可以检查我们的<a class="ae ky" href="https://drive.google.com/file/d/1zQ3GzQrhtR4_cwlSbs7pAA34ekGkwz_c/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">“心脏病”数据集</a>的目标变量是否有类别不平衡:</p><pre class="kj kk kl km gt oa ob oc od aw oe bi"><span id="d10a" class="mx ma it ob b gy of og l oh oi">import seaborn as sns<br/>sns.catplot("target", data=df, kind="count",<br/>            aspect=1.0, height=4)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/350236f13d6e8ed4477e6855d4bf6caf.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/1*TbZWOk_qeSAlWV_tv2P3QA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="2f7e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们的目标变量中没有明显的阶级不平衡。</p><p id="f6dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注:</strong>sci kit-learn中使用的模型评估指标和评分方法的完整列表可在此处找到<a class="ae ky" href="https://scikit-learn.org/stable/modules/model_evaluation.html" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="05be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读！</p><p id="1251" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在https://rukshanpramoditha.medium.com<a class="ae ky" href="https://rukshanpramoditha.medium.com/" rel="noopener">阅读我的其他文章</a></p><p id="804f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除封面图片外的所有图片、代码示例、图表及所有内容均归作者版权所有。特别感谢Unsplash <strong class="lb iu"> </strong>上的<strong class="lb iu">玛丽莎&amp;埃里克</strong>是封面图片的所有者。非常感谢你，<strong class="lb iu">玛丽莎&amp;埃里克</strong>，为我提供了一张出色的封面图片！</p><p id="b799" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi">2021–05–16</p></div></div>    
</body>
</html>