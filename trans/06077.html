<html>
<head>
<title>NLP 101 ⅓ — Feature Engineering and Word Embeddings</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP 101 ⅓ —特征工程和单词嵌入</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nlp-101-%E2%85%93-feature-engineering-and-word-embeddings-f10dffd67bb0?source=collection_archive---------29-----------------------#2021-05-31">https://towardsdatascience.com/nlp-101-%E2%85%93-feature-engineering-and-word-embeddings-f10dffd67bb0?source=collection_archive---------29-----------------------#2021-05-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="30a7" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">自然语言处理中主要概念的简明介绍</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/439b3d62908eb0640ac9dbabe3739cd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ybPCgLdIJGICby0s"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae kv" href="https://unsplash.com/@darya_tryfanava?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Darya Tryfanava </a>拍摄的照片</p></figure><p id="fd59" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">声明:本文是我为<a class="ae kv" href="https://shecancode.io/" rel="noopener ugc nofollow" target="_blank"> shecancode </a>撰写的三部分系列文章的第一部分。与我的其他文章相比，它更深入于理论，并且不包含任何代码(关于这些，请查看我的其他文章)。</p><p id="2814" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇博文是对自然语言处理(NLP)的一个温和的介绍。读完这篇文章后，你会知道一些从文本中提取特征的基本技术，这些特征可以用作机器学习模型的输入，以及什么是单词嵌入。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="f510" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">什么是NLP？</h1><p id="c506" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">NLP是人工智能的一个分支，处理分析、理解和生成人类(自然)语言。您每天都在使用NLP应用程序，例如，当使用Google Translate翻译一段文本时，您的电子邮件服务使用NLP来检测垃圾邮件，自动完成和检查您手机上的语法，等等。</p><h1 id="a4a6" class="lz ma iq bd mb mc mw me mf mg mx mi mj jw my jx ml jz mz ka mn kc na kd mp mq bi translated">入门指南</h1><p id="0509" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">给定一段文本，我们需要将其转换成计算机可读的格式——向量。在本文中，我们将介绍不同的方法，以及不同的预处理技术。</p><p id="fc6f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">标记化</strong></p><p id="45a7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">记号化是将文本分解成称为记号的单个单词的过程。虽然这听起来很简单，但这个问题并不简单。在像英语这样的语言中，空格是单词分隔符的一个很好的近似。然而，如果我们只按空间划分，就会遇到问题。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nb"><img src="../Images/b2198284c84a3825ea3498370b3261c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5CcniOdvNHbvDbLjb3FJwg.jpeg"/></div></div></figure><p id="2840" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们只按空格分开，每个句子的最后一个单词会包含一个标点符号，这是我们不想要的。但是挑出标点符号作为单独的符号并不总是正确的，例如在单词“Mr”中。</p><p id="45b1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">用英语来说，这个问题很简单，只要想出一套可以硬编码以达到良好效果的规则就行了。但是像日语这样不使用空格的语言呢？或者文本是非常特定于领域的，如下例所示？在这种情况下，我们需要使用机器学习来训练标记器。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/13b2d031e58eacbc8058fd8ff6fd1927.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3o7zV1xWUn4qopEAXCoMkQ.jpeg"/></div></div></figure><p id="df0e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">除非您处理一个非常特殊的领域，它需要一个特定于领域的标记器，否则您没有必要编写一个标记器。幸运的是，所有的NLP库都有一套文本处理函数，包括一个用于标记化的函数。Python的一个这样的库是自然语言工具包(NLTK ),它包含nltk.word_tokenize函数。</p><p id="ec6a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">预处理</strong></p><p id="9062" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据任务的不同，您可能希望对文本进行预处理，而不是使用整句。例如，垃圾邮件检测器依赖于数据中存在的某些特征。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/6bb05bbc121e9fb53f06ed7f859a0226.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZHff_F4E9NVQEvbtlmPUjQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">垃圾邮件的特征</p></figure><p id="c4fb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">预处理文本的常用方法包括词条匹配和词干提取、停用词移除和规范化。查看这本<a class="ae kv" href="https://github.com/lisanka93/UCL_F2F_NLP101/blob/main/WEEK_1/Week_1%20Notebook%20-%20Preprocessing.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>中的一些例子，以及<a class="ae kv" rel="noopener" target="_blank" href="/getting-started-with-text-analysis-in-python-ca13590eb4f7">我的这篇</a>前一篇文章。</p><p id="e123" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">用一键向量表示单词</strong></p><p id="de56" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">好了，现在我们已经对文本进行了标记和预处理，是时候把它转换成计算机可读的向量了。这被称为特征提取。词袋(BOW)模型是一种流行且简单的特征提取技术。BOW背后的直觉是，如果两个句子包含一组相似的单词，就说它们是相似的。BOW在语料库(词汇表)中构建m个唯一单词的字典，并将每个单词转换成大小为m的稀疏向量，其中除了该单词在词汇表中的索引之外，所有值都被设置为0。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/151e05f58d450ca70feb1971df795c09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gpb9nUoRk9QqDza0Hbf2Eg.jpeg"/></div></div></figure><p id="8e7a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一个句子可以用向量相加来表示。有不同的方法可以做到这一点:max-pooling只计算一个单词是否出现，而不计算出现的次数。Sum pooling计算每个单词的出现次数(使用max-pooling的“我喜欢，喜欢，NLP”的独热向量与“我喜欢NLP”的相同，而sum pooling将在vector [0，0，2，1，1]中产生)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/41c60353b34c18e47d37b779ea0b86f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nhL4LIl70X0WKLixQge-Ig.jpeg"/></div></div></figure><p id="c0a6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与标记化一样，没有必要从头开始编写代码——所有流行的机器学习库中都有文本矢量化功能，例如scikit-learn的CountVectorizer。</p><h1 id="44bc" class="lz ma iq bd mb mc mw me mf mg mx mi mj jw my jx ml jz mz ka mn kc na kd mp mq bi translated">单词嵌入</h1><p id="86f3" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">现在你知道如何将文本转换成向量了。这些向量可用于训练分类任务的模型，例如，垃圾邮件检测或情感分析。线性分类模型将在下一篇博文中讨论。BOW对于某些任务非常有效，并且非常容易理解和实现。然而，BOW有几个缺点。首先，它产生非常大但稀疏的特征向量。其次，它假设所有的单词都是相互独立的。</p><p id="9c85" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们用一个例子来说明这个问题。假设我们想按主题(体育、政治等)对新闻文章进行分类。).</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/bb369a1db166611dd0d976cb8b21f8d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AXSeQl0RAeykTJJ9A4dq8w.jpeg"/></div></div></figure><p id="b06f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们很容易看出，测试句子应该被标注为A(运动)，因为手球是一项运动。但是电脑不知道。计算机只能看到词汇中包含或不包含的符号。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/12dfc0c5948b8edce977312ab725e17c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xq2peLej8hVvBlwveLilcA.jpeg"/></div></div></figure><p id="88bf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一台计算机不能像我们一样根据我们对该领域的一些先验知识进行归纳。当使用BOW表示句子时，这是一个基本问题——一键编码的向量不能捕捉单词的相似性。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/dec419124b365a4a0f44a7be91dd17ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DEndy93x2v-hYBQ7S-1O8w.jpeg"/></div></div></figure><p id="168b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">解决方案是分布式表示，它引入了一个单词对其他单词的某种依赖。这样，彼此更相似的单词将在嵌入空间中被放置得彼此更近。现在我们可以通过测量单词之间的距离来计算单词的相似程度。</p><p id="8e98" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇文章不会描述创建单词嵌入的算法是如何工作的，但是如果你有兴趣了解更多，可以看看这篇<a class="ae kv" href="https://jalammar.github.io/illustrated-word2vec/" rel="noopener ugc nofollow" target="_blank">博客文章。相反，我想给出一个关于单词嵌入的高级例子。</a></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/4769208d18ca146cd08404b33773ccdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vbaDFuc734QGcM8maHiiFQ.jpeg"/></div></div></figure><p id="38d9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">想象一个包含5个单词的小词汇表:国王、王后、男人、女人和公主。queen的一键向量看起来像左边的那个。</p><p id="7faa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是我们知道，词是丰富的实体，有许多层的内涵和意义。让我们为这5个单词手工制作一些语义特征。我们将每个单词表示为5个语义特征的0到1之间的值:皇室、男性、女性、年龄和可食用性。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/092337673c2169f7a13d728a5020893a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*97l7_98vyGnLTyKiVEonaA.jpeg"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/4ebb4d7c53f5c02d988cdc8507535847.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*X2L6MjSYXq8wtANfmO100g.jpeg"/></div></figure><p id="b3e3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">鉴于“国王”这个词，它对“皇室”这一特征有很高的价值(因为国王是王室的男性成员)，但对女性气质的价值很低(因为他是男性)，对可食用性的价值甚至更低(因为我们通常不吃国王)。在上面虚构的玩具数据集中，有5个语义特征，我们可以一次绘制其中的三个作为3D散点图，每个特征是一个轴/维度。</p><p id="ddf3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您不必自己创建单词嵌入。预先训练的单词嵌入可以<a class="ae kv" href="https://nlp.stanford.edu/projects/glove/" rel="noopener ugc nofollow" target="_blank">下载</a>并在你的模型中使用。</p><p id="a601" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望你觉得这篇博文很有用，并且已经了解了一些基本的NLP任务，一键编码是如何工作的，什么是单词嵌入。</p><p id="26a4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">阅读本系列的第二部分，点击<a class="ae kv" rel="noopener" target="_blank" href="/nlp-101-⅔-linear-models-for-text-classification-8ced8199c2a8">这里</a>。</p></div></div>    
</body>
</html>