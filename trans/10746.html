<html>
<head>
<title>A PySpark Example for Dealing with Larger than Memory Datasets</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">处理大于内存的数据集的PySpark示例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-pyspark-example-for-dealing-with-larger-than-memory-datasets-70dbc82b0e98?source=collection_archive---------1-----------------------#2021-10-17">https://towardsdatascience.com/a-pyspark-example-for-dealing-with-larger-than-memory-datasets-70dbc82b0e98?source=collection_archive---------1-----------------------#2021-10-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3561" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><span class="l ki kj kk bm kl km kn ko kp di">一个关于如何使用Spark对大于内存的数据集执行探索性数据分析的</span>分步教程。</h2></div><p id="40ef" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">使用Jupyter笔记本和Pandas数据框分析大于可用RAM内存的数据集是一个具有挑战性的问题。这个问题已经解决了(例如这里的<a class="ae lm" href="https://www.codementor.io/@guidotournois/4-strategies-to-deal-with-large-datasets-using-pandas-qdw3an95khttps://www.codementor.io/@guidotournois/4-strategies-to-deal-with-large-datasets-using-pandas-qdw3an95k" rel="noopener ugc nofollow" target="_blank">这里的</a>或者这里的<a class="ae lm" rel="noopener" target="_blank" href="/how-to-handle-large-datasets-in-python-with-pandas-and-dask-34f43a897d55">这里的</a>)，但是我在这里的目标有点不同。我将介绍一种对大型数据集进行探索性分析的方法，目的是识别和过滤掉不必要的数据。希望最终熊猫可以处理过滤后的数据集进行剩余的计算。</p><p id="d028" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">这篇文章的想法来自于我最近的一个项目，涉及对开放食品事实数据库的分析。它包含在世界各地销售的产品的营养信息，在撰写本文时，他们提供的csv 出口量为4.2 GB。这比我在Ubuntu虚拟机上的3 GB内存还大。然而，通过使用PySpark，我能够运行一些分析，并且只从我的项目中选择感兴趣的信息。</p></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><p id="787f" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">为了在Ubuntu上设置我的环境，我采取了以下步骤:</p><ol class=""><li id="8f03" class="lu lv it ks b kt ku kw kx kz lw ld lx lh ly ll lz ma mb mc bi translated">安装Anaconda</li><li id="c22a" class="lu lv it ks b kt md kw me kz mf ld mg lh mh ll lz ma mb mc bi translated">安装Java openJDK 11: sudo apt-get安装openjdk-11-jdk。Java版本很重要，因为Spark只能在Java 8或11上运行</li><li id="58e6" class="lu lv it ks b kt md kw me kz mf ld mg lh mh ll lz ma mb mc bi translated">安装Apache Spark(3 . 1 . 2版本为Hadoop 2.7 <a class="ae lm" href="https://www.apache.org/dyn/closer.lua/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz" rel="noopener ugc nofollow" target="_blank">此处</a>)并配置Spark环境(将SPARK_HOME变量添加到PATH)。如果一切顺利，你应该可以在你的终端上启动spark-shell了</li><li id="a068" class="lu lv it ks b kt md kw me kz mf ld mg lh mh ll lz ma mb mc bi translated">安装pyspark:conda install-c conda-forge pyspark</li></ol></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><p id="ef24" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">如果你有兴趣阅读Spark的核心概念，这是一个好的开始。如果没有，您可以打开Jupyter笔记本，导入pyspark.sql模块并创建一个本地SparkSession:</p><figure class="mi mj mk ml gt mm"><div class="bz fp l di"><div class="mn mo l"/></div></figure><p id="204c" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我使用sc.read从我的SparkSession中的大型csv文件读取数据。尝试在只有3 GB RAM的虚拟机上加载4.2 GB文件不会出现任何错误，因为Spark实际上不会尝试读取数据，除非需要某种类型的计算。</p><figure class="mi mj mk ml gt mm"><div class="bz fp l di"><div class="mn mo l"/></div></figure><p id="a104" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">结果是pyspark.sql.dataframe变量。重要的是要记住，此时数据并没有真正载入RAM存储器。只有在对pyspark变量调用一个操作时，才会加载数据，这个操作需要返回一个计算值。例如，如果我要求计算数据集中产品的数量，Spark很聪明，不会为了计算这个值而尝试加载全部4.2 GB的数据(将近200万个产品)。</p><p id="7adc" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我使用pyspark的printSchema函数来获取一些关于数据结构的信息:列及其相关类型:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/0570592ff0095e802401c5788f2bf94c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*jR5PYBr6ksy63ehRXnV1Vw.png"/></div></figure><p id="46d3" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">为了开始探索性分析，我计算了每个国家的产品数量，以了解数据库的组成:</p><figure class="mi mj mk ml gt mm"><div class="bz fp l di"><div class="mn mo l"/></div></figure><p id="05f1" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">BDD_countries也是pyspark数据框架，其结构如下:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/082b6a5deb87b323ea955e5a9a95f870.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*z5A5UCiPINrU7pJcG9MQuQ.png"/></div></figure><p id="1e4a" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">我可以过滤这个新的数据框，只保留数据库中记录了至少5000种产品的国家，并绘制结果:</p><figure class="mi mj mk ml gt mm"><div class="bz fp l di"><div class="mn mo l"/></div></figure><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/976eec8ab395f46d96bdd57f077da316.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*0LK3_hCsxzwSY5YlXwwYHg.png"/></div></figure><p id="a26c" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">例如，从这里我可以过滤掉所有在法国买不到的产品，然后在一个更小、更容易处理的数据集上执行其余的分析。</p></div><div class="ab cl ln lo hx lp" role="separator"><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls lt"/><span class="lq bw bk lr ls"/></div><div class="im in io ip iq"><p id="4839" class="pw-post-body-paragraph kq kr it ks b kt ku ju kv kw kx jx ky kz la lb lc ld le lf lg lh li lj lk ll im bi translated">本文介绍了一种在Python中处理大于内存的数据集的方法。通过使用Spark会话读取数据，可以执行基本的探索性分析计算，而无需实际尝试将完整的数据集加载到内存中。当我们希望能够获得数据的第一印象并寻找识别和过滤掉不必要信息的方法时，这种类型的方法会很有用。</p></div></div>    
</body>
</html>