<html>
<head>
<title>A Minimal Working Example for Deep Q-Learning in TensorFlow 2.0</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TensorFlow 2.0中深度Q学习的最小工作示例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-minimal-working-example-for-deep-q-learning-in-tensorflow-2-0-e0ca8a944d5e?source=collection_archive---------10-----------------------#2021-07-27">https://towardsdatascience.com/a-minimal-working-example-for-deep-q-learning-in-tensorflow-2-0-e0ca8a944d5e?source=collection_archive---------10-----------------------#2021-07-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="12ad" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一个多臂土匪的例子来训练一个Q网络。使用TensorFlow，更新过程只需要几行代码</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4fdddcd7775b83e44b7150c4afcc083f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*O5iYZwPfYpRDrw7d"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">深度，就像深度Q-learning一样。克里斯·米凯尔·克里斯特在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="8064" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">深度Q学习是任何强化学习(RL)实践者的武器库中的一个主要部分。它巧妙地规避了传统Q-learning的一些缺点，并利用神经网络的能力进行复杂值函数的逼近。</p><p id="42fe" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">本文展示了如何在TensorFlow 2.0中实现和训练深度Q网络，并以多臂土匪问题(一个终止的一次性游戏)为例进行了说明。还提供了对时间差异学习的一些扩展。不过，我认为极小工作示例中的<em class="mc">【极小】</em>相当字面上的意思，所以重点是有史以来第一次实现深度Q学习。</p><h2 id="d2b0" class="md me it bd mf mg mh dn mi mj mk dp ml lp mm mn mo lt mp mq mr lx ms mt mu mv bi translated">一些背景</h2><p id="efd9" class="pw-post-body-paragraph lg lh it li b lj mw ju ll lm mx jx lo lp my lr ls lt mz lv lw lx na lz ma mb im bi translated">在深入学习之前，我假设你已经熟悉了<a class="ae ky" rel="noopener" target="_blank" href="/walking-off-the-cliff-with-off-policy-reinforcement-learning-7fdbcdfe31ff">普通Q学习</a>和人工神经网络。没有这些基础知识，尝试深度Q学习可能会是一次令人沮丧的经历。以下更新机制对您来说应该没有秘密:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/fa590d353c63e4edbb2de1dbf861cc25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Ef7pkJT7FWC8cQ9MBT6vg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Q-learning的更新函数[1]</p></figure><p id="a4f4" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">传统的Q-learning在一个查找表中为每个<strong class="li iu">状态-动作对</strong>显式存储一个Q值——本质上是对累积折扣奖励的估计。当在特定状态下采取行动时，观察到的奖励会提高价值评估。查找表的大小为<em class="mc"> |S|×|A| </em>，其中<em class="mc"> S </em>为状态空间，<em class="mc"> A </em>为动作空间。Q-learning对于玩具大小的问题往往很有效，但是对于更大的问题就不行了。通常，不可能在任何地方观察到所有的状态-行为对。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/644763175313db7461962e530dec238f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kwp6MLw7s4Bbw0m2GX-q5g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在16格网格上移动的Q学习表示例。在这种情况下，有16*4=64个状态-动作对，应该学习它们的值Q(s，a)。[图片由作者提供]</p></figure><p id="3870" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">与普通Q-学习相比，<em class="mc">深度</em>Q-学习将状态作为输入，通过多个神经网络层，并输出每个动作的Q值。深度q网络可以被视为一个函数<em class="mc"> f:s→[Q(s,a)]_∀ a ∈ A </em>。通过对所有状态采用单一表示，深度Q学习能够处理大的状态空间。不过，它预先假定了合理数量的动作，因为每个动作都由输出层中的一个节点表示(大小<em class="mc"> |A| </em>)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/9745858325280b2c5406fa9b08170fc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rZgjSJX2I0jdJhhPxz1f-g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">深度Q网络的例子。在本例中，输入是网格(16个图块)的一键编码，而输出表示四个动作中每个动作的Q值。[图片由作者提供]</p></figure><p id="92f3" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在通过网络并获得所有动作的Q值之后，我们照常继续。为了平衡勘探和开发，我们利用了一个基本的<em class="mc">ϵ</em>-贪婪政策。通过概率<em class="mc"> 1-ϵ </em>我们选择最佳动作(输出层上的<code class="fe ne nf ng nh b">argmax</code>操作)，通过概率<em class="mc"> ϵ </em>我们对随机动作进行采样。</p><h2 id="9ae8" class="md me it bd mf mg mh dn mi mj mk dp ml lp mm mn mo lt mp mq mr lx ms mt mu mv bi translated">TensorFlow 2.0实施</h2><p id="4cc0" class="pw-post-body-paragraph lg lh it li b lj mw ju ll lm mx jx lo lp my lr ls lt mz lv lw lx na lz ma mb im bi translated">在TensorFlow中定义Q网络并不难。输入维度等于向量状态的长度，输出维度等于动作的数量(如果可行动作的集合是状态相关的，则可以应用掩码)。Q网络是一种相当简单的神经网络:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni nj l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在TensorFlow 2.0中创建3层Q网络的示例代码。输入是状态向量，输出是每个动作的Q值。</p></figure><p id="8aa9" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">权重更新在很大程度上也是为您处理的，但是您必须向优化器提供一个<strong class="li iu">损失值</strong>。损失代表<em class="mc">观察值</em>和<em class="mc">期望值</em>之间的误差；需要一个可微分的损失函数来正确地执行更新。对于深度Q学习，损失函数通常是简单的均方误差。这实际上是TensorFlow中的内置损失函数(<code class="fe ne nf ng nh b">loss=‘mse’</code>)，但我们将在这里使用<code class="fe ne nf ng nh b">GradientTape</code>功能，跟踪您的所有操作来计算和应用梯度[2]。它提供了更多的灵活性，并且贴近底层数学，这在转向更复杂的RL应用时通常是有益的。</p><p id="584b" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">均方损失函数(观察与前面提到的更新机制的相似性)表示如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/dd24987983661055bb596c68b1a75691.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P3QaRca-aOmLDS3mE3Cl4Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">深度Q学习的均方误差损失函数</p></figure><p id="0510" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">深度Q学习方法的一般TensorFlow实现如下(GradientTape在水下施展魔法):</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ni nj l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">深度Q学习训练程序概述。</p></figure><h2 id="8623" class="md me it bd mf mg mh dn mi mj mk dp ml lp mm mn mo lt mp mq mr lx ms mt mu mv bi translated">多臂土匪</h2><p id="1549" class="pw-post-body-paragraph lg lh it li b lj mw ju ll lm mx jx lo lp my lr ls lt mz lv lw lx na lz ma mb im bi translated">多臂土匪问题是RL[3]中的经典。它定义了若干吃角子老虎机:每台机器<em class="mc"> i </em>都有一个平均收益<em class="mc"> μ_i </em>和一个标准差<em class="mc"> σ_i. </em>每个决策时刻，你玩一台机器，观察由此产生的奖励。当玩得足够频繁时，你可以估计每台机器的平均回报。不言而喻，最佳策略是玩平均收益最高的老虎机。</p><p id="a1a1" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">让我们把Q-learning网络的例子付诸行动(完整的Github代码<a class="ae ky" href="https://github.com/woutervanheeswijk/example_deep_q_learning" rel="noopener ugc nofollow" target="_blank">这里</a>)。我们定义了一个具有三个完全连接的10节点隐藏层的简单神经网络。作为输入，我们使用值为1(表示固定状态)的张量作为输入，四个节点(表示每个机器的Q值)作为输出。网络权重被初始化，使得所有Q值最初都是0。对于权重更新，我们使用学习率为0.001的Adam优化器。</p><p id="885d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">下图显示了一些说明性的结果(10，000次迭代后)。勘探和开发之间的权衡是显而易见的，尤其是在根本不勘探的时候。注意，结果并不过分准确；普通Q-learning实际上对这类问题表现得更好。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/07d55799734c3e527c2c092ef046331a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n6frSfziaiFWfxt6nEKhSQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">多臂土匪问题的q值和真值。ϵ=0.0(左上)、ϵ=0.01(右上)、ϵ=0.1(左下)和ϵ=1.0(右下)经过10，000次迭代后的结果。探索越少，感知的最佳行为越接近。[图片由作者提供]</p></figure><h2 id="3a6a" class="md me it bd mf mg mh dn mi mj mk dp ml lp mm mn mo lt mp mq mr lx ms mt mu mv bi translated">时间差异学习</h2><p id="b5e8" class="pw-post-body-paragraph lg lh it li b lj mw ju ll lm mx jx lo lp my lr ls lt mz lv lw lx na lz ma mb im bi translated">多臂强盗绝对是一个最小的工作例子，但是只处理了我们没有看到直接回报以外的最终情况。让我们看看如何处理非终结的情况。在这种情况下，我们部署时间差异学习—我们使用<em class="mc"> Q(s '，a') </em>来更新<em class="mc"> Q(s，a) </em>。</p><p id="6e36" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">获得对应于下一个状态<em class="mc">s’</em>的Q值本身并不难。你只需将<em class="mc">s’</em>插入到Q网络中，并输出一组Q值。总是选择最大值，因为这是Q学习而不是SARSA，并使用它来计算损失函数:</p><pre class="kj kk kl km gt nm nh nn no aw np bi"><span id="c582" class="md me it nh b gy nq nr l ns nt">next_q_values = tf.stop_gradient(q_network(next_state))    <br/>next_q_value = np.max(next_q_values[0])</span></pre><p id="3805" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">注意，Q-网络是在一个<code class="fe ne nf ng nh b"><strong class="li iu">stop_gradient</strong></code> <strong class="li iu"> </strong>运算符内调用的【4】。提醒一下，<code class="fe ne nf ng nh b">GradientTape</code>跟踪所有的操作，并且同样会使用<code class="fe ne nf ng nh b">next_state</code>输入执行(无意义的)更新。使用<code class="fe ne nf ng nh b">stop_gradient</code>操作符，我们可以安全地利用对应于下一个状态<em class="mc">s’</em>的Q值，而不用担心错误的更新！</p><h2 id="73b3" class="md me it bd mf mg mh dn mi mj mk dp ml lp mm mn mo lt mp mq mr lx ms mt mu mv bi translated">一些实施说明</h2><p id="2936" class="pw-post-body-paragraph lg lh it li b lj mw ju ll lm mx jx lo lp my lr ls lt mz lv lw lx na lz ma mb im bi translated">虽然上面概述的方法原则上可以直接应用于任何RL问题，但您会经常发现性能相当令人失望。即使对于基本问题，如果你的普通Q学习实现胜过你的花哨的深度Q网络，也不要感到惊讶。一般来说，神经网络需要许多观察来学习一些东西，并且通过为所有可能遇到的状态训练单个网络来固有地丢失一些细节水平。</p><p id="636e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">除了良好的神经网络实践(例如，标准化、一键编码、正确的权重初始化)，以下调整可能会大大提高算法的质量[5]:</p><ul class=""><li id="70cd" class="nu nv it li b lj lk lm ln lp nw lt nx lx ny mb nz oa ob oc bi translated"><strong class="li iu">小批量</strong>:不是在每次观察后更新网络，而是使用批量观察来更新Q网络。稳定性通常通过多次观察的训练来提高。每次观察的损失被简单地平均。<code class="fe ne nf ng nh b">tf.one_hot</code>掩码可用于多个动作的更新。</li><li id="345d" class="nu nv it li b lj od lm oe lp of lt og lx oh mb nz oa ob oc bi translated"><strong class="li iu">经验重放</strong>:建立一个先前观察值的缓冲区(存储为<em class="mc"> s，a，r，s’</em>元组)，从缓冲区中抽取一个(或多个，当使用迷你批处理时)，并插入Q网络。这种方法的主要好处是消除了数据中的相关性。</li><li id="805e" class="nu nv it li b lj od lm oe lp of lt og lx oh mb nz oa ob oc bi translated"><strong class="li iu">目标网络</strong>:创建一个只定期更新(比如每100次更新)的神经网络副本。目标网络用于计算<em class="mc">Q(s’，a’)</em>，而原始网络用于确定<em class="mc"> Q(s，a) </em>。此过程通常会产生更稳定的更新。</li></ul><h2 id="22bf" class="md me it bd mf mg mh dn mi mj mk dp ml lp mm mn mo lt mp mq mr lx ms mt mu mv bi translated"><strong class="ak">外卖</strong></h2><ul class=""><li id="1e92" class="nu nv it li b lj mw lm mx lp oi lt oj lx ok mb nz oa ob oc bi translated">一个<strong class="li iu">深度Q网络</strong>是一个简单的神经网络，以状态向量作为输入，输出对应于每个动作的Q值。通过对所有状态使用单一表示，它可以处理比普通Q-learning(使用查找表)大得多的状态空间。</li><li id="f2c3" class="nu nv it li b lj od lm oe lp of lt og lx oh mb nz oa ob oc bi translated">TensorFlow的<code class="fe ne nf ng nh b">GradientTape</code>可以用来更新Q网络。对应的损失函数是接近原始Q学习更新机制的<strong class="li iu">均方误差</strong>。</li><li id="64ac" class="nu nv it li b lj od lm oe lp of lt og lx oh mb nz oa ob oc bi translated">在<strong class="li iu">时间差学习</strong>中，基于<em class="mc">Q(s’，a’)</em>更新<em class="mc"> Q(s，a) </em>的估计值。<code class="fe ne nf ng nh b">stop_gradient</code>算子确保对应于<em class="mc">Q(s’，a’)</em>的梯度被忽略。</li><li id="aa0d" class="nu nv it li b lj od lm oe lp of lt og lx oh mb nz oa ob oc bi translated">深度Q学习伴随着一些<strong class="li iu">实现挑战</strong>。如果香草Q-learning实际上表现更好，不要惊慌，特别是对于玩具大小的问题。</li></ul></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="acfc" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><em class="mc">使用多臂盗匪的最小工作示例的GitHub代码可以在</em> <a class="ae ky" href="https://github.com/woutervanheeswijk/example_deep_q_learning" rel="noopener ugc nofollow" target="_blank"> <em class="mc">这里</em> </a> <em class="mc">找到。</em></p><p id="e49b" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><em class="mc">想要稳定你的深度Q学习算法？下面这篇文章可能会让你感兴趣:</em></p><div class="ol om gp gr on oo"><a rel="noopener follow" target="_blank" href="/how-to-model-experience-replay-batch-learning-and-target-networks-c1350db93172"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">如何对经验重放、批量学习和目标网络进行建模</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">使用TensorFlow 2.0，快速学习稳定和成功的深度Q学习的三个基本技巧</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="oy l oz pa pb ox pc ks oo"/></div></div></a></div><p id="1536" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><em class="mc">转而寻求实施政策梯度方法？请检查我的文章与连续和离散情况下的最小工作示例:</em></p><div class="ol om gp gr on oo"><a rel="noopener follow" target="_blank" href="/a-minimal-working-example-for-continuous-policy-gradients-in-tensorflow-2-0-d3413ec38c6b"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">TensorFlow 2.0中连续策略梯度的最小工作示例</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">一个简单的训练高斯演员网络的例子。定义自定义损失函数并应用梯度胶带…</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="pd l oz pa pb ox pc ks oo"/></div></div></a></div><div class="ol om gp gr on oo"><a rel="noopener follow" target="_blank" href="/a-minimal-working-example-for-discrete-policy-gradients-in-tensorflow-2-0-d6a0d6b1a6d7"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd iu gy z fp ot fr fs ou fu fw is bi translated">TensorFlow 2.0中离散策略梯度的最小工作示例</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">一个训练离散演员网络的多兵种土匪例子。在梯度胶带功能的帮助下…</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="pe l oz pa pb ox pc ks oo"/></div></div></a></div><h2 id="0bec" class="md me it bd mf mg mh dn mi mj mk dp ml lp mm mn mo lt mp mq mr lx ms mt mu mv bi translated">参考</h2><p id="a6e3" class="pw-post-body-paragraph lg lh it li b lj mw ju ll lm mx jx lo lp my lr ls lt mz lv lw lx na lz ma mb im bi translated">[1]萨顿，理查德和安德鲁巴尔托。<em class="mc">强化学习:简介</em>。麻省理工学院出版社，2018。</p><p id="8c93" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">[2] Rosebrock，A. (2020)使用TensorFlow和GradientTape来训练Keras模型。【https://www.tensorflow.org/api_docs/python/tf/GradientTape T4】</p><p id="9bee" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">[3] Ryzhov，I. O .，Frazier，P. I .和Powell，W. B. (2010年)。多臂土匪问题中一期前瞻策略的鲁棒性。Procedia计算机科学，1(1):1635{1644。</p><p id="2e38" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">[4]TensorFlow (2021)。于2021年7月26日从<a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/stop_gradient" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/api_docs/python/tf/stop_gradient</a>获得</p><p id="9655" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">[5]维基百科贡献者(2021)深度Q-learning。于2021年7月26日从<a class="ae ky" href="https://en.wikipedia.org/wiki/Q-learning#Deep_Q-learning" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Q-learning#Deep_Q-learning</a>获得</p></div></div>    
</body>
</html>