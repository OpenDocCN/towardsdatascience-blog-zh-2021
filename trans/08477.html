<html>
<head>
<title>Training BERT Text Classifier on Tensor Processing Unit</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在张量处理器上训练BERT文本分类器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/training-bert-text-classifier-on-tensor-processing-unit-ce0adc8ae449?source=collection_archive---------37-----------------------#2021-08-04">https://towardsdatascience.com/training-bert-text-classifier-on-tensor-processing-unit-ce0adc8ae449?source=collection_archive---------37-----------------------#2021-08-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="43ac" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">为突尼斯阿拉比齐社交媒体情感分析培训拥抱TPU最著名的模特</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/938ce9aefd871337fdf10a948df00813.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s3Bq3CygYRJUf3EIB0nF2Q.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由作者设计|元素由<a class="ae kv" href="https://www.freepik.com/cookie-studio" rel="noopener ugc nofollow" target="_blank"> cookie-studio </a>，<a class="ae kv" href="https://www.freepik.com/rawpixel-com" rel="noopener ugc nofollow" target="_blank"> rawpixel-com </a></p></figure><h1 id="4656" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">介绍</h1><p id="391e" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">说阿拉伯语的人通常在社交媒体上用当地方言表达自己，所以突尼斯人使用<em class="mk">突尼斯阿拉伯语</em>，它由以拉丁字母形式书写的阿拉伯语组成。<em class="mk"/><em class="mk">情感分析</em>依靠文化知识和带上下文信息的词义。我们将使用阿拉伯方言和情感分析来解决这个项目中的问题。</p><p id="d52f" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">比赛在<a class="ae kv" href="https://zindi.africa/" rel="noopener ugc nofollow" target="_blank">津迪</a>举行，由<a class="ae kv" href="https://ai4d.ai/" rel="noopener ugc nofollow" target="_blank"> AI4D </a>组织。在这场比赛中，我们获得了训练和测试数据集。训练数据集由阿拉伯文文本和该文本的情感组成。我们需要建立一个模型，并改进我们的指标，以便在竞争领先者排行榜中获得第一名。</p><p id="e8b7" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">数据集是从不同的社交媒体平台收集的，这个项目的目标是预测测试数据集的标签。标签将决定我们的句子是积极的、消极的还是中性的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/c7a849a04313d513423ac2ab6b0ba41a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Az13mWrl3VFily2J.jpeg"/></div></div></figure><h1 id="eedf" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">描述</h1><p id="742b" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在这个项目中，我们将使用TPU训练多语言BERT模型。</p><h1 id="d004" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">TPU</h1><p id="d3a4" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">张量处理单元(TPU)是谷歌开发的人工智能加速器专用集成电路(ASIC)，有助于减少深度学习模型的训练时间，尤其是谷歌自己的TensorFlow包<a class="ae kv" href="https://en.wikipedia.org/wiki/Tensor_Processing_Unit" rel="noopener ugc nofollow" target="_blank">张量处理单元</a>。与PyTorch相比，在TensorFlow python库中在TPU上训练深度学习模型要简单得多。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/5fc9274da5dec51d90c17fbf7b38948a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7E9bYhv3oRvRV1-9DPwfig.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd ms">张量处理单元| </strong> <a class="ae kv" href="https://en.wikipedia.org/wiki/Tensor_Processing_Unit" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="6df2" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">云TPUs资源最大限度地减少了计算线性代数问题的时间，并且使用大多数线性代数来开发机器学习应用。以前，用庞大的数据集训练一个复杂的深度学习模型需要几周时间，但使用TPUs，训练时间减少到几个小时。<a class="ae kv" href="https://cloud.google.com/tpu/docs/tpus" rel="noopener ugc nofollow" target="_blank">谷歌云</a>。</p><h1 id="d393" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">伯特</h1><p id="ff06" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">BERT(来自变形金刚的双向编码器表示)是Google Research的研究人员在2018年提出的NLP(自然语言处理模型)。BERT基本上是transformer架构的编码器栈<a class="ae kv" href="https://www.analyticsvidhya.com/blog/2021/06/why-and-how-to-use-bert-for-nlp-text-classification/" rel="noopener ugc nofollow" target="_blank"> analyticsvidhya </a>。最初，当BERT发布它时，在许多NLP任务的准确性方面采用了其他模型，不久之后，其他研究人员开始使用BERT架构，并在不同语言中发布更新更好的模型版本。这就是我们基于BERT的多语言封装模型的由来，它在新的低资源语言上表现得相当好。我们的模型是使用最大的维基百科多语言数据语料库对前104种语言进行预训练的，要了解更多关于这个模型的信息，请阅读<a class="ae kv" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">的论文</a>。研究人员使用掩蔽语言建模(MLM)使其对多种语言有效。要了解更多关于该型号的信息，请查看型号卡<a class="ae kv" href="https://huggingface.co/bert-base-multilingual-cased" rel="noopener ugc nofollow" target="_blank">Bert-base-多语言环境</a>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mt"><img src="../Images/e05322b1484b3f16f58fdfb0a724b645.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LYi0FRPvDARZbLZhJpVtfw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">伯特多语模式| <a class="ae kv" href="https://huggingface.co/bert-base-multilingual-cased" rel="noopener ugc nofollow" target="_blank">拥抱脸</a></p></figure><h1 id="d69c" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">密码</h1><p id="3b82" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们将首先设置我们的模型在TPU上训练，然后我们将设置我们的超参数来提高性能。我们将加载和训练突尼斯阿拉伯语方言情感分析数据，然后在<a class="ae kv" href="https://ai4d.ai/" rel="noopener ugc nofollow" target="_blank"> ai4d.ai </a>提供的测试数据集上进行预测。</p><h1 id="8646" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">导入库</h1><p id="ff28" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们已经导入了所有必要的库，以便对BERT模型进行实验或预训练。</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="b001" class="mz kx iq mv b gy na nb l nc nd">import tensorflow as tf<br/>import logging<br/>from tensorflow.keras.layers import (<br/>    Dense,<br/>    Flatten,<br/>    Conv1D,<br/>    Dropout,<br/>    Input,<br/>)<br/>from tensorflow.keras.models import Sequential<br/>from tensorflow.keras.optimizers import Adam<br/>from tensorflow.keras import Model<br/>from tensorflow.keras import regularizers<br/>from transformers import BertTokenizer, TFBertModel<br/>import os<br/>import pandas as pd<br/>import numpy as np<br/>from sklearn.model_selection import train_test_split<br/>from tqdm import tqdm<br/>tqdm.pandas()<br/>import re<br/>import random</span></pre><h1 id="c103" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">为TPU做准备</h1><p id="182c" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">该代码尝试检查TPU集群，然后初始化TPU，最后创建一个TPU策略，稍后我们将使用该策略在云上训练我们的模型。如果你没有TPU，代码将根据你的机器转移到GPU或CPU。</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="feda" class="mz kx iq mv b gy na nb l nc nd">try:<br/>    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()<br/>    tf.config.experimental_connect_to_cluster(tpu)<br/>    tf.tpu.experimental.initialize_tpu_system(tpu)<br/>    strategy = tf.distribute.experimental.TPUStrategy(tpu)<br/>except ValueError:<br/>    strategy = tf.distribute.get_strategy() # for CPU and single GPU<br/>    print('Number of replicas:', strategy.num_replicas_in_sync)</span></pre><h1 id="cbce" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">设置超参数</h1><p id="c106" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">这些超参数是用于实验的，以便我们可以做出改变并获得更好的结果。目前，这些参数产生了迄今为止最好的结果。</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="4d93" class="mz kx iq mv b gy na nb l nc nd">max_length = 140<br/>batch_size = 16<br/>dev_size = 0.1<br/>num_class = 4</span></pre><h1 id="ca4a" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">正在加载BERT标记器</h1><p id="0365" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">使用变形金刚库从拥抱脸下载BERT tokenizer。</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="eec9" class="mz kx iq mv b gy na nb l nc nd">model_name = "bert-base-multilingual-cased"<br/>tokenizer = BertTokenizer.from_pretrained(model_name)</span><span id="b44e" class="mz kx iq mv b gy ne nb l nc nd">Downloading: 100% 996k/996k [00:00&lt;00:00, 5.62MB/s]</span><span id="0ad3" class="mz kx iq mv b gy ne nb l nc nd">Downloading: 100% 29.0/29.0 [00:00&lt;00:00, 950B/s]</span><span id="5115" class="mz kx iq mv b gy ne nb l nc nd">Downloading: 100% 1.96M/1.96M [00:00&lt;00:00, 11.8MB/s]</span></pre><h1 id="af3e" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">读取列车数据集</h1><blockquote class="nf ng nh"><p id="d56e" class="lo lp mk lq b lr ml jr lt lu mm ju lw ni mn lz ma nj mo md me nk mp mh mi mj ij bi translated">数据集可以在<a class="ae kv" href="https://github.com/iCompass-ai/TUNIZI" rel="noopener ugc nofollow" target="_blank">这里</a>找到，并且是在麻省理工学院的许可下。</p></blockquote><p id="20f2" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">训练数据集包含70，000个样本，而我们的测试数据集包含30，000个样本。火车数据集有一个包含唯一标识符的<strong class="lq ir"> id </strong>列、<strong class="lq ir">文本</strong>列，其中包含用阿拉伯语写的推文<strong class="lq ir">、</strong>和<strong class="lq ir">标签</strong>列，其中包含情感<strong class="lq ir"> {-1，0，1}。</strong></p><ul class=""><li id="8454" class="nl nm iq lq b lr ml lu mm lx nn mb no mf np mj nq nr ns nt bi translated"><strong class="lq ir">读取数据:</strong>使用熊猫读取CSV文件</li><li id="c4d8" class="nl nm iq lq b lr nu lu nv lx nw mb nx mf ny mj nq nr ns nt bi translated"><strong class="lq ir">删除重复的推文:</strong>这提高了模型的性能</li><li id="af2d" class="nl nm iq lq b lr nu lu nv lx nw mb nx mf ny mj nq nr ns nt bi translated"><strong class="lq ir">重新映射标签:</strong>这使得编码和解码-1情感变得容易，并最终提高了我们的性能指标。</li><li id="0d27" class="nl nm iq lq b lr nu lu nv lx nw mb nx mf ny mj nq nr ns nt bi translated"><strong class="lq ir">培训开发分割:</strong>用于培训和评估。</li></ul><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="c882" class="mz kx iq mv b gy na nb l nc nd">train_df = pd.read_csv('../input/aranizi-dailect-training-data/Arabizi-Dailect-Train.csv')<br/><br/>train_df.text = train_df.text.astype(str)<br/>train_df.drop_duplicates(subset=['text'],inplace=True)<br/>train_df.label=train_df.label.map({-1:0,1:2,0:1})<br/>train, dev = train_test_split(train_df, test_size=dev_size, random_state=42)</span></pre><h1 id="ca9d" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">加载数据并进行处理</h1><p id="5537" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们使用transformers batch encode plus来编码文本数据，并将最大长度限制为150。我们使用TensorFlow函数将我们的标签转换为分类类型，并将其加载到TensorFlow数据集，该数据集为模型使用我们的数据做准备。我认为缓存有助于TPU存储数据，但你可以删除它或添加它，你的模型将在任何情况下工作。</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="e67d" class="mz kx iq mv b gy na nb l nc nd">def bert_encode(data):<br/>    tokens = tokenizer.batch_encode_plus(<br/>        data, max_length=max_length, padding="max_length", truncation=True<br/>    )<br/>    return tf.constant(tokens["input_ids"])<br/>train_encoded = bert_encode(train.text)<br/>dev_encoded = bert_encode(dev.text)<br/>train_labels = tf.keras.utils.to_categorical(train.label.values, num_classes=num_class)<br/>dev_labels = tf.keras.utils.to_categorical(dev.label.values, num_classes=num_class)<br/>train_dataset = (<br/>    tf.data.Dataset.from_tensor_slices((train_encoded, train_labels))<br/>    .shuffle(100)<br/>    .batch(batch_size)<br/>).cache()<br/>dev_dataset = (<br/>    tf.data.Dataset.from_tensor_slices((dev_encoded, dev_labels))<br/>    .shuffle(100)<br/>    .batch(batch_size)<br/>).cache()</span></pre><h1 id="f167" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">模型</h1><p id="223c" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">这个模型受到了卡格尔的笔记本的影响，他使用了LSTM，而我只是使用了辍学的密集层。Kaggle还提供30小时的免费TPU，这使得任何人都可以更容易地免费使用云计算。</p><p id="5510" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">我们的模型非常简单，我们使用了相同的BERT架构，并添加了3个密集层，丢失率= <em class="mk"> 0.3 </em>。我使用了前两层的relu激活，对于输出层，我们使用了在TPU上表现惊人的soft plus。</p><p id="f92d" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">我也试验过双向密集LSTM层和简单LSTM，但这些模型表现不佳。</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="ec17" class="mz kx iq mv b gy na nb l nc nd">def bert_tweets_model():<br/>    bert_encoder = TFBertModel.from_pretrained(model_name, output_attentions=True)<br/>    input_word_ids = Input(<br/>        shape=(max_length,), dtype=tf.int32, name="input_ids"<br/>    )<br/>    last_hidden_states = bert_encoder(input_word_ids)[0]<br/>    clf_output = Flatten()(last_hidden_states)<br/>    net = Dense(512, activation="relu")(clf_output)<br/>    net = Dropout(0.3)(net)<br/>    net = Dense(440, activation="relu")(net)<br/>    net = Dropout(0.3)(net)<br/>    output = Dense(num_class, activation="softplus")(net)<br/>    model = Model(inputs=input_word_ids, outputs=output)<br/>    return model</span></pre><h1 id="0f19" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">编译模型</h1><p id="c56c" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">您需要添加with strategy.scope():并运行我们的模型，该模型将从Hugging Face服务器下载模型并将其加载到模型变量中。您的模型、优化器和模型编译应该在strategy.scope()下完成，以便在云TPU上进行训练。</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="c53d" class="mz kx iq mv b gy na nb l nc nd">with strategy.scope():<br/>    model = bert_tweets_model()<br/>    adam_optimizer = Adam(learning_rate=1e-5)<br/>    model.compile(<br/>        loss="categorical_crossentropy", optimizer=adam_optimizer, metrics=["accuracy"]<br/>    )<br/>    model.summary()</span><span id="f2e9" class="mz kx iq mv b gy ne nb l nc nd">Downloading: 100% 625/625 [00:00&lt;00:00, 19.9kB/s]</span><span id="1ac9" class="mz kx iq mv b gy ne nb l nc nd">Downloading: 100% 1.08G/1.08G [00:38&lt;00:00, 29.6MB/s]</span></pre><h1 id="0ac3" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">模型结构</h1><p id="0f10" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">你可以看到我们用Dropout添加的模型结构和密集层。</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="6f4a" class="mz kx iq mv b gy na nb l nc nd">Model: "model_1"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>input_ids (InputLayer)       [(None, 140)]             0         <br/>_________________________________________________________________<br/>tf_bert_model_1 (TFBertModel TFBaseModelOutputWithPool 177853440 <br/>_________________________________________________________________<br/>flatten_1 (Flatten)          (None, 107520)            0         <br/>_________________________________________________________________<br/>dense_3 (Dense)              (None, 512)               55050752  <br/>_________________________________________________________________<br/>dropout_76 (Dropout)         (None, 512)               0         <br/>_________________________________________________________________<br/>dense_4 (Dense)              (None, 440)               225720    <br/>_________________________________________________________________<br/>dropout_77 (Dropout)         (None, 440)               0         <br/>_________________________________________________________________<br/>dense_5 (Dense)              (None, 4)                 1764      <br/>=================================================================<br/>Total params: 233,131,676<br/>Trainable params: 233,131,676<br/>Non-trainable params: 0<br/>_________________________________________________________________</span></pre><h1 id="5594" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">培训模式</h1><p id="1fc2" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">终于到了我们在TPU上训练模型的时候了。该模型在TPU上训练需要17分钟，在GPU上训练需要45分钟，在CPU上训练需要2.5小时。你可以清楚地看到性能差异。通过使用<em class="mk"> model.fit </em>，我们在训练数据集上训练了我们的模型，并使用<em class="mk"> dev_dataset </em>作为验证。我们训练精度随着每个时期以及我们的验证精度而提高。</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="a053" class="mz kx iq mv b gy na nb l nc nd">history = model.fit(<br/>    train_dataset,<br/>    batch_size=batch_size,<br/>    epochs=5,<br/>    validation_data=dev_dataset,<br/>    verbose=1,<br/>)</span><span id="9f47" class="mz kx iq mv b gy ne nb l nc nd">Epoch 1/5<br/>4166/4166 [==============================] - 313s 60ms/step - loss: 0.7691 - accuracy: 0.6309 - val_loss: 0.4994 - val_accuracy: 0.7810<br/>Epoch 2/5<br/>4166/4166 [==============================] - 246s 59ms/step - loss: 0.5092 - accuracy: 0.7870 - val_loss: 0.4541 - val_accuracy: 0.8027<br/>Epoch 3/5<br/>4166/4166 [==============================] - 248s 59ms/step - loss: 0.4232 - accuracy: 0.8271 - val_loss: 0.4680 - val_accuracy: 0.8081<br/>Epoch 4/5<br/>4166/4166 [==============================] - 247s 59ms/step - loss: 0.3577 - accuracy: 0.8606 - val_loss: 0.4994 - val_accuracy: 0.8103<br/>Epoch 5/5<br/>4166/4166 [==============================] - 247s 59ms/step - loss: 0.2980 - accuracy: 0.8849 - val_loss: 0.6219 - val_accuracy: 0.8166</span></pre><h1 id="66f6" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">节省重量</h1><p id="fffb" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">让我们保存或建模，以便我们可以再次使用它或将其部署为streamlit web应用程序。</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="b35f" class="mz kx iq mv b gy na nb l nc nd">model.save_weights('weights.h5', overwrite=True)</span></pre><h1 id="f74a" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">显示损失和准确性</h1><p id="3b07" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们可以在下面的线图中看到结果，每个时期我们的训练和验证准确性都有所提高。损失线情节中的故事是非常不同的，在纪元3之后损失开始增加。为了在测试数据集上获得更好的结果，我们应该将历元的数量保持在3。</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="7ab9" class="mz kx iq mv b gy na nb l nc nd">import matplotlib.pyplot as plt<br/>def plot_graphs(history, string):<br/>    plt.plot(history.history[string])<br/>    plt.plot(history.history["val_" + string])<br/>    plt.xlabel("Epochs")<br/>    plt.ylabel(string)<br/>    plt.legend([string, "val_" + string])<br/>    plt.show()<br/>plot_graphs(history, "accuracy")<br/>plot_graphs(history, "loss")</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/96aebbcb684a465c8d8c8531f08325a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/0*wn8BqnJ7iTw198U6.png"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/7b76fa637c75205c0d179e53486ee514.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/0*4p0Oao5yEIIgU6FO.png"/></div></figure><h1 id="180d" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">预测测试数据集并保存提交</h1><p id="2a31" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">是时候使用我们的微调模型来预测测试数据集上的标签了。我们将重复用于训练数据集和预测标签的步骤。</p><ol class=""><li id="43df" class="nl nm iq lq b lr ml lu mm lx nn mb no mf np mj ob nr ns nt bi translated">读取测试数据</li><li id="49a9" class="nl nm iq lq b lr nu lu nv lx nw mb nx mf ny mj ob nr ns nt bi translated">编码</li><li id="d797" class="nl nm iq lq b lr nu lu nv lx nw mb nx mf ny mj ob nr ns nt bi translated">TF数据集</li><li id="8b22" class="nl nm iq lq b lr nu lu nv lx nw mb nx mf ny mj ob nr ns nt bi translated">批量预测</li><li id="92be" class="nl nm iq lq b lr nu lu nv lx nw mb nx mf ny mj ob nr ns nt bi translated">熊猫提交的数据框</li><li id="0ef7" class="nl nm iq lq b lr nu lu nv lx nw mb nx mf ny mj ob nr ns nt bi translated">重新映射标签</li><li id="c78b" class="nl nm iq lq b lr nu lu nv lx nw mb nx mf ny mj ob nr ns nt bi translated">保存CSV</li></ol><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="b874" class="mz kx iq mv b gy na nb l nc nd">test = pd.read_csv("../input/arabizi-dialect/Test (1).csv")<br/>test.text = test.text.astype(str)<br/>test_encoded = bert_encode(test.text)</span><span id="734b" class="mz kx iq mv b gy ne nb l nc nd">##Loading Test Data<br/>test_dataset = tf.data.Dataset.from_tensor_slices(test_encoded).batch(batch_size)</span><span id="b663" class="mz kx iq mv b gy ne nb l nc nd">## Prediction on test Datasets<br/>predicted_tweets = model.predict(test_dataset, batch_size=batch_size)<br/>predicted_tweets_binary = np.argmax(predicted_tweets, axis=-1)</span><span id="31a4" class="mz kx iq mv b gy ne nb l nc nd">## Submisssion <br/>my_submission = pd.DataFrame({"ID": test.ID, "label": predicted_tweets_binary})<br/>my_submission.label = my_submission.label.map({1: -1, 3: 1, 2: 0})<br/>my_submission.to_csv("/kaggle/working/my_submission.csv", index=False)</span></pre><h1 id="171b" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">检查课程和结果</h1><p id="ddc2" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">我们的模型在测试数据集上表现得相当好，并且仅用很少的实验我们就得到最好的结果。你可以尝试嵌入和其他深度学习架构，以产生更好的结果。你也可以尝试集成多个模型来提高你的准确率。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/fccc5d4726da17c6122f00db85d3d8be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1178/format:webp/1*N5FYg227bnFha1_tr9g-PQ.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片作者| Zindi测试分数</p></figure><p id="ab9a" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated">通过使用值计数，我们可以检查预测标签的分布。</p><pre class="kg kh ki kj gt mu mv mw mx aw my bi"><span id="eac5" class="mz kx iq mv b gy na nb l nc nd">my_submission.label.value_counts()</span><span id="b8c1" class="mz kx iq mv b gy ne nb l nc nd"> 1    15356<br/>-1    14208<br/> 0      436</span></pre><blockquote class="nf ng nh"><p id="83f6" class="lo lp mk lq b lr ml jr lt lu mm ju lw ni mn lz ma nj mo md me nk mp mh mi mj ij bi translated">我希望你喜欢我的工作，请♥并与你的朋友和同事分享它，如果你想阅读更多关于数据科学/机器学习的文章，请在<a class="ae kv" href="https://www.linkedin.com/in/1abidaliawan/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae kv" href="https://www.polywork.com/kingabzpro" rel="noopener ugc nofollow" target="_blank"> Polywork </a>上关注我。</p></blockquote></div><div class="ab cl od oe hu of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="ij ik il im in"><p id="424e" class="pw-post-body-paragraph lo lp iq lq b lr ml jr lt lu mm ju lw lx mn lz ma mb mo md me mf mp mh mi mj ij bi translated"><em class="mk">原载于2021年8月4日https://www.analyticsvidhya.com</em><a class="ae kv" href="https://www.analyticsvidhya.com/blog/2021/08/training-bert-text-classifier-on-tensor-processing-unit-tpu" rel="noopener ugc nofollow" target="_blank"><em class="mk"/></a><em class="mk">。</em></p></div></div>    
</body>
</html>