<html>
<head>
<title>Neural Networks for Survival Analysis in R</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于R中生存分析的神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-networks-for-survival-analysis-in-r-1e0421584ab?source=collection_archive---------14-----------------------#2021-04-18">https://towardsdatascience.com/neural-networks-for-survival-analysis-in-r-1e0421584ab?source=collection_archive---------14-----------------------#2021-04-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5416" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">训练、调整和比较生存网络的演示</h2></div><p id="751f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">更新:我已经把本教程中的大部分包从CRAN上拿走了，原因是如果你感兴趣，你可以在这里阅读<a class="ae lb" href="https://twitter.com/RaphaelS101/status/1506321623250571265" rel="noopener ugc nofollow" target="_blank"/>。要安装本教程中的软件包，你只需要将我的R-universe页面添加到你的repos中——这在下面的第一个要点中演示过，或者你可以在这里查看安装这些软件包的其他方法<a class="ae lb" href="https://github.com/mlr-org/mlr3proba/blob/main/README.md#Installation" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="4cec" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我收到了许多关于R中的生存神经网络(“生存网络”)的问题，从“这可能吗？”到“我如何在R中安装Python？”以及“我如何调整这些模型？”。如果你是一个对生存网络感兴趣的R用户，那么这篇文章就是为你准备的！这不是一个如何使用相关软件包的教程，而是一个回答这些问题的演示。</p><p id="e53b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是一个高级演示，我假设你知道:I)什么是生存分析；ii)什么是神经网络(以及常见的超参数)；iii)基本的机器学习(ML)方法，如重采样和调谐。如果需要，我很乐意在以后的文章中全面讨论这些话题。</p><p id="43b9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文中，我们将介绍如何:I)在R中安装Python模块；ii)通过{mlr3proba}使用{survivalmodels}中实现的模型；iii)用{mlr3tuning}调整模型，用{mlr3pipelines}⁴预处理数据；iv)对{mlr3proba}中的模型进行基准测试和比较；v)分析{mlr3benchmark}⁵.的结果这些软件包中有许多都属于mlr3系列，如果您想了解更多，我建议您从mlr3book⁶.开始</p><p id="9ffe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本演示中的代码是一个“玩具”示例，选择在我非常旧的笔记本电脑上快速运行代码，预计所有型号的性能都很差。</p><p id="6ad8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们得到深度学习！</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/0378bd042d644a1913dca7665f654046.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fR5BwWPeM69ufaVM"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">照片由<a class="ae lb" href="https://unsplash.com/@urielsc26?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">乌列尔SC </a>在<a class="ae lb" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="178e" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">安装软件包</h1><p id="d7ff" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">我们将使用几个软件包，确保您安装了以下软件:</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="7dda" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于这个演示，我使用了以下包版本:gg plot(v 3 . 3 . 2)；MLR 3 benchmark(v 0 . 1 . 2)；MLR 3 extra learners(v 0 . 3 . 5)；MLR 3管道(v 0 . 3 . 4)；MLR 3 proba(v 0 . 3 . 2)；MLR 3调优(v 0 . 8 . 0)；生存模型(v0.1.7)。</p><h1 id="91a8" class="lz ma iq bd mb mc my me mf mg mz mi mj jw na jx ml jz nb ka mn kc nc kd mp mq bi translated">生存模型</h1><p id="96fd" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">包{survivalmodels}当前包含神经网络:</p><ul class=""><li id="874b" class="nd ne iq kh b ki kj kl km ko nf ks ng kw nh la ni nj nk nl bi translated">CoxTime⁷</li><li id="17cf" class="nd ne iq kh b ki nm kl nn ko no ks np kw nq la ni nj nk nl bi translated">DeepHit⁸</li><li id="b358" class="nd ne iq kh b ki nm kl nn ko no ks np kw nq la ni nj nk nl bi translated">DeepSurv⁹</li><li id="3b64" class="nd ne iq kh b ki nm kl nn ko no ks np kw nq la ni nj nk nl bi translated">物流危害⁰</li><li id="a42e" class="nd ne iq kh b ki nm kl nn ko no ks np kw nq la ni nj nk nl bi translated">PCHazard</li><li id="c528" class="nd ne iq kh b ki nm kl nn ko no ks np kw nq la ni nj nk nl bi translated">DNNSurv</li></ul><p id="8323" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中的前五个使用{reticulate}来连接伟大的Python {pycox} ⁴包，该包由哈瓦德·克瓦梅编写，这意味着您可以使用r语言中的神经网络来实现Python的速度。DNNSurv使用R {keras} ⁵软件包。</p><p id="405c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文中，我们将只看前五个，因为它们在文献中被更好地建立，并且它们具有相同的接口，这简化了调谐，正如我们将在下面看到的。在这篇文章中没有提供网络的描述，但是如果需要的话，我很乐意在以后的文章中详细描述。</p><h2 id="8cf2" class="nr ma iq bd mb ns nt dn mf nu nv dp mj ko nw nx ml ks ny nz mn kw oa ob mp oc bi translated">在R中使用Python</h2><p id="f12b" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">要在{survivalmodels}中使用Python模型，您需要在R中设置一个Miniconda环境，并安装所需的Python模块。使用{survivalmodels}中的函数可以安装所需的模块:</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="f893" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe od oe of og b">install_pycox</code>使用<code class="fe od oe of og b">reticulate::py_install</code>安装Python包{pycox}和可选的{torch} ⁶ ( <code class="fe od oe of og b">install_torch = TRUE</code>)。<code class="fe od oe of og b">install_keras </code>将安装{keras}和可选的{tensorflow} ⁷ ( <code class="fe od oe of og b">install_tensorflow = TRUE</code>)。</p><h2 id="ea66" class="nr ma iq bd mb ns nt dn mf nu nv dp mj ko nw nx ml ks ny nz mn kw oa ob mp oc bi translated">播种</h2><p id="3a22" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">确保用Python实现的模型产生可再现的结果比通常要稍微复杂一些，因为必须在多个地方设置种子。{survivalmodels}用一个名为<code class="fe od oe of og b">set_seed</code>的函数简化了这一过程。</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="mw mx l"/></div></figure><h1 id="7ffa" class="lz ma iq bd mb mc my me mf mg mz mi mj jw na jx ml jz nb ka mn kc nc kd mp mq bi translated">mlr3proba</h1><p id="07e4" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">为了在安装后运行这些模型，我们将使用不同的界面。{survivalmodels}的功能有限，这对于基本的模型拟合/预测来说是可以的，但神经网络通常需要数据预处理和模型调整，因此我们将使用{mlr3proba}，它是{mlr3} ⁸系列软件包的一部分，包括概率监督学习功能，生存分析是其中的一部分。{mlr3}软件包使用R6 ⁹接口在r中进行面向对象的机器学习。mlr3的完整教程可以在{mlr3book}中找到，其中还有一章是关于{mlr3proba} ⁰.的生存分析</p><p id="0566" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们来设置我们的实验！</p><h2 id="32ec" class="nr ma iq bd mb ns nt dn mf nu nv dp mj ko nw nx ml ks ny nz mn kw oa ob mp oc bi translated">生存数据</h2><p id="20fc" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">我们需要做的第一件事是获取一些生存数据集来训练我们的模型，在{mlr3proba}中，数据集保存在任务中，其中包括关于特征和目标的信息。我们将使用{mlr3proba}、<code class="fe od oe of og b">whas</code>附带的一个任务，以及我们自己设置的一个任务(虽然{mlr3proba}中也已提供，但这只是一个示例)。</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="mw mx l"/></div></figure><h2 id="fa4b" class="nr ma iq bd mb ns nt dn mf nu nv dp mj ko nw nx ml ks ny nz mn kw oa ob mp oc bi translated">获取和调整学习者</h2><p id="ddae" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">现在该你上场了！我们将在{survivalmodels}(除了DNNSurv)中训练和调整Pycox神经网络。调节由{mlr3tuning}包处理。我们不会为模型指定一个定制架构，而是使用默认架构，如果您熟悉PyTorch，那么您可以选择创建自己的架构，如果您愿意的话，可以将它传递给模型中的<code class="fe od oe of og b">custom_net</code>参数。</p><p id="3160" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">超参数配置<br/> </strong>训练和调整神经网络是一门艺术，但对于本文，我们保持简单。我们将使用以下配置来调整神经网络:</p><ul class=""><li id="c9eb" class="nd ne iq kh b ki kj kl km ko nf ks ng kw nh la ni nj nk nl bi translated">辍学分数在[0，1]范围内调整</li><li id="b717" class="nd ne iq kh b ki nm kl nn ko no ks np kw nq la ni nj nk nl bi translated">重量在[0，0.5]范围内衰减</li><li id="00c5" class="nd ne iq kh b ki nm kl nn ko no ks np kw nq la ni nj nk nl bi translated">学习率超过[0，1]</li><li id="abfa" class="nd ne iq kh b ki nm kl nn ko no ks np kw nq la ni nj nk nl bi translated">{1，…，32}上一层中的节点数</li><li id="d2f1" class="nd ne iq kh b ki nm kl nn ko no ks np kw nq la ni nj nk nl bi translated">{1，…，4}上的隐藏层数</li></ul><p id="3612" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为此，我们使用{paradox}包(也是{mlr3}的一部分)来创建超参数搜索空间。{survivalmodels}中的所有Pycox学习器都有一个相同的参数接口，因此只需提供一个搜索空间。在{survivalmodels}中，节点数<code class="fe od oe of og b">num_nodes</code>被指定为任意长度的向量，这是不可直接调整的。因此，我们分别调整一层中的节点数<code class="fe od oe of og b">nodes</code>和层数<code class="fe od oe of og b">k</code>，然后提供一个转换来组合这两者。</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="73ba" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，在我们的转换中，我们假设每层有相同数量的节点，这是一个相当常见的假设，但是我们可以考虑更高级的转换。</p><p id="11af" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们现在将学习者包裹在一个<code class="fe od oe of og b">AutoTuner</code>中，这使得学习者可以很容易地在基准实验中调整。因为我们正在调优多个相似的学习者，所以我们可以创建一个函数来简化<code class="fe od oe of og b">AutoTuner </code>的创建。对于调优，我们使用:2/3分离维持、c索引优化和2次迭代随机搜索。这些设置不应该在实践中使用，只是为了让事情运行得更快。在实践中，我通常推荐三重嵌套交叉验证，<code class="fe od oe of og b">rsmp("cv", folds = 3)</code>和60次迭代随机搜索，<code class="fe od oe of og b">trm("evals", n_evals = 60)</code>。</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="c4c9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们找到我们的学习者并应用我们的函数。对于所有学员，我们将设置以下超级参数:</p><ul class=""><li id="c005" class="nd ne iq kh b ki kj kl km ko nf ks ng kw nh la ni nj nk nl bi translated">嵌套训练数据的30%将被保留作为提前停止的验证数据，<code class="fe od oe of og b">frac = 0.3, early_stopping = TRUE</code></li><li id="615e" class="nd ne iq kh b ki nm kl nn ko no ks np kw nq la ni nj nk nl bi translated">亚当优化器，<code class="fe od oe of og b">optimizer = “adam"</code></li><li id="f9ce" class="nd ne iq kh b ki nm kl nn ko no ks np kw nq la ni nj nk nl bi translated">最多10个历元，<code class="fe od oe of og b">epochs = 10</code></li></ul><p id="3205" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当我们使用早期停止时，历元的数量通常会大幅度增加(比如最少100个)，但在这里会减少以加快运行速度。所有其他超参数使用模型默认值。</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="1aac" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">预处理<br/>T5】所有的神经网络都需要一些数据预处理。{mlr3pipelines}包，特别是<code class="fe od oe of og b">encode</code>和<code class="fe od oe of og b">scale </code> pipeops使这变得简单，它们分别执行一键编码和特征标准化(通过改变参数可获得其他方法)。同样，我们将创建一个适用于所有学习者的函数。</strong></p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="mw mx l"/></div></figure><h2 id="72fa" class="nr ma iq bd mb ns nt dn mf nu nv dp mj ko nw nx ml ks ny nz mn kw oa ob mp oc bi translated">基准</h2><p id="8522" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">我们准备好了！对于我们的实验，我们将使用三重交叉验证，但通常五重交叉验证将是首选，<code class="fe od oe of og b">rsmp("cv", folds = 5)</code>。为了比较，我们还将卡普兰-迈耶和考克斯PH ⁴学习者添加到实验中。我们将把我们的基准结果与哈勒尔的c指数⁵和⁶综合格拉夫得分(也有许多其他措施可用)进行汇总。</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="mw mx l"/></div></figure><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/f406857bd8d1b8fef51970cfd7ff52f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*wAsfiEiXy5QufJSYXZELQQ.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">结果由Harrell's C和Graf综合评分汇总</p></figure><p id="ea58" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们的玩具演示中，我们可以从这些结果中初步得出结论，Cox PH的性能最好，DeepHit的性能最差。</p><h1 id="642d" class="lz ma iq bd mb mc my me mf mg mz mi mj jw na jx ml jz nb ka mn kc nc kd mp mq bi translated">分析结果</h1><p id="c6a3" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">由于我们已经在多个独立数据集上运行了我们的模型，我们可以使用{mlr3benchmark}更详细地比较我们的结果。下面的注释代码只是展示了可能的情况，但没有提供任何细节(如果您对未来的教程感兴趣，请告诉我！).</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="mw mx l"/></div></figure><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/25dd3a3910f3563e045a8fd5e8f688dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*6ucSjLBMao3JxZKRi_NI4Q.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">独立数据集上的弗里德曼检验结果。</p></figure><p id="3ce1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Friedman检验结果表明，两个模型在任何一个测量方面都没有显著差异(假设p ≤ 0.05是显著的)。现在让我们说，如果p ≤ 0.1，模型是显著不同的(我不建议这是普遍的)，所以我们可以看看临界差异图⁷来比较这些模型。</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="mw mx l"/></div></figure><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/32f1e9bbd9124f257ab6872e79a7dd9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*fjxlp5ZZ_NlQ8dKNUU3PtA.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">IGS的临界差异图，左边是表现最好的学生，右边越来越差。粗黑线将没有显著差异的学习者联系起来。</p></figure><p id="1b55" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">结果表明，没有模型优于Kaplan-Meier基线，我们的分析是完整的(对于这个玩具设置，这并不奇怪！).</p><h1 id="2aed" class="lz ma iq bd mb mc my me mf mg mz mi mj jw na jx ml jz nb ka mn kc nc kd mp mq bi translated">摘要</h1><p id="3aa3" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">在这个演示中，我们使用了用Python实现的神经网络，并通过{survivalmodels}进行接口。我们用{mlr3proba}接口加载了这些模型，得到了一些生存任务。我们使用{mlr3tuning}设置超参数配置和调节控制，使用{mlr3pipelines}进行数据预处理。最后，我们使用{mlr3benchmark}来分析多个数据集的结果。我希望这篇文章展示了mlr3接口是如何简化从{survivalmodels}中选择、调整和比较模型的。</p><p id="84d7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">感谢阅读！继续评论，发电子邮件，或发微博给我(拉斐尔101 ),提出问题，评论，或只是聊聊ML和统计！</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="02c8" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">参考</h1><p id="3207" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">¹ Sonabend, R. (2020). survivalmodels: Models for Survival Analysis. CRAN. <a class="ae lb" href="https://cran.r-project.org/package=survivalmodels" rel="noopener ugc nofollow" target="_blank">https://cran.r-project.org/package=survivalmodels</a><br/>² Sonabend, R., Király, F. J., Bender, A., Bischl, B., &amp; Lang, M. (2021). mlr3proba: An R Package for Machine Learning in Survival Analysis. <em class="ok">Bioinformatics</em>. <a class="ae lb" href="https://doi.org/10.1093/bioinformatics/btab039" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1093/bioinformatics/btab039</a><br/>³ Lang, M., Richter, J., Bischl, B., &amp; Schalk, D. (2019). mlr3tuning: Tuning for “mlr3.” CRAN. <a class="ae lb" href="https://cran.r-project.org/package=mlr3tuning" rel="noopener ugc nofollow" target="_blank">https://cran.r-project.org/package=mlr3tuning</a><br/>⁴ Binder, M., Pfisterer, F., Bischl, B., Lang, M., &amp; Dandl, S. (2019). mlr3pipelines: Preprocessing Operators and Pipelines for “mlr3.” CRAN. <a class="ae lb" href="https://cran.r-project.org/package=mlr3pipelines" rel="noopener ugc nofollow" target="_blank">https://cran.r-project.org/package=mlr3pipelines</a><br/>⁵ Sonabend, R., &amp; Pfisterer, F. (2020). mlr3benchmark: Benchmarking analysis for “mlr3.” CRAN. <a class="ae lb" href="https://cran.r-project.org/package=mlr3benchmark" rel="noopener ugc nofollow" target="_blank">https://cran.r-project.org/package=mlr3benchmark</a><br/>⁶ <a class="ae lb" href="https://mlr3book.mlr-org.com/" rel="noopener ugc nofollow" target="_blank">https://mlr3book.mlr-org.com/</a><br/>⁷ Kvamme, H., Borgan, Ø., &amp; Scheel, I. (2019). Time-to-event prediction with neural networks and Cox regression. <em class="ok">Journal of Machine Learning Research</em>, <em class="ok">20</em>(129), 1–30.<br/>⁸ Lee, C., Zame, W. R., Yoon, J., &amp; van der Schaar, M. (2018). Deephit: A deep learning approach to survival analysis with competing risks. In <em class="ok">Thirty-Second AAAI Conference on Artificial Intelligence</em>.<br/>⁹ Katzman, J. L., Shaham, U., Cloninger, A., Bates, J., Jiang, T., &amp; Kluger, Y. (2018). DeepSurv: personalized treatment recommender system using a Cox proportional hazards deep neural network. <em class="ok">BMC Medical Research Methodology</em>, <em class="ok">18</em>(1), 24. <a class="ae lb" href="https://doi.org/10.1186/s12874-018-0482-1" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1186/s12874-018-0482-1</a><br/>¹⁰ Gensheimer, M. F., &amp; Narasimhan, B. (2019). A scalable discrete-time survival model for neural networks. <em class="ok">PeerJ</em>, <em class="ok">7</em>, e6257.<br/>¹¹ Kvamme, H., &amp; Borgan, Ø. (2019). Continuous and discrete-time survival prediction with neural networks. <em class="ok">ArXiv Preprint ArXiv:1910.06724</em>.<br/>¹² Zhao, L., &amp; Feng, D. (2020). DNNSurv: Deep Neural Networks for Survival Analysis Using Pseudo Values. <a class="ae lb" href="https://arxiv.org/abs/1908.02337" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1908.02337</a><br/>¹³ Ushey, K., Allaire, J. J., &amp; Tang, Y. (2020). reticulate: Interface to “Python.” CRAN. <a class="ae lb" href="https://cran.r-project.org/package=reticulate" rel="noopener ugc nofollow" target="_blank">https://cran.r-project.org/package=reticulate</a><br/>¹⁴ Kvamme, H. (2018). pycox. <a class="ae lb" href="https://pypi.org/project/pycox/" rel="noopener ugc nofollow" target="_blank">https://pypi.org/project/pycox/</a><br/>¹⁵ Allaire, J. J., &amp; Chollet, F. (2020). keras: R Interface to “Keras.” CRAN. <a class="ae lb" href="https://cran.r-project.org/package=keras" rel="noopener ugc nofollow" target="_blank">https://cran.r-project.org/package=keras</a><br/>¹⁶ Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., … Lerer, A. (2017). Automatic differentiation in pytorch.<br/>¹⁷ Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., … Zheng, X. (2015). TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. <a class="ae lb" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank">https://www.tensorflow.org/</a><br/>¹⁸ Lang, M., Binder, M., Richter, J., Schratz, P., Pfisterer, F., Coors, S., … Bischl, B. (2019). mlr3: A modern object-oriented machine learning framework in R. <em class="ok">Journal of Open Source Software</em>, <em class="ok">4</em>(44), 1903. <a class="ae lb" href="https://doi.org/10.21105/joss.01903" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.21105/joss.01903</a><br/>¹⁹ Chang, W. (2018). R6: Classes with Reference Semantics. <a class="ae lb" href="https://cran.r-project.org/package=R6" rel="noopener ugc nofollow" target="_blank">https://cran.r-project.org/package=R6</a><br/>²⁰ <a class="ae lb" href="https://mlr3book.mlr-org.com/survival.html" rel="noopener ugc nofollow" target="_blank">https://mlr3book.mlr-org.com/survival.html</a><br/>²¹ Lang, M., Bischl, B., Richter, J., Sun, X., &amp; Binder, M. (2019). paradox: Define and Work with Parameter Spaces for Complex Algorithms. CRAN. <a class="ae lb" href="https://cran.r-project.org/package=paradox" rel="noopener ugc nofollow" target="_blank">https://cran.r-project.org/package=paradox</a><br/>²² Bergstra, J., &amp; Bengio, Y. (2012). Random search for hyper-parameter optimization. <em class="ok">The Journal of Machine Learning Research</em>, <em class="ok">13</em>(1), 281–305.<br/>²³ Kaplan, E. L., &amp; Meier, P. (1958). Nonparametric Estimation from Incomplete Observations. <em class="ok">Journal of the American Statistical Association</em>, <em class="ok">53</em>(282), 457–481. <a class="ae lb" href="https://doi.org/10.2307/2281868" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.2307/2281868</a><br/>²⁴ Cox, D. R. (1972). Regression Models and Life-Tables. <em class="ok">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, <em class="ok">34</em>(2), 187–220.<br/>²⁵ Harrell, F. E., Califf, R. M., &amp; Pryor, D. B. (1982). Evaluating the yield of medical tests. <em class="ok">JAMA</em>, <em class="ok">247</em>(18), 2543–2546. <a class="ae lb" href="http://dx.doi.org/10.1001/jama.1982.03320430047030" rel="noopener ugc nofollow" target="_blank">http://dx.doi.org/10.1001/jama.1982.03320430047030</a><br/>²⁶ Graf, E., Schmoor, C., Sauerbrei, W., &amp; Schumacher, M. (1999). Assessment and comparison of prognostic classification schemes for survival data. <em class="ok">Statistics in Medicine</em>, <em class="ok">18</em>(17–18), 2529–2545. https://doi.org/10.1002/(SICI)1097-0258(19990915/30)18:17/18&lt;2529::AID-SIM274&gt;3.0.CO;2-5<br/>²⁷ Demšar, J. (2006). Statistical comparisons of classifiers over multiple data sets. <em class="ok">Journal of Machine Learning Research</em>, <em class="ok">7</em>(Jan), 1–30.</p></div></div>    
</body>
</html>