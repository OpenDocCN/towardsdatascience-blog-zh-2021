<html>
<head>
<title>Building a Real Life Data Lake in AWS</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在AWS中构建真实的数据湖</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-real-life-data-lake-in-aws-ee3bc9b8bba1?source=collection_archive---------5-----------------------#2021-04-10">https://towardsdatascience.com/building-a-real-life-data-lake-in-aws-ee3bc9b8bba1?source=collection_archive---------5-----------------------#2021-04-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b094" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">AWS特性如何影响为AI构建的ACID数据湖解决方案的架构</h2></div><p id="b285" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有一些重要的用例、需求和架构选择会改变数据湖的构建方式。在这里，我们将关注直接来自数据湖的数据消费的好处，同时解决接近实时的人工智能推理需求。</p><p id="3098" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将简洁地介绍一些架构选择及其含义，同时查看使用Spark over S3的数据工程。</p><p id="7cfa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">目标受众</strong></p><p id="1c1f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">读者应该对Apache Spark和AWS有所了解。</p><p id="2c58" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> AI推理延迟需求</strong></p><p id="2f08" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将专注于推理时间延迟需求以分钟/秒为单位的用例。<br/>在这种情况下，您可以选择基于Apache Spark的架构作为您的流&amp;批处理AI处理的公共基础。您还可以在流式处理和批处理之间共享代码，同时提高下游读取性能。<br/> <em class="lb">如果这不是您的用例</em>，您仍然可以为您的解决方案的数据湖部分实现以下大部分内容。</p><h1 id="e6f2" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">体系结构</h1><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi lu"><img src="../Images/d0b4a2d3ecfe25fbab9bafaeba6aff02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PCDrnBvQqMAcJZ2_ILRO1Q.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">作者图片</p></figure><p id="45d6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，您不必准备好所有选项。例如，一个人可以选择完全放弃Kinesis/Kafka，而仅仅依靠消防软管。<br/>由于我们关注的是基于人工智能的用例，业务级聚合数据湖层超出了本次讨论的范围。此外，我们将关注可以结构化的数据。</p><h1 id="e1d0" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">视觉议程</h1><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi mk"><img src="../Images/a51db5109317544a750fd8840aee7dcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_VpkrZ45_uqbFnyw6yuscg.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">作者图片</p></figure><h1 id="8bdc" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">基础</h1><h2 id="f3c0" class="ml ld iq bd le mm mn dn li mo mp dp lm ko mq mr lo ks ms mt lq kw mu mv ls mw bi translated">你真的有大数据，需要数据湖吗？</h2><p id="c5cc" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">这篇文章讨论大数据。在去之前，请确定你是否拥有/期望拥有大数据。如果没有，就不要去那里。</p><p id="d037" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">300GBs不是“大数据”。</p><h2 id="ff0d" class="ml ld iq bd le mm mn dn li mo mp dp lm ko mq mr lo ks ms mt lq kw mu mv ls mw bi translated">通用数据湖结构</h2><p id="2efb" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">数据应该被划分成适当数量的分区。数据保存在大文件中，通常约为128MB-1GB。维护大数据的二级索引通常成本过高。</p><p id="43a3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，通用解决方案集成了Hive Metastore(即AWS Glue Catalog)用于EDA/BI目的。数据通常来自Kinesis/Kafka等流媒体源或S3着陆区，后者本身通常是Kinesis Firehose的目标。</p><h2 id="ca9e" class="ml ld iq bd le mm mn dn li mo mp dp lm ko mq mr lo ks ms mt lq kw mu mv ls mw bi translated">如何跟踪已经处理过的输入数据</h2><p id="0b05" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">Spark结构化流已经包含<code class="fe nc nd ne nf b">.option("checkpointLocation", checkpoint_path)</code>。它跟踪所有的偏移，不管你的输入:卡夫卡/MSK，S3着陆区，Kinesis等。</p><p id="6eb9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是基本选项，请阅读下面更高级的选项。</p><h2 id="fbee" class="ml ld iq bd le mm mn dn li mo mp dp lm ko mq mr lo ks ms mt lq kw mu mv ls mw bi translated">为什么我应该使用分区？</h2><p id="62a3" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">在大多数情况下，数据湖消费者，ala you AI应用程序，将只需要读取数据的子集。当数据被分区时，Spark等查询工具可以利用分区消除，因此只需要扫描一部分数据。</p><p id="1f4c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，这对于低基数分区键ala数据到达月份很有效。对于其他用例，人们不得不求助于对原始分区键的某种散列/装箱试探法，或者利用诸如<a class="ae ng" href="https://docs.databricks.com/delta/optimizations/file-mgmt.html#z-ordering-multi-dimensional-clustering" rel="noopener ugc nofollow" target="_blank"> Z排序</a>之类的解决方案。</p><h2 id="5380" class="ml ld iq bd le mm mn dn li mo mp dp lm ko mq mr lo ks ms mt lq kw mu mv ls mw bi translated">如何定义分区中文件的数量</h2><p id="8f7a" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">当数据湖消费者读取数据时，将扫描分区中的所有文件，至少是读取元数据。分区中的文件数量应该是扫描分区的活动Spark任务数量的几倍。<br/>也就是说，如果你的分区有30个文件，但你的执行程序数乘以每个执行程序的内核数是130，一般情况下每个执行程序有100个内核不做任何事情，你要为此付出代价，包括现金和查询性能。</p><p id="b953" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其他调配的服务也是如此，如雪花集群大小或红移插槽。</p><h2 id="4de7" class="ml ld iq bd le mm mn dn li mo mp dp lm ko mq mr lo ks ms mt lq kw mu mv ls mw bi translated">如何定义数据格式</h2><p id="1d52" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">因为所有文件至少都要扫描元数据，所以最好使用支持压缩拼花和一些非分栏格式的S3选择。因此，您希望您的数据湖有压缩拼花文件，这是由S3支持的。</p><p id="7de3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文的后面，我们将介绍推荐的使用Parquet的数据湖框架。</p><h1 id="3517" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">近实时人工智能推理的含义</h1><h2 id="e02c" class="ml ld iq bd le mm mn dn li mo mp dp lm ko mq mr lo ks ms mt lq kw mu mv ls mw bi translated">微批处理中的数据摄取</h2><p id="5c6c" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">如果需要接近实时的人工智能推理，那么为您的用例使用正确的火花<code class="fe nc nd ne nf b">.trigger()</code>将数据流入数据湖是很重要的。</p><p id="c96a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个简单的要求可能会产生如下所述的重大后果。</p><h2 id="7948" class="ml ld iq bd le mm mn dn li mo mp dp lm ko mq mr lo ks ms mt lq kw mu mv ls mw bi translated">微批量摄取影响—数据压缩</h2><p id="8ff7" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">微批处理中的数据摄取可能会产生许多更小的文件。<br/>为了提供每个分区可预测的查询时间，我们应该:</p><ul class=""><li id="d7e8" class="nh ni iq kh b ki kj kl km ko nj ks nk kw nl la nm nn no np bi translated">创建具有相似数据大小的分区</li><li id="db9a" class="nh ni iq kh b ki nq kl nr ko ns ks nt kw nu la nm nn no np bi translated">确保将小文件中的数据移动到更大的大约128MB的文件中。这个过程称为压缩。</li></ul><p id="ed46" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">压缩过程提高了下游读取吞吐量，尤其是在对象存储(即S3)方面。</p><p id="ace5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，一些数据湖根本不实现压缩，因为这会增加复杂性，并可能给系统带来额外的延迟。在这种情况下，必须格外小心它们的文件大小和ETL流。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/766237a26af352fbb8e32953ae46e517.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*pV51IjufjIpFCMCc_FjQNw.png"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">作者图片</p></figure><h2 id="411c" class="ml ld iq bd le mm mn dn li mo mp dp lm ko mq mr lo ks ms mt lq kw mu mv ls mw bi translated">数据压缩实现策略</h2><ul class=""><li id="430c" class="nh ni iq kh b ki mx kl my ko nw ks nx kw ny la nm nn no np bi translated">依赖商业实现，如<a class="ae ng" href="https://databricks.com/product/delta-lake-on-databricks" rel="noopener ugc nofollow" target="_blank">数据块</a></li><li id="5c4f" class="nh ni iq kh b ki nq kl nr ko ns ks nt kw nu la nm nn no np bi translated">对记录大小可预测的文件使用<code class="fe nc nd ne nf b">.option("maxRecordsPerFile", ...)</code>。请注意，对于嵌套数据，只考虑顶层行。</li><li id="ac54" class="nh ni iq kh b ki nq kl nr ko ns ks nt kw nu la nm nn no np bi translated">使用<code class="fe nc nd ne nf b">aws s3 ls</code> AWS API对应物评估总分区大小。<code class="fe nc nd ne nf b">repartition*()</code> / <code class="fe nc nd ne nf b">partitionBy()</code>用所需的文件号为Spark &amp;配置单元分区设置正确的数量。</li><li id="c70d" class="nh ni iq kh b ki nq kl nr ko ns ks nt kw nu la nm nn no np bi translated">仅对文件最近发生更改的分区执行上述操作。每次压缩所有分区会耗费太多的时间和开销—请参见下面的详细信息。</li></ul><h2 id="4cf6" class="ml ld iq bd le mm mn dn li mo mp dp lm ko mq mr lo ks ms mt lq kw mu mv ls mw bi translated">清单文件</h2><p id="48b2" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">当数据被压缩时，新的Parquet文件出现在分区中。消费Spark/红移/雪花查询应该读取哪些文件？我们绝对需要添加一个元数据。有两种方法可以解决这个问题:</p><ul class=""><li id="ed62" class="nh ni iq kh b ki kj kl km ko nj ks nk kw nl la nm nn no np bi translated">保存一个分区清单文件，其中的内容列出了每个分区中的最新文件</li><li id="cb09" class="nh ni iq kh b ki nq kl nr ko ns ks nt kw nu la nm nn no np bi translated">使用自动更新这些内容框架。稍后将详细介绍。</li></ul><p id="320f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">否则，我们的工作将是读取陈旧数据或陈旧和最新的数据在一起！</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/8a2c6ecaf6a4ae4765db111a0e6eda62.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*gotBzMg5Ad_T_gBDk1rZrA.png"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">作者图片</p></figure><h1 id="04c0" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">酸性框架</h1><p id="8e37" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">ACID框架支持以下功能:</p><ul class=""><li id="2897" class="nh ni iq kh b ki kj kl km ko nj ks nk kw nl la nm nn no np bi translated">自动数据压缩(仅在商业版本中有一些)</li><li id="495b" class="nh ni iq kh b ki nq kl nr ko ns ks nt kw nu la nm nn no np bi translated">通过附加元数据缩短查询时间。</li><li id="e5e9" class="nh ni iq kh b ki nq kl nr ko ns ks nt kw nu la nm nn no np bi translated">分区清单文件的自动生成</li><li id="aa33" class="nh ni iq kh b ki nq kl nr ko ns ks nt kw nu la nm nn no np bi translated">对数据湖消费者的ACID保证(有些限制适用，见下文)</li></ul><p id="5129" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">选项包括:</p><ul class=""><li id="c232" class="nh ni iq kh b ki kj kl km ko nj ks nk kw nl la nm nn no np bi translated">三角洲湖</li><li id="b5a0" class="nh ni iq kh b ki nq kl nr ko ns ks nt kw nu la nm nn no np bi translated">商业数据块版本—具有开源版本中没有的缓存和Z顺序性能改进</li><li id="ec8a" class="nh ni iq kh b ki nq kl nr ko ns ks nt kw nu la nm nn no np bi translated">阿帕奇胡迪——两种操作模式</li><li id="61d2" class="nh ni iq kh b ki nq kl nr ko ns ks nt kw nu la nm nn no np bi translated">Apache Iceberg —大约在2020年底，Iceberg不支持来自精选数据的流。</li></ul><p id="a2cb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，一些功能，如Delta Catalog，需要Spark 3.0.0+,因此只能在EMR中使用，而不能在Glue中使用。在撰写本文时，AWS Glue不支持spark 3.0.0+。</p><h2 id="2ee4" class="ml ld iq bd le mm mn dn li mo mp dp lm ko mq mr lo ks ms mt lq kw mu mv ls mw bi translated">S3对火花驱动器数量的限制</h2><p id="b1b4" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">由于S3不支持原子重命名这一事实，对于</p><ul class=""><li id="3ad2" class="nh ni iq kh b ki kj kl km ko nj ks nk kw nl la nm nn no np bi translated"><a class="ae ng" href="https://github.com/delta-io/delta/issues/564" rel="noopener ugc nofollow" target="_blank">三角洲湖</a></li><li id="6b00" class="nh ni iq kh b ki nq kl nr ko ns ks nt kw nu la nm nn no np bi translated"><a class="ae ng" href="https://github.com/apache/hudi/issues/2330" rel="noopener ugc nofollow" target="_blank">胡迪</a></li><li id="007a" class="nh ni iq kh b ki nq kl nr ko ns ks nt kw nu la nm nn no np bi translated"><a class="ae ng" href="https://github.com/apache/iceberg/issues/1912" rel="noopener ugc nofollow" target="_blank">冰山</a>我们实际上只能使用单个火花驱动器<strong class="kh ir">在任何给定时间向数据湖写入</strong>。</li></ul><p id="74a1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这也意味着在Spark 之外使用这些框架<a class="ae ng" href="https://github.com/delta-io/connectors/wiki/Delta-Standalone-Reader" rel="noopener ugc nofollow" target="_blank">对于只读访问来说是非常可靠的。</a></p><h2 id="47e8" class="ml ld iq bd le mm mn dn li mo mp dp lm ko mq mr lo ks ms mt lq kw mu mv ls mw bi translated">单火花驱动器的含义</h2><p id="90bc" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">S3限制火花驱动器数量的必然结果:来自几个不同来源的消耗，例如平行的Kinesis &amp; S3着陆区必须发生在一个火花驱动器上！即使你没有连接这些火花流之间的数据，也是如此。毕竟，他们正在更新S3上相同的元数据位置。</p><p id="282e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">实现这一点的方法可以是:</p><ul class=""><li id="4ba2" class="nh ni iq kh b ki kj kl km ko nj ks nk kw nl la nm nn no np bi translated">在单个会话中使用Apache Livy(我自己没有试过)或者</li><li id="3e36" class="nh ni iq kh b ki nq kl nr ko ns ks nt kw nu la nm nn no np bi translated">使用<code class="fe nc nd ne nf b">.trigger(once=True)</code>运行EMR流步骤</li><li id="20eb" class="nh ni iq kh b ki nq kl nr ko ns ks nt kw nu la nm nn no np bi translated"><code class="fe nc nd ne nf b">.union()</code>不同的火花流，一次写入</li></ul><h2 id="6614" class="ml ld iq bd le mm mn dn li mo mp dp lm ko mq mr lo ks ms mt lq kw mu mv ls mw bi translated">数据压缩影响—文件清空</h2><p id="6c22" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">压缩过程和通常的任何数据更新都会创建过时的文件。为了不占用空间/使S3列表开销过大，必须将它们移除。除非您使用的是Databricks commercial Delta lake，否则需要计划和调用此过程。</p><h2 id="163c" class="ml ld iq bd le mm mn dn li mo mp dp lm ko mq mr lo ks ms mt lq kw mu mv ls mw bi translated">S3灾难恢复</h2><p id="3765" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">最好保留生成的S3存储桶的版本，因为它将允许出于灾难恢复目的的跨区域存储桶复制。</p><h2 id="2692" class="ml ld iq bd le mm mn dn li mo mp dp lm ko mq mr lo ks ms mt lq kw mu mv ls mw bi translated">S3清理</h2><p id="d2e0" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">必须设置旧版本文件的S3生命周期规则来清理对象存储。真空是不够的，它只会导致S3删除标记。</p><h1 id="4f57" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">(计划或理论的)纲要</h1><h2 id="743d" class="ml ld iq bd le mm mn dn li mo mp dp lm ko mq mr lo ks ms mt lq kw mu mv ls mw bi translated">模式合并</h2><p id="379d" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">数据模式在现实生活模式中发展。为了支持向模式中引入新列，在spark中使用<code class="fe nc nd ne nf b">.option("mergeSchema","true")</code>。</p><h2 id="8d06" class="ml ld iq bd le mm mn dn li mo mp dp lm ko mq mr lo ks ms mt lq kw mu mv ls mw bi translated">活动采购</h2><p id="1996" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">假设您的数据可以追溯更新。什么是正确的解决方案？</p><ul class=""><li id="bab8" class="nh ni iq kh b ki kj kl km ko nj ks nk kw nl la nm nn no np bi translated">就地更新数据。保存相同数据的单个副本。</li><li id="6733" class="nh ni iq kh b ki nq kl nr ko ns ks nt kw nu la nm nn no np bi translated">引入数据的另一个版本。</li></ul><p id="12f2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">大多数基于人工智能的应用程序发布被执行的推理。购买股票，向客户发送电子邮件等。这是一个与业务相关的标准，指导利用事件源，以便人工智能推理可以追溯到推理时可用的数据谱系。</p><h2 id="9744" class="ml ld iq bd le mm mn dn li mo mp dp lm ko mq mr lo ks ms mt lq kw mu mv ls mw bi translated">JDBC —对嵌套数据模式设计的影响</h2><p id="b5fe" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">有时，我们希望在我们的细粒度数据上启用BI，也就是说，不是在业务级聚合上。</p><p id="0ad8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有时真实世界的数据是嵌套的。请注意，许多JDBC工具(如红移光谱)不允许以直接的方式返回嵌套数据。在这种情况下，在将数据保存到数据湖之前使用<code class="fe nc nd ne nf b">.explode()</code>会更有效。根据您的数据，这可能是唯一的选择。</p><h1 id="5e0d" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">S3访问—附加选项</h1><h2 id="6da9" class="ml ld iq bd le mm mn dn li mo mp dp lm ko mq mr lo ks ms mt lq kw mu mv ls mw bi translated">快速新文件列表—S3—SQS</h2><p id="1af8" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">即使当使用更大的文件时，spark结构化流也必须列出所有文件，这在大数据湖中可能是很多的。Spark的<a class="ae ng" href="https://github.com/qubole/s3-sqs-connector" rel="noopener ugc nofollow" target="_blank"> S3-SQS </a>连接器可以通过保持一个队列监听来自S3的新对象通知来改善这一点。</p><h2 id="61b2" class="ml ld iq bd le mm mn dn li mo mp dp lm ko mq mr lo ks ms mt lq kw mu mv ls mw bi translated">S3从VPC内部访问</h2><p id="3c4a" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">为了通过AWS网络而不是互联网访问数据湖，使用S3 <a class="ae ng" href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html" rel="noopener ugc nofollow" target="_blank"> VPC端点</a>。这主要是作为一个安全特征的好处。有时，VPC端点可以提高性能，但这必须根据地区/使用案例进行测试。</p><h2 id="d69a" class="ml ld iq bd le mm mn dn li mo mp dp lm ko mq mr lo ks ms mt lq kw mu mv ls mw bi translated">初始ETL的着陆区考虑因素</h2><p id="b60e" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">如果您使用的S3登陆区有数百万个小文件，则需要将它们导入到策划数据湖中，以便所有历史数据都适合查询。S3文件列表有很大的开销。列出一百万个文件需要5分钟以上的时间。此外，这不应该用Spark结构化流来完成，因为驱动程序内存会变得OOM。Spark结构化流并不意味着要列出数百万个文件。对于这个用例，最好将文件列在一个本地文件中，然后用Spark批处理来分配导入工作，这样每个任务都可以获得它将读取和转换的文件名的一个子集。</p><h1 id="1cf9" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">摘要</h1><p id="7f41" class="pw-post-body-paragraph kf kg iq kh b ki mx jr kk kl my ju kn ko mz kq kr ks na ku kv kw nb ky kz la ij bi translated">我们已经在接近实时的人工智能推理场景中介绍了数据湖的基础知识。介绍了高级ACID框架，以及它们对S3的限制。还提出了基本的模式考虑，以及S3列出的加速选项。</p><p id="8eb6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我希望这可以作为AWS中大多数数据湖解决方案的蓝图。</p><p id="c3cb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">感谢您坚持到本文结束——请告诉我您的想法！也可以通过<a class="ae ng" href="https://www.linkedin.com/in/borislitvak/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>联系我。</p></div></div>    
</body>
</html>