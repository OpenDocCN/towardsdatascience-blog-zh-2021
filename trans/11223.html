<html>
<head>
<title>Modeling uncertainty in neural networks with TensorFlow Probability</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用张量流概率模拟神经网络中的不确定性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-part-1-an-introduction-2bb564c67d6?source=collection_archive---------7-----------------------#2021-11-03">https://towardsdatascience.com/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-part-1-an-introduction-2bb564c67d6?source=collection_archive---------7-----------------------#2021-11-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="9a33" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="328e" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">第一部分:引言</h2></div><p id="bb1f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="lk">本系列是使用张量流概率库对不确定性建模的简要介绍。我是作为我的</em><a class="ae ll" href="https://pydata.org/global2021/schedule/presentation/13/modeling-aleatoric-and-epistemic-uncertainty-using-tensorflow-and-tensorflow-probability/" rel="noopener ugc nofollow" target="_blank"><em class="lk">PyData Global 2021 talk</em></a><em class="lk">关于神经网络中不确定性估计的补充材料写的。</em></p><p id="0872" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">系列文章:</strong></p><ul class=""><li id="4710" class="lm ln iq kq b kr ks ku kv kx lo lb lp lf lq lj lr ls lt lu bi translated">第一部分:介绍</li><li id="9cfb" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated"><a class="ae ll" rel="noopener" target="_blank" href="/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-a706c2274d12"> <strong class="kq ja">第二部分</strong>:任意不确定性</a></li><li id="a923" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated"><a class="ae ll" rel="noopener" target="_blank" href="/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-d519a4426e9c"> <strong class="kq ja">第三部分</strong>:认知的不确定性</a></li><li id="d5b0" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated"><a class="ae ll" rel="noopener" target="_blank" href="/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-391b29538a7a"> <strong class="kq ja">第四部分:</strong>完全概率性</a></li></ul><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/0118ad33d9a780503e51956eea921b4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*TrbQ3V2V_gtSnsJqbqmnvg.jpeg"/></div><p class="mi mj gj gh gi mk ml bd b be z dk translated">图像由杰曼特里<a class="ae ll" href="https://www.pexels.com/photo/blue-body-of-water-5412/" rel="noopener ugc nofollow" target="_blank">https://www.pexels.com/photo/blue-body-of-water-5412/</a></p></figure><h1 id="acd8" class="mm mn iq bd mo mp mq mr ms mt mu mv mw kf mx kg my ki mz kj na kl nb km nc nd bi translated">为什么要对不确定性建模？</h1><p id="ef6e" class="pw-post-body-paragraph ko kp iq kq b kr ne ka kt ku nf kd kw kx ng kz la lb nh ld le lf ni lh li lj ij bi translated">让我反过来问这个问题——为什么不对不确定性建模？</p><p id="f1d8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">想象你正在建立一个医疗诊断系统。假设您的系统可以预测三类中的一类:(A)恶性肿瘤，(B)良性肿瘤，(C)无肿瘤。你的系统预测你的病人杰西患有恶性肿瘤。预测的<strong class="kq ja"> softmax得分</strong>为<strong class="kq ja"> 0.9 </strong>。你有多大把握你的预测是正确的？</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/6e7313919c89eb247e94e80dbfc6bd6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:710/format:webp/1*CKLXGjO32EcbOBI69iAcLA.png"/></div><p class="mi mj gj gh gi mk ml bd b be z dk translated">我们设计的诊断系统的softmax分布。系统返回(A)类恶性肿瘤的高softmax值。</p></figure><p id="e9b6" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">你会和杰西分享这个诊断吗？</p><p id="41b5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在您回答这个问题之前，让我们将权重不确定性估计添加到我们的模型中。这种不确定性的另一个名字是<strong class="kq ja">认知不确定性</strong>，我们将在本系列接下来的几集里详细讨论。让我们再次看看我们模型的输出。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi nk"><img src="../Images/1f7feea9ec894fc00a435e9b46235b14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eDPOZpDZf9HFTb2RTXA5MA.png"/></div></div><p class="mi mj gj gh gi mk ml bd b be z dk translated">左图:我们设计的诊断系统的softmax发行版。右图:一个softmax分布加上95%的置信区间，超过10000个来自我们模型的概率版本的预测。</p></figure><p id="0561" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">刚刚发生了什么事？我们以前见过左边的情节，但是我们在右边看到了什么？在右边的图中，我们添加了softmax分数的95%置信区间，该分数是通过从我们的模型的概率版本中抽取10.000个样本而获得的。</p><p id="39d4" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这样的结果有可能吗？尽管看起来有些违反直觉，简短的回答是肯定的。在关于认知不确定性的第<strong class="kq ja">集</strong>中，我们将看到真实世界系统以类似方式运行的例子。</p><p id="bd8d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">回到我们最初的问题——为什么(不)对不确定性建模？我们将在本系列的最后一集尝试对这个问题给出一个有根据的答案。同时，我很想听听你的想法。请在下面的评论区分享它们。</p><p id="8a02" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，让我们看看一个工具包，它将帮助我们回答关于不确定性估计的问题——张量流概率。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi np"><img src="../Images/84f71ea28a99a943bf9cb24e96c02c45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VlGzn5SOAyTxMASOOQvMGA.jpeg"/></div></div><p class="mi mj gj gh gi mk ml bd b be z dk translated">张量流概率<a class="ae ll" href="https://www.tensorflow.org/probability/api_docs/python/tfp" rel="noopener ugc nofollow" target="_blank"> API参考页</a>。你真实的照片。</p></figure><h1 id="413f" class="mm mn iq bd mo mp mq mr ms mt mu mv mw kf mx kg my ki mz kj na kl nb km nc nd bi translated">张量流概率</h1><p id="3122" class="pw-post-body-paragraph ko kp iq kq b kr ne ka kt ku nf kd kw kx ng kz la lb nh ld le lf ni lh li lj ij bi translated"><a class="ae ll" href="https://www.tensorflow.org/probability" rel="noopener ugc nofollow" target="_blank">TensorFlow Probability</a>(TFP)是一个概率编程库，是更广泛的tensor flow生态系统的一部分。它不是core TensorFlow库的一部分，需要单独安装导入。安装指南可在<a class="ae ll" href="https://www.tensorflow.org/probability/install" rel="noopener ugc nofollow" target="_blank">文档</a>中找到。</p><p id="663e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">TFP是一个综合库，有超过15个不同的<strong class="kq ja">子模块</strong>。在这个系列中，我们将关注其中的两个:</p><ul class=""><li id="dd55" class="lm ln iq kq b kr ks ku kv kx lo lb lp lf lq lj lr ls lt lu bi translated"><code class="fe nq nr ns nt b">tfp.distributions</code></li><li id="bfc7" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated"><code class="fe nq nr ns nt b">tfp.layers</code></li></ul><h2 id="6157" class="nu mn iq bd mo nv nw dn ms nx ny dp mw kx nz oa my lb ob oc na lf od oe nc iw bi translated">分布</h2><p id="05e6" class="pw-post-body-paragraph ko kp iq kq b kr ne ka kt ku nf kd kw kx ng kz la lb nh ld le lf ni lh li lj ij bi translated">顾名思义，<code class="fe nq nr ns nt b">tfp.distributions</code>为我们提供了分配对象。目前(2021年11月)，你可以在那里找到超过100种不同的发行版。</p><p id="93e1" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">让我们导入库，从简单的开始。我们将初始化一个均值为100、标准差为15的正态分布:</p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="of og l"/></div><p class="mi mj gj gh gi mk ml bd b be z dk translated">导入张量流概率，初始化并检验正态<strong class="ak"><em class="oh">【100，15】</em></strong><em class="oh"/>分布。</p></figure><p id="fd11" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">正如您在上面的代码输出中看到的，我们的分发对象有<code class="fe nq nr ns nt b">batch_shape</code>和<code class="fe nq nr ns nt b">event_shape</code>属性。批次大小告诉我们批次中分布对象的数量。例如，我们可能希望一批中有三个单变量高斯函数，以参数化网络中的三个独立输出层。事件形状传达不同类型的信息。您可以将事件形状视为分布的维度。让我们看一个例子:</p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="of og l"/></div><p class="mi mj gj gh gi mk ml bd b be z dk translated">初始化批量为2的正态分布。<strong class="ak">注意</strong>我们将一个两元素列表传递给构造函数的‘loc’。</p></figure><p id="6c2c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">正如你所看到的，现在我们得到了一个<code class="fe nq nr ns nt b">batch_shape</code>为2的分布。批处理和事件形状也可能具有更高维的形状，即它们的形状可能是矩阵或张量。</p><p id="ca98" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，让我们将这一批两个正态分布转换成一个<strong class="kq ja">单一二维</strong> <strong class="kq ja">分布</strong>:</p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="of og l"/></div><p class="mi mj gj gh gi mk ml bd b be z dk translated">将两个分批正态分布转化为一个2D分布。</p></figure><p id="f90c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在上面的代码中，我们将我们的<code class="fe nq nr ns nt b">normal_batched</code>包装到一个特殊的<code class="fe nq nr ns nt b">tfd.Independent</code>类的实例中。该类将由<code class="fe nq nr ns nt b">reinterpreted_batch_ndims</code>参数指定的多个批量维度转换为事件维度。<code class="fe nq nr ns nt b">normal_2d</code>的批次形状现在为空，分布的事件形状为2。</p><p id="59d8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">您可能还注意到了<code class="fe nq nr ns nt b">normal_batched.mean()</code>和<code class="fe nq nr ns nt b">normal_2d.mean()</code>返回了几乎相同的数组。尽管它们的意思不同。在第一种情况下，我们得到了<strong class="kq ja">两个独立的均值</strong>——一个用于批次中的每个分布。在第二种情况下，我们得到了<strong class="kq ja">一个具有两个分量的单一均值</strong>向量——一个分量对应于我们的<strong class="kq ja"> 2D分布</strong>的每一个维度。</p><p id="4ddb" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了更好地理解这种差异，让我们看看每个TFP发行版提供的一些基本方法，并尝试将它们应用到我们的示例中。</p><p id="b274" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">每个TFP分发对象都有三种基本方法:</p><ul class=""><li id="209e" class="lm ln iq kq b kr ks ku kv kx lo lb lp lf lq lj lr ls lt lu bi translated"><code class="fe nq nr ns nt b">.sample()</code></li><li id="b09f" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated"><code class="fe nq nr ns nt b">.prob()</code></li><li id="9e60" class="lm ln iq kq b kr lv ku lw kx lx lb ly lf lz lj lr ls lt lu bi translated"><code class="fe nq nr ns nt b">.log_prob()</code></li></ul><p id="ee84" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><code class="fe nq nr ns nt b">.sample()</code>允许你从一个分布中取样，<code class="fe nq nr ns nt b">.prob()</code>返回一个分布的密度(概率密度函数— PDF，<a class="ae ll" href="https://math.stackexchange.com/questions/1720053/how-can-a-probability-density-function-pdf-be-greater-than-1" rel="noopener ugc nofollow" target="_blank">不是概率！</a>)和<code class="fe nq nr ns nt b">.log_prob()</code>返回您输入的PDF日志。</p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="of og l"/></div><p class="mi mj gj gh gi mk ml bd b be z dk translated">从批量分布和二维分布中抽取三个样本。在这两种情况下，示例数组的维数是相同的，但是它们的含义是不同的。</p></figure><p id="ff98" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在上面的代码中，我们从<code class="fe nq nr ns nt b">normal_batched</code>采样了三个样本，从<code class="fe nq nr ns nt b">normal_2d</code>采样了三个样本。尽管两种情况下的样本数组大小相同，但它们的含义不同。</p><p id="dfd1" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">当我们评估两种分布的PDF时，我们应该更清楚地看到这一点。请想一想，使用形状样本(3，2)评估两种分布的PDF时，您期望得到什么尺寸。他们会一样吗？不一样？为什么？🤔</p><p id="fd4b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">让我们看一个非常简单的例子。我们将取两个值——100和200——并使用我们的两个分布来评估这些点的pdf。</p><figure class="mb mc md me gt mf"><div class="bz fp l di"><div class="of og l"/></div><p class="mi mj gj gh gi mk ml bd b be z dk translated">对同一样本评估“normal_batched”和“normal_2d”的PDF会产生不同的输出形状。</p></figure><p id="771b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如您所见，<code class="fe nq nr ns nt b">normal_batched.prob(sample)</code>返回了两个值，而<code class="fe nq nr ns nt b">normal_2d.prob(sample)</code>只有一个值。此外，注意<code class="fe nq nr ns nt b">normal_batched</code>两次返回相同的值！你知道为什么吗？如果是这样，请在下面的评论中分享你的答案。</p><p id="8f95" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">批处理分布返回两个值，因为事实上<strong class="kq ja">有两个独立的分布</strong>(只是包含在单个批处理元素中)。同时，我们的第二个分布是一个二维的<strong class="kq ja">单分布</strong>，我们需要两个数字来描述2D空间中的一个点。</p><p id="395d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">说到这里，我们结束了本周的节目。在下一集，我们将讨论<strong class="kq ja">随机不确定性</strong>，并了解如何使用<code class="fe nq nr ns nt b">tfd.layers</code>模块对其建模。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/7506469d273935187b6503bd3ac32071.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*r8YoBGI4gDRcEds-h_XFzw.jpeg"/></div><p class="mi mj gj gh gi mk ml bd b be z dk translated">图片由cotton bro<a class="ae ll" href="https://www.pexels.com/photo/a-white-line-on-the-asphalt-road-5319516/" rel="noopener ugc nofollow" target="_blank">https://www . pexels . com/photo/a-white-line-on-the-asphalt-road-5319516/</a></p></figure><h2 id="2316" class="nu mn iq bd mo nv nw dn ms nx ny dp mw kx nz oa my lb ob oc na lf od oe nc iw bi translated">摘要</h2><p id="a364" class="pw-post-body-paragraph ko kp iq kq b kr ne ka kt ku nf kd kw kx ng kz la lb nh ld le lf ni lh li lj ij bi translated">恭喜你走了这么远！🎉</p><p id="d48f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在本集<em class="lk">用张量流概率对神经网络中的不确定性建模</em> <strong class="kq ja"> <em class="lk"> </em> </strong>系列中，我们看到了一个示例，展示了建模不确定性如何为我们提供有关模型性能的附加信息。</p><p id="7369" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们已经用TFP工具箱中的基本而强大的工具进行了实验。我们探索了分布基础子模块，我们看到了如何使用<code class="fe nq nr ns nt b">tfd.Independent</code>将批量分布转换为多维分布。最后，我们探讨了基本的分配方法:<code class="fe nq nr ns nt b">.sample()</code>和<code class="fe nq nr ns nt b">prob()</code>。</p><p id="6f54" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在下一部分，我们将关注<strong class="kq ja">随机不确定性</strong>。我们将看到<code class="fe nq nr ns nt b">tfd.layers</code>子模块的运行，我们将了解<code class="fe nq nr ns nt b">.log_prob()</code>方法的威力。</p><p id="d738" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">感谢您的阅读，第二部分再见！</p><p id="1eac" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi">________________</p><p id="bfe0" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">❤️对获取更多这样的内容感兴趣吗？使用此链接加入:</p><div class="oi oj gp gr ok ol"><a href="https://aleksander-molak.medium.com/membership" rel="noopener follow" target="_blank"><div class="om ab fo"><div class="on ab oo cl cj op"><h2 class="bd ja gy z fp oq fr fs or fu fw iz bi translated">通过我的推荐链接加入媒体-亚历山大·莫拉克</h2><div class="os l"><h3 class="bd b gy z fp oq fr fs or fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="ot l"><p class="bd b dl z fp oq fr fs or fu fw dk translated">aleksander-molak.medium.com</p></div></div><div class="ou l"><div class="ov l ow ox oy ou oz mg ol"/></div></div></a></div><p id="a3c9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi">_______________</p></div></div>    
</body>
</html>