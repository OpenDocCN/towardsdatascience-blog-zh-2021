<html>
<head>
<title>Kernel Methods: A Simple Introduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">内核方法:简单介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/kernel-methods-a-simple-introduction-4a26dcbe4ebd?source=collection_archive---------6-----------------------#2021-10-14">https://towardsdatascience.com/kernel-methods-a-simple-introduction-4a26dcbe4ebd?source=collection_archive---------6-----------------------#2021-10-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="fb2a" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">机器学习必须知道</h2><div class=""/><div class=""><h2 id="4e42" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">核方法和径向基函数的基础</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/eea680ea9191c6bbfdac6e19cda4434d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*V2XbjEqNCaQsV1p_"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">马库斯·温克勒在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="2f74" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">介绍</h1><p id="70fb" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">偏差-方差困境支配着机器学习方法。如果一个模型太简单，这个模型将很难找到输入和输出之间的适当关系。但是，如果模型太复杂，它在训练中会表现得更好，但是在处理看不见的数据时会表现出更大的差异，此外，复杂的模型通常计算成本更高。在一个理想的世界中，我们想要一个简单的模型，它训练速度快，并且足够复杂，可以找到输入和输出之间的复杂关系。<strong class="mc jd">核方法通过将数据的输入空间映射到高维特征空间来实现这一点，在高维特征空间中可以训练简单的线性模型，从而产生高效、低偏差、低方差的模型。</strong></p><p id="e3a1" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">在这篇文章结束时，希望你能很好地理解这句话的意思以及它为什么重要。</p><h1 id="4abd" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">核心方法</h1><p id="3221" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">在机器学习领域有许多核心方法。支持向量机(SVMs)特别受欢迎，在20世纪后期甚至更受欢迎，当时它们开始胜过神经网络。如今，核方法最适用于中小型数据集，以及结果的可解释性很重要的问题。</p><p id="5635" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">核方法使用核(或基函数)将输入数据映射到不同的空间。在这种映射之后，可以在新的特征空间而不是输入空间上训练简单的模型，这可以导致模型性能的提高。</p><p id="0dd6" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">由于这是对核方法的介绍，我将把重点放在径向基函数上，这是一个非常简单但很常见的核。在以后的文章中，我将详细讨论支持向量机。</p><h1 id="a9f6" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">线性回归和径向基函数</h1><p id="99c6" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">在回归问题中，我们试图估计从X推断Y的最佳函数。如果我们在X和Y之间有非线性关系，就不能简单地用此数据拟合线性模型。然而，内核方法的目标是使用这些线性模型，并且仍然创建非线性关系。</p><p id="e8c0" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">内核方法通过将数据转换到一个更高的维度，并在这个维度上拟合一个线性模型来做到这一点。通过这样做，我们有效地拟合了原始输入空间中的高阶模型。</p><h2 id="8b7b" class="nb lj it bd lk nc nd dn lo ne nf dp ls mj ng nh lu mn ni nj lw mr nk nl ly iz bi translated">线性回归</h2><p id="38ac" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">让我们看看线性回归的解析解，然后我们就可以理解如何使用核方法来生成使用这种线性模型的非线性映射。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nm"><img src="../Images/d2297c69e9d96a8c0241017bb21eef97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fMGKzXH_cGbJrqizCGf80w.png"/></div></div></figure><p id="134c" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">最佳线性回归是使我们的模型预测和目标输出y之间的平方距离最小化的回归。平方误差如上所示。最小化该误差给出了最佳解决方案。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nn"><img src="../Images/54e7002338c319c9171189280c4e2452.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mEZ-DY5hARqnsemBqrYhkg.png"/></div></div></figure><p id="f908" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我们可以对最小二乘误差相对于我们的模型的权重进行微分，以找到产生最小误差的权重向量。结果是伪逆解。</p><p id="c437" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">为了正确理解任何线性代数公式，你必须熟悉每个变量的维数:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi no"><img src="../Images/ded78c1a6ddb9c7d74c9c4dc65cbd1e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pN1_SkHeq9H7m7y6m5Td3w.png"/></div></div></figure><p id="1083" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">输入数据<em class="np"> X </em>是<em class="np"> (Nxd) </em>维，其中<em class="np"> N </em>是数据点的数量，<em class="np"> d </em>是特征的数量。因此，逆计算将是一个<em class="np"> (dxd) </em>矩阵，并且得到的权重矩阵是一个<em class="np"> (dx1) </em>。我们的权重向量与输入数据中的要素具有相同的维数。这是有意义的，因为当我们从X推断Y时，我们取权重和输入数据之间的点积，因此输入必须与我们的权重具有相同的维数。</p><h2 id="320f" class="nb lj it bd lk nc nd dn lo ne nf dp ls mj ng nh lu mn ni nj lw mr nk nl ly iz bi translated">高维空间中的线性回归</h2><p id="74f9" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">核方法通过使用核或一组M基函数将数据矩阵X映射到新的设计矩阵U。新的设计矩阵具有更高的维数(NxM，其中M ≥ d)。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nq"><img src="../Images/1f5fd4176fa818d1b938c26e4cea6357.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WJ8yhXLRNQywb1VK3N3J_g.png"/></div></div></figure><p id="1ff0" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我们可以通过取m个基函数(ϕ)来构造一个设计矩阵u，每个基函数由它们自己的均值和标准差来参数化。上述等式中的平均值将具有<em class="np"> (dx1) </em>的维数。因此，对于输入空间中的每个数据点，我们应用M个基函数，将输入维度<em class="np"> (Nxd) </em>转换为新的设计矩阵<em class="np"> (NxM) </em>。</p><p id="dba3" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">RBF使用高斯基函数。每个基函数代表输入空间中的高斯分布。在所有高斯分布中评估每个数据点。结果是输入向量从d维到M维的映射。</p><p id="c301" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">为了选择参数化这些高斯分布的均值和标准差，可以使用k均值聚类来获得参数化基函数的均值和标准差。</p><p id="fc04" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">现在我们有了设计矩阵U，并且我们已经将输入数据映射到高维空间，我们可以在这个新的特征空间中拟合线性模型。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nr"><img src="../Images/5925a6223c16a79fa6c34367b3c25bfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LZHT1o2JwCbpC21hBEhuJQ.png"/></div></div></figure><p id="67d0" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">通过表达来自特征空间的估计和我们的目标y之间的最小二乘误差，并对我们的新权重向量<em class="np"> l </em>进行微分，我们发现最优解与输入数据中的线性回归的最优解具有相同的形式。</p><p id="a3c3" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这里需要注意的关键点是，我们的权重向量<em class="np"> (l) </em>现在是一个<em class="np"> Mx1 </em>向量，在原来的输入空间中，权重向量是一个<em class="np"> dx1 </em>向量(还记得M &gt; d)。</p><h1 id="a933" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">合成数据示例</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/a665d1d7b13ffdc3276c643c83875dc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*55R-kAprR0ubbjoZErezlQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="f9df" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这是合成的非线性数据。我有10000个数据点，我的Y坐标是一维的。这意味着我的数据矩阵X的维数为(10，000×1)。我们可以通过使用上面看到的伪逆解决方案计算最佳权重，尝试将线性模型拟合到该数据。显然，正如你在上面看到的，它的表现并不好。</p><p id="c2cc" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">通过在高维特征空间中拟合该相同的线性模型，我们获得了数据中真实关系的更好的近似。</p><p id="2575" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">首先，我对每个数据点应用200个基函数。我在输入空间中取200个高斯分布，并在所有基本函数上评估每个数据点。我的新设计矩阵现在是(10，000x200)维。然后，我使用相同的伪逆解来获得这个新特征空间中的最优权重。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/7e7a36d7782dd899ff53e1336fba3f96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*hIR5RJ1DeejozgFiSmnJsQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="fe5c" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">如您所见，RBF模型估计的关系是非线性的，并且与数据吻合得很好。<strong class="mc jd">记住，这个新模型仍然是一个线性回归变量！</strong>但是因为我们在新的特征空间中拟合它，所以我们在原始输入空间中间接地拟合一个复杂的非线性模型。</p><h1 id="0214" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated"><strong class="ak">结论</strong></h1><p id="3b5b" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">核方法使用核(或一组基函数)将我们的低维输入空间映射到高维特征空间。当在新的特征空间中训练线性模型(类型为<em class="np"> ax + b </em>的线性模型)时，我们本质上是在原始输入空间中训练高阶模型(例如类型为<em class="np"> ax +bx +c </em>)。通过这样做，您保留了简单模型的所有优势(如训练速度、有分析解决方案、较低的方差)，但也获得了更复杂模型的优势(更好的映射、更低的偏差)。本质上，这就是内核方法如此强大的原因！</p><h2 id="67da" class="nb lj it bd lk nc nd dn lo ne nf dp ls mj ng nh lu mn ni nj lw mr nk nl ly iz bi translated">支持我</h2><p id="eb71" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">希望这对你有所帮助，如果你喜欢，你可以跟我来！T11】</p><p id="bb19" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">你也可以使用我的推荐链接成为<strong class="mc jd">中级会员</strong>，访问我所有的文章等等:<a class="ae lh" href="https://diegounzuetaruedas.medium.com/membership" rel="noopener">https://diegounzuetaruedas.medium.com/membership</a></p><h1 id="5d05" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">你可能喜欢的其他文章</h1><div class="nt nu gp gr nv nw"><a rel="noopener follow" target="_blank" href="/support-vector-machines-svms-important-derivations-4f50d1e3d4d2"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd jd gy z fp ob fr fs oc fu fw jc bi translated">支持向量机:重要的推导</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">SVM理论的全面阐释和形象化</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">towardsdatascience.com</p></div></div><div class="of l"><div class="og l oh oi oj of ok lb nw"/></div></div></a></div><div class="nt nu gp gr nv nw"><a rel="noopener follow" target="_blank" href="/kalman-filtering-a-simple-introduction-df9a84307add"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd jd gy z fp ob fr fs oc fu fw jc bi translated">卡尔曼滤波:简单介绍</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">高斯噪声下线性系统的最优在线学习算法</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">towardsdatascience.com</p></div></div><div class="of l"><div class="ol l oh oi oj of ok lb nw"/></div></div></a></div></div></div>    
</body>
</html>