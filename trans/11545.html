<html>
<head>
<title>Complete Guide to Regressional Analysis Using Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 Python 进行回归分析的完整指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/complete-guide-to-regressional-analysis-using-python-bbe76b3e451f?source=collection_archive---------2-----------------------#2021-11-15">https://towardsdatascience.com/complete-guide-to-regressional-analysis-using-python-bbe76b3e451f?source=collection_archive---------2-----------------------#2021-11-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d3b5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">最小二乘法(MLR)和加权最小二乘法；拉索(L1)、山脊(L2)和弹性网正规化；核和支持向量机回归</h2></div></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><p id="48bf" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">大家好，欢迎来到这篇关于 Python 回归分析的全面深入的长篇概述！在这次深入探讨中，我们将涉及最小二乘法、加权最小二乘法；套索、脊和弹性网正则化；最后总结一下内核和支持向量机回归！虽然我想涵盖一些高级的回归机器学习模型，如随机森林和神经网络，但它们的复杂性需要它们自己的未来帖子！在这篇文章中，我将从两个方面来探讨回归分析:理论上的和 T2 的应用。从理论方面来说，我将介绍基本级别的算法并导出它们的基本解决方案，而在应用方面，我将使用 Python 中的<em class="ll"> sklearn </em>将这些模型实际应用于现实生活的数据集！</p></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h1 id="8e45" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated"><strong class="ak">目录</strong></h1><ul class=""><li id="965f" class="me mf it kr b ks mg kv mh ky mi lc mj lg mk lk ml mm mn mo bi translated"><strong class="kr iu">什么是回归？</strong></li><li id="5c3a" class="me mf it kr b ks mp kv mq ky mr lc ms lg mt lk ml mm mn mo bi translated"><strong class="kr iu">我们的数据集—医疗费用</strong></li><li id="f89c" class="me mf it kr b ks mp kv mq ky mr lc ms lg mt lk ml mm mn mo bi translated"><strong class="kr iu">如何衡量误差？</strong></li><li id="ad6f" class="me mf it kr b ks mp kv mq ky mr lc ms lg mt lk ml mm mn mo bi translated"><strong class="kr iu">最小二乘解(MLR) </strong></li><li id="64b8" class="me mf it kr b ks mp kv mq ky mr lc ms lg mt lk ml mm mn mo bi translated"><strong class="kr iu">模型的解释</strong></li><li id="ee2a" class="me mf it kr b ks mp kv mq ky mr lc ms lg mt lk ml mm mn mo bi translated"><strong class="kr iu">加权最小二乘法(WLR) </strong></li><li id="b1c2" class="me mf it kr b ks mp kv mq ky mr lc ms lg mt lk ml mm mn mo bi translated"><strong class="kr iu">如何处理过度拟合——正规化</strong></li><li id="a51d" class="me mf it kr b ks mp kv mq ky mr lc ms lg mt lk ml mm mn mo bi translated"><strong class="kr iu">如何处理欠拟合—核回归</strong></li><li id="1951" class="me mf it kr b ks mp kv mq ky mr lc ms lg mt lk ml mm mn mo bi translated"><strong class="kr iu">支持向量机</strong></li><li id="b92b" class="me mf it kr b ks mp kv mq ky mr lc ms lg mt lk ml mm mn mo bi translated"><strong class="kr iu">结论</strong></li></ul></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h1 id="55ca" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated"><strong class="ak">什么是回归？</strong></h1><p id="23cb" class="pw-post-body-paragraph kp kq it kr b ks mg ju ku kv mh jx kx ky mu la lb lc mv le lf lg mw li lj lk im bi translated">在机器学习领域，任务通常分为四大类:<em class="ll">监督学习</em>、<em class="ll">非监督学习</em>、<em class="ll">半监督学习</em>、<em class="ll">学习</em>、<em class="ll">强化学习</em>。回归属于<em class="ll">监督学习</em>的范畴，其目标是学习或模拟一个将一组输入映射到一组输出的函数。在<em class="ll">监督学习</em>中，我们的一组输出通常在统计学中被称为<em class="ll">因变量</em>或者在机器学习社区中被称为<em class="ll">目标</em>变量。这个目标变量可以是离散的，通常称为分类，也可以是连续的，通常称为回归。通过这种方式，回归只是在给定一组输入的情况下，试图预测一个连续的目标变量。</p></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h1 id="be3f" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">我们的数据集—医疗成本</h1><p id="19cb" class="pw-post-body-paragraph kp kq it kr b ks mg ju ku kv mh jx kx ky mu la lb lc mv le lf lg mw li lj lk im bi translated">为了给回归分析的理论方面一些应用，我们将把我们的模型应用于一个真实的数据集:<a class="ae mx" href="https://www.kaggle.com/mirichoi0218/insurance" rel="noopener ugc nofollow" target="_blank">医疗费用个人</a>。这个数据集来源于 Brett Lantz 的教科书:M <em class="ll"> achine Learning with R </em>，其中他的所有与教科书相关的数据集在以下许可下是免版税的:<em class="ll">数据库内容许可(DbCL) v1.0 </em>。</p><p id="e3ea" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">该数据集包含不同个人的 1338 份医疗记录，记录了一些指标:年龄、性别、bmi、孩子数量、是否吸烟以及他们居住的地区。目标是使用这些特征来预测个人的“费用”和医疗成本。</p><p id="3e3d" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">因为这已经是一篇很长的文章了，所以我不会详细讨论探索性的分析和预处理步骤；但是，下面列出了它们:</p><figure class="my mz na nb gt nc"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="176c" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">以下是有关如何加载数据集、将其拆分为特征和目标变量，以及将其划分为测试和训练集的代码:</p><figure class="my mz na nb gt nc"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="cdaf" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">我们将数据集分为训练和测试数据集的原因是，我们在训练集上训练我们的模型，并根据它对以前没有见过的新数据(测试集)的概括程度来评估它。此外，我还必须对我们的目标变量执行<em class="ll">对数转换</em>，因为它遵循严重偏斜的分布。在最小二乘的一些原理假设下，Y 需要服从正态分布，后面会解释。目前，通过 ra <em class="ll">对数</em>或<em class="ll"> BoxCox </em>变换，可以使严重偏斜的正态分布遵循正态分布。</p></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h1 id="f66b" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">如何衡量误差？</h1><p id="ae64" class="pw-post-body-paragraph kp kq it kr b ks mg ju ku kv mh jx kx ky mu la lb lc mv le lf lg mw li lj lk im bi translated">在机器学习社区中，对于衡量误差的最佳方式有很多研究和争论。对于回归来说，大多数误差测量来自于线性代数中的一个概念，称为<em class="ll">范数</em>。范数是允许人们测量张量/矩阵/向量有多大的度量。可以看出，如果这些范数衡量张量有多大，那么机器学习模型的目标就是最小化我们的预期输出和预测输出之间的范数差异！在数学格式中，x 的范数通常定义为:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/40badbc99d56e7a5accfc48be45cceff.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/format:webp/1*b2PjQO6gQd2sX-IN2afwAA.png"/></div></figure><p id="f0c7" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">其中 p 是改变测量值的参数。以下是一些最常见的 p 规范:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/92c362a6bcddfb2f88783fbe484103d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*lVxQZt3yrqgCcQL8YUF3bA.png"/></div></figure><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/33e8be850729ed9abf13efb39c7fed9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*wpUwbbyJiNEPu_rzGrd6fQ.png"/></div><p class="nk nl gj gh gi nm nn bd b be z dk translated">l 无穷范数</p></figure><p id="6277" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">我们如何使用这些标准来帮助我们测量误差？我们可以用它们来衡量模型预测值 f(x)和实际目标变量 y 之间的差异。因此，我们可以用预测值和实际值之间的差异来衡量误差，这可以用一个标准的单个数值来量化:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi no"><img src="../Images/5cc920b88bd39a484d307d1c8aa7bf21.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/format:webp/1*saYLTMT7tfWQf_xARRe-RQ.png"/></div><p class="nk nl gj gh gi nm nn bd b be z dk translated">我们如何定义错误</p></figure><p id="90b2" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">机器学习中最常见的两种误差测量是<em class="ll">均方误差</em> (MSE)和<em class="ll">平均绝对误差</em> (MAE):</p><div class="my mz na nb gt ab cb"><figure class="np nc nq nr ns nt nu paragraph-image"><img src="../Images/36bfd822863cb503573f982ab9465789.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*xn-wEZMkTONL5XrPyR1xkA.png"/></figure><figure class="np nc nv nr ns nt nu paragraph-image"><img src="../Images/e2e74515555d930aae11424e6b84ae54.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*Sq6OjMrwNMbVf7PPX-qEvg.png"/></figure></div><p id="f16d" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">在我们今天要研究的机器学习模型中，由于误差平方的凸性，MSE 被选择作为量化误差的度量，通俗地说，由于绝对运算的导数未定义，数值方法更容易最小化平方数而不是绝对值。</p><p id="de49" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">上述误差测量只存在一个问题，它们没有解释模型相对于目标值的表现如何，只解释了误差的大小。误差大是否意味着模型差？误差小就代表模型好吗？如果目标变量的变化很小，好的模型可以具有非常大的 MSE，而差的模型可以具有小的 MSE。例如，假设我们有两条不同的线用于两个不同的数据集。左侧数据集的预测具有比右侧数据集更低的 MSE，这是否意味着左侧的模型更好？我猜你会说右边的预测线比左边的好，尽管右边的数据集在 Y 变量中有更大的变化，所以有更高的 MSE。</p><div class="my mz na nb gt ab cb"><figure class="np nc nw nr ns nt nu paragraph-image"><img src="../Images/41bd0b7a0fc13c23c2a3fcfb35c8c325.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*VFKjY-lcnVCcWKGyziORRw.png"/></figure><figure class="np nc nx nr ns nt nu paragraph-image"><img src="../Images/f3c37717237aedabd3b03279720372f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*GhWJ84cFAiaw4020yE5zPQ.png"/></figure></div><p id="6c34" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">仅使用 MSE 或 MAE 的问题是，它没有考虑目标变量的变化。如果目标变量有很多方差，如右边的数据集，那么 MSE 自然会更高。用于考虑目标变量变化的一个流行指标被称为<em class="ll">决定系数，</em>通常被称为<em class="ll"> R 的平方:</em></p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/7761cb9bf22393fe1f8c79ee817cd97f.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*WwuhYtb89KOicLNo7Zpjgw.png"/></div></figure><p id="2ce1" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">从上面我们可以看到，R 的平方与<em class="ll">残差平方和</em> (RSS)与<em class="ll">总平方和</em> (TSS)的比值成正比。r 的平方范围从(-无穷大，1)。其中解释是被解释的目标变量变化的百分比。例如，假设一个模型的 R 平方值为<em class="ll"> 0.88 </em>，那么这个模型解释了目标变量大约 88%的可变性。因此，更大的 R 平方值是更可取的，因为模型将解释更大百分比的目标变量。然而，如果模型的 RSS 大于 TSS，那么 R 平方度量将是负的，这意味着模型的方差超过了目标的方差，也就是说模型很差。</p></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h1 id="7995" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">最小二乘解</h1><p id="0fb5" class="pw-post-body-paragraph kp kq it kr b ks mg ju ku kv mh jx kx ky mu la lb lc mv le lf lg mw li lj lk im bi translated">现在我们已经定义了我们的误差度量，是时候介绍我们的第一个经典机器学习模型了，<em class="ll">最小二乘</em>！与将要讨论的大多数模型一样，最小二乘法基于这样的假设，即<em class="ll">因变量/目标变量</em>是<em class="ll">特征变量</em>的线性组合(假设 k 个特征):</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/44a071fced37ce73a404365864261d28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1016/format:webp/1*iWSZQEW8tRmpLa36ffXwqg.png"/></div></figure><p id="0ddf" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">系数的目标是作为相应输入变量的斜率，截距是作为输入变量为零时目标变量开始的点。根据上述强有力的假设，我们的目标是找到对系数的准确预测:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/ff71979195d624fdfbfca71de63ff965.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*2dGG0c5pJ5-xBO4IccIIiQ.png"/></div></figure><p id="dd85" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">因为我们的β估计不精确，所以我们会有一个误差项，ε。这可以用矩阵格式写成如下:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/60dd55b68df19c688bc3834803a163be.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*YjXQHuYDL9A7_uM8Nxq5jw.png"/></div><p class="nk nl gj gh gi nm nn bd b be z dk translated">其中<em class="oc"> n </em>是实例/记录的数量</p></figure><p id="9681" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">为了导出β的估计系数，有两个主要的推导过程。我两样都给。首先，使用简单的矩阵操作:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="oe of di og bf oh"><div class="gh gi od"><img src="../Images/8ede421618f627edd7499950ccb42128.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1qljONCB7Yv9Ak43lfiUcQ.png"/></div></div></figure><p id="77c7" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">第二种推导是最常见的，通过使用梯度试图最小化差异的<em class="ll">期望</em>。在统计学中，期望值通常被定义为随机变量的加权平均值:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/c3295b4b3ae923fcc1c0a7e4de554a11.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/format:webp/1*OroIJeDYZxrQi4N6C5stZg.png"/></div></figure><p id="13fc" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">可以转换成矩阵格式:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/2bae5527d3bedb28d21c2027d90d481b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*zC8lI9TVPQU2kg0rm3jusQ.png"/></div></figure><p id="5e38" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">然后，我们可以求出 J 的梯度，设它等于零，求β的解析解！</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/ede98db78bcfcbe09a68ed12823adc5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*4trXPD1XpsF1VWx2QhSogQ.png"/></div></figure><p id="448c" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">正如我们所看到的，这两种方法导致了相同的解决方案！事实上，他们是平等的！然而，如果你一直高度关注，我们已经做了三个大的假设:Y 是正态分布的；X^T*X 是可逆的；ε的期望值为零，方差不变。虽然这些假设在实践中有时会被打破，但最小二乘模型仍然表现良好！我希望现在你能理解为什么我们必须对我们的目标变量进行对数变换来达到正态性！</p><p id="9f1d" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">标准最小二乘法的时间复杂度是 O(k ),因为求矩阵的逆的时间复杂度是 O(n ),但是我们的矩阵结果，X^T*X 实际上是 k 乘 k，其中 k 是特征/列的数量。</p><p id="4fba" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">现在我们已经讨论了最小二乘法的理论背景，让我们把它应用到我们的问题中！我们可以使用 sklearn 库中的 LinearRegression 对象来实现我们的最小二乘解！</p><figure class="my mz na nb gt nc"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="33dd" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">为了评估我们的模型，我们可以查看测试和训练数据集的 MSE 和 R 值:</p><div class="my mz na nb gt ab cb"><figure class="np nc ol nr ns nt nu paragraph-image"><div role="button" tabindex="0" class="oe of di og bf oh"><img src="../Images/5bfbf016904ade82b6632c093481c5f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:998/format:webp/1*Awv8lstmsX52dNBjhcDsQw.png"/></div></figure><figure class="np nc om nr ns nt nu paragraph-image"><div role="button" tabindex="0" class="oe of di og bf oh"><img src="../Images/5c87604782d51269390cf2287bb33af3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*pQZDecy9sMFeKcN4gneENQ.png"/></div></figure></div><p id="3c5c" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">从上面左侧的柱状图中我们可以看出，测试集和训练集的 MSE 都非常低，只有 0.19 左右；然而，如果你记得的话，目标变量经历了<em class="ll">对数变换</em>，这意味着 0.016 的 MSE 对于目标变量的规模来说并不算小。因此，更好的测量方法是评估 R 值，我们可以从右侧的柱状图中看出，R 值还不错。我们的模型只解释了训练集的目标变量的大约 79%的可变性和测试集的大约 76%的可变性。尽管其结果在某些情况下是合理的，但对于这个简单的数据集，当模型在预测目标变量方面表现不佳时，这被称为<em class="ll">欠拟合</em>。我们将在后面介绍如何解决这个问题的一些方法。</p></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h1 id="de2a" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">模型的解释</h1><p id="bce7" class="pw-post-body-paragraph kp kq it kr b ks mg ju ku kv mh jx kx ky mu la lb lc mv le lf lg mw li lj lk im bi translated">与其他回归模型相比，线性回归的一大优势是它的简单性和解释能力。可以评估每个 beta 系数，以解释模型如何实现其预测。像这样的机器学习模型被称为<em class="ll">白盒方法</em>，这意味着模型如何实现其输出是显而易见的。另一方面，未知如何实现预测的公式过程的机器学习模型被称为<em class="ll">黑盒方法</em>。</p><p id="a1d7" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">例如，下面是我们的贝塔系数值:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="oe of di og bf oh"><div class="gh gi on"><img src="../Images/785d0278da06b9a16a46523fa95412fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*34P2ciOTeXJHHibFMVGwxA.png"/></div></div></figure><p id="1769" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">不幸的是，因为我们使用对数来缩放目标变量，所以系数值是在解释目标的对数方面。为了解决这个问题，我们可以通过对数的倒数，即指数运算来重新调整系数。这是我们的指数贝塔系数:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="oe of di og bf oh"><div class="gh gi oo"><img src="../Images/c34481fe11ff5f9d560e98a7c099d2e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HpzvGwdvv0eG403e00inYw.png"/></div></div></figure><p id="299b" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">指数贝塔系数的解释是目标变量的百分比变化。例如，当一个人吸烟时，他的医疗费用增加了<strong class="kr iu">116.8%((</strong>2.168–1)* 100)<strong class="kr iu">。</strong>另一方面，如果这个人不吸烟，那么他们的医疗费用将减少<strong class="kr iu">63.9%</strong>((0.461–1)* 100)。实质上，任何大于 1 的β系数都会增加医疗费用的百分比，而任何小于 1 的β系数都会降低医疗费用的百分比。此外，我们可以明确地看到，最大的指数化β系数属于吸烟者，这意味着在其他变量中，该变量，即该人是否吸烟，对医疗费用的影响最大。</p><p id="ac5c" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">假设我们没有进行对数变换，那么我们如何解释贝塔系数呢？</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="oe of di og bf oh"><div class="gh gi op"><img src="../Images/bf389e09ed782c427e314c269fcfe5e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rDeHOSkhU3s8o2AMwKOPDg.png"/></div></div></figure><p id="f8da" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">这种解释是根据目标变量的单位尺度进行的。因为我们目标变量是以美元衡量的，我们可以看到，如果一个人是吸烟者，那么他的医疗费用将增加 11907 美元。如果他们不吸烟，则减少 11，907 美元。就年龄而言，我们可以看到，随着年龄的增长，他们的医疗费用每年会增加 264 美元。如果我们来看性别，系数是一样的，这意味着不管这个人是男是女，医疗费用都不会上升。希望你能看到解释最小二乘回归系数的力量。</p></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h1 id="9689" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated"><strong class="ak">加权最小二乘解</strong></h1><p id="e572" class="pw-post-body-paragraph kp kq it kr b ks mg ju ku kv mh jx kx ky mu la lb lc mv le lf lg mw li lj lk im bi translated">最小二乘法的一个主要假设是误差ε正态分布，方差恒定:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/4d86d3e3c3ea9042d6c302900b2c98be.png" data-original-src="https://miro.medium.com/v2/resize:fit:298/format:webp/1*TdXoxyokfLhWItjpfgugtQ.png"/></div></figure><p id="c051" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">人们可以通过绘制残差 f(x)-y 与实际值的关系来检验这一假设，通常称为<em class="ll">残差图</em>。这是我们以前的模型在训练样本上的残差图:</p><figure class="my mz na nb gt nc"><div class="bz fp l di"><div class="nd ne l"/></div></figure><div class="my mz na nb gt ab cb"><figure class="np nc or nr ns nt nu paragraph-image"><div role="button" tabindex="0" class="oe of di og bf oh"><img src="../Images/f53c90d441d86834c4ae986c505f5351.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*yP8OulKEBavWukcnJLixCw.png"/></div></figure><figure class="np nc os nr ns nt nu paragraph-image"><div role="button" tabindex="0" class="oe of di og bf oh"><img src="../Images/3d7bbf80fc15c4ef08b13989ce254cbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*CyFVc-xtxW6bU6IWzM0N3A.png"/></div></figure></div><p id="b484" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">正如我们从上面可以看到的，我们的残差的方差没有零均值，也没有恒定方差，因为它是高度非线性的。此外，我们可以看到，随着目标变量接近其最大值，残差平方呈现出略微上升的趋势。方差非常数的残差称为<em class="ll">异方差。</em>对抗异方差的一种方法是通过<em class="ll">加权最小二乘法</em>。加权最小二乘法类似于标准最小二乘法；但是，每个观察值都有其独特的权重。这样，权重较大的观测值比权重较小的观测值更容易被模型拟合。为了举例说明加权的威力，下面我们有两条预测线，一条未加权，一条未加权。正如我们从加权预测中看到的，权重较高的实例将具有更好的拟合，因为与权重较低的实例相比，模型将更倾向于固定这些点的预测线。</p><div class="my mz na nb gt ab cb"><figure class="np nc ot nr ns nt nu paragraph-image"><img src="../Images/8dd7a363482850205448560b8e95f2d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*Q9NuHYh2PGIT55JPb0Cfmw.png"/></figure><figure class="np nc ou nr ns nt nu paragraph-image"><img src="../Images/2ca1ba9f3fee835e61cbe6569a929764.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*uZlOO72x9IBt5EFJUx_3cA.png"/></figure></div><p id="c6a5" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">现在是推导加权最小二乘解的时候了。首先，我们希望最小化加权残差的期望值:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/3530e1ade9e00d65e3357f324508bb30.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*TF5S8V2buIpQs4S9AWshqA.png"/></div></figure><p id="ed42" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">这可以转换成矩阵格式:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/8e9968ddc35cd92f7c048aeb31c8c864.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*yg09ZNxu_h0sL76RQNcw_Q.png"/></div></figure><p id="900f" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">现在可以找到偏导数，并将其设置为零，以找到解析解:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/db00d7e34f7fa11112a8e7c4fc84af34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*ytaH-RgtlEbKNLpnhqRdsg.png"/></div></figure><p id="c252" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">正如我们从上面看到的，该解决方案与线性回归的解决方案非常相似，除了使用对角矩阵 W，包含每个实例的权重。</p><p id="db78" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">WLS 的主要功能之一是能够对不同的实例进行加权，以在模型中给出偏好，要么创建<em class="ll">同方差</em>，恒定方差，要么对某些记录进行更好的建模。然而，WLS 的一个主要缺点是如何确定权重。在我们的问题中，我们希望将残差固定为具有恒定方差。下面我描述了实践中发现的四种主要类型的残差方差。左上角展示了理想情况，其中方差是常数，平均值为零。右上角展示了残差方差如何随 y 增长，揭示了一个“<em class="ll">扩音器</em>型分布。左下方描绘了非线性残差，表明我们的模型缺乏创建关联的复杂性。最后，右下角展示了一个<em class="ll">二项式</em>残差方差。WLS 通常仅在发现二项式或扩音器型残差图时使用，因为非线性残差只能通过添加非线性特征来固定。</p><div class="my mz na nb gt ab cb"><figure class="np nc oy nr ns nt nu paragraph-image"><div role="button" tabindex="0" class="oe of di og bf oh"><img src="../Images/026816d018ec34a6a2c121dd9fefc2c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*4UfVq1XIFQpRmpN322xahQ.png"/></div></figure><figure class="np nc oz nr ns nt nu paragraph-image"><div role="button" tabindex="0" class="oe of di og bf oh"><img src="../Images/9cbc2f8564d49e0dfd581a8d505a0ef0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*lJYcKOFR88JFo76QPSdvPQ.png"/></div></figure></div><div class="ab cb"><figure class="np nc pa nr ns nt nu paragraph-image"><div role="button" tabindex="0" class="oe of di og bf oh"><img src="../Images/2c2f76377b85a1be0d02b2dd05053462.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*jkwegbRDZKqjnT8w9yWE6A.png"/></div></figure><figure class="np nc pb nr ns nt nu paragraph-image"><div role="button" tabindex="0" class="oe of di og bf oh"><img src="../Images/ad19a11478067b64b545307b4bd6b182.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*lK2Ho8Lu_gpUXFR5bh3GPw.png"/></div></figure></div><p id="1d81" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">二项式残差和扩音器残差的常见解决方案是使权重等于残差的平方:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/ef6fe2cfe9433af9be57c5971f7ee902.png" data-original-src="https://miro.medium.com/v2/resize:fit:390/format:webp/1*AhxHEQ8fOkPK3Xg5OkRhvQ.png"/></div></figure><p id="2daa" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">正如我们所看到的，这在直觉上是有意义的，我们根据实例的误差大小来加权实例。</p><p id="660e" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">在 sklearn 中，这很简单，只需创建另一个模型并添加额外的 weight_sample 参数:</p><figure class="my mz na nb gt nc"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="3e9f" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">如果您要测试上面的这些权重，残差图将看起来相似，并且具有相似的 R 和 MSE 分数，这是因为我们的残差方差是高度非线性的:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/f7c2cda854f04c61e29c290fb85f1e45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*7o0uxj_2CtGEo2T_bC0xew.png"/></div></figure><p id="c233" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">对于这种情况，我们只有一个解决方案，尝试添加非线性项以解释方差。我们将用核回归来讨论它，但首先我们需要讨论正则化。</p></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h1 id="6f96" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated"><strong class="ak">如何处理过拟合—正则化</strong></h1><p id="3cac" class="pw-post-body-paragraph kp kq it kr b ks mg ju ku kv mh jx kx ky mu la lb lc mv le lf lg mw li lj lk im bi translated">假设我们在一个给定的数据集上训练了一个线性回归模型，在它的应用和部署过程中，我们发现它的表现极差，尽管在训练数据上有很好的 MSE 和 R 分数；这被称为<em class="ll">过拟合</em>——当测试数据集的指标比训练数据集差得多时。举个例子:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/ed74762dd0b8d0a190dcbfac3fb51a79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/0*LGfKAy87npsMa6Wn.png"/></div></figure><p id="90cf" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">从上面我们可以看到，我们有一个点的线性趋势；然而，如果我们要拟合 10 次多项式，我们可以在我们的训练数据集上将 MSE 和 R 人工最小化为零。尽管如此，我们可以直观地看到，当看到新数据时，模型将很难概括。<em class="ll">正则化</em>通过向损失函数添加<em class="ll">惩罚项</em>来工作，该惩罚项将惩罚模型的参数；在线性回归的例子中，β系数。</p><p id="def4" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">谈到线性回归，有两种主要的正则化类型:<em class="ll">岭</em>和<em class="ll">套索</em>。首先，让我们从岭回归开始，通常称为 L2 正则化，因为它的惩罚项平方β系数以获得幅度。岭回归背后的思想是惩罚大的贝塔系数。岭回归试图最小化的损失函数如下:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/b29b954c948834144ecc8f76d7da1c44.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/format:webp/1*tDS_Vm42LvgYV8a11rm7JQ.png"/></div></figure><p id="4057" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">从上面我们可以看到，损失函数和之前完全一样，只是现在增加了红色的惩罚项。参数 lambda 缩放惩罚。例如，如果λ= 0，那么在最小二乘法中，函数与之前相同；然而，随着λ变大，该模型将导致欠拟合，因为它将β系数的大小罚为零。让我们看下面一个简单的例子:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/05e0e53b12e95b58f1aff2466f2c91bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1150/format:webp/1*NVPjkuuZGHCM7RHSvof9fg.png"/></div></figure><p id="d65a" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">正如我们从上面看到的，当使用 20 次多项式模型来近似这些点并且 lambda=0 时，我们没有惩罚，并且在蓝线中表现出极端的过度拟合。当我们开始将 lambda 增加到 0.5 时，用橙色线表示，我们开始真正地模拟底层分布；然而，我们可以看到，当紫线中的 lambda=100 时，我们的模型开始变成一条直线，导致欠拟合，因为惩罚项迫使系数为零。在实践中选择 lambda 值要么在<em class="ll">验证</em>集<em class="ll">上执行，要么通过<em class="ll">交叉验证</em>执行。通过这种方式，我们在具有不同 lambda 值的训练数据集上重新训练我们的模型，并且选择在我们的验证集上表现最好的模型作为测试数据集的最终模型。</em></p><p id="a27e" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">从数学上讲，我们的损失函数可以转化为矩阵形式:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/cd012ad378426dc39fb2ed2c2a582e8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*7L8r6nHbMYAdBO5Iau-40g.png"/></div></figure><p id="d41e" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">其中，β可以像前面一样通过找到梯度并将其设置为零来求解:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/a962ae5bfc368551ae88f7187d471cc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*plI96LcOvAxQBY7t_ujzJA.png"/></div></figure><p id="5f13" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">现在我们已经讨论了岭回归背后的数学理论，让我们将它应用到我们的数据集。在实践中，为了获得良好的泛化误差，人们会希望在验证集而不是测试集上调整 lambda 值；但是，为了节省空间，我将在测试集上进行:</p><figure class="my mz na nb gt nc"><div class="bz fp l di"><div class="nd ne l"/></div></figure><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/9278072a7df6e95bd50543f84673e199.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*ROi88m6Czw_VULfmvOE7gg.png"/></div></figure><p id="d9fd" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">正如我们从上面看到的，随着我们增加我们的 lambda 值，我们在训练和测试集上的误差急剧增加；此外，测试集的最小误差似乎在λ= 0 左右。如果你没记错的话，我们的数据集不是因为<em class="ll">过拟合</em>而失败，而是因为<em class="ll">欠拟合</em>！所以用正则化是没有意义的，这也是为什么我们的测试误差越来越差而不是越来越好！我只是想展示如果你的模型显示过度拟合，如何使用岭回归！</p><p id="2c89" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">下一个要介绍的正则化方法是<em class="ll">套索</em>，它通常被称为<em class="ll"> L1 正则化</em>，因为它的罚项是建立在β系数的绝对值上的:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/910a0c0ad690bffc176313ef03da0000.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*Lb1avpnwv_B7jVX5eqCGbg.png"/></div></figure><p id="22de" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">请注意，岭正则化和套索正则化之间的唯一区别是，岭正则化是β系数的平方，而套索正则化是取绝对值。两者之间的主要区别在于，Ridge 会惩罚 beta 系数的大小，而 Lasso 会将某些 beta 系数值驱动为零，从而导致<em class="ll">特征选择</em>。</p><p id="7429" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">这些类型的惩罚条款通常可以被重写为约束问题:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/08d7c66d230107b550125f21c12ffdb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/format:webp/1*cZcZIbtP-M-nov_M5OHYuA.png"/></div></figure><p id="ea7b" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">因为岭回归平方贝塔系数，绘制约束将导致一个圆；而套索会导致正方形。如果我们要绘制两个β系数(下图中称为 w1 和 w2)值的对比图，我们可能会得到以下结果:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="oe of di og bf oh"><div class="gh gi pm"><img src="../Images/51ee99879280b53017d1705ac5ece4af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*kgpx8dGugtJwNJqk.png"/></div></div><p class="nk nl gj gh gi nm nn bd b be z dk translated">由尼科瓜罗—自己的作品，CC 由 4.0，<a class="ae mx" href="https://commons.wikimedia.org/w/index.php?curid=58258966" rel="noopener ugc nofollow" target="_blank">https://commons.wikimedia.org/w/index.php?curid=58258966</a></p></figure><p id="1391" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">红线表示两个系数值可以取的值范围，随着 w1 的系数值增加，w2 的值开始减小。当我们绘制 L1 范数约束时:| w1 |+| w2 |≤λ，我们可以看到它用虚线方块表示。这个方框与红线相交的地方就是系数的选择值，我们可以看到，这会导致 w1 的值为零。另一方面，当我们画出我们的 L2 范数约束:w1+w2≤λ时，我们得到一个圆，如虚线圆所示。这个圆与红线相交的地方就是约束的选择值，我们可以看到 w1 和 w2 都是小的非零值。</p><p id="d487" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">不幸的是，由于绝对值运算的梯度未定义，使用矩阵计算很难找到 Lasso 正则化中β的解析解，因此经常使用类似<em class="ll">坐标下降</em>的数值方法。由于这些算法的复杂性质，我不会详述数学。在 python 中，套索回归可以按如下方式执行:</p><figure class="my mz na nb gt nc"><div class="bz fp l di"><div class="nd ne l"/></div></figure><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/45426a215cde28c606008a1ab90b4358.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*ppa23PIKSwfne7duReSqSw.png"/></div></figure><p id="a8d3" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">正如我们从上面可以看到的，随着我们增加我们的λ值，我们在训练和测试集上的误差急剧增加，最终在λ= 15 附近收敛。误差收敛的原因是因为我们的 lambda 值对于模型来说太大了，它将所有的 beta 系数逼为零。如果你没记错，我们的数据集不是因为<em class="ll">过拟合</em>而失败，而是因为<em class="ll">欠拟合</em>！所以用正则化是没有意义的，这也是为什么我们的测试误差越来越差而不是越来越好！我只是想展示如果你的模型过度拟合，你如何使用套索回归！</p><p id="068c" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">我要介绍的最后一个正则化技术是弹性网，它可以协调脊和套索，因为脊惩罚大的系数，而套索驱使系数为零。弹性网背后的想法是创建一个惩罚，既创建特征选择又最小化权重的大小。弹性网有许多不同的版本，下面是最常见的两种:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi po"><img src="../Images/14ec4e964247a584a908c52eb13fad07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*FsrOl3C4VrEPKp3Ec9QdMQ.png"/></div></figure><p id="7dea" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">可以看到，惩罚项是脊和套索的组合，每个都有自己的 lambda 值来控制每个惩罚项对模型的影响程度。</p></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h1 id="a875" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated"><strong class="ak">如何处理欠拟合—核回归</strong></h1><p id="fcaf" class="pw-post-body-paragraph kp kq it kr b ks mg ju ku kv mh jx kx ky mu la lb lc mv le lf lg mw li lj lk im bi translated">我们已经讨论了当一个模型开始过拟合时该怎么办，但是当一个模型过拟合时该怎么办呢？到目前为止，在我们的示例数据集中，我们的模型已经显示出各种不适合的迹象:非线性残差和相对简单的数据集上的较差 R 值。处理欠拟合最常见的方法是利用一个<em class="ll">内核</em>。核是满足三个主要特性密度函数:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/0c98c10d8a154bba5902811d9678430d.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*XoFU4GQVCVOxXFOifXkccw.png"/></div></figure><p id="3e5b" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">统计社区通常将内核回归称为<em class="ll">非参数回归</em>技术<em class="ll"> </em>。使用核的前提是，如果我们将输入变量映射到一个更高的维度，那么问题可以很容易地被分类或预测。要了解这在实践中是如何工作的，最简单的例子是通过一个简单的分类问题。假设我们有两个组，我们希望只使用直线/超平面来分类。我们可以在下面看到，没有一条线能够将这两个组进行分类:</p><div class="my mz na nb gt ab cb"><figure class="np nc pq nr ns nt nu paragraph-image"><div role="button" tabindex="0" class="oe of di og bf oh"><img src="../Images/29ff5253d420f84715dd760c65d87638.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*yEEXEfCJHCs1_Vbfnt50yg.png"/></div></figure><figure class="np nc pr nr ns nt nu paragraph-image"><img src="../Images/01e4f3d9fd148939ade0d8058a357854.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*woyzF0Wb4Ux7Q5mu3lxfUA.png"/></figure></div><p id="2a5d" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">然而，如果我们将数据转移到一个更高的维度(正如我们在右手边看到的)，现在存在一个能够对数据进行分类的超平面。</p><p id="e461" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">举一个回归的例子，假设我们只有一个特征变量 X，其中目标变量 Y 等于 X。我们可以在左下方的图片中看到，线性模型将无法准确地表示这些数据。然而，如果我们将特征变量 X 投影到一个更高的维度 X，那么我们可以看到我们的线性模型符合一条完美的直线。</p><div class="my mz na nb gt ab cb"><figure class="np nc ps nr ns nt nu paragraph-image"><div role="button" tabindex="0" class="oe of di og bf oh"><img src="../Images/d453d9228ab141d7416cc73afef248d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*5hgf0VcBv6sR4145U490sg.png"/></div></figure><figure class="np nc pt nr ns nt nu paragraph-image"><div role="button" tabindex="0" class="oe of di og bf oh"><img src="../Images/dd59895e5287212c50b5ec5324310ecd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*DBu_zW_wGUviXiLRDnI-BQ.png"/></div></figure></div><p id="58b0" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">我希望现在我已经让你相信了把我们的特征变量投射到更高维度的力量！然而，现在的问题是，我们如何做到这一点？首先，我们需要定义一个函数，通常表示为 phi，它将我们的变量映射到更高维的输入空间。在内核回归中，这是通过<em class="ll">内核函数来实现的。</em>最流行和最基本的内核之一是<em class="ll">多项式内核，</em>它简单地对特征变量进行幂运算。让我们举一个简单的例子，我们只有两个变量:x1 和 x2；然后，我们想通过简单地使用 2 次方的多项式核将它映射到一个更高维的空间:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi pu"><img src="../Images/c4eb7845415d4805906ff343cdd558f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*pgRdzRo0WuZD-Z6D9HN2NA.png"/></div></figure><p id="23ba" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">正如我们从上面看到的，我们使用多项式幂为 2 的 phi 函数将原始数据 x1 和 x2 映射到一个更高的维度。唯一的问题是，现在我们的时间复杂度与我们的多项式 O(k^p).的幂成正比我们可以通过<em class="ll">内核技巧来降低这种复杂性。</em>首先，让我们使用 phi(x)重新计算我们的损失/误差指标。注意，核回归利用岭回归，因为系数往往非常大，这就是为什么这种方法通常被称为<em class="ll">核岭回归</em>:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/ac227d6d585766e31fbbc1d7a3b17063.png" data-original-src="https://miro.medium.com/v2/resize:fit:1260/format:webp/1*WWBIez3_GtmYDMggyEYRBQ.png"/></div></figure><p id="f836" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">我们可以看到β的推导实际上是递归的，这意味着最优β是它自身的函数。然而，如果我们将这个 beta 值代入我们的误差指标，我们会得到:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="oe of di og bf oh"><div class="gh gi pw"><img src="../Images/408c92fd5ff5ae0674fce475e9b6f6d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l_j3QZztcZxq2PCkXx9gcg.png"/></div></div></figure><p id="f71c" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">如图所示，当我们用新的β值降低损失函数时，我们得到 phi(x_i)*phi(x_j)，其中 phi(x)是 O(k^p 运算，这使得该过程非常耗时。然而，<em class="ll">的诀窍</em>，在于</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi px"><img src="../Images/4879105a292d45205df2ae05ab17fbc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:654/format:webp/1*s68ZV4ZQlS6JSv05ljpkQA.png"/></div></figure><p id="8924" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">为了给出一个具体的例子，让我们将它应用于我们之前的核函数，一个二次多项式:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi py"><img src="../Images/31c51c4fb963d52ed51fb70a3a2aa155.png" data-original-src="https://miro.medium.com/v2/resize:fit:1010/format:webp/1*Drq_axfzZSY34WSzF6JOZg.png"/></div></figure><p id="f9f6" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">正如我们从上面看到的，<em class="ll">内核技巧</em>是这样的事实:两个数据点的点积转换为高维映射与两点间点积的高维映射相同！这样节省了大量的时间和计算资源！现在，我们可以将它转换回损失函数，得到:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi pz"><img src="../Images/2c13bebbb4057294b320b93c8215acde.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*zHvygM-gMIZeKVQgR2meSg.png"/></div></figure><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi qa"><img src="../Images/ea29faa7d6bd4d53026dd8041c238970.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*0PhEW36elZGbtkl2KvOLZg.png"/></div></figure><p id="06a2" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">从上面我们可以看到，损失函数的格式与最小二乘法非常相似，只是其中 K=X，alpha=beta。为了找到最优β，我们首先找到α的最优解，然后把它代入β！</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi qb"><img src="../Images/effe2f88f91ec8c954474a0d63950f0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1336/format:webp/1*BWUjQkWXw3ur3KW6dwazSQ.png"/></div></figure><p id="356e" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">标准最小二乘法的时间复杂度是 O(k ),但是现在我们的矩阵结果是一个 n 乘 n 矩阵，因为 K 是 n 乘 n；因此，核回归的时间复杂度为 O(n)，在数据量很大的情况下，计算量非常大！一种常见的解决方案是简单地从整个数据集中采样数据，使得 n 很小。</p><p id="4fd1" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">内核回归需要调整的另一个超参数是选择使用哪个<em class="ll">内核函数</em>。最常见的三种如下:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi qc"><img src="../Images/85277ea215f01d1f417dd167c960f130.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*aRGX8Q12Ws5_bbvmsRYZaw.png"/></div></figure><p id="49cf" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">正如我们从上面所看到的，每个内核函数都有自己的一组超参数需要优化，这增加了复杂性。</p><p id="0062" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">既然我们已经讨论了理论背景，让我们将核岭回归应用于我们的问题！对于这个例子，我将只展示多项式核，因为它是最常见的。因为 Kernel Ridge 也有一个 lambda/penalty 项，所以我将展示增加罚项对测试数据集的影响。请注意，在实践中，您可能希望在验证集上这样做，而不是在测试集上。</p><figure class="my mz na nb gt nc"><div class="bz fp l di"><div class="nd ne l"/></div></figure><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi qd"><img src="../Images/08102041e80a2bf44a0556742db02507.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*JwZltea76hdLMiQEhjrtfA.png"/></div></figure><p id="2989" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">从上面的图中我们可以看出，增加λ的惩罚项实际上降低了测试和训练误差的 R 值；然而，在实践中可能不是这样，所以总是测试不同的正则化值。然而，我们可以立即看到，使用核回归将测试数据集上的 R 从 0.76 增加到 0.83，这意味着我们的模型现在可以解释目标变量约 83%的可变性，略好于 76%。现在，让我们检查训练数据集上的残差图，并与标准线性回归进行比较:</p><div class="my mz na nb gt ab cb"><figure class="np nc pt nr ns nt nu paragraph-image"><div role="button" tabindex="0" class="oe of di og bf oh"><img src="../Images/9af6a2e0f65170a6c093a4e01077d880.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*7o0uxj_2CtGEo2T_bC0xew.png"/></div></figure><figure class="np nc ps nr ns nt nu paragraph-image"><div role="button" tabindex="0" class="oe of di og bf oh"><img src="../Images/fc17adc8f95b44607631fce26588a3d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*YFYMxqo1170izt-IRzxpXg.png"/></div></figure></div><p id="b5e1" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">正如我们从上面看到的，我们的核脊残差图(在右手边)明确地将残差的方差均衡为 7 到 9 之间的 Y 值的常数；然而，Y 值在 9 和 10.5 之间的极大残差仍然存在；表明我们的模型对其中的一些点拟合不足。</p><p id="91a1" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">核回归的一个缺点是失去了模型的可解释性，因为现在β系数不是针对特征变量而是针对数据观察值，因为新数据的预测由下式给出:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi qe"><img src="../Images/0fe0ce349f962d2b67ad2c12d1f9c2ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1098/format:webp/1*dRBPlmYBTTEIlYmCbZptWA.png"/></div></figure><p id="1b4e" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">正如我们所看到的，对于一个新的预测，我们从新数据和β被训练的数据之间的点积形成一个新的核矩阵 K，乘以保存系数的α向量。由于这种高维映射，模型如何从简单的特征变量获得其结果的可解释性丧失，使得核回归成为一种<em class="ll">黑盒方法</em>。</p><p id="dda2" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">然而，你可能会想，如果核回归是一种黑盒方法，因为到更高维度的投影被总结为数据实例之间的一个值，那么我们为什么不手动投影我们的特征空间呢？例如，假设我们有以下具有三个变量的特征空间，并将其投影到二次多项式:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi qf"><img src="../Images/210fa962c01c44ac4cf9aace71a1d871.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*WUFc6GUkJ_ATs8kt6EKV7w.png"/></div></figure><p id="169f" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">现在，我们已经将初始数据维度投影到一个更高的维度，允许我们执行岭回归来获得白盒贝塔系数！然而，问题是我们在最小二乘推导中假设(X^T*X)是可逆的，这假设 x 是线性独立的，这意味着没有一列是另一列的组合；但是我们可以清楚地看到，我们新投影的数据是原始数据维度的线性组合！这样，我们增加的高维项越多，逆就越有可能不存在。内核回归避免了这个问题，因为它在数据实例之间投影点积，这里我们假设数据实例是独立采样的。</p></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h1 id="69a9" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated"><strong class="ak">支持向量机</strong></h1><p id="07ba" class="pw-post-body-paragraph kp kq it kr b ks mg ju ku kv mh jx kx ky mu la lb lc mv le lf lg mw li lj lk im bi translated">对于我们深入回归分析的最后一个方法，我们将看看核岭回归的一个密切对应的支持向量机(SVMs)。为了给出支持向量机背后的基本直觉，让我们切换到分类的目标，其中我们想要找到一个决策边界来分类两个组，并且我们有三个可能的模型:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="oe of di og bf oh"><div class="gh gi qg"><img src="../Images/ddac4e705ae60bcbd9f976025e0d066e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VfVhKie3Jbg0C3zyvil0LQ.png"/></div></div></figure><p id="5cfc" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">问题是三个决策边界都对所有点进行了正确的分类，所以现在的问题是哪一个更好？理想的模型应该是红线，因为它不太靠近 1 级或 2 级。SVM 通过增加一个关于决策边界的余量来解决这个问题，通常称为<em class="ll">支持向量:</em></p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="oe of di og bf oh"><div class="gh gi qh"><img src="../Images/77886d8c25de7e10daaf5c40541254b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JsZgYdtWsD1fHEH2QKEZgw.png"/></div></div></figure><p id="d04e" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">通过添加这些支持向量，我们的模型能够“感觉”出数据，以找到一个决策边界，该边界可以最小化这些支持向量范围内的误差。SVM 有两种类型，<em class="ll">软边际</em>和<em class="ll">硬边际</em>。硬边界使模型找到决策边界，使得没有数据实例在支持向量边界内；而软边距允许实例在边距内。硬边界仅适用于可线性分类的数据，对异常值极其敏感，因此软边界是最常见的 SVM 类型。</p><p id="d785" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">这些支持向量的宽度，即余量，通常表示为ε。支持向量回归的误差函数类似于最小二乘法的误差函数，因为它假设目标变量是特征变量的线性组合:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi qi"><img src="../Images/d6530e6468bd448b23609182f62225e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:356/format:webp/1*H7_LWKNXMplSiBJ28YpM-Q.png"/></div></figure><p id="20e0" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">然而，损失/误差函数的构造与之前不同，因为我们想要最小化β，以确保<em class="ll">的平坦度</em>，这意味着我们想要小的β系数，以便没有特征变量系数变得太大，导致过度拟合。此外，我们还希望最小化残差，使其小于裕度宽度，记为ε:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi qj"><img src="../Images/b5efe8c4cde5049bde5d27a3996d7643.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*3fg_KlwiSTQIpXwm_E7YPw.png"/></div></figure><p id="f683" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">然而，问题是，对于给定的ε，可能不存在满足该条件的模型(<em class="ll">硬裕度</em>，导致使用<em class="ll">松弛变量的替代函数(</em>称为<em class="ll">软裕度):</em></p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi qk"><img src="../Images/9447db3015107a0922573784ab2a677c.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*EseA9SYWPvllxBIt2XaFpg.png"/></div></figure><p id="78c9" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">不幸的是，用来解决这个问题的数学不再像找到一个导数并设置它等于零那么简单，而是涉及到<em class="ll">二次规划。由于这种复杂性，我将跳过数学来寻找最终的解决方案。</em></p><p id="d16a" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">因为 SVM 利用了数据矩阵 X，所以可以通过核函数利用非线性映射来实现非线性回归平面。我将跳过这背后的数学，因为它变得混乱和复杂；然而，这个想法和上面提到的内核脊是一样的。好的一面是内核技巧在这里仍然适用，从而节省了时间和计算。既然我们已经讨论了 SVM 理论的一面，让我们把它应用到我们的问题中吧！</p><p id="9115" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">与核岭回归一样，有许多可能核函数可供使用，这次我将测试其中的三种:多项式、RBF 和线性。此外，SVM 还需要两个更重要的超参数，C 和ε。ε是余量宽度，C 是正则化项。在实践中，只有正则化项 C 被改变，因为改变边缘宽度将会极大地导致差的结果。这里我们有三个内核，在测试集上评估了不同 C 值下的默认参数。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi ql"><img src="../Images/72816110a5e499460f4f01bba1a5078e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1176/format:webp/1*k6b6un8BTZAjNhuJCXmMRQ.png"/></div></figure><p id="a9b0" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">正如我们所看到的，对于这个特定的数据集，通过增加 C 值，几乎所有三个内核都增加了测试集的 R 值。我们可以看到，RBF 内核的性能最好，因此让我们更深入地检查一下它在 C=100 时的结果:</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div class="gh gi qm"><img src="../Images/49ab26683b2b3c45ec1935051ea4032c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*wcULhTFQmXdh_KaHyq8HVA.png"/></div></figure><p id="80ee" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">正如我们所见，我们在测试数据集上的 R 优于最小二乘法，解释了目标变量 81%的可变性，但不如具有多项式核的核岭回归好。但是，请注意，这在实践中可能并不总是发生。人们可以检查这个模型的残差图，但它与之前的非常相似，因为 R 非常相似。</p><p id="4d20" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">不幸的是，与核岭回归一样，因为支持向量机是基于核而不是特征变量找到它们的系数的，所以对模型如何实现其预测的解释丢失了，这使得 SVM 成为一种黑盒方法。</p></div><div class="ab cl ki kj hx kk" role="separator"><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn ko"/><span class="kl bw bk km kn"/></div><div class="im in io ip iq"><h1 id="e067" class="lm ln it bd lo lp lq lr ls lt lu lv lw jz lx ka ly kc lz kd ma kf mb kg mc md bi translated">结论</h1><p id="9309" class="pw-post-body-paragraph kp kq it kr b ks mg ju ku kv mh jx kx ky mu la lb lc mv le lf lg mw li lj lk im bi translated">如果你已经走到这一步，恭喜你！我希望你已经在数据科学和机器学习领域学到了很多关于回归的知识！</p><p id="3ea4" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">作为快速回顾，我们介绍了我们的第一个模型，<strong class="kr iu">最小二乘</strong>，它简单地假设目标变量是特征变量的线性组合，目标是找到这些系数。出现的问题是最小二乘法是建立在一些假设上的，即误差具有恒定的方差和零均值。然而，在实践中，这经常被违反，因为通过评估残差图，可以观察到残差的非线性。假设残差遵循特定趋势，如二项式或扩音器，<strong class="kr iu">加权最小二乘法</strong>可用于创建满足这些假设的模型。最小二乘法及其衍生物的众多优点之一是其开放的<strong class="kr iu">白盒</strong>性质，这意味着模型预测可以通过特征变量的系数直接观察到。</p><p id="2c04" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">在我们的模型训练误差低但测试误差高的情况下，我们需要包含正则化以防止<strong class="kr iu">过度拟合</strong>。我们讨论了三种最常见的正规化类型:<strong class="kr iu">脊</strong>、<strong class="kr iu">套索</strong>和<strong class="kr iu">弹性网</strong>。岭正则化缩小了系数的值，而 Lasso 将一些系数驱动到零，而弹性网寻求协调这两者。</p><p id="116c" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">另一方面，我们的模型<strong class="kr iu">没有过度拟合，而是欠拟合</strong>，训练和测试误差都很高。为了解决这个问题，我们使用核函数将我们的特征空间投影到更高维度，希望预测平面能够拟合数据。这是通过两种方法执行的— <strong class="kr iu">核岭回归</strong>和<strong class="kr iu">支持向量机</strong>。两者之间的区别是误差/损失函数的公式，其中 SVM 的包括一个误差幅度，以尽量减少。然而，这些高维映射模型的问题是，模型如何根据特征变量实现其预测的解释丢失了，使它们都成为<strong class="kr iu">黑盒</strong>方法。</p><p id="bee5" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">实际上，没有可以利用的最佳模型。如果你想向没有机器学习背景的商人展示你的模型，那么使用 LR 或 WLR 来解释不同特征的重要性将是有益的，因为它们都是白盒方法，然后报告黑盒方法的分数，因为它们往往表现得更好。</p><p id="6d4d" class="pw-post-body-paragraph kp kq it kr b ks kt ju ku kv kw jx kx ky kz la lb lc ld le lf lg lh li lj lk im bi translated">总而言之，我希望你已经从上面讨论的主题中学到了很多，无论是理论上还是应用上！</p></div></div>    
</body>
</html>