<html>
<head>
<title>A Complete Sentiment Analysis Project Using Python’s Scikit-Learn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Python的Scikit-Learn的完整情感分析项目</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-complete-sentiment-analysis-project-using-pythons-scikit-learn-b9ccbb0405c2?source=collection_archive---------8-----------------------#2021-08-28">https://towardsdatascience.com/a-complete-sentiment-analysis-project-using-pythons-scikit-learn-b9ccbb0405c2?source=collection_archive---------8-----------------------#2021-08-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/635ef8b0beb24b3a9e93317d4df18b13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ydAGe88xo1CT9mvx"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://unsplash.com/@tengyart?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">腾雅特</a>在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><div class=""/><div class=""><h2 id="224c" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">比较Python中情感分析项目的两种不同矢量器和三种机器学习模型</h2></div><p id="dfe1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">情感分析是自然语言处理的重要组成部分之一。它不同于使用数字数据的机器学习，因为算法不能直接处理文本数据。它需要被转换成数字形式。因此，文本数据在输入机器学习模型之前会进行矢量化处理。矢量化有不同的方法。本文将使用两种类型的矢量器和三种机器学习模型来演示情感分析。</p><h2 id="8a54" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">数据预处理</h2><p id="5a38" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">我正在为这个项目使用来自Kaggle的亚马逊婴儿产品数据集。如果你想继续，请随意从这个链接下载<a class="ae jg" href="https://www.kaggle.com/sameersmahajan/reviews-of-amazon-baby-products" rel="noopener ugc nofollow" target="_blank">数据集。</a></p><p id="ad6e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">原始数据集有三个特征:名称(产品名称)、评论(客户对产品的评论)和评级(客户对产品的评级，范围从1到5)。评论栏将是输入栏，评级栏将用于了解评论的观点。以下是一些重要的数据预处理步骤:</p><ol class=""><li id="9b61" class="ms mt jj la b lb lc le lf lh mu ll mv lp mw lt mx my mz na bi translated">该数据集大约有183，500行数据。有1147个空值。我只需要去掉那些空值。</li><li id="4036" class="ms mt jj la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated">由于数据集相当大，运行一些机器学习算法需要花费大量时间。因此，我在这个项目中使用了30%的数据，仍然是54，000个数据。样本具有代表性。</li><li id="839e" class="ms mt jj la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated">如果评分为1和2，将被视为差评或负面评价。而如果评论是3、4、5，那么该评论将被认为是好的评论或积极的评论。因此，我在数据集中添加了一个名为“情感”的新列，用1表示正面评价，用0表示负面评价。</li></ol><blockquote class="ng"><p id="f56c" class="nh ni jj bd nj nk nl nm nn no np lt dk translated">也许我在一个块中放了很多代码。如果数量很多，请将其分解成小块，以便更好地理解。</p></blockquote><p id="bbe6" class="pw-post-body-paragraph ky kz jj la b lb nq kk ld le nr kn lg lh ns lj lk ll nt ln lo lp nu lr ls lt im bi translated">下面是代码块，它导入数据集，抽取30%的代表性样本，并添加新列“情感”:</p><pre class="nv nw nx ny gt nz oa ob oc aw od bi"><span id="784a" class="lu lv jj oa b gy oe of l og oh">import pandas as pd<br/>df = pd.read_csv('amazon_baby.csv')</span><span id="0dff" class="lu lv jj oa b gy oi of l og oh">#getting rid of null values<br/>df = df.dropna()</span><span id="b104" class="lu lv jj oa b gy oi of l og oh">#Taking a 30% representative sample<br/>import numpy as np<br/>np.random.seed(34)<br/>df1 = df.sample(frac = 0.3)</span><span id="bf93" class="lu lv jj oa b gy oi of l og oh">#Adding the sentiments column<br/>df1['sentiments'] = df1.rating.apply(lambda x: 0 if x in [1, 2] else 1)</span></pre><p id="b234" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是数据集现在的样子。这是前5行数据:</p><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/93452c29953afbd064ae387398a8561a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*aXhUcLlHMAS5_Q6v.png"/></div></figure><h2 id="4503" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">情感分析</h2><p id="ccc7" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">在开始情感分析之前，有必要定义输入特征和标签。这里只有一个特征，那就是‘回顾’。标签将是“情绪”。这个项目的目标是训练一个模型，如果一个评论是积极的或消极的，可以输出。</p><pre class="nv nw nx ny gt nz oa ob oc aw od bi"><span id="373e" class="lu lv jj oa b gy oe of l og oh">X = df1['review']<br/>y = df1['sentiments']</span></pre><p id="deb7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，我将使用计数矢量化工具作为矢量化方法。本文将关注如何应用矢量器。所以，我不想谈细节。但是，如果您对矢量器完全陌生，请随意查看本文以了解更多关于计数矢量器的信息。</p><h2 id="08ee" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">计数矢量器</h2><p id="db46" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">我将使用一个计数矢量化工具对review列中的文本数据进行矢量化处理(本项目的训练特性)，然后使用scikit-learn模型中的三个不同的分类模型。之后，评估这个数据集上的模型，找出准确性、混淆矩阵、真阳性率和真阴性率。以下是步骤。</p><ol class=""><li id="7da8" class="ms mt jj la b lb lc le lf lh mu ll mv lp mw lt mx my mz na bi translated">第一步是将数据集分成训练集和测试集。</li><li id="9707" class="ms mt jj la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated">向量化检查列之外的输入要素(训练和测试数据)</li><li id="b3be" class="ms mt jj la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated">从scikit学习库中导入模型。</li><li id="1b2e" class="ms mt jj la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated">找到准确度分数</li><li id="cde6" class="ms mt jj la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated">找出真正的正利率和负利率。</li></ol><p id="d370" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我将对三个不同的分类器重复相同的过程。这里使用的分类器是逻辑回归、支持向量机和K最近邻分类器。我将在本文结尾总结结果。</p><blockquote class="ok ol om"><p id="c407" class="ky kz on la b lb lc kk ld le lf kn lg oo li lj lk op lm ln lo oq lq lr ls lt im bi translated"><strong class="la jk">逻辑回归</strong></p></blockquote><p id="01b5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是逻辑回归的代码块。我在代码之间使用了注释。</p><pre class="nv nw nx ny gt nz oa ob oc aw od bi"><span id="ec95" class="lu lv jj oa b gy oe of l og oh">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, <br/>                                        test_size = 0.5, random_state=24)</span><span id="451d" class="lu lv jj oa b gy oi of l og oh">from sklearn.feature_extraction.text import CountVectorizer<br/>cv = CountVectorizer()</span><span id="1fa3" class="lu lv jj oa b gy oi of l og oh">#Vectorizing the text data<br/>ctmTr = cv.fit_transform(X_train)<br/>X_test_dtm = cv.transform(X_test)</span><span id="dca8" class="lu lv jj oa b gy oi of l og oh">from sklearn.linear_model import LogisticRegression</span><span id="12e4" class="lu lv jj oa b gy oi of l og oh">#Training the model<br/>lr = LogisticRegression()<br/>lr.fit(ctmTr, y_train)</span><span id="e276" class="lu lv jj oa b gy oi of l og oh">#Accuracy score<br/>lr_score = lr.score(X_test_dtm, y_test)<br/>print("Results for Logistic Regression with CountVectorizer")<br/>print(lr_score)</span><span id="a36f" class="lu lv jj oa b gy oi of l og oh">#Predicting the labels for test data<br/>y_pred_lr = lr.predict(X_test_dtm)<br/>from sklearn.metrics import confusion_matrix</span><span id="769f" class="lu lv jj oa b gy oi of l og oh">#Confusion matrix<br/>cm_lr = confusion_matrix(y_test, y_pred_lr)</span><span id="1ae4" class="lu lv jj oa b gy oi of l og oh">tn, fp, fn, tp = confusion_matrix(y_test, y_pred_lr).ravel()<br/>print(tn, fp, fn, tp)</span><span id="8119" class="lu lv jj oa b gy oi of l og oh">#True positive and true negative rates<br/>tpr_lr = round(tp/(tp + fn), 4)<br/>tnr_lr = round(tn/(tn+fp), 4)</span><span id="6862" class="lu lv jj oa b gy oi of l og oh">print(tpr_lr, tnr_lr)</span></pre><p id="66a6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如您所见，我有打印准确性、真阳性、假阳性、真阴性、假阴性、真阴性率和假阴性率的打印语句。</p><blockquote class="ok ol om"><p id="60ac" class="ky kz on la b lb lc kk ld le lf kn lg oo li lj lk op lm ln lo oq lq lr ls lt im bi translated"><strong class="la jk">支持向量机</strong></p></blockquote><p id="f2c5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我将重复与之前完全相同的过程来使用支持向量机。</p><pre class="nv nw nx ny gt nz oa ob oc aw od bi"><span id="0733" class="lu lv jj oa b gy oe of l og oh">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, <br/>                                        test_size = 0.5, random_state=123)</span><span id="b862" class="lu lv jj oa b gy oi of l og oh">#Vectorizing the text data<br/>cv = CountVectorizer()<br/>ctmTr = cv.fit_transform(X_train)<br/>X_test_dtm = cv.transform(X_test)</span><span id="962c" class="lu lv jj oa b gy oi of l og oh">from sklearn import svm</span><span id="d541" class="lu lv jj oa b gy oi of l og oh">#Training the model<br/>svcl = svm.SVC()<br/>svcl.fit(ctmTr, y_train)</span><span id="dd40" class="lu lv jj oa b gy oi of l og oh">svcl_score = svcl.score(X_test_dtm, y_test)<br/>print("Results for Support Vector Machine with CountVectorizer")<br/>print(svcl_score)</span><span id="e916" class="lu lv jj oa b gy oi of l og oh">y_pred_sv = svcl.predict(X_test_dtm)</span><span id="2b8b" class="lu lv jj oa b gy oi of l og oh">#Confusion matrix<br/>cm_sv = confusion_matrix(y_test, y_pred_sv)</span><span id="a2ae" class="lu lv jj oa b gy oi of l og oh">tn, fp, fn, tp = confusion_matrix(y_test, y_pred_sv).ravel()<br/>print(tn, fp, fn, tp)</span><span id="0090" class="lu lv jj oa b gy oi of l og oh">tpr_sv = round(tp/(tp + fn), 4)<br/>tnr_sv = round(tn/(tn+fp), 4)</span><span id="2c6f" class="lu lv jj oa b gy oi of l og oh">print(tpr_sv, tnr_sv)</span></pre><p id="f901" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我应该提醒你，支持向量机比逻辑回归需要更多的时间。</p><blockquote class="ok ol om"><p id="9368" class="ky kz on la b lb lc kk ld le lf kn lg oo li lj lk op lm ln lo oq lq lr ls lt im bi translated"><strong class="la jk"> K最近邻</strong></p></blockquote><p id="728f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我将运行一个KNN分类器，得到和以前一样的评估矩阵。代码几乎是一样的。</p><pre class="nv nw nx ny gt nz oa ob oc aw od bi"><span id="a691" class="lu lv jj oa b gy oe of l og oh">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, <br/>                                        test_size = 0.5, random_state=143)</span><span id="7891" class="lu lv jj oa b gy oi of l og oh">from sklearn.feature_extraction.text import CountVectorizer<br/>cv = CountVectorizer()<br/>ctmTr = cv.fit_transform(X_train)<br/>X_test_dtm = cv.transform(X_test)</span><span id="1028" class="lu lv jj oa b gy oi of l og oh">from sklearn.neighbors import KNeighborsClassifier<br/>knn = KNeighborsClassifier(n_neighbors=5)<br/>knn.fit(ctmTr, y_train)</span><span id="de6e" class="lu lv jj oa b gy oi of l og oh">knn_score = knn.score(X_test_dtm, y_test)<br/>print("Results for KNN Classifier with CountVectorizer")<br/>print(knn_score)</span><span id="e6d6" class="lu lv jj oa b gy oi of l og oh">y_pred_knn = knn.predict(X_test_dtm)</span><span id="786e" class="lu lv jj oa b gy oi of l og oh">#Confusion matrix<br/>cm_knn = confusion_matrix(y_test, y_pred_knn)</span><span id="4a20" class="lu lv jj oa b gy oi of l og oh">tn, fp, fn, tp = confusion_matrix(y_test, y_pred_knn).ravel()<br/>print(tn, fp, fn, tp)</span><span id="bec6" class="lu lv jj oa b gy oi of l og oh">tpr_knn = round(tp/(tp + fn), 4)<br/>tnr_knn = round(tn/(tn+fp), 4)</span><span id="6849" class="lu lv jj oa b gy oi of l og oh">print(tpr_knn, tnr_knn)</span></pre><p id="3b72" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">KNN分类器比支持向量机分类器耗时少。</p><p id="450b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">至此，计数向量机方法的三个分类器就完成了。</p><h2 id="6950" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">TFIDF矢量器</h2><p id="3d38" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">接下来，我将使用TF-IDF矢量器。众所周知，这个矢量器是一个更受欢迎的矢量器，因为它使用了词的频率这一术语。请随意<a class="ae jg" href="https://medium.com/@cmukesh8688/tf-idf-vectorizer-scikit-learn-dbc0244a911a" rel="noopener">查看本文</a>以了解关于TF-IDF矢量器的详细信息。</p><p id="af7b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我将遵循与计数矢量器完全相同的过程。只有矢量器是不同的。但这不是问题。超级酷的sklearn库会照常打理计算部分。</p><blockquote class="ok ol om"><p id="4d04" class="ky kz on la b lb lc kk ld le lf kn lg oo li lj lk op lm ln lo oq lq lr ls lt im bi translated"><strong class="la jk">逻辑回归</strong></p></blockquote><p id="01ec" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用TF-IDF矢量器进行逻辑回归的完整代码块:</p><pre class="nv nw nx ny gt nz oa ob oc aw od bi"><span id="80fc" class="lu lv jj oa b gy oe of l og oh">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, <br/>                                        test_size = 0.5, random_state=45)</span><span id="cb4e" class="lu lv jj oa b gy oi of l og oh">from sklearn.feature_extraction.text import TfidfVectorizer</span><span id="a3a4" class="lu lv jj oa b gy oi of l og oh">#tfidf vectorizer</span><span id="e3f3" class="lu lv jj oa b gy oi of l og oh">vectorizer = TfidfVectorizer()</span><span id="1b62" class="lu lv jj oa b gy oi of l og oh">X_train_vec = vectorizer.fit_transform(X_train)</span><span id="a927" class="lu lv jj oa b gy oi of l og oh">X_test_vec = vectorizer.transform(X_test)</span><span id="7d93" class="lu lv jj oa b gy oi of l og oh">from sklearn.linear_model import LogisticRegression</span><span id="a64a" class="lu lv jj oa b gy oi of l og oh">lr = LogisticRegression()</span><span id="1054" class="lu lv jj oa b gy oi of l og oh">lr.fit(X_train_vec, y_train)</span><span id="9d9f" class="lu lv jj oa b gy oi of l og oh">lr_score = lr.score(X_test_vec, y_test)<br/>print("Results for Logistic Regression with tfidf")<br/>print(lr_score)</span><span id="c56c" class="lu lv jj oa b gy oi of l og oh">y_pred_lr = lr.predict(X_test_vec)</span><span id="4603" class="lu lv jj oa b gy oi of l og oh">#Confusion matrix<br/>from sklearn.metrics import confusion_matrix<br/>cm_knn = confusion_matrix(y_test, y_pred_lr)</span><span id="f43e" class="lu lv jj oa b gy oi of l og oh">tn, fp, fn, tp = confusion_matrix(y_test, y_pred_lr).ravel()<br/>print(tn, fp, fn, tp)</span><span id="b954" class="lu lv jj oa b gy oi of l og oh">tpr_knn = round(tp/(tp + fn), 4)<br/>tnr_knn = round(tn/(tn+fp), 4)</span><span id="071c" class="lu lv jj oa b gy oi of l og oh">print(tpr_knn, tnr_knn)</span></pre><p id="4f5c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如你所见，除了矢量器部分，你可以重用之前的代码。</p><blockquote class="ok ol om"><p id="d656" class="ky kz on la b lb lc kk ld le lf kn lg oo li lj lk op lm ln lo oq lq lr ls lt im bi translated"><strong class="la jk">支持向量机</strong></p></blockquote><p id="748e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">除了矢量器之外，这也将是与前面的支持向量机相同的过程。</p><pre class="nv nw nx ny gt nz oa ob oc aw od bi"><span id="fbb6" class="lu lv jj oa b gy oe of l og oh">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, <br/>                                        test_size = 0.5, random_state=55)</span><span id="4972" class="lu lv jj oa b gy oi of l og oh">vectorizer = TfidfVectorizer()<br/>X_train_vec = vectorizer.fit_transform(X_train)<br/>X_test_vec = vectorizer.transform(X_test)</span><span id="e74a" class="lu lv jj oa b gy oi of l og oh">from sklearn import svm<br/>#params = {'kernel':('linear', 'rbf'), 'C':[1, 10, 100]}<br/>svcl = svm.SVC(kernel = 'rbf')<br/>#clf_sv = GridSearchCV(svcl, params)<br/>svcl.fit(X_train_vec, y_train)<br/>svcl_score = svcl.score(X_test_vec, y_test)<br/>print("Results for Support Vector Machine with tfidf")<br/>print(svcl_score)</span><span id="ca3e" class="lu lv jj oa b gy oi of l og oh">y_pred_sv = svcl.predict(X_test_vec)</span><span id="7693" class="lu lv jj oa b gy oi of l og oh">#Confusion matrix<br/>from sklearn.metrics import confusion_matrix<br/>cm_sv = confusion_matrix(y_test, y_pred_sv)</span><span id="1617" class="lu lv jj oa b gy oi of l og oh">tn, fp, fn, tp = confusion_matrix(y_test, y_pred_sv).ravel()<br/>print(tn, fp, fn, tp)</span><span id="0bae" class="lu lv jj oa b gy oi of l og oh">tpr_sv = round(tp/(tp + fn), 4)<br/>tnr_sv = round(tn/(tn+fp), 4)</span><span id="3180" class="lu lv jj oa b gy oi of l og oh">print(tpr_sv, tnr_sv)</span></pre><p id="689e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">和以前一样，这将比逻辑回归花费更多的时间。所以，这可能需要一些耐心。</p><blockquote class="ok ol om"><p id="0ee4" class="ky kz on la b lb lc kk ld le lf kn lg oo li lj lk op lm ln lo oq lq lr ls lt im bi translated"><strong class="la jk"> K个最近邻居</strong></p></blockquote><p id="a675" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是最后一个。</p><pre class="nv nw nx ny gt nz oa ob oc aw od bi"><span id="bffa" class="lu lv jj oa b gy oe of l og oh">from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, <br/>                                        test_size = 0.5, random_state=65)</span><span id="2e58" class="lu lv jj oa b gy oi of l og oh">vectorizer = TfidfVectorizer()<br/>X_train_vec = vectorizer.fit_transform(X_train)<br/>X_test_vec = vectorizer.transform(X_test)</span><span id="20d9" class="lu lv jj oa b gy oi of l og oh">from sklearn.neighbors import KNeighborsClassifier<br/>knn = KNeighborsClassifier(n_neighbors=5)<br/>knn.fit(X_train_vec, y_train)</span><span id="3a66" class="lu lv jj oa b gy oi of l og oh">knn_score = knn.score(X_test_vec, y_test)<br/>print("Results for KNN Classifier with tfidf")<br/>print(knn_score)</span><span id="0c22" class="lu lv jj oa b gy oi of l og oh">y_pred_knn = knn.predict(X_test_vec)</span><span id="4239" class="lu lv jj oa b gy oi of l og oh">#Confusion matrix<br/>cm_knn = confusion_matrix(y_test, y_pred_knn)</span><span id="bf48" class="lu lv jj oa b gy oi of l og oh">tn, fp, fn, tp = confusion_matrix(y_test, y_pred_knn).ravel()<br/>print(tn, fp, fn, tp)</span><span id="d905" class="lu lv jj oa b gy oi of l og oh">tpr_knn = round(tp/(tp + fn), 4)<br/>tnr_knn = round(tn/(tn+fp), 4)</span><span id="d74d" class="lu lv jj oa b gy oi of l og oh">print(tpr_knn, tnr_knn)</span></pre><h2 id="b91f" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">结果</h2><p id="e269" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">这里我总结了上面所有六个代码块的结果。</p><figure class="nv nw nx ny gt iv gh gi paragraph-image"><div class="gh gi or"><img src="../Images/9828cd827caaee34ec93fc2559d5623b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*9wARDAjJTjP2fxKhjT5BRg.png"/></div></figure><p id="97a5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下是一些重要的发现:</p><ol class=""><li id="6591" class="ms mt jj la b lb lc le lf lh mu ll mv lp mw lt mx my mz na bi translated">总的来说，TF-IDF矢量器给出的结果比count矢量器部分稍好。对于矢量器。</li><li id="3b58" class="ms mt jj la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated">考虑到整体准确性、真阳性率和真阴性率，逻辑回归是本项目使用的所有三个分类器中最好的。</li><li id="187a" class="ms mt jj la b lb nb le nc lh nd ll ne lp nf lt mx my mz na bi translated">KNN分类器似乎不适合这个项目。尽管真正的正利率看起来非常好，但真正的负利率看起来非常差。</li></ol><h2 id="9654" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">结论</h2><p id="cd69" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">我已经做了这个项目作为我的课程的一部分，并决定分享它。您可能会奇怪，为什么每次我训练一个模型时，我都要将数据集分成训练和测试两部分。这是这门课的要求。这个想法是某个训练测试分割可能偏向某个分类器。因此，我必须在每个模型之前分割数据集。</p><p id="807c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">欢迎在<a class="ae jg" href="https://twitter.com/rashida048" rel="noopener ugc nofollow" target="_blank">推特</a>上关注我，并喜欢我的<a class="ae jg" href="https://www.facebook.com/rashida.smith.161" rel="noopener ugc nofollow" target="_blank">脸书</a>页面。</p><h2 id="906d" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">更多阅读</h2><div class="is it gp gr iu os"><a rel="noopener follow" target="_blank" href="/all-the-datasets-you-need-to-practice-data-science-skills-and-make-a-great-portfolio-74f2eb53b38a"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd jk gy z fp ox fr fs oy fu fw ji bi translated">练习数据科学技能和制作优秀投资组合所需的所有数据集</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">大量不同种类的数据集</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">towardsdatascience.com</p></div></div><div class="pb l"><div class="pc l pd pe pf pb pg ja os"/></div></div></a></div><div class="is it gp gr iu os"><a rel="noopener follow" target="_blank" href="/pair-plot-and-pairgrid-in-details-f782975032ea"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd jk gy z fp ox fr fs oy fu fw ji bi translated">详细的Pair plot和PairGrid</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">用真实数据演示Pandas和Seaborn库中的配对图</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">towardsdatascience.com</p></div></div><div class="pb l"><div class="ph l pd pe pf pb pg ja os"/></div></div></a></div><div class="is it gp gr iu os"><a rel="noopener follow" target="_blank" href="/an-overview-of-performance-evaluation-metrics-of-machine-learning-classification-algorithms-7a95783a762f"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd jk gy z fp ox fr fs oy fu fw ji bi translated">机器学习(分类)算法性能评价指标综述</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">开发一个分类模型和计算所有流行的性能评估指标使用…</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">towardsdatascience.com</p></div></div><div class="pb l"><div class="pi l pd pe pf pb pg ja os"/></div></div></a></div><div class="is it gp gr iu os"><a rel="noopener follow" target="_blank" href="/four-popular-feature-selection-methods-for-efficient-machine-learning-in-python-fdd34762efdb"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd jk gy z fp ox fr fs oy fu fw ji bi translated">Python中高效机器学习的四种流行特征选择方法</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">使用真实数据集执行特征选择方法，并在每个方法后检索所选特征</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">towardsdatascience.com</p></div></div><div class="pb l"><div class="pj l pd pe pf pb pg ja os"/></div></div></a></div><div class="is it gp gr iu os"><a href="https://pub.towardsai.net/a-complete-guide-to-confidence-interval-t-test-and-z-test-in-r-for-data-scientists-cd16dd2d0eec" rel="noopener  ugc nofollow" target="_blank"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd jk gy z fp ox fr fs oy fu fw ji bi translated">数据科学家置信区间、t检验和z检验完全指南</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">大量的实践练习</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">pub.towardsai.net</p></div></div><div class="pb l"><div class="pk l pd pe pf pb pg ja os"/></div></div></a></div></div></div>    
</body>
</html>