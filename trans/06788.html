<html>
<head>
<title>Understanding Masked Language Models (MLM) and Causal Language Models (CLM) in NLP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解自然语言处理中的掩蔽语言模型(MLM)和因果语言模型(CLM)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-masked-language-models-mlm-and-causal-language-models-clm-in-nlp-194c15f56a5?source=collection_archive---------2-----------------------#2021-06-19">https://towardsdatascience.com/understanding-masked-language-models-mlm-and-causal-language-models-clm-in-nlp-194c15f56a5?source=collection_archive---------2-----------------------#2021-06-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8af4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">自然语言处理中的语言模型(视觉和例子)</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/657d9d43355bf7f95ac1cde98a1f6bf5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*di3xgymKYox6erq8sTljdA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来自<a class="ae ky" href="https://unsplash.com/photos/W8KTS-mhFUE" rel="noopener ugc nofollow" target="_blank">源</a>的修改图像</p></figure><p id="af43" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">大多数现代NLP系统都遵循一个非常标准的方法来训练各种用例的新模型，即<strong class="lb iu"> <em class="lv">首先预训练，然后微调</em> </strong> <em class="lv">。</em>这里，<em class="lv"> </em>预训练的目标是利用大量的未标记文本，建立一个语言理解的通用模型，然后在机器翻译、文本摘要等各种特定的NLP任务上进行微调。</p><blockquote class="lw lx ly"><p id="72fa" class="kz la lv lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">在这篇博客中，我们将讨论两种流行的预训练方案，即<strong class="lb iu">掩蔽语言建模</strong> (MLM)和<strong class="lb iu">因果语言建模</strong> (CLM)。</p></blockquote><p id="78ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">没时间看完整个博客？然后看这个快速的&lt; 60秒的YouTube短片—</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mc md l"/></div></figure><h2 id="17b7" class="me mf it bd mg mh mi dn mj mk ml dp mm li mn mo mp lm mq mr ms lq mt mu mv mw bi translated"><strong class="ak">屏蔽语言模型解释</strong></h2><p id="edeb" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">在屏蔽语言建模下，我们通常屏蔽给定句子中的某个百分比的单词，并且该模型被期望<strong class="lb iu">基于该句子中的其他单词来预测那些被屏蔽的单词</strong>。这样的训练方案使得这个<strong class="lb iu">模型在本质上是双向的</strong>，因为屏蔽单词的表示是基于出现在它左右的单词而学习的<strong class="lb iu">。<em class="lv">你也可以把这想象成</em> <strong class="lb iu"> <em class="lv">一种填空题式的问题陈述</em> </strong> <em class="lv">。</em></strong></p><p id="908b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">下图显示了同样的— </em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/58218ae9bca1dbe8af6bd87fa277b97c.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*eD7YzjE92fjXZf8zf0qq_w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">蒙版语言模型|作者图片</p></figure><p id="1a31" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">下图显示了损失计算步骤的更详细视图— </em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/1b167559f28cd23a3a1ac5b4414a3719.png" data-original-src="https://miro.medium.com/v2/resize:fit:862/format:webp/1*q55V5tFUKTf0_ahVqMOlQg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">有损失的掩蔽语言模型|作者图片</p></figure><p id="ff85" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，屏蔽词的表示可以是基于注意力的，如<strong class="lb iu"> BERT </strong>及其变体，或者你也可以设计成没有注意力。基于<strong class="lb iu"> Alpha </strong> <em class="lv">(注意力权重)</em>的分布，您可以对每一个其他输入单词的表示进行加权，以学习被屏蔽单词的表示，例如<em class="lv">—Alpha = 1将对周围的单词给予相等的权重(这意味着每个单词在屏蔽表示中具有相等的贡献)。</em></p><h1 id="7632" class="ne mf it bd mg nf ng nh mj ni nj nk mm jz nl ka mp kc nm kd ms kf nn kg mv no bi translated">因果语言模型解释</h1><p id="daa2" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">在因果语言模型下，这里的想法仍然是<strong class="lb iu">预测给定句子中的屏蔽标记</strong>，但与MLM不同，该模型被允许只<strong class="lb iu">考虑出现在它左边的单词</strong>来做同样的事情<em class="lv">(理想情况下，这可能只是左边或右边，想法是使它单向)</em>。这样的训练方案使得这个模型<strong class="lb iu">本质上是单向的</strong>。</p><p id="69a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">正如您在下图</em>中看到的，该模型预计会根据出现在句子左侧的单词来预测出现在句子中的掩码标记。并且基于模型对实际标签所做的预测，我们计算交叉熵损失并将其反向传播以训练模型参数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/e14a8f293a3c43837d4b4823c8a2c722.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*9aARJcJORtIUwG78C6nyzA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">因果语言模型|作者图片</p></figure><p id="d1d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">下图显示了损失计算步骤的更详细视图— </em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/918d5b7a591e274abfb2b4212dba2c89.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*ziMD2NP9q0pvn8tKLMU38w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">带有损失的因果语言模型|作者图片</p></figure><p id="af32" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，屏蔽词的表达可以是基于注意力的，就像在<strong class="lb iu"> GPT </strong>和变体中一样，或者你也可以设计成没有注意力，就像我们过去在LSTM时代那样。基于<strong class="lb iu"> Alpha </strong> <em class="lv">(见图)</em>的分布，您可以对每隔一个输入单词的表示进行加权，以学习屏蔽单词的表示，例如<em class="lv">—Alpha = 1将对周围的单词给予相等的权重(意味着每个单词将对学习的屏蔽表示具有相等的贡献)。</em></p><p id="5061" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些系统也被称为仅解码器模型，因为在典型的编码器-解码器架构中，如在机器翻译、文本摘要等中，解码器<em class="lv">(文本生成器)</em>的工作方式类似。</p><h1 id="6251" class="ne mf it bd mg nf ng nh mj ni nj nk mm jz nl ka mp kc nm kd ms kf nn kg mv no bi translated">什么时候用什么？</h1><p id="13ea" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">当目标是学习输入文档的良好表示时，MLM损失是优选的，而当我们希望学习生成流畅文本的系统时，CLM是最优选的。此外，直觉上这是有意义的，因为当学习每个单词的良好输入表示时，你会想知道它出现的单词是左还是右，而当你想学习一个生成文本的系统时，你只能看到你到目前为止生成的所有内容<em class="lv">(就像人类如何书写一样)</em>。因此，制造一个在生成文本的同时也能窥视另一面的系统会引入偏见，限制模型的创造能力。</p><p id="a411" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然在训练包含编码器和解码器的整个架构时，您会经常发现MLM和CLM损失。两者都有各自的优点和局限性，<em class="lv">一种叫做</em> <a class="ae ky" href="https://arxiv.org/abs/1906.08237" rel="noopener ugc nofollow" target="_blank"> <em class="lv"> XLNet </em> </a> <em class="lv">的新模式使用了一种置换技术来利用两者的优点(MLM和CLM)。</em></p></div><div class="ab cl nr ns hx nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="im in io ip iq"><p id="ea2f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望你喜欢读这篇文章。如果你愿意支持我成为一名作家，可以考虑注册<a class="ae ky" href="https://prakhar-mishra.medium.com/membership" rel="noopener">成为</a>中的一员。每月只需5美元，你就可以无限制地使用Medium</p><p id="9fc4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望你喜欢阅读这篇博客。谢谢大家！</p></div></div>    
</body>
</html>