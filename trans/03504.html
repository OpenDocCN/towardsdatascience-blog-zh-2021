<html>
<head>
<title>Advanced YoloV5 tutorial — Enhancing YoloV5 with Weighted Boxes Fusion</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">高级YoloV5教程—使用加权盒融合增强YoloV5</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/advanced-yolov5-tutorial-enhancing-yolov5-with-weighted-boxes-fusion-3bead5b71688?source=collection_archive---------4-----------------------#2021-03-21">https://towardsdatascience.com/advanced-yolov5-tutorial-enhancing-yolov5-with-weighted-boxes-fusion-3bead5b71688?source=collection_archive---------4-----------------------#2021-03-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3201" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">关于使用YoloV5和提升其WBF性能的深入教程</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4092ae7b36d1189a0f71bbccf912659d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*CyaEhdtB_y4dEdTG"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">埃里克·卡里姆·科内利斯在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="34b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有大量的YoloV5教程，本文的目的不是重复这些内容，而是对其进行扩展。我最近参加了一个数据科学对象检测比赛，虽然我找到了大量创建基线的教程，但我没有找到任何关于如何扩展它的建议。此外，我想强调YoloV5配置中可能影响性能的最重要部分，因为毕竟数据科学主要是关于实验和超参数调整。</p><p id="de71" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在此之前，我只想说，就框架和库如何工作而言，使用对象检测模型不同于使用图像分类模型。这是我注意到的事情，我花了一段时间才明白过来。大多数流行的对象检测模型，如YoloV5、EfficientDet，都使用命令行界面来训练和评估，而不是使用编码方法。这意味着，从字面上看，你需要做的就是获得特定格式(COCO或VOC)的数据，然后将cmd指向它。这通常不同于使用代码训练和评估模型的影像分类模型。</p><h2 id="8a14" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">数据预处理</h2><p id="3d35" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">YoloV5希望你有两个目录，一个用于训练，一个用于验证。在这两个目录中，你需要另外两个目录，“图像”和“标签”。图像将包含实际的图像和标签应该有一个. txt文件为每个图像与该图像的注释，文本文件应该有相同的名称作为其相应的图像。</p><p id="388e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注释格式如下:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="b76c" class="lv lw it mu b gy my mz l na nb">&lt;'class_id'&gt; &lt;'x_center'&gt; &lt;'y_center'&gt; &lt;width'&gt; &lt;'height'&gt;</span></pre><p id="56ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要在代码中实现这一点，您可能需要一个与此类似的函数，其中原始数据框包含图像条目、它们的类id以及它们的边界框:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="eaff" class="lv lw it mu b gy my mz l na nb">def create_file(df, split_df, train_file, train_folder, fold):<br/>    <br/>    os.makedirs('labels/train/', exist_ok=True)<br/>    os.makedirs('images/train/', exist_ok=True)<br/>    os.makedirs('labels/val/', exist_ok=True)<br/>    os.makedirs('images/val/', exist_ok=True)<br/>    <br/>    list_image_train = split_df[split_df[f'fold_<strong class="mu iu">{</strong>fold<strong class="mu iu">}</strong>']==0]['image_id']    <br/>    train_df = df[df['image_id'].isin(list_image_train)].reset_index(drop=True)<br/>    val_df = df[~df['image_id'].isin(list_image_train)].reset_index(drop=True)<br/>    <br/>    for train_img <strong class="mu iu">in</strong> tqdm(train_df.image_id.unique()):<br/>        with open('labels/train/<strong class="mu iu">{</strong>train_img<strong class="mu iu">}</strong>.txt', 'w+') as f:<br/>            row = train_df[train_df['image_id']==train_img]\<br/>            [['class_id', 'x_center', 'y_center', 'width', 'height']].values<br/>            row[:, 1:] /= SIZE # Image size, 512 here<br/>            row = row.astype('str')<br/>            for box <strong class="mu iu">in</strong> range(len(row)):<br/>                text = ' '.join(row[box])<br/>                f.write(text)<br/>                f.write('<strong class="mu iu">\n</strong>')<br/>        shutil.copy(f'<strong class="mu iu">{</strong>train_img<strong class="mu iu">}</strong>.png', <br/>                f'images/train/<strong class="mu iu">{</strong>train_img<strong class="mu iu">}</strong>.png')<br/>        <br/>    for val_img <strong class="mu iu">in</strong> tqdm(val_df.image_id.unique()):<br/>        with open(f'<strong class="mu iu">{</strong>labels/val/<strong class="mu iu">{</strong>val_img<strong class="mu iu">}</strong>.txt', 'w+') as f:<br/>            row = val_df[val_df['image_id']==val_img]\<br/>            [['class_id', 'x_center', 'y_center', 'width', 'height']].values<br/>            row[:, 1:] /= SIZE<br/>            row = row.astype('str')<br/>            for box <strong class="mu iu">in</strong> range(len(row)):<br/>                text = ' '.join(row[box])<br/>                f.write(text)<br/>                f.write('<strong class="mu iu">\n</strong>')<br/>        shutil.copy(f'<strong class="mu iu">{</strong>val_img<strong class="mu iu">}</strong>.png', <br/>                f'images/val/<strong class="mu iu">{</strong>val_img<strong class="mu iu">}</strong>.png')</span></pre><p id="ab9a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">来源:<a class="ae ky" href="https://www.kaggle.com/nxhong93/yolov5-chest-512" rel="noopener ugc nofollow" target="_blank">卡格尔</a></p><p id="fcfa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意:不要忘记保存在标签文本文件<strong class="lb iu">中的边界框的坐标必须被标准化(从0到1)。</strong>这很重要。此外，如果图像有多个注释，在文本文件中，每个注释(预测+边界框)将位于单独的一行。</p><p id="b83b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">之后，您需要一个配置文件，其中包含标签的名称、类的数量以及训练和验证路径。</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="29b9" class="lv lw it mu b gy my mz l na nb">import yaml<br/>classes = [ ‘Aortic enlargement’,<br/> ‘Atelectasis’,<br/> ‘Calcification’,<br/> ‘Cardiomegaly’,<br/> ‘Consolidation’,<br/> ‘ILD’,<br/> ‘Infiltration’,<br/> ‘Lung Opacity’,<br/> ‘Nodule/Mass’,<br/> ‘Other lesion’,<br/> ‘Pleural effusion’,<br/> ‘Pleural thickening’,<br/> ‘Pneumothorax’,<br/> ‘Pulmonary fibrosis’]</span><span id="44b2" class="lv lw it mu b gy nc mz l na nb">data = dict(<br/> train = ‘../vinbigdata/images/train’, # training images path<br/> val = ‘../vinbigdata/images/val’, # validation images path<br/> nc = 14, # number of classes<br/> names = classes<br/> )</span><span id="6f03" class="lv lw it mu b gy nc mz l na nb">with open(‘./yolov5/vinbigdata.yaml’, ‘w’) as outfile:<br/> yaml.dump(data, outfile, default_flow_style=False)</span></pre><p id="fb74" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，您需要做的就是运行这个命令:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="a145" class="lv lw it mu b gy my mz l na nb">python train.py — img 640 — batch 16 — epochs 30 — data ./vinbigdata.yaml — cfg models/yolov5x.yaml — weights yolov5x.pt</span></pre><h2 id="d57f" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated"><strong class="ak">根据经验需要注意的事项:</strong></h2><p id="897a" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">好了，现在我们已经略读了基础知识，让我们看一下重要的东西:</p><ol class=""><li id="b348" class="nd ne it lb b lc ld lf lg li nf lm ng lq nh lu ni nj nk nl bi translated">不要忘记标准化坐标</li><li id="69e7" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">如果您的初始性能比预期的差得多，发生这种情况的最可能的原因(我在许多其他竞争对手身上看到过这种情况)是您在预处理方面做了一些错误的事情。这看起来很琐碎，但有很多细节你必须要注意，尤其是如果你是第一次。</li><li id="1cf6" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">YoloV5有多个型号(yolov5s，yolov5m，yolov5l，yolov5x)，不要只挑最大的一个，因为它可能会过拟合。从中等水平的基线开始，并努力提高它。</li><li id="4f13" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">虽然我是在512张图片上训练的，但是我发现将-img标志作为640传递可以提高性能</li><li id="57ee" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">不要忘记加载预训练的重量(-重量标志)。迁移学习将极大地提高你的表现，并节省你大量的训练时间(在我的例子中，大约50个周期，每个周期大约需要20分钟！)</li><li id="9ff6" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">Yolov5x需要大量的内存，当在批量大小为4的512个图像上训练时，它需要大约14GB的GPU内存(大多数GPU都有8GB左右的内存)。</li><li id="5435" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">YoloV5 <strong class="lb iu">已经使用了增强，</strong>你可以选择你喜欢的或者不喜欢的，你所需要做的就是使用yolov5/data/hyp.scratch.yml</li><li id="2dd7" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">默认的yolov5训练脚本使用了<a class="ae ky" href="https://wandb.ai/site" rel="noopener ugc nofollow" target="_blank">权重和偏差</a>，老实说这给人留下了非常深刻的印象，它在模型训练时保存了你所有的指标<strong class="lb iu">。但是，如果您想关闭它，只需将WANDB_MODE="dryrun "添加到训练脚本标志中</strong></li><li id="c67f" class="nd ne it lb b lc nm lf nn li no lm np lq nq lu ni nj nk nl bi translated">我希望早点发现的一件事是，YoloV5将大量有用的指标保存到目录yolov5/runs/train/exp/中。训练后，您可以找到“混淆_矩阵. png”和“结果. png ”,其中results.png应该是这样的:</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/8b31af1267a93a6566ee261d67475582.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z3UyfpehiDMphieOZDqZ9Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由作者转载。可能最重要的两个指标是mAP@0.5和mAP@0.5:0.95</p></figure><h2 id="ff2a" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">用WBF预处理</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/916678260683702c59dfdc0277c1f068.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wDQLvDI74N9psmfk"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae ky" href="https://unsplash.com/@devetpan?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Siebe Warmoeskerken </a>拍摄</p></figure><p id="139e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好了，现在你已经调整了超参数，升级了你的模型，测试了多种图像尺寸和交叉验证。是时候介绍一些提高性能的技巧了。</p><p id="f9b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">加权盒子融合是一种在训练之前(清理数据集)或训练之后(使预测更准确)动态融合盒子的方法。如果你想知道更多，你可以看看我的文章:</p><div class="nt nu gp gr nv nw"><a rel="noopener follow" target="_blank" href="/wbf-optimizing-object-detection-fusing-filtering-predicted-boxes-7dc5c02ca6d3"><div class="nx ab fo"><div class="ny ab nz cl cj oa"><h2 class="bd iu gy z fp ob fr fs oc fu fw is bi translated">WBF:优化目标检测——融合和过滤预测框</h2><div class="od l"><h3 class="bd b gy z fp ob fr fs oc fu fw dk translated">加权盒融合已经成为优化目标检测模型的新SOTA</h3></div><div class="oe l"><p class="bd b dl z fp ob fr fs oc fu fw dk translated">towardsdatascience.com</p></div></div><div class="of l"><div class="og l oh oi oj of ok ks nw"/></div></div></a></div><p id="60a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要使用它来预处理数据集(大多数竞争对手的性能大约提高了10–20%)，您可以使用如下代码:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="d9cc" class="lv lw it mu b gy my mz l na nb">from ensemble_boxes import *</span><span id="af03" class="lv lw it mu b gy nc mz l na nb">for image_id <strong class="mu iu">in</strong> tqdm(df['image_id'], leave=False):<br/>        image_df = df[df['image_id']==image_id].reset_index(drop=True)<br/>        h, w = image_df.loc[0, ['height', 'width']].values<br/>        boxes = image_df[['x_min', 'y_min',<br/>                          'x_max', 'y_max']].values.tolist()<br/>        # Normalise all the bounding boxes (by dividing them  by size-1<br/>        boxes = [[j/(size-1) for j <strong class="mu iu">in</strong> i] for i <strong class="mu iu">in</strong> boxes]<br/>        scores = [1.0]*len(boxes) # set all of the scores to 1 since we only have 1 model here<br/>        labels = [float(i) for i <strong class="mu iu">in</strong> image_df['class_id'].values]</span><span id="46c0" class="lv lw it mu b gy nc mz l na nb">        boxes, scores, labels = weighted_boxes_fusion([boxes], [scores], [labels],weights=None,iou_thr=iou_thr,<br/>                                          skip_box_thr=skip_box_thr)</span><span id="00ab" class="lv lw it mu b gy nc mz l na nb">        list_image.extend([image_id]*len(boxes))<br/>        list_h.extend([h]*len(boxes))<br/>        list_w.extend([w]*len(boxes))<br/>        list_boxes.extend(boxes)<br/>        list_cls.extend(labels.tolist())<br/>    # bring the bounding boxes back to their original size  (by multiplying by size - 1)</span><span id="9754" class="lv lw it mu b gy nc mz l na nb">    list_boxes = [[int(j*(size-1)) for j <strong class="mu iu">in</strong> i] for i <strong class="mu iu">in</strong> list_boxes]<br/>    new_df['image_id'] = list_image<br/>    new_df['class_id'] = list_cls<br/>    new_df['h'] = list_h<br/>    new_df['w'] = list_w<br/>    # Unpack the coordinates from the  bounding boxes</span><span id="4b3f" class="lv lw it mu b gy nc mz l na nb">    new_df['x_min'], new_df['y_min'], \<br/>    new_df['x_max'], new_df['y_max'] = np.transpose(list_boxes)</span></pre><p id="d3b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这应该在将边界框坐标保存到注释文件之前完成。你也可以用同样的方法用YoloV5预测边界框后再尝试使用。</p><p id="d052" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，在训练YoloV5之后，运行:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="8596" class="lv lw it mu b gy my mz l na nb">!python detect.py — weights /runs/train/exp/weights\<br/> — img 640\<br/> — conf 0.005\<br/> — iou 0.45\<br/> — source $test_dir\<br/> — save-txt — save-conf — exist-ok</span></pre><p id="2bc8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后从以下位置提取方框、分数和标签:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="9fea" class="lv lw it mu b gy my mz l na nb">runs/detect/exp/labels</span></pre><p id="7d71" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">并将它们传递给:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="db9c" class="lv lw it mu b gy my mz l na nb">boxes, scores, labels = weighted_boxes_fusion([boxes], [scores], [labels],weights=None,iou_thr=iou_thr,<br/>                                          skip_box_thr=skip_box_thr)</span></pre><p id="46de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我不想包含所有的后处理代码，因为它包含了很多我正在做的比赛的细节。</p><h2 id="7993" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">最后的想法</h2><p id="bfc3" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我希望你已经了解了一些关于延长基线的事情，我认为最重要的事情是转移学习，图像增强，模型复杂性，前&amp;后处理技术。使用YoloV5，您可以轻松控制和使用这些方面来提升您的性能。</p></div></div>    
</body>
</html>