<html>
<head>
<title>Implementing PCA from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始实施 PCA</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementing-pca-from-scratch-ea3970714d2b?source=collection_archive---------7-----------------------#2021-12-10">https://towardsdatascience.com/implementing-pca-from-scratch-ea3970714d2b?source=collection_archive---------7-----------------------#2021-12-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="cfb0" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">将实现与 Scikit-Learn 的 PCA 进行比较</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ff671a5c864ff18c520f52c469b72858.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*35bAOUFcEVaI9bgj"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="http://@jeremythomasphoto" rel="noopener ugc nofollow" target="_blank">杰瑞米·托马斯</a>在<a class="ae ky" href="https://unsplash.com/photos/O6N9RV2rzX8" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><blockquote class="kz la lb"><p id="9b8d" class="lc ld le lf b lg lh ju li lj lk jx ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">本文是故事<a class="ae ky" href="https://pub.towardsai.net/machine-learning-1096c38e6a18?sk=12a7d697e584f20e224bbd78f805fbe2" rel="noopener ugc nofollow" target="_blank">的续篇</a>主成分分析变量约简。在上一篇文章中，我谈到了一种最著名和最广泛使用的方法，称为主成分分析。它采用高效的线性变换，在获取最大信息量的同时降低高维数据集的维数。它生成主成分，即数据集中原始要素的线性组合。此外，我一步一步地展示了如何用 Python 实现这项技术。起初我认为这篇文章足以解释 PCA，但我觉得缺少了一些东西。我使用单独的代码行实现了 PCA，但是当您每次想为不同的问题调用它们时，它们是低效的。更好的方法是创建一个类，当您想在一个地方封装数据结构和过程时，这是很有效的。此外，由于所有代码都在这个独特的类中，修改起来真的很容易。</p></blockquote><p id="19ce" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated"><strong class="lf iu">目录</strong>:</p><ol class=""><li id="1bbc" class="mc md it lf b lg lh lj lk lz me ma mf mb mg ly mh mi mj mk bi translated">资料组</li><li id="d4ad" class="mc md it lf b lg ml lj mm lz mn ma mo mb mp ly mh mi mj mk bi translated"><a class="ae ky" href="#8006" rel="noopener ugc nofollow">实施 PCA </a></li><li id="eee2" class="mc md it lf b lg ml lj mm lz mn ma mo mb mp ly mh mi mj mk bi translated"><a class="ae ky" href="#24ef" rel="noopener ugc nofollow">未标准化的 PCA</a></li><li id="d062" class="mc md it lf b lg ml lj mm lz mn ma mo mb mp ly mh mi mj mk bi translated"><a class="ae ky" href="#b5ac" rel="noopener ugc nofollow">标准化的 PCA</a></li><li id="41de" class="mc md it lf b lg ml lj mm lz mn ma mo mb mp ly mh mi mj mk bi translated"><a class="ae ky" href="#66df" rel="noopener ugc nofollow">带 Sklearn 的 PCA</a></li></ol></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h1 id="c3c0" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">1.资料组</h1><p id="15cc" class="pw-post-body-paragraph lc ld it lf b lg np ju li lj nq jx ll lz nr lo lp ma ns ls lt mb nt lw lx ly im bi translated">在实现 PCA 算法之前，我们将导入乳腺癌威斯康星数据集，该数据集包含关于在 569 名患者中诊断出的乳腺癌的数据<br/>。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="daaf" class="nz my it nv b gy oa ob l oc od"><em class="le">import</em> pandas <em class="le">as</em> pd<br/><em class="le">import</em> numpy <em class="le">as</em> np<br/><em class="le">import</em> random<br/><em class="le">from</em> sklearn.datasets <em class="le">import</em> load_breast_cancer<br/>import plotly.express as px</span><span id="0824" class="nz my it nv b gy oe ob l oc od">data = load_breast_cancer(as_frame=True)<br/>X,y,df_bre = data.data,data.target,data.frame<br/>diz_target = {0:'malignant',1:'benign'}<br/>y = np.array([diz_target[y1] for y1 in y])<br/>df_bre['target'] = df_bre['target'].apply(lambda x: diz_target[x])<br/>df_bre.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/48df57cd7efe10f9a8b51423ab0dcb81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TkxaHdDxZ9Ry9XKrTcG0Dg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者插图。</p></figure><p id="9a1c" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">我们可以注意到有 30 个数字特征和一个目标变量，指定肿瘤是良性的(目标=1)还是恶性的(目标=0)。我将目标变量转换为字符串，因为 PCA 不使用它，我们只在以后的可视化中需要它。</p><p id="a960" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">在这种情况下，我们希望了解肿瘤是良性还是恶性时，其特征可变性的差异。这真的很难用简单的探索性分析来显示，因为我们有两个以上的协变量。例如，我们可以设想一个只有六个特征的散布矩阵，用目标变量来着色。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="c63f" class="nz my it nv b gy oa ob l oc od">fig = px.scatter_matrix(df_bre,dimensions=list(df_bre.columns)[:5], color="target")<br/>fig.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/f85dd15d1e89923a27efcb37a3f3519e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qt6jS0WqXRQDVPSiXKeLpA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者插图。</p></figure><p id="3e61" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">当然，我们可以在所有这些散点图中观察到两个不同的集群，但是如果我们同时绘制所有的特征，这将是混乱的。因此，我们需要这个多元数据集的一个紧凑的表示，它可以由主成分分析提供。</p><h1 id="8006" class="mx my it bd mz na oh nc nd ne oi ng nh jz oj ka nj kc ok kd nl kf ol kg nn no bi translated">2.认证后活动的实施</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/6a2ef5fb2257d36723b38eb0bbdad592.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d52CQw0ETYnJcRmdw1pwcA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PCA 程序的框图。作者插图。</p></figure><p id="0bd1" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">上图总结了获取主成分(或 k 维特征向量)的步骤。将应用相同的逻辑来构建该类。</p><p id="0ddd" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">我们定义了<code class="fe on oo op nv b">PCA_impl</code>类，它在开始时初始化了三个属性。最重要的属性是我们想要提取的组件的数量。此外，我们还可以通过设置<code class="fe on oo op nv b">random_state</code>等于 True 并仅在需要时标准化数据集来每次重现相同的结果。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oq or l"/></div></figure><p id="36d0" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">这个类也包括两个方法，<code class="fe on oo op nv b">fit</code>和<code class="fe on oo op nv b">fit_transform</code>，类似于 scikit-learn 的 PCA。虽然第一种方法提供了计算主成分的大部分程序，但<code class="fe on oo op nv b">fit_transform</code>方法也对原始特征矩阵 x 进行了变换。除了这两种方法，我还想可视化主成分，而无需每次指定 Plotly 表达式的函数。加速 PCA 产生的潜在变量的分析是非常有用的。</p><h1 id="24ef" class="mx my it bd mz na oh nc nd ne oi ng nh jz oj ka nj kc ok kd nl kf ol kg nn no bi translated">3.未经标准化的 PCA</h1><p id="604c" class="pw-post-body-paragraph lc ld it lf b lg np ju li lj nq jx ll lz nr lo lp ma ns ls lt mb nt lw lx ly im bi translated">最后，定义了<code class="fe on oo op nv b">PCA_impl</code>类。我们只需要毫不费力地调用类和相应的方法。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oq or l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/07fe4728772a75b0d367e605c4efe9a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_TbAEfSSgCFqR_zrl-K4EA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者插图。</p></figure><p id="3e54" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">我们可以访问在<code class="fe on oo op nv b">fit</code>和<code class="fe on oo op nv b">fit_transform</code>方法中计算出的<code class="fe on oo op nv b">var_explained</code>和<code class="fe on oo op nv b">cum_var_explained</code>属性。值得注意的是，我们只用一个组件就捕获了 98%。让我们使用之前定义的方法来可视化 2D 和 3D 散点图:</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="9988" class="nz my it nv b gy oa ob l oc od">pca1.pca_plot2d()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/751edda0cdd389fbd58a207862e7fcc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KV9VPjdTPZVsIQHvOnVZXw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者插图。</p></figure><p id="fde8" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">从可视化中，我们可以观察到出现了两个聚类，一个用蓝色标记，代表患有恶性癌症的患者，另一个关于良性癌症。此外，蓝色星团似乎比其他星团包含更多的可变性。此外，我们看到两组之间有轻微的重叠。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="f50f" class="nz my it nv b gy oa ob l oc od">pca1.pca_plot3d()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/6c1f1c7dac3c5be0eef0e3635a93ae58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oZvamkEgLZ8iH35k4Jf-Hg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者插图。</p></figure><p id="f212" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">现在，我们来看看包含前三个部分的 3D 散点图。它没有之前的散点图那么清晰，但即使在这个图中也出现了类似的行为。基于目标变量，肯定有两个不同的组。通过观察这种三维表示，我们发现了新的信息:两名恶性肿瘤患者相对于所有其他患者而言，似乎具有完全不同的价值观。这一点在我们之前展示的 2D 图或散点图中可以稍微注意到。</p><h1 id="b5ac" class="mx my it bd mz na oh nc nd ne oi ng nh jz oj ka nj kc ok kd nl kf ol kg nn no bi translated">4.标准化的 PCA</h1><p id="5d8c" class="pw-post-body-paragraph lc ld it lf b lg np ju li lj nq jx ll lz nr lo lp ma ns ls lt mb nt lw lx ly im bi translated">让我们重复上一节的相同过程。我们只在开始时添加标准化，以检查结果中是否有任何差异。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oq or l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/02e74bd267cd07ee747bb70d417024d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yBh4j01tT31Pl6vNzkof9Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者插图。</p></figure><p id="9724" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">与前一种情况不同，我们可以注意到关于主成分的值的范围更受限制，并且解释的方差的 80%被三个成分捕获。特别地，第一成分的贡献从 0.99 变为 0.44。这可以通过以下事实来证明:所有变量都具有相同的度量单位，因此，PCA 能够对每个特征赋予相同的权重。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="298d" class="nz my it nv b gy oa ob l oc od">pca1.pca_plot2d()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/b1019b583e4f8b97a0afbba1a167ec7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kguYgN-BvZfOoLquSyHtMw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者插图。</p></figure><p id="4081" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">通过查看带有前两个分量的散点图，可以确认这些观察结果。聚类更加明显，并且具有更低的值。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="a3df" class="nz my it nv b gy oa ob l oc od">pca1.pca_plot3d()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/834a694ea7605ac35c2029bf3555492a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0CNEFUOpFJHElx1CjOwolg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者插图。</p></figure><p id="f5f3" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">3D 表示更容易阅读和理解。最后，我们可以得出结论，两组患者具有不同的特征变异性。此外，仍有两个数据点与其余数据分开。</p><h1 id="66df" class="mx my it bd mz na oh nc nd ne oi ng nh jz oj ka nj kc ok kd nl kf ol kg nn no bi translated">5.带 Sklearn 的 PCA</h1><p id="6f26" class="pw-post-body-paragraph lc ld it lf b lg np ju li lj nq jx ll lz nr lo lp ma ns ls lt mb nt lw lx ly im bi translated">此时，我们可以应用 Sklearn 实现的 PCA 与我的实现进行比较。我应该指出，在这种比较中有一些差异需要考虑。虽然我的 PCA 实现基于协方差矩阵，但 scikit-learn 的 PCA 涉及输入数据的<strong class="lf iu">居中</strong>，并采用<strong class="lf iu">奇异值分解</strong>将数据投影到更低维的空间。</p><p id="7b60" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">之前我们看到标准化是应用 PCA 前非常重要的一步。由于 Sklearn 的算法已经从每个特征的列中减去了平均值，所以我们只需要将每个数值变量除以它自己的标准偏差。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="f38f" class="nz my it nv b gy oa ob l oc od">X_copy = X.copy().astype('float32')<br/>X_copy /= np.std(X_copy, axis=0)</span></pre><p id="9b9e" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">现在，我们将组件数量和 random_state 传递给 PCA 类，并调用<code class="fe on oo op nv b">fit_transform</code>方法来获取主组件。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oq or l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/cfe0a9b0bbdad4ce34e31fad5c4c48e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mO4Y8qDCR7XslYgALOENFw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者插图。</p></figure><p id="ae34" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">sklearn 的 PCA 实现了与标准化实现的 PCA 相同的结果。</p><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="1230" class="nz my it nv b gy oa ob l oc od">fig = px.scatter(components, x=0, y=1, color=df.label,labels={'0': 'PC 1', '1': 'PC 2'})<br/>fig.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/2e1e24f04008d6a7928513511560ee07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7xW21EGal6ZIhYGncbhzWw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者插图。</p></figure><pre class="kj kk kl km gt nu nv nw nx aw ny bi"><span id="77b0" class="nz my it nv b gy oa ob l oc od">fig = px.scatter_3d(components, x=0, y=1,z=2, color=df.label,labels={'0': 'PC 1', '1': 'PC 2','2':'PC 3'})<br/>fig.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/b602beae81393f31251b62835a58db06.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m8rSqE7o7R65H0GMzEMBDA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者插图。</p></figure><p id="f04f" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">同样，散点图复制了我们在上一节中看到的内容。</p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h1 id="800d" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">最终想法:</h1><p id="889c" class="pw-post-body-paragraph lc ld it lf b lg np ju li lj nq jx ll lz nr lo lp ma ns ls lt mb nt lw lx ly im bi translated">我希望这篇文章对你有用。本文的目的是提供主成分分析的一个更紧凑的实现。在这种情况下，我的实现和 Sklearn 的 PCA 提供了相同的结果，但如果您使用不同的数据集，有时它们可能会略有不同。这里的 GitHub 代码是<a class="ae ky" href="https://github.com/eugeniaring/PCA-Implementation-with-Python/blob/main/pcaimplementation.ipynb" rel="noopener ugc nofollow" target="_blank"/>。感谢阅读。祝您愉快！</p><p id="0a1c" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated"><strong class="lf iu">参考文献:</strong></p><p id="698b" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">[1] <a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html" rel="noopener ugc nofollow" target="_blank">乳腺癌威斯康星州(诊断)数据集</a></p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><p id="2cc0" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">你喜欢我的文章吗？<a class="ae ky" href="https://eugenia-anello.medium.com/membership" rel="noopener"> <em class="le">成为会员</em> </a> <em class="le">每天无限获取数据科学新帖！这是一种间接的支持我的方式，不会给你带来任何额外的费用。如果您已经是会员，</em> <a class="ae ky" href="https://eugenia-anello.medium.com/subscribe" rel="noopener"> <em class="le">订阅</em> </a> <em class="le">每当我发布新的数据科学和 python 指南时，您都会收到电子邮件！</em></p></div></div>    
</body>
</html>