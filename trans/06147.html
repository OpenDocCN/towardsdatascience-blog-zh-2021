<html>
<head>
<title>Stochastic Gradient Descent explained in real life</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">现实生活中解释的随机梯度下降</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/stochastic-gradient-descent-explained-in-real-life-predicting-your-pizzas-cooking-time-b7639d5e6a32?source=collection_archive---------2-----------------------#2021-06-02">https://towardsdatascience.com/stochastic-gradient-descent-explained-in-real-life-predicting-your-pizzas-cooking-time-b7639d5e6a32?source=collection_archive---------2-----------------------#2021-06-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8d1e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">预测你的披萨的烹饪时间</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/853a8e323a7efa60d777b1608e8a6a17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xOyCnRulq3sqfIUejYaNYg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="5936" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">梯度下降</strong>是挑选最适合训练数据的模型的最流行方法之一。通常，这是最小化损失函数的模型，例如，最小化线性回归中的<a class="ae lr" href="https://en.wikipedia.org/wiki/Residual_sum_of_squares" rel="noopener ugc nofollow" target="_blank">残差平方和</a>。</p><p id="542a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">随机梯度下降</strong>是一种随机梯度下降自旋。它改进了梯度下降法的局限性，在大规模数据集上表现得更好。这就是为什么它被广泛用作像<a class="ae lr" href="https://en.wikipedia.org/wiki/Deep_learning" rel="noopener ugc nofollow" target="_blank">深度学习</a>这样的大规模、在线机器学习方法中的优化算法。</p><p id="7298" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但是为了更好地理解随机梯度下降，你需要从梯度下降部分开始。</p><h1 id="7b9f" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">柯西的遗产:梯度下降</h1><p id="cd21" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated"><strong class="kx ir">梯度下降</strong>和优化算法的历史并不是从现代机器学习开始的，它可以一直追溯到19世纪的法国。</p><p id="6649" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">法国数学家、工程师和物理学家奥古斯丁·柯西是复分析和抽象代数领域的先驱。他的影响永远印在数学和物理学中，有几个定理和概念以他的名字命名，如<a class="ae lr" href="https://en.wikipedia.org/wiki/Cauchy_distribution" rel="noopener ugc nofollow" target="_blank">柯西分布</a>、<a class="ae lr" href="https://en.wikipedia.org/wiki/Cauchy%27s_integral_theorem" rel="noopener ugc nofollow" target="_blank">柯西定理</a>或<a class="ae lr" href="https://en.wikipedia.org/wiki/Cauchy_elastic_material" rel="noopener ugc nofollow" target="_blank">柯西弹性材料</a>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/f987ffc822c26851ebc3c06ba78144ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*2MMjugk--BUUg-9RnqfKDw.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">法国数学家奥古斯丁-卢伊斯·柯西(<a class="ae lr" href="https://en.wikipedia.org/wiki/File:Augustin-Louis_Cauchy_1901.jpg" rel="noopener ugc nofollow" target="_blank">学分</a>)</p></figure><p id="e53b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在柯西的众多关键贡献中，据说他还发明了<a class="ae lr" href="https://en.wikipedia.org/wiki/Gradient_descent" rel="noopener ugc nofollow" target="_blank">梯度下降</a>。</p><p id="7d65" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">柯西的动机是他想不用解微分方程来计算天体的轨道，而是解代表天体运动的代数方程[1]。</p><h1 id="2ae4" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">梯度下降，最陡的下降</h1><p id="8ae4" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated"><strong class="kx ir">梯度下降</strong>也叫最速下降。该算法背后的思想是找到一种达到函数最小值的有效方法。</p><p id="731f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这正是你在机器学习模型中想要做的！</p><p id="c3be" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">模型的损失函数，通常被称为<strong class="kx ir">成本函数</strong>，告诉你模型拟合训练数据的好坏。成本函数越小，模型拟合得越好。</p><p id="4d2f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">梯度下降的特别之处在于，要找到一个函数的最小值，这个函数本身需要是一个<a class="ae lr" href="https://en.wikipedia.org/wiki/Differentiable_function" rel="noopener ugc nofollow" target="_blank">可微的</a> <a class="ae lr" href="https://en.wikipedia.org/wiki/Convex_function" rel="noopener ugc nofollow" target="_blank">凸函数</a>。这些名字听起来很复杂，但是有一种直觉。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/ed1f84efb5e1eb8ec70c3b3f5b0eba56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y0N923n5SFspV4SkzKZSwA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="mr">凸函数和非凸函数、可微函数和不可微函数的例子。</em></p></figure><h1 id="b458" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">凸函数</h1><p id="8174" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">当你听到凸函数时，想象一个形状像碗的函数。</p><p id="88d9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最简单的凸函数是抛物线。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/ec1b68664a33c06ab9bdcf4fb1a14778.png" data-original-src="https://miro.medium.com/v2/resize:fit:1044/format:webp/1*G5-tvY59V21F0bpftaszzA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">抛物线的例子。</p></figure><h1 id="843b" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">可微分函数</h1><p id="7997" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">一个<a class="ae lr" href="https://en.wikipedia.org/wiki/Differentiable_function" rel="noopener ugc nofollow" target="_blank">可微的</a>函数是任何给定的函数，它对它的每个点都有导数。</p><p id="ceb4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">函数在<em class="mt"> x </em>的导数是该函数在点(x，f(x))的图形的斜率。</p><p id="b644" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们来解开<strong class="kx ir">导数</strong>的概念。</p><p id="b141" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">线性函数的<strong class="kx ir">斜率是它上升或下降的速率</strong>，你可以认为它已经描述了那个函数的方向。但是这种直觉只对线性函数有效。</p><p id="0bdc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于非线性函数，不同点的斜率可能不同。要找到非线性函数在特定点<em class="mt"> p </em>的斜率，你需要找到 <strong class="kx ir">切线在<em class="mt"> p </em> </strong>点的斜率<strong class="kx ir">。</strong></p><blockquote class="mu mv mw"><p id="d7e5" class="kv kw mt kx b ky kz jr la lb lc ju ld mx lf lg lh my lj lk ll mz ln lo lp lq ij bi translated">函数在点<em class="iq"> p </em>的切线是最接近该点函数的直线。</p></blockquote><p id="a55f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">正切是一个近似值，因此，寻找特定点的斜率正切，就像目测该点的函数斜率[2]。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi na"><img src="../Images/13baa03fa174ea9dc19ae330dcae1226.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r2fEL7yvjufMXWbGpk3W5g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">点<em class="mr"> p </em> (x，f(x))处的切线示例。</p></figure><p id="987f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了增加切线斜率的精确度，你需要找到点<em class="mt"> p. </em>处割线的<strong class="kx ir">斜率</strong></p><blockquote class="mu mv mw"><p id="7109" class="kv kw mt kx b ky kz jr la lb lc ju ld mx lf lg lh my lj lk ll mz ln lo lp lq ij bi translated">割线是逼近切线的一种更精确的方法。</p></blockquote><p id="bfe1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">割线仍然是一个近似值，但它更精确，因为它使用了两个点，切点和第二个点。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nb"><img src="../Images/4fa2274051c3a101820b235ac08b7cfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*348SK8lRU4KqSoO_epGfKw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">割线的斜率。</p></figure><p id="9fe8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当选择的第二个点越接近切点时，近似效果就越好。</p><p id="6874" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，为了找到函数<em class="mt"> f </em>在切点<em class="mt"> p </em> = (x，f(x))处的<em class="mt">精确斜率，当x的变化极限趋于零时，你就找到了<strong class="kx ir">割线。</strong></em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/13180e694b99ee00e4585aef1218acdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:774/format:webp/1*coyDltxXqEUICaCKwD-vHQ.png"/></div></figure><p id="0b2a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果极限存在，那就是函数f在点(x，f(x)) 的<strong class="kx ir">导数。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/5850ff4b8e24259194a9fb48c861311b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eySx82D7Yp3ApTXxmuL19A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">函数<em class="mr"> f </em>在点<em class="mr"> p </em>的导数。</p></figure><p id="b03d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但是你可能想知道，所有这些和梯度下降有什么关系？</p><h1 id="b8f0" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">到底什么是梯度？</h1><p id="29a6" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">函数的<strong class="kx ir">梯度是组织成向量【3】的所有偏导数的集合，用一个倒三角形表示，称为<a class="ae lr" href="https://en.wikipedia.org/wiki/Nabla_symbol" rel="noopener ugc nofollow" target="_blank"> nabla </a>。</strong></p><p id="6b8a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在机器学习模型中，您可以将梯度视为偏导数的集合，每个偏导数都与模型的一个特征相关。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/cf48dcbb135b8f7573f38bb92030b552.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*76qj0EwmL-Y1ulDNguBanw.png"/></div></div></figure><p id="5829" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">函数的梯度总是指向函数中增加最大的方向</strong>。如果你的函数是一座山，梯度总是想要到达顶峰。</p><p id="be6e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">要了解梯度如何总是朝着最大增加的方向发展，您可以用Python编写一个简单的测试。</p><p id="46e3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">定义一个二次函数，然后计算几次梯度。</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="761d" class="nk lt iq ng b gy nl nm l nn no">def gradient_loop(runs=3):<br/>    <em class="mt">""" Repeatedly computes the gradient of a function<br/>        Computes the gradient given the starting points and then uses the result of the gradient to feed the next iteration, with new points.<br/>        Prints out the result of the function at each iteration<br/>        :param: runs: number of iterations to compute<br/>    """</em></span><span id="2529" class="nk lt iq ng b gy np nm l nn no">    # starting points<br/>    x = np.array([1, 2, 3])<br/>    <br/>    # quadratic function, a parabola<br/>    y = x**2<br/>    <br/>    for run in range(0, runs):<br/>        print("Iter " + str(run) + ": Y=" + str(y))</span><span id="448d" class="nk lt iq ng b gy np nm l nn no">        # compute first derivative<br/>        x = np.gradient(y, 1)</span><span id="1005" class="nk lt iq ng b gy np nm l nn no">        # update the function output<br/>        y = x ** 2<br/></span><span id="47f1" class="nk lt iq ng b gy np nm l nn no">gradient_loop()</span></pre><p id="bf1b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这种情况下，函数<em class="mt"> gradient_loop </em>取一条抛物线<em class="mt"> y = x，</em>和一些由<em class="mt"> x </em>表示的点。然后，它计算函数在这些点上的梯度，并将结果作为下一次迭代的点集。</p><p id="2b44" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">由于梯度<em class="mt">总是想要到达山的顶峰</em>，你可以看到函数值在每次迭代中增加。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/a829456bb7376a726e0e471ca9dacd7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*eAsvnh2x5tOO37_JjgR0PQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">梯度环路测试的输出。</p></figure><p id="a1af" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你正在寻找一个函数的最大值，这将是完美的，但是在梯度下降中你想要找到最小值，你想要到达山的<em class="mt">底部。</em></p><p id="df45" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">所以，在梯度下降中，你需要让梯度往反方向走[4]。</p><h1 id="05bd" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">梯度下降算法</h1><p id="fb70" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">在非常高的层次上，或者在伪代码中，梯度下降遵循以下步骤:</p><ol class=""><li id="0d91" class="nr ns iq kx b ky kz lb lc le nt li nu lm nv lq nw nx ny nz bi translated">在函数中选择一个随机点<em class="mt"> w </em>，这是起点</li><li id="00a7" class="nr ns iq kx b ky oa lb ob le oc li od lm oe lq nw nx ny nz bi translated">当梯度没有收敛时:</li></ol><p id="3381" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">2a。计算在<em class="mt"> w </em>处的负梯度，梯度向相反方向。</p><p id="422e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">2b。用2a的结果更新点<em class="mt"> w </em>，并返回步骤1。</p><p id="06e5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">3.成功，你已经找到了最小值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/43cb1e76922e060c3445df1ff802d5d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sUj0ZEau1ooTNg6bnYlebg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">梯度下降沿函数向下爬升，直到达到绝对最小值。</p></figure><p id="7773" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> 1。在功能</strong>中选择一个随机点</p><p id="6038" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在初始步骤中，你只需要在函数中选取一个随机点，用行向量<em class="mt"> w </em>表示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/332cac339dd741eb3ecb29670034c774.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*h5VjHbeDJRBlMIr65SlzGw.png"/></div></figure><p id="3da4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> 2。而梯度还没有收敛</strong></p><p id="5358" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是算法的迭代部分，想法是继续下去，直到你达到最小值。</p><p id="1579" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但是你不会知道什么时候停下来。你运行这个算法的全部原因是为了找到最小值。</p><p id="ce76" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了减轻这种情况，您设置了一个<strong class="kx ir">收敛阈值</strong>。</p><p id="9416" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果与前一次迭代相比，点<em class="mt"> w </em>的新梯度变化没有超过您定义的收敛阈值，则算法已经收敛。你宣布你已经找到了这个函数的最小值。</p><p id="ce15" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，虽然梯度没有收敛，但在每次迭代中，你会收敛</p><ol class=""><li id="257c" class="nr ns iq kx b ky kz lb lc le nt li nu lm nv lq nw nx ny nz bi translated"><strong class="kx ir">计算<em class="mt"> w </em>处的负梯度。</strong></li><li id="3681" class="nr ns iq kx b ky oa lb ob le oc li od lm oe lq nw nx ny nz bi translated"><strong class="kx ir">用2a的结果更新点<em class="mt"> w </em>，并返回步骤1。</strong></li></ol><p id="0c32" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从数学上来说，在每次迭代中，你运行一个<strong class="kx ir">更新规则</strong>，在这里你定义你要下降的下一个点，一个在函数中<em class="mt">更靠下的点。</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/ffadba32b47a18070a91f85ec1062fcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*Fq70-itiANiVovGFUkfc-g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="mr">步骤t+1的梯度下降更新规则。</em></p></figure><p id="2de9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这就是为什么函数是凸的至关重要。如果函数不是凸的，你可能认为你已经找到了绝对最小值，但是你实际上是在一个局部最小值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/0a1fec87cdb880f9c808a7e41fde6bb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*E3Kqfi5KmyhiBtLILxh-Gg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">梯度下降出错的例子。在非凸函数中，梯度下降可能会找到<em class="mr">局部最小值</em>而不是函数的绝对最小值。</p></figure><p id="b74c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">查看更新规则，有当前点<em class="mt"> w </em>，梯度，然后有学习率α。</p><h1 id="5754" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">这个学习率α是多少？</h1><p id="953b" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">学习率α是梯度下降在达到全局最小值之前一直采用的步长，它直接影响算法的性能。</p><h2 id="7fe6" class="nk lt iq bd lu oj ok dn ly ol om dp mc le on oo me li op oq mg lm or os mi ot bi translated">学习率太大</h2><p id="2ffb" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">当学习速度太快时，你就迈出了大步。你可以跨过最小值几次，然后来回几次。在大步长的情况下，梯度下降甚至可能永远不会达到最小值和收敛。</p><p id="bfc5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">发现alpha太大的一个方法是当梯度不是在每一步都减小的时候。</p><h2 id="624a" class="nk lt iq bd lu oj ok dn ly ol om dp mc le on oo me li op oq mg lm or os mi ot bi translated">学习率太小</h2><p id="a0ff" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">当学习速度太慢时，你每次只能迈出很小的一步。算法最终会收敛，但这可能需要很长时间。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ou"><img src="../Images/c41ba34297121e00ad981f04fb92082d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*93pgaCG4DeKohag8ZkS-uA.jpeg"/></div></div></figure><h1 id="f105" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">梯度下降的局限性</h1><p id="09c3" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">梯度下降是一个强大的算法。但是在数据集不断增长的现实世界中，它有两个主要限制:</p><ul class=""><li id="085c" class="nr ns iq kx b ky kz lb lc le nt li nu lm nv lq ov nx ny nz bi translated">计算整个数据集的导数是耗时的，</li><li id="f28e" class="nr ns iq kx b ky oa lb ob le oc li od lm oe lq ov nx ny nz bi translated">所需的内存与数据集的大小成正比。</li></ul><p id="814d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">梯度下降的更新规则应用于数据集中的所有数据点，这一切都在一个步骤中完成。随着数据集的增加，这种计算会变得更慢，因此收敛时间会增加。</p><p id="c54d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这些计算通常在内存中完成，因此数据集越大，内存需求就越大[5]。</p><p id="009f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这就是随机梯度下降的用武之地！</p><h1 id="57dc" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">随机梯度下降</h1><p id="6eb0" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">考虑到梯度下降的局限性，<a class="ae lr" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" rel="noopener ugc nofollow" target="_blank">随机梯度下降</a>成为解决性能问题和加速大型数据集收敛的一种方法。</p><p id="8e10" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">随机梯度下降是梯度下降的概率近似。这是一个近似值，因为在每一步中，算法都会计算随机选取的一个观察值的梯度，而不是计算整个数据集的梯度。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/3f4956aa91df69548a08909d2e947ab7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*iyL9xWO7YyQGWEC46Gantw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="mr">步骤t+1的随机梯度下降更新规则。</em></p></figure><p id="b135" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当数据集包含数百万个观察值时，这代表了显著的性能改进。</p><p id="cfa4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">随机梯度下降的唯一条件是随机选取的观测值的期望值是函数在点<em class="mt"> w </em>的<a class="ae lr" href="https://en.wikipedia.org/wiki/Subderivative" rel="noopener ugc nofollow" target="_blank">次梯度</a>。</p><p id="531e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">与梯度下降法相比，随机梯度下降法速度更快，更适合大规模数据集。</p><p id="9021" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但由于梯度不是针对整个数据集计算的，而是针对每次迭代中的一个随机点计算的，因此更新的方差更大。与梯度下降相比，这使得成本函数在每次迭代中波动更大，使得算法更难收敛。</p><p id="5a80" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">随机梯度下降的新变化已被开发来解决这些问题。例如<em class="mt">小批量</em>随机梯度下降，通过在每次迭代中从数据集中选取一个<em class="mt"> n </em>个观察值的样本而不是一个【5】来解决方差问题。</p><p id="6eca" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们看看随机梯度下降的作用！</p><h1 id="41ff" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">预测比萨饼的烹饪时间</h1><p id="a375" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">谈到完美的自制披萨，你和你的好朋友有不同的偏好。</p><p id="d27c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最近，在你们的一次群聊中，有人开始了关于如何制作完美披萨的讨论，并暗示他们已经找到了自己最喜欢的食谱。</p><p id="0136" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">他们甚至可以根据不同的配料组合来确定烹饪时间，每次都能得到可预测的结果。</p><p id="814e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一段时间以来，你也一直试图完善你的食谱。但是，你可以用精确的烹饪时间制作你个人最喜欢的披萨，这个想法很有趣。</p><p id="837d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">那么，为什么不利用你的数据科学技能来解决这个问题呢？</p><p id="b2c3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">操作第一顺序，开始实验，记笔记！尽管你还不知道正确的比例，但有四样东西可以做出完美的披萨:</p><ul class=""><li id="bc56" class="nr ns iq kx b ky kz lb lc le nt li nu lm nv lq ov nx ny nz bi translated">烤箱温度，</li><li id="e15a" class="nr ns iq kx b ky oa lb ob le oc li od lm oe lq ov nx ny nz bi translated">奶酪，</li><li id="106b" class="nr ns iq kx b ky oa lb ob le oc li od lm oe lq ov nx ny nz bi translated">酱，</li><li id="56af" class="nr ns iq kx b ky oa lb ob le oc li od lm oe lq ov nx ny nz bi translated">额外的配料。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ow"><img src="../Images/425fc7ec8d0ada2e5cddeb1368d091f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hSx_xlNTTzmWwZM1FV1OCw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">制作完美自制披萨的要素。</p></figure><p id="30fa" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">回顾您详细的烹饪领域笔记，您的数据集如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ox"><img src="../Images/7ecb81e4081a3a251f8cd9f6b45dc87b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cgr90McnaoFdByxL5FSTPg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">你最近做的一些自制披萨实验。</p></figure><p id="586b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">你还假设烹饪时间和制作完美披萨的<em class="mt">特征</em>之间存在线性关系。因此，为了预测您的完美自制披萨的烹饪时间，您决定使用<a class="ae lr" href="https://en.wikipedia.org/wiki/Linear_regression" rel="noopener ugc nofollow" target="_blank">线性回归</a>模型，并使用随机梯度下降来最小化成本函数<a class="ae lr" href="https://en.wikipedia.org/wiki/Ordinary_least_squares" rel="noopener ugc nofollow" target="_blank">普通最小二乘法</a>。</p><p id="68d6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">ScikitLearn有非常全面的关于随机梯度下降和不同可用函数的<a class="ae lr" href="https://scikit-learn.org/stable/modules/sgd.html" rel="noopener ugc nofollow" target="_blank">文档。还有足够的空间来调整不同的参数。</a></p><p id="ba52" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">您将使用整个数据集来训练模型，然后预测披萨的烹饪时间，该披萨具有:</p><ul class=""><li id="9f04" class="nr ns iq kx b ky kz lb lc le nt li nu lm nv lq ov nx ny nz bi translated">烤箱温度:510华氏度，</li><li id="496f" class="nr ns iq kx b ky oa lb ob le oc li od lm oe lq ov nx ny nz bi translated">50克奶酪，</li><li id="538f" class="nr ns iq kx b ky oa lb ob le oc li od lm oe lq ov nx ny nz bi translated">35g酱，</li><li id="fdd1" class="nr ns iq kx b ky oa lb ob le oc li od lm oe lq ov nx ny nz bi translated">10克你喜欢的配料。</li></ul><p id="7755" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在模型适合数据集之前，您需要使用<a class="ae lr" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html" rel="noopener ugc nofollow" target="_blank">标准缩放器</a>缩放您的要素。因为我们的目标是逐步达到函数的最小值，所以让所有的特征都处于相同的尺度有助于这个过程。它使得每个步骤在所有特征上具有相同的<em class="mt">尺寸</em>。</p><p id="2068" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">使用<a class="ae lr" href="https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html" rel="noopener ugc nofollow" target="_blank">管线</a>，保证在使用模型之前缩放所有特征。<a class="ae lr" href="https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline" rel="noopener ugc nofollow" target="_blank">Python中的管道</a>用于链接应用于估计器的变换，在这种情况下是随机梯度下降。</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="20ae" class="nk lt iq ng b gy nl nm l nn no">import time<br/>import numpy as np<br/>from sklearn.linear_model import SGDRegressor<br/>from sklearn.pipeline import make_pipeline<br/>from sklearn.preprocessing import StandardScaler</span><span id="4217" class="nk lt iq ng b gy np nm l nn no">def stochastic_gradient_descent(feature_array, target_array, to_predict, learn_rate_type="invscaling"):<br/>    <em class="mt">""" Computes Ordinary Least SquaresLinear Regression with Stochastic Gradient Descent as the optimization algorithm.<br/>        :param feature_array: array with all feature vectors used to train the model<br/>        :param target_array: array with all target vectors used to train the model<br/>        :param to_predict: feature vector that is not contained in the training set. Used to make a new prediction<br/>        :param learn_rate_type: algorithm used to set the learning rate at each iteration.<br/>        :return: Predicted cooking time for the vector to_predict and the R-squared of the model.<br/>"""</em></span><span id="10b7" class="nk lt iq ng b gy np nm l nn no">    # Pipeline of transformations to apply to an estimator. First applies Standard Scaling to the feature array.<br/>    # Then, when the model is fitting the data it runs Stochastic Gradient Descent as the optimization algorithm.<br/>    # The estimator is always the last element.<br/>    <br/>    start_time = time.time()<br/>    linear_regression_pipeline = make_pipeline(StandardScaler(), SGDRegressor(learning_rate=learn_rate_type))<br/>    <br/>     linear_regression_pipeline.fit(feature_array, target_array)<br/>     stop_time = time.time()<br/>     <br/>     print("Total runtime: %.6fs" % (stop_time - start_time))<br/>     print("Algorithm used to set the learning rate: " + learn_rate_type)<br/>     print("Model Coeffiecients: " + str(linear_regression_pipeline[1].coef_))<br/>     print("Number of iterations: " + str(linear_regression_pipeline[1].n_iter_))</span><span id="0c63" class="nk lt iq ng b gy np nm l nn no">    # Make a prediction for a feature vector not in the training set<br/>    prediction = np.round(linear_regression_pipeline.predict(to_predict), 0)[0]<br/>    print("Predicted cooking time: " + str(prediction) + " minutes")</span><span id="92c8" class="nk lt iq ng b gy np nm l nn no">    r_squared = np.round(linear_regression_pipeline.score(feature_array, target_array).reshape(-1, 1)[0][0], 2)<br/>    print("R-squared: " + str(r_squared))</span><span id="06e2" class="nk lt iq ng b gy np nm l nn no">feature_array = [[500, 80, 30, 10],<br/>                 [550, 75, 25, 0],<br/>                 [475, 90, 35, 20],<br/>                 [450, 80, 20,25],<br/>                 [465, 75, 30, 0],<br/>                 [525, 65, 40, 15],<br/>                 [400, 85, 33, 0],<br/>                 [500, 60, 30, 30],<br/>                 [435, 45, 25, 0]]</span><span id="4b3f" class="nk lt iq ng b gy np nm l nn no">target_array = [17, 11, 21, 23, 22, 15, 25, 18, 16]<br/>to_predict = [[510, 50, 35, 10]]</span><span id="7529" class="nk lt iq ng b gy np nm l nn no">stochastic_gradient_descent(feature_array, target_array, to_predict)</span></pre><p id="0879" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">你的披萨需要13分钟才能做好，算法需要249次迭代才能找到最小值！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oy"><img src="../Images/2ced3d212a1540e83f0a59c0124b1fad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pCioCHl4GD6JC8w_bnmYhQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">带有随机梯度下降的线性回归的输出，使用<em class="mr">逆缩放</em>选择学习率。</p></figure><p id="cbe7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在回归的<a class="ae lr" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html" rel="noopener ugc nofollow" target="_blank">随机梯度下降的Python实现中，选择学习率的默认算法是<em class="mt">逆缩放。</em></a></p><p id="2ad2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但是有许多选择学习率的方法，这直接影响算法的执行。</p><h1 id="e4fc" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">如何选择学习速度</h1><p id="8988" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">根据问题和手头的数据集，可以使用不同的算法来选择学习率。</p><p id="f29d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最简单的<em class="mt">算法</em>是<strong class="kx ir">选择一个常数</strong>，一个足够小的数，比如10^-3.你开始使用它，并逐渐增加或减少它取决于如何快速或缓慢梯度下降找到最小值。</p><p id="6901" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但是，理想情况下，你会希望有一个自动的方法来选择学习率。而不是基于常数并通过反复试验来调整。</p><p id="cc1e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">例如，你可以使用一个<strong class="kx ir">自适应算法</strong>，它根据之前迭代的梯度来选择学习速率。例如，Adam[6] Adadelta[7]使用过去梯度的衰减平均值。</p><p id="707c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">重新运行上面的代码，现在有了一个<em class="mt">自适应的</em>学习速率，只需要改变<em class="mt"> learn_rate_type </em>参数。</p><pre class="kg kh ki kj gt nf ng nh ni aw nj bi"><span id="4193" class="nk lt iq ng b gy nl nm l nn no">stochastic_gradient_descent(feature_array, target_array, to_predict, learn_rate_type="adaptive")</span></pre><p id="b9bd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">你的披萨还是要13分钟才能做好，但是随机梯度下降找到最小值只需要98次迭代！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oy"><img src="../Images/62a56460bcbd5359479d9404f71f56a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pc_YDTQKqH4thpGklP24YA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用<em class="mr">自适应</em>算法选择学习率的随机梯度下降线性回归的输出。</p></figure><p id="638e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">预测的烹饪时间仍然相同，但有几处改进:</p><ul class=""><li id="ac25" class="nr ns iq kx b ky kz lb lc le nt li nu lm nv lq ov nx ny nz bi translated">r平方较高，意味着模型更适合该数据集。</li><li id="8c43" class="nr ns iq kx b ky oa lb ob le oc li od lm oe lq ov nx ny nz bi translated">总运行时间低于使用<em class="mt">反向缩放</em>时的运行时间。</li><li id="869c" class="nr ns iq kx b ky oa lb ob le oc li od lm oe lq ov nx ny nz bi translated">该模型收敛速度更快，只需要减少60%的迭代次数。</li></ul><p id="b897" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这个小数据集中，运行时间和迭代次数的差异是很小的，实际上可以忽略不计。</p><p id="dae2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但是你可以想象在一个拥有数千个特征和数百万个观测值的数据集中改变学习率算法的影响！</p></div><div class="ab cl oz pa hu pb" role="separator"><span class="pc bw bk pd pe pf"/><span class="pc bw bk pd pe pf"/><span class="pc bw bk pd pe"/></div><div class="ij ik il im in"><p id="2778" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">你刚刚用随机梯度下降法模拟了你最喜欢的披萨食谱的烹饪时间！</p><p id="ac85" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是一个简单的例子，但我希望你喜欢它！现在，您对梯度下降的威力以及随机梯度下降如何更好地解决现实世界的大规模问题有了更好的理解。</p><p id="c7db" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="mt">感谢阅读！</em></p><h1 id="d92b" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">参考</h1><p id="4f14" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">[1]莱马里查尔，C. (2012年)。<a class="ae lr" href="https://www.math.uni-bielefeld.de/documenta/vol-ismp/40_lemarechal-claude.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="mt">柯西与梯度法</em> </a>。Doc Math Extra:251–254</p><p id="eb55" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[2] Larson，R. &amp; Edwards B. (2006) <em class="mt">微积分:一种应用方法</em>(第7版。)霍顿·米夫林</p><p id="ee1e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae lr" href="https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/the-gradient" rel="noopener ugc nofollow" target="_blank"/>，可汗书院</p><p id="574f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[4]沙莱夫-施瓦茨和本-戴维(2014年)。<em class="mt">理解机器学习:从理论到算法</em>。剑桥:剑桥大学出版社。</p><p id="8ea9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[5]鲁德，S. (2017)。<a class="ae lr" href="https://arxiv.org/abs/1609.04747" rel="noopener ugc nofollow" target="_blank">梯度下降优化算法概述</a>。arXiv预印本arXiv:1609.04747</p><p id="1f7f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[6] Diederik P. Kingma和Jimmy Ba (2017) <a class="ae lr" href="https://arxiv.org/abs/1412.6980" rel="noopener ugc nofollow" target="_blank">亚当:随机优化的一种方法</a> arXiv预印本arXiv:1412.6980</p><p id="a284" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[7]泽勒，马修·d .(2012)<a class="ae lr" href="https://arxiv.org/abs/1212.5701" rel="noopener ugc nofollow" target="_blank">阿达德尔塔:一种自适应学习率方法</a>。arXiv预印本arXiv:1212.5701。</p><p id="9261" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Rie Johnson和张彤。2013.使用预测方差缩减加速随机梯度下降。第26届神经信息处理系统国际会议论文集第1卷(NIPS'13)。</p><p id="f7ae" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="mt">图片由作者提供，除非另有说明。</em></p></div></div>    
</body>
</html>