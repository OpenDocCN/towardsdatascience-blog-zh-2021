<html>
<head>
<title>Over-smoothing issue in graph neural network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">图形神经网络中的过平滑问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/over-smoothing-issue-in-graph-neural-network-bddc8fbc2472?source=collection_archive---------8-----------------------#2021-06-06">https://towardsdatascience.com/over-smoothing-issue-in-graph-neural-network-bddc8fbc2472?source=collection_archive---------8-----------------------#2021-06-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/cbb031d9bb865385cd958c77f5c06856.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QIqYDdypluF8hTnQeYxKiQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">[作者插图]</p></figure><div class=""/><p id="6025" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg"> TLDR </strong>:这个故事给出了图形神经网络的一个高级入口:如何和为什么，然后介绍伴随着消息传递框架的一个严重问题，它代表了当今GNN的主要特征。别忘了使用下面的参考资料来加深你对GNNs的理解！</p></div><div class="ab cl la lb hu lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="ij ik il im in"><h1 id="fef1" class="lh li jf bd lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated">图形神经网络图解指南</h1><p id="5fb2" class="pw-post-body-paragraph kc kd jf ke b kf mf kh ki kj mg kl km kn mh kp kq kr mi kt ku kv mj kx ky kz ij bi translated">图形神经网络或简称GNN是用于图形数据的深度学习(DL)模型。最近几年，它们变得相当热门。这种趋势在DL领域并不新鲜:每年我们都会看到一个新模型脱颖而出，要么在基准测试中显示最先进的结果，要么在已经使用的模型中显示全新的机制/框架<em class="mk">(但当你阅读它时会非常直观)</em>。这种反思让我们质疑这种致力于图形数据的新模型存在的理由。</p><h2 id="957c" class="ml li jf bd lj mm mn dn ln mo mp dp lr kn mq mr lv kr ms mt lz kv mu mv md mw bi translated">我们为什么需要GNNs？</h2><p id="957e" class="pw-post-body-paragraph kc kd jf ke b kf mf kh ki kj mg kl km kn mh kp kq kr mi kt ku kv mj kx ky kz ij bi translated"><strong class="ke jg">图形无处不在:</strong>图形数据丰富，相信是最<strong class="ke jg">自然</strong>和<strong class="ke jg">灵活</strong>的方式来呈现我们每天产生或消耗的素材。我们无需费力去列举一整套图表数据示例，从大多数公司和社交网络(如脸书或Twitter)中使用的关系数据库，到链接科学和文学中的知识创造的引用图表。由于图像的网格结构，甚至图像也可以被视为图形。</p><figure class="my mz na nb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mx"><img src="../Images/bca4e35316053719726ca8b6be79f01d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w-MS5vGdIrCN15UZxuyZSw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">对于关系数据库，实体是节点，关系(一对一，一对多)定义了我们的边。至于图像，像素是节点，相邻像素可以用来定义边缘[作者举例]</p></figure><p id="c6d2" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">能够捕捉图形中所有可能信息的模型</strong>:正如我们所看到的，图形数据无处不在，并且采用具有特征向量的互连节点的形式。是的，我们可以使用一些<a class="ae nc" href="https://en.wikipedia.org/wiki/Multilayer_perceptron#:~:text=A%20multilayer%20perceptron%20(MLP)%20is,artificial%20neural%20network%20(ANN).&amp;text=An%20MLP%20consists%20of%20at,uses%20a%20nonlinear%20activation%20function." rel="noopener ugc nofollow" target="_blank">多层感知器模型</a>来解决我们的下游任务，但是我们将失去图拓扑提供给我们的连接。至于卷积神经网络，它们的机制致力于图的特殊情况:网格结构的输入，其中节点是完全连接的，没有稀疏性。也就是说，唯一剩下的解决方案是<strong class="ke jg">一个可以建立在两者中给出的信息之上的模型:我们的图中的节点特征和本地结构，</strong>这可以减轻我们的下游任务；这正是GNN所做的。</p><h2 id="ba5c" class="ml li jf bd lj mm mn dn ln mo mp dp lr kn mq mr lv kr ms mt lz kv mu mv md mw bi translated"><strong class="ak">GNNs训练什么任务？</strong></h2><p id="6566" class="pw-post-body-paragraph kc kd jf ke b kf mf kh ki kj mg kl km kn mh kp kq kr mi kt ku kv mj kx ky kz ij bi translated">既然我们已经适度地证明了这种模型的存在，我们将揭示它们的用法。事实上，我们可以在很多任务上训练GNNs:大型图中的节点分类(<em class="mk">根据属性及其关系</em>对社交网络中的用户进行细分)，或整个图分类(为药物应用对蛋白质结构进行分类)。除了分类之外，回归问题也可以在图形数据之上公式化，不仅在节点上工作，也在边上工作。</p><p id="6760" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">总之，图形神经网络的应用是无止境的，并且取决于用户的目标和他们拥有的数据类型。为了简单起见，我们将集中于唯一图中的节点分类任务，其中我们尝试将由节点的特征向量领导的节点图的子集映射到一组预定义的类别/类。</p><p id="3a4a" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">该问题假设存在一个训练集，其中我们有一组带标签的节点，并且我们图中的所有节点都有一个特定的特征向量，我们记为x。我们的目标是预测验证集中特征节点的标签。</p><figure class="my mz na nb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nd"><img src="../Images/d8778d7ea6f8c967811bc976e2a14328.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zal-hlhIh8Xfe2sav8Js9g.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">节点分类示例:所有节点都有一个特征向量；彩色节点被标记，而白色节点未被标记[作者插图]</p></figure><h2 id="52b0" class="ml li jf bd lj mm mn dn ln mo mp dp lr kn mq mr lv kr ms mt lz kv mu mv md mw bi translated">引擎盖下的GNN</h2><p id="5a40" class="pw-post-body-paragraph kc kd jf ke b kf mf kh ki kj mg kl km kn mh kp kq kr mi kt ku kv mj kx ky kz ij bi translated">既然我们已经设置了我们的问题，是时候了解GNN模型将如何被训练来为未标记的节点输出类了。事实上，我们希望我们的模型不仅使用我们的节点的特征向量，而且利用我们处理的图结构。</p><p id="88eb" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">使GNN独特的这最后一个陈述必须被限制在某个假设内，该假设声明<strong class="ke jg"> <em class="mk">邻居节点倾向于共享相同的标签</em> </strong>。GNN通过使用消息传递形式来合并它，这个概念将在本文中进一步讨论。我们将介绍一些我们将在后面考虑的瓶颈。</p><p id="47f8" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">抽象够了，现在让我们看看gnn是如何构造的。事实上，GNN模型包含一系列通过更新的节点表示进行通信的层(每层为每个节点输出一个嵌入向量，然后该向量被用作下一层的输入以在其上进行构建)。</p><p id="6e12" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们的模型的目的是构建这些嵌入(<em class="mk">对于每个节点</em>)，集成节点的初始特征向量和关于围绕它们的局部图结构的信息。一旦我们有了良好表示的嵌入，我们就为这些嵌入提供一个经典的Softmax层来输出相关的类。</p><figure class="my mz na nb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ne"><img src="../Images/103a7c156b0e6eac145c84ba2265c606.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h29upvMguOlr7er7tk2vxQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">GNN的目标是将节点特征转换成能够感知图形结构的特征</p></figure><p id="d0ef" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">为了构建这些嵌入，GNN层使用一种简单的机制，称为<strong class="ke jg">消息传递</strong>，这有助于图节点与其邻居交换信息，从而一层又一层地更新它们的嵌入向量。</p><ul class=""><li id="47a6" class="nf ng jf ke b kf kg kj kk kn nh kr ni kv nj kz nk nl nm nn bi translated"><strong class="ke jg">消息传递框架</strong></li></ul><p id="7314" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这一切都从一些节点开始，用向量x描述它们的属性，然后每个节点通过置换等变函数(平均值，最大值，最小值)从它的邻居节点收集其他特征向量..).换句话说，这是一个对节点排序不敏感的函数。这个操作叫做<strong class="ke jg">聚合，它</strong>输出一个<strong class="ke jg">消息向量。</strong></p><p id="24a2" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">第二步是<strong class="ke jg">更新功能，</strong>其中节点将从其邻居收集的信息(<em class="mk">消息向量</em>)与其自己的信息(<em class="mk">其特征向量</em>)相结合，以构建<strong class="ke jg">新的向量h:嵌入</strong>。</p><p id="88d0" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这种聚集和更新函数的实例化因论文而异。你可以参考GCN[1]，GraphSage[2]，GAT[3]或者其他人，但是消息传递的思想是不变的。</p><figure class="my mz na nb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi no"><img src="../Images/3bc001ad7ce18a41cfb7b9a911212699.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4vdkH74GFgMQXBDGY5BKzQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">我们的GNN模型的第一层从特征向量x0到它的新嵌入h的图解[作者图解]</p></figure><p id="3ced" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">这个框架背后的直觉是什么？嗯，我们希望我们的节点的新嵌入考虑到本地图结构，这就是为什么我们从邻居的节点聚合信息。通过这样做，人们可以直观地预见到一组邻居节点在聚集后将具有更相似的表示，这将在最后减轻我们的分类任务。所有这一切在我们的第一个假设(<strong class="ke jg"> <em class="mk">邻居节点倾向于共享同一个标签</em> </strong>)的情况下总是成立的。</strong></p><ul class=""><li id="7ee4" class="nf ng jf ke b kf kg kj kk kn nh kr ni kv nj kz nk nl nm nn bi translated"><strong class="ke jg">GNNs中的图层组成</strong></li></ul><p id="2d2b" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">现在我们已经理解了消息传递的主要机制，是时候理解层在GNN的上下文中意味着什么了。</p><p id="3915" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">回想上一节，每个节点使用来自其邻居的信息来更新其嵌入，因此自然的扩展是使用来自其邻居的邻居的信息(<em class="mk">或第二跳邻居</em>)来增加其感受域并变得更加了解图结构。这就是我们GNN模型的第二层。</p><p id="c253" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">通过聚集来自N跳邻居的信息，我们可以将其推广到N层。</p><figure class="my mz na nb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi np"><img src="../Images/5b28fdae3aa6979ba6f6e63fc95c7b04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hEF_yKkfOeovAtoBT29WxA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">一层又一层，节点可以访问更多的图节点和更具图结构意识的嵌入</p></figure><p id="04cc" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">至此，您已经对gnn如何工作有了一个高层次的理解，并且您可能能够发现为什么这种形式主义会有问题。首先，在深度学习的背景下谈论GNN假设深度(许多层)的存在。这意味着节点可以访问来自远处的节点的信息，这些节点可能与它们不相似。一方面，消息传递形式主义试图软化相邻节点之间的距离(平滑)以方便我们稍后的分类。另一方面，它可以在另一个方向上工作，通过使我们所有的节点嵌入相似，因此我们将不能分类未标记的节点(过度平滑)。</p><p id="ef20" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在下一节中，我将尝试解释什么是平滑和过度平滑，我们将讨论平滑作为增加GNN图层的自然效果，我们将了解为什么它会成为一个问题。</p><p id="624c" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我还将尝试量化它(从而使它可跟踪)，并在此量化的基础上，使用关于此问题的已发表论文中的解决方案来解决它。</p><h1 id="0fdd" class="lh li jf bd lj lk nq lm ln lo nr lq lr ls ns lu lv lw nt ly lz ma nu mc md me bi translated">GNNs中的过度平滑问题</h1><p id="aaf4" class="pw-post-body-paragraph kc kd jf ke b kf mf kh ki kj mg kl km kn mh kp kq kr mi kt ku kv mj kx ky kz ij bi translated">尽管消息传递机制有助于我们利用图结构中封装的信息，但如果与GNN深度结合，它可能会引入一些限制。换句话说，我们对一个更具表达性和更了解图结构的模型的追求(通过增加更多的层，使节点可以有一个大的感受域)可以转化为一个对节点一视同仁的模型(节点表示收敛到不可区分的向量[4])。</p><p id="bf13" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这种平滑现象不是错误，也不是特例，而是GNN的本质，我们的目标是缓解它。</p><h2 id="4a62" class="ml li jf bd lj mm mn dn ln mo mp dp lr kn mq mr lv kr ms mt lz kv mu mv md mw bi translated">为什么会出现过度平滑？</h2><p id="2a27" class="pw-post-body-paragraph kc kd jf ke b kf mf kh ki kj mg kl km kn mh kp kq kr mi kt ku kv mj kx ky kz ij bi translated">消息传递框架使用前面介绍的两个主要函数<strong class="ke jg">聚合</strong>和<strong class="ke jg">更新</strong>，它们从邻居那里收集特征向量，并将它们与节点自身的特征相结合，以更新它们的表示。该操作的工作方式使得交互节点(在该流程中)具有非常相似的表示。</p><p id="1aef" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们将尝试在模型的第一层中说明这一点，以显示平滑为什么会发生，然后添加更多的层来显示这种表示平滑如何随着层的增加而增加。</p><blockquote class="nv nw nx"><p id="6d36" class="kc kd mk ke b kf kg kh ki kj kk kl km ny ko kp kq nz ks kt ku oa kw kx ky kz ij bi translated">注:<strong class="ke jg">过度平滑以节点嵌入的相似性的形式表现出来。</strong>所以我们使用颜色，其中<strong class="ke jg">不同的颜色意味着矢量嵌入的不同</strong>。此外，为了简化我们的示例，我们将只更新突出显示的4个节点。</p></blockquote><figure class="my mz na nb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ob"><img src="../Images/7701edd6d523fa21eb90caf3d1b227e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qgX58AhXLgzuF7NpK5jq8A.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">我们GNN的第一层[作者插图]</p></figure><p id="7d3f" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">正如您在第一层中看到的<strong class="ke jg">，节点可以访问一跳邻居。例如，您可能还会观察到，<strong class="ke jg">节点</strong> <strong class="ke jg"> 2 </strong>和<strong class="ke jg">节点3 </strong>几乎可以访问相同的信息，因为它们相互链接并具有共同的邻居，唯一的区别在于它们最后的邻居(紫色和黄色)。<strong class="ke jg">我们可以预测，它们的嵌入会略有相似。</strong>至于<strong class="ke jg">节点1 </strong>和<strong class="ke jg">节点4 </strong>，它们相互作用但有不同的邻居。因此我们可以预测<strong class="ke jg">他们的新嵌入将会不同。</strong></strong></p><p id="20a2" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们<strong class="ke jg">通过给每个节点分配新的嵌入来更新我们的图</strong>，并移动到<strong class="ke jg">第二层</strong>并重复相同的过程。</p><figure class="my mz na nb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/cf1ce3bfdfb3ba32d02d01cd4dcfaf70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WUyYprbzZsvSEMl-QMs-cA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">GNN的第二层[作者插图]</p></figure><p id="b00c" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在我们的GNN的第二层中，节点1、4和2、3的计算图分别几乎相同。我们可以预期，我们对这些节点的新的更新嵌入将更加相似，甚至对于在某种程度上“<em class="mk">幸存于</em>”的<strong class="ke jg">节点1 </strong>和<strong class="ke jg">节点4 </strong>，第一层现在将具有相似的嵌入<strong class="ke jg">、</strong>，因为额外的层给予它们对图的更多部分的访问，增加了<strong class="ke jg">访问相同节点</strong>的概率。</p><p id="a4f7" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这个简化的例子显示了过度平滑是GNN深度的结果。公平地说，这与真实案例相去甚远，但它仍然给出了这种现象发生背后的原因。</p><h2 id="ed80" class="ml li jf bd lj mm mn dn ln mo mp dp lr kn mq mr lv kr ms mt lz kv mu mv md mw bi translated">为什么这真的是一个问题？</h2><p id="9e78" class="pw-post-body-paragraph kc kd jf ke b kf mf kh ki kj mg kl km kn mh kp kq kr mi kt ku kv mj kx ky kz ij bi translated">既然我们了解了过度平滑发生的原因，以及它是由GNN图层合成的效果设计而成的原因，那么是时候强调我们为什么应该关注它，并提出解决方案来克服它了。</p><p id="2578" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">首先，学习我们的嵌入的目标是在最后将它们提供给分类器，以便预测它们的标签。考虑到这种过度平滑的影响，我们最终会对没有相同标签的节点进行类似的嵌入，这将导致它们的错误标签。</p><p id="74d9" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">人们可能认为减少层数会降低过度平滑的效果。是的，但是这意味着在复杂结构数据的情况下不能利用多跳信息，因此不能提高我们的最终任务性能。</p><p id="e9fa" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">为了强调最后一句话，我将用一个现实生活中常见的例子来说明它。想象一下，我们正在处理一个有数千个节点的社交网络图。一些新用户刚刚登录平台，订阅了他们朋友的个人资料。我们的目标是找到主题建议来填充他们的提要。</p><figure class="my mz na nb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oc"><img src="../Images/9a338988fe8ad23ec6ad30d96ba741ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gEc0PLkZQpGlJWGVDEU9hA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">一个虚构的社交网络[作者插图]</p></figure><p id="6cef" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">给定这个假想的社交网络，在我们的GNN模型中仅使用1或2层，我们将仅了解到我们的用户关心供应链话题，但是我们错过了他可能喜欢的其他多样化话题，因为他的朋友的互动。</p><p id="412e" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">总之，由于过度平滑是一个问题，我们在低效率模型和更有深度但在节点表示方面缺乏表达性的模型之间遇到了一个折衷。</p><h2 id="da27" class="ml li jf bd lj mm mn dn ln mo mp dp lr kn mq mr lv kr ms mt lz kv mu mv md mw bi translated">怎么才能量化呢？</h2><p id="7782" class="pw-post-body-paragraph kc kd jf ke b kf mf kh ki kj mg kl km kn mh kp kq kr mi kt ku kv mj kx ky kz ij bi translated">既然我们已经清楚过度平滑是一个问题，我们应该关注它，我们必须量化它，以便我们可以在训练GNN模型时跟踪它。不仅如此，通过将量化作为正则项添加到我们的目标函数中，量化还将为我们提供一个用作数值惩罚的度量。</p><p id="376f" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">根据我最近的阅读，大量的论文讨论了GNN的过度平滑问题，他们都提出了一个量化指标来证明他们对这个问题的假设，并验证他们的解决方案。</p><p id="96d5" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我从两篇讨论这个问题的不同论文中选择了两个指标。</p><ul class=""><li id="50da" class="nf ng jf ke b kf kg kj kk kn nh kr ni kv nj kz nk nl nm nn bi translated"><strong class="ke jg"> MAD和MADGap【5】</strong></li></ul><p id="0dcd" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">Deli Chen等人引入了MAD和MADGap两个量化指标来度量图节点表示的光滑性和过度光滑性。</p><p id="164a" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">一方面，MAD计算图中节点表示(嵌入)之间的<strong class="ke jg">平均距离，并使用它来显示平滑是向GNN模型添加更多层的自然效果。基于这一度量，他们将其扩展到MADGap，MADGap测量不同类别节点之间表示的<strong class="ke jg"/><strong class="ke jg">相似性。</strong>这种概括建立在主要假设上，即当节点交互时，它们或者可以从来自同一类的节点获得重要信息，或者可以通过与来自其他类的节点交互获得噪声。</strong></p><figure class="my mz na nb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi od"><img src="../Images/4ae75d6b1d05a3a159307c7cd0001b6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E1WTF9Av_dooK7mO2ZDhmA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">当节点访问图的更多部分时，我们可能会访问影响最终嵌入的噪声节点</p></figure><p id="b7d4" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在这篇论文中引起我兴趣的是作者质疑消息传递形式主义所基于的主要假设的方式(邻居节点可能有相似的标签)。事实上，他们的测量MADGap不仅仅是过度平滑的测量，而是相对于节点收集的信号的信噪比的测量。因此，观察到这一比率逐层降低证明了图拓扑和下游任务目标之间的<strong class="ke jg">差异。</strong></p><ul class=""><li id="ebd6" class="nf ng jf ke b kf kg kj kk kn nh kr ni kv nj kz nk nl nm nn bi translated"><strong class="ke jg">组距离比【6】</strong></li></ul><p id="28ce" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">周凯雄等人引入了另一个应变前向度量，但其目标与MADGap相同，即<strong class="ke jg">组距离比。</strong>此指标计算两个平均距离，然后计算它们的比率。我们首先将节点放在相对于其标签的特定组中。然后，为了构造我们的比率的命名器，我们计算每两个节点组之间的成对距离，然后对结果距离进行平均。至于分母，我们计算每组的平均距离，然后计算平均值。</p><figure class="my mz na nb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oe"><img src="../Images/683e2860b8c1e6ae65e91ed2c1d4feb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ao6R4A5srQwOE9R01R5Pzw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">解释如何计算组距离比率的图解[作者图解]</p></figure><p id="3378" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">具有小的比率意味着不同组中节点嵌入之间的平均距离小，因此我们可以根据它们的嵌入来混合组，这是过度平滑的证明。</p><p id="68e7" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们的目标是保持一个高的<strong class="ke jg">组距离比率</strong>，以使节点类在嵌入方面有所不同，这将减轻我们的下游任务。</p><h2 id="adfc" class="ml li jf bd lj mm mn dn ln mo mp dp lr kn mq mr lv kr ms mt lz kv mu mv md mw bi translated">有没有解决过度平滑的方法？</h2><p id="b9f9" class="pw-post-body-paragraph kc kd jf ke b kf mf kh ki kj mg kl km kn mh kp kq kr mi kt ku kv mj kx ky kz ij bi translated"><strong class="ke jg">直接监管术语？</strong></p><p id="f10b" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">既然我们已经量化了过度平滑问题，您可能会认为我们的工作已经结束，将这一指标作为监管条款添加到我们的损失目标中就足够了。剩下的问题是，在我们的训练会话的每次迭代中计算这些度量(<em class="mk">如上所述</em>)可能在计算上是昂贵的，因为我们需要访问我们的图中的所有训练节点，然后进行一些距离计算，处理与节点数量成二次比例的节点对(C(2，n) = n * (n -1) / 2 = O(n))。</p><p id="873e" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke jg">间接解决方案？</strong></p><p id="8880" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">由于这个原因，所有讨论过平滑问题的论文都考虑通过更容易实现并且对过平滑有影响的其他间接解决方案来克服这个计算问题。我们不会详细讨论这些解决方案，但是您可以在下面找到其中一些的参考资料。</p><p id="c028" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">对于我们的例子，我们将处理由周凯雄等人引入的<strong class="ke jg">可微分组归一化[6]</strong>。DGN将节点分配到组中，并独立地归一化它们，以输出下一层的新嵌入矩阵。</p><p id="d479" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这个附加层是为了优化先前定义的<strong class="ke jg">组距离比</strong>或<strong class="ke jg"> <em class="mk">组</em> </strong>而构建的。事实上，在一个组内嵌入的节点的归一化使得它们的嵌入非常相似(减少<strong class="ke jg"> <em class="mk"> Rgroup </em> </strong>的分子)，并且这些使用可训练参数的缩放和移位使得来自不同组的嵌入不同(增加<strong class="ke jg"> <em class="mk"> Rgroup </em> </strong>的分子)。</p><figure class="my mz na nb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi of"><img src="../Images/c0516de6053d7d5f5de4ce90a3898f9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YWcJFd8p5h9xQga1LmFh2A.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">可区分组规范化是如何工作的？[作者插图]</p></figure><p id="30cf" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">为什么会起作用？第一次阅读这篇论文时，我没有看到添加这个归一化层和优化<strong class="ke jg"> Rgrou比率之间的联系，</strong>然后我观察到，这个层一方面使用可训练的分配矩阵，因此它有来自我们的损失函数的反馈，所以它被引导在理想情况下将节点分配给它们的真实类。另一方面，我们也有同样由损失函数引导的移动和缩放参数。这些参数用于区分不同组的嵌入，因此有助于下游任务。</p><h1 id="9b9a" class="lh li jf bd lj lk nq lm ln lo nr lq lr ls ns lu lv lw nt ly lz ma nu mc md me bi translated">开场白和结论</h1><p id="ef22" class="pw-post-body-paragraph kc kd jf ke b kf mf kh ki kj mg kl km kn mh kp kq kr mi kt ku kv mj kx ky kz ij bi translated">这篇文章可能很长，但它只是触及了图形神经网络及其问题的表面，我试图从gnn的一个小探索开始，并展示它们如何能够——用这样一个简单的机制——打开我们在其他DL架构的上下文中无法想到的潜在应用。这种简单性受到许多问题的限制，阻碍了它们的表达能力(目前…)，研究人员的目标是在寻求利用图形数据的全部力量时克服它。</p><p id="b4f2" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">就我而言，我阅读了不同的论文讨论GNNs的一些限制和瓶颈，但将它们统一起来的一个共同点是<strong class="ke jg">所有这些问题都可以与我们用来训练我们的图模型的主要机制相联系，这就是消息传递</strong>。我可能不是专家，但我必须就此提出一些问题。不断列举这些问题并试图解决它们真的值得吗？既然我们还处于这个有趣领域的第一次迭代，为什么不考虑一个新的机制并尝试一下呢？</p></div><div class="ab cl la lb hu lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="ij ik il im in"><p id="0874" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">[1] Kipf，T. N. (2016年9月9日)。基于图卷积网络的半监督分类。ArXiv.Org。<a class="ae nc" href="https://arxiv.org/abs/1609.02907" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1609.02907</a></p><p id="892d" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">[2]汉密尔顿，W. L. (2017年6月7日)。大型图上的归纳表示学习。ArXiv.Org。<a class="ae nc" href="https://arxiv.org/abs/1706.02216" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1706.02216</a></p><p id="f209" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">[3]veli kovi，P. (2017年10月30日)。图形注意力网络。ArXiv.Org。<a class="ae nc" href="https://arxiv.org/abs/1710.10903" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1710.10903</a></p><p id="e6eb" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">[4] Oono，K. (2019年5月27日)。图形神经网络指数地失去了节点分类的表达能力。ArXiv.Org。<a class="ae nc" href="https://arxiv.org/abs/1905.10947" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1905.10947</a></p><p id="3698" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">[5]陈d .(2019 . 9 . 7)。从拓扑角度度量和缓解图神经网络的过光滑问题。ArXiv.Org。<a class="ae nc" href="https://arxiv.org/abs/1909.03211" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1909.03211</a></p><p id="5b05" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">[6]周，K. (2020年6月12日).用可微分的组规范化走向更深层次的图形神经网络。ArXiv.Org。<a class="ae nc" href="https://arxiv.org/abs/2006.06972" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2006.06972</a></p></div><div class="ab cl la lb hu lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="ij ik il im in"><p id="b0f4" class="pw-post-body-paragraph kc kd jf ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我想对<strong class="ke jg">齐纳布·SAIF</strong>和<a class="ae nc" href="https://badr-moufad.medium.com/" rel="noopener">T5】巴德尔·穆法德 </a>为这部不大的作品做出的贡献表示感谢。不要忘了我的阅读团队，他们一直在给我发草稿。谢谢大家，直到我们在另一个故事中相遇</p></div></div>    
</body>
</html>