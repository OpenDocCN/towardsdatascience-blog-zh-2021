<html>
<head>
<title>Ridge, LASSO, and ElasticNet Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">脊线、套索和弹性网回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ridge-lasso-and-elasticnet-regression-b1f9c00ea3a3?source=collection_archive---------2-----------------------#2021-04-02">https://towardsdatascience.com/ridge-lasso-and-elasticnet-regression-b1f9c00ea3a3?source=collection_archive---------2-----------------------#2021-04-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3d8d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">正则化的持续冒险和防止模型过度拟合的永恒追求！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/115309a9a30066d574c4ff81b7f90ca3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*0_N5bV721QeYcTKF4X1Njw.jpeg"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">套索、山脊和弹性网回归|照片由Daniele Levis Pulusi拍摄</p></figure><p id="0784" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这篇文章是上周介绍使用线性回归的<a class="ae ku" rel="noopener" target="_blank" href="/regularization-and-linear-regression-bcaeba547c46">正则化的延续。莴苣那边回到本质上，用更先进的技术简化我们的模型，使最好的数据科学/机器学习模型成为可能。我们如何简化我们的模型？通过移除尽可能多的特征。我为什么要这么做？我想要一个更简单的模型。为什么我想要一个更简单的模型？因为一个简单的模型概括得更好。泛化是什么意思？<strong class="kx iu">这意味着你可以在现实世界中使用它</strong>。该模型不仅解释了训练数据，而且也有更好的机会解释我们以前没有见过的数据。</a></p><h2 id="a8b5" class="lr ls it bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">正规化</h2><p id="fb48" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">正则化试图通过添加调谐参数λ或α来控制方差:</p><h2 id="8d60" class="lr ls it bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">套索(L1正则化)</h2><ul class=""><li id="96a5" class="mp mq it kx b ky mk lb ml le mr li ms lm mt lq mu mv mw mx bi translated">正则项惩罚系数的绝对值</li><li id="72fe" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">将不相关的值设置为0</li><li id="8093" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">可能会移除模型中的过多特征</li></ul><h2 id="41a4" class="lr ls it bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">岭回归(<strong class="ak"> L2正则化</strong>)</h2><ul class=""><li id="8e13" class="mp mq it kx b ky mk lb ml le mr li ms lm mt lq mu mv mw mx bi translated">惩罚回归系数的大小(大小的平方)</li><li id="5185" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">强制降低<em class="nd"> B </em>(斜率/部分斜率)系数，但不是0</li><li id="4dc7" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">不会删除不相关的功能，但会最小化它们的影响</li></ul><p id="bf8b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">现在不要担心λ。现在假设λ是1。对于岭回归，我们惩罚回归系数的大小。因此，较大的回归系数是较大的惩罚。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi ne"><img src="../Images/49cba194e38d6a9f6e67bfa3b07d5409.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uK4qRdiVxlzszBFf"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">b和c是回归系数</p></figure><p id="fe51" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">所以在这个岭回归中，我会取<strong class="kx iu"> b </strong>的平方，并把它加到<strong class="kx iu"> c </strong>的平方上。这就是我的惩罚。现在比这稍微复杂一点，因为有时我想让惩罚变强或变弱，我现在引入这个λ，这个调谐参数。所以我的惩罚现在变成了:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nj"><img src="../Images/a6931f6a7ecb95582988b1146ddbabed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*X_ha1fCNxGYOXXqu"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">也称为L2或欧几里德范数</p></figure><p id="ffee" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这就是岭回归的作用。通过在我的成本函数中加入这个惩罚，在训练过程中，它将保持这些b和这些c很小。这样做可以简化模型。这意味着任何给定的列都不会有那么大的功率。</p><p id="3f61" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">套索做了几乎相同的事情。除了正则化罚函数没有平方:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nj"><img src="../Images/f77a4e368f03f972bd2c11dc698d508d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*KwVfRWXAu2Dgdfp9"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">也称为L1或曼哈顿/出租车规范</p></figure><p id="a281" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">请注意，因为这里可能有负值，所以需要在部分斜率的系数上加上绝对值。</p><p id="454a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">上例中的系数b和c由您使用的算法调整。我们通过引入一个不为1的lambda罚值给它添加了另一个怪癖——它一直在幕后，但为了简单起见，我们将其保持为1。当需要评估你的模型的好坏时，通过成本函数，我们将把一个我们称为λ的惩罚乘以你的系数(或系数的平方)，这将是一个额外的惩罚。</p><p id="c4df" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们仍然有我们的一般成本函数。这只是对成本函数增加。我们的一般成本函数总是相同的，它是这样的，我们有y——预测的y，取它的平方，然后把所有这些加起来。这是我们的成本函数，基线。现在，为了正则化的附加惩罚是使用所谓的L2范数的岭回归，或者使用所谓的L1范数的LASSO(最小绝对收缩和选择算子)回归。</p><p id="a6f5" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">对于这两种类型的回归，较大的系数对模型不利。对于岭，由于平方的原因，损失不成比例地更大，但是一般来说，较大的系数会降低模型性能，较大的系数会降低拟合优度。</p><p id="29a3" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">名词（noun的缩写）b:<strong class="kx iu"><em class="nd">L</em></strong><em class="nd"/>符号是对闵可夫斯基规范和<strong class="kx iu"> <em class="nd"> Lp </em> </strong> <em class="nd"> </em>空格的引用。这些只是将出租车和欧几里德距离的概念推广到以下表达式中的<em class="nd"> p &gt; 0p &gt; 0 </em>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nj"><img src="../Images/fb951486e6774c4d1eccb2c27fe26662.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*jxt0lMsMKyqQ5tLg"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">对于那些还记得线性代数的人</p></figure><h2 id="bbf8" class="lr ls it bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">里脊回归</h2><p id="1367" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">再次重申，岭回归是一种限制回归中独立变量(列/特征/属性)数量的方法。正则最小二乘准则最小化误差的最小平方加上正则项，正则项是常数和系数平方之和的乘积:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nj"><img src="../Images/b4d872085d2b2bf95aa4e0e0281a5d8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JIXBmlxn_0VPPK4t"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">岭回归成本函数，以另一种方式表达|在某些文献中，有时使用α代替λ</p></figure><p id="b50b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">本质上，这是为了防止部分斜率项变得过大。</p><p id="ad64" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这里我称这个惩罚系数为α，而不是λ。这有点令人困惑，因为在这个行话中，alpha有几个非常相关的意思。所以我想提醒你，一般来说，你可能会有一些问题，lambda是一个安全的赌注。α至少有两个非常接近但仍然不同的含义。</p><p id="74ef" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这里，我们有这个β的平方。什么是贝塔？β是系数的向量。所以β是一个向量，在我之前的例子中是b和c。记住，我有y = a +b*X1 + c*X2，b和c是两个系数。β是一个向量，这里由两个数字组成。在我的例子中，这两个数字是b和c；a是截距，有时容易混淆地称为b。</p><p id="213f" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">所以β的平方和上面的等式完全一样。什么是偏坡？部分斜率的另一个词是系数。为什么我们称之为偏坡？如果你记得，我们有一个斜率，当我们有一个线性回归时，y = a + b*X，b是斜率。如果你有多个x，那么y = a + b*X1 + c*X2 + d*X3等等。然后，在我刚才提到的例子中，b，c和d都是部分斜率，这是因为它们乘以了列。列的输入值随变量的变化而变化的方式会影响输出。如果系数或部分斜率越大，影响就越大。当我说更大，我的意思是在数量上。</p><p id="38d6" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">所以脊目标<strong class="kx iu">是残差平方和(RSS)+λ*(系数的平方和)</strong>。</p><ul class=""><li id="6e01" class="mp mq it kx b ky kz lb lc le nk li nl lm nm lq mu mv mw mx bi translated">使用lambda来平衡最小化RSS和最小化系数</li><li id="2a9f" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">当lambda = 0时，结果与常规线性回归相同:您已经移除了岭回归损失</li><li id="66e3" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">随着λ增加到无穷大，系数变得更接近于0</li><li id="20e9" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">任何非零λ值都会将系数缩小到小于非正则化回归的系数</li><li id="3e50" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">随着lambda (alpha)的增加，模型复杂度降低</li></ul><p id="4f0c" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">Lambda只是一个标量，应该使用交叉验证(调整)从您的数据中学习。所以λ总是正的，介于零和无穷大之间。给你一个范围的基本概念，它很少比1大很多，可能是2或3，通常是1的一小部分。这是为什么呢？嗯，事情就是这样解决的。我刚才说的有反例，但是作为一个普遍的启发，你通常会发现λ在0和之间，一个小值，让我们说大约2。</p><p id="e900" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">如果模型的复杂度随着lambda的增加而降低，你怎么能看出这一点？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nn"><img src="../Images/235c1beee1d1e3ce082bcdc76ca691e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aCivvOcF1tVvRD8prl64aA.jpeg"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">我们的回归线与蓝点吻合得很好</p></figure><p id="a542" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在上面的例子中，我近似了某种形式的回归，在这种情况下，可能不是线性回归。这种回归非常有效。然后我们添加了lambda参数。希望事情有所好转。如果我在这里看这个，我可能会想要右边的那个。我的理由是，我看到这些蓝点，它们到处飞，它们在那条黑线的上下。我预计这些蓝点会有更多的变化，会有很多变化。所以我认为，试图找到一个模型，试图最小化这些蓝点的残差，会试图最小化它太多，会发生的是，它会变得太复杂，它会过度拟合。然后，当新的数据出现时，模型从未见过的数据，这些新数据不会出现在模型适合的地方。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi no"><img src="../Images/b57b4dfcfea0017a425dcc37b846fa67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9oNj4QMsODqGfGuOn_vdKA.jpeg"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">红色圆圈很可能过度拟合残差</p></figure><p id="a7ce" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">例如，这个用红色圈起来的小怪癖，看起来很不错。但我猜测这只是因为一两个数据点有相对高的y值。我认为这只是一个怪癖。也许这应该直接过去，就像它在最右边的图像上一样。因为如果我看一下这些蓝点上升和下降的总量，我真的无法证明创建一个试图下降以最小化这一个蓝点的残余然后再上升的模型是正确的。因此，小的λ值可能是过度拟合的，我相信右边的1/100λ值实际上更好地拟合了数据，并且将更好地概括。</p><h2 id="2716" class="lr ls it bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">套索</h2><p id="4879" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">套索回归做了几乎相同的事情；这是限制回归中自变量数量的另一种方法。唯一的区别是，它使用绝对值。现在的问题是，因为数学，它不只是减少系数，拉索实际上删除了系数。许多人喜欢LASSO的原因是:<strong class="kx iu">系数实际上可以变为零，而在岭回归中，它们不会变为零，它们会变小，但永远不会消失</strong>。</p><p id="0f92" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">现在，你可以用最后一种方法，如果你看到系数变得很小，你可以说，“好吧，我把这一列扔掉，因为它的系数很小。”LASSO的好处在于它实际上会以一种智能的方式为你做到这一点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nj"><img src="../Images/c4a56043bb94dfd4c7e81268bbf0bd20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*r9MmZLQX46s4KVdx"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">y是j点的预测值，有I项具有β系数—λ限制β</p></figure><p id="8437" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">系数的向量。你只需要取绝对值，而不是平方它们。</p><h2 id="8e71" class="lr ls it bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">线性代数</h2><p id="c342" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">接下来是线性代数的快速回顾。为什么我们要快速复习线性代数？嗯，因为机器学习是大量的线性代数，你对机器学习算法理解得越多，通常你就能更好地应用它们。<strong class="kx iu">在研究数据科学时，你不是在研究机器学习，你是在研究如何使用机器学习算法</strong>。</p><p id="dbb6" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">矩阵的代数性质；</p><ul class=""><li id="f34e" class="mp mq it kx b ky kz lb lc le nk li nl lm nm lq mu mv mw mx bi translated">加/减矩阵:必须具有相同的维数(m * n)</li><li id="3c34" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">对于矩阵乘法:内部尺寸必须匹配</li><li id="a11a" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">记住矩阵乘法是不可交换的:A x B！= B x A</li><li id="537a" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">矩阵乘以它的逆矩阵得到单位矩阵</li><li id="b9b7" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">矩阵除法未定义</li></ul><h2 id="2fe6" class="lr ls it bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">矩阵线性回归</h2><p id="e11c" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">考虑线性回归公式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nj"><img src="../Images/9c9d380004a649d39b8c40de59e3dcad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*JnsHXROQ9WS7dgQG"/></div></div></figure><p id="89c6" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">写成矩阵:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nj"><img src="../Images/76c07418e20083166ff0b0f09defd430.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Z-26flhaNz1MQlcd"/></div></div></figure><p id="4b80" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">可以改写为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nj"><img src="../Images/fd2b43dc634114f8c4d6bbe9faed97cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0UOpAdGToI6UyByG"/></div></div></figure><p id="3b48" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">对于矩阵的线性回归，如果你有，比方说10个案例，这意味着10个点，你将有10列，那么将有一个精确的，最好的解决方案。对此会有一个简单的解决方案。然而，并不是在每种情况下，都有共线性的情况，这意味着一列与另一列基本相同，或者与其他一些列的组合相同。当这种情况发生时，你通常有无限多的解决方案。我们不喜欢这样，因为有无限多的解，意味着你没有任何有用的解。如果有这样的情况，根本没有解决办法。但是一般情况下，或者你希望的情况是，你有一个好的解决方案。</p><p id="48d1" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">大多数时候，当我们做机器学习或数据科学时，我们的案例比变量多得多；我们的数据点比列多得多；我们的行数比列数多得多——我只是用稍微不同的词一遍又一遍地重复同样的事情。这将给我们带来一个问题，首先，这意味着我们的x矩阵不是一个方阵。</p><p id="d1dd" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">所以当我们想要反转它的时候，我会告诉你们我们想要这样做，这会给我们带来一些问题，伴随着共线性之类的问题。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi np"><img src="../Images/4b72561c8be2828fcea5173ca2c58909.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AwyzxvvGCf1HiWjQE_lDxw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">简洁地说:y = X *β+ε</p></figure><p id="dee5" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这些概念在代码中得到最好的展示。</p><h1 id="5f17" class="nq ls it bd lt nr ns nt lw nu nv nw lz jz nx ka mc kc ny kd mf kf nz kg mi oa bi translated">Python示例:奇异值分解</h1><p id="67a5" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">在前面的中，我提到逐步回归是一种正则化的方法，通过向前或向后的方法，一次找到一个有意义的特征。我现在引入奇异值分解(SVD)作为另一种有意义的寻找特征的方法。</p><p id="487e" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">正则化方法稳定了<strong class="kx iu">模型矩阵</strong>的逆。我们将使用奇异值分解方法来稳定模型矩阵。</p><p id="fa62" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">名词（noun的缩写）b:逐步回归是一个计算密集型过程，因为我们必须多次重新计算模型。存在允许计算更新模型的方法，但是对于大量的特征，存在许多排列。我们需要能够处理数百、数千甚至数百万个特征的方法。考虑一个只有20个特征的小数据集。没有交互项的可能线性模型的数量由下式给出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nj"><img src="../Images/36cae26ac3a01c5a7dc7976cd51142c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*w1UBgYnKAYwWKfDr"/></div></div></figure><p id="3901" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">也就是帕斯卡三角形第21行的和。结果是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi ob"><img src="../Images/e1b8823bfa6f170b534657e518a68902.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GcUl192zPdQWg8XZ"/></div></div></figure><p id="ab9a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">虽然根据我们的算法，这将是要计算的模型的最大数量，但是当天真地考虑逐步回归时，你可以看到这在计算上变得多么困难。</p><p id="94fa" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">为了理解特征选择/转换的动机和方法，我会复习一些线性代数。</p><h2 id="943a" class="lr ls it bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">线性代数评论</h2><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="8f26" class="lr ls it od b gy oh oi l oj ok">import numpy as np</span><span id="6b9a" class="lr ls it od b gy ol oi l oj ok">a_list = [2]*3<br/>print(a_list)<br/>print(type(a_list))<br/>a = np.array([2]*3)<br/>print(a)<br/>print(type(a))<br/>b = np.arange(1, 4)<br/>print(b)<br/>print(type(b))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/c84525f6be3cae04dc2644b4db6fe468.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/format:webp/1*n7pFNehaDPimGBscPEQhuw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">Jupyter类似地显示列表和数组</p></figure><p id="9131" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">矩阵乘法和加法</p><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="c458" class="lr ls it od b gy oh oi l oj ok">a2d = np.array([[2]*3, [3]*3])<br/>print(‘a: \n{} \n a-size: {}’.format(a2d, a2d.shape))<br/>b2d = np.reshape(np.arange(1,7), newshape=(2, 3))<br/>print(‘\nb: \n{} \n b-size: {}’.format(b2d, b2d.shape))</span><span id="a531" class="lr ls it od b gy ol oi l oj ok"># Addition<br/>print(‘\nAddition: \n {}’.format(a2d + b2d))<br/># Multiplication<br/>print(‘\nMultiplication: \n {}’.format(a2d * b2d))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/c65557db04175b818ca5ba1eac8c6ac5.png" data-original-src="https://miro.medium.com/v2/resize:fit:338/format:webp/1*CINY3mSObBW-8ojBH3p5Ww.png"/></div></figure><h2 id="73e1" class="lr ls it bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">矩阵转置</h2><p id="8971" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">我们也可以通过翻转行和列来转置一个二维矩阵。我们使用<code class="fe oo op oq od b">numpy</code>方法<code class="fe oo op oq od b">np.transpose</code>。</p><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="310a" class="lr ls it od b gy oh oi l oj ok">print(‘Try transposing on a vector…’)<br/>print(np.transpose(a))<br/># Transposition on a vector does not work</span><span id="d821" class="lr ls it od b gy ol oi l oj ok">print(‘Now reshape a into 2D and then transpose:’)<br/>print(np.transpose(np.reshape(a, newshape=(1,3))))<br/>print(‘Now transpose a 2d matrix:’)<br/>print(np.transpose(a2d))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/202a5a3a51c51934cb79c000703d600f.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/format:webp/1*c-CoXJLr_fKDvMQnhFA8OA.png"/></div></figure><h2 id="cd8b" class="lr ls it bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">矩阵点积</h2><p id="375f" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">我们需要知道如何计算两个等长向量的<strong class="kx iu">点积</strong>，也称为<strong class="kx iu">标量积</strong>或<strong class="kx iu">内积</strong>，通过逐元素乘法，然后将它们相加:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi os"><img src="../Images/296f7ece3b0e1898498cbf213d7ce93c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5TGmuOCmx4hOtGlh"/></div></div></figure><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="d247" class="lr ls it od b gy oh oi l oj ok">print(‘np.dot(a, b) = np.dot({}, {})’.format(a, b))<br/>np.dot(a, b)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/287aea200854f4211a62259581feb3a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*EBLSTjDbpmAzj0ww4dxoUw.png"/></div></figure><p id="83a1" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">向量与其自身的内积的平方根是向量的长度或L2范数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi nj"><img src="../Images/04b29e140b30cdc913b00e0e0caa7960.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Wh8Cqr7BZxhO2NI9"/></div></div></figure><p id="3afe" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们也可以把内积写成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi ou"><img src="../Images/dcfd90f2decfe34f277c8092daa6e84f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zxaatyFMS1fzZvd7"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/1b2b7587be3e889eb7a2b099666405fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:256/0*kl-z6Dtr97FrW4XH"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi ow"><img src="../Images/451a024007a18d4eb75ffd7415eb6623.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MxYqDFnVgMjH5tW9"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">注意正交向量的内积是0</p></figure><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="a52d" class="lr ls it od b gy oh oi l oj ok">aa = np.array([1, 0, 0])<br/>bb = np.array([0, 1, 1])<br/>print(inner_prod(aa, bb))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/93ee5b4228aca19b8fa517459d93efd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:52/format:webp/1*WdfeDemQdsqUMEcZq4fn6Q.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">线性无关的向量是完全正交的</p></figure><p id="0a01" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">下面是一些关于矩阵的运算。设A和B是m=4行乘以n=3列的矩阵:</p><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="ae7f" class="lr ls it od b gy oh oi l oj ok">A = np.array([[4]*3]*4) <br/>print(A)<br/>B = np.array(np.reshape(np.arange(1, 13), newshape = (4, 3)))<br/>print(B)<br/>C = np.array(np.reshape(np.arange(1, 9), newshape = (4, 2)))<br/>print(C)<br/>np.dot(np.transpose(A), C)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/6a3d7e6b1a936d3d1bc392fdfb3be492.png" data-original-src="https://miro.medium.com/v2/resize:fit:384/format:webp/1*ilygMFy4EIOLf2g1ejUDsg.png"/></div></figure><h2 id="ceb2" class="lr ls it bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">单位矩阵</h2><p id="b414" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">这个特殊的正方形矩阵在对角线上有1，在其他地方有0:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi oz"><img src="../Images/dd9fa945b0085a7b80ca2548706c1502.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zX9l0oRlZ3-QNrKR"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">1的矩阵等价物</p></figure><p id="ccad" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">乘以任何矩阵的恒等式给出了那个矩阵。如果AB是一个矩形矩阵，那么:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi pa"><img src="../Images/9fc717d9feae6c4eaf57243c782cee58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*nhNr_gchlyroLEvL"/></div></div></figure><p id="a323" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在<code class="fe oo op oq od b">numpy</code>中，单位矩阵被称为<code class="fe oo op oq od b">np.eye</code>，并以行数/列数作为自变量:</p><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="3195" class="lr ls it od b gy oh oi l oj ok">I3 = np.eye(3)<br/>I4 = np.eye(4)</span><span id="c014" class="lr ls it od b gy ol oi l oj ok">print(‘I (3X3) = \n{}’.format(I3))<br/>print(np.dot(I3, AtB))</span><span id="1d1d" class="lr ls it od b gy ol oi l oj ok">print(‘I (4x4) = \n{}’.format(I4))<br/>print(np.dot(I4, ABt))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/3ab38582e5deb44135cbe5951b2e715c.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/format:webp/1*cAWDft2Lt_k3NvVSkTA6GQ.png"/></div></figure><h1 id="43e4" class="nq ls it bd lt nr ns nt lw nu nv nw lz jz nx ka mc kc ny kd mf kf nz kg mi oa bi translated">矩阵的逆</h1><p id="5ce2" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">定义并计算矩阵的逆矩阵，使得:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/78f4eff1cc6f9f9fb9473b89f6cb613a.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/0*cc91Q_KmwtTboyWv"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/d3f5492b629c3082d87545ff4dda7370.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/0*MJJU47Vsupq7JuJT"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/2abd29fa76da5c33be31d1844c724a8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/0*FcfmsC8RKXssMEGN"/></div></figure><p id="1d4b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">用<code class="fe oo op oq od b">numpy</code>中的线性代数方法，用<code class="fe oo op oq od b">np.linalg.inv</code>求矩阵M的逆</p><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="1f98" class="lr ls it od b gy oh oi l oj ok">M = np.array([[1., 4.], [-3., 2.]])<br/>M_inverse = np.linalg.inv(M)</span><span id="3a07" class="lr ls it od b gy ol oi l oj ok">print(‘M: \n{}’.format(M))<br/>print(‘\nM_inv = \n{}’.format(M_inverse))<br/>print(‘\nM_inv * M = \n{}’.format(np.dot(M_inverse, M)))<br/>print(‘\nM * M_inv = \n{}’.format(np.dot(M, M_inverse)))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/fb4f9bc091246fa8db25d77a51a7c969.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*XR1zFOU-t-ZpjEqzUEhrZg.png"/></div></figure><p id="ad97" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">抛开这个简短的回顾，我们现在可以开始用SVD正则化模型了。</p><h1 id="946d" class="nq ls it bd lt nr ns nt lw nu nv nw lz jz nx ka mc kc ny kd mf kf nz kg mi oa bi translated">奇异值分解</h1><p id="12ae" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">在机器学习中，我们经常会遇到不能直接求逆的矩阵。相反，我们需要对a进行分解，以便计算A^−1，这可以通过SVD来实现:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi pg"><img src="../Images/5edf28d21fc5333ef868746fca59d0df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0iKonB-3gB3LyeZX"/></div></div></figure><ul class=""><li id="e5c6" class="mp mq it kx b ky kz lb lc le nk li nl lm nm lq mu mv mw mx bi translated">u是正交单位范数左奇异向量</li><li id="d60d" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">V是正交单位范数右奇异向量，V∫是共轭转置；对于实值A，这只是VT</li><li id="be52" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">d是奇异值的对角矩阵，据说定义了一个<strong class="kx iu">谱</strong></li><li id="8954" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">a由奇异值缩放的奇异向量的线性组合组成</li></ul><p id="9479" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">要计算矩阵的SVD并查看结果，请执行下面单元格中的代码。我们使用称为<code class="fe oo op oq od b">np.linalg.svd</code>的<code class="fe oo op oq od b">numpy</code>方法，使用<code class="fe oo op oq od b">s</code>获得奇异值:</p><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="1e8b" class="lr ls it od b gy oh oi l oj ok">U, s, V = np.linalg.svd(B, full_matrices=False)<br/>print(‘B: {}’.format(B)) # ESHx<br/>print(‘U: {}’.format(U))<br/>print(‘s: {}’.format(s))<br/>print(‘V: {}’.format(V))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/341317a8d879b168f91d1ccb6333cd95.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*1ekRI3ZV3OnpdqjXfp04dQ.png"/></div></figure><h1 id="19b7" class="nq ls it bd lt nr ns nt lw nu nv nw lz jz nx ka mc kc ny kd mf kf nz kg mi oa bi translated">岭和套索回归正则化</h1><p id="e4c3" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">到目前为止，我们已经研究了两种处理过度参数化模型的方法；基于逐步回归和奇异值分解的特征选择。在本节中，我们将探索基于优化的机器学习模型最广泛使用的正则化方法，<strong class="kx iu">岭回归</strong>。</p><p id="dc0a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">让我们从研究线性回归问题的矩阵方程公式开始。目标是在给定数据𝑥x向量和<strong class="kx iu">模型矩阵</strong> A的情况下，计算最小化均方残差的<strong class="kx iu">模型系数</strong>或权重的向量。我们可以将我们的模型表示为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi pi"><img src="../Images/9753d49e76845877c42f3dc3c187d26b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*xumy-Hf7L7A20P9x"/></div></div></figure><p id="70fd" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">执行下面单元格中的代码，它计算λ值为<code class="fe oo op oq od b">0.1</code>的𝐴^𝑇 * 𝐴+𝜆 )^-1 * A^T矩阵</p><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="eb32" class="lr ls it od b gy oh oi l oj ok">U, s, V = np.linalg.svd(M, full_matrices=False)</span><span id="cf42" class="lr ls it od b gy ol oi l oj ok"># Calculate the inverse singular value matrix from SVD<br/>lambda_val = 1.0<br/>d = np.diag(1. / (s + lambda_val))</span><span id="1527" class="lr ls it od b gy ol oi l oj ok">print(‘Inverse Singular Value Matrix:’)<br/>print(d)</span><span id="ae5c" class="lr ls it od b gy ol oi l oj ok"># Compute pseudo-inverse<br/>mInv = np.dot(np.transpose(V), np.dot(d, np.transpose(U)))</span><span id="4bff" class="lr ls it od b gy ol oi l oj ok">print(‘M Inverse’)<br/>print(mInv)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/b38e487aabbf304bbfc8360226a38f49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*U0SBJDS9JxYkIe06-FYOQA.png"/></div></figure><h2 id="f6c9" class="lr ls it bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">偏差-方差权衡</h2><p id="5066" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">偏差-方差权衡是机器学习模型的一个基本属性。为了理解这种权衡，让我们将模型的均方误差分解如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nf ng di nh bf ni"><div class="gh gi pk"><img src="../Images/106d640c1970bac96a2344ada8d43085.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*HlfGnbe1w6QgIHpR"/></div></div></figure><p id="bf94" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">正则化会减少方差，但会增加偏差。选择正则化参数以最小化delta_X。在许多情况下，这一过程证明是具有挑战性的，涉及大量耗时的反复试验。</p><p id="a86e" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu">不可约误差</strong>是模型精度的极限。即使你有一个完美的模型，没有偏差或方差，不可减少的误差是数据和模型中固有的。你不能做得更好。</p><p id="dc50" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><code class="fe oo op oq od b">statsmodels</code>包允许我们计算一系列岭回归解。实现这一点的函数使用了一种叫做“Elasticnet”的方法，我们知道岭回归是elastic-net的一个特例，稍后我会详细介绍这一点。</p><p id="ddfd" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu">注意:这是我之前在高尔顿家族数据集上工作的延续。</strong></p><p id="0d1e" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">下面单元格中的代码计算20个lambda值的解:</p><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="31dd" class="lr ls it od b gy oh oi l oj ok"># Ridge regression with various penalties in Statsmodels<br/># sequence of lambdas<br/>log_lambda_seq = np.linspace(-6, 2, 50)<br/>lambda_seq = np.exp(log_lambda_seq)</span><span id="db4c" class="lr ls it od b gy ol oi l oj ok">coeffs_array = []<br/>rsq_array = []<br/>formula = ‘childHeight ~ mother + father + mother_sqr + father_sqr + 1’</span><span id="8a15" class="lr ls it od b gy ol oi l oj ok">for lamb in lambda_seq:<br/> ridge_model = sm.ols(formula, data=male_df).fit_regularized(method=’elastic_net’, alpha=lamb, L1_wt=0)<br/> coeffs_array.append(list(ridge_model.params))<br/> predictions = ridge_model.fittedvalues<br/> residuals = [x — y for x, y in zip(np.squeeze(predictions), childHeight)]</span><span id="b274" class="lr ls it od b gy ol oi l oj ok">SSR = np.sum(np.square(residuals))<br/> SST = np.sum(np.square(childHeight — np.mean(childHeight)))</span><span id="126a" class="lr ls it od b gy ol oi l oj ok">rsq = 1 — (SSR / SST) <br/> rsq_array.append(rsq)</span><span id="32e1" class="lr ls it od b gy ol oi l oj ok"># pull out partial slopes (drop intercept version)<br/>beta_coeffs = [x[1:] for x in coeffs_array]<br/>plt.plot(log_lambda_seq, beta_coeffs)<br/>plt.title(‘Partial Slopes vs Log-Lambda’)<br/>plt.ylabel(‘Partial Slope Values’)<br/>plt.xlabel(‘Log-Lambda’)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/d92899f64fee965bbcf92a455549eab7.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*CbM9BBaAhnjY2ocicThBdg.png"/></div></figure><p id="526b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">绘制局部斜率与R的关系:</p><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="a0ea" class="lr ls it od b gy oh oi l oj ok"># % deviance explained<br/>plt.plot(rsq_array, beta_coeffs)<br/>plt.xlim([0.0, 0.25])<br/>plt.title(‘Partial Slopes vs Log-Lambda’)<br/>plt.xlabel(‘R-squared’)<br/>plt.ylabel(‘Partial Slopes’)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/3ca137c551bb03bb4d7afb19f57f04fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*X6YRZEO-XEzHQnUuNTus1g.png"/></div></figure><p id="a8ca" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">注意，随着lambda的增加，4个模型系数的值向零减小。当所有系数都为零时，模型预测标签的所有值都为零！换句话说，λ的高值给出了高度偏向的解，但是方差非常低。</p><p id="33c9" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">对于较小的λ值，情况正好相反。该解决方案具有较低的偏差，但相当不稳定，具有最大方差。这个<strong class="kx iu">偏差-方差权衡</strong>是机器学习中的一个关键概念。</p><h1 id="a829" class="nq ls it bd lt nr ns nt lw nu nv nw lz jz nx ka mc kc ny kd mf kf nz kg mi oa bi translated">套索回归</h1><p id="8955" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">我们也可以使用其他规范进行正则化。LASSO或L1正则化限制了模型系数绝对值的总和。L1范数有时被称为曼哈顿范数，因为距离的测量就像你在一个矩形的街道网格上行进一样。</p><p id="4a17" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">您也可以将LASSO回归视为限制模型系数向量值的L1范数。λ的值决定了系数向量的范数对解的约束程度。</p><p id="0248" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">执行与之前相同的计算，但是使用不同的正则化范数和相关联的惩罚:</p><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="d774" class="lr ls it od b gy oh oi l oj ok"># LASSO regression with a sequence of lambdas<br/># sequence of lambdas<br/>log_lambda_seq = np.linspace(-6, 2, 50)<br/>lambda_seq = np.exp(log_lambda_seq)</span><span id="6a2a" class="lr ls it od b gy ol oi l oj ok">coeffs_array = []<br/>rsq_array = []<br/>formula = ‘childHeight ~ mother + father + mother_sqr + father_sqr + 1’</span><span id="93b6" class="lr ls it od b gy ol oi l oj ok">for lamb in lambda_seq:<br/> ridge_model = sm.ols(formula, data=male_df).fit_regularized(method=’elastic_net’, alpha=lamb, L1_wt=1)<br/> coeffs_array.append(list(ridge_model.params))<br/> predictions = ridge_model.fittedvalues<br/> residuals = [x — y for x, y in zip(np.squeeze(predictions), childHeight)]</span><span id="1c5f" class="lr ls it od b gy ol oi l oj ok">SSR = np.sum(np.square(residuals))<br/> SST = np.sum(np.square(childHeight — np.mean(childHeight)))</span><span id="cc71" class="lr ls it od b gy ol oi l oj ok">rsq = 1 — (SSR / SST)<br/> rsq_array.append(rsq)</span><span id="fcfa" class="lr ls it od b gy ol oi l oj ok"># drop intercept version<br/>beta_coeffs = [x[1:] for x in coeffs_array]<br/>plt.plot(log_lambda_seq, beta_coeffs)<br/>plt.title(‘Partial Slopes vs Log-Lambda’)<br/>plt.ylabel(‘Partial Slope Values’)<br/>plt.xlabel(‘Log-Lambda’)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/88c9d4dac8337ce6a5cf0add8c7ee0e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*Itld6sXEyw_DDOeg5NdCaw.png"/></div></figure><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="b8bb" class="lr ls it od b gy oh oi l oj ok"># plot partial slopes vs R squared (% deviance explained)<br/>plt.plot(rsq_array, beta_coeffs)<br/>plt.xlim([0.0, 0.25])<br/>plt.title(‘Partial Slopes vs Log-Lambda’)<br/>plt.xlabel(‘R-squared’)<br/>plt.ylabel(‘Partial Slopes’)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi po"><img src="../Images/26d0e48ee67aed86ad4bf8db4c647313.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*atGaYDT1FbiZZWHe7SmJuQ.png"/></div></figure><p id="8d4b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">注意，模型系数比L2正则化受到更严格的约束。事实上，可能的模型系数中只有两个具有非零值！这是典型的L1或拉索回归。</p><h1 id="d048" class="nq ls it bd lt nr ns nt lw nu nv nw lz jz nx ka mc kc ny kd mf kf nz kg mi oa bi translated">弹性净回归</h1><p id="34c8" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated"><strong class="kx iu">弹性网</strong>算法使用L1和L2正则化的加权组合。正如您可能看到的，LASSO和Ridge regression使用了相同的函数，只有<code class="fe oo op oq od b">L1_wt</code>参数发生了变化。这个论点决定了偏斜率的L1范数的权重。如果正则化是纯L2(山脊)，如果正则化是纯L1(拉索)。</p><p id="6ce9" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">下面单元格中的代码对每种回归方法给予同等的权重。执行这段代码并检查结果:</p><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="f6d0" class="lr ls it od b gy oh oi l oj ok"># ElasticNet Regression with a sequence of lambdas<br/># sequence of lambdas<br/>log_lambda_seq = np.linspace(-6, 0, 50)<br/>lambda_seq = np.exp(log_lambda_seq)</span><span id="246e" class="lr ls it od b gy ol oi l oj ok">coeffs_array = []<br/>rsq_array = []<br/>formula = ‘childHeight ~ mother + father + mother_sqr + father_sqr + 1’</span><span id="1134" class="lr ls it od b gy ol oi l oj ok">for lamb in lambda_seq:<br/> ridge_model = sm.ols(formula, data=male_df).fit_regularized(method=’elastic_net’, alpha=lamb, L1_wt=0.75)<br/> coeffs_array.append(list(ridge_model.params))<br/> predictions = ridge_model.fittedvalues<br/> residuals = [x — y for x, y in zip(np.squeeze(predictions), childHeight)]</span><span id="d9ac" class="lr ls it od b gy ol oi l oj ok">SSR = np.sum(np.square(residuals))<br/> SST = np.sum(np.square(childHeight — np.mean(childHeight)))</span><span id="b58d" class="lr ls it od b gy ol oi l oj ok">rsq = 1 — (SSR / SST)<br/> rsq_array.append(rsq)</span><span id="849b" class="lr ls it od b gy ol oi l oj ok"># pull out partial slopes (drop intercept version)<br/>beta_coeffs = [x[1:] for x in coeffs_array]<br/>plt.plot(log_lambda_seq, beta_coeffs)<br/>plt.title(‘Partial Slopes vs Log-Lambda’)<br/>plt.ylabel(‘Partial Slope Values’)<br/>plt.xlabel(‘Log-Lambda’)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/4105b1225683f36fa5ed606bc92caa1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1038/format:webp/1*ZFH-W_9Mpll1C3eg5IfWkg.png"/></div></figure><pre class="kj kk kl km gt oc od oe of aw og bi"><span id="7cc2" class="lr ls it od b gy oh oi l oj ok"># plot partial slopes vs R squared (% deviance explained)<br/>plt.plot(rsq_array, beta_coeffs)<br/>plt.xlim([0.0, 0.25])<br/>plt.title(‘Partial Slopes vs Log-Lambda’)<br/>plt.xlabel(‘R-squared’)<br/>plt.ylabel(‘Partial Slopes’)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/36826cceaf0911d6e1218a6aae59755b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1040/format:webp/1*Wvq2iSKX8avYPpe5O7F9EQ.png"/></div></figure><p id="4c39" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">请注意，弹性网络模型结合了L2和L1正则化的一些行为。</p><h1 id="256f" class="nq ls it bd lt nr ns nt lw nu nv nw lz jz nx ka mc kc ny kd mf kf nz kg mi oa bi translated">结论</h1><p id="e71d" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">在这篇文章和上周的文章中，我研究了3种方法来解决过度拟合模型的问题:</p><ul class=""><li id="fbf1" class="mp mq it kx b ky kz lb lc le nk li nl lm nm lq mu mv mw mx bi translated">逐步回归，一次消除一个要素</li><li id="48b3" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">奇异值分解寻找有意义的特征</li><li id="79ef" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">套索、脊和弹性网正则化以稳定过度参数化模型</li></ul><p id="74cd" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">重要术语包括:</p><ul class=""><li id="da2b" class="mp mq it kx b ky kz lb lc le nk li nl lm nm lq mu mv mw mx bi translated">AIC-根据模型参数数量调整的模型对数似然</li><li id="851a" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">偏差——模型相对可能性的度量(方差的推广)</li><li id="b53e" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">奇异值分解将矩阵描述为一系列向量U，V，D的线性组合</li><li id="98c5" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">奇异值s-用作比例项的奇异值对角矩阵</li><li id="6fa7" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">λ—添加到奇异值上的小偏差项，用于稳定逆奇异值矩阵</li><li id="ba11" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">PCA主成分分析，通过使用函数和奇异值分解来确定可以从彼此独立的线性组合中获得哪些新特征</li><li id="5d07" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">PCR主成分回归使用PCA来执行“正则化”回归</li><li id="b8f9" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">L1正则化-使用“曼哈顿”规范的套索正则化</li><li id="9653" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">L2范数正则化-限制模型系数向量值的“欧几里德”范数的岭正则化</li><li id="225c" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">弹性网算法——L1和L2正则化的加权组合</li></ul><p id="f4d5" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我简要回顾了线性代数运算:</p><ul class=""><li id="90cd" class="mp mq it kx b ky kz lb lc le nk li nl lm nm lq mu mv mw mx bi translated">添加向量和矩阵</li><li id="e759" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">向量和矩阵相乘</li><li id="7d58" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">用<code class="fe oo op oq od b">np.transpose</code>转置矩阵</li><li id="42ed" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">用<code class="fe oo op oq od b">np.dot</code>点积或标量积或内积</li><li id="30d4" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">向量的L2范数</li><li id="ebc8" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">带<code class="fe oo op oq od b">np.eye</code>的单位矩阵</li><li id="12a7" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">用<code class="fe oo op oq od b">np.linalg.inv</code>求矩阵的逆</li><li id="1172" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">用<code class="fe oo op oq od b">np.linalg.svd</code>进行奇异值分解</li><li id="5b06" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">用<code class="fe oo op oq od b">np.diag</code>创建对角矩阵</li></ul><p id="c183" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">到目前为止，我一直严格地使用线性回归模型。接下来，我将看看线性模型上广泛使用的变体，称为<a class="ae ku" rel="noopener" target="_blank" href="/logistic-regression-cebee0728cbf"> <strong class="kx iu">逻辑回归</strong> </a> <strong class="kx iu">！</strong></p><p id="3e04" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在<a class="ae ku" href="https://www.linkedin.com/in/james-a-w-godwin/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>上找到我</p><p id="b4f9" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><em class="nd">物理学家兼数据科学家——可用于新机遇| SaaS |体育|初创企业|扩大规模</em></p></div></div>    
</body>
</html>