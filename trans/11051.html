<html>
<head>
<title>8 Terms You Should Know about Bayesian Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于贝叶斯神经网络你应该知道的8个术语</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/8-terms-you-should-know-about-bayesian-neural-network-467a16266ea0?source=collection_archive---------5-----------------------#2021-10-28">https://towardsdatascience.com/8-terms-you-should-know-about-bayesian-neural-network-467a16266ea0?source=collection_archive---------5-----------------------#2021-10-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="79a1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">先验、后验、贝叶斯定理、负对数似然、KL散度、替代、变分推理和证据下界的意义</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a638d1f233bc09d8ba84024ffd1f50a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NOs0XSgEfhHSy8yaZBXRiQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://blog.cyda.hk/" rel="noopener ugc nofollow" target="_blank"> cyda </a></p></figure><h1 id="87f9" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">目标</h1><p id="aaae" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在上一篇文章中，我们介绍了贝叶斯神经网络(BNN)。对于那些刚到BNN的人，确保你已经检查了下面的链接，以便熟悉标准神经网络(SNN)和BNN之间的区别。</p><div class="mn mo gp gr mp mq"><a rel="noopener follow" target="_blank" href="/why-you-should-use-bayesian-neural-network-aaf76732c150"><div class="mr ab fo"><div class="ms ab mt cl cj mu"><h2 class="bd iu gy z fp mv fr fs mw fu fw is bi translated">为什么你应该使用贝叶斯神经网络？</h2><div class="mx l"><h3 class="bd b gy z fp mv fr fs mw fu fw dk translated">贝叶斯神经网络解释了模型中的不确定性，并提供了权重和权重的分布</h3></div><div class="my l"><p class="bd b dl z fp mv fr fs mw fu fw dk translated">towardsdatascience.com</p></div></div><div class="mz l"><div class="na l nb nc nd mz ne ks mq"/></div></div></a></div><p id="a602" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">今天我们就跳到核心，学习背后的数学公式。从这篇文章中，你会学到不同的BNN相关术语…</p><ol class=""><li id="ec2c" class="nk nl it lt b lu nf lx ng ma nm me nn mi no mm np nq nr ns bi translated">我们如何利用<strong class="lt iu">贝叶斯推断</strong>的概念来更新模型权重和输出的概率分布。</li><li id="5b00" class="nk nl it lt b lu nt lx nu ma nv me nw mi nx mm np nq nr ns bi translated">我们将为贝叶斯神经网络使用什么具体的<strong class="lt iu">损失函数</strong>来优化模型。</li><li id="a8d8" class="nk nl it lt b lu nt lx nu ma nv me nw mi nx mm np nq nr ns bi translated">不同的<strong class="lt iu">技术和方法</strong>在现实生活场景中处理未知的分配问题。</li></ol></div><div class="ab cl ny nz hx oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="im in io ip iq"><h1 id="cb28" class="kz la it bd lb lc of le lf lg og li lj jz oh ka ll kc oi kd ln kf oj kg lp lq bi translated">贝叶斯推理</h1><p id="c0ac" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">从上一篇文章中，我们知道贝叶斯神经网络会将模型权重和输出视为变量。我们不是找到一组最优估计，而是拟合它们的概率分布。</p><p id="4d2d" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">但问题是<em class="ok">“我们怎么知道他们的分布是什么样的？”要回答这个问题，你必须了解什么是先验、后验和贝叶斯定理。下面，我们将用一个例子来说明。假设有两节课——科学课和艺术课，同学们要么戴眼镜，要么不戴眼镜。现在我们从班上随机挑选一个同学，你能说出那个同学戴眼镜的概率吗？</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/b95be7c1cd877e5fe6ed5d5b9ab21df9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GocaIEZICqxzqi_vvnboAQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://blog.cyda.hk/" rel="noopener ugc nofollow" target="_blank"> cyda </a></p></figure><blockquote class="om on oo"><p id="1faa" class="lr ls ok lt b lu nf ju lw lx ng jx lz op nh mc md oq ni mg mh or nj mk ml mm im bi translated"><strong class="lt iu"> 1。先验概率</strong> ( <strong class="lt iu">先验</strong>)</p></blockquote><p id="6b47" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">先验在考虑任何证据之前表达自己的信念。因此，在没有提供任何进一步信息的情况下，您可能会猜测该同学戴眼镜的概率为0.5，因为(30+20)/(30+20+15+35)= 50/100 = 0.5。这里，我们称0.5为先验概率。</p><blockquote class="om on oo"><p id="2d9d" class="lr ls ok lt b lu nf ju lw lx ng jx lz op nh mc md oq ni mg mh or nj mk ml mm im bi translated"><strong class="lt iu"> 2。后验概率</strong> ( <strong class="lt iu">后验</strong>)</p></blockquote><p id="6d93" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">Posterior在考虑一些证据后表达一个人的信念。让我们继续上面的例子。如果现在我告诉你那个同学其实是理科班的呢？你怎么看那个同学现在戴眼镜的概率？有了更多的信息，你可能会改变你的信念，更新概率，对吗？更新后的概率我们称之为后验概率。</p><blockquote class="om on oo"><p id="0fe2" class="lr ls ok lt b lu nf ju lw lx ng jx lz op nh mc md oq ni mg mh or nj mk ml mm im bi translated"><strong class="lt iu"> 3。贝叶斯定理</strong></p></blockquote><p id="83f3" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">贝叶斯定理是根据证据将先验概率更新为后验概率的数学公式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/6361c4d093dcf1f8fefcb9e59ec4b2b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C7tGN4RJUCVzrS99xGVX6w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://blog.cyda.hk/" rel="noopener ugc nofollow" target="_blank"> cyda </a>拍摄</p></figure><p id="b045" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">a是我们感兴趣的事件，即“该同学戴眼镜”，而X是证据，即“该同学在科学课上”。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/6725feb73e1f2e2e63162a8077c494df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k5NnEYk9xXXL8saYydepGw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://blog.cyda.hk/" rel="noopener ugc nofollow" target="_blank"> cyda </a>拍摄</p></figure><p id="22e5" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">现在你明白后验概率是如何根据证据更新的了。对于贝叶斯神经网络，权重的后验概率计算如下</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/e83c202b9a28622477f7f00295bbaffc.png" data-original-src="https://miro.medium.com/v2/resize:fit:900/format:webp/1*aOlQdlbIQjotg4YP3JEAzQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://blog.cyda.hk/" rel="noopener ugc nofollow" target="_blank"> cyda </a></p></figure></div><div class="ab cl ny nz hx oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="im in io ip iq"><h1 id="48c6" class="kz la it bd lb lc of le lf lg og li lj jz oh ka ll kc oi kd ln kf oj kg lp lq bi translated">损失函数</h1><p id="f3c8" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">现在您已经理解了更新权重和输出的公式，但是我们忽略了一件重要的事情，那就是估计概率分布的评估。在下文中，我们将讨论BNN经常使用的两种关键测量方法。</p><blockquote class="om on oo"><p id="99e7" class="lr ls ok lt b lu nf ju lw lx ng jx lz op nh mc md oq ni mg mh or nj mk ml mm im bi translated"><strong class="lt iu"> 4。负对数可能性</strong></p></blockquote><p id="65b0" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">对于回归问题，我们将始终使用均方差(MSE)作为SNN的损失函数，因为我们只有一个点估计。然而，我们将在BNN做一些不同的事情。有了预测的分布，我们将使用负对数似然作为损失函数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/4864bb8ede6228c321562749d4db5041.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8rS6qgurA0sUHn_F0_n-Pw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://blog.cyda.hk/" rel="noopener ugc nofollow" target="_blank"> cyda </a></p></figure><p id="e408" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">好了，我们一个一个来解释。</p><p id="93f8" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">似然性是作为预测分布函数的观测数据的联合概率。换句话说，我们想知道数据的分布有多像我们预测的分布。可能性越大，我们预测的分布就越准确。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/38883598a5d02c79b2f6d0ea41be157a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*Dkn3bpm_S8y3wbEgXlD4rg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://blog.cyda.hk/" rel="noopener ugc nofollow" target="_blank"> cyda </a>拍摄</p></figure><p id="d238" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">对于对数似然，我们有它是因为计算简单。通过利用对数属性(log ab = log a + log b ),我们现在可以使用求和来代替乘法。</p><p id="86b5" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">最后但同样重要的是，我们添加负号以形成负对数似然，因为在机器学习中，我们总是通过最小化成本函数或损失函数来优化目标函数，而不是最大化它。</p><blockquote class="om on oo"><p id="00b6" class="lr ls ok lt b lu nf ju lw lx ng jx lz op nh mc md oq ni mg mh or nj mk ml mm im bi translated"><strong class="lt iu"> 5。库尔贝克-莱布勒散度</strong> ( <strong class="lt iu"> KL散度</strong>)</p></blockquote><p id="e814" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">KL散度是量化一个分布和另一个分布有多大的差异。假设<em class="ok"> p </em>是真实分布，而<em class="ok"> q </em>是预测分布。实际上，它正好等于两个分布之间的交叉熵减去真实分布的熵<em class="ok"> p </em>。换句话说，它解释了预测分布<em class="ok"> q </em>还可以改进到什么程度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/8e5c2255644720018ce55ec76a2e5579.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SA9G4IcieAboZ5hmtihGHw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://blog.cyda.hk/" rel="noopener ugc nofollow" target="_blank"> cyda </a></p></figure><p id="fe61" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">对于那些不知道熵和交叉熵是什么的人来说，简单来说，熵是代表真实分布<em class="ok"> p </em>的“成本”的最低边界，而交叉熵是使用预测分布<em class="ok"> q </em>来代表真实分布<em class="ok"> p </em>的“成本”。由此，KL散度将代表预测分布<em class="ok"> q </em>的“成本”可以进一步降低多少。</p><p id="40f4" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">所以回到今天的焦点，<em class="ok"> p </em>将指模型权重和输出的真实分布，而<em class="ok"> q </em>将是我们预测的分布。我们将使用KL散度来计算两个分布之间的差异，以便更新我们预测的分布。</p></div><div class="ab cl ny nz hx oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="im in io ip iq"><h1 id="20b3" class="kz la it bd lb lc of le lf lg og li lj jz oh ka ll kc oi kd ln kf oj kg lp lq bi translated">问题与解决方案</h1><p id="2cb3" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">不幸的是，边际概率<em class="ok"> P(D) </em>通常很难处理，因为很难找到下面积分的封闭形式。源于此，对于一个复杂的系统来说，后验的<em class="ok">P(</em><strong class="lt iu"><em class="ok">w</em></strong><em class="ok">| D)</em>也是棘手的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/f6599df99af217d7acf0882bcb4aa8da.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*uJKYgsqrPc6-TEVlKZXjiA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">cyda 拍摄的照片</p></figure><p id="243b" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated"><em class="ok">为了解决这个问题，统计学家们发展了一种叫做</em> <strong class="lt iu"> <em class="ok">变分推断</em> </strong> <em class="ok">的方法，通过最小化</em> <strong class="lt iu"> <em class="ok">替代</em> </strong> <em class="ok">模型的</em> <strong class="lt iu"> <em class="ok">证据下界</em> </strong> <em class="ok">来近似真实的后验分布。</em></p><p id="f294" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">不要担心加粗的条款。我会一一解释。</p><blockquote class="om on oo"><p id="6d51" class="lr ls ok lt b lu nf ju lw lx ng jx lz op nh mc md oq ni mg mh or nj mk ml mm im bi translated"><strong class="lt iu"> 6。代理人</strong></p></blockquote><p id="b72b" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">代理模型是一个简单的模型，用来代替我们感兴趣的复杂模型。它很容易使用，并且和复杂模型一样好。一般来说，代理模型会在统计分布家族中，所以我们有它的解析解。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/4ec0514e1eb72399f00626206b98788c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bf_kI7Qmi4aH_yiLw7GcFA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://blog.cyda.hk/" rel="noopener ugc nofollow" target="_blank"> cyda </a></p></figure><blockquote class="om on oo"><p id="5051" class="lr ls ok lt b lu nf ju lw lx ng jx lz op nh mc md oq ni mg mh or nj mk ml mm im bi translated"><strong class="lt iu"> 7。变分推理(六</strong></p></blockquote><p id="af05" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">变分推断就是用一个变分分布<em class="ok"> q* </em>来代替真实的后验分布<em class="ok">p(</em><strong class="lt iu"><em class="ok">w</em></strong><em class="ok">| D)</em>的概念。但是代理模型那么多，怎么才能保证<em class="ok"> q* </em>足够好的表示<em class="ok">p(</em><strong class="lt iu"><em class="ok">w</em></strong><em class="ok">| D)</em>？答案很简单，我们可以用刚刚学过的KL散度。</p><p id="5b4a" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">在代理模型Q中，我们试图找到最优的一个</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/469fcbd8555bf1493f513188e5a9abaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*xBl95eb_jIz8Rx_OueKBSw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://blog.cyda.hk/" rel="noopener ugc nofollow" target="_blank"> cyda </a></p></figure><blockquote class="om on oo"><p id="ddf0" class="lr ls ok lt b lu nf ju lw lx ng jx lz op nh mc md oq ni mg mh or nj mk ml mm im bi translated"><strong class="lt iu"> 8。证据下界</strong></p></blockquote><p id="9f2d" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">然而，同样的问题仍然存在，因为我们没有后验概率分布。我们能做的是把KL散度改写成</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/02694f5d774806607ab26ec951e22d35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*R9zDF18CFyAwd7-Uuowd9w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://blog.cyda.hk/" rel="noopener ugc nofollow" target="_blank"> cyda </a>拍摄</p></figure><p id="dee7" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">通过考虑</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/0b35ec24bc77eeadfa527bc2740f90b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:622/format:webp/1*ScRaalx3SeKEt4KtpdfdxQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://blog.cyda.hk/" rel="noopener ugc nofollow" target="_blank"> cyda </a></p></figure><p id="3767" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">我们可以总结出下面的公式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/70928364e8f19da41eead6cebb92e21c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*xI4AZF3diLGtGNyxiLnQDw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://blog.cyda.hk/" rel="noopener ugc nofollow" target="_blank"> cyda </a></p></figure><p id="74f1" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">已知KL散度是非负数，证据是0和1之间的概率，因此对数证据必须是非正数，我们可以容易地推导出<em class="ok"> L(w) </em>是证据的下限。这就是我们称之为“证据下界”的原因。换句话说，我们现在可以通过优化找到最优的一个<em class="ok"> q* </em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/11a9a6847f50770fa53d212687ece305.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*ISRtPdTtdzIvYKoVb4KsGg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://blog.cyda.hk/" rel="noopener ugc nofollow" target="_blank"> cyda </a></p></figure></div><div class="ab cl ny nz hx oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="im in io ip iq"><h1 id="02fb" class="kz la it bd lb lc of le lf lg og li lj jz oh ka ll kc oi kd ln kf oj kg lp lq bi translated">结论</h1><p id="07ee" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">如果你已经看完了以上所有内容，恭喜你！希望你现在已经对贝叶斯神经网络背后的数学概念有了最基本的理解。在接下来的文章中，我将更多地关注关于如何使用张量流概率构建BNN模型的编码观点。敬请期待！=)</p></div></div>    
</body>
</html>