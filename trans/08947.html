<html>
<head>
<title>How to Design a Reinforcement Learning Reward Function for a Lunar Lander 🛸</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何为月球着陆器🛸设计强化学习奖励函数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-design-reinforcement-learning-reward-function-for-a-lunar-lander-562a24c393f6?source=collection_archive---------10-----------------------#2021-08-18">https://towardsdatascience.com/how-to-design-reinforcement-learning-reward-function-for-a-lunar-lander-562a24c393f6?source=collection_archive---------10-----------------------#2021-08-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/d1d8cb02d43a75839db366598f408836.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QjUZI1bCsa0pRZkIKadB_w.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">照片来源于<a class="ae kf" href="https://www.nasa.gov/sites/default/files/thumbnails/image/for_press_release.jpg" rel="noopener ugc nofollow" target="_blank">Nasa</a>；作者代码</p></figure><p id="7b19" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">想象外星人👽受到攻击，而你正试图让Lander🛸登陆月球，你会考虑哪些因素来成功完成任务？</p><p id="0fa0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下是一些注意事项:</p><ul class=""><li id="a217" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">在着陆垫上着陆<em class="ln"> vs </em>离开着陆垫</li><li id="c9ec" class="le lf it ki b kj lo kn lp kr lq kv lr kz ls ld lj lk ll lm bi translated">低速着陆<em class="ln"> vs高速坠毁</em></li><li id="9a07" class="le lf it ki b kj lo kn lp kr lq kv lr kz ls ld lj lk ll lm bi translated">使用尽可能少的燃料<em class="ln">对</em>使用大量燃料</li><li id="373e" class="le lf it ki b kj lo kn lp kr lq kv lr kz ls ld lj lk ll lm bi translated">尽快接近目标<em class="ln"> vs </em>悬在空中</li></ul><p id="2f63" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">罚什么？奖励什么？如何平衡多重约束？如何在我们的奖励函数中表现这些想法？</p></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h2 id="acc1" class="ma mb it bd mc md me dn mf mg mh dp mi kr mj mk ml kv mm mn mo kz mp mq mr ms bi translated">强化学习中的奖励函数</h2><p id="8a47" class="pw-post-body-paragraph kg kh it ki b kj mt kl km kn mu kp kq kr mv kt ku kv mw kx ky kz mx lb lc ld im bi translated">强化学习(RL)是机器学习中的一个分支，它利用代理训练中的试错问题解决方法。在我们的例子中，代理将尝试着陆月球着陆器，比如说，10k次，以学习如何在不同的状态下做出更好的行动。</p><p id="1bc4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">奖励功能是一个<strong class="ki iu">激励机制</strong>，它使用奖励和惩罚来告诉代理什么是正确的，什么是错误的。逆向物流中代理人的目标是最大化总报酬。有时我们需要牺牲眼前的回报，以最大化总回报。</p><h2 id="65b2" class="ma mb it bd mc md me dn mf mg mh dp mi kr mj mk ml kv mm mn mo kz mp mq mr ms bi translated">月球着陆器奖励函数中的规则</h2><p id="afb8" class="pw-post-body-paragraph kg kh it ki b kj mt kl km kn mu kp kq kr mv kt ku kv mw kx ky kz mx lb lc ld im bi translated">月球着陆器奖励功能中的奖励和惩罚规则的一些想法可以是:</p><ul class=""><li id="f3d2" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">以足够低的速度降落在正确的地方，给予高额奖励</li><li id="290b" class="le lf it ki b kj lo kn lp kr lq kv lr kz ls ld lj lk ll lm bi translated">如果着陆器在着陆区外着陆，则给予处罚</li><li id="0ed2" class="le lf it ki b kj lo kn lp kr lq kv lr kz ls ld lj lk ll lm bi translated">根据剩余燃料的百分比给予奖励</li><li id="7e94" class="le lf it ki b kj lo kn lp kr lq kv lr kz ls ld lj lk ll lm bi translated">如果在表面着陆时速度超过阈值(坠毁),给予很大的惩罚</li><li id="16cd" class="le lf it ki b kj lo kn lp kr lq kv lr kz ls ld lj lk ll lm bi translated">给予距离奖励以鼓励着陆器接近目标</li></ul><h2 id="f7ae" class="ma mb it bd mc md me dn mf mg mh dp mi kr mj mk ml kv mm mn mo kz mp mq mr ms bi translated">如何用python代码表示规则</h2><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi my"><img src="../Images/dc0d8304791152c786c5645cff3edba7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*av2M_B5lOnm7F3Fx"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">作者代码</p></figure><p id="026c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如上图所示，变量<strong class="ki iu"><em class="ln">fuel _ conservation</em></strong>是一个介于0和1之间的值。当成功降落在着陆场时，收到的奖励会乘以<strong class="ki iu"><em class="ln">fuel _ conservation</em></strong>以鼓励着陆器尽量少用燃料。</p><p id="d071" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果着陆器在目标点之外着陆，我们会给它一个-10的小惩罚。如果着陆器以很高的速度坠毁，我们会给一个很大的惩罚-100。</p><p id="43e7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"><em class="ln">distance _ reward = 1-(distance _ to _ goal/distance _ max)* * 0.5</em></strong>使用0.5的幂，随着着陆器越来越接近着陆垫，为代理提供平滑的奖励梯度。</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="22f4" class="ma mb it ne b gy ni nj l nk nl"># Encourage lander to use as little fuel as possible<br/># i.e. 0.85, or 0.32<br/>fuel_conservation = fuel_remaining / total_fuel</span><span id="4221" class="ma mb it ne b gy nm nj l nk nl">if distance_to_goal is decreasing:<br/>    if speed &lt; threshold:<br/>        if position is on landing pad:<br/>            # Land successfully; give a big reward<br/>            landing_reward = 100<br/>            # Multiply percentage of remaining fuel<br/>            reward = landing_reward * fuel_conservation<br/>        else:<br/>            # Landing outside of landing pad<br/>            reward = -10<br/>    else:<br/>        # Crashed<br/>        reward = -100<br/>else:<br/>    # Encourage agents to approach the surface instead of<br/>    # hanging in the air<br/>    distance_reward = 1 - (distance_to_goal / distance_max)**0.5<br/>    reward = distance_reward * fuel_conservation</span></pre></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h2 id="d4b3" class="ma mb it bd mc md me dn mf mg mh dp mi kr mj mk ml kv mm mn mo kz mp mq mr ms bi translated">结论</h2><p id="3a13" class="pw-post-body-paragraph kg kh it ki b kj mt kl km kn mu kp kq kr mv kt ku kv mw kx ky kz mx lb lc ld im bi translated">在本文中，我们以月球着陆器为例，演示如何构建一个带有奖惩规则的高级奖励函数。</p><p id="ff5e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在RL模型的培训过程中，奖励功能指导代理从试验和错误中学习:</p><ul class=""><li id="0c8b" class="le lf it ki b kj kk kn ko kr lg kv lh kz li ld lj lk ll lm bi translated">我该怎么办？如何在行动之间进行选择？</li><li id="77f1" class="le lf it ki b kj lo kn lp kr lq kv lr kz ls ld lj lk ll lm bi translated">最大化总回报的更好行动是什么？</li><li id="f834" class="le lf it ki b kj lo kn lp kr lq kv lr kz ls ld lj lk ll lm bi translated">如何评价不同状态下动作的好/坏？</li></ul><p id="6c9e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">祝着陆愉快！希望外星人会和平到来。👽☮️✌️🕊🛸</p></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h2 id="c868" class="ma mb it bd mc md me dn mf mg mh dp mi kr mj mk ml kv mm mn mo kz mp mq mr ms bi translated">参考资料:</h2><ul class=""><li id="3ab4" class="le lf it ki b kj mt kn mu kr nn kv no kz np ld lj lk ll lm bi translated"><a class="ae kf" href="https://youtu.be/CvQ-1JAPsCw" rel="noopener ugc nofollow" target="_blank">关于RL的Youtube教程视频</a></li><li id="16ef" class="le lf it ki b kj lo kn lp kr lq kv lr kz ls ld lj lk ll lm bi translated"><a class="ae kf" href="https://gym.openai.com/envs/LunarLander-v2/" rel="noopener ugc nofollow" target="_blank">open ai的lunar lander-v2</a></li></ul></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h1 id="d72e" class="nq mb it bd mc nr ns nt mf nu nv nw mi nx ny nz ml oa ob oc mo od oe of mr og bi translated">报名参加🦞:的Udemy课程</h1><h2 id="8dd1" class="ma mb it bd mc md me dn mf mg mh dp mi kr mj mk ml kv mm mn mo kz mp mq mr ms bi translated"><a class="ae kf" href="https://www.udemy.com/course/recommender-system-with-machine-learning-and-statistics/?referralCode=178D030EF728F966D62D" rel="noopener ugc nofollow" target="_blank">具有机器学习和统计的推荐系统</a></h2><figure class="mz na nb nc gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi oh"><img src="../Images/1bc21612997f8444fd0645e2051bfaf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rKOCPURU3yAr82_zvOXIJw.jpeg"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">https://www . udemy . com/course/recommender-system-with-machine-learning-and-statistics/？referral code = 178d 030 ef 728 f 966 d62d</p></figure></div></div>    
</body>
</html>