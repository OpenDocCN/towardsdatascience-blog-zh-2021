<html>
<head>
<title>Weekly review of Reinforcement Learning papers #11</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习试卷#11的每周回顾</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/weekly-review-of-reinforcement-learning-papers-11-7e1780ddf176?source=collection_archive---------39-----------------------#2021-06-08">https://towardsdatascience.com/weekly-review-of-reinforcement-learning-papers-11-7e1780ddf176?source=collection_archive---------39-----------------------#2021-06-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8e88" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">每周一，我都会发表我研究领域的4篇论文。大家来讨论一下吧！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a459be11f1f3c9ea85c758b5923a02bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*q6ahK_hpUOhG-fn5.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="d4c8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[ <a class="ae lu" rel="noopener" target="_blank" href="/weekly-review-of-reinforcement-learning-papers-10-be5947715b26?sk=3d43102b9d5c0f070360473c478ce87e"> ←上一次回顾</a> ][ <a class="ae lu" rel="noopener" target="_blank" href="/weekly-review-of-reinforcement-learning-papers-12-9ec3a81720?sk=18228793ad496e86a916a7d890e53634">下一次回顾→ </a></p><h1 id="3ea6" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">论文1:奖励就够了</h1><p id="11a3" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">西尔弗博士、辛格博士、普雷科普博士和萨顿博士(2021年)。<a class="ae lu" href="https://www.sciencedirect.com/science/article/pii/S0004370221000862?casa_token=eBT6fsPFa_0AAAAA:y7xJVFADBC686GrpoqfZXhV5myYxmg5fI4fYVo17h0m2j0Mm4L11SWMd7eidBn_2pHgrYr2wYg" rel="noopener ugc nofollow" target="_blank">奖励充足</a>。<em class="ms">人工智能</em>，103535。</p><p id="2c91" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi mt translated">本文提出的假设是，在足够复杂的环境中，报酬最大化是智能出现的充分条件。他们在这篇论文中举了一只松鼠的例子，这只松鼠想要得到尽可能多的坚果。为了实现他的目标，他必须执行许多后续任务:感知他的环境，四处移动，爬树，与其他松鼠交流，理解季节的循环……在他们的假设中，<strong class="la iu">所有这些后续任务都将被隐含地学习</strong>由于单个奖励的最大化，即获得坚果数量的最大化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/0b7966b958081999d740880ec8ab8390.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3JWOIRxlKzaw8qR9sXklQA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae lu" href="https://www.sciencedirect.com/science/article/pii/S0004370221000862?casa_token=eBT6fsPFa_0AAAAA:y7xJVFADBC686GrpoqfZXhV5myYxmg5fI4fYVo17h0m2j0Mm4L11SWMd7eidBn_2pHgrYr2wYg" rel="noopener ugc nofollow" target="_blank">文章</a>:松鼠学会了复杂的行为，这些行为是最大限度地消耗食物所必需的。这也与厨房机器人的例子相关。</p></figure><p id="7f53" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">他们以他们之前关于国际象棋和围棋的工作为例。代理人只受报酬最大化的训练。经纪人没有被教授年轻球员通常被教授的开局和战术。所以他创造了自己的开口和战术。他们被证明是非常创新的，有时与专家们所习惯的非常不同。阿尔法围棋著名的“37步棋”就是一次完美的演示，彻底动摇了这一局的所有高手。</p><p id="1adb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个假设正确吗？单一价值的最大化能产生复杂的智能吗？每个人都有自己的看法。一些人认为这是对科学结果的必要简单性的新证明，而另一些人则认为他们的解释往往是同义反复。而你，你怎么看？</p><blockquote class="nd ne nf"><p id="8a5b" class="ky kz ms la b lb lc ju ld le lf jx lg ng li lj lk nh lm ln lo ni lq lr ls lt im bi translated">一切都应该尽可能简单，但不能再简单了。——爱因斯坦</p></blockquote><h1 id="fe43" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">论文2:Android enforcement学习平台</h1><p id="58d8" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">Toyama，d .，Hamel，p .，Gergely，a .，Comanici，g .，Glaese，a .，Ahmed，z .，…&amp; pre COPD，D. (2021年)。<a class="ae lu" href="https://arxiv.org/abs/2105.13231" rel="noopener ugc nofollow" target="_blank">andro idenv:Android的强化学习平台</a>。<em class="ms"> arXiv预印本arXiv:2105.13231 </em>。</p><p id="9d75" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi mt translated">你知道gridworld，你知道Atari，你知道Mujoco控制环境。今天，你发现了androidEnv，一个与Android环境交互的RL环境。该操作系统完全模拟并封装在OpenAI gym环境中，您可以像与该框架的所有其他环境一样与它交互。观察对应于像素矩阵，动作对应于您在触摸屏上的移动。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/a429d44d831e98ed784ac4607a1e9f4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/1*n0UWDbGhRJ9AeHSpyB6gVw.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来自<a class="ae lu" href="https://deepmind.com/research/publications/androidenv" rel="noopener ugc nofollow" target="_blank">博客</a>的动画:无名氏是我们的新联系人。</p></figure><p id="b4c6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这为大量可行的任务开辟了道路。这个环境更加有趣，因为集成了两个主要元素:<br/> (1) <strong class="la iu">实时</strong>。如你所知，一个代理可以花一些时间来选择它的行动。通常，环境被阻塞，并且正在等待代理的动作。这里的情况不是这样。环境在代理的考虑时间内继续进行。此外，Android渲染可能需要一些时间:例如，当你滚动网页时，会有一个小的滑动效果，使其变得自然。这已经包括在内，所以代理人将不得不学习适应这种影响和自己的审议时间，使其更难学习。<br/> (2) <strong class="la iu">原始动作</strong>。动作规范是低级的:一个位置和触/举。就是这样。因此，代理必须学习复杂的手势，比如拖放、敲击或滑动。</p><p id="4cc6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，让我们把焦点放在你从一开始就在疑惑的问题上:要完成的任务是什么？<strong class="la iu">奖励函数</strong>是什么？实际上，这是由用户通过操作系统日志来指定的。让我们举一个例子:我希望我的代理将John Doe添加到我的联系人中。如果完成此任务，则可能会添加一个“<em class="ms">【信息】【2021/06/07】John Doe”形式的系统日志作为新联系人。“通过检测这种日志的出现，我可以决定何时给我的代理人报酬。是的，这有点复杂，但我想这是他们找到的指定奖励的最干净的方法。</em></p><h1 id="97bb" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">论文3:走向更深层次的强化学习</h1><p id="c70e" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">Bjorck，j .、Gomes，C. P .和Weinberger，K. Q .，<a class="ae lu" href="https://arxiv.org/abs/2106.01151" rel="noopener ugc nofollow" target="_blank">走向更深的深度强化学习</a>，<em class="ms"> arXiv预印本arXiv:2106.01151 </em>。</p><p id="cc00" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi mt translated"><span class="l mu mv mw bm mx my mz na nb di">正如</span>你可能已经注意到，在深度强化学习中，最佳神经网络结构的问题很少被问到。基本上，如果观察是一个图像，我们用CNN，如果不是，用MLP。在RL中，主要致力于算法:基于值还是基于策略？探索还是剥削？正规化与否？基于模型与否？很明显，从来没有人问过神经网络的类型问题。</p><p id="e1a2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这篇文章中，作者讨论了使用最新神经网络的影响，使用现代技巧，如跳过连接。目标是提高学习性能，同时保持相同的算法，这里是软演员评论家(SAC)。那么在你看来，神经网络的选择有多重要？</p><p id="260f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">作者研究了增加标准化和剩余连接对SAC的影响。如果天真地实现，很可能造成模型的不稳定。因此，他们建议使用一种通过频谱归一化进行平滑的<strong class="la iu"/>(图中的“平滑”)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/01009697f404048c82884364a0182e7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IhIxiittcjpbanhYG3gWzw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">摘自<a class="ae lu" href="https://arxiv.org/abs/2106.01151" rel="noopener ugc nofollow" target="_blank">文章</a>中的一幅图:使用现代架构的SAC的学习曲线(归一化和剩余连接)，平滑和不平滑。1水平增量= 10万时间步长。</p></figure><p id="157e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这种平滑允许稳定学习。在连续控制任务上获得的结果通常比在网络架构上不工作时获得的结果好得多。</p><p id="ef58" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">结论是显而易见的:我们必须花时间研究RL中的网络架构。这会对学习成绩产生重要影响。这是每个人或多或少都有的直觉，感谢他们一劳永逸地证明了这一点。</p><h1 id="e9d1" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">论文4:决策转换器:通过序列建模的强化学习</h1><p id="18c1" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">Chen，l .，Lu，k .，Rajeswaran，a .，Lee，k .，Grover，a .，Laskin，m .，Abbeel，p .，Srinivas，a .，Mordatch，I. (2021)。<a class="ae lu" href="https://arxiv.org/abs/2106.01345" rel="noopener ugc nofollow" target="_blank">决策转换器:通过序列建模进行强化学习</a>。<em class="ms"> arXiv预印本arXiv:2106.01345 </em>。</p><p id="32cf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi mt translated">增强学习可以利用语言建模学习的巨大进步。特别是，它可以利用变压器的力量，这是一种最近的神经网络架构，它大大提高了学习性能，包括一种注意力机制。</p><p id="137d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">作者提出了决策转换器，一种将RL问题表示为条件序列建模的架构。忘记你所知道的政策梯度或基于价值的学习方法。方法不同。在这里，它不再是一个最大化回报函数的问题，而是决定允许达到预期回报的行动。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/8dad160c43cb53ee2d0b9576198a6d95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NFzMWJ6lEqUJ6Rc3OEPTKA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来自<a class="ae lu" href="https://arxiv.org/abs/2106.01345" rel="noopener ugc nofollow" target="_blank">论文</a>的图:决策转换器架构</p></figure><p id="c26b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">基本上，给定一个交互数据集，transformer agent将学习哪个动作导致了哪个奖励。将状态和期望回报作为输入，代理将<strong class="la iu">预测获得期望回报的概率最大化的行动</strong>。因此，如果我们想获得最大的奖励，我们只需要在输入中指定最大可能的奖励。</p><p id="edb9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">为什么这种方法比传统的RL方法更有效？</strong>主要原因是transform能够将奖励与其产生的行为联系起来。如你所知，奖励是之前所有行为的产物。但是每个行动都或多或少地促成了这种回报。他们中的一些人甚至强烈限制了奖励。当环境具有这种特征(行为远离其结果)时，经典的LR方法就有麻烦了。Transformer在这方面处理的非常好。</p><p id="dc1c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然而，在现阶段，有些事情我还不清楚。为什么我们会希望一个代理人能够准确地获得不是最大的回报？在一个状态下，代理人如何知道它可以要求的最大报酬？(不行，每次要求十亿的奖励是不行的。)希望能尽快发布更多解释。</p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><p id="e46a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">谢谢你把我的文章看完。我很乐意阅读你的评论。</p></div></div>    
</body>
</html>