<html>
<head>
<title>Paper explained: Unsupervised Learning of Visual Features by Contrasting Cluster Assignments</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">论文解释:通过对比聚类分配的视觉特征的无监督学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/paper-explained-unsupervised-learning-of-visual-features-by-contrasting-cluster-assignments-f9e87db3cb9b?source=collection_archive---------10-----------------------#2021-12-03">https://towardsdatascience.com/paper-explained-unsupervised-learning-of-visual-features-by-contrasting-cluster-assignments-f9e87db3cb9b?source=collection_archive---------10-----------------------#2021-12-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ae8c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">为什么SwAV模型架构对自我监督的预培训如此有效的秘密</h2></div><p id="2e4d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">多年来，教会神经网络在没有人类监督的情况下理解周围的世界一直是计算机视觉研究社区的北极星之一。最近，多个出版物显示了新方法在该领域取得重大进展的潜力。</p><p id="aa9f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">发表的最有前景的方法之一是Caron等人从今年开始发表的一篇名为<a class="ae le" href="https://arxiv.org/pdf/2006.09882.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lf">“通过对比聚类分配进行视觉特征的无监督学习”</em> </a>的论文。在这篇文章中，我们将回顾作者介绍的思想，SwAV模型架构以及自我监督预训练的结果。我尽量让文章简单，这样即使没有什么先验知识的读者也能理解。事不宜迟，我们开始吧！</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi lg"><img src="../Images/1ac3968e78ff365c073cec0e24453810.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RShBX0goWoe_26hJ2z7xqA.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">SwAV训练程序的可视化。来源:<a class="ae le" href="https://github.com/facebookresearch/swav" rel="noopener ugc nofollow" target="_blank">【1】</a></p></figure><h1 id="9673" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">先决条件:计算机视觉的自我监督预培训</h1><p id="545d" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">在我们深入研究SwAV论文之前，有必要快速回顾一下自我监督预培训是怎么回事。如果你熟悉自我监督的预备培训，可以跳过这一部分。</p><p id="b489" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">传统上，计算机视觉模型总是使用<strong class="kk iu">监督学习</strong>来训练。这意味着人类看着这些图像，并为它们创建了各种标签，这样模型就可以学习这些标签的模式。例如，人类注释者可以为图像分配一个类标签，或者在图像中的对象周围绘制边界框。但是，任何接触过标注任务的人都知道，创建足够的训练数据集的工作量很大。</p><p id="9e7b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">相比之下，<strong class="kk iu">自我监督学习不需要任何人为创造的标签</strong>。顾名思义，<strong class="kk iu">模特学会自我监督</strong>。在计算机视觉中，对这种自我监督进行建模的最常见方式是获取图像的不同裁剪或对其应用不同的增强，并通过模型传递修改后的输入。尽管图像包含相同的视觉信息，但看起来并不相同，<strong class="kk iu">我们让模型知道这些图像仍然包含相同的视觉信息</strong>，即相同的对象。<strong class="kk iu">这导致模型学习相同对象的相似潜在表示(输出向量)。</strong></p><p id="3941" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以稍后在这个预训练的模型上应用迁移学习。通常，这些模型然后在10%的带有标签的数据上进行训练，以执行下游任务，如对象检测和语义分割。</p><h1 id="61bb" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">SwAV如何使用无监督学习来理解周围的视觉世界</h1><p id="e48d" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">SwAV的培训管道分为多个阶段。</p><p id="9cca" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在第一部分中，创建图像的不同裁剪。其他技术总是使用固定的大小来裁剪图像，而SwAV引入了多裁剪策略:不仅创建了两个较大的裁剪，还拍摄了多达4个较小的裁剪。这是SwAV与其他方法相比能够提升其性能的关键之一。也可以应用其他图像增强，如模糊和翻转。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi mt"><img src="../Images/9a21f71cb9bb177bfbd975547ee8ffc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XI2j-1I57Fm08ioqxU1Vxg.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">SwAV从同一幅图像中创建不同大小的作物进行训练。这被称为多作物策略。来源:自己的策展，图片来自<a class="ae le" href="https://unsplash.com/photos/UJh4sYnVmNE" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="6655" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来，图像通过卷积神经网络成对传递<strong class="kk iu">。在论文中，作者使用了流行的<a class="ae le" href="https://arxiv.org/pdf/1512.03385.pdf" rel="noopener ugc nofollow" target="_blank"> ResNet-50 </a>架构，他们发现随着模型宽度的增加，性能也有所提高。对于每一个图像，ResNet将输出一个维数为128的特征向量。</strong></p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi mu"><img src="../Images/459ebe0c88583f87f884b53e2749e1e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ytj-GUmE_7HXhNckCJADbA.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">该可视化显示了将图像的每种不同裁剪输入到ResNet，ResNet输出每种图像的矢量表示。来源:<a class="ae le" href="https://github.com/facebookresearch/swav" rel="noopener ugc nofollow" target="_blank">【1】</a></p></figure><p id="30e4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦这些特征向量或代码被计算出来，它们就与所谓的原型向量相匹配。这些是定义可学习视觉特征空间之间的可能区别的空间的向量，例如，狗和猫之间的区别由不同的原型向量表示。制定代码到原型的映射，以最大化图像特征和原型之间的相似性。这个过程的输出是一个集群分配。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi mv"><img src="../Images/3b5590ca83c9e62e19d85889c90a854d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cTlnpvwz2LbdR_NwgPkCwA.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">将输出特征向量匹配到原型向量空间导致将向量分配给原型聚类。来源:<a class="ae le" href="https://github.com/facebookresearch/swav" rel="noopener ugc nofollow" target="_blank">【1】</a></p></figure><p id="9c26" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们已经创建了这些聚类分配，我们做一个假设:<strong class="kk iu">如果输入的作物来自同一幅图像，我们希望以这样一种方式学习模型，即它为同一幅图像的作物输出相似的向量。</strong></p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi mw"><img src="../Images/da47113ee8ddf5d04da2ce24c0e864e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vNpOlrOP3OaA27UI6KG88g.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">匹配聚类分配向量的简化可视化。来源:<a class="ae le" href="https://github.com/facebookresearch/swav" rel="noopener ugc nofollow" target="_blank">【1】</a></p></figure><p id="1553" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因此，应该可以交换两个输出向量(作物1得到向量2，作物2得到向量1)并且<strong class="kk iu">使模型学习预测另一个输入作物</strong>的向量输出。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi mx"><img src="../Images/ffc7de8fdad8f3ca9502739b1c779a67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fORrkuV2pmujlC-SpI7eVw.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">交换集群分配。这成为新的训练目标。来源:<a class="ae le" href="https://github.com/facebookresearch/swav" rel="noopener ugc nofollow" target="_blank">【1】</a></p></figure><p id="480c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些交换的向量现在是另一个输入的预测目标，现在成为我们的学习目标，我们用它来反向传播模型的权重。</p><p id="07f2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种机制允许SwAV学习相同图像或对象类的<strong class="kk iu">一致矢量表示</strong>。这种能力对下游迁移学习至关重要。</p><p id="3013" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，让我们看看一些结果。</p><h1 id="b049" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">结果</h1><p id="db84" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">当仅使用来自预训练的冻结权重在ImageNet上对线性分类评估SwAV模型时，它优于所有其他最先进的自我监督方法。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi my"><img src="../Images/6d9fe21b8c9f43e1bd79c03a24f0db50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GSiNELwahCr_1QhFlhmYlA.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">SwAV在ImageNet数据集上的性能。来源:<a class="ae le" href="https://arxiv.org/pdf/2006.09882.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a></p></figure><p id="aef1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最令人印象深刻的是，自我监督模型的<strong class="kk iu">性能几乎与仅使用注释数据的完全监督模型的性能相当。</strong>所有评估均使用ResNet-50 (R50)骨架进行。在右图中，更多参数的性能差异表明SwAV也优于其他方法。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi mz"><img src="../Images/899296698b7ece5192046a3110f382f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oukgGTeLvlcTKtJVmXWyaw.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">对SwAV的评估与对差异数据集的全监督训练程序的比较。来源:<a class="ae le" href="https://arxiv.org/pdf/2006.09882.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a></p></figure><p id="8ea8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">线性分类的这种性能优势在其他数据集上也同样适用。令人难以置信的是，当用作Mask R-CNN等对象检测模型的主干并仅使用ImageNet中10%的标记数据进行微调时，所示的所有对象检测架构都优于相同模型架构的完全监督训练。<strong class="kk iu">这意味着对象检测主干的自我监督预训练可以产生比完全监督训练模型更好的性能。</strong></p><h1 id="5376" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">包装它</h1><p id="a8d8" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">在本文中，您已经了解了SwAV，这是一种利用自我监督的预训练来为下游任务(如对象检测)实现新的性能高度的论文。在某些任务上，它甚至优于完全监督的方法。虽然我希望这个故事能让你对这篇论文有一个很好的初步了解，但是还有很多东西需要发现。因此，我会鼓励你自己阅读这篇论文，即使你是这个领域的新手。你必须从某个地方开始；)</p><p id="ee0a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你对论文中介绍的方法有更多的细节感兴趣，请随时在Twitter上给我留言，我的账户链接在我的媒体简介上。</p><p id="8b85" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我希望你喜欢这篇论文的解释。如果你对这篇文章有任何意见，或者如果你看到任何错误，请随时留下评论。</p><p id="4dc6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">最后但同样重要的是，如果你想在高级计算机视觉领域更深入地探索，考虑成为我的追随者</strong>。我试着每周发一篇文章，让你和其他人了解计算机视觉研究的最新进展。</p></div><div class="ab cl na nb hx nc" role="separator"><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf ng"/><span class="nd bw bk ne nf"/></div><div class="im in io ip iq"><p id="9d31" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">参考资料:</p><p id="6fe8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[1] SwAV GitHub实现:【https://github.com/facebookresearch/swav T2】</p><p id="8f9c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2]卡隆、玛蒂尔德等人，“通过对比聚类分配对视觉特征进行无监督学习。”<em class="lf"> arXiv预印本arXiv:2006.09882 </em> (2020)。<a class="ae le" href="https://arxiv.org/pdf/2006.09882.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2006.09882.pdf</a></p></div></div>    
</body>
</html>