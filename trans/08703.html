<html>
<head>
<title>Learnings from reproducing DQN for Atari games</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为雅达利游戏复制DQN的启示</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learnings-from-reproducing-dqn-for-atari-games-1630d35f01a9?source=collection_archive---------18-----------------------#2021-08-11">https://towardsdatascience.com/learnings-from-reproducing-dqn-for-atari-games-1630d35f01a9?source=collection_archive---------18-----------------------#2021-08-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="3955" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">从零开始实现这种强化学习算法的真实写照</h2></div><p id="2388" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">深度学习研究人员倾向于说，获得更好理解和实践的最快和最有效的方法之一是复制关键论文的结果。我决定试一试，从谷歌DeepMind的开创性论文<a class="ae lb" href="https://www.nature.com/articles/nature14236" rel="noopener ugc nofollow" target="_blank">“通过深度强化学习实现人类水平的控制”</a>开始，谷歌Deep mind的深度Q网络(DQN)算法在人类甚至超人的水平上玩经典的Atari游戏(如Pong和Breakout)。更令人惊讶的是，它的性能推广到大约50个Atari游戏，使用相同的算法和相同的超参数！</p><p id="7037" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这里，我展示了我的过程、学习和结果的总结，特别是列出了我遇到的所有错误(甚至是愚蠢的或非常具体的错误！)—我的目的是展示这个项目的原始、真实的写照，这是过去的我在开始时会欣赏的。我还在GitHub上开源了我的项目，对于那些对具体实现好奇的人来说:【https://github.com/dennischenfeng/dqn<a class="ae lb" href="https://github.com/dennischenfeng/dqn" rel="noopener ugc nofollow" target="_blank"/>。快速声明:这不是一个操作指南。在为Atari游戏构建DQN方面，有很多好的指南；我只是想分享我的经历。</p><p id="3c7f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在完成这个项目后，我认为它并不像我最初预期的那样令人生畏(它只是需要持续的坚持)，我希望像你一样的读者在阅读后会有同样的感觉！举个例子，我有编写软件的专业经验，但我只是在过去几个月里才真正投入到深度强化学习的研究和实践中。</p><h1 id="742d" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">内容</h1><ol class=""><li id="572d" class="lu lv iq kh b ki lw kl lx ko ly ks lz kw ma la mb mc md me bi translated"><a class="ae lb" href="#8cd3" rel="noopener ugc nofollow">介绍材料</a></li><li id="8b5a" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated"><a class="ae lb" href="#11cf" rel="noopener ugc nofollow">第1部分:实现代码初稿</a></li><li id="8ee7" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated"><a class="ae lb" href="#00cf" rel="noopener ugc nofollow">第2部分:简易基准测试</a></li><li id="d0b0" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated"><a class="ae lb" href="#1c85" rel="noopener ugc nofollow">第3部分:Atari环境下的测试</a></li><li id="c318" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated"><a class="ae lb" href="#df0c" rel="noopener ugc nofollow">第4部分:简易基准测试(重温)</a></li><li id="8ebb" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated"><a class="ae lb" href="#cbf6" rel="noopener ugc nofollow">第5部分:Atari环境下的测试(重访)</a></li><li id="4674" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated"><a class="ae lb" href="#6239" rel="noopener ugc nofollow">结论</a></li><li id="35d4" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated"><a class="ae lb" href="#6fcf" rel="noopener ugc nofollow">附录:我遇到的其他bug</a></li></ol><h1 id="8cd3" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">介绍材料</h1><p id="deb2" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">在我开始这个项目之前，我已经做了一些阅读和免费的在线课程。为此，我建议:</p><ul class=""><li id="d4c4" class="lu lv iq kh b ki kj kl km ko mn ks mo kw mp la mq mc md me bi translated"><a class="ae lb" href="https://course.fast.ai/" rel="noopener ugc nofollow" target="_blank"> fast.ai的《程序员实用深度学习》课程</a></li><li id="59bf" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mq mc md me bi translated"><a class="ae lb" href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html" rel="noopener ugc nofollow" target="_blank"> OpenAI Spinning Up的短短3页《RL入门》系列</a></li><li id="5eb7" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mq mc md me bi translated"><a class="ae lb" href="https://www.youtube.com/playlist?list=PL_iWQOsE6TfURIIhCrlt-wj9ByIVpbfGc" rel="noopener ugc nofollow" target="_blank">加州大学柏克莱分校“CS 285:深度强化学习”视频讲座</a></li></ul><p id="35d2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">激励我开始这个项目的其他优秀资源:</p><ul class=""><li id="5ec6" class="lu lv iq kh b ki kj kl km ko mn ks mo kw mp la mq mc md me bi translated"><a class="ae lb" href="http://amid.fish/reproducing-deep-rl" rel="noopener ugc nofollow" target="_blank">马修·拉兹的《经验教训再现深度强化学习论文》</a>。关于他从实现一个比我在这里做的更广泛的算法中学到的东西的精彩阅读。</li><li id="9ff1" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mq mc md me bi translated"><a class="ae lb" href="https://www.youtube.com/watch?v=8EcdaCk9KaQ" rel="noopener ugc nofollow" target="_blank">约翰·舒尔曼的“深度RL研究的具体细节”视频</a>。关于深度RL的好建议和直觉，甚至给出了一些具体实现DQN的技巧。</li></ul><h1 id="11cf" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">第1部分:实现代码的初稿</h1><p id="6fae" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">在阅读了<a class="ae lb" href="https://www.nature.com/articles/nature14236" rel="noopener ugc nofollow" target="_blank">论文</a>(尤其是包含伪代码和使用的超参数的补充信息部分)之后，我首先将论文的伪代码翻译成代码。下面是DQN算法的一个粗略的概念性分解(遵循本文中的伪代码):</p><ol class=""><li id="6f7a" class="lu lv iq kh b ki kj kl km ko mn ks mo kw mp la mb mc md me bi translated">在环境中执行一个动作(雅达利游戏)。用概率ε (epsilon)随机选择动作。否则，选择“最佳”行动，即我们选择基于当前行动价值估计值q的最大化价值(回报)的行动。注意，ε在训练过程中缓慢减少(退火),以减少训练后期的探索量(通过随机行动表现出来)。</li><li id="a73f" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">接收观察(游戏屏幕图像快照)和奖励(游戏分数的增值)并将数据存储在“重放存储器”中为了更新Q，我们将重复地从重放记忆中取样，类似于我们作为人如何从记忆中学习和改进我们的决策。</li><li id="85b0" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">从重放记忆中采样数据，粗略地把它当作带标签的训练数据(观察和动作是输入，奖励是标签)，并采取梯度下降步骤来更新q。</li><li id="5a87" class="lu lv iq kh b ki mf kl mg ko mh ks mi kw mj la mb mc md me bi translated">重复一遍。</li></ol><p id="1e18" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我编写代码的时候，将独立的部分模块化，将它们的实现从主算法中分离出来是至关重要的。例如，对于退火的epsilon(通过训练减少探索)、重放存储器(回放过去转换的存储容器)和观察的预处理(跳帧、灰度、裁剪、堆叠帧)，我为其中的每一个创建了独立的函数/模块，以减少DQN实现中的交互和复杂性。</p><p id="b4e5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，为每个重要的部分和功能编写单元测试，最终在整个项目中为我节省了大量调试时间(如预期的那样),所以这也是值得的。</p><h1 id="00cf" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">第2部分:简单基准测试</h1><p id="d1ac" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">为了测试代码，我试图在CartPole环境(env)上训练模型，从<a class="ae lb" href="https://gym.openai.com/" rel="noopener ugc nofollow" target="_blank"> OpenAI的</a> <code class="fe mr ms mt mu b"><a class="ae lb" href="https://gym.openai.com/" rel="noopener ugc nofollow" target="_blank">gym</a></code>。</p><p id="4986" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后经过一些学习率的调整，我开始看到一些生命的迹象:剧集回归是在100到200的数量级上，而最大值是500。</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi mv"><img src="../Images/346a749513fe76925b727243f0a9197d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DMn-04HsDkBx4i2EHdxVWA.png"/></div></div><p class="nh ni gj gh gi nj nk bd b be z dk translated">在横竿环境下进行三次训练。(左)X轴是包络步骤的数量，y轴是10次评估发作的平均发作回报(得分)。注意:横竿最高每集收益为500。(右)X轴也是包络步数，y轴是训练损耗。(图片来源:作者)</p></figure><p id="9daf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">仅仅通过手动运行一些不同的超参数迭代，我无法实现达到500的训练模型，所以我转向使用<a class="ae lb" href="https://optuna.readthedocs.io/en/stable/" rel="noopener ugc nofollow" target="_blank"> Optuna </a>进行通宵自动化100次试验超参数研究。它似乎找到了达到最高分的超参数配置，所以我很满意，并进入下一步。<strong class="kh ir">(可惜后来才发现这是侥幸；参见</strong> <a class="ae lb" href="#df0c" rel="noopener ugc nofollow"> <strong class="kh ir">第四部分</strong> </a> <strong class="kh ir">)。</strong></p><figure class="mw mx my mz gt na gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nl"><img src="../Images/56c85b2f98afa6901ecf7a1607d1f494.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*c_D3eozk-qmdxABR"/></div></div><p class="nh ni gj gh gi nj nk bd b be z dk translated">平行坐标图(由Optuna生成),用于100次具有不同超参数的训练运行。从左到右的每条曲线代表一次训练，与纵轴的交点表示该次训练中每个超参数的选定值。如颜色条所示，深蓝色曲线表示目标值较高(剧集回归)。(图片来源:作者)</p></figure><h1 id="1c85" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">第3部分:Atari环境下的测试</h1><p id="d83e" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">通过查看DQN在各种雅达利环境下的训练曲线(参见<a class="ae lb" href="https://arxiv.org/pdf/1710.02298.pdf" rel="noopener ugc nofollow" target="_blank">彩虹DQN论文</a>的附录)，我选择了Pong和Breakout作为开始，因为它们在训练早期具有明显的正斜率，也因为我在年轻时玩它们时最了解它们。</p><p id="7988" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nm">【Bug】在我第一次运行突围的时候，我的评估步骤是拖延进度→原来突围需要你按下“开火”按钮来产生球，否则游戏会闲置！最初，我创建了一个按“FIRE”启动的包装器env，但后来我创建了一个在空闲后终止游戏的包装器env(因为我希望模型学习像按“FIRE”这样重要的东西)。</em></p><p id="11a6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这一点上，我的模型实际上是在训练，但它是以蜗牛的速度训练的——大约每小时100k步。为了给出背景，原始论文为每个模型训练200米环境步骤(4⨉50米更新步骤)！显然我需要加快速度；有理由相信我的代码中隐藏着一些低效之处。我求助于<code class="fe mr ms mt mu b">CProfile</code>，这是一个方便的内置python包，可以分析函数/方法调用的运行时。</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><div class="gh gi nn"><img src="../Images/bd17ef29ce4970033d8cfbd2f6438f5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jE_rIyTVJaYrY5Z_CPNw1Q.png"/></div></div><p class="nh ni gj gh gi nj nk bd b be z dk translated">训练运行的CProfile输出示例。“tottime”列显示了最后一列中指定的特定函数的计算持续时间(以秒为单位)。(图片来源:作者)</p></figure><p id="a7e7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nm">【Bug】使用</em> <code class="fe mr ms mt mu b"><em class="nm">Cprofile</em></code> <em class="nm">，我发现了一个令人费解的怪癖，将一个大型numpy数组列表转换成pytorch张量的效率很低→在转换成pytorch张量之前先转换成numpy数组要快得多。</em></p><p id="2ed5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有一段时间，我没有取得太大的性能增益。我试图通过在每一步给它一个参数化的固定奖励来使env变得更容易(就像在CartPole中)，我增加了minibatch的大小以减少随机性，并且我试图通过重复保存模型并在下一次直接培训课程开始时加载它来训练更多的env步骤(我必须这样做，因为我的培训课程仅限于给定的小时数)。</p><p id="2b19" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">尽管如此，性能并不比随机好多少，即使在5到10M包络步长之后。有点沮丧，但这意味着我可能在某个地方犯了一个至关重要的错误，所以我回到了更简单的环境，通过更快的反馈循环来分析性能。</p><h1 id="df0c" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">第4部分:简单基准测试(重温)</h1><p id="9578" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">啊哦。我使用之前“优化”的超参数在CartPole上进行了3次重复训练，得到了以下结果:</p><figure class="mw mx my mz gt na gh gi paragraph-image"><div class="gh gi no"><img src="../Images/f6652228ba92071b429bd763bbd7128f.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*5cyZjjt_sIDCywbnG2oBew.png"/></div><p class="nh ni gj gh gi nj nk bd b be z dk translated">三次(平稳的)横撑训练。x轴是包络步骤的数量，y轴是10次评估发作的平均发作回报。(图片来源:作者)</p></figure><p id="fa7d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">看起来每次运行的方差太高了；有时候，在整个训练过程中，剧集回报率都低于50，这是非常糟糕的。原来我早期的基准测试是侥幸的！</p><p id="d19a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="nm">【大错】在我最初的基准测试中，我跳过了重复性/方差研究，因为我太兴奋了，以至于无法继续进行Atari envs。这花费了我几个星期的努力。</em> </strong></p><p id="fc15" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过仔细检查我的代码，大量使用<code class="fe mr ms mt mu b">Cprofile</code>来识别效率低下的地方，并与一个已知良好的DQN开源实现(来自<a class="ae lb" href="https://github.com/DLR-RM/stable-baselines3" rel="noopener ugc nofollow" target="_blank">稳定基线3 </a>)进行比较，我消除了一些错误。</p><p id="450a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nm">【Bug】我的损失函数里有一个因子2错了，因为我实现平滑不正确→我最后用了pytorch </em> <a class="ae lb" href="https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html" rel="noopener ugc nofollow" target="_blank"> <em class="nm">的</em> </a> <code class="fe mr ms mt mu b"><a class="ae lb" href="https://pytorch.org/docs/stable/generated/torch.nn.SmoothL1Loss.html" rel="noopener ugc nofollow" target="_blank"><em class="nm">smooth_l1_loss</em></a></code> <em class="nm">。</em></p><p id="e3ba" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nm">【Bug】我在用</em> <code class="fe mr ms mt mu b"><em class="nm">np.random.choice</em></code> <em class="nm">(带替换)对重放内存进行采样，</em> <code class="fe mr ms mt mu b"><em class="nm">Cprofile</em></code> <em class="nm">告诉我效率相当低→用</em> <code class="fe mr ms mt mu b"><em class="nm">np.random.randint</em></code> <em class="nm">显著降低了运行时间。</em></p><p id="0286" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nm">【Bug】不小心对每一个梯度步长运行了一个通过目标网络的反向传播(应该长期保持不变)！→在目标网络操作过程中使用</em> <code class="fe mr ms mt mu b"><em class="nm">torch.no_grad</em></code> <em class="nm">(抑制梯度计算)消除了不必要的反向通道。</em></p><p id="41fd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nm">【Bug】一个showstopper bug:我不小心把</em> <code class="fe mr ms mt mu b"><em class="nm">done</em></code> <em class="nm">写成了</em> <code class="fe mr ms mt mu b"><em class="nm">(1 — done)</em></code> <em class="nm">，其中</em> <code class="fe mr ms mt mu b"><em class="nm">done</em></code> <em class="nm">不是0就是1，有效地给了Q更新一集是否终止的完整相反信息。哦。</em></p><p id="74cd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，在排除了所有这些错误之后，模型可以可靠地求解CartPole:</p><div class="mw mx my mz gt ab cb"><figure class="np na nq nr ns nt nu paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><img src="../Images/021d66e6f0ff794ca9ff96e59f6d4f34.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*xb6fZARODp0diP7R2nHKtw.png"/></div></figure><figure class="np na nv nr ns nt nu paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><img src="../Images/1209c0a77331131f881d018f5dd2b258.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*gPNZg6Cplw8ECw5vSxJL4Q.png"/></div></figure><figure class="np na nw nr ns nt nu paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><img src="../Images/90f148d3bc1e2a09e9c93757ab0b942b.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/1*zK5cnq2loGDiT9RWLjxv1g.gif"/></div><p class="nh ni gj gh gi nj nk bd b be z dk nx di ny nz translated">(左)10次横撑训练的平均值。指示1个标准误差的误差带为红色。(中间)代表训练运行，其中x轴是包络步数，y轴是超过100个评估集的平均集返回。(右)一个训练有素的代理人的游戏，他的目标是移动手推车，使杆子保持平衡而不倾倒。(图片和gif来源:作者)</p></figure></div><p id="c056" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当代理可以在连续100集内达到475或更高的平均集回报率时，我们的模型可以稳健地实现这一点(每次运行在培训的某个点达到500)，则认为CartPole环境(<code class="fe mr ms mt mu b">Cartpole-v1</code>)已经解决。</p><p id="f7b0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我从CartPole上的DQN基准测试中学到的一件事是:不要太担心最初不稳定的性能下降，因为只要有足够的训练，它仍然能够解决环境问题。挑剔的性能下降可能是由于在训练期间发生的分布变化，例如，在模型快速学习的早期，观察值(测试分布)将偏离训练分布。</p><p id="52f5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了多样化，我也在FrozenLake上测试:</p><div class="mw mx my mz gt ab cb"><figure class="np na oa nr ns nt nu paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><img src="../Images/57434ae97c50610814387703bdd90a4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*kdAxTREJ3X2EPy58Yv_beA.png"/></div></figure><figure class="np na ob nr ns nt nu paragraph-image"><img src="../Images/7e1019a1b6ffd92a36c693e4a0ddb937.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*I_IFmRRuaNuP903Ga4t9_Q.png"/></figure><figure class="np na oc nr ns nt nu paragraph-image"><img src="../Images/4c7b6946b304d75dfbd235a9389adad2.png" data-original-src="https://miro.medium.com/v2/resize:fit:162/1*9vjrwqpd9IXlD8mP-Rwi4w.gif"/><p class="nh ni gj gh gi nj nk bd b be z dk od di oe nz translated">(左)FrozenLake上10次训练跑步的平均值。指示1个标准误差的误差带为红色。(中间)代表训练运行，其中x轴是包络步数，y轴是超过100个评估集的平均集返回。(右)一个完全训练有素的代理的游戏性，其目标是通过走过冻结的空间F从开始位置S导航到目标位置G，而不会掉进洞空间h。问题是地板很滑，实际的迈步方向可以从预定方向随机旋转90°。屏幕顶部会显示代理每一步的输入方向。(图片和gif来源:作者)</p></figure></div><p id="6bcd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当代理人的平均回报阈值超过0.78时，认为FrozenLake ( <code class="fe mr ms mt mu b">FrozenLake-v0</code>)已解决。看起来我们的模型也达到了这一点(在训练过程中的某个时刻超过阈值)！</p><h1 id="cbf6" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">第5部分:Atari环境下的测试(重温)</h1><p id="30c0" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">通过Pong、Breakout和Freeway(另一款Atari游戏，其训练曲线的初始斜率非常正)进行测试，我欣喜地发现，该模型终于能够学习智能游戏了！</p><div class="mw mx my mz gt ab cb"><figure class="np na of nr ns nt nu paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><img src="../Images/63a603c1660cc05beb86bc7cd58b79a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*8Ss-8iqJsDsZzQeqtVfqUg.png"/></div></figure><figure class="np na og nr ns nt nu paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><img src="../Images/9836bbda6d2079925771a68f6d9e17d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*QObf-6_NDOtsqnEp1wsnrw.png"/></div></figure><figure class="np na oh nr ns nt nu paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><img src="../Images/c62ed55fdd9bc0b33fe164d4fb92df8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*zq4bBBun9N0DUBDsSnzbqg.png"/></div></figure></div><figure class="mw mx my mz gt na gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/cab6427fd3ac3d4e23c59c122dc9be7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/1*FOKDlOtLrHjEUpKAEf6blQ.gif"/></div><p class="nh ni gj gh gi nj nk bd b be z dk translated">(上图)在Pong上运行三次训练，其中x轴是包络步数，y轴是单个评估集的集返回。(下图)完全训练有素的代理人(绿色玩家)的游戏性，其目标是将球击过对手的球拍。在这里，我添加了少量的随机性(10%的随机行动机会)来显示代理如何处理更多样的场景。没有额外的随机性，代理每次都以非常相似的方式击败对手。(图片和gif来源:作者)</p></figure><div class="mw mx my mz gt ab cb"><figure class="np na oj nr ns nt nu paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><img src="../Images/038fccee85227223029800167888810a.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*GzYJGDHFfoo1eaHUTeY5Xw.png"/></div></figure><figure class="np na ok nr ns nt nu paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><img src="../Images/a27e530be77f0c6ab0217cb6b78c10f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*6bFeR6gOULiXVghE-RNyoA.png"/></div></figure><figure class="np na ol nr ns nt nu paragraph-image"><div role="button" tabindex="0" class="nb nc di nd bf ne"><img src="../Images/0ebbe6949df6ec56cac4a6c5800686d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*H9FXy01JkggjK5XY7e3jmA.png"/></div></figure></div><figure class="mw mx my mz gt na gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/625d9467d13b0a215b349ba50d570f8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/1*0-9GcoIo3RDCOh4HGuFFjw.gif"/></div><p class="nh ni gj gh gi nj nk bd b be z dk translated">(上图)在Freeway上运行三次训练，其中x轴是环境步数，y轴是单次评估剧集的剧集返回。(下图)完全训练有素的代理(左侧玩家)的游戏性，其目标是在避开汽车的同时，尽可能快地引导小鸡穿过马路。(图片和gif来源:作者)</p></figure><p id="ba37" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">至于突破，训练曲线显示它的学习超越了随机行为，但它似乎需要比Pong和Freeway更多的步骤才能达到人类的表现。我目前正在使用云计算(谷歌计算引擎)来启动更长的运行，我计划在完成后更新这篇文章或开始一个新的！</p><h1 id="6239" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">结论</h1><p id="95e3" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">总的来说，这绝对是一个有趣的兼职项目，我会把它推荐给那些对强化学习的本质工程感兴趣的人，因为我自己在做这件事的时候也学到了很多。感谢阅读！</p><h1 id="6fcf" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">附录:我遇到的其他错误</h1><p id="1dbb" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">为了完整起见，我在这里展示了我遇到的其他bug，这些bug对我自己的经验来说可能更具体一些，对读者来说可能不太通用。</p><p id="aef7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nm">【Bug】对于我的第一次运行，即使在200k env步骤之后,(训练)集返回几乎没有增加→我发现我忘记增加更新目标Q网络的计数器。</em></p><p id="d76e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nm"> [Bug]在这个过程的早期，我注意到我的内存使用量达到了最高点，比预期的重放内存字节数高出了大约10倍！→啊哈，</em> <code class="fe mr ms mt mu b"><em class="nm">numpy</em></code> <em class="nm">数组默认使用数据类型</em> <code class="fe mr ms mt mu b"><em class="nm">float64</em></code> <em class="nm">，而不是预期的</em> <code class="fe mr ms mt mu b"><em class="nm">uint8</em></code> <em class="nm">。</em></p><p id="f1e5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nm"> [Bug]在第3部分中，为了让env更容易学习，我意识到我正在训练一个完整的突围游戏(5次生命)→重读这篇论文，我发现在1次生命后终止是正确的方法。</em></p><p id="af48" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nm">【Bug】一个相当明显的:我了解到我需要手动将网络参数和输入张量发送到GPU，以便利用它— </em> <code class="fe mr ms mt mu b"><em class="nm">to(torch.device("cuda"))</em></code> <em class="nm">。此外，我还通过在GPU上运行图像预处理(重新缩放、灰度、裁剪)发现了一个加速。</em></p></div></div>    
</body>
</html>