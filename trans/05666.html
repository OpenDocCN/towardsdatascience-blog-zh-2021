<html>
<head>
<title>Transformers: Implementing NLP Models in 3 Lines of Code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变形金刚:用3行代码实现NLP模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/transformers-implementing-nlp-models-in-3-lines-of-code-475639c3611d?source=collection_archive---------16-----------------------#2021-05-20">https://towardsdatascience.com/transformers-implementing-nlp-models-in-3-lines-of-code-475639c3611d?source=collection_archive---------16-----------------------#2021-05-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8c1e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">transformers库简介，用于实现不同NLP任务的最新模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/8fd0d3feb89f3402a906dbac1fcd3ca0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Igbr2yQp9zPvvTx1oPUemw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图一。变形金刚|作者图片</p></figure><p id="2342" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用最先进的自然语言处理模型从未如此简单。拥抱脸[ <a class="ae lv" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank"> 1 </a> ]已经开发了一个名为<code class="fe lw lx ly lz b">transformers</code>的强大库，它允许我们以一种非常简单的方式实现和利用各种各样最先进的NLP模型。在这篇博客中，我们将了解如何安装和使用transformers库来完成不同的任务，例如:</p><ul class=""><li id="a94b" class="ma mb it la b lb lc le lf lh mc ll md lp me lt mf mg mh mi bi translated"><strong class="la iu">文本分类</strong></li><li id="c4dc" class="ma mb it la b lb mj le mk lh ml ll mm lp mn lt mf mg mh mi bi translated"><strong class="la iu">问答</strong></li><li id="ef42" class="ma mb it la b lb mj le mk lh ml ll mm lp mn lt mf mg mh mi bi translated"><strong class="la iu">屏蔽语言建模</strong></li><li id="6fa4" class="ma mb it la b lb mj le mk lh ml ll mm lp mn lt mf mg mh mi bi translated"><strong class="la iu">文字生成</strong></li><li id="7e04" class="ma mb it la b lb mj le mk lh ml ll mm lp mn lt mf mg mh mi bi translated"><strong class="la iu">命名实体识别</strong></li><li id="8171" class="ma mb it la b lb mj le mk lh ml ll mm lp mn lt mf mg mh mi bi translated"><strong class="la iu">文本摘要</strong></li><li id="dd82" class="ma mb it la b lb mj le mk lh ml ll mm lp mn lt mf mg mh mi bi translated"><strong class="la iu">翻译</strong></li></ul><p id="1b1e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以在我们开始回顾不同任务的每个实现之前，让我们安装<code class="fe lw lx ly lz b">transformers</code>库。在我的情况下，我在macOS上工作，当试图用<code class="fe lw lx ly lz b">pip</code>直接安装时，我得到了一个<a class="ae lv" href="https://github.com/huggingface/transformers/issues/2831" rel="noopener ugc nofollow" target="_blank">错误</a>，我通过之前安装的<code class="fe lw lx ly lz b">Rust</code>编译器解决了这个错误，如下所示:</p><pre class="kj kk kl km gt mo lz mp mq aw mr bi"><span id="a212" class="ms mt it lz b gy mu mv l mw mx">$ curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh</span></pre><p id="d6ca" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">之后我用<code class="fe lw lx ly lz b">pip</code>直接安装了<code class="fe lw lx ly lz b">transformers</code>，如下所示:</p><pre class="kj kk kl km gt mo lz mp mq aw mr bi"><span id="e095" class="ms mt it lz b gy mu mv l mw mx">$ pip install transformers</span></pre><p id="a3b0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">太好了，通过前面的两个步骤，库应该已经被正确安装了。因此，让我们从不同的实现开始，让我们开始吧！</p><h1 id="3114" class="my mt it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">文本分类</h1><p id="ea20" class="pw-post-body-paragraph ky kz it la b lb np ju ld le nq jx lg lh nr lj lk ll ns ln lo lp nt lr ls lt im bi translated">文本分类任务包括将给定的文本分配到给定的一组类中的特定类。情感分析是文本分类问题中最常解决的问题。</p><p id="a86b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">要通过<code class="fe lw lx ly lz b">transformers</code>库使用文本分类模型，我们只需要两个参数<code class="fe lw lx ly lz b">task</code>和<code class="fe lw lx ly lz b">model</code>，它们分别指定要处理的问题类型和要使用的模型。考虑到拥抱脸存储库中托管的模型的多样性，我们可以开始使用其中的一些。<a class="ae lv" href="https://huggingface.co/models?pipeline_tag=text-classification" rel="noopener ugc nofollow" target="_blank">在这里</a>你可以找到文本分类任务的模型集合。</p><p id="d837" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在图2中，我们可以看到用于情感分析的<code class="fe lw lx ly lz b">bert-base-multilingual-uncasced-sentiment</code>模型的实现。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/af6e9e78d1210ed839a76f9dc1cb9000.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eaZWqsttPt29DZm1KKt3Nw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图二。文本分类|按作者分类的图片</p></figure><p id="fe48" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出是:</p><pre class="kj kk kl km gt mo lz mp mq aw mr bi"><span id="0120" class="ms mt it lz b gy mu mv l mw mx">Result: [{'label': 'NEG', 'score': 0.9566874504089355}]</span></pre><p id="f402" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">根据您选择实现的模型，这将是要获得的结果。考虑阅读每个模型的文档以了解它们在什么数据集上被训练以及它们执行什么类型的分类是很重要的。<code class="fe lw lx ly lz b">transformers</code>的另一个巨大优势是，如果你有自己的模型托管在拥抱脸存储库中，你也可以通过这个库使用它。</p><h1 id="0903" class="my mt it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">问题回答</h1><p id="6b88" class="pw-post-body-paragraph ky kz it la b lb np ju ld le nq jx lg lh nr lj lk ll ns ln lo lp nt lr ls lt im bi translated"><em class="lu">抽取式问题回答</em>的任务是试图在给定的上下文中找到给定问题的答案。这个任务最有代表性的数据集之一是斯坦福问答数据集。</p><p id="1bc0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">完成这项任务。变压器<code class="fe lw lx ly lz b">pipeline</code>需要<code class="fe lw lx ly lz b">context</code>和<code class="fe lw lx ly lz b">question</code>。在下面的例子中,<code class="fe lw lx ly lz b">context</code>是由<em class="lu">爱丽丝漫游奇境记</em>中的一段确定的，这个问题指的是该段中描述的一个事件。在下图中，您可以看到实现方式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/520f1039672d41d6ce8222880f2466b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tUEtYvdWhwh7YpaHa5g02g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图二。问答|作者图片</p></figure><p id="67be" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出是:</p><pre class="kj kk kl km gt mo lz mp mq aw mr bi"><span id="1a5a" class="ms mt it lz b gy mu mv l mw mx">Answer: 'her sister'</span></pre><p id="e3f5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于这个任务，我们选择模型<code class="fe lw lx ly lz b">robert-base-squad-v1</code>，然而，在<a class="ae lv" href="https://huggingface.co/models?pipeline_tag=table-question-answering" rel="noopener ugc nofollow" target="_blank">拥抱脸库</a>中，我们可以为这个任务找到不同的替代模型，值得看看其中的一些。</p><h1 id="4a67" class="my mt it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">掩蔽语言建模</h1><p id="57d8" class="pw-post-body-paragraph ky kz it la b lb np ju ld le nq jx lg lh nr lj lk ll ns ln lo lp nt lr ls lt im bi translated">屏蔽语言建模任务是用屏蔽标记屏蔽给定文本句子的标记，其中要求模型用适当的标记填充每个屏蔽。</p><p id="4384" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于这种类型的任务，transformers <code class="fe lw lx ly lz b">pipeline</code>只需要任务的名称(在本例中为<code class="fe lw lx ly lz b">fill-mask</code>)，然后是指定要屏蔽的令牌的文本序列，在下图中我们可以看到实现:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/3ea0fb875d5d7045517fcd183834c319.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z8lJGhweKAb_EdoNcyvc_Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3。蒙版语言建模|作者图片</p></figure><p id="3420" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出是:</p><pre class="kj kk kl km gt mo lz mp mq aw mr bi"><span id="07aa" class="ms mt it lz b gy mu mv l mw mx">[{'sequence': ' Horror movies are often very scary to people',<br/>  'score': 0.12314373254776001,<br/>  'token': 28719,<br/>  'token_str': ' Horror'},<br/> {'sequence': ' horror movies are often very scary to people',<br/>  'score': 0.052469268441200256,<br/>  'token': 8444,<br/>  'token_str': ' horror'},<br/> {'sequence': 'Ghost movies are often very scary to people',<br/>  'score': 0.05243474990129471,<br/>  'token': 38856,<br/>  'token_str': 'Ghost'},<br/> {'sequence': 'War movies are often very scary to people',<br/>  'score': 0.03345327079296112,<br/>  'token': 20096,<br/>  'token_str': 'War'},<br/> {'sequence': 'Action movies are often very scary to people',<br/>  'score': 0.029487883672118187,<br/>  'token': 36082,<br/>  'token_str': 'Action'}]</span></pre><p id="5e8c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">结果显示为令牌及其各自属性的列表。在这种情况下，得分最高的令牌是<code class="fe lw lx ly lz b">Horror</code>，最后一个令牌是<code class="fe lw lx ly lz b">Action</code>。</p><h1 id="112f" class="my mt it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">文本生成</h1><p id="d5de" class="pw-post-body-paragraph ky kz it la b lb np ju ld le nq jx lg lh nr lj lk ll ns ln lo lp nt lr ls lt im bi translated">文本生成任务指的是相对于确定的上下文创建语法和语义正确的文本部分。在这种情况下，<code class="fe lw lx ly lz b">pipeline</code>初始化需要使用<code class="fe lw lx ly lz b">task</code>的类型和<code class="fe lw lx ly lz b">model</code>，就像前面的任务一样。最后，<code class="fe lw lx ly lz b">pipeline</code>实例需要两个参数，即<code class="fe lw lx ly lz b">context</code>(或种子)和要生成的序列的长度<code class="fe lw lx ly lz b">max_length</code>。要生成的序列数是一个可选参数。</p><p id="745a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下图显示了生成5个文本序列的<code class="fe lw lx ly lz b">GPT-2</code>模型的实现:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/c5f11dc446313d51b92a0058172447bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xX21W30BeznevGHOt6-YrQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4。文本生成|作者图片</p></figure><p id="2cb8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出是:</p><pre class="kj kk kl km gt mo lz mp mq aw mr bi"><span id="4a86" class="ms mt it lz b gy mu mv l mw mx">[{'generated_text': 'My name is Fernando, I am from Mexico and live for a reason. I am a musician and the best producer, you might call me a poet'},</span><span id="5c00" class="ms mt it lz b gy nv mv l mw mx"> {'generated_text': 'My name is Fernando, I am from Mexico and I make an app with a lot of friends to keep us safe!" said Fernando.\n\nThe'},</span><span id="e9d8" class="ms mt it lz b gy nv mv l mw mx"> {'generated_text': 'My name is Fernando, I am from Mexico and I am an atheist. I am living in a town called Tanta and I am living in the'},</span><span id="3839" class="ms mt it lz b gy nv mv l mw mx"> {'generated_text': 'My name is Fernando, I am from Mexico and I have been doing this gig since the age of 21 and I am the first person to record this'},</span><span id="89ba" class="ms mt it lz b gy nv mv l mw mx"> {'generated_text': 'My name is Fernando, I am from Mexico and I am in Mexico", he said.\n\nHis name may be a reference to his birthplace and'}]</span></pre><p id="b450" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有点好笑的是<code class="fe lw lx ly lz b">GPT-2</code>生成的序列是基于一个住在墨西哥的人，他的名字叫费尔南多。</p><h1 id="bed2" class="my mt it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">命名实体识别</h1><p id="a201" class="pw-post-body-paragraph ky kz it la b lb np ju ld le nq jx lg lh nr lj lk ll ns ln lo lp nt lr ls lt im bi translated">命名实体识别任务是指给给定文本序列的每个标记分配一个类。对于该任务的执行，只需要将任务标识符<code class="fe lw lx ly lz b">ner</code>分配给<code class="fe lw lx ly lz b">pipeline</code>初始化。随后，对象只接收一个文本流。在下图中，我们可以看到实施过程:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/fa346368cf2263addf868a19bc395a93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mBuEcFIHSsi9eZya0XTi7Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图5。命名实体识别|作者图片</p></figure><p id="9f27" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出是:</p><pre class="kj kk kl km gt mo lz mp mq aw mr bi"><span id="92f8" class="ms mt it lz b gy mu mv l mw mx">('Fernando', 'I-PER')<br/>('Mexico', 'I-LOC')<br/>('Learning', 'I-ORG')<br/>('Engineer', 'I-MISC')<br/>('Hit', 'I-ORG')<br/>('##ch', 'I-ORG')</span></pre><p id="016b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于本例，这些类是:</p><ul class=""><li id="0583" class="ma mb it la b lb lc le lf lh mc ll md lp me lt mf mg mh mi bi translated"><strong class="la iu"> I-MISC </strong>，杂项实体</li><li id="ef6d" class="ma mb it la b lb mj le mk lh ml ll mm lp mn lt mf mg mh mi bi translated"><strong class="la iu">根据</strong>，人名</li><li id="0448" class="ma mb it la b lb mj le mk lh ml ll mm lp mn lt mf mg mh mi bi translated"><strong class="la iu"> I-ORG </strong>，组织</li><li id="58ef" class="ma mb it la b lb mj le mk lh ml ll mm lp mn lt mf mg mh mi bi translated"><strong class="la iu"> I-LOC </strong>，位置</li></ul><p id="a6f2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有趣的是，公司作为一个组织被正确地分配。</p><h1 id="775e" class="my mt it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">文本摘要</h1><p id="dc08" class="pw-post-body-paragraph ky kz it la b lb np ju ld le nq jx lg lh nr lj lk ll ns ln lo lp nt lr ls lt im bi translated">文本摘要任务是指在给定一个确定文本的情况下提取一个摘要。为了初始化<code class="fe lw lx ly lz b">pipeline</code>，需要<code class="fe lw lx ly lz b">task</code>的定义以及<code class="fe lw lx ly lz b">summarization</code>标识符。随后，对于任务的执行，仅需要<code class="fe lw lx ly lz b">text</code>和要生成的最大和最小序列长度作为自变量。在下图中我们可以看到这个任务的实现:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/64d4d1d6c186cc629c47868de2d4859b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D8Td6XPd0HgvXYxewBvboA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图6。摘要|作者图片</p></figure><p id="a3a4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出是:</p><pre class="kj kk kl km gt mo lz mp mq aw mr bi"><span id="2278" class="ms mt it lz b gy mu mv l mw mx">[{'summary_text': ' Machine learning is an important component of the growing field of data science . Machine learning, deep learning, and neural networks are all sub-fields of artificial intelligence . As big data continues to grow, the market demand for data scientists will increase, requiring them to assist in the identification of the most relevant business questions .'}]</span></pre><p id="c384" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如我们所看到的，由模型生成的摘要相对于输入文本是正确的。与前面的任务一样，我们可以使用各种模型进行文本摘要，如<code class="fe lw lx ly lz b">BART</code>、<code class="fe lw lx ly lz b">DistilBart</code>、<code class="fe lw lx ly lz b">Pegasus</code> [ <a class="ae lv" href="https://huggingface.co/models?pipeline_tag=summarization" rel="noopener ugc nofollow" target="_blank"> 4 </a>。</p><h1 id="dd03" class="my mt it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">翻译</h1><p id="7ad2" class="pw-post-body-paragraph ky kz it la b lb np ju ld le nq jx lg lh nr lj lk ll ns ln lo lp nt lr ls lt im bi translated">翻译任务是指将一种给定语言的文本转换成另一种语言。<code class="fe lw lx ly lz b">transformers</code>库允许以非常简单的方式使用最先进的模型进行翻译，例如<code class="fe lw lx ly lz b">T5</code>。流水线用要解决的任务的标识符初始化，该标识符指的是原始语言和要翻译的语言，例如从英语翻译成法语，标识符是:<code class="fe lw lx ly lz b">translation_en_to_fr</code>。最后，生成的对象接收要翻译的文本作为参数。在下图中我们可以看到一个从英语到法语的文本翻译器的实现:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/7d84ed710994922a643819bfd5cbc190.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1_9TGXEzlY7SFNVgorcWew.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图7。翻译|作者图片</p></figure><p id="8ffb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出是:</p><pre class="kj kk kl km gt mo lz mp mq aw mr bi"><span id="3f8c" class="ms mt it lz b gy mu mv l mw mx">L'apprentissage automatique est une branche de l'intelligence artificielle (AI) et de la science informatique qui se concentre sur l'utilisation de données et d'algorithmes pour imiter la façon dont les humains apprennent, en améliorant progressivement sa précision.</span></pre><h1 id="024e" class="my mt it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">结论</h1><p id="0a7f" class="pw-post-body-paragraph ky kz it la b lb np ju ld le nq jx lg lh nr lj lk ll ns ln lo lp nt lr ls lt im bi translated">在这篇教程博客中，我们看到了如何使用<code class="fe lw lx ly lz b">transformers</code>库以一种非常简单的方式实现最先进的NLP模型。</p><p id="ceda" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这篇博客中，我们看到了如何实现一些最常见的任务，然而重要的是要提到，这篇博客中显示的例子仅仅是为了推断，然而<code class="fe lw lx ly lz b">transformers</code>库的一个伟大的属性是，它提供了能够基于那些已经训练好的模型微调我们自己的模型的方法。这将是下一篇博客的好主题。</p><h1 id="a455" class="my mt it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated"><strong class="ak">参考文献</strong></h1><p id="2354" class="pw-post-body-paragraph ky kz it la b lb np ju ld le nq jx lg lh nr lj lk ll ns ln lo lp nt lr ls lt im bi translated">【1】<a class="ae lv" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank">抱紧脸</a></p><p id="0d18" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">【2】<a class="ae lv" href="https://rajpurkar.github.io/SQuAD-explorer/" rel="noopener ugc nofollow" target="_blank">小队</a> : <em class="lu">斯坦福问答数据集</em></p><p id="b595" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">【3】<a class="ae lv" href="https://www.gutenberg.org/files/11/11-h/11-h.htm" rel="noopener ugc nofollow" target="_blank">爱丽丝梦游仙境</a></p><p id="5e6a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[4] <a class="ae lv" href="https://huggingface.co/models?pipeline_tag=summarization" rel="noopener ugc nofollow" target="_blank">文本摘要模型</a></p></div></div>    
</body>
</html>