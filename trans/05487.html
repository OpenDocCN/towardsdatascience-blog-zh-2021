<html>
<head>
<title>An Introduction to PyTorch Lightning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch闪电简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-introduction-of-pytorch-lightning-230d03bcb262?source=collection_archive---------9-----------------------#2021-05-15">https://towardsdatascience.com/an-introduction-of-pytorch-lightning-230d03bcb262?source=collection_archive---------9-----------------------#2021-05-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="1915" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">只需更改一个参数，即可将代码从单GPU移植到多GPU训练</h2><div class=""/><div class=""><h2 id="1341" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">PyTorch lightning帮助您将代码扩展到多GPU培训，无需任何工程工作</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/82405a1ac94c47b6f1abc157df18da0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*x3i5Poku0ZpFzdHF"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">Johannes Plenio 在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="0afd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">坊间传言PyTorch lightning是普通PyTorch的更好版本。但是，它能给我们的世界带来什么样的共识呢？嗯，它帮助研究人员通过多GPU训练、Fp-16训练和TPU支持的训练来扩展模型，只需改变一个参数。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="me mf l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://giphy.com/" rel="noopener ugc nofollow" target="_blank">吉菲</a></p></figure><p id="ed05" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">是的，你没看错。只要改变一个论点，你就可以做所有这些惊人的事情。还记得我们过去是如何编写多GPU训练代码的吗？我们必须了解PyTorch支持的不同训练架构，然后自己实现它们。谢天谢地，再也没有了！PyTorch Lightning将改变我们编写代码的方式，并帮助我们在一秒钟内完成所有这些任务。</p><p id="428f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了涵盖所有的变化，我用以下方式构建了这个博客:</p><ul class=""><li id="93b1" class="mg mh it lk b ll lm lo lp lr mi lv mj lz mk md ml mm mn mo bi translated">PyTorch和PyTorch Lightning在实现上的差异的总体比较</li><li id="9c12" class="mg mh it lk b ll mp lo mq lr mr lv ms lz mt md ml mm mn mo bi translated">比较每个模块，如模型、损失函数等。为了PyTorch和PyTorch闪电</li><li id="661a" class="mg mh it lk b ll mp lo mq lr mr lv ms lz mt md ml mm mn mo bi translated">多GPU训练</li><li id="d40e" class="mg mh it lk b ll mp lo mq lr mr lv ms lz mt md ml mm mn mo bi translated">TPU培训</li><li id="844b" class="mg mh it lk b ll mp lo mq lr mr lv ms lz mt md ml mm mn mo bi translated">FP16培训</li><li id="9361" class="mg mh it lk b ll mp lo mq lr mr lv ms lz mt md ml mm mn mo bi translated">提前停止</li><li id="89f1" class="mg mh it lk b ll mp lo mq lr mr lv ms lz mt md ml mm mn mo bi translated">LR取景器</li></ul><h1 id="6948" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated"><strong class="ak">py torch和PyTorch Lightning的基本对比</strong></h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nm"><img src="../Images/91b09db547f5240c3f919c8c6b645b4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-GDzOk_UJElhGtnc9g6zHA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">两个框架之间的比较(图片由作者提供)</p></figure><blockquote class="nn no np"><p id="94a9" class="li lj nq lk b ll lm kd ln lo lp kg lq nr ls lt lu ns lw lx ly nt ma mb mc md im bi translated">具有相同颜色的代码块表示相同模块的实现。例如，两个框架中的模型定义都是浅绿色的。</p></blockquote><p id="9e57" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">首先要注意的是，PyTorch lightning在类<em class="nq"> Net </em>本身<em class="nq">中加入了<em class="nq"> train_dataloader、configure _ optimizers、training_step </em>。</em>另外，请注意，我们在PyTorch中编写的整个训练循环在PyTorch lightning中只有几行。</p><h1 id="d987" class="mu mv it bd mw mx my mz na nb nc nd ne ki nf kj ng kl nh km ni ko nj kp nk nl bi translated">通用深度学习代码组件</h1><p id="26c0" class="pw-post-body-paragraph li lj it lk b ll nu kd ln lo nv kg lq lr nw lt lu lv nx lx ly lz ny mb mc md im bi translated">一般来说，深度学习代码具有以下组件</p><ul class=""><li id="03f9" class="mg mh it lk b ll lm lo lp lr mi lv mj lz mk md ml mm mn mo bi translated">模型</li><li id="9cfc" class="mg mh it lk b ll mp lo mq lr mr lv ms lz mt md ml mm mn mo bi translated">数据</li><li id="a8e5" class="mg mh it lk b ll mp lo mq lr mr lv ms lz mt md ml mm mn mo bi translated">失败</li><li id="5f3c" class="mg mh it lk b ll mp lo mq lr mr lv ms lz mt md ml mm mn mo bi translated">【计算机】优化程序</li><li id="ace8" class="mg mh it lk b ll mp lo mq lr mr lv ms lz mt md ml mm mn mo bi translated">训练和测试回路</li><li id="d7c9" class="mg mh it lk b ll mp lo mq lr mr lv ms lz mt md ml mm mn mo bi translated">记录</li></ul><p id="8a13" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们了解PyTorch和PyTorch lightning在上述组件方面的区别。</p><h2 id="f304" class="nz mv it bd mw oa ob dn na oc od dp ne lr oe of ng lv og oh ni lz oi oj nk iz bi translated">模型</h2><p id="1b53" class="pw-post-body-paragraph li lj it lk b ll nu kd ln lo nv kg lq lr nw lt lu lv nx lx ly lz ny mb mc md im bi translated">正如我们上面注意到的，模型架构和定义是相同的，除了在Pytorch Lightning中所有其他函数定义也在同一个类中。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ok"><img src="../Images/acf15173643ae0128e2ba4bfb247c6ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z7ZzjPsEsbzUkwGyRhRxiw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">模型对比(图片由作者提供)</p></figure><h2 id="f05e" class="nz mv it bd mw oa ob dn na oc od dp ne lr oe of ng lv og oh ni lz oi oj nk iz bi translated">数据</h2><p id="44aa" class="pw-post-body-paragraph li lj it lk b ll nu kd ln lo nv kg lq lr nw lt lu lv nx lx ly lz ny mb mc md im bi translated">在Pytorch Lightning中定义数据加载器有两种方法。</p><ul class=""><li id="8164" class="mg mh it lk b ll lm lo lp lr mi lv mj lz mk md ml mm mn mo bi translated">您可以在<em class="nq"> Net </em>类中定义<em class="nq"> train_dataloder </em>和<em class="nq"> val_dataloader </em>函数，就像前面所做的那样(在第一个例子中)</li><li id="6979" class="mg mh it lk b ll mp lo mq lr mr lv ms lz mt md ml mm mn mo bi translated">你可以像PyTorch中一样定义自己的<em class="nq"> train_dataloader </em>和<em class="nq"> val_dataloader </em>到<em class="nq"> trainer.fit </em>，如下图所示。</li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ol mf l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">MNIST数据加载器</p></figure><p id="5d32" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用上述方法，您可以为PyTorch和PyTorch lightning定义数据加载器。两者的主要区别在于Pytorch Lightning中的<em class="nq"> trainer.fit() </em>将所有数据加载器作为参数。</p><pre class="ks kt ku kv gt om on oo op aw oq bi"><span id="3801" class="nz mv it on b gy or os l ot ou">trainer.fit(net, train_dataloader, val_dataloader)<br/>trainer.test(net, test_dataloader)</span></pre><h2 id="825a" class="nz mv it bd mw oa ob dn na oc od dp ne lr oe of ng lv og oh ni lz oi oj nk iz bi translated">失败</h2><p id="39e4" class="pw-post-body-paragraph li lj it lk b ll nu kd ln lo nv kg lq lr nw lt lu lv nx lx ly lz ny mb mc md im bi translated">对于n类分类，我们要计算交叉熵损失。交叉熵与NegativeLogLikelihood(log _ soft max)相同，所以我们将使用它来代替。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ov"><img src="../Images/960efe045e60d90455ce4b60850a4516.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m3up1rTfU_vZbX2O2ShbOg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">损失比较(图片由作者提供)</p></figure><h2 id="9162" class="nz mv it bd mw oa ob dn na oc od dp ne lr oe of ng lv og oh ni lz oi oj nk iz bi translated">【计算机】优化程序</h2><p id="5d7d" class="pw-post-body-paragraph li lj it lk b ll nu kd ln lo nv kg lq lr nw lt lu lv nx lx ly lz ny mb mc md im bi translated">让我们使用Adam优化器。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ow"><img src="../Images/6d557f8f4c7a2bae72138eee3dea8c48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UV-AsA649-aCWzb8olXLdw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">优化程序比较(图片由作者提供)</p></figure><p id="3f67" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="nq">自我</em>。<em class="nq"> parameters() </em>这里传递的，包括模型中定义的所有可学习的参数。</p><h2 id="b480" class="nz mv it bd mw oa ob dn na oc od dp ne lr oe of ng lv og oh ni lz oi oj nk iz bi translated">训练和验证循环</h2><p id="c1db" class="pw-post-body-paragraph li lj it lk b ll nu kd ln lo nv kg lq lr nw lt lu lv nx lx ly lz ny mb mc md im bi translated">在PyTorch，我们必须</p><ul class=""><li id="6461" class="mg mh it lk b ll lm lo lp lr mi lv mj lz mk md ml mm mn mo bi translated">定义训练循环</li><li id="a5b1" class="mg mh it lk b ll mp lo mq lr mr lv ms lz mt md ml mm mn mo bi translated">加载数据</li><li id="e07b" class="mg mh it lk b ll mp lo mq lr mr lv ms lz mt md ml mm mn mo bi translated">通过模型传递数据</li><li id="cdd6" class="mg mh it lk b ll mp lo mq lr mr lv ms lz mt md ml mm mn mo bi translated">计算损失</li><li id="6f9d" class="mg mh it lk b ll mp lo mq lr mr lv ms lz mt md ml mm mn mo bi translated">做零工</li><li id="1d38" class="mg mh it lk b ll mp lo mq lr mr lv ms lz mt md ml mm mn mo bi translated">反向传播损失函数。</li></ul><p id="910a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然而，在PyTorch闪电中，我们必须</p><ul class=""><li id="9d3f" class="mg mh it lk b ll lm lo lp lr mi lv mj lz mk md ml mm mn mo bi translated">定义<em class="nq"> training_step </em>和<em class="nq"> validation_step，</em>在这里我们定义我们希望数据如何通过模型</li><li id="3c7c" class="mg mh it lk b ll mp lo mq lr mr lv ms lz mt md ml mm mn mo bi translated">计算损失</li></ul><p id="2e77" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们甚至不用指定<em class="nq"> model.train() </em>和<em class="nq"> model.eval() </em>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ox"><img src="../Images/2f1e5c1d0260cd8d7d6a037409da712b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jF7AdkJ9mtlvqrvYBpwDpQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">训练和验证比较(图片由作者提供)</p></figure><h2 id="c3e7" class="nz mv it bd mw oa ob dn na oc od dp ne lr oe of ng lv og oh ni lz oi oj nk iz bi translated">记录</h2><p id="7332" class="pw-post-body-paragraph li lj it lk b ll nu kd ln lo nv kg lq lr nw lt lu lv nx lx ly lz ny mb mc md im bi translated">Tensorboard是美国研究人员最常用的记录器之一。所以正如你在PyTorch lightning上面注意到的，在函数<em class="nq"> training_step </em>和<em class="nq"> validation_step的最后一行，提到了self.log() </em>，这是用来记录训练损失的。这将创建一个名为lightning_logs的文件夹，并在其中保存所有日志和时期。</p><h2 id="a4cd" class="nz mv it bd mw oa ob dn na oc od dp ne lr oe of ng lv og oh ni lz oi oj nk iz bi translated">概述</h2><ul class=""><li id="9132" class="mg mh it lk b ll nu lo nv lr oy lv oz lz pa md ml mm mn mo bi translated">模型定义过程类似，但有两个主要区别。1)所有其他函数也是用PyTorch lightning的模型类本身定义的。2)神经网络。Pytorch中的模块在PyTorch lightning中被nn.LightningModule覆盖。</li><li id="1bc8" class="mg mh it lk b ll mp lo mq lr mr lv ms lz mt md ml mm mn mo bi translated">可以用同样的方式定义数据加载器。对于PyTorch lightning，我们必须在<em class="nq"> train.fit() </em>时通过<em class="nq"> train_loader </em>和<em class="nq"> val_loader </em></li><li id="2be4" class="mg mh it lk b ll mp lo mq lr mr lv ms lz mt md ml mm mn mo bi translated">Optimizer和loss可以用同样的方式定义，但是它们需要作为PyTorch lightning的主类中的一个函数存在。</li><li id="e6ac" class="mg mh it lk b ll mp lo mq lr mr lv ms lz mt md ml mm mn mo bi translated">培训和验证循环在PyTorch lightning中预定义。我们必须定义<em class="nq">训练_步骤</em>和<em class="nq">验证_步骤</em>，即给定一个数据点/批次，我们希望如何通过该模型传递数据。</li><li id="4538" class="mg mh it lk b ll mp lo mq lr mr lv ms lz mt md ml mm mn mo bi translated">用于记录的函数是预定义的，可以在Pytorch Lightning中直接调用。</li></ul><p id="f8bb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">既然我们知道了这两者之间的区别，那就让我们来理解为什么我们首先要开始这个比较。为了使代码可伸缩并减少我们的工程工作，让我们看看感兴趣的特性。</p><h2 id="af55" class="nz mv it bd mw oa ob dn na oc od dp ne lr oe of ng lv og oh ni lz oi oj nk iz bi translated">多GPU训练</h2><p id="ae4d" class="pw-post-body-paragraph li lj it lk b ll nu kd ln lo nv kg lq lr nw lt lu lv nx lx ly lz ny mb mc md im bi translated">我们可以使用下面的代码来实现。</p><pre class="ks kt ku kv gt om on oo op aw oq bi"><span id="da1e" class="nz mv it on b gy or os l ot ou">trainer = Trainer(gpus=8, distributed_backend='dp')</span></pre><p id="7916" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您可以定义要用于分布式培训的GPU数量，以及要使用的后端。这里我定义了“dp ”,它是并行分布的。也可以定义为‘DDP’，即分布式数据并行。</p><h2 id="26bb" class="nz mv it bd mw oa ob dn na oc od dp ne lr oe of ng lv og oh ni lz oi oj nk iz bi translated">TPU培训</h2><p id="e35d" class="pw-post-body-paragraph li lj it lk b ll nu kd ln lo nv kg lq lr nw lt lu lv nx lx ly lz ny mb mc md im bi translated">我们可以使用下面的代码来实现。</p><pre class="ks kt ku kv gt om on oo op aw oq bi"><span id="dea1" class="nz mv it on b gy or os l ot ou">trainer = Trainer(tpu_cores<strong class="on jd">=</strong>[5])</span></pre><p id="f2d3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这个代码意味着模型将在ID为5的TPU核心上训练。我们还可以通过使用单个参数启用多TPU训练来定义需要使用多少个内核。</p><h2 id="f706" class="nz mv it bd mw oa ob dn na oc od dp ne lr oe of ng lv og oh ni lz oi oj nk iz bi translated">FP16培训</h2><p id="45ea" class="pw-post-body-paragraph li lj it lk b ll nu kd ln lo nv kg lq lr nw lt lu lv nx lx ly lz ny mb mc md im bi translated">这是我最喜欢的。FP16有助于加快训练过程，而不会对性能造成太大影响。</p><pre class="ks kt ku kv gt om on oo op aw oq bi"><span id="b027" class="nz mv it on b gy or os l ot ou">trainer = Trainer(precision<strong class="on jd">=</strong>16)</span></pre><h2 id="3db2" class="nz mv it bd mw oa ob dn na oc od dp ne lr oe of ng lv og oh ni lz oi oj nk iz bi translated">提前停止</h2><p id="31f1" class="pw-post-body-paragraph li lj it lk b ll nu kd ln lo nv kg lq lr nw lt lu lv nx lx ly lz ny mb mc md im bi translated">这用于在训练模型时，如果您看不到模型性能有任何进一步的提高，则停止训练。我们如何持续检查模型性能是否在提高？为此，我们可以使用验证损失或准确性。</p><pre class="ks kt ku kv gt om on oo op aw oq bi"><span id="e3b2" class="nz mv it on b gy or os l ot ou">from pytorch_lightning.callbacks.early_stopping import EarlyStopping</span><span id="07ec" class="nz mv it on b gy pb os l ot ou">def validation_step(...):<br/>    self.log('val_loss', loss)</span><span id="9d89" class="nz mv it on b gy pb os l ot ou">trainer = Trainer(callbacks=[EarlyStopping(monitor='val_loss', patience=3)])</span></pre><p id="3bee" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在上面的例子中，培训师将跟踪验证的准确性。如果过去3个时期(耐心值)的性能没有改善，那么它将停止训练，从而防止过度拟合。</p><h2 id="4ea7" class="nz mv it bd mw oa ob dn na oc od dp ne lr oe of ng lv og oh ni lz oi oj nk iz bi translated">LR取景器</h2><p id="f582" class="pw-post-body-paragraph li lj it lk b ll nu kd ln lo nv kg lq lr nw lt lu lv nx lx ly lz ny mb mc md im bi translated">学习率是最重要的超参数之一，正确设置初始学习率至关重要。否则，在训练模型时，可能会出现模型收敛到无法提供最佳性能的局部最优值的情况。</p><pre class="ks kt ku kv gt om on oo op aw oq bi"><span id="b4b0" class="nz mv it on b gy or os l ot ou">net <strong class="on jd">=</strong> LightningMNISTClassifier()<br/><br/><em class="nq"># finds learning rate automatically</em><br/><em class="nq"># sets hparams.lr or hparams.learning_rate to that learning rate</em><br/>trainer <strong class="on jd">=</strong> Trainer(auto_lr_find<strong class="on jd">=True</strong>)<br/><br/>trainer<strong class="on jd">.</strong>tune(net)</span></pre><p id="bf74" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了找到最佳学习率，你必须使参数<em class="nq"> auto_lr_find </em>为真<em class="nq">，</em>，然后调整训练器(使用<em class="nq"> trainer.tune() </em>)，这将帮助你找到学习率。之后可以调用<em class="nq"> trainer.fit() </em>对模型进行训练。</p></div><div class="ab cl pc pd hx pe" role="separator"><span class="pf bw bk pg ph pi"/><span class="pf bw bk pg ph pi"/><span class="pf bw bk pg ph"/></div><div class="im in io ip iq"><p id="835d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">只需添加一些参数，Pytorch Lightning就能帮助我们享受许多功能，而每一项功能对我们来说都至关重要。现在您有了一个基本的想法，通过编写您的第一个神经网络并探索该库提供的不同功能，开始实践PyTorch lightning。</p><p id="63f2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你想学习深度学习的很酷的应用，请查看我们关于深度学习语音处理的博客<a class="ae lh" rel="noopener" target="_blank" href="/all-you-need-to-know-to-start-speech-processing-with-deep-learning-102c916edf62">这里</a>。</p><p id="a355" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="nq">成为</em> <a class="ae lh" href="https://medium.com/@AnveeNaik/membership" rel="noopener"> <em class="nq">介质会员</em> </a> <em class="nq">解锁并阅读介质上的许多其他故事。关注我们的</em> <a class="ae lh" href="https://medium.com/@AnveeNaik" rel="noopener"> <em class="nq">中</em> </a> <em class="nq">阅读更多此类博文</em>。</p></div></div>    
</body>
</html>