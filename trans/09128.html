<html>
<head>
<title>Successfully Navigating through Principal Component Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">成功浏览主成分分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/principal-component-analysis-93b96b35ddc?source=collection_archive---------26-----------------------#2021-08-23">https://towardsdatascience.com/principal-component-analysis-93b96b35ddc?source=collection_archive---------26-----------------------#2021-08-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="bc59" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">实践教程</h2><div class=""/><div class=""><h2 id="258a" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">处理非常大的数据集可能很难获得有意义的结果。出于这个原因，降维可能是一个简单而又相对快速的尝试。这就是PCA派上用场的地方，有几种可能的方法让你的数据进入低维。</h2></div><h2 id="e67d" class="kr ks it bd kt ku kv dn kw kx ky dp kz la lb lc ld le lf lg lh li lj lk ll iz bi translated">本文提供了一个简洁的概述，并浏览了投影的数学基础，考虑了特征值/特征向量、奇异值分解(SVD)以及主成分分析(PCA ),并展示了如何在Python中通过计算实现降维。</h2><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lm"><img src="../Images/22d35494a56cca30d4b231253932548f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*72uMLdtJqk1bDaOP"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">美丽图案由<a class="ae mc" href="https://unsplash.com/@mitchel3uo?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">米切尔罗</a></p></figure><p id="cfa1" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">PCA是很多，但无论如何，这不是什么新鲜事。在大多数情况下，当谈到图像的有损压缩、降维以及当前机器学习宣传的特征提取时，人们会提到PCA。即使人们最终可能不会使用这样的投影数据，PCA也可以快速地将数据集减少到几个维度，然后可以用于在简单的散点图上可视化。</p><p id="c912" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">等待...使用一个N <em class="mw"> x </em> M的数据集，并将其简化为一个N <em class="mw"> x </em> 2的数据集，可以在二维空间中绘制——这听起来是不是非常有趣？绝对的！让我们从一个简洁而轻松的理论回顾开始。</p><h1 id="9492" class="mx ks it bd kt my mz na kw nb nc nd kz ki ne kj ld kl nf km lh ko ng kp ll nh bi translated"><strong class="ak">理论背景</strong></h1><p id="b94f" class="pw-post-body-paragraph md me it mf b mg ni kd mi mj nj kg ml la nk mn mo le nl mq mr li nm mt mu mv im bi translated">首先，让我们看一个相当简单的正交投影例子。假设我们想将一个数据点x投影到一条简单的直线上(因此，是一个一维空间)。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/40de7d87e2124adad953123fae3af596.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*jFzItV1_PyHMzLDlgvmrow.png"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">x在线L上的投影—来源:<a class="ae mc" href="https://textbooks.math.gatech.edu/ila/linear-transformations.html" rel="noopener ugc nofollow" target="_blank">交互式线性代数</a></p></figure><p id="bfc0" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">我们的目标是找到蓝点(x在L上，“xl”)，如上图所示。</p><p id="d5d2" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">为此，在快速浏览投影时，我们需要记住以下两个标准:</p><pre class="ln lo lp lq gt no np nq nr aw ns bi"><span id="eae4" class="kr ks it np b gy nt nu l nv nw">Given<br/>I.  cu: sometimes denoted λ*u # scaling u, where λ is unknown<br/>II. (x-cu)*u = 0 # due to orthogonality</span></pre><p id="98c1" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">结合上面这两个条件，我们得到:</p><pre class="ln lo lp lq gt no np nq nr aw ns bi"><span id="f19e" class="kr ks it np b gy nt nu l nv nw">(x - cu) * u = 0 # again, orthogonality<br/>where u*u is the norm² of u</span></pre><p id="7ee5" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">简单的代数然后让我们到达<em class="mw"> c </em>:</p><pre class="ln lo lp lq gt no np nq nr aw ns bi"><span id="fe9e" class="kr ks it np b gy nt nu l nv nw">c = ux / |u|² # or<br/>  = ux / uu</span></pre><p id="a9d8" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">最终我们通过用<em class="mw"> c </em>缩放<em class="mw"> u </em>得到<em class="mw"> xl </em>。</p><p id="6a37" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">我们刚刚掌握了将x投影到<em class="mw"> L </em>的线上，或者换句话说，将<strong class="mf jd">投影到标量值</strong>上。让我们进一步把这个想法推广到更高维度的空间。我们认为我们的矩阵<em class="mw">是一个</em>一个N <em class="mw"> x </em> M矩阵，而<em class="mw"> W </em>是<em class="mw"> A </em>的列空间，或者简单地说是跨越<em class="mw"> A </em>空间的所有向量(通常也称为具有枢轴位置的向量)。因为我们的目标是将数据点<em class="mw"> x </em>投影到我们的超平面<em class="mw"> W </em>上。</p><p id="197c" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">正如我们之前看到的(<em class="mw">请始终考虑转置上标，这些上标在</em>下面的代码框中被遗漏了):</p><pre class="ln lo lp lq gt no np nq nr aw ns bi"><span id="4d77" class="kr ks it np b gy nt nu l nv nw">(x-cu)*u = 0 is now defined as: (x-Ac)*A = 0<br/>= Ax - AAc = 0 <br/>= Ax = AAc</span></pre><p id="b2cb" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">将所有这些放在一起，我们可以使用给定的定理来概括这种投影行为:</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/61b82b8d7b130543f42b0b101b0d96cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:216/format:webp/1*0Z-Z0IsGxrTvdYGpG8CLew.png"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">因此W中的x等于Ac</p></figure><p id="f52f" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">从这一步我们可以了解到，<em class="mw"> Ac </em>等于<em class="mw"> W </em>中的投影<em class="mw"> x </em>，因此我们在求解常数<em class="mw"> c </em>时找到了<em class="mw"> W </em>中的投影点<em class="mw"> x </em>。</p><p id="bfb1" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">如果我们把我们刚才所做的当作一个例子或一般的食谱:</p><pre class="ln lo lp lq gt no np nq nr aw ns bi"><span id="0bac" class="kr ks it np b gy nt nu l nv nw">1. Given matrix A and data point x<br/>2. Calculate the matrix multiplication AA<br/>3. Calculate the matrix transformation Ax<br/>4. Form an augmented matrix for AA|Ax and do a row reduction to obtain pivot positions for AA. This gets us to c.<br/>5. Look at c and 🎉</span></pre><p id="c12a" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">现在，让我们跳回PCA。我们已经找到了一种投射数据的方法，如果我们把它投射到一条线上，我们已经处于一个较低的维度，那么这和PCA有什么联系呢？在PCA中，我们试图以一种捕捉到数据中的<strong class="mf jd">最大方差</strong>的方式来投影数据。听我说:</p><p id="e65e" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">在机器学习中，我们经常试图最小化数据中的方差，在这种情况下，情况完全相反。当我们降低数据的维度时，我们希望捕捉尽可能多的可变性。举个例子，如果你发现前两个主成分(我们很快就会谈到这一点，只要想想<em class="mw">的前两列</em>)捕获了95%的数据可变性，我们可以说其他N列(把它想象成[任意定义的] 20个附加列)只解释了我们数据中5%的方差。</p><p id="914c" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">令人困惑。让我们继续前进，让烟雾散去。</p><p id="1c00" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">正如我们刚才所说的，我们关心方差，因此我们创建一个矩阵来捕捉方差。方差(和协方差)的矩阵称为协方差矩阵，这里表示为<em class="mw"> S </em>。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/39355bae0dfd169ff12ecace64079c80.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*lULEAHjwVWJ4UKMVWPg-Gw.png"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">x-bar是平均值，x <em class="nz"> n </em>个体x值，S是协方差矩阵- <a class="ae mc" href="https://ucilnica.fri.uni-lj.si/pluginfile.php/892/course/section/510/bishop-pca.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="2aa2" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">同样，我们希望看到投影数据中的最大方差，这显然需要一个<strong class="mf jd">最大化问题— </strong>换句话说，我们希望相对于u最大化<em class="mw"> uSu </em>。请注意，由于u是一个单位向量，我们必须强制两个单位向量的乘积等于1。</p><p id="3366" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">这是一个典型的问题，我们的老朋友娄祖钰用他的拉格朗日乘数对此进行了解释。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/0f709f7e79ed37d64824be0076e7e2a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:388/format:webp/1*Ca8V3c3uklkIuqU0dY91nQ.png"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">协方差矩阵+单位向量约束</p></figure><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/cd1e7e69ad8a77a92d6a686911d8a55a.png" data-original-src="https://miro.medium.com/v2/resize:fit:202/format:webp/1*RJb0lSO20-Ow8nbxkcpOug.png"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">推导第一个等式并将其设置为零</p></figure><p id="3d2e" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">这个方程对许多人来说非常熟悉，它是特征值/向量计算中的典型例子。如果我们现在从左边乘以u，我们求解λ的方程:</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/04fd98999b3545098ece2d11b14dbf51.png" data-original-src="https://miro.medium.com/v2/resize:fit:216/format:webp/1*NyrQxsv4rTwbCADl9IhdFw.png"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">目标:找出最大的特征值及其最大的特征向量</p></figure><p id="9fe3" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">根据我们刚刚发现的，我们可以总结为我们正在寻找<strong class="mf jd">个最大特征值及其相关特征向量，因为它们为我们提供了投影数据</strong>的最大方差。回到我之前提到的，PCA上下文中的<strong class="mf jd">特征向量被称为主成分</strong>。要更深入地了解这篇文章的理论部分，请随意查阅这篇<a class="ae mc" href="https://ucilnica.fri.uni-lj.si/pluginfile.php/892/course/section/510/bishop-pca.pdf" rel="noopener ugc nofollow" target="_blank">的伟大工作论文</a>。</p><p id="0c17" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">在我们讨论PCA的实现之前，我想概述一下线性代数的一个重要概念，它也用在PCA的上下文中，即奇异值分解(SVD)。</p><p id="1c6d" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">SVD将形状为<em class="mw"> M </em> x <em class="mw"> N </em>的矩阵分解成三个矩阵，一个酉矩阵U、一个矩形对角矩阵适马和另一个酉矩阵V:</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/ccf564fafd1a2a71f54203d3aa4eeefe.png" data-original-src="https://miro.medium.com/v2/resize:fit:116/format:webp/1*oIiu6dvSHRxvYk_aN3UDSA.png"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">分解矩阵M</p></figure><p id="5da2" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">在五氯苯甲醚的世界里，这与我们有什么关系？因为u(左奇异值)是M*M^T的本征向量，v也是M^T*M.的本征向量(右奇异值)，所以Sigma是一个对角矩阵，包含与u和v相关的各自的本征值，因此您可以看到它的发展方向。</p><p id="32d9" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">如果你找不到任何关于网飞的好东西，我完全可以推荐你去看Gilbert Strang教授(麻省理工学院)关于奇异值分解的课——你不会后悔的:</p><figure class="ln lo lp lq gt lr"><div class="bz fp l di"><div class="od oe l"/></div></figure><h1 id="f614" class="mx ks it bd kt my mz na kw nb nc nd kz ki ne kj ld kl nf km lh ko ng kp ll nh bi translated">PCA的技术实现</h1><p id="b025" class="pw-post-body-paragraph md me it mf b mg ni kd mi mj nj kg ml la nk mn mo le nl mq mr li nm mt mu mv im bi translated">显然，在处理数据时，我们不会手动完成所有这些，因此我将提供一个关于如何在Python中使用PCA的简要指南。让我们通过scikit-learn中的Scipy和implmentations来看看PCA的以下选项:</p><ul class=""><li id="88ca" class="of og it mf b mg mh mj mk la oh le oi li oj mv ok ol om on bi translated">主成分分析</li><li id="f7ae" class="of og it mf b mg oo mj op la oq le or li os mv ok ol om on bi translated">德拉贡诺夫狙击步枪（Snayperskaya Vinyovka Dragunov的缩写）</li><li id="edd5" class="of og it mf b mg oo mj op la oq le or li os mv ok ol om on bi translated">特征向量/值</li></ul><p id="9a95" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">在接下来的几个段落中，我们将对一个数据示例执行降维，并将有损图像转换回其原始形状。</p><p id="2c7a" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">下面的篮球图像被选为我们的数据集，它将进一步用于应用主成分分析，或者在这种特殊情况下进行压缩。最终我们将使用PCA的思想来提供低维图像，并进一步将图像转换回原始空间。</p><p id="6ddb" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">换句话说，我们执行降维，然后将图像转换回原始维度(“逆”)—如果我们只想将数据投影到一个更低的维度上—回想一下我在一开始提到的散点图—我们希望最终得到一个<em class="mw"> Nx2 </em>矩阵(显然，这对图像不是很有用)。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ot"><img src="../Images/a98c8c642913cfa8c51f5c88ab797259.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kqi-yHdIEhTDBTeD2Sw5-Q.jpeg"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">献给2021年ACC冠军🏆— GT男子篮球队—图片由<a class="ae mc" href="https://unsplash.com/photos/5nk3wSFUWZc" rel="noopener ugc nofollow" target="_blank">本·赫尔希</a>提供</p></figure><h2 id="d464" class="kr ks it bd kt ku kv dn kw kx ky dp kz la lb lc ld le lf lg lh li lj lk ll iz bi translated">主成分分析</h2><p id="ea67" class="pw-post-body-paragraph md me it mf b mg ni kd mi mj nj kg ml la nk mn mo le nl mq mr li nm mt mu mv im bi translated">Scikit-learn的PCA实现使用SVD来投影到一个更低维度的空间，<strong class="mf jd">为你居中数据</strong>，但是不缩放。由于居中，PCA非常方便，因为它节省了另一行代码，而且还提供了简单的转换函数，可以毫无麻烦地链接起来。</p><figure class="ln lo lp lq gt lr"><div class="bz fp l di"><div class="ou oe l"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">scikit learn中的超级简单PCA</p></figure><p id="9641" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">仅仅两行“PCA-code”就让我们得到了一个更低维的数据集/压缩图像，这不是很神奇吗？顺便说一下，如果我们删除代码中的<em class="mw">逆变换</em>部分，我们会得到更低维度的数据。</p><p id="2e30" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">40个组件的结果有损耗，但令人印象深刻:</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ov"><img src="../Images/a7c117e1ab405bc196d8a62b9b534c1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*FcMjjNE9T23hHSogbxdI8A.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">使用40个主成分的压缩图像</p></figure><p id="14ad" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">仅参考我之前的陈述，如果我们不讨论图像，而是数据集，我们可能希望将数据减少到只有2个组件(并绘制它们)。我任意选择了40个组件，让压缩后的图像更加赏心悦目。</p><h2 id="cede" class="kr ks it bd kt ku kv dn kw kx ky dp kz la lb lc ld le lf lg lh li lj lk ll iz bi translated">奇异值分解</h2><p id="a1c2" class="pw-post-body-paragraph md me it mf b mg ni kd mi mj nj kg ml la nk mn mo le nl mq mr li nm mt mu mv im bi translated">请注意，截断的SVD不会使<strong class="mf jd">的数据</strong>居中，所以为了得到我们刚刚得到的结果，您需要扣除平均值——mu，就像代码中被截断的希腊字母一样。</p><figure class="ln lo lp lq gt lr"><div class="bz fp l di"><div class="ou oe l"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">sklearn的SVD实现</p></figure><p id="c72f" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">或者，我们可以简单地使用Numpy版本的SVD，如果我们想要获得U、适马和V.T .的三个分解矩阵，这是非常有用的。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/ccf564fafd1a2a71f54203d3aa4eeefe.png" data-original-src="https://miro.medium.com/v2/resize:fit:116/format:webp/1*oIiu6dvSHRxvYk_aN3UDSA.png"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">分解矩阵M</p></figure><p id="7807" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">下一个实现准确地返回三个矩阵，这些矩阵可用于重构矩阵M(的简化版本)。</p><figure class="ln lo lp lq gt lr"><div class="bz fp l di"><div class="ou oe l"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">Numpy奇异值分解</p></figure><p id="8f04" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">结果与我们在PCA实施中已经看到的没有什么不同:</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ov"><img src="../Images/a7c117e1ab405bc196d8a62b9b534c1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*FcMjjNE9T23hHSogbxdI8A.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">奇异值分解下的压缩图像</p></figure><p id="1deb" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">顺便提一下，如果你的矩阵不是半正定的(<strong class="mf jd"> psd </strong>，所以所有特征值都大于或等于<strong class="mf jd">0【因此是“半”】)，我不会使用SVD，而是使用下面PCA方法中概述的“特征值”。关于psd矩阵的奇异值分解的有趣讨论可在此处找到:</strong></p><div class="ow ox gp gr oy oz"><a href="https://math.stackexchange.com/a/3818408" rel="noopener  ugc nofollow" target="_blank"><div class="pa ab fo"><div class="pb ab pc cl cj pd"><h2 class="bd jd gy z fp pe fr fs pf fu fw jc bi translated">负定矩阵的特征值和奇异值</h2><div class="pg l"><h3 class="bd b gy z fp pe fr fs pf fu fw dk translated">感谢为数学栈交换贡献一个答案！请务必回答问题。提供详细信息…</h3></div><div class="ph l"><p class="bd b dl z fp pe fr fs pf fu fw dk translated">math.stackexchange.com</p></div></div><div class="pi l"><div class="pj l pk pl pm pi pn lw oz"/></div></div></a></div><h2 id="361e" class="kr ks it bd kt ku kv dn kw kx ky dp kz la lb lc ld le lf lg lh li lj lk ll iz bi translated">特征值/特征向量</h2><blockquote class="po pp pq"><p id="9e3a" class="md me mw mf b mg mh kd mi mj mk kg ml pr mm mn mo ps mp mq mr pt ms mt mu mv im bi translated">在你开始研究这个之前，如果你只是想得到一个较低维度的数据集(或者在这个例子中是图像)，那么<strong class="mf jd">只要跳回到PCA/SVD </strong>就可以了，不要理会下面显示的手动步骤。</p></blockquote><p id="54ce" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">作为第一步，我们需要获得我们之前看到的协方差矩阵。为此，取我们的初始矩阵A，从矩阵中的所有值x中减去平均值。这为我们提供了一个<strong class="mf jd">新的居中矩阵<em class="mw">一个</em> </strong> <em class="mw">。</em></p><p id="89fc" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">下面的块只是使用特征向量分解来获得一个压缩图像——如此简单！</p><figure class="ln lo lp lq gt lr"><div class="bz fp l di"><div class="ou oe l"/></div></figure></div><div class="ab cl pu pv hx pw" role="separator"><span class="px bw bk py pz qa"/><span class="px bw bk py pz qa"/><span class="px bw bk py pz"/></div><div class="im in io ip iq"><p id="fbed" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">当我们这样做的时候:“特征”在机器学习中几乎无处不在，所以重温这些从来都不是一个坏主意。如果你感兴趣，可以在这里找到更多的建议:</p><div class="ow ox gp gr oy oz"><a rel="noopener follow" target="_blank" href="/identify-well-connected-users-in-a-social-network-19ea8dd50b16"><div class="pa ab fo"><div class="pb ab pc cl cj pd"><h2 class="bd jd gy z fp pe fr fs pf fu fw jc bi translated">在无向图中识别良好连接的用户</h2><div class="pg l"><h3 class="bd b gy z fp pe fr fs pf fu fw dk translated">这主要是关于使用无向图和Scipy的稀疏矩阵实现(首席运营官)来存储数据和…</h3></div><div class="ph l"><p class="bd b dl z fp pe fr fs pf fu fw dk translated">towardsdatascience.com</p></div></div><div class="pi l"><div class="qb l pk pl pm pi pn lw oz"/></div></div></a></div></div><div class="ab cl pu pv hx pw" role="separator"><span class="px bw bk py pz qa"/><span class="px bw bk py pz qa"/><span class="px bw bk py pz"/></div><div class="im in io ip iq"><h1 id="da8a" class="mx ks it bd kt my qc na kw nb qd nd kz ki qe kj ld kl qf km lh ko qg kp ll nh bi translated">结论</h1><p id="3e03" class="pw-post-body-paragraph md me it mf b mg ni kd mi mj nj kg ml la nk mn mo le nl mq mr li nm mt mu mv im bi translated">主成分分析是一种非常有用的方法，可以降低数据集的复杂性。只需要一点理论和几行代码，我们就能在一个简单的散点图中展示甚至更高维的数据。PCA的概念当然可以被增强到各种更深入的概念，例如主成分回归，其可以启发对另一天的展望。</p><p id="28b2" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated">{照顾好自己，如果可以的话，也照顾好别人}</p><p id="a7c4" class="pw-post-body-paragraph md me it mf b mg mh kd mi mj mk kg ml la mm mn mo le mp mq mr li ms mt mu mv im bi translated"><em class="mw"> —借用史蒂芬·都伯纳</em></p></div></div>    
</body>
</html>