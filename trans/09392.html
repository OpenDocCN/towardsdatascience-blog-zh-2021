<html>
<head>
<title>The Ultimate Beginner Guide to Boosting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">助推终极初学者指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-ultimate-beginner-guide-to-boosting-84e5a95136a1?source=collection_archive---------32-----------------------#2021-08-31">https://towardsdatascience.com/the-ultimate-beginner-guide-to-boosting-84e5a95136a1?source=collection_archive---------32-----------------------#2021-08-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5ec4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">假人的集合方法。包含动手Python代码的循序渐进教程。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/daf0a2d477f254911a35fecd9776fcd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*K862aDM7rWt7Ce5mJLa36w.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/photos/OHOU-5UVIYQ" rel="noopener ugc nofollow" target="_blank"> SpaceX </a>在Unsplash上拍摄的照片</p></figure><p id="e43a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv"> Boosting </em>是来自<em class="lv">集成学习</em>范式的元算法，其中多个模型(通常称为“弱学习者”)被训练来解决同一个问题，并组合起来以获得更好的结果。这篇文章介绍了你所需要的一切，以便与助推起飞。杰出的计算理论家莱斯利·瓦里安推动了这一范式。</p><p id="b03d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我最近写了关于bagging的文章，bagging根据数据在多个引导上构建相同的模型，并结合每个模型的预测来获得总体分类或预测。</p><div class="lw lx gp gr ly lz"><a href="https://betterprogramming.pub/bagging-tutorial-classify-higgs-boson-particles-with-ai-941801559231" rel="noopener  ugc nofollow" target="_blank"><div class="ma ab fo"><div class="mb ab mc cl cj md"><h2 class="bd iu gy z fp me fr fs mf fu fw is bi translated">装袋教程——用人工智能分类希格斯玻色子粒子</h2><div class="mg l"><h3 class="bd b gy z fp me fr fs mf fu fw dk translated">Python代码集成学习实用指南</h3></div><div class="mh l"><p class="bd b dl z fp me fr fs mf fu fw dk translated">better编程. pub</p></div></div><div class="mi l"><div class="mj l mk ml mm mi mn ks lz"/></div></div></a></div><p id="c2c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">打包的一个巨大优势是我们可以并行化它。另一方面，boosting通过从弱学习者开始并添加一个基于弱学习者残差训练的新模型来顺序工作。</p><p id="d750" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Boosting重复几次，每个模型都在原始数据集的变化版本上进行训练，并使用学习率进行正则化。与装袋不同，助推利用团队合作。每个运行的模型决定了下一个模型将关注的特性。因此，强化以一种适应性很强的方式依次训练弱的学习者。</p><p id="95f0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每个模型可能不太适合数据，但整体的线性组合可以是富有表现力和灵活性的。添加到集合中的每个新的简单模型补偿了当前集合的弱点。</p><p id="cd15" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将通过注意单个分类器所犯错误的模式以及如何根据这些错误训练弱模型来提高准确性来激励Boosting。</p><h1 id="9158" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">为什么升压有效？</h1><p id="497a" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">让我们使用UCI机器学习库中希格斯数据集的一个小子集。<a class="ae ky" href="https://www.nature.com/articles/ncomms5308" rel="noopener ugc nofollow" target="_blank">这份2014年的论文</a>包含了关于数据的更多细节。</p><p id="2d17" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">每一行代表高能质子束碰撞的实验。类别栏区分产生希格斯玻色子的碰撞(值1)和只产生背景噪音的碰撞(值0)。我们对使用bagging技术预测类别感兴趣。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/452d6f93c5089f7b670759e564bafff9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5PWK7LqnskvFnPV6.png"/></div></div></figure><p id="37cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将深度为3的决策树<code class="fe nm nn no np b">tree1</code>与训练数据进行拟合。对于每个预测值，我们绘制一个比较两种分布的图:正确分类的样本和错误分类的样本的预测值。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="e2c7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们在下图中证明的那样，正确预测和错误预测的样本值分布几乎相同。这一证据表明，样本被错误分类的可能性并不取决于其价值。因此，我们可以通过将决策树与另一个根据错误分类信息训练的树相结合来提高决策树的预测能力。这就是做助推背后的动力。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/13e908e81228c345321835a787cd4ed0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HFR5dM7WV3xHP90IacYDrw.png"/></div></div></figure><h1 id="c97f" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">升压是如何工作的？</h1><p id="41b3" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">理解boosting的最好方法是从头开始使用两个分类器实现它的简化版本。</p><p id="5ab5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们将第一个分类器<code class="fe nm nn no np b">tree1</code>定义为深度为3的简单决策树。第二个分类器<code class="fe nm nn no np b">tree2</code>是另一个深度为3的决策树。<code class="fe nm nn no np b">tree1</code>在原始训练数据集上进行训练。<code class="fe nm nn no np b">tree2</code>在对<code class="fe nm nn no np b">tree1</code>误分类的样本应用权重2后获得的改变后的训练数据集上进行训练。因此，<code class="fe nm nn no np b">tree2</code>从弱学习者<code class="fe nm nn no np b">tree1</code>的残差中学习。通过对两棵树的预测进行平均来计算总体分类。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="8d62" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果表明如何通过在由另一个弱学习器产生的残差上训练一个弱学习器，并平均两个分类器的预测概率；我们提高了整体测试精度。这是助推机制的一个很好的例证。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/cbe126a2d96311d33435eda5e5d8855b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kupXBleGucf0R0TzlMlLKg.png"/></div></div></figure><h1 id="ec1f" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">梯度增强是如何工作的？</h1><p id="92c2" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">直观地，我们添加到我们的集成模型中的每个弱学习者模型从集成的错误中学习。因此，随着每一次加法，加权残差影响下一个弱学习者。如果我们将权重视为一个调整参数，那么我们可以使用最流行的优化技术:梯度下降来找到它的最优值。</p><div class="lw lx gp gr ly lz"><a href="https://pub.towardsai.net/do-you-understand-gradient-descent-and-backpropagation-most-dont-929d65f57a6c" rel="noopener  ugc nofollow" target="_blank"><div class="ma ab fo"><div class="mb ab mc cl cj md"><h2 class="bd iu gy z fp me fr fs mf fu fw is bi translated">你理解梯度下降和反向传播吗？大多数人不知道。</h2><div class="mg l"><h3 class="bd b gy z fp me fr fs mf fu fw dk translated">机器学习中一种常用优化算法背后的简单数学直觉。</h3></div><div class="mh l"><p class="bd b dl z fp me fr fs mf fu fw dk translated">pub.towardsai.net</p></div></div><div class="mi l"><div class="nu l mk ml mm mi mn ks lz"/></div></div></a></div><p id="cf52" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了掌握梯度推进是如何工作的，让我们重新考虑深度为1、2、3和4的几个决策树作为弱学习器。这一次，我们将应用学习率为0.05的梯度下降，并运行boosting进行800次迭代。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="d91f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用boosting，我们在每个新的迭代中从以前的决策树的残差中学习，因此我们观察到训练集准确性不断增加，直到我们将过度拟合数据并观察到接近100%的稳态。</p><p id="0744" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如下图所示，树的深度对趋势有很大的影响:树越大，训练集的准确性越高，而不管迭代的次数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/a232168ae55dc0955b2ed93750239d26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-pCkRKhD3qsPBlA8Z_PK2A.png"/></div></div></figure><p id="d6de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">深度1和深度2的树可能是非常慢的学习者，这需要更多的迭代次数。深度3和深度4的树学习速度更快；测试集的精确度在第一次迭代中稳步增加，直到达到峰值，然后随着迭代次数的增加而降低。深度1和深度2的树对数据的拟合不足，而深度4的树对数据的拟合过度最多。深度3表示最低的方差和最低的偏差。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/ee620d207e91e67e860cb648f902926b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wb0hSmiM_wuexRVF4g1lOw.png"/></div></div></figure><p id="05cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基于我们刚刚制作的图表，弱学习者深度和迭代次数的什么组合看起来是最佳的？</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="d431" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在97次提升迭代之后发现的梯度提升的深度3决策树分类器在低方差和低偏差方面似乎是最佳的。</p><p id="05ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">梯度提升比使用前一部分中更新的权重的手动提升的深度3树实现了更好的准确性。</p><p id="e4ab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">精确度的提高表明梯度推进实际上很有帮助。特别是，与我之前的文章中通过装袋获得的<a class="ae ky" href="https://betterprogramming.pub/bagging-tutorial-classify-higgs-boson-particles-with-ai-941801559231" rel="noopener ugc nofollow" target="_blank">最佳精度相比，它的表现更好。</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/e6e2a636c60fbcc017d9f95fc4a58412.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NSox_koPL_XHfntNLh25JQ.png"/></div></div></figure><h1 id="5e3a" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">结论</h1><p id="8c0a" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">Bagging和boosting是所谓的“集成”技术，通过聚合许多弱学习模型，如单决策树，大大提高了预测的准确性。</p><p id="9096" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以对每棵树单独进行套袋训练。因此，装袋技术更适合于并行化，例如，在多核CPU计算机上。提升是连续的，因为每个树都是使用前一个树构建的。</p><p id="075a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当模型过度拟合时，装袋对于减少方差是非常有用的。升压适用于降低欠拟合模型中的偏差。</p></div></div>    
</body>
</html>