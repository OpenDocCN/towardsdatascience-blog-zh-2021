<html>
<head>
<title>Understand Generalized Linear Models, and How It Relates to Linear, Logistic and Poisson Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解广义线性模型，以及它与线性、逻辑和泊松回归的关系</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understand-generalized-linear-models-and-how-it-relates-to-linear-logistic-and-poisson-regression-53f3aea8a9d?source=collection_archive---------5-----------------------#2021-10-23">https://towardsdatascience.com/understand-generalized-linear-models-and-how-it-relates-to-linear-logistic-and-poisson-regression-53f3aea8a9d?source=collection_archive---------5-----------------------#2021-10-23</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="b9e7" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">教育</h2><div class=""/><div class=""><h2 id="5dfc" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">永远不要再混淆线性回归和广义线性回归，带着清晰的思路离开。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/f2d947b58af1fbb618d647e24c17aab7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*y7Js9e3L681Gb_4D"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">亚历山大·杜默在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="71ea" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://www.kaggle.com/kaggle-survey-2021" rel="noopener ugc nofollow" target="_blank"> 2021 Kaggle </a>调查显示，与去年一样，数据科学界最常用的算法是线性或逻辑回归。我一点也不惊讶。不同特征的加权求和毕竟是最直观的做法。然而，对于大多数初学者来说，线性回归和其他类型的回归之间的区别变得不那么明显了。</p><p id="ed47" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当你读完这篇文章后，你会对线性回归与逻辑回归和泊松回归的区别有一个非常清晰的概念。此外，你也会清楚地理解当它用于分类任务时，我们仍然称之为逻辑“回归”的原因。</p><p id="1a3e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你以前曾经对这些不同类型的回归感到困惑，那么你来对地方了。然而，我请求你在接下来的几分钟内暂停你对这个主题的先验知识。如果你做到了，我保证你会带着水晶般的清澈离开。让我们开始吧。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="4f88" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">什么是广义线性模型？</h1><p id="b383" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">简而言之，广义线性模型(GLM)是一种数学模型，它将输出(响应变量的函数，稍后将详细介绍)与一个或多个输入变量(也称为探索变量)联系起来。下面的等式显示了输出如何与<em class="ni"> n </em>预测变量的线性求和相关。有相应的n+1个系数项(每个系数项对应于<em class="ni"> n </em>个预测变量，还有一个附加项帮助对任何偏移进行建模)</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/2a0c8fcbaaa4caeb5ccf41c432593edd.png" data-original-src="https://miro.medium.com/v2/resize:fit:852/format:webp/1*tYkTsCYtxAh7RAaUDAMENw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">GLM方程(图片由作者提供)</p></figure><p id="0c01" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">无论是进行线性回归、逻辑回归还是泊松回归，上述等式的右边(输入要素的加权组合)保持不变。</p><p id="2aa3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们来谈谈等式的左边，即输出。这是随机成分。它是响应变量期望值的函数。为简单起见，我们称期望值为<em class="ni"> Y </em>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/412e9499fdf98bb8ffc7ee8b3a1699b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:138/format:webp/1*5bxraD0Drq5uwsxa1KjkRQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">GLM方程中的输出部分(图片由作者提供)</p></figure><p id="db58" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">函数，<strong class="lk jd"> g(。)</strong>，称为链接功能。正是这个链接功能使得<em class="ni"> Y </em>的分布与右侧兼容(输入的线性组合)。</p><p id="62cc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当函数<strong class="lk jd"> g(.)</strong>是一个恒等函数，那么GLM方程就简化为一个正规的线性回归方程。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/7a7437394198512b39915013ba1b70da.png" data-original-src="https://miro.medium.com/v2/resize:fit:726/format:webp/1*i65Ch-8xbUFi-cvjJ87kaA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">当函数g是恒等式时，GLM方程就相当于一个简单的线性回归</p></figure><p id="a1a6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">换句话说，当连接函数是恒等式时，常规线性回归是广义线性模型的特例。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="3bb9" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">常规线性回归和GLM的主要区别</h1><p id="1a32" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">正如我前面提到的，常规线性回归是GLM的特例。然而，在进一步讨论之前，让我们快速地解释一下这两者之间的一些关键区别。</p><p id="c23f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">常规线性回归的关键假设是输出<em class="ni"> Y </em>的每个值都是独立的，输出是正态分布的，并且<em class="ni"> Y </em>的平均值通过线性组合与预测变量相关。在GLM中，输出不局限于正态分布，而是可以属于指数族的任何成员。</p><p id="d13e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">要解决常规线性回归问题，您可以采用最小二乘法或最大似然估计。它们会给出相同的结果。然而，GLM只能用最大似然估计法求解。</p><p id="e8be" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，正则线性回归模型有时也被称为标准最小二乘模型，是由高斯在1809年发明的，而GLM是由内尔德和威德伯恩在1972年发明的。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="c1d2" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">GLM与线性回归、逻辑回归和泊松回归有什么关系？</h1><p id="e855" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">既然你开始将线性回归视为GLM的一个特例，我们可以继续并确定GLM的其他情况。</p><p id="248a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当连接函数是logit(比例的自然对数)时，我们最终得到一个逻辑回归方程。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/7604c21b3f0e5bc1573d8275804bb841.png" data-original-src="https://miro.medium.com/v2/resize:fit:870/format:webp/1*DL8xz_w62B327xuqpwSfWg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">逻辑回归方程，其中输出是“赔率”的自然对数(图片由作者提供)</p></figure><p id="118b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当结果是二元的(例如，成功/失败，有疾病/没有疾病)时，逻辑回归是最合适的。在这样的应用中，比率(<em class="ni">Y/(1-</em>Y<em class="ni">)</em>类似于成功概率与失败概率的比率(也称为“赔率”)。</p><p id="0e35" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当关联函数是比率的自然对数时，我们最终得到一个泊松回归方程。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/4cb5f3c3ca83f3f7e357d3577d897899.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*Gw5TOupXK7V0svmMV0N4aw.png"/></div></figure><p id="e85b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当结果是给定时间间隔内的计数或给定时间内发生的事件数时，泊松回归最适合。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="1afc" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">链接函数与激活函数的关系</h1><p id="a511" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated"><strong class="lk jd">链接功能<em class="ni"> g(。)</em> </strong>是一个<strong class="lk jd">可逆</strong>函数，它转换输出的期望值，使其与线性预测器部分(GLM等式的右边)兼容。然而，在机器学习社区中，我们经常首先被介绍到链接函数的逆函数。这就是所谓的激活功能。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi no"><img src="../Images/474e53454610123220c803a8e3090f50.png" data-original-src="https://miro.medium.com/v2/resize:fit:860/format:webp/1*7bOINz-bGqhuvxrg26BgQg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">带有激活函数的GLM方程，它是链接函数的反函数(图片由作者提供)</p></figure><p id="26d0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">链接功能的逆功能与激活功能相同。术语“链接函数”在统计学文献中很常见，而术语“激活函数”在机器学习文献中更常见。</p><p id="3225" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">例如，如果您对逻辑回归的GLM方程(如前所示)的两边取指数，并应用简单的代数运算，您将最终得到以下逻辑回归方程(在机器学习文献中更常见)。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi np"><img src="../Images/513e0abff6a0e782e0b75cdae5a78f75.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*KgoS0hcWm7vFA7rzyhuSBw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">逻辑回归方程，它在机器学习文献中的表达方式(图片由作者提供)</p></figure></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="0bb8" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">最后的想法</h1><p id="c182" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">人们经常混淆和混淆这些概念。更糟糕的是，当用于分类时，人们经常称之为逻辑回归。这部分是因为我们，在机器学习社区，已经将监督学习分为分类(当输出是离散的)和回归(当输出是连续的)。</p><p id="d349" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当然，逻辑回归用于分类，但它仍然是一种回归技术。只有当你理解了广义线性模型，一个更重要的概念，这才有意义。</p><p id="7723" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最终，您将不同的输入变量组合成一个加权和，无论使用线性、逻辑还是泊松回归，都需要确定未知系数。</p><div class="nq nr gp gr ns nt"><a href="https://ahmarshah.medium.com/membership" rel="noopener follow" target="_blank"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd jd gy z fp ny fr fs nz fu fw jc bi translated">通过我的推荐链接加入Medium-Ahmar Shah博士(牛津)</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">ahmarshah.medium.com</p></div></div><div class="oc l"><div class="od l oe of og oc oh lb nt"/></div></div></a></div></div></div>    
</body>
</html>