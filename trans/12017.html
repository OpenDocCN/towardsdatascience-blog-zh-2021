<html>
<head>
<title>Combining Two Optimizers for a Powerful Method to Train Your Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">将两个优化器结合起来，形成一个训练模型的强大方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-efficient-hybrid-algorithm-to-solve-nonlinear-least-squares-problems-262801bcfe25?source=collection_archive---------8-----------------------#2021-12-03">https://towardsdatascience.com/an-efficient-hybrid-algorithm-to-solve-nonlinear-least-squares-problems-262801bcfe25?source=collection_archive---------8-----------------------#2021-12-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="8c6a" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="83c4" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">当Levenberg-Marquardt遇到准牛顿。是的，我们用Python从头开始构建它！</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/832ddeee49ec9a3a69dea1a43c8d952a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*QZvzgiM2VnhYyx8M"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@goshua13?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">约书亚·阿拉贡</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="557f" class="ln lo it lj b gy lp lq l lr ls"><strong class="lj jd">Table of Contents</strong> (read till the end to see how you can get the complete python code of this story)</span><span id="acd2" class="ln lo it lj b gy lt lq l lr ls">· <a class="ae lh" href="#48e0" rel="noopener ugc nofollow">Seeing the Big Picture</a><br/>· <a class="ae lh" href="#e640" rel="noopener ugc nofollow">Defining The Problem</a><br/>· <a class="ae lh" href="#d4e9" rel="noopener ugc nofollow">A Hybrid Method</a><br/>  ∘ <a class="ae lh" href="#0b95" rel="noopener ugc nofollow">Objective Function</a><br/>  ∘ <a class="ae lh" href="#d56d" rel="noopener ugc nofollow">General Algorithm</a><br/>  ∘ <a class="ae lh" href="#9602" rel="noopener ugc nofollow">Levenberg-Marquardt Step</a><br/>  ∘ <a class="ae lh" href="#3fca" rel="noopener ugc nofollow">Quasi-Newton Step</a><br/>· <a class="ae lh" href="#d1e8" rel="noopener ugc nofollow">Implementation</a><br/>· <a class="ae lh" href="#440b" rel="noopener ugc nofollow">Conclusion</a></span></pre><h1 id="48e0" class="lu lo it bd lv lw lx ly lz ma mb mc md ki me kj mf kl mg km mh ko mi kp mj mk bi translated">看到全局</h1><p id="01d0" class="pw-post-body-paragraph ml mm it mn b mo mp kd mq mr ms kg mt mu mv mw mx my mz na nb nc nd ne nf ng im bi nh translated"><span class="l ni nj nk bm nl nm nn no np di">在</span>之前的文章中，我们已经看到了<a class="ae lh" rel="noopener" target="_blank" href="/complete-step-by-step-gradient-descent-algorithm-from-scratch-acba013e8420">梯度下降</a>和<a class="ae lh" rel="noopener" target="_blank" href="/complete-step-by-step-conjugate-gradient-algorithm-from-scratch-202c07fb52a8">共轭梯度</a>算法的作用，作为两种最简单的优化方法。我们实现了线搜索来搜索目标函数的优化方向。</p><p id="a11b" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">还有另一种生成步骤的方法，称为<em class="nv">信任区域</em>。信赖域方法在当前迭代周围定义一个区域，在该区域内，它们相信模型是目标函数的适当表示，然后选择步长作为该区域中模型的近似极小值。实际上，与线搜索不同，它们同时选择方向和步长。</p><p id="44a6" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">还有很多其他优化方法，如牛顿法和拟牛顿法。在本文中，我们将使用Levenberg-Marquardt和拟牛顿法的混合，利用信赖域进行步长选择。此外，这篇文章带来了一个不同的问题。我们将得到一个数据集和数据的非线性模型，然后找到模型的最佳优化参数，而不是找到目标函数的优化器。优化的参数是由那些给出最小残差平方的人定义的。有些人可能称之为曲线拟合问题。</p><p id="de66" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">让我们导入一些库。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nw nx l"/></div></figure><h1 id="e640" class="lu lo it bd lv lw lx ly lz ma mb mc md ki me kj mf kl mg km mh ko mi kp mj mk bi translated">定义问题</h1><p id="f97f" class="pw-post-body-paragraph ml mm it mn b mo mp kd mq mr ms kg mt mu mv mw mx my mz na nb nc nd ne nf ng im bi translated">如下创建一个虚构的数据集。它有11个观察值和2个特征，第一个特征相当于第二个特征的索引。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nw nx l"/></div></figure><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="56a2" class="ln lo it lj b gy lp lq l lr ls">t 	  y<br/>------------------<br/>[[ 0.0000 0.0000]<br/> [ 2.0000 3.5500]<br/> [ 4.0000 3.8200]<br/> [ 6.0000 2.9800]<br/> [ 8.0000 2.3200]<br/> [10.0000 1.4800]<br/> [12.0000 1.0200]<br/> [14.0000 0.8100]<br/> [16.0000 0.4100]<br/> [18.0000 0.4200]<br/> [20.0000 0.1500]]</span><span id="6783" class="ln lo it lj b gy lt lq l lr ls">Data shape: (11, 2)</span></pre><p id="0c33" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">将数据可视化为卡特修斯平面中的点(<em class="nv"> t </em>，<em class="nv"> y </em> ) = ( <em class="nv"> tᵢ </em>，<em class="nv"> yᵢ </em>)，其中<em class="nv"> i </em> = 1，2，…，11。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nw nx l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="ab gu cl ny"><img src="../Images/c0ca9394f168a12d4330f5b4ac9db422.png" data-original-src="https://miro.medium.com/v2/format:webp/1*QXxEZ79EuKrO8x-fKjj11w.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="bbfc" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">我们立即意识到数据没有线性趋势(这里的点(0，0)是<em class="nv">而不是</em>一个异常值)。因此，我们使用的模型也是非线性的。相反，我们将使用两个指数函数的线性组合，根据定义，它被声明为</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/477a26c69c3d4f4f04dc2de786e9c490.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*iAAp3Z-W6UpLxBwzBIwnGg.png"/></div></figure><p id="ba18" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">如前所述，参数<em class="nv"> x = </em> [ <em class="nv"> x₁ </em>，<em class="nv"> x₂ </em>，<em class="nv"> x₃ </em>，<em class="nv"> x₄ </em>，将使用Levenberg-Marquardt (LM)和拟牛顿(QN)的混合方法进行估计。</p><h1 id="d4e9" class="lu lo it bd lv lw lx ly lz ma mb mc md ki me kj mf kl mg km mh ko mi kp mj mk bi translated">混合方法</h1><h2 id="0b95" class="ln lo it bd lv oa ob dn lz oc od dp md mu oe of mf my og oh mh nc oi oj mj iz bi translated">目标函数</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/c44c47dbfdeeaf2c0314d30ea249c902.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*i6UhNDI2LCMFs8NhEidVig.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由<a class="ae lh" href="https://www.flaticon.com/authors/vectors-market" rel="noopener ugc nofollow" target="_blank">矢量-市场</a>在<a class="ae lh" href="http://flaticon.com" rel="noopener ugc nofollow" target="_blank">平面图标</a>上显示</p></figure><p id="1313" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">总的来说，我们要为一组<em class="nv"> m </em>数据点(<em class="nv"> tᵢ </em>、<em class="nv"> yᵢ </em>)建立一个模型<em class="nv"> M </em> ( <em class="nv"> x </em>、<em class="nv"> t </em>)。为此，目标函数的具体定义是必要的。对于最小二乘问题，这个目标函数是每个数据点残差的平方和。数据点的残差由实际数据到模型预测的距离来定义，即通过用实际数据点减去该点的模型预测来定义。从数学上讲，第<em class="nv"> i </em>个数据点的残差为</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/e3b378492428822905036dda6f1ca576.png" data-original-src="https://miro.medium.com/v2/resize:fit:358/format:webp/1*fXCAt5bO9_hvduK5yREyXA.png"/></div></figure><p id="08a9" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">或者以向量的形式</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi om"><img src="../Images/b26c87c15d1c8d1ae23c651856ca33c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:330/format:webp/1*dQ0tjRjzS-lwryjv5p6JHA.png"/></div></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="9fe8" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">由于目标函数是每个数据点残差的平方和，因此可以写成</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi on"><img src="../Images/43e51b01f5284c8e5fd19dffedc81345.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*JU96vGg-dqBDMUqXjqrRvA.png"/></div></figure><p id="2982" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">为方便推导<em class="nv"> F </em>，附上上述系数。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="a544" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">设ǁ⋅ǁ是向量的最大范数的符号。我们的任务是找到使<em class="nv"> F </em>最小的<em class="nv"> x </em>。为此，我们将找到<em class="nv"> x </em>，其中ǁ <em class="nv"> g </em> ( <em class="nv"> x </em> )ǁ ≈ 0，其中<em class="nv">g</em>=<em class="nv">f’</em>是<em class="nv"> F </em>的一阶导数。衍生</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oo"><img src="../Images/7a5a2cd08237411965b99ee9d2dce536.png" data-original-src="https://miro.medium.com/v2/resize:fit:368/format:webp/1*TrU4OQFqbcz8iAVMjxj_QA.png"/></div></div></figure><p id="e555" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">w.r.t .每个<em class="nv"> xⱼ </em>利用链式法则，我们得到</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi op"><img src="../Images/d761aba7a51974a2ec342e704e8132ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/1*ILF9aLa-J3T8RCJeYbTxwg.png"/></div></figure><p id="2fc7" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">定义雅可比矩阵</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/2a2bc94692517aa071cb4516e79c3291.png" data-original-src="https://miro.medium.com/v2/resize:fit:306/format:webp/1*OIzgimdsRIHJHVT9OhfH-w.png"/></div></figure><p id="85eb" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated"><em class="nv"> g </em>的值可以计算如下</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi or"><img src="../Images/61a3b678676a713cae27ce71c1af7d2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*m7WMv8U3V9MeY_-oBXyTqw.png"/></div></figure><p id="c996" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">下面这个python函数，除了返回<em class="nv"> g </em>之外，还返回<em class="nv"> J </em>，后面会派上用场。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nw nx l"/></div></figure><h2 id="d56d" class="ln lo it bd lv oa ob dn lz oc od dp md mu oe of mf my og oh mh nc oi oj mj iz bi translated">一般算法</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/dabd40c3e0bd0627eecf6d7908891ca2.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*GPuTBV64ODTnufBUSInVpA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://www.flaticon.com/authors/becris" rel="noopener ugc nofollow" target="_blank"> becris </a>在<a class="ae lh" href="http://flaticon.com" rel="noopener ugc nofollow" target="_blank">平面图标</a>上拍摄的图像</p></figure><p id="6985" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">设<em class="nv"> x* </em>为残差平方和的极小值。最小二乘问题可以用一般的最优化方法来解决，但我们将提出更有效的特殊方法。1988年，Madsen [1]提出了一种混合方法，该方法将LM方法(二次收敛，如果<em class="nv"> F </em> ( <em class="nv"> x* </em> ) = 0，否则线性收敛)与QN方法相结合，即使<em class="nv"> F </em> ( <em class="nv"> x* </em> ) ≠ 0，该方法也能给出超线性收敛。</p><p id="44ce" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">在信赖域方法中，算法在当前迭代<em class="nv"> x </em>的邻域中生成<em class="nv"> F </em>行为的<em class="nv">信任</em>模型<em class="nv"> L </em>，</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi os"><img src="../Images/9c2e4b7a95c7a1fa8e0db641207748ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:294/format:webp/1*X_CwzRReOlnQsYqbMLaafA.png"/></div></div></figure><p id="eb0d" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">其中<em class="nv"> h </em>是要采取的步骤。我们假设我们知道一个正数δ，使得该模型在半径为δ、以<em class="nv"> x </em>为中心的球内足够精确，并且该算法将步长确定为</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/5a0ceaa59637deb4dd7eb905b747117c.png" data-original-src="https://miro.medium.com/v2/resize:fit:416/format:webp/1*smQ-4i4oM_k2sNJoskRpnw.png"/></div></figure><p id="1ea4" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">在<em class="nv">阻尼型</em>中，步长确定为</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/0ece3c7b32920af8c148b34e28c9b46b.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*7E1G39MrgYlqylqEhTpv8g.png"/></div></figure><p id="1a02" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">其中<em class="nv">阻尼参数</em> <em class="nv"> μ </em> ≥ 0。该术语</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/95d162d9e08a44081f1b2db1ddc3072f.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/1*qjyqAUVBY2R1eAYoFOlO6g.png"/></div></figure><p id="edd4" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">是用来惩罚大步流星的。</p><p id="381d" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">从现在开始，我们将开始迭代。因此，我们将引入一个索引<em class="nv"> k </em>，这意味着所讨论的变量是针对第<em class="nv"> k </em>次迭代的。例如，<em class="nv"> xₖ </em>是<em class="nv"> x </em>在第<em class="nv"> k </em>次迭代的值，如此类推<em class="nv">fₖ</em>=<em class="nv">f</em>(<em class="nv">xₖ</em>)<em class="nv">gₖ</em>=<em class="nv">g</em>(<em class="nv">xₖ</em>)<em class="nv">jₖ</em>=<em class="nv">j</em>(【t54</p><p id="bced" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">在迭代开始之前，重要的是初始化阻尼参数<em class="nv"> μ </em> = <em class="nv"> μ </em> ₀和海森近似<em class="nv"> B </em> ₀(海森在迭代<em class="nv"> x </em>时由<em class="nv">f】’</em>(<em class="nv">x</em>)定义，QN方法使用其近似<em class="nv"> B </em>)。</p><p id="aacb" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated"><em class="nv"> B </em> ₀的合理选择是对称正定矩阵<em class="nv"> I </em>，即单位矩阵。<em class="nv"> μ </em> ₀的选择应与中元素的大小有关</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/46e93e2ef187c9ddccbb50c166ed20e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:190/format:webp/1*O_db_mj4YZfLGo1yC_ui3A.png"/></div></figure><p id="d9bd" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">例如通过让</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/41b26ff47f43b0b03ddb1e76892ee949.png" data-original-src="https://miro.medium.com/v2/resize:fit:288/format:webp/1*DgO8Z6HemPOLUiCTvdXI9Q.png"/></div></figure><p id="c217" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">其中<em class="nv"> aᵢᵢ </em> ⁰是<em class="nv"> A </em> ₀的第<em class="nv"> i </em>个对角线分量，<em class="nv"> τ </em>由用户选择。在本文中，我们将设置<em class="nv"> τ </em> = 1 × 10⁻。</p><p id="7d25" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">下面这个python函数，除了返回<em class="nv"> μ </em> ₀，还返回<em class="nv"> A </em> ₀，后面会派上用场。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="39ba" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">在初始化了<em class="nv"> μ </em> ₀和<em class="nv"> B </em> ₀之后，迭代从LM方法的一系列步骤开始。如果性能指示<em class="nv"> F </em> ( <em class="nv"> x* </em>)显著非零，那么我们切换到QN方法以获得更好的性能。我们可能会得到一个指示，即最好切换回LM方法，因此也有一个机制，我们将在后面介绍。迭代继续进行，直到满足以下停止标准之一:</p><ul class=""><li id="b521" class="oy oz it mn b mo nq mr nr mu pa my pb nc pc ng pd pe pf pg bi translated">目标函数的梯度的范数足够接近零，也就是说，</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/9ee9d91c9ecb5b73ce5d392627b1e8d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:190/format:webp/1*E8SCRoJcTi79UP6d9q_fLA.png"/></div></figure><ul class=""><li id="45b1" class="oy oz it mn b mo nq mr nr mu pa my pb nc pc ng pd pe pf pg bi translated">步长的范数满足</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/765eb47090e9e11cd9f1517cebcd2ad1.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*C90lOd-O_OcUB_UtYOfi4Q.png"/></div></figure><ul class=""><li id="8cbf" class="oy oz it mn b mo nq mr nr mu pa my pb nc pc ng pd pe pf pg bi translated">采取的步骤数是1000</li></ul><p id="7903" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">本文将<em class="nv"> ε </em> ₁和<em class="nv"> ε </em> ₂设为1 × 10⁻⁵.</p><p id="571a" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">在每一步之后，无论是通过LM还是QN方法，<em class="nv"> Bₖ </em>通过首先定义来更新</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/0c1869a6b0683474312f6967301acaea.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/1*oUsYADAS3Gt0Qb8VY-jpNA.png"/></div></figure><p id="109f" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">如果</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/0626f112f2caf81fea7aee00a878a80f.png" data-original-src="https://miro.medium.com/v2/resize:fit:160/format:webp/1*F3lUJE8KhJnLE2123IIr9A.png"/></div></figure><p id="2d84" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">然后更新<em class="nv"> Bₖ </em>由</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/824236c3438e99f280008a0552a4f688.png" data-original-src="https://miro.medium.com/v2/resize:fit:738/format:webp/1*CJ2qZuOg86sxp1kcfYnFqA.png"/></div></figure><p id="c75b" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">其中<em class="nv"> vₖ </em> = <em class="nv"> Bₖhₖ </em>。否则，<em class="nv"> Bₖ </em>不变，即<em class="nv"> Bₖ₊₁ </em> = <em class="nv"> Bₖ </em>。</p><p id="82c2" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">由LM或QN方法找到的新的<em class="nv">xₖ₊₁</em>=<em class="nv">xₖ</em>+<em class="nv">hₖ</em>然后被传递到下一次迭代。整个过程可以写成下面的代码。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="c7d2" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">现在，让我们来看看每一步的细节。上面的<code class="fe pk pl pm lj b">LMstep()</code>和<code class="fe pk pl pm lj b">QNstep()</code>函数是怎么回事？</p><h2 id="9602" class="ln lo it bd lv oa ob dn lz oc od dp md mu oe of mf my og oh mh nc oi oj mj iz bi translated">勒文伯格-马夸特步骤</h2><p id="ee48" class="pw-post-body-paragraph ml mm it mn b mo mp kd mq mr ms kg mt mu mv mw mx my mz na nb nc nd ne nf ng im bi translated">在混合迭代开始时，首先使用LM方法。在第<em class="nv"> k </em>次迭代时，通过求解找到步骤<em class="nv"> hₖ </em></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/fad4f258301066a7d8f4ff817e77e231.png" data-original-src="https://miro.medium.com/v2/resize:fit:390/format:webp/1*GYKs9xQfTPpQ3GGc6gmPYA.png"/></div></figure><p id="fe5d" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">然后计算</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi po"><img src="../Images/179113dd365f8d7da700ab94d8aee3b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:348/format:webp/1*5NmqWm7b_0l5YeOa0HZlmA.png"/></div></figure><p id="c627" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">其中<em class="nv"> Fₖ₊₁ </em>就是<em class="nv">f</em>(<em class="nv">xₖ</em>+<em class="nv">hₖ</em>)。</p><p id="b90e" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">然后我们考虑两种情况:</p><ol class=""><li id="d67e" class="oy oz it mn b mo nq mr nr mu pa my pb nc pc ng pp pe pf pg bi translated">如果<em class="nv">ϱ</em>t80】0，这意味着迭代成功，我们可以更新<em class="nv">xₖ₊₁</em>=<em class="nv">xₖ</em>+<em class="nv">hₖ</em>。此时，我们需要检查<em class="nv"> F </em> ( <em class="nv"> x* </em>)是否远离零，即通过检查:如果ǁ<em class="nv">gₖ</em>ǁ&lt;0.02⋅<em class="nv">fₖ</em>连续三次迭代成功，那么我们就换成QN法进行下一次迭代。另外，更新</li></ol><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/0c7c910592619716f986e6a673f44e43.png" data-original-src="https://miro.medium.com/v2/resize:fit:796/format:webp/1*vLTTC0FIVYwMy9YjwT3oEg.png"/></div></figure><p id="6f5c" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">2.如果<em class="nv"> ϱ </em> ≤ 0，则迭代不成功，我们不会更新<em class="nv"> xₖ </em>，即<em class="nv"> xₖ₊₁ </em> = <em class="nv"> xₖ </em>。对于这种情况，更新<em class="nv"> μₖ₊₁ </em> = <em class="nv"> μₖν </em>和<em class="nv"> ν </em> := 2 <em class="nv"> ν </em>。</p><p id="1ef2" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">上面说的<em class="nv"> μ </em>的更新方法是尼尔森开发的。我们也有另一个由马夸特的变化，如下所示。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="e2ec" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">整个LM步骤如下所示。请注意，在下面的代码中，我们正在传递δ<em class="nv">ₖ</em>，这是QN步骤将使用的信赖域半径，以防我们在未来的迭代中将步骤计算方法更改为QN。这里我们不用δ<em class="nv">ₖ</em>。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nw nx l"/></div></figure><h2 id="3fca" class="ln lo it bd lv oa ob dn lz oc od dp md mu oe of mf my og oh mh nc oi oj mj iz bi translated">准牛顿步</h2><p id="0728" class="pw-post-body-paragraph ml mm it mn b mo mp kd mq mr ms kg mt mu mv mw mx my mz na nb nc nd ne nf ng im bi translated">由于QN方法使用信赖域进行步长计算，因此在开始使用QN方法时，初始化信赖域半径很重要</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/13373a512283a5d90098eee4c66e5826.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/format:webp/1*ldReN5kE9TQ80whrz6Iz1g.png"/></div></figure><p id="2123" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">其中<em class="nv"> hₗₘ </em>是前一个LM步获得的步。但是不要担心，这个初始化部分是不需要的，因为我们总是在LM和QN步骤中计算δ<em class="nv">ₖ</em>。如你所见，上面的最后一个公式已经嵌入到<code class="fe pk pl pm lj b">LMstep()</code>函数行14中。</p><p id="377f" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">在第<em class="nv"> k </em>次迭代时，通过求解找到步骤<em class="nv"> hₖ </em></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/20f23c50c01f0f8063f14776e3076772.png" data-original-src="https://miro.medium.com/v2/resize:fit:214/format:webp/1*fZtdOcTDOCekl-wCzdXSCg.png"/></div></figure><p id="f921" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">如果ǁ<em class="nv">hₖ</em>ǁ&gt;δ<em class="nv">ₖ</em>，那么将<em class="nv"> hₖ </em>的长度减少为δ<em class="nv">ₖ</em>，即通过设置</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/325b8599b0205f949c705878cef3b059.png" data-original-src="https://miro.medium.com/v2/resize:fit:298/format:webp/1*5jPpoQtkpd6H9BuO1_YYOw.png"/></div></figure><p id="fe4a" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">δ<em class="nv">ₖ</em>然后在<em class="nv"> ϱ </em>的几个条件下被更新，其使用与我们在LM步骤中使用的公式相同的公式计算:</p><ul class=""><li id="5a6d" class="oy oz it mn b mo nq mr nr mu pa my pb nc pc ng pd pe pf pg bi translated">如果<em class="nv">ϱ</em>t94】0.25，则设定δ<em class="nv">ₖ₊₁</em>=δ<em class="nv">ₖ</em>/2</li><li id="e033" class="oy oz it mn b mo pu mr pv mu pw my px nc py ng pd pe pf pg bi translated">如果<em class="nv">ϱ</em>t95】0.75，则设定δ<em class="nv">ₖ₊₁</em>= max {δ<em class="nv">ₖ</em>，3 ǁ <em class="nv"> hₖ </em> ǁ}</li><li id="6094" class="oy oz it mn b mo pu mr pv mu pw my px nc py ng pd pe pf pg bi translated">否则，δ<em class="nv">ₖ₊₁</em>=δ<em class="nv">ₖ</em></li></ul><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="e012" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">接下来，我们考虑两种情况:</p><ol class=""><li id="6dc7" class="oy oz it mn b mo nq mr nr mu pa my pb nc pc ng pp pe pf pg bi translated">如果<em class="nv">fₖ₊₁</em>≤(1+<em class="nv">δ</em>)<em class="nv">fₖ</em>和ǁ<em class="nv">gₖ₊₁</em>ǁ&lt;ǁ<em class="nv">gₖ</em>ǁ，或者<em class="nv">fₙt34&lt;<em class="nv">fₙ</em>，那么迭代成功，我们可以更新<em class="nv"> x此时，我们专注于使<em class="nv"> g </em>更接近零，因此我们接受<em class="nv"> F </em>的值稍微增加，例如小至<em class="nv"> δ </em> = <em class="nv"> √ε </em>乘以先前的<em class="nv"> F </em>，其中<em class="nv"> ε </em>是机器ε。</em></em></li><li id="2da0" class="oy oz it mn b mo pu mr pv mu pw my px nc py ng pp pe pf pg bi translated">否则迭代视为失败，<em class="nv"> xₖ </em>不会更新，即<em class="nv"> xₖ₊₁ </em> = <em class="nv"> xₖ </em>。</li></ol><p id="2d8c" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">如果梯度下降得不够快，即ǁ<em class="nv">gₖ₊₁</em>ǁ≥ǁ<em class="nv">gₖ</em>ǁ，那么我们转到LM步进行下一次迭代。</p><p id="928e" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">整个QN步骤可以如下所示。注意，在下面的代码中，我们像在LM步骤中一样计算<em class="nv"> ϱ </em>。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nw nx l"/></div></figure><h1 id="d1e8" class="lu lo it bd lv lw lx ly lz ma mb mc md ki me kj mf kl mg km mh ko mi kp mj mk bi translated">履行</h1><p id="a80f" class="pw-post-body-paragraph ml mm it mn b mo mp kd mq mr ms kg mt mu mv mw mx my mz na nb nc nd ne nf ng im bi translated">我们现在准备应用我们的知识来解决本文开头定义的问题。我们将在三种不同的情况下改变初始点:</p><ul class=""><li id="651e" class="oy oz it mn b mo nq mr nr mu pa my pb nc pc ng pd pe pf pg bi translated">₀ = [-1，1，-10，10]</li><li id="14ed" class="oy oz it mn b mo pu mr pv mu pw my px nc py ng pd pe pf pg bi translated">₀ = [-4，1，2，-3]</li><li id="2702" class="oy oz it mn b mo pu mr pv mu pw my px nc py ng pd pe pf pg bi translated">₀ = [0，0，0，0]</li></ul><p id="67d2" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">但首先，让我们引入一个python函数<code class="fe pk pl pm lj b">MakeAnimation()</code>来动画化算法的学习过程。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nw nx l"/></div></figure><h2 id="f5ff" class="ln lo it bd lv oa ob dn lz oc od dp md mu oe of mf my og oh mh nc oi oj mj iz bi translated">场景1</h2><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nw nx l"/></div></figure><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="e5b6" class="ln lo it lj b gy lp lq l lr ls">Iteration:1	x = [-1.0000 0.9500 -10.0000 9.9997], <br/>		gradient = 64932772695986634752.0000, step = LM<br/>Iteration:2	x = [-1.0000 0.9001 -10.0000 9.9994], <br/>		gradient = 8871075343566318592.0000, step = LM<br/>Iteration:3	x = [-1.0000 0.8510 -10.0000 9.9992], <br/>		gradient = 1250145055141384192.0000, step = LM<br/>Iteration:4	x = [-1.0000 0.8050 -10.0000 9.9990], <br/>		gradient = 199964648437467488.0000, step = LM<br/>Iteration:5	x = [-1.0000 0.7686 -10.0000 9.9988], <br/>		gradient = 46950703393293552.0000, step = LM<br/>Iteration:6	x = [-1.0000 0.7454 -10.0000 9.9987], <br/>		gradient = 18613190948807260.0000, step = LM<br/>Iteration:7	x = [-1.0000 0.7283 -10.0000 9.9986], <br/>		gradient = 9421138350386658.0000, step = LM<br/>Iteration:8	x = [-1.0000 0.7127 -10.0000 9.9985], <br/>		gradient = 5067642599353893.0000, step = LM<br/>Iteration:9	x = [-1.0000 0.6974 -10.0000 9.9984], <br/>		gradient = 2761173975265880.5000, step = LM<br/>Iteration:10	x = [-1.0000 0.6822 -10.0000 9.9983], <br/>		gradient = 1508437660486741.0000, step = LM<br/>Iteration:11	x = [-1.0000 0.6670 -10.0000 9.9983], <br/>		gradient = 824513384402736.2500, step = LM<br/>Iteration:12	x = [-1.0000 0.6518 -10.0000 9.9982], <br/>		gradient = 450734053994430.5000, step = LM<br/>Iteration:13	x = [-1.0000 0.6366 -10.0000 9.9981], <br/>		gradient = 246409971021028.8438, step = LM<br/>Iteration:14	x = [-1.0000 0.6214 -10.0000 9.9980], <br/>		gradient = 134711382454059.4531, step = LM<br/>Iteration:15	x = [-1.0000 0.6062 -10.0000 9.9980], <br/>		gradient = 73647395414745.6094, step = LM<br/>Iteration:16	x = [-1.0000 0.5909 -10.0000 9.9979], <br/>		gradient = 40264084923738.5469, step = LM<br/>Iteration:17	x = [-1.0000 0.5757 -10.0000 9.9978], <br/>		gradient = 22013352246245.9766, step = LM<br/>Iteration:18	x = [-1.0000 0.5605 -10.0000 9.9977], <br/>		gradient = 12035471978657.0332, step = LM<br/>Iteration:19	x = [-1.0000 0.5452 -10.0000 9.9976], <br/>		gradient = 6580356674017.7891, step = LM<br/>Iteration:20	x = [-1.0000 0.5299 -10.0000 9.9976], <br/>		gradient = 3597874150520.1475, step = LM<br/>Iteration:21	x = [-1.0000 0.5146 -10.0000 9.9975], <br/>		gradient = 1967223429070.2634, step = LM<br/>Iteration:22	x = [-1.0000 0.4993 -10.0000 9.9974], <br/>		gradient = 1075656653534.5968, step = LM<br/>Iteration:23	x = [-1.0000 0.4840 -10.0000 9.9973], <br/>		gradient = 588175745878.2034, step = LM<br/>Iteration:24	x = [-1.0000 0.4687 -10.0000 9.9973], <br/>		gradient = 321629128815.2425, step = LM<br/>Iteration:25	x = [-1.0000 0.4534 -10.0000 9.9972], <br/>		gradient = 175881428072.9430, step = LM<br/>Iteration:26	x = [-1.0000 0.4380 -10.0000 9.9971], <br/>		gradient = 96183963098.1861, step = LM<br/>Iteration:27	x = [-1.0000 0.4226 -10.0000 9.9970], <br/>		gradient = 52602379508.8333, step = LM<br/>Iteration:28	x = [-1.0000 0.4072 -10.0000 9.9969], <br/>		gradient = 28769372395.6593, step = LM<br/>Iteration:29	x = [-1.0000 0.3918 -10.0000 9.9969], <br/>		gradient = 15735488020.7650, step = LM<br/>Iteration:30	x = [-1.0000 0.3763 -10.0000 9.9968], <br/>		gradient = 8607119032.2862, step = LM<br/>Iteration:31	x = [-1.0000 0.3609 -10.0000 9.9967], <br/>		gradient = 4708326165.4597, step = LM<br/>Iteration:32	x = [-1.0000 0.3454 -10.0000 9.9966], <br/>		gradient = 2575789155.9012, step = LM<br/>Iteration:33	x = [-1.0000 0.3298 -10.0000 9.9965], <br/>		gradient = 1409268155.2911, step = LM<br/>Iteration:34	x = [-1.0000 0.3142 -10.0000 9.9965], <br/>		gradient = 771119669.0625, step = LM<br/>Iteration:35	x = [-1.0000 0.2986 -10.0000 9.9964], <br/>		gradient = 421988746.9021, step = LM<br/>Iteration:36	x = [-1.0000 0.2829 -10.0000 9.9963], <br/>		gradient = 230960695.7163, step = LM<br/>Iteration:37	x = [-1.0000 0.2672 -10.0000 9.9962], <br/>		gradient = 126427578.6262, step = LM<br/>Iteration:38	x = [-1.0000 0.2514 -10.0000 9.9961], <br/>		gradient = 69218466.4778, step = LM<br/>Iteration:39	x = [-1.0000 0.2356 -10.0000 9.9960], <br/>		gradient = 37904445.7661, step = LM<br/>Iteration:40	x = [-1.0000 0.2197 -10.0000 9.9960], <br/>		gradient = 20761560.5190, step = LM<br/>Iteration:41	x = [-1.0000 0.2036 -10.0000 9.9959], <br/>		gradient = 11374897.3379, step = LM<br/>Iteration:42	x = [-1.0000 0.1875 -10.0000 9.9958], <br/>		gradient = 6234064.0376, step = LM<br/>Iteration:43	x = [-1.0000 0.1713 -10.0000 9.9957], <br/>		gradient = 3417848.7040, step = LM<br/>Iteration:44	x = [-1.0000 0.1550 -10.0000 9.9956], <br/>		gradient = 1874634.8523, step = LM<br/>Iteration:45	x = [-1.0000 0.1385 -10.0000 9.9955], <br/>		gradient = 1028702.8782, step = LM<br/>Iteration:46	x = [-1.0000 0.1218 -10.0000 9.9954], <br/>		gradient = 564808.7854, step = LM<br/>Iteration:47	x = [-1.0000 0.1049 -10.0000 9.9953], <br/>		gradient = 310298.2705, step = LM<br/>Iteration:48	x = [-1.0000 0.0877 -10.0000 9.9952], <br/>		gradient = 170587.3249, step = LM<br/>Iteration:49	x = [-1.0000 0.0703 -10.0000 9.9951], <br/>		gradient = 93845.4326, step = LM<br/>Iteration:50	x = [-1.0000 0.0524 -10.0000 9.9950], <br/>		gradient = 51660.5782, step = LM<br/>Iteration:51	x = [-1.0000 0.0341 -10.0000 9.9949], <br/>		gradient = 28451.8639, step = LM<br/>Iteration:52	x = [-1.0000 0.0153 -10.0000 9.9947], <br/>		gradient = 15670.8490, step = LM<br/>Iteration:53	x = [-0.9999 -0.0043 -10.0000 9.9946], <br/>		gradient = 8624.8546, step = LM<br/>Iteration:54	x = [-0.9999 -0.0246 -10.0000 9.9944], <br/>		gradient = 4736.1605, step = LM<br/>Iteration:55	x = [-0.9998 -0.0461 -10.0000 9.9941], <br/>		gradient = 2587.7373, step = LM<br/>Iteration:56	x = [-0.9996 -0.0687 -10.0000 9.9939], <br/>		gradient = 1399.9325, step = LM<br/>Iteration:57	x = [-0.9993 -0.0928 -10.0000 9.9936], <br/>		gradient = 743.3551, step = LM<br/>Iteration:58	x = [-0.9987 -0.1184 -10.0001 9.9931], <br/>		gradient = 381.2687, step = LM<br/>Iteration:59	x = [-0.9976 -0.1456 -10.0001 9.9925], <br/>		gradient = 183.0379, step = LM<br/>Iteration:60	x = [-0.9952 -0.1736 -10.0002 9.9917], <br/>		gradient = 76.6425, step = LM<br/>Iteration:61	x = [-0.9894 -0.1972 -10.0005 9.9901], <br/>		gradient = 26.4382, step = LM<br/>Iteration:62	x = [-0.9744 -0.2105 -10.0012 9.9871], <br/>		gradient = 6.8472, step = LM<br/>Iteration:63	x = [-0.9336 -0.2136 -10.0030 9.9797], <br/>		gradient = 4.4702, step = LM<br/>Iteration:64	x = [-0.8269 -0.2088 -10.0070 9.9631], <br/>		gradient = 4.2573, step = LM<br/>Iteration:65	x = [-0.6379 -0.1953 -10.0103 9.9430], <br/>		gradient = 6.5882, step = LM<br/>Iteration:66	x = [-0.5576 -0.1832 -10.0023 9.9499], <br/>		gradient = 1.3240, step = LM<br/>Iteration:67	x = [-0.5559 -0.1827 -9.9933 9.9681], <br/>		gradient = 0.0227, step = LM<br/>Iteration:68	x = [-0.5544 -0.1829 -10.0000 9.9888], <br/>		gradient = 0.0087, step = LM<br/>Iteration:69	x = [-0.5527 -0.1832 -10.0389 10.0305], <br/>		gradient = 0.0117, step = LM<br/>Iteration:70	x = [-0.5482 -0.1843 -10.1523 10.1442], <br/>		gradient = 0.0548, step = LM<br/>Iteration:71	x = [-0.5372 -0.1868 -10.4402 10.4325], <br/>		gradient = 0.2795, step = LM<br/>Iteration:72	x = [-0.5173 -0.1916 -11.0244 11.0175], <br/>		gradient = 1.0133, step = LM<br/>Iteration:73	x = [-0.5025 -0.1958 -11.5877 11.5812], <br/>		gradient = 0.7806, step = LM<br/>Iteration:74	x = [-0.4883 -0.2000 -12.1891 12.1835], <br/>		gradient = 0.8407, step = LM<br/>Iteration:75	x = [-0.4787 -0.2030 -12.6707 12.6659], <br/>		gradient = 0.4784, step = LM<br/>Iteration:76	x = [-0.4704 -0.2057 -13.1195 13.1156], <br/>		gradient = 0.4040, step = LM<br/>Iteration:77	x = [-0.4656 -0.2073 -13.4055 13.4021], <br/>		gradient = 0.1502, step = LM<br/>Iteration:78	x = [-0.4629 -0.2083 -13.5758 13.5728], <br/>		gradient = 0.0525, step = LM<br/>Iteration:79	x = [-0.4622 -0.2085 -13.6183 13.6154], <br/>		gradient = 0.0030, step = LM<br/>Iteration:80	x = [-0.4622 -0.2085 -13.6218 13.6188], <br/>		gradient = 0.0000, step = LM<br/>Iteration:81	x = [-0.4622 -0.2085 -13.6218 13.6188], <br/>		gradient = 0.0000, step = LM</span><span id="4439" class="ln lo it lj b gy lt lq l lr ls">Final estimated parameters: [-0.4622 -0.2085 -13.6218 13.6188]</span></pre><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pz nx l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">视频作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="cd77" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">该算法运行81次迭代，产生参数估计结果<em class="nv"> x* </em> = [-0.46，-0.21，-13.62，13.62]，ǁ <em class="nv"> g </em> ( <em class="nv"> x* </em> )ǁ ≈ 0。换句话说，该算法在满足停止准则ǁ <em class="nv"> gₖ </em> ǁ ≤ <em class="nv"> ε </em> ₁的情况下成功收敛。还可以看出，该算法在给定数据的曲线拟合中总是使用LM方法。这意味着每连续3次迭代，就有一次迭代<em class="nv"> xₖ </em>满足ǁ<em class="nv">gₖ</em>ǁ≥0.02⋅<em class="nv">fₖ</em>。</p><h2 id="8938" class="ln lo it bd lv oa ob dn lz oc od dp md mu oe of mf my og oh mh nc oi oj mj iz bi translated">场景2</h2><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nw nx l"/></div></figure><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="1462" class="ln lo it lj b gy lp lq l lr ls">Iteration:1	x = [-4.0000 0.9500 2.0000 -2.9990], <br/>		gradient = 5843944048763837440.0000, step = LM<br/>Iteration:2	x = [-4.0000 0.9001 2.0000 -2.9981], <br/>		gradient = 798393836723220864.0000, step = LM<br/>Iteration:3	x = [-4.0000 0.8510 2.0000 -2.9973], <br/>		gradient = 112511464378970688.0000, step = LM<br/>Iteration:4	x = [-4.0000 0.8050 2.0000 -2.9965], <br/>		gradient = 17995869734728704.0000, step = LM<br/>Iteration:5	x = [-4.0000 0.7687 2.0000 -2.9959], <br/>		gradient = 4224958744295105.0000, step = LM<br/>Iteration:6	x = [-4.0000 0.7454 2.0000 -2.9955], <br/>		gradient = 1674832246409442.5000, step = LM<br/>Iteration:7	x = [-4.0000 0.7283 2.0000 -2.9952], <br/>		gradient = 847705835813273.6250, step = LM<br/>Iteration:8	x = [-4.0000 0.7128 2.0000 -2.9950], <br/>		gradient = 455980000478269.1875, step = LM<br/>Iteration:9	x = [-4.0000 0.6975 2.0000 -2.9947], <br/>		gradient = 248446618239291.2812, step = LM<br/>Iteration:10	x = [-4.0000 0.6823 2.0000 -2.9945], <br/>		gradient = 135727074063636.5469, step = LM<br/>Iteration:11	x = [-4.0000 0.6671 2.0000 -2.9942], <br/>		gradient = 74188516827416.2656, step = LM<br/>Iteration:12	x = [-4.0000 0.6519 2.0000 -2.9940], <br/>		gradient = 40556383042130.0234, step = LM<br/>Iteration:13	x = [-4.0000 0.6367 2.0000 -2.9937], <br/>		gradient = 22171597557325.9102, step = LM<br/>Iteration:14	x = [-4.0000 0.6215 2.0000 -2.9934], <br/>		gradient = 12121123175141.8398, step = LM<br/>Iteration:15	x = [-4.0000 0.6063 2.0000 -2.9932], <br/>		gradient = 6626677353862.0400, step = LM<br/>Iteration:16	x = [-4.0000 0.5910 2.0000 -2.9929], <br/>		gradient = 3622898207455.6821, step = LM<br/>Iteration:17	x = [-4.0000 0.5758 2.0000 -2.9927], <br/>		gradient = 1980725783949.0017, step = LM<br/>Iteration:18	x = [-4.0000 0.5606 2.0000 -2.9924], <br/>		gradient = 1082932002365.9087, step = LM<br/>Iteration:19	x = [-4.0000 0.5453 2.0000 -2.9922], <br/>		gradient = 592089574010.1783, step = LM<br/>Iteration:20	x = [-4.0000 0.5300 2.0000 -2.9919], <br/>		gradient = 323730717048.8542, step = LM<br/>Iteration:21	x = [-4.0000 0.5148 2.0000 -2.9916], <br/>		gradient = 177007495788.6531, step = LM<br/>Iteration:22	x = [-4.0000 0.4995 2.0000 -2.9914], <br/>		gradient = 96785827387.7305, step = LM<br/>Iteration:23	x = [-4.0000 0.4842 2.0000 -2.9911], <br/>		gradient = 52923125898.6776, step = LM<br/>Iteration:24	x = [-4.0000 0.4688 2.0000 -2.9909], <br/>		gradient = 28939714442.5817, step = LM<br/>Iteration:25	x = [-4.0000 0.4535 2.0000 -2.9906], <br/>		gradient = 15825580540.4678, step = LM<br/>Iteration:26	x = [-4.0000 0.4381 2.0000 -2.9903], <br/>		gradient = 8654531799.0783, step = LM<br/>Iteration:27	x = [-4.0000 0.4228 2.0000 -2.9901], <br/>		gradient = 4733127210.2534, step = LM<br/>Iteration:28	x = [-4.0000 0.4074 2.0000 -2.9898], <br/>		gradient = 2588665603.7885, step = LM<br/>Iteration:29	x = [-4.0000 0.3919 2.0000 -2.9895], <br/>		gradient = 1415891078.6356, step = LM<br/>Iteration:30	x = [-4.0000 0.3765 2.0000 -2.9893], <br/>		gradient = 774485568.1282, step = LM<br/>Iteration:31	x = [-4.0000 0.3610 2.0000 -2.9890], <br/>		gradient = 423672773.1613, step = LM<br/>Iteration:32	x = [-4.0000 0.3455 2.0000 -2.9887], <br/>		gradient = 231785640.8201, step = LM<br/>Iteration:33	x = [-4.0000 0.3300 2.0000 -2.9885], <br/>		gradient = 126819882.7775, step = LM<br/>Iteration:34	x = [-4.0000 0.3144 2.0000 -2.9882], <br/>		gradient = 69396974.7767, step = LM<br/>Iteration:35	x = [-4.0000 0.2988 2.0000 -2.9879], <br/>		gradient = 37980046.8186, step = LM<br/>Iteration:36	x = [-4.0000 0.2831 2.0000 -2.9876], <br/>		gradient = 20789502.5936, step = LM<br/>Iteration:37	x = [-4.0000 0.2674 2.0000 -2.9874], <br/>		gradient = 11382078.3240, step = LM<br/>Iteration:38	x = [-4.0000 0.2516 2.0000 -2.9871], <br/>		gradient = 6233151.4407, step = LM<br/>Iteration:39	x = [-4.0000 0.2358 2.0000 -2.9868], <br/>		gradient = 3414510.4438, step = LM<br/>Iteration:40	x = [-4.0000 0.2198 2.0000 -2.9865], <br/>		gradient = 1871192.9516, step = LM<br/>Iteration:41	x = [-4.0000 0.2038 2.0000 -2.9862], <br/>		gradient = 1025947.0256, step = LM<br/>Iteration:42	x = [-4.0000 0.1877 2.0000 -2.9859], <br/>		gradient = 562874.5612, step = LM<br/>Iteration:43	x = [-4.0000 0.1714 2.0000 -2.9856], <br/>		gradient = 309077.3735, step = LM<br/>Iteration:44	x = [-4.0000 0.1550 2.0000 -2.9853], <br/>		gradient = 169908.6045, step = LM<br/>Iteration:45	x = [-4.0000 0.1384 2.0000 -2.9850], <br/>		gradient = 93547.0603, step = LM<br/>Iteration:46	x = [-4.0000 0.1215 2.0000 -2.9847], <br/>		gradient = 51612.8671, step = LM<br/>Iteration:47	x = [-4.0000 0.1044 2.0000 -2.9843], <br/>		gradient = 28559.4009, step = LM<br/>Iteration:48	x = [-4.0000 0.0869 2.0000 -2.9839], <br/>		gradient = 15867.2252, step = LM<br/>Iteration:49	x = [-4.0000 0.0688 2.0000 -2.9835], <br/>		gradient = 8865.7762, step = LM<br/>Iteration:50	x = [-4.0000 0.0500 2.0000 -2.9830], <br/>		gradient = 4993.1844, step = LM<br/>Iteration:51	x = [-4.0000 0.0303 2.0000 -2.9825], <br/>		gradient = 2843.2836, step = LM<br/>Iteration:52	x = [-4.0000 0.0092 2.0000 -2.9818], <br/>		gradient = 1643.5749, step = LM<br/>Iteration:53	x = [-4.0000 -0.0138 2.0000 -2.9809], <br/>		gradient = 969.1664, step = LM<br/>Iteration:54	x = [-4.0000 -0.0397 2.0001 -2.9798], <br/>		gradient = 585.9656, step = LM<br/>Iteration:55	x = [-4.0000 -0.0695 2.0001 -2.9782], <br/>		gradient = 364.7187, step = LM<br/>Iteration:56	x = [-4.0000 -0.1047 2.0003 -2.9760], <br/>		gradient = 233.9465, step = LM<br/>Iteration:57	x = [-4.0000 -0.1465 2.0005 -2.9730], <br/>		gradient = 154.2224, step = LM<br/>Iteration:58	x = [-4.0000 -0.1954 2.0008 -2.9690], <br/>		gradient = 103.9455, step = LM<br/>Iteration:59	x = [-4.0000 -0.2516 2.0014 -2.9638], <br/>		gradient = 71.2219, step = LM<br/>Iteration:60	x = [-4.0000 -0.3152 2.0023 -2.9571], <br/>		gradient = 49.3390, step = LM<br/>Iteration:61	x = [-4.0000 -0.3865 2.0037 -2.9486], <br/>		gradient = 34.3994, step = LM<br/>Iteration:62	x = [-4.0000 -0.4659 2.0060 -2.9378], <br/>		gradient = 24.0583, step = LM<br/>Iteration:63	x = [-3.9999 -0.5540 2.0095 -2.9242], <br/>		gradient = 16.8404, step = LM<br/>Iteration:64	x = [-3.9999 -0.6510 2.0148 -2.9069], <br/>		gradient = 11.7791, step = LM<br/>Iteration:65	x = [-3.9998 -0.7574 2.0228 -2.8848], <br/>		gradient = 8.2216, step = LM<br/>Iteration:66	x = [-3.9998 -0.8733 2.0348 -2.8567], <br/>		gradient = 5.7186, step = LM<br/>Iteration:67	x = [-3.9996 -0.9990 2.0522 -2.8209], <br/>		gradient = 3.9571, step = LM<br/>Iteration:68	x = [-3.9995 -1.1349 2.0768 -2.7754], <br/>		gradient = 2.7180, step = LM<br/>Iteration:69	x = [-3.9992 -1.2813 2.1103 -2.7183], <br/>		gradient = 1.8476, step = LM<br/>Iteration:70	x = [-3.9987 -1.4390 2.1533 -2.6485], <br/>		gradient = 1.2392, step = LM<br/>Iteration:71	x = [-3.9980 -1.6079 2.2038 -2.5681], <br/>		gradient = 0.8190, step = LM<br/>Iteration:72	x = [-3.9968 -1.7865 2.2547 -2.4842], <br/>		gradient = 0.5357, step = LM<br/>Iteration:73	x = [-3.9949 -1.9699 2.2945 -2.4089], <br/>		gradient = 0.3513, step = LM<br/>Iteration:74	x = [-3.9920 -2.1517 2.3148 -2.3524], <br/>		gradient = 0.2345, step = LM<br/>Iteration:75	x = [-3.9878 -2.3283 2.3156 -2.3155], <br/>		gradient = 0.1603, step = LM<br/>Iteration:76	x = [-3.9863 -2.3635 2.3124 -2.3081], <br/>		gradient = 0.1487, step = QN<br/>Iteration:77	x = [-3.9803 -2.4682 2.2972 -2.2863], <br/>		gradient = 0.1189, step = QN<br/>Iteration:78	x = [-3.9577 -2.7803 2.2395 -2.2211], <br/>		gradient = 0.0613, step = QN<br/>Iteration:79	x = [-3.9319 -3.0541 2.1785 -2.1639], <br/>		gradient = 0.0344, step = QN<br/>Iteration:80	x = [-3.9000 -3.3271 2.1135 -2.1068], <br/>		gradient = 0.0193, step = QN<br/>Iteration:81	x = [-3.8784 -3.4807 2.0776 -2.0748], <br/>		gradient = 0.0140, step = QN<br/>Iteration:82	x = [-3.8657 -3.5574 2.0604 -2.0588], <br/>		gradient = 0.0119, step = QN<br/>Iteration:83	x = [-3.8510 -3.6338 2.0437 -2.0428], <br/>		gradient = 0.0101, step = QN<br/>Iteration:84	x = [-3.8335 -3.7098 2.0276 -2.0269], <br/>		gradient = 0.0086, step = QN<br/>Iteration:85	x = [-3.8227 -3.7473 2.0198 -2.0191], <br/>		gradient = 0.0080, step = QN<br/>Iteration:86	x = [-3.8090 -3.7840 2.0123 -2.0114], <br/>		gradient = 0.0074, step = QN<br/>Iteration:87	x = [-3.7911 -3.8190 2.0053 -2.0041], <br/>		gradient = 0.0073, step = QN<br/>Iteration:88	x = [-3.7664 -3.8489 1.9996 -1.9979], <br/>		gradient = 0.0076, step = QN<br/>Iteration:89	x = [-3.7581 -3.8559 1.9998 -1.9981], <br/>		gradient = 0.0077, step = LM<br/>Iteration:90	x = [-3.7330 -3.8766 2.0004 -1.9986], <br/>		gradient = 0.0081, step = LM<br/>Iteration:91	x = [-3.6535 -3.9361 2.0028 -2.0011], <br/>		gradient = 0.0096, step = LM<br/>Iteration:92	x = [-3.3739 -4.0948 2.0180 -2.0161], <br/>		gradient = 0.0168, step = LM<br/>Iteration:93	x = [-1.8970 -4.4431 2.1579 -2.1553], <br/>		gradient = 0.3573, step = LM<br/>Iteration:94	x = [-1.8970 -4.4431 2.1579 -2.1553], <br/>		gradient = 0.3573, step = LM<br/>Iteration:95	x = [-1.8970 -4.4431 2.1579 -2.1553], <br/>		gradient = 0.3573, step = LM<br/>Iteration:96	x = [-1.8970 -4.4431 2.1579 -2.1553], <br/>		gradient = 0.3573, step = LM<br/>Iteration:97	x = [-0.4821 -4.4514 2.3330 -2.2979], <br/>		gradient = 12.9667, step = LM<br/>Iteration:98	x = [-0.4821 -4.4514 2.3330 -2.2979], <br/>		gradient = 12.9667, step = LM<br/>Iteration:99	x = [-0.4821 -4.4514 2.3330 -2.2979], <br/>		gradient = 12.9667, step = LM<br/>Iteration:100	x = [-0.4821 -4.4514 2.3330 -2.2979], <br/>		gradient = 12.9667, step = LM<br/>Iteration:101	x = [-0.4821 -4.4514 2.3330 -2.2979], <br/>		gradient = 12.9667, step = LM<br/>Iteration:102	x = [-0.3361 -4.4514 2.3517 -2.2985], <br/>		gradient = 23.2634, step = LM<br/>Iteration:103	x = [-0.3361 -4.4514 2.3517 -2.2985], <br/>		gradient = 23.2634, step = LM<br/>Iteration:104	x = [-0.0195 -4.4514 2.3844 -2.3000], <br/>		gradient = 141.8487, step = LM<br/>Iteration:105	x = [-0.0542 -4.4514 2.5491 -2.3129], <br/>		gradient = 2.7825, step = LM<br/>Iteration:106	x = [-0.0622 -4.4515 2.7408 -2.3468], <br/>		gradient = 3.8567, step = LM<br/>Iteration:107	x = [-0.0733 -4.4516 3.1240 -2.5063], <br/>		gradient = 5.6536, step = LM<br/>Iteration:108	x = [-0.0887 -4.4519 3.7063 -3.0302], <br/>		gradient = 7.1668, step = LM<br/>Iteration:109	x = [-0.1056 -4.4518 4.4259 -4.0061], <br/>		gradient = 6.3086, step = LM<br/>Iteration:110	x = [-0.1190 -4.4484 5.0608 -4.9286], <br/>		gradient = 3.4276, step = LM<br/>Iteration:111	x = [-0.1236 -4.4311 5.3256 -5.3076], <br/>		gradient = 0.7579, step = LM<br/>Iteration:112	x = [-0.1239 -4.3717 5.3572 -5.3565], <br/>		gradient = 0.0396, step = LM<br/>Iteration:113	x = [-0.1239 -4.1687 5.3583 -5.3584], <br/>		gradient = 0.0016, step = LM<br/>Iteration:114	x = [-0.1240 -3.2559 5.3610 -5.3611], <br/>		gradient = 0.0427, step = LM<br/>Iteration:115	x = [-0.1240 -3.2559 5.3610 -5.3611], <br/>		gradient = 0.0427, step = LM<br/>Iteration:116	x = [-0.1240 -3.2559 5.3610 -5.3611], <br/>		gradient = 0.0427, step = LM<br/>Iteration:117	x = [-0.1248 -1.1768 5.4018 -5.4026], <br/>		gradient = 4.7877, step = LM<br/>Iteration:118	x = [-0.1248 -1.1768 5.4018 -5.4026], <br/>		gradient = 4.7877, step = LM<br/>Iteration:119	x = [-0.1248 -1.1768 5.4018 -5.4026], <br/>		gradient = 4.7877, step = LM<br/>Iteration:120	x = [-0.1248 -1.1768 5.4018 -5.4026], <br/>		gradient = 4.7877, step = LM<br/>Iteration:121	x = [-0.1248 -1.1768 5.4018 -5.4026], <br/>		gradient = 4.7877, step = LM<br/>Iteration:122	x = [-0.1244 -1.1290 5.5183 -5.4517], <br/>		gradient = 0.1904, step = LM<br/>Iteration:123	x = [-0.1292 -1.0057 5.7614 -5.6610], <br/>		gradient = 1.1816, step = LM<br/>Iteration:124	x = [-0.1382 -0.8383 6.2791 -6.1944], <br/>		gradient = 3.1482, step = LM<br/>Iteration:125	x = [-0.1464 -0.7806 6.8138 -6.7630], <br/>		gradient = 1.7846, step = LM<br/>Iteration:126	x = [-0.1599 -0.6524 7.7451 -7.7208], <br/>		gradient = 5.7805, step = LM<br/>Iteration:127	x = [-0.1658 -0.6428 8.2978 -8.2944], <br/>		gradient = 0.8161, step = LM<br/>Iteration:128	x = [-0.1770 -0.5710 9.2787 -9.2788], <br/>		gradient = 4.5341, step = LM<br/>Iteration:129	x = [-0.1818 -0.5588 9.8477 -9.8546], <br/>		gradient = 0.7032, step = LM<br/>Iteration:130	x = [-0.1902 -0.5204 10.7897 -10.7955], <br/>		gradient = 2.9184, step = LM<br/>Iteration:131	x = [-0.1941 -0.5090 11.3439 -11.3510], <br/>		gradient = 0.6201, step = LM<br/>Iteration:132	x = [-0.1999 -0.4878 12.1563 -12.1617], <br/>		gradient = 1.6519, step = LM<br/>Iteration:133	x = [-0.2027 -0.4799 12.6128 -12.6178], <br/>		gradient = 0.3816, step = LM<br/>Iteration:134	x = [-0.2059 -0.4697 13.1434 -13.1472], <br/>		gradient = 0.5800, step = LM<br/>Iteration:135	x = [-0.2073 -0.4656 13.4035 -13.4068], <br/>		gradient = 0.1173, step = LM<br/>Iteration:136	x = [-0.2082 -0.4629 13.5699 -13.5729], <br/>		gradient = 0.0504, step = LM<br/>Iteration:137	x = [-0.2085 -0.4622 13.6148 -13.6177], <br/>		gradient = 0.0034, step = LM<br/>Iteration:138	x = [-0.2085 -0.4622 13.6188 -13.6218], <br/>		gradient = 0.0000, step = LM<br/>Iteration:139	x = [-0.2085 -0.4622 13.6188 -13.6218], <br/>		gradient = 0.0000, step = LM</span><span id="f882" class="ln lo it lj b gy lt lq l lr ls">Final estimated parameters: [-0.2085 -0.4622 13.6188 -13.6218]</span></pre><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="qa nx l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">视频作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="65a2" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">该算法运行多达139次迭代，产生参数估计结果<em class="nv"> x* </em> = [-0.21，-0.46，13.62，-13.62]，ǁ <em class="nv"> g </em> ( <em class="nv"> x* </em> )ǁ ≈ 0。换句话说，该算法在满足停止准则ǁ <em class="nv"> gₖ </em> ǁ ≤ <em class="nv"> ε </em> ₁的情况下成功收敛。还可以看出，在给定数据的曲线拟合过程中，算法在第76–88次迭代时将步长方法改为QN，然后再改回LM。这意味着在迭代76之前的最后3次迭代中，没有迭代<em class="nv"> xₖ </em>满足ǁ<em class="nv">gₖ</em>ǁ≥0.02⋅<em class="nv">fₖ</em>从而QN方法被认为在迭代76之后能够比LM方法更快地收敛。然后，在迭代88，我们得到ǁ<em class="nv">gₖ₊₁</em>ǁ≥ǁ<em class="nv">gₖ</em>ǁ，这意味着使用QN方法梯度下降得不够快，使得该方法切换到LM用于下一次迭代。</p><p id="51c2" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">因为初始值<em class="nv"> x </em> ₀离解很远，所以需要比场景1更多的迭代。还有，注意到目前为止满足收敛值<em class="nv"> x* </em>的解有两个，分别是<em class="nv"> x* </em> = [-0.46，-0.21，-13.62，13.62]和<em class="nv"> x* </em> = [-0.21，-0.46，13.62，-13.62]。</p><h2 id="b973" class="ln lo it bd lv oa ob dn lz oc od dp md mu oe of mf my og oh mh nc oi oj mj iz bi translated">场景3</h2><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nw nx l"/></div></figure><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="d8b7" class="ln lo it lj b gy lp lq l lr ls">Iteration:1	x = [0.0000 0.0000 0.7705 0.7705], <br/>		gradient = 42.5292, step = LM<br/>Iteration:2	x = [-0.0815 -0.0815 1.3987 1.3987], <br/>		gradient = 16.7516, step = LM<br/>Iteration:3	x = [-0.0400 -0.0400 1.2150 1.2150], <br/>		gradient = 18.3468, step = LM<br/>Iteration:4	x = [-0.0607 -0.0695 1.3851 1.3851], <br/>		gradient = 4.8184, step = LM<br/>Iteration:5	x = [-0.0607 -0.0695 1.3851 1.3851], <br/>		gradient = 4.8184, step = LM<br/>Iteration:6	x = [-0.0607 -0.0695 1.3851 1.3851], <br/>		gradient = 4.8184, step = LM<br/>Iteration:7	x = [-0.0607 -0.0695 1.3851 1.3851], <br/>		gradient = 4.8184, step = LM<br/>Iteration:8	x = [-0.0607 -0.0695 1.3851 1.3851], <br/>		gradient = 4.8184, step = LM<br/>Iteration:9	x = [-0.0607 -0.0695 1.3851 1.3851], <br/>		gradient = 4.8184, step = LM<br/>Iteration:10	x = [-0.0631 -0.0584 1.3830 1.3830], <br/>		gradient = 1.3008, step = LM<br/>Iteration:11	x = [-0.0596 -0.0627 1.3811 1.3811], <br/>		gradient = 0.4178, step = LM<br/>Iteration:12	x = [-0.0620 -0.0601 1.3793 1.3792], <br/>		gradient = 0.2770, step = LM<br/>Iteration:13	x = [-0.0604 -0.0616 1.3775 1.3775], <br/>		gradient = 0.2522, step = LM<br/>Iteration:14	x = [-0.0613 -0.0605 1.3758 1.3757], <br/>		gradient = 0.2432, step = LM<br/>Iteration:15	x = [-0.0604 -0.0612 1.3737 1.3737], <br/>		gradient = 0.2332, step = LM<br/>Iteration:16	x = [-0.0615 -0.0597 1.3708 1.3708], <br/>		gradient = 0.2541, step = LM<br/>Iteration:17	x = [-0.0582 -0.0627 1.3678 1.3677], <br/>		gradient = 0.6396, step = LM<br/>Iteration:18	x = [-0.0582 -0.0627 1.3678 1.3677], <br/>		gradient = 0.6396, step = LM<br/>Iteration:19	x = [-0.0607 -0.0600 1.3668 1.3668], <br/>		gradient = 0.1995, step = LM<br/>Iteration:20	x = [-0.0603 -0.0604 1.3658 1.3658], <br/>		gradient = 0.1930, step = LM<br/>Iteration:21	x = [-0.0603 -0.0601 1.3639 1.3638], <br/>		gradient = 0.1840, step = LM<br/>Iteration:22	x = [-0.0593 -0.0605 1.3587 1.3586], <br/>		gradient = 0.2076, step = LM<br/>Iteration:23	x = [-0.0593 -0.0605 1.3587 1.3586], <br/>		gradient = 0.2076, step = LM<br/>Iteration:24	x = [-0.0593 -0.0605 1.3587 1.3586], <br/>		gradient = 0.2076, step = LM<br/>Iteration:25	x = [-0.0608 -0.0590 1.3571 1.3570], <br/>		gradient = 0.2311, step = LM<br/>Iteration:26	x = [-0.0584 -0.0611 1.3556 1.3555], <br/>		gradient = 0.3672, step = LM<br/>Iteration:27	x = [-0.0609 -0.0586 1.3545 1.3544], <br/>		gradient = 0.3171, step = LM<br/>Iteration:28	x = [-0.0590 -0.0603 1.3536 1.3536], <br/>		gradient = 0.1788, step = LM<br/>Iteration:29	x = [-0.0600 -0.0593 1.3528 1.3527], <br/>		gradient = 0.1280, step = LM<br/>Iteration:30	x = [-0.0594 -0.0598 1.3519 1.3519], <br/>		gradient = 0.1237, step = LM<br/>Iteration:31	x = [-0.0597 -0.0594 1.3510 1.3510], <br/>		gradient = 0.1192, step = LM<br/>Iteration:32	x = [-0.0597 -0.0594 1.3508 1.3510], <br/>		gradient = 0.1183, step = QN<br/>Iteration:33	x = [-0.0597 -0.0594 1.3500 1.3510], <br/>		gradient = 0.1155, step = QN<br/>Iteration:34	x = [-0.0596 -0.0594 1.3475 1.3510], <br/>		gradient = 0.1084, step = QN<br/>Iteration:35	x = [-0.0592 -0.0594 1.3401 1.3510], <br/>		gradient = 0.1055, step = QN<br/>Iteration:36	x = [-0.0589 -0.0594 1.3328 1.3510], <br/>		gradient = 0.2028, step = QN<br/>Iteration:37	x = [-0.0595 -0.0585 1.3319 1.3501], <br/>		gradient = 0.1538, step = LM<br/>Iteration:38	x = [-0.0595 -0.0585 1.3319 1.3501], <br/>		gradient = 0.1538, step = LM<br/>Iteration:39	x = [-0.0587 -0.0592 1.3314 1.3496], <br/>		gradient = 0.0704, step = LM<br/>Iteration:40	x = [-0.0591 -0.0588 1.3310 1.3492], <br/>		gradient = 0.0640, step = LM<br/>Iteration:41	x = [-0.0588 -0.0590 1.3306 1.3488], <br/>		gradient = 0.0618, step = LM<br/>Iteration:42	x = [-0.0588 -0.0590 1.3304 1.3488], <br/>		gradient = 0.0615, step = QN<br/>Iteration:43	x = [-0.0588 -0.0590 1.3300 1.3488], <br/>		gradient = 0.0610, step = QN<br/>Iteration:44	x = [-0.0587 -0.0590 1.3288 1.3488], <br/>		gradient = 0.0582, step = QN<br/>Iteration:45	x = [-0.0586 -0.0590 1.3275 1.3488], <br/>		gradient = 0.0571, step = QN<br/>Iteration:46	x = [-0.0586 -0.0590 1.3263 1.3488], <br/>		gradient = 0.0554, step = QN<br/>Iteration:47	x = [-0.0585 -0.0590 1.3251 1.3488], <br/>		gradient = 0.0646, step = QN<br/>Iteration:48	x = [-0.0590 -0.0585 1.3247 1.3484], <br/>		gradient = 0.0576, step = LM<br/>Iteration:49	x = [-0.0585 -0.0589 1.3243 1.3481], <br/>		gradient = 0.0494, step = LM<br/>Iteration:50	x = [-0.0589 -0.0585 1.3240 1.3477], <br/>		gradient = 0.0455, step = LM<br/>Iteration:51	x = [-0.0585 -0.0588 1.3237 1.3474], <br/>		gradient = 0.0409, step = LM<br/>Iteration:52	x = [-0.0588 -0.0585 1.3233 1.3471], <br/>		gradient = 0.0393, step = LM<br/>Iteration:53	x = [-0.0585 -0.0588 1.3230 1.3468], <br/>		gradient = 0.0377, step = LM<br/>Iteration:54	x = [-0.0588 -0.0585 1.3228 1.3465], <br/>		gradient = 0.0363, step = LM<br/>Iteration:55	x = [-0.0585 -0.0587 1.3225 1.3462], <br/>		gradient = 0.0348, step = LM<br/>Iteration:56	x = [-0.0587 -0.0585 1.3222 1.3459], <br/>		gradient = 0.0335, step = LM<br/>Iteration:57	x = [-0.0585 -0.0587 1.3219 1.3457], <br/>		gradient = 0.0321, step = LM<br/>Iteration:58	x = [-0.0587 -0.0585 1.3217 1.3454], <br/>		gradient = 0.0307, step = LM<br/>Iteration:59	x = [-0.0584 -0.0587 1.3214 1.3452], <br/>		gradient = 0.0294, step = LM<br/>Iteration:60	x = [-0.0587 -0.0584 1.3212 1.3449], <br/>		gradient = 0.0319, step = LM<br/>Iteration:61	x = [-0.0584 -0.0587 1.3209 1.3447], <br/>		gradient = 0.0330, step = LM<br/>Iteration:62	x = [-0.0587 -0.0584 1.3207 1.3445], <br/>		gradient = 0.0357, step = LM<br/>Iteration:63	x = [-0.0584 -0.0587 1.3205 1.3442], <br/>		gradient = 0.0362, step = LM<br/>Iteration:64	x = [-0.0586 -0.0584 1.3203 1.3440], <br/>		gradient = 0.0363, step = LM<br/>Iteration:65	x = [-0.0584 -0.0586 1.3201 1.3439], <br/>		gradient = 0.0321, step = LM<br/>Iteration:66	x = [-0.0586 -0.0584 1.3199 1.3437], <br/>		gradient = 0.0281, step = LM<br/>Iteration:67	x = [-0.0584 -0.0586 1.3198 1.3435], <br/>		gradient = 0.0228, step = LM<br/>Iteration:68	x = [-0.0585 -0.0584 1.3196 1.3434], <br/>		gradient = 0.0203, step = LM<br/>Iteration:69	x = [-0.0584 -0.0585 1.3195 1.3432], <br/>		gradient = 0.0195, step = LM<br/>Iteration:70	x = [-0.0585 -0.0584 1.3193 1.3431], <br/>		gradient = 0.0188, step = LM<br/>Iteration:71	x = [-0.0584 -0.0585 1.3192 1.3429], <br/>		gradient = 0.0181, step = LM<br/>Iteration:72	x = [-0.0585 -0.0584 1.3190 1.3428], <br/>		gradient = 0.0174, step = LM<br/>Iteration:73	x = [-0.0584 -0.0585 1.3189 1.3426], <br/>		gradient = 0.0166, step = LM<br/>Iteration:74	x = [-0.0585 -0.0583 1.3187 1.3425], <br/>		gradient = 0.0175, step = LM<br/>Iteration:75	x = [-0.0583 -0.0585 1.3186 1.3423], <br/>		gradient = 0.0212, step = LM<br/>Iteration:76	x = [-0.0585 -0.0583 1.3185 1.3422], <br/>		gradient = 0.0264, step = LM<br/>Iteration:77	x = [-0.0583 -0.0585 1.3183 1.3421], <br/>		gradient = 0.0264, step = LM<br/>Iteration:78	x = [-0.0585 -0.0583 1.3182 1.3420], <br/>		gradient = 0.0201, step = LM<br/>Iteration:79	x = [-0.0583 -0.0584 1.3182 1.3419], <br/>		gradient = 0.0130, step = LM<br/>Iteration:80	x = [-0.0584 -0.0583 1.3181 1.3418], <br/>		gradient = 0.0124, step = LM<br/>Iteration:81	x = [-0.0584 -0.0584 1.3180 1.3417], <br/>		gradient = 0.0120, step = LM<br/>Iteration:82	x = [-0.0584 -0.0583 1.3179 1.3416], <br/>		gradient = 0.0115, step = LM<br/>Iteration:83	x = [-0.0583 -0.0584 1.3178 1.3415], <br/>		gradient = 0.0110, step = LM<br/>Iteration:84	x = [-0.0584 -0.0583 1.3176 1.3414], <br/>		gradient = 0.0186, step = LM<br/>Iteration:85	x = [-0.0584 -0.0583 1.3176 1.3414], <br/>		gradient = 0.0186, step = LM<br/>Iteration:86	x = [-0.0583 -0.0584 1.3176 1.3413], <br/>		gradient = 0.0112, step = LM<br/>Iteration:87	x = [-0.0584 -0.0583 1.3175 1.3412], <br/>		gradient = 0.0095, step = LM<br/>Iteration:88	x = [-0.0583 -0.0584 1.3174 1.3412], <br/>		gradient = 0.0092, step = LM<br/>Iteration:89	x = [-0.0583 -0.0584 1.3174 1.3412], <br/>		gradient = 0.0090, step = QN<br/>Iteration:90	x = [-0.0583 -0.0584 1.3173 1.3412], <br/>		gradient = 0.0260, step = QN<br/>Iteration:91	x = [-0.0584 -0.0583 1.3173 1.3411], <br/>		gradient = 0.0085, step = LM<br/>Iteration:92	x = [-0.0583 -0.0584 1.3172 1.3411], <br/>		gradient = 0.0083, step = LM<br/>Iteration:93	x = [-0.0583 -0.0583 1.3171 1.3410], <br/>		gradient = 0.0080, step = LM<br/>Iteration:94	x = [-0.0583 -0.0583 1.3171 1.3409], <br/>		gradient = 0.0077, step = LM<br/>Iteration:95	x = [-0.0584 -0.0583 1.3170 1.3409], <br/>		gradient = 0.0075, step = LM<br/>Iteration:96	x = [-0.0583 -0.0584 1.3169 1.3408], <br/>		gradient = 0.0110, step = LM<br/>Iteration:97	x = [-0.0584 -0.0582 1.3169 1.3407], <br/>		gradient = 0.0165, step = LM<br/>Iteration:98	x = [-0.0583 -0.0584 1.3168 1.3407], <br/>		gradient = 0.0109, step = LM<br/>Iteration:99	x = [-0.0583 -0.0583 1.3168 1.3406], <br/>		gradient = 0.0062, step = LM<br/>Iteration:100	x = [-0.0583 -0.0583 1.3167 1.3406], <br/>		gradient = 0.0059, step = LM<br/>Iteration:101	x = [-0.0583 -0.0583 1.3167 1.3406], <br/>		gradient = 0.0057, step = LM<br/>Iteration:102	x = [-0.0583 -0.0583 1.3167 1.3405], <br/>		gradient = 0.0055, step = LM<br/>Iteration:103	x = [-0.0583 -0.0583 1.3166 1.3404], <br/>		gradient = 0.0075, step = LM<br/>Iteration:104	x = [-0.0583 -0.0583 1.3166 1.3404], <br/>		gradient = 0.0075, step = LM<br/>Iteration:105	x = [-0.0583 -0.0583 1.3165 1.3404], <br/>		gradient = 0.0090, step = LM<br/>Iteration:106	x = [-0.0583 -0.0583 1.3165 1.3403], <br/>		gradient = 0.0099, step = LM<br/>Iteration:107	x = [-0.0583 -0.0583 1.3164 1.3403], <br/>		gradient = 0.0064, step = LM<br/>Iteration:108	x = [-0.0583 -0.0583 1.3164 1.3403], <br/>		gradient = 0.0064, step = QN<br/>Iteration:109	x = [-0.0583 -0.0583 1.3164 1.3403], <br/>		gradient = 0.0043, step = LM<br/>Iteration:110	x = [-0.0583 -0.0583 1.3164 1.3402], <br/>		gradient = 0.0041, step = LM<br/>Iteration:111	x = [-0.0583 -0.0583 1.3164 1.3402], <br/>		gradient = 0.0040, step = LM<br/>Iteration:112	x = [-0.0583 -0.0583 1.3163 1.3402], <br/>		gradient = 0.0038, step = LM<br/>Iteration:113	x = [-0.0583 -0.0583 1.3163 1.3401], <br/>		gradient = 0.0050, step = LM<br/>Iteration:114	x = [-0.0583 -0.0583 1.3163 1.3401], <br/>		gradient = 0.0050, step = LM<br/>Iteration:115	x = [-0.0583 -0.0583 1.3162 1.3401], <br/>		gradient = 0.0045, step = LM<br/>Iteration:116	x = [-0.0583 -0.0583 1.3162 1.3401], <br/>		gradient = 0.0044, step = LM<br/>Iteration:117	x = [-0.0583 -0.0583 1.3162 1.3400], <br/>		gradient = 0.0040, step = LM<br/>Iteration:118	x = [-0.0583 -0.0583 1.3162 1.3400], <br/>		gradient = 0.0040, step = QN<br/>Iteration:119	x = [-0.0583 -0.0583 1.3162 1.3400], <br/>		gradient = 0.0037, step = LM<br/>Iteration:120	x = [-0.0583 -0.0583 1.3161 1.3400], <br/>		gradient = 0.0032, step = LM<br/>Iteration:121	x = [-0.0583 -0.0583 1.3161 1.3400], <br/>		gradient = 0.0030, step = LM<br/>Iteration:122	x = [-0.0583 -0.0583 1.3161 1.3400], <br/>		gradient = 0.0027, step = LM<br/>Iteration:123	x = [-0.0583 -0.0583 1.3161 1.3399], <br/>		gradient = 0.0026, step = LM<br/>Iteration:124	x = [-0.0583 -0.0583 1.3161 1.3399], <br/>		gradient = 0.0025, step = LM<br/>Iteration:125	x = [-0.0583 -0.0583 1.3160 1.3399], <br/>		gradient = 0.0024, step = LM<br/>Iteration:126	x = [-0.0583 -0.0583 1.3160 1.3399], <br/>		gradient = 0.0023, step = LM<br/>Iteration:127	x = [-0.0583 -0.0583 1.3160 1.3399], <br/>		gradient = 0.0022, step = LM<br/>Iteration:128	x = [-0.0583 -0.0583 1.3160 1.3398], <br/>		gradient = 0.0021, step = LM<br/>Iteration:129	x = [-0.0583 -0.0583 1.3160 1.3398], <br/>		gradient = 0.0020, step = LM<br/>Iteration:130	x = [-0.0583 -0.0583 1.3160 1.3398], <br/>		gradient = 0.0020, step = LM<br/>Iteration:131	x = [-0.0583 -0.0583 1.3159 1.3398], <br/>		gradient = 0.0022, step = LM<br/>Iteration:132	x = [-0.0583 -0.0583 1.3159 1.3398], <br/>		gradient = 0.0024, step = LM<br/>Iteration:133	x = [-0.0583 -0.0583 1.3159 1.3398], <br/>		gradient = 0.0026, step = LM<br/>Iteration:134	x = [-0.0583 -0.0583 1.3159 1.3397], <br/>		gradient = 0.0025, step = LM<br/>Iteration:135	x = [-0.0583 -0.0583 1.3159 1.3397], <br/>		gradient = 0.0023, step = LM<br/>Iteration:136	x = [-0.0583 -0.0583 1.3159 1.3397], <br/>		gradient = 0.0018, step = LM<br/>Iteration:137	x = [-0.0583 -0.0583 1.3159 1.3397], <br/>		gradient = 0.0015, step = LM<br/>Iteration:138	x = [-0.0583 -0.0583 1.3158 1.3397], <br/>		gradient = 0.0014, step = LM<br/>Iteration:139	x = [-0.0583 -0.0583 1.3158 1.3397], <br/>		gradient = 0.0014, step = LM</span><span id="294a" class="ln lo it lj b gy lt lq l lr ls">Final estimated parameters: [-0.0583 -0.0583 1.3158 1.3397]</span></pre><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="pz nx l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">视频作者<a class="ae lh" href="http://dwiuzila.medium.com/membership" rel="noopener">作者</a></p></figure><p id="7887" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">该算法运行139次迭代，产生参数估计结果<em class="nv"> x* </em> = [-0.06，-0.06，1.32，1.34]，ǁ<em class="nv">gₖ</em>ǁ≈0.0014&gt;T4】ε₁.换句话说，算法没有成功收敛，但是迭代已经终止，因为满足了停止准则ǁ<em class="nv">hₖ</em>ǁ≤<em class="nv">ε</em>₂(ǁ<em class="nv">xₖ</em>ǁ+<em class="nv">ε</em>₂)。还可以看出，在给定数据的曲线拟合过程中，由于情景2中描述的原因，算法反复将步长方法更改为QN，反之亦然。从上图来看，显然，找到的解决方案<em class="nv"> x* </em>不够好。</p><p id="5984" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">通过在迭代过程中查看<em class="nv"> x </em>的值，观察到<em class="nv"> x </em> ₁ ≈ <em class="nv"> x </em> ₂和<em class="nv"> x </em> ₃ ≈ <em class="nv"> x </em> ₄.因此，我们可以得出结论，选择初始值<em class="nv"> x </em> ₀ = [0，0，0，0]是一个糟糕的选择，因为该值是对称的，并且该算法不能破坏对称性。你可以试着看看同样的事情发生在选择其他初始对称值上，比如<em class="nv"> x </em> ₀ = [-1，-1，-1]，<em class="nv"> x </em> ₀ = [1，1，1，1]，或者<em class="nv"> x </em> ₀ = [2，2，2，2]。哦，这只是一个巧合。你怎么想呢?</p><p id="34f1" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">从实验中你还发现了什么有趣的东西？</p><h1 id="440b" class="lu lo it bd lv lw lx ly lz ma mb mc md ki me kj mf kl mg km mh ko mi kp mj mk bi translated">结论</h1><p id="d615" class="pw-post-body-paragraph ml mm it mn b mo mp kd mq mr ms kg mt mu mv mw mx my mz na nb nc nd ne nf ng im bi translated">我们开发了一种有效的算法来解决非线性最小二乘问题。该算法结合了Levenberg-Marquardt和拟牛顿法，利用信赖域进行步长选择。该算法能以超线性收敛速度准确找到模型的最佳参数。但是，不良的参数初始化可能会导致不好的结果。在我们的研究案例中，该算法找到了符合数据的以下两个模型:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qb"><img src="../Images/3690cd68c682a182b38d982242421bdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*aSWTZv0XMZMH209L1RnJYg.png"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi qc"><img src="../Images/af6f5d74273987248902910a3c1d051c.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*WqUtBTM12A4q68hyASupvQ.png"/></div></figure><p id="5b35" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">当然，通过重新排列参数<em class="nv"> x </em>的组件，这两个模型是等价的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qd"><img src="../Images/e1a6e3674ab93bcb99796285f9d0175c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*6HsoGpmIb1oibJc_JWbqJA.gif"/></div></div></figure></div><div class="ab cl qe qf hx qg" role="separator"><span class="qh bw bk qi qj qk"/><span class="qh bw bk qi qj qk"/><span class="qh bw bk qi qj"/></div><div class="im in io ip iq"><p id="4859" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">🔥你好！如果你喜欢这个故事，想支持我这个作家，可以考虑 <a class="ae lh" href="https://dwiuzila.medium.com/membership" rel="noopener"> <strong class="mn jd"> <em class="nv">成为会员</em> </strong> </a> <em class="nv">。每月只需5美元，你就可以无限制地阅读媒体上的所有报道。如果你注册使用我的链接，我会赚一小笔佣金。</em></p><p id="bd58" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">🔖<em class="nv">想了解更多关于经典机器学习模型的工作原理，以及它们如何优化参数？或者MLOps大型项目的例子？有史以来最优秀的文章呢？继续阅读:</em></p><div class="ql qm gp gr qn"><div role="button" tabindex="0" class="ab bv gv cb fp qo qp bn qq lb ex"><div class="qr l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw qs qt fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l qs qt fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----262801bcfe25--------------------------------" rel="noopener follow" target="_top">艾伯斯·乌兹拉</a></p></div></div><div class="qw qx gw l"><h2 class="bd jd wc oy fp wd fr fs we fu fw jc bi translated">从零开始的机器学习</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wf au wg wh wi sr wj an eh ei wk wl wm el em eo de bk ep" href="https://dwiuzila.medium.com/list/machine-learning-from-scratch-b35db8650093?source=post_page-----262801bcfe25--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wn l fo"><span class="bd b dl z dk">8 stories</span></div></div></div><div class="rj dh rk fp ab rl fo di"><div class="di rb bv rc rd"><div class="dh l"><img alt="" class="dh" src="../Images/4b97f3062e4883b24589972b2dc45d7e.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*CWNoicci28F2TUQc-vKijw.png"/></div></div><div class="di rb bv re rf rg"><div class="dh l"><img alt="" class="dh" src="../Images/b1f7021514ba57a443fe0db4b7001b26.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*wSRsSHYnIiGJFAqC"/></div></div><div class="di bv rh ri rg"><div class="dh l"><img alt="" class="dh" src="../Images/deb73e42c79667024a46c2c8902b81fa.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*HVEz7KwzO0tv1Q4d"/></div></div></div></div></div><div class="ql qm gp gr qn"><div role="button" tabindex="0" class="ab bv gv cb fp qo qp bn qq lb ex"><div class="qr l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw qs qt fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l qs qt fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----262801bcfe25--------------------------------" rel="noopener follow" target="_top">艾伯斯·乌兹拉</a></p></div></div><div class="qw qx gw l"><h2 class="bd jd wc oy fp wd fr fs we fu fw jc bi translated">高级优化方法</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wf au wg wh wi sr wj an eh ei wk wl wm el em eo de bk ep" href="https://dwiuzila.medium.com/list/advanced-optimization-methods-26e264a361e4?source=post_page-----262801bcfe25--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wn l fo"><span class="bd b dl z dk">7 stories</span></div></div></div><div class="rj dh rk fp ab rl fo di"><div class="di rb bv rc rd"><div class="dh l"><img alt="" class="dh" src="../Images/15b3188b0f29894c2bcf3d0965515f44.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*BVMamoNudzn9UlAE"/></div></div><div class="di rb bv re rf rg"><div class="dh l"><img alt="" class="dh" src="../Images/3249ba2cf680952e2ccdff36d8ebf4a7.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*C1fv3HJdh1RBspwN"/></div></div><div class="di bv rh ri rg"><div class="dh l"><img alt="" class="dh" src="../Images/a73f0494533d8a08b01c2b899373d2b9.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*QZvzgiM2VnhYyx8M"/></div></div></div></div></div><div class="ql qm gp gr qn"><div role="button" tabindex="0" class="ab bv gv cb fp qo qp bn qq lb ex"><div class="qr l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw qs qt fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l qs qt fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----262801bcfe25--------------------------------" rel="noopener follow" target="_top">艾伯斯乌兹拉</a></p></div></div><div class="qw qx gw l"><h2 class="bd jd wc oy fp wd fr fs we fu fw jc bi translated">MLOps大型项目</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wf au wg wh wi sr wj an eh ei wk wl wm el em eo de bk ep" href="https://dwiuzila.medium.com/list/mlops-megaproject-6a3bf86e45e4?source=post_page-----262801bcfe25--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wn l fo"><span class="bd b dl z dk">6 stories</span></div></div></div><div class="rj dh rk fp ab rl fo di"><div class="di rb bv rc rd"><div class="dh l"><img alt="" class="dh" src="../Images/41b5d7dd3997969f3680648ada22fd7f.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*EBS8CP_UnStLesXoAvjeAQ.png"/></div></div><div class="di rb bv re rf rg"><div class="dh l"><img alt="" class="dh" src="../Images/41befac52d90334c64eef7fc5c4b4bde.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*XLpRKnIMcJzBzCwvXrLvsw.png"/></div></div><div class="di bv rh ri rg"><div class="dh l"><img alt="" class="dh" src="../Images/80908ef475e97fbc42efe3fae0dfcff5.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*K_gzBmjv-ZHlU0Q6HeXclQ.jpeg"/></div></div></div></div></div><div class="ql qm gp gr qn"><div role="button" tabindex="0" class="ab bv gv cb fp qo qp bn qq lb ex"><div class="qr l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw qs qt fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l qs qt fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----262801bcfe25--------------------------------" rel="noopener follow" target="_top">艾伯斯乌兹拉</a></p></div></div><div class="qw qx gw l"><h2 class="bd jd wc oy fp wd fr fs we fu fw jc bi translated">我最好的故事</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wf au wg wh wi sr wj an eh ei wk wl wm el em eo de bk ep" href="https://dwiuzila.medium.com/list/my-best-stories-d8243ae80aa0?source=post_page-----262801bcfe25--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wn l fo"><span class="bd b dl z dk">24 stories</span></div></div></div><div class="rj dh rk fp ab rl fo di"><div class="di rb bv rc rd"><div class="dh l"><img alt="" class="dh" src="../Images/0c862c3dee2d867d6996a970dd38360d.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*K1SQZ1rzr4cb-lSi"/></div></div><div class="di rb bv re rf rg"><div class="dh l"><img alt="" class="dh" src="../Images/392d63d181090365a63dc9060573bcff.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*hSKy6kKorAfHjHOK"/></div></div><div class="di bv rh ri rg"><div class="dh l"><img alt="" class="dh" src="../Images/f51725806220b60eccf5d4c385c700e9.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*HiyGwoGOMI5Ao_fd"/></div></div></div></div></div><div class="ql qm gp gr qn"><div role="button" tabindex="0" class="ab bv gv cb fp qo qp bn qq lb ex"><div class="qr l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw qs qt fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l qs qt fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----262801bcfe25--------------------------------" rel="noopener follow" target="_top">艾伯斯乌兹拉</a></p></div></div><div class="qw qx gw l"><h2 class="bd jd wc oy fp wd fr fs we fu fw jc bi translated">R中的数据科学</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wf au wg wh wi sr wj an eh ei wk wl wm el em eo de bk ep" href="https://dwiuzila.medium.com/list/data-science-in-r-0a8179814b50?source=post_page-----262801bcfe25--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wn l fo"><span class="bd b dl z dk">7 stories</span></div></div></div><div class="rj dh rk fp ab rl fo di"><div class="di rb bv rc rd"><div class="dh l"><img alt="" class="dh" src="../Images/e52e43bf7f22bfc0889cc794dcf734dd.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*10B3radiyQGAp-QA"/></div></div><div class="di rb bv re rf rg"><div class="dh l"><img alt="" class="dh" src="../Images/945fa9100c2a00b46f8aca3d3975f288.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*o6A863Vdwq7ThlmW"/></div></div><div class="di bv rh ri rg"><div class="dh l"><img alt="" class="dh" src="../Images/3ca9e4b148297dbc4e7da0a180cf9c99.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*ekmX89TW6N8Bi8bL"/></div></div></div></div></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><a href="https://dwiuzila.medium.com/membership"><div class="gh gi rp"><img src="../Images/f767019309a71e9b3b70d2f9b1016aad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q_dzEmnimgItuotIZ-y73A.png"/></div></a></figure></div><div class="ab cl qe qf hx qg" role="separator"><span class="qh bw bk qi qj qk"/><span class="qh bw bk qi qj qk"/><span class="qh bw bk qi qj"/></div><div class="im in io ip iq"><p id="3602" class="pw-post-body-paragraph ml mm it mn b mo nq kd mq mr nr kg mt mu ns mw mx my nt na nb nc nu ne nf ng im bi translated">[1] K. Madsen (1988): <em class="nv">一种用于非线性最小二乘法的组合高斯-牛顿和拟牛顿方法</em>。DTU数值分析研究所(现为IMM的一部分)。报告NI-88–10。</p></div></div>    
</body>
</html>