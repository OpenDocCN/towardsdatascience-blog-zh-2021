<html>
<head>
<title>MLP-Mixer: An all-MLP Architecture for Vision</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">MLP混合器:全MLP的视觉建筑</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mlp-mixer-an-all-mlp-architecture-for-vision-7438fac99a06?source=collection_archive---------19-----------------------#2021-06-07">https://towardsdatascience.com/mlp-mixer-an-all-mlp-architecture-for-vision-7438fac99a06?source=collection_archive---------19-----------------------#2021-06-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6217" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">回到美好的老MLP？</h2></div><h1 id="82ef" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">介绍</h1><p id="a472" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi lt translated">来自Google Research &amp;的研究人员Google Brain团队发布了一种新的架构MLP混合器，它没有CNN或注意力层，但仍然可以实现与ViT(视觉变压器)、BiT(大传输)、HaloNet和NF-Net等架构相当的性能。它在训练中的速度提高了3倍，并且随着训练数据量的增加而出色地扩展。作者将这种架构放在一起，因为他们觉得我们可以在没有非常重要的CNN和注意力层的情况下获得出色的结果，这些层需要大量的计算。在这个过程中，他们还展示了更高的吞吐量/秒/核，以及该模型比大多数当前最先进的模型更好地扩展到更大的数据集</p><h1 id="1a57" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">背景</h1><p id="34d1" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在这篇论文发表之前，CNN和自我关注通常是图像分类和计算机视觉的首选机制。这在最近最先进的视觉变压器(ViT)中得到了展示，该变压器成功地将注意力层应用于计算机视觉任务。虽然这些操作是非常计算密集型的，需要花费数天时间在TPU上训练，但作者提出了深度学习中的一个基本概念——MLP(多层感知器)！该架构依赖于在空间和特征通道上重复的各种矩阵乘法。</p><p id="6a2c" class="pw-post-body-paragraph kx ky iq kz b la mc jr lc ld md ju lf lg me li lj lk mf lm ln lo mg lq lr ls ij bi translated">这怎么可能呢？难道我们不是从MLP、CNN、剩余CNN(即ResNets、DenseNets、NF-net、ViT等等)发展而来的吗？现在，作者说回到MLP？是的，你没听错，</p><blockquote class="mh"><p id="93ad" class="mi mj iq bd mk ml mm mn mo mp mq ls dk translated">虽然回旋和注意力对于良好的表现都是足够的，但它们都不是必需的</p></blockquote><p id="9dbf" class="pw-post-body-paragraph kx ky iq kz b la mr jr lc ld ms ju lf lg mt li lj lk mu lm ln lo mv lq lr ls ij bi translated">这些都是巨大的索赔，让我们理解为什么这个架构的工作，他们的索赔是正确的！</p><h1 id="a798" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">体系结构</h1><p id="62ba" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">谈到MLP混合器模型，有相当多的事情正在发生。我们有补丁形式的输入，就像ViT一样。然后是混合器层，其中发生两个操作，具有GELU激活的完全连接的层，以及最后的线性分类头。跳过连接和正则化，如Dropout和Layer Norm也进入了体系结构。</p><figure class="mx my mz na gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi mw"><img src="../Images/2daead54aa8e2255e5525613095d15b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-LddwW7LDDTLrl3CrKDXmQ.jpeg"/></div></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">MLP混频器架构。图片鸣谢——MLP混合器<a class="ae nm" href="https://arxiv.org/pdf/2105.01601.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><p id="d067" class="pw-post-body-paragraph kx ky iq kz b la mc jr lc ld md ju lf lg me li lj lk mf lm ln lo mg lq lr ls ij bi translated">正如我们在该架构底部看到的，网络的输入是图像补丁的形式。这些面片被线性地投影到一个H维的潜在表示(它是隐藏的)中，并被传递到混合器层。这里需要注意的一点是，H值与面片数量或面片大小无关，这使得网络能够线性增长，而不是像ViT那样呈二次方增长。这减少了计算参数，提高了吞吐量，约为120张图像/秒/核，几乎是ViT 32张图像/秒/核的3倍。</p><p id="86c8" class="pw-post-body-paragraph kx ky iq kz b la mc jr lc ld md ju lf lg me li lj lk mf lm ln lo mg lq lr ls ij bi translated">现在，事情变得有趣了。参考展示混合器层的图像的顶部，当投影时，补丁被转换成类似表格的形式，我们称之为x。补丁在彼此之上分层。</p><p id="a7a2" class="pw-post-body-paragraph kx ky iq kz b la mc jr lc ld md ju lf lg me li lj lk mf lm ln lo mg lq lr ls ij bi translated">混合器层由2个MLP块组成。第一块(令牌混合MLP块)作用于X的转置，即线性投影表(X)的列。所有补丁的每一行都具有相同的通道信息。这被馈送到2个完全连接的层的块。该块保存相同类型的信息——跨片识别图像中的特征，即聚集出现该特征的所有通道。权重在图中所示的MLP 1图层中共享。</p><p id="d1ab" class="pw-post-body-paragraph kx ky iq kz b la mc jr lc ld md ju lf lg me li lj lk mf lm ln lo mg lq lr ls ij bi translated">第二个块(通道混合MLP块)在x的另一个转置之后作用于行。这里，获取每个面片并在该面片的所有通道上应用计算。这是仅在该补丁中寻找特征并将其与通道相关联，而在令牌混合块中，它在所有通道中搜索特征。</p><p id="1005" class="pw-post-body-paragraph kx ky iq kz b la mc jr lc ld md ju lf lg me li lj lk mf lm ln lo mg lq lr ls ij bi translated">这种结构是作者引用的独特的CNN，</p><blockquote class="nn no np"><p id="a6db" class="kx ky nq kz b la mc jr lc ld md ju lf nr me li lj ns mf lm ln nt mg lq lr ls ij bi translated">我们的架构可以被视为一个独特的CNN，它使用(1×1)卷积进行信道混合，使用单通道深度卷积进行令牌混合。然而，反之则不然，因为CNN不是混频器的特例。</p></blockquote><p id="9e5c" class="pw-post-body-paragraph kx ky iq kz b la mc jr lc ld md ju lf lg me li lj lk mf lm ln lo mg lq lr ls ij bi translated">此外，该架构使用变压器架构中常见的层规范。下图说明了层规范与批处理规范，以便快速理解。更多关于层定额<a class="ae nm" href="https://leimao.github.io/blog/Layer-Normalization/" rel="noopener ugc nofollow" target="_blank">这里</a>。GELU激活包括用于非线性的跳过连接和用于正则化的丢弃连接。</p><figure class="mx my mz na gt nb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nu"><img src="../Images/cc1d2130daa9b70389ad3d2f617c4f2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tTHI_aZSsxj9X4-EdHnT2Q.jpeg"/></div></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">层规范化与批规范化。图片来源——power norm<a class="ae nm" href="http://proceedings.mlr.press/v119/shen20e/shen20e.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><h1 id="8bc9" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">基准和结果</h1><figure class="mx my mz na gt nb gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/063260ee166db6adc791c4ff385317c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1142/format:webp/1*PGbhkRNTieQch3-APf7SKQ.jpeg"/></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">跨数据集比较Mixer与其他模型。图片鸣谢——MLP混合器<a class="ae nm" href="https://arxiv.org/pdf/2105.01601.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><p id="dbb4" class="pw-post-body-paragraph kx ky iq kz b la mc jr lc ld md ju lf lg me li lj lk mf lm ln lo mg lq lr ls ij bi translated">正如我们在表中看到的，与其他SOTAs相比，ImageNet top-1验证准确性方面的性能相当不错。但是对于MLP混合器来说，真正的交易是双倍的——图像/秒/核心的吞吐量和TPU训练时间。混合器在图像/秒/核的吞吐量方面远远领先于ViT，比ViT的32 (ImageNet)高出105分，比ViT的15(JFT-300)高出40(图像/秒/核)。</p><p id="e209" class="pw-post-body-paragraph kx ky iq kz b la mc jr lc ld md ju lf lg me li lj lk mf lm ln lo mg lq lr ls ij bi translated">当谈到ImageNet上的TPU训练时间时，ViT胜过Mixer，但不是JFT-300。因此，当我们看到桌面上更多的数字时，我们可以说,“混音器table正在挑战其他索塔。不一定胜过他们，但仍然很有竞争力”。</p><p id="ac70" class="pw-post-body-paragraph kx ky iq kz b la mc jr lc ld md ju lf lg me li lj lk mf lm ln lo mg lq lr ls ij bi translated">到了“它随更多数据而扩展”的部分，下图说明了当训练集大小增加时，Mixer、ViT和BiT的性能。</p><figure class="mx my mz na gt nb gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/0258198b02cf0c0191cf5fc9338f2690.png" data-original-src="https://miro.medium.com/v2/resize:fit:878/format:webp/1*wvPEMaBQwF1a0OoGbxPBFA.jpeg"/></div><p class="ni nj gj gh gi nk nl bd b be z dk translated">训练子集大小与ImageNet top-1 val准确性的关系。图片鸣谢——MLP混合器<a class="ae nm" href="https://arxiv.org/pdf/2105.01601.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><p id="6446" class="pw-post-body-paragraph kx ky iq kz b la mc jr lc ld md ju lf lg me li lj lk mf lm ln lo mg lq lr ls ij bi translated">混合器在图的右侧紧密匹配，这表明5次拍摄ImageNet top-1精度。但是我们需要从左侧观察曲线，其中训练子集大小从10M开始，一直到300M。在所有情况下，混合器具有更好的曲线缩放，并且不会在任何时间点使曲线变平。这意味着，它不会因为更多的训练数据而饱和，而ViT和BiT模型一旦超过100M的训练数据子集大小就会饱和(精度不会提高)。</p><p id="cb98" class="pw-post-body-paragraph kx ky iq kz b la mc jr lc ld md ju lf lg me li lj lk mf lm ln lo mg lq lr ls ij bi translated">此外，在该模型的所有变体与其他SOTAs的其他全面比较中，考虑到与性能相关的预训练方面，当有更多数据时，混合器会受益。预训练数据越小，其在验证准确性方面与SOTAs的差距就越大。一定要看看报纸上的这篇文章。报纸上还有一个非常有趣的部分——<strong class="kz ir">MLP混频器的设计思想如何可以追溯到CNN&amp;变形金刚？。</strong>这就是为什么要确保阅读这篇论文，了解消融术、相关工作、不同的实验设置、重量的可视化以及没有成功的事情！</p><h1 id="840f" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">结论</h1><p id="8a26" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">MLP混合器在吞吐量和训练时间等各种重要指标上都名列前茅。在ImageNet top-1验证准确性方面，它并不落后，在JFT-300上进行了预训练，得分为87.78。就所提供的训练数据量而言，与ViT &amp; BiT相比，该网络具有极佳的扩展性。但是，有一点需要深思——归纳偏差。根据本文中的训练子集大小与ImageNet top-1 val准确度图，ViT在300米处变平，而Mixer-MLP仍在上升。如果数据的规模是600M，可能会发生什么？密切关注预训练数据集大小和归纳偏差的作用。说到这里，作者的一个更有趣的评论是——“这种设计对NLP是否适用？”。时间会证明一切。</p><h1 id="745c" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">参考</h1><p id="4540" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">[1]https://arxiv.org/pdf/2105.01601.pdfMLP-混频器:<a class="ae nm" href="https://arxiv.org/pdf/2105.01601.pdf" rel="noopener ugc nofollow" target="_blank"/></p><p id="adeb" class="pw-post-body-paragraph kx ky iq kz b la mc jr lc ld md ju lf lg me li lj lk mf lm ln lo mg lq lr ls ij bi translated">[2]维特:【https://arxiv.org/pdf/2010.11929】T2</p><p id="bd8e" class="pw-post-body-paragraph kx ky iq kz b la mc jr lc ld md ju lf lg me li lj lk mf lm ln lo mg lq lr ls ij bi translated">[3]位:【https://arxiv.org/pdf/1912.11370】T4</p><p id="fd41" class="pw-post-body-paragraph kx ky iq kz b la mc jr lc ld md ju lf lg me li lj lk mf lm ln lo mg lq lr ls ij bi translated">[4]光环网:<a class="ae nm" href="https://arxiv.org/pdf/2103.12731" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2103.12731</a></p><p id="3e48" class="pw-post-body-paragraph kx ky iq kz b la mc jr lc ld md ju lf lg me li lj lk mf lm ln lo mg lq lr ls ij bi translated">[5]层定额:<a class="ae nm" href="https://arxiv.org/pdf/2003.07845" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2003.07845</a></p><p id="b337" class="pw-post-body-paragraph kx ky iq kz b la mc jr lc ld md ju lf lg me li lj lk mf lm ln lo mg lq lr ls ij bi translated">[6]NF-Nets:<a class="ae nm" href="https://arxiv.org/pdf/2102.06171v1.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2102.06171v1.pdf</a></p><p id="1748" class="pw-post-body-paragraph kx ky iq kz b la mc jr lc ld md ju lf lg me li lj lk mf lm ln lo mg lq lr ls ij bi translated">[7]ImageNet:<a class="ae nm" href="http://www.image-net.org/" rel="noopener ugc nofollow" target="_blank">http://www.image-net.org/</a></p></div></div>    
</body>
</html>