<html>
<head>
<title>Question Answering with a fine-tuned BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用微调过的BERT回答问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/question-answering-with-a-fine-tuned-bert-bc4dafd45626?source=collection_archive---------1-----------------------#2021-05-16">https://towardsdatascience.com/question-answering-with-a-fine-tuned-bert-bc4dafd45626?source=collection_archive---------1-----------------------#2021-05-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7350" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">…在斯坦福大学的CoQA数据集上使用拥抱脸变形器和PyTorch</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/eb573f3398fe9a8bc935821f915ded53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PsajrNCukw6m1h4Vs49bUw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/@taypaigey" rel="noopener ugc nofollow" target="_blank">泰勒</a>在<a class="ae kv" href="https://unsplash.com/photos/NTur2_QKpg0" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="6643" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每当我想到问题回答系统时，我首先想到的是教室——一个老师提出的问题，一个或几个学生举手🙋来回答这个问题。也就是说，回答问题对人类来说可能是一项微不足道的任务，但对机器来说却不是如此微不足道。要回答任何问题，机器都需要克服许多不同的挑战，如词汇空缺、共指消解、语言歧义等。😏为此，机器需要大量的训练数据和智能架构来理解和存储文本中的重要信息。NLP的最新进展已经释放了机器理解文本和执行不同任务的能力。👌</p><p id="4edc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我们将一起完成一项常用的任务——问答。我们将使用拥抱脸变形金刚库中已经可用的微调BERT模型来回答基于CoQA数据集的故事的问题。我确信，通过查看代码，您会意识到使用一个微调的模型来达到我们的目的是多么容易。😁</p><p id="ae05" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="ls">注意:</em> </strong> <em class="ls">在本文中我们将不深入讨论BERT架构的细节。但是，在需要或可能的情况下，我会提供解释。</em></p><p id="8186" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">文中使用的版本:</strong>火炬- 1.7.1，变形金刚- 4.4.2</p><p id="4a13" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们首先回答与本文相关的几个重要问题。</p><p id="f7ef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">什么是抱脸和变形金刚？</strong>🤔</p><p id="60ea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank">拥抱脸</a>是自然语言处理(NLP)技术的开源提供商。您可以使用拥抱脸最先进的模型来构建、训练和部署您自己的模型。<a class="ae kv" href="https://huggingface.co/transformers/" rel="noopener ugc nofollow" target="_blank">变形金刚</a>是他们的NLP库。我强烈推荐你去看看拥抱脸团队所做的惊人的工作，以及他们收集的大量预先训练好的NLP模型。</p><p id="ee7d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">什么是CoQA？</strong>🤔</p><p id="36e4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://arxiv.org/pdf/1808.07042.pdf" rel="noopener ugc nofollow" target="_blank"> CoQA </a>是斯坦福NLP于2019年发布的会话式问答数据集。这是一个用于构建对话式问答系统的大规模数据集。该数据集旨在测量机器理解文本段落并回答对话中出现的一系列相互关联的问题的能力。该数据集的独特之处在于，每个对话都是通过两个人群工作者以问答的形式就一段内容进行聊天来收集的，因此，这些问题是对话式的。要了解JSON数据的格式，请参考这个<a class="ae kv" href="http://downloads.cs.stanford.edu/nlp/data/coqa/coqa-train-v1.0.json" rel="noopener ugc nofollow" target="_blank">链接</a>。我们将使用来自JSON数据集的故事、问题和答案来构成我们的数据框架。</p><p id="53d1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">伯特是什么？</strong>🤔</p><p id="1d3e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank"> BERT </a>是来自变压器的双向编码器表示。它是最流行和最广泛使用的NLP模型之一。BERT模型可以通过查看单词前后的单词来考虑单词的完整上下文，这对于理解查询背后的意图特别有用。由于它的双向性，它对语言的上下文和流程有更深的理解，因此，现在被用于许多NLP任务中。关于BERT的更多细节以及代码。🙃</p><p id="bbd4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">变形金刚库有很多不同的<a class="ae kv" href="https://huggingface.co/transformers/model_doc/bert.html#" rel="noopener ugc nofollow" target="_blank"> BERT模型</a>。很容易从这个库中找到一个特定于任务的模型并完成我们的任务。</p><p id="929b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">那么，让我们开始吧，但首先让我们看看我们的数据集。😊</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lt"><img src="../Images/b80a4ed2c024a536f0f93f51d2800e53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HrZPaUEKIvMUJ5FV81mIzA.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lu"><img src="../Images/7ef70211d691055b80928d65998a6918.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BkpMqgKKzilgj0FFMLlclw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">斯坦福的JSON数据</p></figure><p id="5712" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">JSON数据有很多字段。出于我们的目的，我们将使用来自“问题”和“答案”的“故事”、“输入文本”，并形成我们的数据框架。</p><h2 id="0c6f" class="lv lw iq bd lx ly lz dn ma mb mc dp md lf me mf mg lj mh mi mj ln mk ml mm mn bi translated"><strong class="ak">安装变压器</strong></h2><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="9f29" class="lv lw iq mp b gy mt mu l mv mw"><strong class="mp ir">!pip install</strong> transformers</span></pre><h2 id="da5f" class="lv lw iq bd lx ly lz dn ma mb mc dp md lf me mf mg lj mh mi mj ln mk ml mm mn bi translated"><strong class="ak">导入库</strong></h2><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="6d74" class="lv lw iq mp b gy mt mu l mv mw"><strong class="mp ir">import</strong> pandas <strong class="mp ir">as</strong> pd<br/><strong class="mp ir">import</strong> numpy <strong class="mp ir">as</strong> np<br/><strong class="mp ir">import</strong> torch<br/><strong class="mp ir">from</strong> transformers <strong class="mp ir">import</strong> BertForQuestionAnswering<br/><strong class="mp ir">from</strong> transformers <strong class="mp ir">import</strong> BertTokenizer</span></pre><h2 id="fcd8" class="lv lw iq bd lx ly lz dn ma mb mc dp md lf me mf mg lj mh mi mj ln mk ml mm mn bi translated">从斯坦福网站加载数据</h2><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="a95a" class="lv lw iq mp b gy mt mu l mv mw">coqa = <strong class="mp ir">pd.read_json</strong>('<a class="ae kv" href="http://downloads.cs.stanford.edu/nlp/data/coqa/coqa-train-v1.0.json'" rel="noopener ugc nofollow" target="_blank">http://downloads.cs.stanford.edu/nlp/data/coqa/coqa-train-v1.0.json'</a>)<br/>coqa<strong class="mp ir">.head()</strong></span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mx"><img src="../Images/28fe40a70537d638d3af47ad9338967e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FrD-KnfAyE5Vx4N56Q10sg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">加载的数据集</p></figure><h2 id="b566" class="lv lw iq bd lx ly lz dn ma mb mc dp md lf me mf mg lj mh mi mj ln mk ml mm mn bi translated">数据清理</h2><p id="4539" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">我们将处理“数据”列，所以让我们删除“版本”列。</p><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="5348" class="lv lw iq mp b gy mt mu l mv mw"><strong class="mp ir">del</strong> coqa["version"]</span></pre><p id="be40" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于每一个问答配对，我们将把链接的故事附加到它上面。</p><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="c630" class="lv lw iq mp b gy mt mu l mv mw"><em class="ls">#required columns in our dataframe</em><br/>cols = ["text","question","answer"]</span><span id="491a" class="lv lw iq mp b gy nd mu l mv mw"><em class="ls">#list of lists to create our dataframe</em><br/>comp_list = []<br/><strong class="mp ir">for</strong> index, row <strong class="mp ir">in</strong> coqa<strong class="mp ir">.iterrows()</strong>:<br/>    <strong class="mp ir">for</strong> i <strong class="mp ir">in</strong> <strong class="mp ir">range</strong>(<strong class="mp ir">len</strong>(row["data"]["questions"])):<br/>        temp_list = []<br/>        temp_list<strong class="mp ir">.append</strong>(row["data"]["story"])<br/>        temp_list<strong class="mp ir">.append</strong>(row["data"]["questions"][i]["input_text"])<br/>        temp_list<strong class="mp ir">.append</strong>(row["data"]["answers"][i]["input_text"])<br/>        comp_list<strong class="mp ir">.append</strong>(temp_list)</span><span id="523f" class="lv lw iq mp b gy nd mu l mv mw">new_df = <strong class="mp ir">pd.DataFrame</strong>(comp_list, columns=cols) </span><span id="11f3" class="lv lw iq mp b gy nd mu l mv mw"><em class="ls">#saving the dataframe to csv file for further loading</em><br/>new_df<strong class="mp ir">.to_csv</strong>("CoQA_data.csv", index=<strong class="mp ir">False</strong>)</span></pre><h2 id="74aa" class="lv lw iq bd lx ly lz dn ma mb mc dp md lf me mf mg lj mh mi mj ln mk ml mm mn bi translated">从本地CSV文件加载数据</h2><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="4660" class="lv lw iq mp b gy mt mu l mv mw">data = <strong class="mp ir">pd.read_csv</strong>("CoQA_data.csv")<br/>data<strong class="mp ir">.head()</strong></span></pre><p id="a5ff" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是我们清理后的数据。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/8e02cd9f9d751a590bd96f257d3d7840.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C5uTFeTMHrJ64BZil8_mCw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">清理的数据</p></figure><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="87ce" class="lv lw iq mp b gy mt mu l mv mw"><strong class="mp ir">print</strong>("Number of question and answers: ", <strong class="mp ir">len</strong>(data))</span></pre><p id="c27f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">数据集有很多问题和答案，我们来数一下。</p><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="8ee7" class="lv lw iq mp b gy mt mu l mv mw">Number of question and answers:  108647</span></pre><h2 id="1ef5" class="lv lw iq bd lx ly lz dn ma mb mc dp md lf me mf mg lj mh mi mj ln mk ml mm mn bi translated">构建聊天机器人</h2><p id="415f" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">使用这些预先训练好的模型的最大好处是，只需两行简单的代码就可以加载模型及其标记器。😲不就是单纯的哇吗？对于文本分类这样的任务，我们需要在数据集上微调BERT。但对于问答任务，我们甚至可以使用已经训练好的模型，即使我们的文本来自完全不同的领域，也能获得不错的结果。为了得到更好的结果，我们使用了一个伯特模型，这个模型是根据小队基准进行微调的。</p><p id="21f6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于我们的任务，我们将使用变形金刚库中的<strong class="ky ir"> BertForQuestionAnswering </strong>类。</p><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="107e" class="lv lw iq mp b gy mt mu l mv mw">model = <strong class="mp ir">BertForQuestionAnswering.from_pretrained</strong>('bert-large-uncased-whole-word-masking-finetuned-squad')<br/>tokenizer = <strong class="mp ir">BertTokenizer.from_pretrained</strong>('bert-large-uncased-whole-word-masking-finetuned-squad')</span></pre><p id="c758" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">预计下载需要几分钟，因为BERT-large是一个非常大的模型，有24层和340M参数，使其成为1.34GB的模型。</p><h2 id="7272" class="lv lw iq bd lx ly lz dn ma mb mc dp md lf me mf mg lj mh mi mj ln mk ml mm mn bi translated">问问题</h2><p id="5c0f" class="pw-post-body-paragraph kw kx iq ky b kz my jr lb lc mz ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">让我们随机选择一个问题编号。</p><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="e40c" class="lv lw iq mp b gy mt mu l mv mw">random_num = <strong class="mp ir">np.random.randint</strong>(0,<strong class="mp ir">len</strong>(data))</span><span id="cbc3" class="lv lw iq mp b gy nd mu l mv mw">question = data["question"][random_num]<br/>text = data["text"][random_num]</span></pre><p id="055e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们将问题和文本标记成一对。</p><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="dfa6" class="lv lw iq mp b gy mt mu l mv mw">input_ids = tokenizer<strong class="mp ir">.encode</strong>(question, text)<br/><strong class="mp ir">print</strong>("The input has a total of {} tokens."<strong class="mp ir">.format</strong>(<strong class="mp ir">len</strong>(input_ids)))</span></pre><p id="61a5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们来看看这个问题和文本对有多少个令牌。</p><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="2b82" class="lv lw iq mp b gy mt mu l mv mw">The input has a total of 427 tokens.</span></pre><p id="7905" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了看看我们的记号赋予器在做什么，让我们打印出记号和它们的id。</p><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="6ee8" class="lv lw iq mp b gy mt mu l mv mw">tokens = tokenizer<strong class="mp ir">.convert_ids_to_tokens</strong>(input_ids)</span><span id="5b13" class="lv lw iq mp b gy nd mu l mv mw"><strong class="mp ir">for</strong> token, id <strong class="mp ir">in</strong> <strong class="mp ir">zip</strong>(tokens, input_ids):<br/>    <strong class="mp ir">print</strong>('{:8}{:8,}'<strong class="mp ir">.format</strong>(token,id))</span></pre><div class="kg kh ki kj gt ab cb"><figure class="nf kk ng nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/e4491b6a5fd79dd3cc5e55e79c0db047.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*521ml0xTQXNZKTiYsDrIEw.png"/></div></figure><figure class="nf kk nl nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/c9f28639e370952ec381b0ae38a1ed21.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*r1g-N-vJbDmBApYr89BP8A.png"/></div></figure><figure class="nf kk nm nh ni nj nk paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/41d186c2f452033ab6286def8bf581e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/format:webp/1*5GD1L38aQfgB5g2vv0dIpA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk nn di no np translated">带有id的令牌</p></figure></div><p id="17fb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">BERT有一种处理标记化输入的独特方法。从上面的截图中，我们可以看到两个特殊的令牌[CLS]和[SEP]。[CLS] token代表分类，它代表句子级别的分类，在我们分类时使用。BERT使用的另一个令牌是[SEP]。它用于分隔两段文本。你可以在上面的截图里看到两个[SEP]令牌，一个在问题后面，一个在正文后面。</p><p id="769d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">除了“标记嵌入”，BERT内部还使用了“段嵌入”和“位置嵌入”。片段嵌入有助于BERT区分问题和文本。在实践中，如果嵌入来自句子1，我们使用0的向量，否则如果嵌入来自句子2，我们使用1的向量。位置嵌入有助于确定单词在序列中的位置。所有这些嵌入都被馈送到输入层。</p><p id="b791" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">变形金刚库可以使用<em class="ls">pretrainedtokenizer . encode _ plus()</em>自行创建片段嵌入。但是，我们甚至可以创造我们自己的。为此，我们只需要为每个令牌指定一个0或1。</p><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="941c" class="lv lw iq mp b gy mt mu l mv mw"><em class="ls">#first occurence of [SEP] token</em><br/>sep_idx = input_ids<strong class="mp ir">.index</strong>(tokenizer<strong class="mp ir">.sep_token_id</strong>)<br/><strong class="mp ir">print</strong>("SEP token index: ", sep_idx)</span><span id="e93d" class="lv lw iq mp b gy nd mu l mv mw"><em class="ls">#number of tokens in segment A (question) - this will be one more than the sep_idx as the index in Python starts from 0<br/></em>num_seg_a = sep_idx+1<br/><strong class="mp ir">print</strong>("Number of tokens in segment A: ", num_seg_a)</span><span id="3881" class="lv lw iq mp b gy nd mu l mv mw"><em class="ls">#number of tokens in segment B (text)</em><br/>num_seg_b = <strong class="mp ir">len</strong>(input_ids) - num_seg_a<br/><strong class="mp ir">print</strong>("Number of tokens in segment B: ", num_seg_b)</span><span id="b4fd" class="lv lw iq mp b gy nd mu l mv mw"><em class="ls">#creating the segment ids</em><br/>segment_ids = [0]*num_seg_a + [1]*num_seg_b</span><span id="d5d2" class="lv lw iq mp b gy nd mu l mv mw">#making sure that every input token has a segment id<strong class="mp ir"><br/>assert</strong> <strong class="mp ir">len</strong>(segment_ids) == <strong class="mp ir">len</strong>(input_ids)</span></pre><p id="eea1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是输出。</p><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="1b1d" class="lv lw iq mp b gy mt mu l mv mw">SEP token index: 8<br/>Number of tokens in segment A: 9<br/>Number of tokens in segment B: 418</span></pre><p id="7b65" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们把它输入到我们的模型中。</p><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="ce9d" class="lv lw iq mp b gy mt mu l mv mw"><em class="ls">#token input_ids to represent the input and token segment_ids to differentiate our segments - question and text</em><br/>output = <strong class="mp ir">model</strong>(<strong class="mp ir">torch.tensor</strong>([input_ids]),  token_type_ids=<strong class="mp ir">torch.tensor</strong>([segment_ids]))</span></pre><p id="32a9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">查看最可能的开始和结束单词，并且仅当结束标记在开始标记之后时才提供答案。</p><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="e5c8" class="lv lw iq mp b gy mt mu l mv mw"><em class="ls">#tokens with highest start and end scores</em><br/>answer_start = <strong class="mp ir">torch.argmax</strong>(output.start_logits)<br/>answer_end = <strong class="mp ir">torch.argmax</strong>(output.end_logits)</span><span id="1327" class="lv lw iq mp b gy nd mu l mv mw"><strong class="mp ir">if</strong> answer_end &gt;= answer_start:<br/>    answer = " "<strong class="mp ir">.join</strong>(tokens[answer_start:answer_end+1])<br/><strong class="mp ir">else:</strong><br/>    <strong class="mp ir">print</strong>("I am unable to find the answer to this question. Can you please ask another question?")<br/>    <br/><strong class="mp ir">print</strong>("\nQuestion:\n{}"<strong class="mp ir">.format</strong>(question<strong class="mp ir">.capitalize()</strong>))<br/><strong class="mp ir">print</strong>("\nAnswer:\n{}."<strong class="mp ir">.format</strong>(answer<strong class="mp ir">.capitalize()</strong>))</span></pre><p id="81dd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是我们的问题和答案。</p><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="6172" class="lv lw iq mp b gy mt mu l mv mw"><strong class="mp ir">Question:</strong><br/>Who is the acas director?<br/><strong class="mp ir"><br/>Answer:</strong><br/>Agnes karin ##gu.</span></pre><p id="fdbd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">哇！伯特预测到了正确的答案——“艾格尼丝·卡琳古”。但是，回复里这个“##”是什么？继续读下去！📙</p><p id="e11d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">伯特使用<strong class="ky ir">词块标记化</strong>。在BERT中，生僻字被分解成子字/片段。单词块标记化使用##来分隔已拆分的标记。例如:“Karin”是一个常用词，所以wordpiece不会拆分它。然而，“Karingu”是一个罕见的词，所以wordpiece将其拆分为单词“Karin”和“##gu”。注意，它在古前面加了##，表示它是拆分后的第二个词。</p><p id="e8b1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用单词块标记化背后的想法是减少词汇表的大小，从而提高训练性能。想想这些词，跑，跑，跑者。如果没有单词块标记化，模型必须独立地存储和学习所有三个单词的含义。但是，通过词块标记化，这三个单词中的每一个都将被拆分为“run”和相关的“# #后缀”(如果有后缀的话，例如，“run”，“##ning”，“##ner”)。现在，模型将学习单词“run”的上下文，其余的意思将被编码在后缀中，这将从具有类似后缀的其他单词中学习。</p><p id="4146" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">很有趣，对吧？我们可以使用下面的简单代码来重构这些单词。</p><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="2448" class="lv lw iq mp b gy mt mu l mv mw">answer = tokens[answer_start]</span><span id="13ec" class="lv lw iq mp b gy nd mu l mv mw"><strong class="mp ir">for</strong> i <strong class="mp ir">in</strong> <strong class="mp ir">range</strong>(answer_start+1, answer_end+1):<br/>    <strong class="mp ir">if </strong>tokens[i][0:2] == "##":<br/>        answer += tokens[i][2:]<br/>  <strong class="mp ir">  else:</strong><br/>        answer += " " + tokens[i]</span></pre><p id="619a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上面的答案现在将变成:<strong class="ky ir"> Agnes karingu </strong></p><p id="691e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们把这个问答过程变成一个简单的函数。</p><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="df0b" class="lv lw iq mp b gy mt mu l mv mw"><strong class="mp ir">def</strong> question_answer(question, text):<br/>    <br/>    <em class="ls">#tokenize question and text as a pair</em><br/>    input_ids = tokenizer<strong class="mp ir">.encode</strong>(question, text)<br/>    <br/>   <em class="ls"> #string version of tokenized ids</em><br/>    tokens = tokenizer<strong class="mp ir">.convert_ids_to_tokens</strong>(input_ids)<br/>    <br/>  <em class="ls">  #segment IDs</em><br/>    #first occurence of [SEP] token<br/>    sep_idx = input_ids<strong class="mp ir">.index</strong>(tokenizer<strong class="mp ir">.sep_token_id</strong>)</span><span id="6772" class="lv lw iq mp b gy nd mu l mv mw"><em class="ls">    #number of tokens in segment A (question)</em><br/>    num_seg_a = sep_idx+1</span><span id="1894" class="lv lw iq mp b gy nd mu l mv mw"><em class="ls">    #number of tokens in segment B (text)</em><br/>    num_seg_b = <strong class="mp ir">len</strong>(input_ids) - num_seg_a<br/>    <br/>    <em class="ls">#list of 0s and 1s for segment embeddings</em><br/>    segment_ids = [0]*num_seg_a + [1]*num_seg_b</span><span id="2e0e" class="lv lw iq mp b gy nd mu l mv mw">    <strong class="mp ir">assert</strong> <strong class="mp ir">len</strong>(segment_ids) == <strong class="mp ir">len</strong>(input_ids)<br/>    <br/>    <em class="ls">#model output using input_ids and segment_ids</em><br/>    output = <strong class="mp ir">model</strong>(<strong class="mp ir">torch.tensor</strong>([input_ids]), token_type_ids=<strong class="mp ir">torch.tensor</strong>([segment_ids]))<br/>    <br/>    <em class="ls">#reconstructing the answer</em><br/>    answer_start = <strong class="mp ir">torch.argmax</strong>(output.start_logits)<br/>    answer_end = <strong class="mp ir">torch.argmax</strong>(output.end_logits)</span><span id="61db" class="lv lw iq mp b gy nd mu l mv mw"><strong class="mp ir">    if</strong> answer_end &gt;= answer_start:<br/>        answer = tokens[answer_start]<br/>        <strong class="mp ir">for</strong> i <strong class="mp ir">in</strong> range(answer_start+1, answer_end+1):<br/>            <strong class="mp ir">if</strong> tokens[i][0:2] == "##":<br/>                answer += tokens[i][2:]<br/>            <strong class="mp ir">else:</strong><br/>                answer += " " + tokens[i]<br/>                <br/>    <strong class="mp ir">if</strong> answer<strong class="mp ir">.startswith</strong>("[CLS]"):<br/>        answer = "Unable to find the answer to your question."<br/>    <br/>    <strong class="mp ir">print</strong>("\nPredicted answer:\n{}"<strong class="mp ir">.format</strong>(answer<strong class="mp ir">.capitalize()</strong>))</span></pre><p id="73c6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们使用数据集中的一个文本和问题来测试这个函数。😛</p><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="748e" class="lv lw iq mp b gy mt mu l mv mw"><strong class="mp ir">text</strong> = """New York (CNN) -- More than 80 Michael Jackson collectibles -- including the late pop star's famous rhinestone-studded glove from a 1983 performance -- were auctioned off Saturday, reaping a total $2 million. Profits from the auction at the Hard Rock Cafe in New York's Times Square crushed pre-sale expectations of only $120,000 in sales. The highly prized memorabilia, which included items spanning the many stages of Jackson's career, came from more than 30 fans, associates and family members, who contacted Julien's Auctions to sell their gifts and mementos of the singer. Jackson's flashy glove was the big-ticket item of the night, fetching $420,000 from a buyer in Hong Kong, China. Jackson wore the glove at a 1983 performance during \"Motown 25,\" an NBC special where he debuted his revolutionary moonwalk. Fellow Motown star Walter \"Clyde\" Orange of the Commodores, who also performed in the special 26 years ago, said he asked for Jackson's autograph at the time, but Jackson gave him the glove instead. "The legacy that [Jackson] left behind is bigger than life for me,\" Orange said. \"I hope that through that glove people can see what he was trying to say in his music and what he said in his music.\" Orange said he plans to give a portion of the proceeds to charity. Hoffman Ma, who bought the glove on behalf of Ponte 16 Resort in Macau, paid a 25 percent buyer's premium, which was tacked onto all final sales over $50,000. Winners of items less than $50,000 paid a 20 percent premium."""</span><span id="5265" class="lv lw iq mp b gy nd mu l mv mw"><strong class="mp ir">question</strong> = "Where was the Auction held?"</span><span id="3c55" class="lv lw iq mp b gy nd mu l mv mw">question_answer(question, text)</span><span id="cfe4" class="lv lw iq mp b gy nd mu l mv mw"><em class="ls">#original answer from the dataset</em><strong class="mp ir"><br/>print</strong>("Original answer:\n", data<strong class="mp ir">.loc</strong>[data["question"] == question]["answer"].values[0]))</span></pre><p id="3c1c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">输出:</p><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="a842" class="lv lw iq mp b gy mt mu l mv mw"><strong class="mp ir">Predicted answer:</strong><br/>Hard rock cafe in new york ' s times square</span><span id="1b36" class="lv lw iq mp b gy nd mu l mv mw"><strong class="mp ir">Original answer:</strong><br/>Hard Rock Cafe</span></pre><p id="cfbd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一点也不差。事实上，我们的伯特模型给出了更详细的回答。</p><p id="7c72" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是一个小函数，用来测试BERT对上下文的理解程度。我只是把回答问题的过程作为一个循环来玩这个模型。💃</p><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="a106" class="lv lw iq mp b gy mt mu l mv mw"><strong class="mp ir">text</strong> = <strong class="mp ir">input</strong>("Please enter your text: \n")<br/><strong class="mp ir">question</strong> = <strong class="mp ir">input</strong>("\nPlease enter your question: \n")</span><span id="2f6c" class="lv lw iq mp b gy nd mu l mv mw"><strong class="mp ir">while</strong> <strong class="mp ir">True</strong>:<br/>    question_answer(question, text)<br/>    <br/>    flag = <strong class="mp ir">True</strong><br/>    flag_N = <strong class="mp ir">False</strong><br/>    <br/>    <strong class="mp ir">while</strong> flag:<br/>        response = <strong class="mp ir">input</strong>("\nDo you want to ask another question based on this text (Y/N)? ")<br/>        <strong class="mp ir">if</strong> response[0] == "Y":<br/>            question = <strong class="mp ir">input</strong>("\nPlease enter your question: \n")<br/>            flag = False<br/>        <strong class="mp ir">elif</strong> response[0] == "N":<br/>            <strong class="mp ir">print</strong>("\nBye!")<br/>            flag = False<br/>            flag_N = True<br/>            <br/>    <strong class="mp ir">if</strong> flag_N == <strong class="mp ir">True</strong>:<br/>        <strong class="mp ir">break</strong></span></pre><p id="7f41" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">而且，结果！😎</p><pre class="kg kh ki kj gt mo mp mq mr aw ms bi"><span id="ae8d" class="lv lw iq mp b gy mt mu l mv mw"><strong class="mp ir">Please enter your text: </strong><br/>The Vatican Apostolic Library (), more commonly called the Vatican Library or simply the Vat, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it is much older, it is one of the oldest libraries in the world and contains one of the most significant collections of historical texts. It has 75,000 codices from throughout history, as well as 1.1 million printed books, which include some 8,500 incunabula.   The Vatican Library is a research library for history, law, philosophy, science and theology. The Vatican Library is open to anyone who can document their qualifications and research needs. Photocopies for private study of pages from books published between 1801 and 1990 can be requested in person or by mail.   In March 2014, the Vatican Library began an initial four-year project of digitising its collection of manuscripts, to be made available online.   The Vatican Secret Archives were separated from the library at the beginning of the 17th century; they contain another 150,000 items.   Scholars have traditionally divided the history of the library into five periods, Pre-Lateran, Lateran, Avignon, Pre-Vatican and Vatican.   The Pre-Lateran period, comprising the initial days of the library, dated from the earliest days of the Church. Only a handful of volumes survive from this period, though some are very significant.<br/><br/><strong class="mp ir">Please enter your question: </strong><br/>When was the Vat formally opened?<br/><br/><strong class="mp ir">Answer:</strong><br/>1475<br/><strong class="mp ir"><br/>Do you want to ask another question based on this text (Y/N)?</strong> Y<br/><br/><strong class="mp ir">Please enter your question: </strong><br/>what is the library for?<br/><br/><strong class="mp ir">Answer:</strong><br/>Research library for history , law , philosophy , science and theology<br/><br/><strong class="mp ir">Do you want to ask another question based on this text (Y/N)?</strong> Y<br/><br/><strong class="mp ir">Please enter your question: </strong><br/>for what subjects?<br/><br/><strong class="mp ir">Answer:</strong><br/>History , law , philosophy , science and theology</span><span id="08d1" class="lv lw iq mp b gy nd mu l mv mw"><strong class="mp ir">Do you want to ask another question based on this text (Y/N)?</strong> N<br/><br/>Bye!</span></pre><p id="2496" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">瞧啊。很好用！🤗</p><p id="53bb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望这篇文章能让你了解我们如何轻松地使用来自拥抱面部变形库的预训练模型并执行我们的任务。如果你想把代码看做一个笔记本文件，这里有<a class="ae kv" href="https://github.com/chetnakhanna16/CoQA_QuesAns_BERT/blob/main/CoQA_BERT_QuestionAnswering.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>链接。</p><p id="b5d3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">参考文献:</strong></p><ol class=""><li id="3531" class="nq nr iq ky b kz la lc ld lf ns lj nt ln nu lr nv nw nx ny bi translated"><a class="ae kv" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/</a></li><li id="73f4" class="nq nr iq ky b kz nz lc oa lf ob lj oc ln od lr nv nw nx ny bi translated"><a class="ae kv" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1810.04805.pdf</a></li><li id="874d" class="nq nr iq ky b kz nz lc oa lf ob lj oc ln od lr nv nw nx ny bi translated"><a class="ae kv" href="https://arxiv.org/pdf/1808.07042.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1808.07042.pdf</a></li><li id="abe0" class="nq nr iq ky b kz nz lc oa lf ob lj oc ln od lr nv nw nx ny bi translated"><a class="ae kv" href="https://github.com/google-research/bert/issues/44" rel="noopener ugc nofollow" target="_blank">https://github.com/google-research/bert/issues/44</a></li></ol></div><div class="ab cl oe of hu og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="ij ik il im in"><p id="2e8f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">感谢大家阅读这篇文章。请分享您宝贵的反馈或建议。快乐阅读！📗 🖌</p></div></div>    
</body>
</html>