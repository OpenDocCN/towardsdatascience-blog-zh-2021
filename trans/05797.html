<html>
<head>
<title>Toxicity in AI Text Generation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能文本生成中的毒性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/toxicity-in-ai-text-generation-9e9d9646e68f?source=collection_archive---------13-----------------------#2021-05-24">https://towardsdatascience.com/toxicity-in-ai-text-generation-9e9d9646e68f?source=collection_archive---------13-----------------------#2021-05-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/34bf7025dc829ec23e1df2f0c25b9d5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tzUrPqp11NCtMgy9GZqlLA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">Julia Nikulski创作的图形显示了聊天机器人在与用户对话时产生的有毒语言。由<a class="ae jg" href="https://www.flaticon.com/" rel="noopener ugc nofollow" target="_blank"> Flaticon </a>的<a class="ae jg" href="https://creativemarket.com/Becris" rel="noopener ugc nofollow" target="_blank"> Becris </a>和<a class="ae jg" href="https://www.freepik.com/" rel="noopener ugc nofollow" target="_blank"> Freepik </a>制作的图标。</p></figure><h2 id="2a63" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/fairness-and-bias" rel="noopener" target="_blank">公平和偏见</a></h2><div class=""/><div class=""><h2 id="f0f7" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">为什么语言模型会产生有害的输出，对此能做些什么</h2></div><p id="e0b4" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我最近实施了一个小型的NLP项目，在这个项目中，我问了两个开放域聊天机器人36个恋爱问题。<strong class="lj jt">最初作为消遣的东西，让我注意到了AI文本生成中的毒性问题</strong>。我最初问了语言模型(LM)GPT-2 36个问题。但我被该模型产生的一些答案震惊了，这些答案包含仇恨和有问题的语言，我决定不公布它的答案。</p><p id="d8bf" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">除了一些关于AI出问题的轶事之外，我以前没有处理过文本生成的这个方面。然而，我意识到，意识到在面向用户的项目中应用语言模型所带来的潜在危害是至关重要的。所以我阅读了为什么语言模型倾向于创造出如此令人憎恶的语言，以及如何解决这个问题。</p><p id="ea71" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">各种研究论文研究了自回归LMs固有的毒性和社会偏见——如GPT-2<a class="ae jg" href="https://www.aclweb.org/anthology/2020.findings-emnlp.301.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a><a class="ae jg" href="https://www.aclweb.org/anthology/2020.findings-emnlp.301.pdf" rel="noopener ugc nofollow" target="_blank"/>——以及双向编码器模型——如<a class="ae jg" href="https://www.aclweb.org/anthology/N19-1063.pdf" rel="noopener ugc nofollow" target="_blank">伯特</a><a class="ae jg" href="https://www.aclweb.org/anthology/N19-1063.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a>。本文概述了有毒语言的产生，并涵盖了其主要问题和解决方案。我讨论了文本生成中的毒性意味着什么，为什么会发生，以及目前是如何解决的。我还提出了一些与解毒语言模型相关的伦理考虑。</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi me"><img src="../Images/ec4583b18835dfaca14743fb3f01600d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*29zTmMN4mmcdiukX"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated"><a class="ae jg" href="https://unsplash.com/@rocknrollmonkey?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">摇滚猴子</a>在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="963f" class="mj mk jj bd ml mm mn mo mp mq mr ms mt ky mu kz mv lb mw lc mx le my lf mz na bi translated">1.什么是毒性？</h1><p id="abda" class="pw-post-body-paragraph lh li jj lj b lk nb kt lm ln nc kw lp lq nd ls lt lu ne lw lx ly nf ma mb mc im bi translated">正如<a class="ae jg" href="https://github.com/lilianweng" rel="noopener ugc nofollow" target="_blank"> Lilian Weng </a>在她的<a class="ae jg" href="https://lilianweng.github.io/lil-log/2021/03/21/reducing-toxicity-in-language-models.html" rel="noopener ugc nofollow" target="_blank">博客文章</a>中提到的，关于LMs毒性的确切定义各不相同。<a class="ae jg" href="https://jigsaw.google.com/" rel="noopener ugc nofollow" target="_blank"> Jigsaw </a>和谷歌反滥用技术团队开发了<a class="ae jg" href="https://www.perspectiveapi.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt">透视API </strong> </a>，<strong class="lj jt">识别在线对话中的有毒语言</strong>。他们将毒性定义为</p><blockquote class="ng nh ni"><p id="2e7b" class="lh li md lj b lk ll kt lm ln lo kw lp nj lr ls lt nk lv lw lx nl lz ma mb mc im bi translated">粗鲁、不尊重或不合理的评论可能会让你退出讨论。</p></blockquote><p id="f0ae" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt"> Dhamala等人</strong><a class="ae jg" href="https://arxiv.org/pdf/2101.11718.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="lj jt">【3】</strong></a><strong class="lj jt">创建了一个数据集和度量标准来调查开放式文本生成中的社会偏见和毒性</strong>。他们将有毒语言定义为传达内容</p><blockquote class="ng nh ni"><p id="731f" class="lh li md lj b lk ll kt lm ln lo kw lp nj lr ls lt nk lv lw lx nl lz ma mb mc im bi translated">不尊重、辱骂、不愉快和/或有害。</p></blockquote><p id="4022" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">Pavlopoulos等人<a class="ae jg" href="https://arxiv.org/pdf/2006.00998.pdf" rel="noopener ugc nofollow" target="_blank">【4】</a>研究了潜在毒性评论的背景是否有助于提高毒性检测模型的性能。他们认为<strong class="lj jt">毒性是一个<em class="md">总括术语</em>，包括几个亚型，如攻击性、辱骂性和仇恨性语言</strong>。基于这些定义，毒性不是一个明确定义的概念。这是一个宽泛的术语，涵盖了各种形式的攻击性、有问题的或有害的语言。</p><h1 id="f902" class="mj mk jj bd ml mm mn mo mp mq mr ms mt ky mu kz mv lb mw lc mx le my lf mz na bi translated">2.毒性检测与毒性产生</h1><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nm"><img src="../Images/5e83aef5c099f3fd85308ee1716a7ab3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d4k-PRw-warACDpklCh1mw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">Julia Nikulski制作的图形展示了有毒语言检测和有毒语言生成之间的区别。由<a class="ae jg" href="https://smashicons.com/" rel="noopener ugc nofollow" target="_blank"> Smashicons </a>、<a class="ae jg" href="https://www.flaticon.com/authors/dinosoftlabs" rel="noopener ugc nofollow" target="_blank">dinosoftlab</a>、<a class="ae jg" href="https://creativemarket.com/Becris" rel="noopener ugc nofollow" target="_blank"> Becris </a>、<a class="ae jg" href="https://www.freepik.com/" rel="noopener ugc nofollow" target="_blank"> Freepik </a>和<a class="ae jg" href="https://www.flaticon.com/authors/eucalyp" rel="noopener ugc nofollow" target="_blank">eucalype</a>从<a class="ae jg" href="https://www.flaticon.com/" rel="noopener ugc nofollow" target="_blank"> Flaticon </a>制作的图标。</p></figure><p id="c8bf" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在我为本文进行研究的过程中，我发现涉及NLP中毒性的学术论文可以大致分为两类，一类是涉及<strong class="lj jt">毒性语言检测的主题，另一类是毒性语言生成</strong>。虽然这篇博客文章的重点是有毒语言的生成，但是在寻找有毒语言模型的解决方案时，对有毒语言检测的一般理解是有帮助的。</p><p id="be76" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">各种研究专注于开发自动系统来检测人类书写的有毒语言，并在网上讨论中流行。虽然骂人的话似乎表明有毒，但没有这种话的语言仍然可能有毒。简单的“<em class="md">我一看就知道</em>”——在构建数据集和模型来检测毒性时，这种方法是不够的。语境和语言中的细微差别很重要，决定什么导致冒犯通常是主观的<a class="ae jg" href="https://arxiv.org/pdf/2102.00086.pdf" rel="noopener ugc nofollow" target="_blank">【6】</a>。Sheth等人<a class="ae jg" href="https://arxiv.org/pdf/2104.10788.pdf" rel="noopener ugc nofollow" target="_blank">【7】</a>对与定义和识别有毒物质相关的挑战进行了出色的概述。</p><p id="3d0a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在这一毒性检测领域，存在<strong class="lj jt">偏差检测模型</strong>的问题。用于检测毒性的自然语言理解(NLU)模型已被证明包括偏见——特别是对少数民族产生的文本和少数民族的身份提及<a class="ae jg" href="https://www.aclweb.org/anthology/2020.findings-emnlp.301.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a>——这限制了模型正确标记有毒语言的能力<a class="ae jg" href="https://arxiv.org/pdf/2102.00086.pdf" rel="noopener ugc nofollow" target="_blank">【6】</a>。通过将无毒内容标记为有毒来压制少数群体的声音也引发了伦理问题。</p><p id="b56c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">与种族、性别、宗教、民族、性取向和其他受保护身份相关的社会偏见也存在于为生成文本而设计的<strong class="lj jt"> LMs中</strong>。它们体现在这些模型产生的有毒语言中。这些LMs还输出包含诅咒语、威胁和侮辱的语言。Gehman等人<a class="ae jg" href="https://www.aclweb.org/anthology/2020.findings-emnlp.301.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a>表明，五种不同的LMs——包括GPT、GPT-2和GPT-3——<strong class="lj jt">在100代中均产生至少一种毒性评论</strong>。即使相对无害的输入作为提示提供给这些模型，也会导致有毒文本的生成。此处显示了该研究的互动概述<a class="ae jg" href="https://toxicdegeneration.allenai.org/" rel="noopener ugc nofollow" target="_blank">。</a></p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nn"><img src="../Images/84a07608c9b7c64e14ca5b47e5d89e43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PYIMIntu2Tf05ujj"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">马库斯·温克勒在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="8dda" class="mj mk jj bd ml mm mn mo mp mq mr ms mt ky mu kz mv lb mw lc mx le my lf mz na bi translated">3.为什么会出现毒性？</h1><p id="cb92" class="pw-post-body-paragraph lh li jj lj b lk nb kt lm ln nc kw lp lq nd ls lt lu ne lw lx ly nf ma mb mc im bi translated">最先进的(SOTA)语言模型——如GPT-2和GPT-3——使用来自网络的大型文本语料库进行预训练。LMs学习预测序列中的下一个单词(或句子中的单词)。如果模型被输入包含诅咒语或暴力语言的训练数据，它将在训练阶段学习预测这些单词，并在稍后生成包含它们的输出。在通过这些模型进行推断的过程中，也学习和复制了具有刻板印象、贬低或忽略受保护身份的有偏见的语言。</p><p id="0dfe" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/dirty-secrets-of-bookcorpus-a-key-dataset-in-machine-learning-6ee2927e8650">对<a class="ae jg" href="https://huggingface.co/datasets/bookcorpus" rel="noopener ugc nofollow" target="_blank"> BookCorpus </a>的一项调查</a>——一个经常用于预训练包含超过11，000本书的LMs的数据集——显示它包括<strong class="lj jt">与性别有关的有问题的内容，以及对流派、宗教和作者的歪曲表现</strong>。<a class="ae jg" href="https://skylion007.github.io/OpenWebTextCorpus/" rel="noopener ugc nofollow" target="_blank">OpenWebTextCorpus</a>——一个重现用于GPT-2的训练数据的数据集——包含Reddit上出站链接的内容。Gehman等人<a class="ae jg" href="https://www.aclweb.org/anthology/2020.findings-emnlp.301.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a>证明<strong class="lj jt">该数据集包含至少50，000个毒性句子，根据<a class="ae jg" href="https://www.perspectiveapi.com/" rel="noopener ugc nofollow" target="_blank"> Perspective API </a>测得的</strong>毒性分数(可解释为概率)为0.51或更高。</p><p id="02bf" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如前所述，在确定什么是有毒的时候，环境很重要。不幸的是，<strong class="lj jt">在特定语境中无毒的语言可以被语言模型</strong>重新“洗牌”，当给予暗示性提示<a class="ae jg" href="https://www.aclweb.org/anthology/2020.findings-emnlp.301.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a>时，语言模型会产生有毒的输出。为了说明这一点，你可以想一想在客观描述犯罪时使用的词语。这些词然后被模型应用到一个句子中来表达威胁，清楚地显示出毒性。因此，控制预训练LMs的生成过程以及使用干净无毒的数据集进行预训练对于避免有毒输出非常重要<a class="ae jg" href="https://www.aclweb.org/anthology/2020.findings-emnlp.301.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a>。</p><h1 id="97cc" class="mj mk jj bd ml mm mn mo mp mq mr ms mt ky mu kz mv lb mw lc mx le my lf mz na bi translated">4.如何减少毒性的产生？</h1><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi no"><img src="../Images/d825d5a82a4823fee78926ac827066b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tcAfEICkloFZyJdP8Zod4Q.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">由Julia Nikulski创建的图形显示了语言模型的不同解毒策略。由<a class="ae jg" href="https://www.flaticon.com/" rel="noopener ugc nofollow" target="_blank"> Flaticon </a>的<a class="ae jg" href="https://smashicons.com/" rel="noopener ugc nofollow" target="_blank"> Smashicons </a>、<a class="ae jg" href="https://www.flaticon.com/authors/dinosoftlabs" rel="noopener ugc nofollow" target="_blank">dinosoftlab</a>和<a class="ae jg" href="https://www.freepik.com/" rel="noopener ugc nofollow" target="_blank"> Freepik </a>制作的图标。</p></figure><p id="79c0" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">许多在复杂性和资源强度上不同的方法旨在减少语言模型产生的有害内容。我将简单介绍一些方法，并列出它们的优点和局限性。列出所有可用的方法超出了本文的范围。</p><p id="3562" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">根据geh man et al .<a class="ae jg" href="https://www.aclweb.org/anthology/2020.findings-emnlp.301.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a>，这些方法可以分为基于<strong class="lj jt">数据或基于解码的策略</strong>。基于数据的策略包括模型的额外预训练和改变模型参数，这使得这些方法在计算上很昂贵。基于解码的方法只是修改了LM的解码算法，模型参数保持不变<a class="ae jg" href="https://www.aclweb.org/anthology/2020.findings-emnlp.301.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a>。<strong class="lj jt">因此，解码策略通常具有从业者更容易获得且成本更低的优势</strong>。</p><p id="999a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">策略名称中的链接指向解释它们的博客文章(如果有的话)或介绍这些方法的原始学术论文。</p><h2 id="1742" class="np mk jj bd ml nq nr dn mp ns nt dp mt lq nu nv mv lu nw nx mx ly ny nz mz jp bi translated"><a class="ae jg" href="https://medium.com/jasonwu0731/pre-finetuning-domain-adaptive-pre-training-of-language-models-db8fa9747668" rel="noopener">领域自适应预训练(DAPT) </a></h2><p id="16a4" class="pw-post-body-paragraph lh li jj lj b lk nb kt lm ln nc kw lp lq nd ls lt lu ne lw lx ly nf ma mb mc im bi translated"><strong class="lj jt">基于数据的策略</strong>:使用无毒数据集<a class="ae jg" href="https://arxiv.org/pdf/2004.10964.pdf" rel="noopener ugc nofollow" target="_blank"> [ </a> <a class="ae jg" href="https://www.aclweb.org/anthology/2020.findings-emnlp.301.pdf" rel="noopener ugc nofollow" target="_blank"> 1 </a>，<a class="ae jg" href="https://arxiv.org/pdf/2004.10964.pdf" rel="noopener ugc nofollow" target="_blank"> 9 </a>，<a class="ae jg" href="https://arxiv.org/pdf/2104.06390.pdf" rel="noopener ugc nofollow" target="_blank"> 10 </a>对LM进行额外的预训练。<strong class="lj jt">优势</strong>:最有效的减毒策略之一<a class="ae jg" href="https://www.aclweb.org/anthology/2020.findings-emnlp.301.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a>。<strong class="lj jt">局限性</strong>:计算量大<a class="ae jg" href="https://www.aclweb.org/anthology/2020.findings-emnlp.301.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a>。需要用于训练的额外数据，当通过众包利用人类时，收集这些数据可能会很昂贵。</p><h2 id="ba06" class="np mk jj bd ml nq nr dn mp ns nt dp mt lq nu nv mv lu nw nx mx ly ny nz mz jp bi translated"><a class="ae jg" href="https://www.aclweb.org/anthology/2020.findings-emnlp.301.pdf" rel="noopener ugc nofollow" target="_blank">属性条件</a>(基于<a class="ae jg" href="https://arxiv.org/abs/1909.05858" rel="noopener ugc nofollow" target="_blank"> CTRL </a>)</h2><p id="3246" class="pw-post-body-paragraph lh li jj lj b lk nb kt lm ln nc kw lp lq nd ls lt lu ne lw lx ly nf ma mb mc im bi translated"><strong class="lj jt">基于数据的策略</strong>:使用添加了“有毒”或“无毒”属性的训练样本进行进一步的LM预训练。在推断(文本生成)过程中，属性“无毒”可以添加到给予模型<a class="ae jg" href="https://www.aclweb.org/anthology/2020.findings-emnlp.301.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a>的提示中。<strong class="lj jt">局限性</strong>:计算开销大。不如DAPT、PPLM和词汇转换<a class="ae jg" href="https://www.aclweb.org/anthology/2020.findings-emnlp.301.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a>有效。</p><h2 id="59cc" class="np mk jj bd ml nq nr dn mp ns nt dp mt lq nu nv mv lu nw nx mx ly ny nz mz jp bi translated"><a class="ae jg" href="https://lilianweng.github.io/lil-log/2021/03/21/reducing-toxicity-in-language-models.html" rel="noopener ugc nofollow" target="_blank">阻止列表(文字过滤)</a></h2><p id="bbf6" class="pw-post-body-paragraph lh li jj lj b lk nb kt lm ln nc kw lp lq nd ls lt lu ne lw lx ly nf ma mb mc im bi translated"><strong class="lj jt">基于解码的策略</strong>:在LM中，不需要的单词——如诅咒语、亵渎语和侮辱语——被分配零概率，以防止它们被生成(参见<a class="ae jg" href="https://www.aclweb.org/anthology/2020.findings-emnlp.301.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a>和<a class="ae jg" href="https://lilianweng.github.io/lil-log/2021/03/21/reducing-toxicity-in-language-models.html" rel="noopener ugc nofollow" target="_blank">此处</a>)。<strong class="lj jt">优点</strong>:易于实现。<strong class="lj jt">局限性</strong>:没有这些词的毒性<a class="ae jg" href="https://lilianweng.github.io/lil-log/2021/03/21/reducing-toxicity-in-language-models.html" rel="noopener ugc nofollow" target="_blank">仍然会发生</a>。这些词可以被接受使用的上下文被忽略。</p><h2 id="03f5" class="np mk jj bd ml nq nr dn mp ns nt dp mt lq nu nv mv lu nw nx mx ly ny nz mz jp bi translated"><a class="ae jg" href="https://lilianweng.github.io/lil-log/2021/03/21/reducing-toxicity-in-language-models.html" rel="noopener ugc nofollow" target="_blank">词汇转移</a></h2><p id="13c6" class="pw-post-body-paragraph lh li jj lj b lk nb kt lm ln nc kw lp lq nd ls lt lu ne lw lx ly nf ma mb mc im bi translated"><strong class="lj jt">基于解码的策略</strong>:使用模型词汇中每个记号的毒性和非毒性的二维表示来提高生成非毒性记号的概率<a class="ae jg" href="https://www.aclweb.org/anthology/2020.findings-emnlp.301.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a>。<strong class="lj jt">优势</strong>:显示出比阻断列表<a class="ae jg" href="https://www.aclweb.org/anthology/2020.findings-emnlp.301.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a>更好地降低毒性。<strong class="lj jt">限制</strong>:实现起来比阻止列表更复杂。</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oa"><img src="../Images/4d5c10dc742d69e2296bb4ec27c2597f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*AWKjkLHFhwPEX5yj"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">照片由<a class="ae jg" href="https://unsplash.com/@memee_the_muse?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Breanna Louise </a>在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h2 id="0ada" class="np mk jj bd ml nq nr dn mp ns nt dp mt lq nu nv mv lu nw nx mx ly ny nz mz jp bi translated"><a class="ae jg" href="https://analyticsindiamag.com/what-are-plug-and-play-language-models-pplm-nlp/" rel="noopener ugc nofollow" target="_blank">即插即用语言模型(PPLM) </a></h2><p id="d685" class="pw-post-body-paragraph lh li jj lj b lk nb kt lm ln nc kw lp lq nd ls lt lu ne lw lx ly nf ma mb mc im bi translated"><strong class="lj jt">基于解码的策略</strong>:使用一个简单的模型(词袋或单层分类器)作为鉴别器(或属性模型)，通过改变LM的隐藏表示来指导其语言生成[ <a class="ae jg" href="https://www.aclweb.org/anthology/2020.findings-emnlp.301.pdf" rel="noopener ugc nofollow" target="_blank"> 1 </a>，<a class="ae jg" href="https://arxiv.org/pdf/1912.02164.pdf" rel="noopener ugc nofollow" target="_blank"> 11 </a> ]。<strong class="lj jt">优点</strong>:最有效的减毒策略之一<a class="ae jg" href="https://www.aclweb.org/anthology/2020.findings-emnlp.301.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a>。<strong class="lj jt">限制</strong>:计算量非常大。</p><h2 id="50ad" class="np mk jj bd ml nq nr dn mp ns nt dp mt lq nu nv mv lu nw nx mx ly ny nz mz jp bi translated"><a class="ae jg" href="https://blog.einstein.ai/gedi/" rel="noopener ugc nofollow" target="_blank">生成鉴别器(GeDi) </a></h2><p id="2be8" class="pw-post-body-paragraph lh li jj lj b lk nb kt lm ln nc kw lp lq nd ls lt lu ne lw lx ly nf ma mb mc im bi translated"><strong class="lj jt">基于解码的策略</strong>:使用属性条件(或类别条件)LM作为鉴别器，它使用贝叶斯规则为主LM可以生成的所有潜在的下一个令牌计算类别可能性(例如，有毒或无毒)。优势:计算效率比PPLM高。在解毒方面胜过PPLM。<strong class="lj jt">局限性</strong>:仍然是比较复杂的策略之一。</p><h2 id="b7e3" class="np mk jj bd ml nq nr dn mp ns nt dp mt lq nu nv mv lu nw nx mx ly ny nz mz jp bi translated"><a class="ae jg" href="https://lilianweng.github.io/lil-log/2021/03/21/reducing-toxicity-in-language-models.html" rel="noopener ugc nofollow" target="_blank">自去偏置</a></h2><p id="5462" class="pw-post-body-paragraph lh li jj lj b lk nb kt lm ln nc kw lp lq nd ls lt lu ne lw lx ly nf ma mb mc im bi translated"><strong class="lj jt">基于解码的策略</strong>:通过在提供给LM [ <a class="ae jg" href="http://arxiv.org/abs/2103.00453" rel="noopener ugc nofollow" target="_blank"> 13 </a> ]的输入提示前添加一个简短的属性描述(如“以下文本包含毒性”)，使用自去偏置算法来降低有毒单词产生的概率。<strong class="lj jt">优势</strong>:与CTRL策略相比，不需要额外的培训。<a class="ae jg" href="https://lilianweng.github.io/lil-log/2021/03/21/reducing-toxicity-in-language-models.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt">限制</strong> </a>:该策略目前仅使用<a class="ae jg" href="https://www.perspectiveapi.com/" rel="noopener ugc nofollow" target="_blank"> Perspective API </a>提供的毒性和偏差属性进行评估。它经常过滤掉无害的单词。它的解毒能力受限于模型对相关偏差和毒性的“意识”[ <a class="ae jg" href="http://arxiv.org/abs/2103.00453" rel="noopener ugc nofollow" target="_blank"> 13 </a> ]。</p><p id="826d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在受控世代的保护伞下，还有各种其他降低毒性的策略[ <a class="ae jg" href="https://arxiv.org/pdf/1912.02164.pdf" rel="noopener ugc nofollow" target="_blank"> 11 </a> ]<a class="ae jg" href="https://lilianweng.github.io/lil-log/2021/03/21/reducing-toxicity-in-language-models.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt">文本风格转移</strong> </a>是一种将有毒句子翻译成无毒版本的相关方法。然而，它的主要应用一直与解决人类在社交媒体上产生的有毒文本有关，而不是消除语言模型的毒性。</p><p id="2504" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">Gehman等人<a class="ae jg" href="https://www.aclweb.org/anthology/2020.findings-emnlp.301.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a>在他们的调查中发现，网络文本语料库和神经语言生成都包含大量的有毒内容。他们的结果表明，没有一种解毒策略能清除所有的有毒语言。然而，这些策略降低了毒性分数和在25个文本生成中产生一次毒性评论的概率。总的来说，这显示了基于解码的方法的<strong class="lj jt">前景，同时也强调了为语言模型仔细选择预训练数据集</strong>的重要性。</p><h1 id="851c" class="mj mk jj bd ml mm mn mo mp mq mr ms mt ky mu kz mv lb mw lc mx le my lf mz na bi translated">5.解毒和语言模型的伦理考量</h1><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nm"><img src="../Images/90ecaa8dce66fcdbae1ac7e6e7b148ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8MlQCqN8n2eLqbqXjDZXdg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">Julia Nikulski创作的图表展示了偏见对语言解毒的影响。从<a class="ae jg" href="https://www.flaticon.com/" rel="noopener ugc nofollow" target="_blank"> Flaticon </a>中<a class="ae jg" href="https://www.flaticon.com/authors/vitaly-gorbachev" rel="noopener ugc nofollow" target="_blank"> Vitaly Gorbachev </a>、<a class="ae jg" href="https://smashicons.com/" rel="noopener ugc nofollow" target="_blank"> Smashicons </a>、<a class="ae jg" href="https://www.flaticon.com/authors/dinosoftlabs" rel="noopener ugc nofollow" target="_blank">dinosoftlab</a>、<a class="ae jg" href="https://creativemarket.com/Becris" rel="noopener ugc nofollow" target="_blank"> Becris </a>和<a class="ae jg" href="https://www.freepik.com/" rel="noopener ugc nofollow" target="_blank"> Freepik </a>制作的图标。</p></figure><p id="f2ed" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">第2节简要地谈到了毒性检测中的偏见问题，这种偏见经常以少数人的言论被误认为有毒为代价。这种偏见也存在于解毒的语言模型中。<strong class="lj jt">如果少数民族身份提及和少数民族方言被解毒策略归类为有毒</strong>，那么<strong class="lj jt">少数民族和NLP系统之间的交流就会受阻</strong>。LMs无法理解和响应用户提示可能被视为微侵犯[ <a class="ae jg" href="https://arxiv.org/pdf/2104.06390.pdf" rel="noopener ugc nofollow" target="_blank"> 10 </a> ]。</p><p id="4d66" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在无毒的环境中，有偏见的解毒也会导致回避某些话题，如宗教或性别。当用户与NLP系统交互时，这种回避可能会导致自我污名化，因为他们可能会觉得自己的身份和现实无关[ <a class="ae jg" href="https://arxiv.org/pdf/2104.06390.pdf" rel="noopener ugc nofollow" target="_blank"> 10 </a> ]。因此，开发语言模型的解毒技术以及以减少偏差的方式设计它们是至关重要的。</p><p id="9bab" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">妇女和少数群体成员参与设计过程</strong>有助于提高认识，减少这些系统中的偏见。此外，如果仔细选择，用于语言模型和解毒策略的预训练的数据集可以减少毒性和偏差。由谁来决定对哪些数据语言模型进行训练是一个复杂的问题，没有简单的答案。但是，需要讨论。</p><p id="2891" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">另外，在创建数据集时需要考虑<strong class="lj jt">标注和</strong><a class="ae jg" href="https://arxiv.org/pdf/2104.06390.pdf" rel="noopener ugc nofollow" target="_blank">10</a>。注释偏差意味着负责创建标记数据集和确定句子毒性的人类可能会错误地将少数民族方言标记为有毒。抽样偏见增加了偏见解毒，因为有毒语言往往针对少数群体，在少数群体身份提及和有毒词语之间建立了关联。</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ob"><img src="../Images/c83694d905fc57f30e18ad3f24f1830c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ivBg2pN1xwebR6ID"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">照片由<a class="ae jg" href="https://unsplash.com/@aleksandraboguslawska?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">亚历山大拉·博古斯拉夫斯卡</a>在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><h1 id="1e08" class="mj mk jj bd ml mm mn mo mp mq mr ms mt ky mu kz mv lb mw lc mx le my lf mz na bi translated">最后的想法</h1><p id="5483" class="pw-post-body-paragraph lh li jj lj b lk nb kt lm ln nc kw lp lq nd ls lt lu ne lw lx ly nf ma mb mc im bi translated">语言模型中的毒性是一个比我预期的更加复杂和广泛的话题，无论是从技术实现还是伦理考虑。喷出仇恨和有毒语言、放大刻板印象和社会偏见的语言模型不利于公共话语、心理健康和民主制度。同时，有偏见的解毒策略会导致某些话题和无毒语言的污名化，压制少数群体的声音。因此，<strong class="lj jt">实现负责任、无毒且无偏见的LMs需要细微差别、偏见意识以及最终用户参与模型设计的所有步骤</strong>。</p><p id="211c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在写这篇文章的时候，<strong class="lj jt">我考虑了我处理NLP模型的责任</strong>。这项研究告诉我，让语言模型面向用户可能会有严重的影响，应该经过深思熟虑。我还决定不发表在我的<a class="ae jg" rel="noopener" target="_blank" href="/36-questions-to-fall-in-love-with-ai-89037cc040ab"> 36问题项目</a>中产生的任何有害内容，因为我不希望未来通过抓取网络创建的任何数据集包含这些有问题的例子。</p></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><p id="1889" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">你想在媒体上阅读更多高质量的故事吗？考虑注册一个支持我和其他媒体作者的会员。</p><div class="is it gp gr iu oj"><a href="https://medium.com/@julia.nikulski/membership" rel="noopener follow" target="_blank"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd jt gy z fp oo fr fs op fu fw js bi translated">通过我的推荐链接加入Medium-Julia Nikulski</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">作为一个媒体会员，你的会员费的一部分给了你所阅读的作家，你可以在…上看到所有的故事</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">medium.com</p></div></div><div class="os l"><div class="ot l ou ov ow os ox ja oj"/></div></div></a></div></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><p id="a5a2" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">您是否希望展示您的数据科学项目，但不确定如何展示？查看我的指南<strong class="lj jt">如何建立数据科学组合网站</strong>:</p><div class="is it gp gr iu oj"><a rel="noopener follow" target="_blank" href="/how-to-build-a-data-science-portfolio-website-335b0f253822"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd jt gy z fp oo fr fs op fu fw js bi translated">如何建立数据科学作品集网站</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">展示您的作品——从零开始创建网站</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">towardsdatascience.com</p></div></div><div class="os l"><div class="oy l ou ov ow os ox ja oj"/></div></div></a></div></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><h1 id="2e2f" class="mj mk jj bd ml mm oz mo mp mq pa ms mt ky pb kz mv lb pc lc mx le pd lf mz na bi translated">学术论文参考文献</h1><p id="62b7" class="pw-post-body-paragraph lh li jj lj b lk nb kt lm ln nc kw lp lq nd ls lt lu ne lw lx ly nf ma mb mc im bi translated">[1]盖曼，s .，古鲁兰根，s .，萨普，m .，崔，y .，史密斯，N. A. (2020)。真实毒性提示:评估语言模型中的神经毒性退化。 <em class="md">计算语言学协会的发现:EMNLP 2020 </em>，3356–3369。</p><p id="f9c8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[2]梅，c .，王，a .，博迪亚，s .，鲍曼，S. R .，&amp;鲁丁格，R. (2019)。<a class="ae jg" href="https://doi.org/10.18653/v1/N19-1063" rel="noopener ugc nofollow" target="_blank">在句子编码器中测量社会偏见</a>。<em class="md">计算语言学协会北美分会2019年会议论文集:人类语言技术，第1卷(长短论文)</em>，622–628。</p><p id="ddac" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[3] Dhamala，j .，Sun，t .，Kumar，v .，Krishna，s .，Pruksachatkun，y .，Chang，K.-W .，&amp; Gupta，R. (2021)。<a class="ae jg" href="https://arxiv.org/pdf/2101.11718.pdf" rel="noopener ugc nofollow" target="_blank"> BOLD:用于测量开放式语言生成偏差的数据集和指标。</a><em class="md">《2021年美国计算机学会公平、问责和透明会议论文集</em>，862–872页。</p><p id="8d81" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[4] Pavlopoulos，j .，Sorensen，j .，Dixon，l .，Thain，n .，Androutsopoulos，I. (2020年)。<a class="ae jg" href="http://arxiv.org/abs/2006.00998" rel="noopener ugc nofollow" target="_blank">毒性检测:环境真的重要吗？</a><em class="md">ArXiv:2006.00998【Cs】</em>。</p><p id="44b9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[5]瓦伊迪亚，a .，麦，f .，&amp;宁，Y. (2020)。<a class="ae jg" href="http://arxiv.org/abs/1909.09758" rel="noopener ugc nofollow" target="_blank">多任务学习降低有毒评论检测中模型偏倚的实证分析。</a><em class="md">ArXiv:1909.09758【Cs】</em>。</p><p id="2ed7" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[6]周，x .，Sap，m .，Swayamdipta，s .，Smith，N. A .，，Choi，Y. (2021)。<a class="ae jg" href="http://arxiv.org/abs/2102.00086" rel="noopener ugc nofollow" target="_blank">有毒语言检测自动化去偏的挑战。</a><em class="md">ArXiv:2102.00086【Cs】</em>。</p><p id="666c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[7] Sheth，a .，Shalin，V. L .，&amp; Kursuncu，U. (2021年)。<a class="ae jg" href="http://arxiv.org/abs/2104.10788" rel="noopener ugc nofollow" target="_blank">定义和检测社交媒体的毒性:背景和知识是关键。</a><em class="md">ArXiv:2104.10788【Cs】</em>。</p><p id="5d14" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[8]t . Dias Oliva，d . m . Antonia lli和a . Gomes(2021年)。<a class="ae jg" href="https://doi.org/10.1007/s12119-020-09790-w" rel="noopener ugc nofollow" target="_blank">战斗仇恨言论，消音变装女王？内容调节中的人工智能与LGBTQ在线之声的风险。</a> <em class="md">性&amp;文化</em>，<em class="md"> 25 </em> (2)，700–732。</p><p id="dc48" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[9] Gururangan，s .，Marasovi，a .，Swayamdipta，s .，Lo，k .，Beltagy，I .，Downey，d .，和Smith，N. A. (2020年)。不要停止预训练:让语言模型适应领域和任务。<em class="md">ArXiv:2004.10964【Cs】</em>。</p><p id="0096" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[10]徐，a .，帕塔克，e .，华莱士，e .，古鲁兰根，s .，萨普，m .，&amp;克莱因，D. (2021)。<a class="ae jg" href="https://arxiv.org/pdf/2104.06390.pdf" rel="noopener ugc nofollow" target="_blank">去除语言模型的毒性有边缘化少数群体声音的风险。</a><em class="md">ArXiv:2104.06390【Cs】</em>。</p><p id="a109" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[11] Dathathri，s .，Madotto，a .，Lan，j .，Hung，j .，Frank，e .，Molino，p .，Yosinski，j .，和Liu，R. (2020年)。<a class="ae jg" href="http://arxiv.org/abs/1912.02164" rel="noopener ugc nofollow" target="_blank">即插即用语言模型:控制文本生成的简单方法</a>。<em class="md">ArXiv:1912.02164【Cs】</em>。</p><p id="4770" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[12] Krause，b .，Gotmare，A. D .，McCann，b .，Keskar，N. S .，Joty，s .，Socher，r .，&amp; Rajani，N. F. (2020年)。<a class="ae jg" href="http://arxiv.org/abs/2009.06367" rel="noopener ugc nofollow" target="_blank"> GeDi:生成鉴别器引导的序列生成。</a><em class="md">ArXiv:2009.06367【Cs】</em>。</p><p id="d8f2" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[13]茨韦塔纳·希克、乌杜帕和舒茨(2021年)。<a class="ae jg" href="http://arxiv.org/abs/2103.00453" rel="noopener ugc nofollow" target="_blank">自我诊断和自我去偏见:一个减少自然语言处理中基于语料库的偏见的建议。</a><em class="md">ArXiv:2103.00453【Cs】</em>。</p></div></div>    
</body>
</html>