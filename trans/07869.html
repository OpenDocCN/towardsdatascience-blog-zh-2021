<html>
<head>
<title>Applied Bayesian Inference with PyMC3 Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyMC3应用贝叶斯推理第1部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/applied-bayesian-inference-pt-1-322b25093f62?source=collection_archive---------14-----------------------#2021-07-19">https://towardsdatascience.com/applied-bayesian-inference-pt-1-322b25093f62?source=collection_archive---------14-----------------------#2021-07-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="4ffe" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="0d05" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">条件世界导论</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/c2397297a8b867755688bb0fc4d16c5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hSxE3MtU1T9-_5kS-GmvTQ.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">毛伊岛上空的银河。作者图片</p></figure><p id="931f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这个故事是为那些已经稍微熟悉统计学和Python，并希望将他们的技能提升到一个新水平的人而写的。我将从哲学开始，然后尝试直接用Python实现概念，为您构建一个更具可操作性的指南。我已经无数次读到过这个世界，但直到我实际应用它之前，我对自己的能力没有信心，所以我恳求你找到一个你感兴趣的数据集，然后一头扎进去！</p><p id="9101" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">有帮助的先决条件:熟悉统计学直到假设检验，初级到中级Python技能。</p><p id="35ab" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在这个故事中，我将介绍贝叶斯思维、概率编程介绍，以及一个用PyMC3 &amp; ArviZ掷硬币的例子。</p><h1 id="b093" class="md me it bd mf mg mh mi mj mk ml mm mn ki mo kj mp kl mq km mr ko ms kp mt mu bi translated"><strong class="ak">思维贝叶斯</strong></h1><p id="aa97" class="pw-post-body-paragraph lh li it lj b lk mv kd lm ln mw kg lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">当我们听到“统计”这个词时，我们通常会更多地考虑长期运行的频率。我们的脑海中会立即浮现出这样的例子:<em class="na">硬币正面朝上的概率，在六面骰子上掷出3的概率，闪电击中的概率</em>。举个例子:如果你想知道一枚硬币正面朝上的概率，那么你可以想象一个世界，你把一枚硬币抛了无数次，然后看看在令人难以置信的大量抛硬币或试验中，它有多少次正面朝上。这个经典的思想流派被称为<strong class="lj jd">频率主义者</strong>，其中概率被认为是经过多次试验后某一事件相对频率的极限。有些情况下，“长期频率”思维没有太多的逻辑意义，比如:<em class="na">考试得A的概率</em>和<em class="na">选举结果的概率。</em>频率主义者通过调用替代现实来解决这些问题，并说在所有这些现实中，发生的频率决定了可能性。本质上，频率主义者将参数(测试统计试图估计的)视为确定性的，或非随机的变量。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/8e655f1c804441bb83689f119aef02ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:396/format:webp/1*KE3AHh-qNbaTuJegmrR47g.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">当n趋近于无穷大时，真实概率就接近于。作者图片</p></figure><p id="f206" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">与这一学派相反的是<strong class="lj jd">贝叶斯</strong>，其中概率表达了对一个事件的相信程度。因为这篇文章的大部分内容将在贝叶斯世界中进行，所以让我举例来介绍这种思维方式。</p><p id="f7f7" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这个例子来自行为心理学家丹尼尔·卡内曼和阿莫斯·特沃斯基，在<em class="na">思考快与慢</em> (Kahneman，2011)中得到推广:</p><p id="433b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">“史蒂夫非常害羞和孤僻，总是乐于助人，但对人或现实世界不感兴趣。温顺整洁的灵魂。他需要秩序和结构，热爱细节。”</p><p id="82c2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">史蒂夫更有可能成为图书管理员还是农民？</p><p id="33c1" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">他们发现绝大多数人选择图书管理员(90%)而不是农民(10%)。不管答案如何，丹尼尔&amp;阿莫斯发现，人们甚至不考虑世界上图书馆员与农民的比例来做出决定，这是不合理的。即使是关于图书馆员和农民的比例，这也不是关于人们实际上知道真实的答案，而是关于思考去<em class="na">估计</em>它。正如来自<a class="ae nc" href="https://www.youtube.com/watch?v=HZGCoVF3YvM" rel="noopener ugc nofollow" target="_blank"> 3Blue1Brown </a>的Grant Sanderson如此雄辩地指出的那样，这一点可以用图形更好地说明:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/2b5141c5a53d9d105abb2deab043d7ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*qZHoJfv0yyhOYI58Ijqr2A.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="69e1" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们说，在我们的估计中，我们认为图书管理员约占农民总数的1/21。我们的目标是根据描述找出史蒂夫成为图书管理员的可能性。假设我们发现证据表明，在10名图书管理员和200名农民中，有4名图书管理员和20名农民符合这个描述。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ne"><img src="../Images/9c74e38d86a79ebeb4509039d7d6bb02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RgVS9jljgLaZGP4744u4bA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="49b5" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，考虑到我们的信念、证据和目标，我们可以宣称概率为:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/23938a8268e21b316cb57f0275eee917.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*Y_Zxw2k3i2aHEvVcEUt7zw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="5936" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在让我们说，在对这个话题做了更多的阅读之后，我们发现图书馆员与农民的比例并没有那么低。那么，在这种情况下，我们所要做的就是更新我们先前的信念(实质上是加宽细长的蓝色条)并重新计算。这是贝叶斯哲学的真正精髓；没有“对”或“错”的概念，因为你在用你先前的知识和新的证据不断更新你对世界的理解。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ng"><img src="../Images/1ca97baea2e025dd02e1fff3bda5b733.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UKJhxKZ9x7Mk53dCOFmZnw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="ed97" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">本质上，贝叶斯主义者将参数(最有可能解释数据的分布)视为随机变量。下面是我们的例子如何映射到贝叶斯世界中的术语:</p><ul class=""><li id="042b" class="nh ni it lj b lk ll ln lo lq nj lu nk ly nl mc nm nn no np bi translated">先验:P(H) = 1/21。在看到证据之前我们所知道的参数值。</li><li id="af7e" class="nh ni it lj b lk nq ln nr lq ns lu nt ly nu mc nm nn no np bi translated">证据(又名边际可能性):P(E) = 24。证据E不考虑事件的可能性有多大。</li><li id="829f" class="nh ni it lj b lk nq ln nr lq ns lu nt ly nu mc nm nn no np bi translated">可能性:P(E|H) = 4/10 = 2/5。给定假设证据有多大可能？</li><li id="c252" class="nh ni it lj b lk nq ln nr lq ns lu nt ly nu mc nm nn no np bi translated">后验:P(H|E) = 4/24 = 1/6。给定证据，假设的可能性有多大。</li></ul><p id="460f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">综上所述，我们得到:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/f2b8547594494e89bf6af324658b330c.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*G2VCFv9YPAU7h_ClGzWVRw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="132f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">当我们把它简化时，我们得到:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/0eb652e3fa77d5a734144588f9db83c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*_ePcPEftlP8pgn_IsdcByQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">贝叶斯定理。作者图片</p></figure><p id="a31e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">顺便说一下，由于在贝叶斯世界中获得洞见的“主观”本质，它可能会被混淆，并很容易被简化为仅仅是观点。这可能是一个懒惰的概括，因为所有的统计(频率主义者和贝叶斯)都需要假设，无论在哪里做出假设，从数据到见解，你都有一个相对主观的工艺。</p><p id="4d51" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这说明了数据的真正价值，因为当我们收集无限量的数据(或证据)时，比如说当<em class="na"> N </em>接近无穷大时，贝叶斯结果往往与频率主义者的结果一致。因此可以推断，统计推断对于大的<em class="na"> N. </em>来说或多或少是客观的。问题是我们通常没有无限的资源(时间、能量、数据、计算等)。)，所以我们不得不在现有证据较少的世界里工作。在频率主义者的世界里，这将产生具有更大方差和更大置信区间的结果。贝叶斯分析在这里可以大放异彩，因为你保留了不确定性，突出了小数据集上统计推断的不稳定性。请注意，这并不意味着贝叶斯在小数据集上比Frequentist“更准确”。两者都需要假设才能获得洞察力，数据集越小，需要做出的假设就越多/越广泛。</p></div><div class="ab cl nx ny hx nz" role="separator"><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc od"/><span class="oa bw bk ob oc"/></div><div class="im in io ip iq"><h1 id="298f" class="md me it bd mf mg oe mi mj mk of mm mn ki og kj mp kl oh km mr ko oi kp mt mu bi translated">概率规划</h1><p id="b620" class="pw-post-body-paragraph lh li it lj b lk mv kd lm ln mw kg lp lq mx ls lt lu my lw lx ly mz ma mb mc im bi translated">请记住，我们的目标是计算后验概率，理想情况下是基于大量数据(以有效的方式)。由于过去的斗争和计算时代的发展，我们已经开发了概率编程语言(PPL)，即PyMC3。PPL旨在帮助在模型创建和推理之间划清界限，同时也使设计和调试更加容易。这个框架允许各种数值方法作为通用推理机来应用，以自动化整个过程的推理部分。使用相对较少的代码行，就可以以有效和合理的方式创建复杂的概率模型。为了可视化PyMC3结果，我们将使用ArviZ库，正如您将看到的，它是为了补充PyMC3而惊人地构建的。</p><p id="8a60" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们来看看一些最著名的推理引擎。许多这方面的知识在<a class="ae nc" href="https://learning.oreilly.com/library/view/bayesian-analysis-with/9781785883804/ch02.html" rel="noopener ugc nofollow" target="_blank"> <em class="na">用Python进行贝叶斯分析</em>(马丁2016) </a>中得到了令人难以置信的解释，所以我将从那里汲取灵感:</p><p id="0d3d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">非马尔可夫方法:</p><ul class=""><li id="38de" class="nh ni it lj b lk ll ln lo lq nj lu nk ly nl mc nm nn no np bi translated">网格计算</li><li id="46ef" class="nh ni it lj b lk nq ln nr lq ns lu nt ly nu mc nm nn no np bi translated">二次逼近</li><li id="16b2" class="nh ni it lj b lk nq ln nr lq ns lu nt ly nu mc nm nn no np bi translated">变分法</li></ul><p id="f82c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">马尔可夫方法:</p><ul class=""><li id="b43b" class="nh ni it lj b lk ll ln lo lq nj lu nk ly nl mc nm nn no np bi translated">大都会黑斯廷斯</li><li id="4068" class="nh ni it lj b lk nq ln nr lq ns lu nt ly nu mc nm nn no np bi translated">哈密尔顿蒙特卡罗/无掉头采样器</li></ul><p id="dbaa" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">由于被称为MCMC ( <a class="ae nc" href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo" rel="noopener ugc nofollow" target="_blank">蒙特卡罗马尔可夫链</a>)模型的马尔可夫方法是当今最流行的方法之一，所以我在这里只集中讨论这些方法。我发现学习这些方法的理论的最好方法是通过做和倒退，所以让我们先从一个简单的，人为的抛硬币的例子开始。</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="e115" class="oo me it ok b gy op oq l or os">import pandas as pd<br/>import numpy as np<br/>import pymc3 as pm<br/>import arviz as az<br/>from arviz.plots.plot_utils import xarray_var_iter<br/>import scipy.stats as stats<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline</span><span id="c199" class="oo me it ok b gy ot oq l or os">np.random.seed = 0</span><span id="099b" class="oo me it ok b gy ot oq l or os">#the number of samples<br/>N=15</span><span id="0ef6" class="oo me it ok b gy ot oq l or os">#establishing prior and getting observed data<br/>theta_real = .5<br/>observed_data=stats.bernoulli.rvs(p=theta_real, size=N)</span><span id="7d08" class="oo me it ok b gy ot oq l or os">#the number of heads<br/>k=observed_data.sum()</span><span id="1ed4" class="oo me it ok b gy ot oq l or os">print(observed_data)<br/>print(f"There are {k} heads")</span><span id="ce20" class="oo me it ok b gy ot oq l or os">[1 0 1 1 1 1 1 0 0 0 0 1 0 1 1]<br/>There are 9 heads</span></pre><p id="97c1" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">上面的代码只运行15次概率(theta_real)为0.5的硬币投掷。在大多数真实的用例中，这需要估计。现在我们有了数据，让我们来构建模型:</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="9bdd" class="oo me it ok b gy op oq l or os">#fit the observed data <br/>with pm.Model() as coin_flip:<br/>    theta=pm.Beta('theta', alpha=1, beta=1)<br/>    y=pm.Bernoulli('y', theta, observed=observed_data)<br/>    <br/>pm.model_to_graphviz(coin_flip)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/c37d76ed6cf24a2a40438f819d5b5058.png" data-original-src="https://miro.medium.com/v2/resize:fit:362/format:webp/1*mabav4ASfOWXYeY9RHGa4w.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="a327" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为了在这里阐述思维过程，这本书<a class="ae nc" href="https://learning.oreilly.com/library/view/bayesian-methods-for/9780133902914/ch02.html" rel="noopener ugc nofollow" target="_blank"> <em class="na">黑客的贝叶斯方法:概率编程和贝叶斯推理</em> (Davidson-Pilon，2015) </a>提供了一个惊人的心理模型指南。当你构建贝叶斯模型时，你应该问自己的第一个问题是“<em class="na">这些数据是如何产生的？”</em>。</p><p id="6eeb" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd"> 1。</strong>我们首先思考，“描述这个抛硬币数据的最佳随机变量是什么？”二项式随机变量是一个很好的候选变量，因为它可以很好地表示具有n次独立试验的二元决策过程。这是代码块中的第三行，代表我们的可能性。单个成功/失败实验也称为伯努利试验或伯努利实验，一系列结果称为伯努利过程；对于单次试验，即<em class="na"> n </em> = 1，二项分布是伯努利分布。</p><p id="d10b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd"> 2。</strong>接下来，我们想，“好吧，假设掷硬币是二项分布的，我需要什么来得到二项分布？”嗯，二项分布有一个参数θ(<em class="na">θ)</em>。</p><p id="4962" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd"> 3。</strong>我们知道<em class="na"> θ </em>吗？是的，但是让我们假设我们没有。在大多数真实情况下，我们不会，我们需要估计它，但对于我们的例子，我们知道它是0.5。但是我们可以引入一个天真的先验，好像我们不知道<em class="na"> θ。</em>这是代码块中的第二行，代表我们的先验。</p><p id="ab2c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd"> 4。</strong>什么是<em class="na"> θ </em>的好分布？Beta是好的，因为它模拟了有限范围的事物，比如0到1。贝塔分布有两个决定其形状的参数，阿尔法(α)和贝塔(β)。</p><p id="7dee" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd"> 5。我们知道α和β可能是什么吗？不。在这一点上，我们可以继续并给这些参数分配一个分布，但一旦我们达到一个设定的无知水平，最好停止(记住我们只进行了15次试验作为证据)；尽管我们对<em class="na"> θ </em>有一个先验的信念(“当我们接近无穷大时，它可能会趋向于0.5”)，但我们对α和β并没有什么强有力的信念。所以最好就此停止建模。</strong></p><p id="d485" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">6。那么，α和β的合适值是多少呢？贝塔分布的形状因这些值而异，但首先我们将α和β指定为1。这等同于从[0，1]开始的均匀分布。这是一个弱信息先验，因为它基本上假设没有先验知识，知道公平抛硬币头的概率是多少。本质上，该参数可以是0到1之间的任何值。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ov"><img src="../Images/176143fce26b01c553ad09c4e61ca426.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*0S13GxAVAPBb6Yzl.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">贝塔分布的概率密度函数，<a class="ae nc" href="https://en.wikipedia.org/wiki/Beta_distribution#/media/File:Beta_distribution_pdf.svg" rel="noopener ugc nofollow" target="_blank">公共域</a></p></figure><p id="0f7e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><em class="na">这种思维方式“这个数据是怎么产生的？”</em>非常关键。本质上，我们对世界上的某个过程有一些假设，并希望对该过程建模，因此我们希望选择适当的分布组合来创建模型。如果我们假设公平抛硬币的概率代表贝塔分布，并且我们希望我们的模型描述它，那么由于共轭性，选择我们的先验分布为贝塔分布和可能性为二项式分布是一个重要的选择。如果后验分布与先验概率分布在同一概率分布族中，则先验和后验被称为共轭分布<strong class="lj jd">、</strong>，先验被称为似然函数的共轭先验。本质上，如果我们使用贝塔分布作为先验，使用二项式分布作为似然，我们将得到贝塔分布作为后验。</p><p id="5c86" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">共轭性确保了后验概率的数学易处理性，这一点很重要，因为贝叶斯统计中的一个常见问题是我们无法解析地解决后验概率。幸运的是，无论我们是否选择共轭先验，现代计算方法都使我们能够解决贝叶斯问题，但重要的是要认识到制作适当模型的价值。</p><p id="c846" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在让我们按下推理引擎按钮，看看我们得到了什么:</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="4335" class="oo me it ok b gy op oq l or os">with coin_flip:<br/> step = pm.Metropolis()<br/> trace = pm.sample(10000, step=step, return_inferencedata=True)</span></pre><p id="03c0" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">默认情况下，PyMC3将运行Metropolis推理引擎(Metropolis-Hastings)，但是我们也可以显式地声明它。我们选择要运行的样本数量，现在让我们看看我们的模型学习数据的程度。</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="14a8" class="oo me it ok b gy op oq l or os">var_names = ["theta"]<br/>lines = list(xarray_var_iter(trace.posterior[var_names].mean(dim=("chain", "draw"))))<br/>az.plot_trace(trace, lines=lines);</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ow"><img src="../Images/4b7078f142b31a8cc20654909f548a61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IivpXS-PAhcxu_UBxaRCCA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="770f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">正如我们所看到的，后验概率的平均值接近0.5，但确实有点偏。考虑到这只是15次试验，我们可以想象如果我们进行更多的试验，模型会自然地找到它的方向！对于左边的图，我们寻找的是一个相对平滑的核密度估计(KDE)图。没有那么顺利，可能是多种原因造成的(推理机的选择，数据中的观察次数，样本数等。).对于右边的图，我们要找的是没有明显的模式。我们希望右边的图类似于白噪声，没有明显的“分歧”被描述；本质上，我们想要各种蒙特卡罗马尔可夫链的良好混合。</p><p id="c6e2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在我们进一步分析后验分布之前，让我们看看是否能得到一个更好的KDE图。</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="58dc" class="oo me it ok b gy op oq l or os">with coin_flip:<br/> step = pm.NUTS()<br/> trace = pm.sample(10000, step=step, return_inferencedata=True)</span><span id="26c1" class="oo me it ok b gy ot oq l or os">var_names = ["theta"]<br/>lines = list(xarray_var_iter(trace.posterior[var_names].mean(dim=("chain", "draw"))))<br/>az.plot_trace(trace, lines=lines);</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ow"><img src="../Images/a6cb0d13e9836b55bd51fec9100fb0c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DX_NRz7d4Kv03Oi-6Rzq_A.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="c23f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这样看起来好多了！但是为什么呢？我们的第一个模型和这个模型的区别在于推理机。我们首先使用了默认的Metropolis-Hastings推理引擎，现在我们尝试了哈密尔顿蒙特卡罗/无U形转弯采样器。这<a class="ae nc" href="https://learning.oreilly.com/library/view/bayesian-analysis-with/9781785883804/ch02.html" rel="noopener ugc nofollow" target="_blank">页</a>相当精彩地解释了它们的区别(概念上和数学上的),我将尝试在这里总结它们的区别:</p><p id="1263" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">Metropolis-Hastings使我们能够从任何概率为<em class="na"> p(x) </em>的分布中获取样本，只要我们能够计算出至少一个与其成比例的值。这在概念上类似于试图从船上测量湖底的形状，方法是随机使用一根长棍来测量与一点的距离，然后在更深的方向上反复移动，并且只在这些方向上移动。正如你所想象的，经过大量的迭代，你会对湖底的形状有一个非常精确的描述，这在贝叶斯世界中也是相似的(湖底是后验分布)。</p><p id="4b9e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">汉密尔顿蒙特卡罗/NUTS源于这样一种认识，即获取大量样本可能会非常昂贵(时间和计算)。这种方法基本上与Metropolis-Hastings相同，只是我们没有建议随机移动我们的船，而是做了一些更聪明的事情；我们沿着湖底的曲线移动小船。为了决定下一步去哪里，我们让一个球在湖底滚动，从我们现在的位置开始。我们扔一个球，让它滚动一小会儿，然后我们把船移到球所在的地方。现在，我们使用Metropolis标准来接受或拒绝这一步，就像我们在Metropolis-Hastings方法中看到的那样。熟悉用Adam优化的梯度下降的人可能会发现这种方法类似。</p><p id="89e6" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">像所有事情一样，使用更聪明的哈密尔顿方法有一个权衡。HMC的每一步都比大都会-黑斯廷斯的每一步计算起来更昂贵，但是HMC接受这一步的概率比大都会高得多。我强烈建议阅读之前链接的页面，深入了解这个概念，但现在让我们继续前进。</p><p id="4d34" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们还可以打印出一个数据框架，显示我们的后验分布建模的关键指标:</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="17c6" class="oo me it ok b gy op oq l or os">az.summary(trace)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/5e61f5e3e33170db1c92f21842ea2991.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*PJ5izPxMIkz5jsT7p9EfMg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="ad38" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">默认情况下，我们可以获得相当多的信息:平均值、标准差、最高密度区间(HDI)等等。返回的数量之一是蒙特卡洛标准误差(MCSE)平均值和标准偏差。这是对采样方法引入的误差的估计。它被定义为链的标准偏差除以它们的有效样本量。该估计考虑到样本并非真正相互独立。后验分布的另一个快速质量检查是r_hat度量。经验法则是，您希望该值接近1.0，因为这表示链中的收敛。通常，高于1.05的值是一个关注点，1.0和1.5之间的值值得研究。</p><p id="aef6" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这里有趣的是，即使进行了15次试验，我们的估计仍然得到接近0.5的平均值，但肯定比我们想要的要高。看到r_hat告诉我们，我们的模型在构造中也不一定会遇到错误，所以我们的许多估计结果都是因为我们的数据很少。</p><p id="18cd" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为了解释高密度间隔，让我们直观地看一下:</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="af53" class="oo me it ok b gy op oq l or os">az.plot_posterior(trace, kind='kde', ref_val=0.5);</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/6c3c30a260ccbab9aad8cbe1558c8779.png" data-original-src="https://miro.medium.com/v2/resize:fit:892/format:webp/1*eFyXBNcsOLCEKKeypsUwEA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="f8de" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在贝叶斯推理中，与频率主义推理不同，我们得到值的整个分布。每次ArviZ计算和报告HPD时，默认情况下，它将使用94%的值。在频率主义者的世界里，我们有p值和置信区间。在贝叶斯世界中，这被称为可信区间。这里我们可以解释为，有94%的概率，硬币抛头的信念在0.45到53之间。这里的好处是，我们用频率主义者世界中相对混乱的术语和解释来换取贝叶斯世界中明智的估计。不利的一面是，我们失去了一个更明确的行动来换取一系列的信念。例如，在这种情况下，硬币可能会稍微偏向正面，因为大部分区间都在0.5以上，但也可能是0.38。频繁主义者的世界可能会给我们提供一个更明确的答案，但一如既往:你一直在做出假设以获得洞察力。</p><p id="8004" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">最后，我们可以运行后验预测检验(PPC)来验证我们的模型。这里的想法是，我们比较观察数据和预测数据，找出这两组数据之间的差异。主要目标是检查自动一致性。这是通过使用来自后部的每次绘制的参数从模型生成数据来完成的。理想情况下，生成的数据和观察到的数据应该或多或少相似。即使我们把所有的建模都做对了，他们也可能会看走眼。对原因的调查可能会导致我们模型的重大改进，这是我们以前可能没有想到的。</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="0de2" class="oo me it ok b gy op oq l or os">with coin_flip:<br/> ppc = pm.sample_posterior_predictive(trace, var_names=[“theta”, “y”])</span><span id="aaf5" class="oo me it ok b gy ot oq l or os">ppc['y'].shape</span><span id="3b29" class="oo me it ok b gy ot oq l or os">(20000, 15)</span><span id="4fbd" class="oo me it ok b gy ot oq l or os">az.plot_ppc(az.from_pymc3(posterior_predictive=ppc, model=coin_flip));</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/b0568fa15ebe669640a0b4c02befec4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*c74sOp4Jo1p3QpVTwGbrKw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="4186" class="oo me it ok b gy op oq l or os">fig, ax = plt.subplots(figsize=(10, 5))<br/>ax.hist([y.mean() for y in ppc['y']], bins=19, alpha=0.5)<br/>ax.axvline(observed_data.mean())<br/>ax.axvline(0.5, c='r')<br/>ax.set(title='Posterior Predictive Check', xlabel='Observed Data', ylabel='Frequency');</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/b54ae90d2bb9d07e8323c448034f939d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*yorqGPDOPhG1KXDsCP-KCQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="6d93" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">看起来我们的贝叶斯模型能够很好地学习我们观察到的数据分布！这一点尤其值得注意，因为我们只进行了15项试验，坦白地说，这是相当疯狂的。像任何数据项目一样，高性能模型不仅仅是好的度量数字。我们对掷硬币的了解告诉我们，0.6对于公平的掷硬币来说没有多大意义。</p><p id="126a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">所以现在，以真正的贝叶斯方式，让我们收集更多的证据，更新我们的先验，看看会发生什么。</p><p id="d289" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">根据新证据更新信念</strong></p><p id="4f1c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">假设我们进行1000次相同的掷硬币试验:</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="0a70" class="oo me it ok b gy op oq l or os">#the number of samples<br/>N_updated=1000</span><span id="dbdb" class="oo me it ok b gy ot oq l or os">#establishing prior and getting observed data<br/>theta_real = .5<br/>observed_data_updated=stats.bernoulli.rvs(p=theta_real, size=N_updated)</span><span id="2b90" class="oo me it ok b gy ot oq l or os">#the number of heads<br/>k_updated=observed_data_updated.sum()</span><span id="5702" class="oo me it ok b gy ot oq l or os">print(observed_data_updated)<br/>print(f”There are {k_updated} heads”)</span><span id="7dfc" class="oo me it ok b gy ot oq l or os">[0 1 0 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 0 1 0 1 0 0 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 1 1 0 1 0 0 1 1 0 1 0 0 1 1 0 0 1 1 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 0 0 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 0 1 1 0 1 0 0 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1 1 0 0 0 1 1 0 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 1 1 1 0 1 0 0 0 0 1 0 0 1 1 0 0 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 0 0 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 0 1 0 0 1 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 1 0 0 1 0 1 1 1 0 0 1 1 1 0 0 0 0 0 0 1 1 1 1 0 1 1 1 1 1 1 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 0 0 1 0 1 0 1 0 1 1 0 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 1 0 0 0 0 0 1 1 0 1 1 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 1 1 0 0 0 1 1 0 1 0 0 1 1 0 0 1 1 0 1 0 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 1 1 0 0 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 0 1 0 0 0 0 0 0 1 0 1 0 1 1 0 0 1 1 1 0 0 1 1 1 0 1 1 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 1 1 1 1 0 0 1 1 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 1 1 0 0 0 0 0 1 1 1 0 1 0 1 1 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 1 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 1 1 1 1 0 0 0 1 1 0 0 1 1 1 0 0 0 0 1 1 1 0 0 0 0 1 1 1 0 0 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 1 0 0 1 0 0 0 1 0 0 0 1 1 1 1 1 1 0 0 0 1 0 0 0 1 1 0 0 0 1 1 1 0 1 1 1 0 0 1 1 0 0 1 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 0 1 0 1 0 1 0 0 1]<br/>There are 511 heads</span></pre><p id="f9a2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在我们构建模型时，我们也可以更新之前的Beta分布，使其看起来更像一个能更好地反映掷硬币的形状。</p><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="21ae" class="oo me it ok b gy op oq l or os">#fit the observed data <br/>with pm.Model() as coin_flip_updated:<br/> theta=pm.Beta(‘theta’, alpha=2, beta=2)<br/> y=pm.Bernoulli(‘y’, theta, observed=observed_data_updated)<br/> <br/>pm.model_to_graphviz(coin_flip_updated)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/4bc71e831ad286af6b414d9f25f6e3cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/format:webp/1*RHBrTPfE_awrnH5UahutLw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="e38f" class="oo me it ok b gy op oq l or os">with coin_flip_updated:<br/> step = pm.NUTS()<br/> trace_updated = pm.sample(10000, step=step, return_inferencedata=True)</span><span id="0c41" class="oo me it ok b gy ot oq l or os">var_names = ["theta"]<br/>lines = list(xarray_var_iter(trace_updated.posterior[var_names].mean(dim=("chain", "draw"))))<br/>az.plot_trace(trace_updated, lines=lines);</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pc"><img src="../Images/54f8db1787dc8c693f59f827d5725137.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4h821zsumacGKOzTm8tS5A.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="2364" class="oo me it ok b gy op oq l or os">az.summary(trace_updated)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/9e2337beeb0da8000de89ce37486565b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*fPWQQataLitvM8oiAfzL1w.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="deec" class="oo me it ok b gy op oq l or os">az.plot_posterior(trace_updated, kind=’kde’, ref_val=0.5);</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/8ca0f0bdb9219aeb629f37ef5a3b336a.png" data-original-src="https://miro.medium.com/v2/resize:fit:886/format:webp/1*tF0kiA_53uMgTGE92eBRwQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="fa52" class="oo me it ok b gy op oq l or os">with coin_flip_updated:<br/> ppc_updated = pm.sample_posterior_predictive(trace_updated, var_names=[‘theta’, ‘y’])</span><span id="3f32" class="oo me it ok b gy ot oq l or os">az.plot_ppc(az.from_pymc3(posterior_predictive=ppc_updated, model=coin_flip_updated));</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/add801ad9031c6467b986fadf7642a11.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*MSz6vSk5pZk2jJsmYcsjQw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><pre class="ks kt ku kv gt oj ok ol om aw on bi"><span id="0bcc" class="oo me it ok b gy op oq l or os">fig, ax = plt.subplots(figsize=(10, 5))<br/>ax.hist([y.mean() for y in ppc_updated[‘y’]], bins=19, alpha=0.5)<br/>ax.axvline(observed_data_updated.mean())<br/>ax.axvline(0.5, c='r')<br/>ax.set(title=’Posterior Predictive Check’, xlabel=’Observed Data’, ylabel=’Frequency’);</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/7b747b751f4c534f90e2dcf8590d75df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*RVr8Mhg74MUo08k7JdUbfg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="80e2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这里令人惊讶的是，我们的贝叶斯模型能够超越我们的证据(其平均值约为0.51)，并确定公平抛硬币的真实平均概率:0.5。只要收集新的证据，更新我们先前的信念，重塑我们的数据，我们就能够对掷硬币现象做出更好的估计！</p><p id="31f8" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">数学上已经证明，如果我们想扩展逻辑以包括不确定性，我们必须使用概率和概率论。正如我们所看到的，贝叶斯定理只是概率规则的逻辑结果。因此，另一种思考贝叶斯统计的方式是在处理不确定性时作为逻辑的延伸。</p><p id="4605" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这个世界还有许多令人着迷的深度，在第2部分中，我计划将这些应用到真实世界的数据集中！看这里:<a class="ae nc" rel="noopener" target="_blank" href="/applied-bayesian-inference-with-python-pt-2-80bcd63b507e">用Python pt应用贝叶斯推理。2 </a></p><p id="0940" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">参考文献</strong></p><p id="aa2d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[1]奥斯瓦尔多·马丁，<a class="ae nc" href="https://learning.oreilly.com/library/view/bayesian-analysis-with/9781785883804/" rel="noopener ugc nofollow" target="_blank">用Python进行贝叶斯分析</a></p><p id="0751" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[2]卡梅隆·戴维森-皮隆，<a class="ae nc" href="https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers" rel="noopener ugc nofollow" target="_blank">黑客的概率编程和贝叶斯方法</a></p><p id="2f1b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">【3】卡西·科济尔科夫，<a class="ae nc" rel="noopener" target="_blank" href="/statistics-are-you-bayesian-or-frequentist-4943f953f21b">统计学:你是贝叶斯主义者还是频率主义者？</a></p></div></div>    
</body>
</html>