<html>
<head>
<title>Semantic Search: Measuring Meaning From Jaccard to Bert</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">语义搜索:从雅克卡到伯特的意义测量</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/semantic-search-measuring-meaning-from-jaccard-to-bert-a5aca61fc325?source=collection_archive---------11-----------------------#2021-06-29">https://towardsdatascience.com/semantic-search-measuring-meaning-from-jaccard-to-bert-a5aca61fc325?source=collection_archive---------11-----------------------#2021-06-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8e59" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用这些一流技术增强搜索能力</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d8b84083a7f00c61e7fa1819b150721a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dz3nP3sxgMCZqwGgOEFF_Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片—关于<a class="ae ky" href="https://www.pinecone.io/learn/semantic-search/" rel="noopener ugc nofollow" target="_blank">松果. io </a>的原创文章</p></figure><p id="a62e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated">相似性搜索是人工智能和机器学习领域中发展最快的领域之一。其核心是将相关信息匹配在一起的过程。</p><p id="d320" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你很有可能是通过搜索引擎找到这篇文章的——最有可能是谷歌。也许你搜索了类似“什么是相似性搜索？”或者“传统vs向量相似性搜索”。</p><p id="dd9f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Google处理了您的查询，并使用了许多相同的相似性搜索要素，我们将在本文中了解这些要素，将您带到—这篇文章。</p><p id="1978" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果相似性搜索是一家市值1.65万亿美元的公司——全球第五大最有价值公司[1]的成功核心，那么很有可能<em class="me">值得进一步了解。</em></p><p id="19e4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://www.pinecone.io/learn/what-is-similarity-search/" rel="noopener ugc nofollow" target="_blank">相似性搜索</a>是一个复杂的话题，有无数种技术可以用来构建有效的搜索引擎。</p><p id="7bdd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将讨论这些技术中最有趣也是最强大的一些，特别关注语义搜索。我们将了解它们是如何工作的，它们擅长什么，以及我们如何自己实现它们。</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h1 id="0de1" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">传统搜索</h1><p id="1006" class="pw-post-body-paragraph kz la it lb b lc ne ju le lf nf jx lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">我们在传统阵营中开始了我们的搜索之旅，在这里我们找到了几个关键角色，例如:</p><ul class=""><li id="8b12" class="nj nk it lb b lc ld lf lg li nl lm nm lq nn lu no np nq nr bi translated"><strong class="lb iu"> Jaccard相似度</strong></li><li id="d74e" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated"><strong class="lb iu">w-收缩</strong></li><li id="357d" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated">皮尔逊相似性</li><li id="c0e8" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated"><strong class="lb iu"> Levenshtein距离</strong></li><li id="c0e9" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated">归一化谷歌距离</li></ul><p id="35b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所有这些都是用于相似性搜索的很好的指标——其中我们将讨论三个最流行的指标，Jaccard相似性、w-shingling和Levenshtein距离。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx ny l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">视频演练涵盖相同的三种传统相似性方法。</p></figure><h2 id="0c7c" class="nz mn it bd mo oa ob dn ms oc od dp mw li oe of my lm og oh na lq oi oj nc ok bi translated">雅克卡相似性</h2><p id="cfc3" class="pw-post-body-paragraph kz la it lb b lc ne ju le lf nf jx lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">Jaccard相似性是一个简单但有时很强大的相似性度量。给定两个序列，<strong class="lb iu"> A </strong>和<strong class="lb iu"> B </strong> —我们找出两者之间共有元素的数量，并将其除以两个序列中元素的总数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/c3d08c8a4ff4f3013720022fbf5839be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*98-7RC6yGrWdcjQRnDeaPw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Jaccard相似性度量两个序列之间的交集超过两个序列之间的并集。</p></figure><p id="737f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">给定两个整数序列，我们将写出:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om ny l"/></div></figure><p id="77f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里我们确定了<strong class="lb iu">两个</strong>共有的<em class="me">唯一的</em>整数<code class="fe on oo op oq b">3</code>和<code class="fe on oo op oq b">4</code>——两个序列共有十个整数，其中<strong class="lb iu">八个</strong>是唯一的值— <code class="fe on oo op oq b">2/8</code>给出了我们的Jaccard相似性得分<code class="fe on oo op oq b">0.25</code>。</p><p id="cd22" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们也可以对文本数据执行同样的操作，我们所做的就是用<em class="me">标记</em>替换<em class="me">整数</em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4a009ededd04e7bc20930f125346c494.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f4YFiVV5MUVGhXfPNwc7HA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">计算两个句子<strong class="bd or"> a </strong>和<strong class="bd or"> b </strong>之间的Jaccard相似度。</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om ny l"/></div></figure><p id="2ef7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们所料，我们发现句子<code class="fe on oo op oq b">b</code>和<code class="fe on oo op oq b">c</code>得分更高。现在，它并不完美——两个句子除了像<em class="me">、</em>、<em class="me">、【a】、</em>、<em class="me">、</em>这样的词之外什么都不共享，尽管语义不同，但可以返回高的Jaccard分数。</p><p id="59c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些缺点可以通过使用预处理技术部分解决，如停用词移除、词干化/词汇化等。然而，正如我们很快会看到的，一些方法完全避免了这些问题。</p><h2 id="4177" class="nz mn it bd mo oa ob dn ms oc od dp mw li oe of my lm og oh na lq oi oj nc ok bi translated">w-收缩</h2><p id="3c69" class="pw-post-body-paragraph kz la it lb b lc ne ju le lf nf jx lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">另一个类似的技术是<strong class="lb iu">w-shringing</strong>。w-shingling使用了与<em class="me">交集/并集</em>完全相同的逻辑，但是使用了“瓦片区”。句子<strong class="lb iu"> a </strong>的两个重叠部分看起来像这样:</p><pre class="kj kk kl km gt os oq ot ou aw ov bi"><span id="277c" class="nz mn it oq b gy ow ox l oy oz">a = {'his thought', 'thought process', 'process is', ...} </span></pre><p id="66fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们将在我们的<em class="me">和</em>重叠的句子之间使用同样的<code class="fe on oo op oq b">intersection / union</code>计算，如下所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om ny l"/></div></figure><p id="43d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用2瓦片区，我们在句子<strong class="lb iu"> b </strong>和<strong class="lb iu"> c </strong>之间找到三个匹配瓦片区，导致相似度为<strong class="lb iu"> 0.125 </strong>。</p><h2 id="4eac" class="nz mn it bd mo oa ob dn ms oc od dp mw li oe of my lm og oh na lq oi oj nc ok bi translated">莱文斯坦距离</h2><p id="422b" class="pw-post-body-paragraph kz la it lb b lc ne ju le lf nf jx lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">比较两个字符串的另一个流行度量是Levenshtein距离。它的计算方法是将一个字符串转换为另一个字符串所需的操作次数，计算公式如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pa"><img src="../Images/46b80e2df6e1fe970ef7de5e88c44cda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E2wcUdykvT_Ug3vR04zzzA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">莱文斯坦距离公式。</p></figure><p id="b770" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，这是一个看起来相当复杂的公式——如果你理解它，太好了！如果没有，不要担心，我们会分解它。</p><p id="8426" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">变量<code class="fe on oo op oq b">a</code>和<code class="fe on oo op oq b">b</code>代表我们的两个字符串，<code class="fe on oo op oq b">i</code>和<code class="fe on oo op oq b">j</code>分别代表<code class="fe on oo op oq b">a</code>和<code class="fe on oo op oq b">b</code>中的字符位置。所以给定琴弦:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/00b1573a1de39965979c79a099f1b392.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wf0c8VwWZqTO90jVeL2vHQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">“Levenshtein”和一个混淆的“Livinshten”。</p></figure><p id="a3a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们会发现:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/c11646153dde24ca721641c852b4bbd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NmnWwb3S1kVkORFZZYk_yA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">我们从1到单词的长度对单词本身进行索引，第零个索引确实作为一个<strong class="bd or"> none </strong>字符存在(下一步将详细介绍)。</p></figure><p id="c9d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">轻松点。现在，掌握这个公式背后的逻辑的一个很好的方法是通过可视化Wagner-Fischer算法——它使用一个简单的矩阵来计算我们的Levenshtein距离。</p><p id="9f1e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将两个单词<code class="fe on oo op oq b">a</code>和<code class="fe on oo op oq b">b</code>放在矩阵的任意一个轴上——我们将<em class="me"> none </em>字符作为一个空格。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/b8b39ffffc6e23884dada59acae82d0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DsM8JXIOxgaWyXGEls2GLw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">我们的空Wagner-Fischer矩阵——我们将用它来计算<strong class="bd or">‘Levenshtein’</strong>和<strong class="bd or">‘livinhten’</strong>之间的Levenshtein距离。</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om ny l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用代码初始化我们的空瓦格纳菲舍尔矩阵。</p></figure><p id="3816" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们遍历矩阵中的每个位置，并应用我们之前看到的复杂公式。</p><p id="a1f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们公式的第一步是<code class="fe on oo op oq b">if min(i, j) = 0</code>——我们在这里说的是，在我们的两个位置<code class="fe on oo op oq b">i</code>和<code class="fe on oo op oq b">j</code>中，要么是<code class="fe on oo op oq b">0</code>？如果是，我们转到<code class="fe on oo op oq b">max(i, j)</code>，它告诉我们将矩阵中的当前位置分配给两个位置<code class="fe on oo op oq b">i</code>和<code class="fe on oo op oq b">j</code>中较高的一个:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5cd5108c4388293871f3d248eaee132a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0ezQLa3RfOq_qTnkHCDnuw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">我们从右边开始，沿着边缘在<strong class="bd or"> i </strong>和/或<strong class="bd or"> j <em class="pd"> </em> </strong> <em class="pd">为0的地方，矩阵位置将用</em> <strong class="bd or"> <em class="pd"> max(i，j) </em> </strong> <em class="pd">填充。</em></p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om ny l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">上面可视化的<strong class="ak"> min(i，j) == 0 </strong>后接<strong class="ak"> max(i，j) </strong>运算—转换成代码。</p></figure><p id="3c6b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们已经处理了矩阵的外部边缘——但我们仍然需要计算内部值——这是我们的最佳路径所在。</p><p id="736c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">回到<code class="fe on oo op oq b">if min(i, j) = 0</code> —如果<code class="fe on oo op oq b">0</code>都不是呢？然后，我们进入<code class="fe on oo op oq b">min {</code>部分的复杂部分。我们需要为每一行计算一个值，然后我们取<strong class="lb iu"> min </strong> imum值。</p><p id="48db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们已经知道了这些值，它们在我们的矩阵中:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/34380c78eee8feb8fc5e3f6919067f7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pDs_YjTcJaZ2y_UJv17rjw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">对于矩阵中的每个新位置，我们从三个相邻的位置(左上方的圆圈)中取最小值。</p></figure><p id="2565" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe on oo op oq b">lev(i-1, j)</code>其他操作都是索引操作——我们提取那个位置的值。然后我们取三个中的最小值。</p><p id="ec37" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">只剩下一个操作了。只有在<code class="fe on oo op oq b">a[i] != b[i]</code>的情况下，左侧的<code class="fe on oo op oq b">+1</code>才应适用——这是对不匹配字符的惩罚。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/9d7cf396b949f65b613b5b97915c577e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KydRaE-i7P7jQi-qk5_5Pg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">如果一个[我]！= b[j]我们将<strong class="bd or"> 1 </strong>加到最小值上——这是对不匹配字符的惩罚。</p></figure><p id="dc82" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将所有这些放入一个贯穿整个矩阵的迭代循环中，看起来像这样:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om ny l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用瓦格纳-费歇尔矩阵的全Levenshtein距离计算。</p></figure><p id="02dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在已经计算了矩阵中的每个值——这些值代表了从字符串<code class="fe on oo op oq b">a</code>到位置<code class="fe on oo op oq b">i</code>再到字符串<code class="fe on oo op oq b">b</code>到位置<code class="fe on oo op oq b">j</code>的转换所需的操作数。</p><p id="6cbe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们正在寻找将<code class="fe on oo op oq b">a</code>转换为<code class="fe on oo op oq b">b</code>的操作数——所以我们在<code class="fe on oo op oq b">lev[-1, -1]</code>取数组右下角的值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pe"><img src="../Images/8fc76ce58f36cc268d9d7b5de356b9fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kax2797jLZNEoXSF09K7vQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过矩阵的最佳路径——在右下角的[-1，-1]位置，我们有两个字符串之间的Levenshtein距离。</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om ny l"/></div></figure></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h1 id="d675" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">向量相似性搜索</h1><p id="41b4" class="pw-post-body-paragraph kz la it lb b lc ne ju le lf nf jx lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">对于基于向量的搜索，我们通常会找到几种向量构建方法中的一种:</p><ul class=""><li id="f9f8" class="nj nk it lb b lc ld lf lg li nl lm nm lq nn lu no np nq nr bi translated"><strong class="lb iu"> TF-IDF </strong></li><li id="55f3" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated"><strong class="lb iu"> BM25 </strong></li><li id="3d0b" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated">word2vec/doc2vec</li><li id="1ebf" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated"><strong class="lb iu">伯特</strong></li><li id="2d29" class="nj nk it lb b lc ns lf nt li nu lm nv lq nw lu no np nq nr bi translated">使用</li></ul><p id="8458" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与<em class="me">近似</em>最近邻(ANN)的一些实现相结合，这些基于向量的方法是相似性搜索领域中的MVP。</p><p id="c4cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将讨论TF-IDF、BM25和基于BERT的方法——因为这些方法很容易成为最常见的方法，并且涵盖了稀疏和密集的<a class="ae ky" href="https://www.pinecone.io/learn/vector-embeddings/" rel="noopener ugc nofollow" target="_blank">矢量表示</a>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nx ny l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">视频演练涵盖相同的三种基于向量的相似性方法。</p></figure><p id="0c4f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di"> 1。</span><strong class="lb iu">TF-IDF</strong>——向量相似性搜索的鼻祖，诞生于20世纪70年代。它由两部分组成，即<strong class="lb iu"> T </strong> erm <strong class="lb iu"> F </strong>频率(TF)和<strong class="lb iu"> I </strong>反向<strong class="lb iu"> D </strong>文件<strong class="lb iu"> F </strong>频率(IDF)。</p><p id="0ebd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">TF组件计算一个术语在一个文档中出现的次数，并将其除以同一文档中的术语总数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c6de71a0404f4de3d1d055c9f07e2da8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kbwGx1sTAgASfpr2AgztCg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">TF-IDF的术语频率(TF)组件计算我们的查询(“香蕉”)的频率，并除以所有标记的频率。</p></figure><p id="e9e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是我们计算的前半部分，我们有当前<strong class="lb iu"> D </strong>文件<code class="fe on oo op oq b">f(q,D)</code>内我们<strong class="lb iu"> q </strong>查询的<strong class="lb iu"> f </strong>频率—超过当前<strong class="lb iu"> D </strong>文件<code class="fe on oo op oq b">f(t,D)</code>内所有<strong class="lb iu">t</strong>erm的<strong class="lb iu"> f </strong>频率。</p><p id="dd96" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">频率是一个很好的衡量标准，但是不能让我们区分常用词和不常用词。如果我们要搜索单词“the”——只使用TF，我们会赋予这个句子与搜索“香蕉”相同的相关性。</p><p id="bde4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这很好，直到我们开始比较文档或使用更长的查询进行搜索。我们不希望像<em class="me">、</em>、<em class="me">、</em>、<em class="me">、【it】、</em>这样的词被排在和<em class="me">、【香蕉】、</em>、<em class="me">、【街道】、</em>一样高的位置。</p><p id="4eae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">理想情况下，我们希望更罕见的单词之间的匹配得分更高。为此，我们可以将TF乘以第二项——IDF。文档频率衡量一个单词在我们所有文档中出现的频率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5de315eeab048dcc7174b38ebf35a5c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kjCF73ViI5bbtNdUvZ7PoA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">TF-IDF的逆文档频率(IDF)组件计算包含我们的查询的文档的数量。</p></figure><p id="95d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个例子中，我们有三个句子。当我们计算常用词<em class="me">是</em>的IDF时，我们返回的数字要比更罕见的词<em class="me">“森林”</em>的IDF低得多。</p><p id="a920" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们要同时搜索单词<em class="me">‘is’</em>和<em class="me">‘forest’</em>，我们会像这样合并TF和IDF:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/883a4511782078a2e817a55815da25d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yj80uKIYjcZygnzzj0uAsg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">我们计算文档<strong class="bd or"> a </strong>、<strong class="bd or"> b </strong>和<strong class="bd or"> c </strong>的<strong class="bd or"> TF('is '，D) </strong>和<strong class="bd or"> TF('forest '，D) </strong>得分。IDF值跨所有文档—因此我们只计算一次<strong class="bd or"> IDF('is') </strong>和<strong class="bd or"> IDF('forest') </strong>。然后，我们通过将<strong class="bd or">乘以<strong class="bd or"> TF </strong>和<strong class="bd or"> IDF </strong>分量来获得每个文档中两个单词的TF-IDF值。句子<strong class="bd or"> a </strong>得分最高为<strong class="bd or">‘森林’</strong>，<strong class="bd or">‘是’</strong>总得分<strong class="bd or"> 0 </strong>为<strong class="bd or">IDF(‘是’)</strong>得分为<strong class="bd or"> 0 </strong>。</strong></p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om ny l"/></div></figure><p id="09de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这很好，但是<em class="me">向量</em>相似性搜索在这里起什么作用呢？好吧，我们使用我们的词汇表(我们数据集中所有单词的大列表)，并计算每个单词的TF-IDF。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/291da457a99156a7d167e042a051a501.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0XYLsq-qqbDjTaRx-q6hfQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">我们计算词汇表中每个单词的TF-IDF值，以创建一个TF-IDF向量。对每个文档重复这一过程。</p></figure><p id="5387" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以将所有这些放在一起创建TF-IDF向量，如下所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om ny l"/></div></figure><p id="f8c1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从那里我们得到了TF-IDF向量。值得注意的是，vocab的大小很容易在20K+的范围内，因此使用这种方法产生的向量非常稀疏，这意味着我们无法编码任何语义意义。</p><p id="35f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di"> 2。</span> <strong class="lb iu"> BM25 </strong> —作为TF-IDF的继任者，Okapi BM25是优化TF-IDF的结果，主要是为了根据文档长度规范化结果。</p><p id="79d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">TF-IDF很棒，但当我们开始比较几个提及时，可能会返回可疑的结果</p><p id="9164" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们拿了两篇500字的文章，发现文章A提到了‘丘吉尔’6次，而文章B提到了‘丘吉尔’12次——我们应该认为文章A只有一半相关吗？不太可能。</p><p id="33e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">BM25通过修改TF-IDF解决了这个问题:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pf"><img src="../Images/9f63203caa83d741b0326f4249d0214e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZcoK__nPvwBSRrx-YHNEzg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">BM25配方。</p></figure><p id="5cff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个看起来相当糟糕的等式——但它只不过是我们的TF-IDF公式加上一些新参数而已！让我们比较两个TF组件:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pg"><img src="../Images/2237c4acdc6c665ebf8d15723a273f13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IpPI6oInp5YJCIwcSzdEfA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">BM25的TF部分(左)对比TF-IDF的TF(右)。</p></figure><p id="884d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后我们有IDF部分，它甚至没有引入任何新的参数—它只是从TF-IDF重新安排了我们的旧IDF。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ph"><img src="../Images/0790f4791231d04f118cd601ec59b8cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HBDG6-olYDfzHxocuDvv2A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">BM25的IDF部分(左)对比TF-IDF的IDF(右)。</p></figure><p id="257d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，这个修改的结果是什么？如果我们取一个包含12个标记的序列，并逐渐输入越来越多的“匹配”标记，我们会得到以下分数:</p><div class="kj kk kl km gt ab cb"><figure class="pi kn pj pk pl pm pn paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/4ba6c51f83caf9119f0036baf502841c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*GFzDazLg0MfJJrDysqZw3g.png"/></div></figure><figure class="pi kn pj pk pl pm pn paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/75e18756f79370b0885d95d16ecdfe12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*Ie0IV6XSiBdrfRWddUYhGw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk po di pp pq translated">TF-IDF(左)和BM25(右)算法的比较，使用12个标记的句子和相关标记的递增数量(x轴)。</p></figure></div><p id="f6f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">TF-IDF分数随着相关令牌的数量线性增加。因此，如果频率翻倍，TF-IDF得分也会翻倍。</p><p id="ad02" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">BM25抑制分数增加，当相关令牌从两个增加到四个时，我们看到一个<em class="me"> x1.25 </em>的增加，当我们再次从四个增加到八个时，我们看到一个<em class="me"> x1.13 </em>的增加。</p><p id="a73b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">听起来很酷！但是我们如何用Python实现呢？同样，我们会像TF-IDF实现一样保持简洁明了。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om ny l"/></div></figure><p id="a18c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们已经为<code class="fe on oo op oq b">k</code>和<code class="fe on oo op oq b">b</code>使用了默认参数——我们的输出看起来很有希望。查询<code class="fe on oo op oq b">'purple'</code>只匹配句子<code class="fe on oo op oq b">a</code>，而<code class="fe on oo op oq b">'bananas'</code>对<code class="fe on oo op oq b">b</code>和<code class="fe on oo op oq b">c</code>的得分都是合理的——但由于字数较少，<code class="fe on oo op oq b">c</code>的得分略高。</p><p id="214f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了从中构建向量，我们做了与TF-IDF完全相同的事情。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om ny l"/></div></figure><p id="7932" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样，就像我们的TF-IDF向量一样，这些是<em class="me">稀疏</em>向量。我们将无法对语义进行编码——而是专注于语法。让我们看看如何开始考虑语义。</p><p id="840e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di"> 3。</span><strong class="lb iu">BERT</strong>——或者来自Transformers的双向编码器表示——是一个非常流行的transformer模型，用于NLP中的几乎所有东西。</p><p id="32cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过12层左右的编码器，BERT将大量信息编码成一组密集的T21向量。每个密集向量通常包含768个值——我们通常为BERT编码的每个句子提供512个这样的向量。</p><p id="3d1d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些向量包含了我们所能看到的语言的数字表示。如果需要，我们还可以从不同的层提取这些向量，但通常是从最后一层提取。</p><p id="474e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，有了两个正确编码的密集向量，我们可以使用类似余弦相似性的相似性度量来计算它们的语义相似性。对齐程度越高的向量语义越相似，反之亦然。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pr"><img src="../Images/e6b0e522423c7b8c866c42b1dbfb481e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1BpukXLUjqNvP3f69jDSnA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">向量之间的角度越小(用余弦相似度计算),意味着它们越对齐。对于密集向量，这与更大的语义相似性相关。</p></figure><p id="6685" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但有一个问题，每个序列由512个向量表示，而不是一个向量。</p><p id="798c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，这是伯特的另一个精彩改编的地方。Sentence-BERT允许我们创建一个单一的向量来代表我们的完整序列，也称为<em class="me">句子向量</em> [2]。</p><p id="3ed4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们有两种实现SBERT的方法——使用<code class="fe on oo op oq b">sentence-tranformers</code>库的简单方法，或者使用<code class="fe on oo op oq b">transformers</code> <em class="me">和</em> PyTorch的稍微不太简单的方法。</p><p id="988e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将涉及这两个方面，从PyTorch方法的<code class="fe on oo op oq b">transformers</code>开始，这样我们可以直观地了解这些向量是如何构建的。</p><p id="2514" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你用过HF transformers库，前几个步骤看起来会很熟悉。我们初始化我们的SBERT模型和标记器，标记我们的文本，并通过模型处理我们的标记。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om ny l"/></div></figure><p id="022f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在这里添加了一个新句子，句子<strong class="lb iu"> g </strong>具有与<strong class="lb iu"> b </strong>相同的<em class="me">语义</em>含义，但没有相同的关键字。由于缺乏共有词，我们以前的所有方法都很难找到这两个序列之间的相似性——记住这一点，以后再说。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om ny l"/></div></figure><p id="8d20" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们有长度为<em class="me"> 768 — </em>的向量，但这些是<strong class="lb iu">而不是</strong> <em class="me">句子向量</em>，因为我们对序列中的每个标记都有一个向量表示(这里是128，因为我们使用SBERT —对于BERT-base是512)。我们需要执行一个<strong class="lb iu"> mean pooling </strong>操作来创建句子向量。</p><p id="2f2d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们做的第一件事是将我们的<code class="fe on oo op oq b">embeddings</code>张量中的每个值乘以它各自的<code class="fe on oo op oq b">attention_mask</code>值。<code class="fe on oo op oq b">attention_mask</code>包含<strong class="lb iu">1</strong>，这里我们有‘实令牌’(例如不是填充令牌)，而<strong class="lb iu">0</strong>在别处——这个操作允许我们忽略非实令牌。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om ny l"/></div></figure><p id="00e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些是我们的句子向量，利用它们，我们可以通过计算它们之间的余弦相似度来衡量相似度。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om ny l"/></div></figure><p id="1f26" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们可视化我们的阵列，我们可以很容易地识别更高相似性的句子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ps"><img src="../Images/efe259b46bd729bf63e480e2d6b5a50a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*esx7jE-O8IGkVsRGZrVZtQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">热图显示了我们的SBERT句子向量之间的余弦相似性——句子<strong class="bd or"> b </strong>和<strong class="bd or"> g </strong>之间的得分被圈起来。</p></figure><p id="4dbd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，回想一下之前的笔记，关于句子<strong class="lb iu"> b </strong>和<strong class="lb iu"> g </strong>具有基本相同的意思，而<strong class="lb iu">没有</strong>共享<em class="me">任何</em>相同的关键字。</p><p id="681e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们希望SBERT及其卓越的语言语义表示能够确定这两个句子是相似的——令人惊讶的是，这两个句子之间的相似性是我们第二高的分数，为0.66(上面画了圈)。</p><p id="0c3e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，<strong class="lb iu">另一种(简单的)方法是使用句子变形器</strong>。为了获得与上面完全相同的输出，我们编写:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om ny l"/></div></figure><p id="1df5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当然，这要容易得多。</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="e30a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是Jaccard，Levenshtein和Bert的历史之旅！</p><p id="4ef7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们总共讨论了五种不同的技术，从简单的Jaccard相似性和Levenshtein距离开始。然后使用稀疏矢量进行搜索——TF-IDF和BM25，最后使用SBERT完成最新的密集矢量表示。</p><p id="4fa4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望你喜欢这篇文章。如果您有任何问题或建议，请通过推特<a class="ae ky" href="https://twitter.com/jamescalam" rel="noopener ugc nofollow" target="_blank">或在下面的评论中告诉我。如果你对更多类似的内容感兴趣，我也会在YouTube上发布。</a></p><p id="8799" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读！</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="8df2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://www.pinecone.io/learn/" rel="noopener ugc nofollow" target="_blank">📚在Pinecone.io了解有关可扩展搜索的更多信息</a></p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h1 id="826f" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">参考</h1><p id="6fc2" class="pw-post-body-paragraph kz la it lb b lc ne ju le lf nf jx lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">[1]<a class="ae ky" href="https://companiesmarketcap.com/alphabet-google/marketcap/" rel="noopener ugc nofollow" target="_blank">Alphabet(谷歌)</a>市值，公司市值</p><p id="f0d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2] N. Reimers，I. Gurevych，<a class="ae ky" href="https://arxiv.org/abs/1908.10084" rel="noopener ugc nofollow" target="_blank">句子-BERT:使用连体BERT网络的句子嵌入</a> (2019)，2019年实证方法2019年会议录</p><p id="05bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://github.com/pinecone-io/examples/tree/master/semantic_search_intro" rel="noopener ugc nofollow" target="_blank">笔记本回购</a></p><p id="d611" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">colab for<a class="ae ky" href="https://colab.research.google.com/github/pinecone-io/examples/blob/master/semantic_search_intro/jaccard.ipynb" rel="noopener ugc nofollow" target="_blank">JAC card</a>|<a class="ae ky" href="https://colab.research.google.com/github/pinecone-io/examples/blob/master/semantic_search_intro/levenshtein.ipynb" rel="noopener ugc nofollow" target="_blank">Levenshtein</a>|<a class="ae ky" href="https://colab.research.google.com/github/pinecone-io/examples/blob/master/semantic_search_intro/tfidf.ipynb" rel="noopener ugc nofollow" target="_blank">TF-IDF</a>|<a class="ae ky" href="https://colab.research.google.com/github/pinecone-io/examples/blob/master/semantic_search_intro/bm25.ipynb" rel="noopener ugc nofollow" target="_blank">BM25</a>|<a class="ae ky" href="https://colab.research.google.com/github/pinecone-io/examples/blob/master/semantic_search_intro/sbert.ipynb" rel="noopener ugc nofollow" target="_blank">SBERT</a></p><p id="2f92" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://bit.ly/nlp-transformers" rel="noopener ugc nofollow" target="_blank">🤖《变形金刚》课程NLP的70%折扣</a></p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="2fe2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="me">*除非另有说明，所有图片均出自作者之手</em></p></div></div>    
</body>
</html>