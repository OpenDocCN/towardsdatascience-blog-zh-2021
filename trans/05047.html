<html>
<head>
<title>Weekly review of Reinforcement Learning papers #7</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习论文#7的每周回顾</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/weekly-review-of-reinforcement-learning-papers-7-e5c726c2fafd?source=collection_archive---------24-----------------------#2021-05-03">https://towardsdatascience.com/weekly-review-of-reinforcement-learning-papers-7-e5c726c2fafd?source=collection_archive---------24-----------------------#2021-05-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="00cf" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">每周一，我都会发表我研究领域的4篇论文。大家来讨论一下吧！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6a94d3673b7cf4203d721359ce23f66a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*CHfg-p69LT7uTv-Q.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="818a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[ <a class="ae lu" rel="noopener" target="_blank" href="/weekly-review-of-reinforcement-learning-papers-6-2f919fe2a479?source=friends_link&amp;sk=da5442ea1ab2dcccd2e6223d4f2d7f9a"> ←上一次回顾</a> ][ <a class="ae lu" href="https://qgallouedec.medium.com/weekly-review-of-reinforcement-learning-papers-8-9d02a67b2e8a?sk=a9f56ffd7721f0956f8319456eec4d5f" rel="noopener">下一次回顾→ </a></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="65ae" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">论文1:神经科学和强化学习之间的学习差距</h1><p id="d5dd" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">Wauthier，S. T .，Mazzaglia，p .，atal，o .，De Boom，c .，Verbelen，t .，和Dhoedt，B. (2021年)。<a class="ae lu" href="https://arxiv.org/abs/2104.10995" rel="noopener ugc nofollow" target="_blank">神经科学和强化学习之间的学习鸿沟</a>。arXiv预印本arXiv:2104.10995 。</p><p id="5afe" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi na translated">有两种可能的配置。奖励用红圈表示。在第一种配置中，奖励在左边，在第二种配置中，奖励在右边。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/7fa62251ca9de1284155435d2f61fe5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1180/format:webp/1*eN6nRAbmm2Y5kBwJSIqdOg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来自报纸的数字</p></figure><p id="d517" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在其初始位置，代理无法区分这两种配置。所以增加了一条线索:一个窗户后面的圆圈，如果奖励在左边，它是蓝色的，如果奖励在右边，它是绿色的。最后一个细节，特工不允许探索T的两个分支，一旦他进去了一个，他就不能回来了。很简单，不是吗？你认为DQN或PPO会在多少次迭代中解决这个问题？打个赌。</p><p id="cf30" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面是令人惊讶的坏结果:在最好的情况下，<strong class="la iu">DQN/彩虹、PPO和DreamerV2的成功率是50% </strong>。代理人不可能做得比这更好。50%意味着代理似乎知道奖励出现的两个位置，但似乎无法预测奖励是在左边还是在右边(即使底部的彩色圆圈给了它这个信息)。代理人似乎没有把线索圈的颜色和悬赏的位置联系起来。有人可能会说这种观察是不全面的，解决这个问题需要记忆机制。DreamerV2有记忆机制，成功率还是50%。</p><p id="5753" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我必须承认我对这些结果感到惊讶，但我认为这揭示了许多RL作品的偏见。当前的基准测试限制太多，并且使用许多技巧来使学习过程有效。这些技巧虽然重要，但往往在报纸上甚至没有提及。作者提出的一种方法是再次接近神经科学。我喜欢他们的结论。</p><h1 id="4a61" class="mc md it bd me mf nk mh mi mj nl ml mm jz nm ka mo kc nn kd mq kf no kg ms mt bi translated">论文2:复杂行动空间中的学习和计划</h1><p id="9c2d" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">休伯特，t .，施利特维泽，j .，安东诺格鲁，I .，巴雷卡廷，m .，施密特，s .，和西尔弗，D. (2021)。<a class="ae lu" href="https://arxiv.org/abs/2104.06303" rel="noopener ugc nofollow" target="_blank">复杂动作空间的学习与规划</a>。<em class="mz"> arXiv预印本arXiv:2104.06303 </em>。</p><p id="d370" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi na translated"><span class="l nb nc nd bm ne nf ng nh ni di">作用空间可以是有限离散的，无限离散的，连续的，甚至多维连续的。行动空间越复杂，正确评估政策就越困难。仅仅列举所有可能的行动可能是不可能的。在本文中，作者提出了一个相当自然的想法:<strong class="la iu">关注发生概率最高的动作</strong>。如何评价这种发生概率？你不用，你只要<strong class="la iu">样</strong>就可以了。这种抽样是问题的核心，因此他们给他们的框架起了个名字:抽样MuZero(他们使用MuZero，但是他们的方法应该在所有基于策略迭代的方法上工作)。<br/>所以我们不再对整个动作空间进行推理，而只对动作的缩减子集进行推理。于是问题出现了:为了让学习正确进行，必须对多少个动作进行采样？显然，采样的动作越多，学习就越好，但是计算时间越长也越重要:这是一种折衷。为了说明这种折衷，作者将他们的方法应用于围棋游戏:362种可能的动作。是的，行动空间的大小是非常合理的，但它允许他们与理论上的最大值进行比较，理论上的最大值在于可以访问整个行动空间。所以他们通过取样15、25、50和100个动作来比较学习曲线。结果如下:</span></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/02a4c54245e49570564713137c9ab4f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_x9YA3WP7wK9Ik8Y1gcOqw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图来自<a class="ae lu" href="https://arxiv.org/abs/2104.06303" rel="noopener ugc nofollow" target="_blank">文章</a>:经典围棋棋局结果。Elo衡量玩家的水平。</p></figure><p id="a18d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们看到，通过对50个动作(不到15%的可能动作)进行采样，学习曲线非常接近对应于访问所有动作的曲线。<br/>连续动作空间也获得了类似的结果(DeepMind控制套件，Real-WorldRL套件)。这是一个有趣的技巧，可以大大减少计算时间。</p><h1 id="b645" class="mc md it bd me mf nk mh mi mj nl ml mm jz nm ka mo kc nn kd mq kf no kg ms mt bi translated">论文3:基于模型的强化学习的模块库</h1><p id="9c72" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">Pineda，l .，Amos，b .，Zhang，a .，Lambert，N. O .，&amp; Calandra，R. (2021)。MBRL库:<a class="ae lu" href="https://arxiv.org/abs/2104.10159" rel="noopener ugc nofollow" target="_blank">基于模型的强化学习的模块化库</a>。<em class="mz"> arXiv预印本arXiv:2104.10159 </em>。</p><p id="357c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi na translated">基于模型的强化学习框架不再需要被证明。当与环境交互的成本很高时，基于模型的RL仍然是给出最佳结果的方法。然而，实现没有无模型方法那么直接。作者提议建立一个图书馆，他们称之为MBRL图书馆。它是一个机器学习库，用于基于连续状态-动作空间中的模型进行强化学习(我们可以很遗憾不能使用离散空间)。图书馆使用PyTorch。</p><p id="1b2d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">说实话，我还没用过，不过一直在浏览代码。基于模型学习的示例代码仍然不多。目前，只有MBPO和宠物，仅此而已。我猜目标是贡献者丰富他们的图书馆。我很期待看到这一举措是否会在基于模型的RL社区中产生诱惑。0.1.0版本发布不到两周，敬请关注。这里是<a class="ae lu" href="https://github.com/facebookresearch/mbrl-lib" rel="noopener ugc nofollow" target="_blank">回购</a>。</p><h1 id="a9dd" class="mc md it bd me mf nk mh mi mj nl ml mm jz nm ka mo kc nn kd mq kf no kg ms mt bi translated">论文4:进化强化学习算法</h1><p id="d063" class="pw-post-body-paragraph ky kz it la b lb mu ju ld le mv jx lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">共同雷耶斯，法学博士，苗，y，彭，d，雷亚尔，e，莱文，s，乐，Q. V，…和浮士德，A. (2021)。<a class="ae lu" href="https://arxiv.org/abs/2101.03958" rel="noopener ugc nofollow" target="_blank">进化强化学习算法</a>。arXiv预印本arXiv:2101.03958 。</p><p id="3721" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi na translated">学习就是学会学习。它是机器学习的一个完整分支，与强化学习有一些交叉。这是其中之一。<br/>出发点是所有的强化学习算法都可以用图来表示。这里有一个DQN的例子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/e6a7145ca223eb7f0864a1bdb91dacd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7XEZtmCbC_xsST5OrnH9Og.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图来自<a class="ae lu" href="https://arxiv.org/abs/2101.03958" rel="noopener ugc nofollow" target="_blank">文章</a>:DQN的可视化，作为一个计算图。输出就是损失。</p></figure><p id="b417" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">先说一组算法，随机的或者文献上的。这些算法必须首先在所谓的“障碍”环境中表现良好，然后才能被允许在一组更困难的环境中进行训练。如果一个算法过不了这一关，它就会被淘汰。其他的用来更新不时变异的算法群体。在训练结束时，在测试环境中评估性能最佳的算法，该测试环境不同于它已经训练过的所有环境。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/109cd157a5e0eb20ba55d1a2c2b62e58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RjDU7M2qFFUBdNRwuXqI2Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图来自<a class="ae lu" href="https://arxiv.org/abs/2101.03958" rel="noopener ugc nofollow" target="_blank">文章</a>:方法概述。</p></figure><p id="bae2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">令人惊讶的结果:通过在简单的经典控制和gridworld任务上从头开始学习(没有任何最先进的算法)，这种方法重新发现了时差(TD)算法！</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="69cc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我很高兴向你们展示我本周的阅读材料。请随时向我发送您的反馈。</p></div></div>    
</body>
</html>