<html>
<head>
<title>Journey to the center of the neuron</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通往神经元中心的旅程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/journey-to-the-center-of-the-neuron-c614bfee3f9?source=collection_archive---------16-----------------------#2021-04-21">https://towardsdatascience.com/journey-to-the-center-of-the-neuron-c614bfee3f9?source=collection_archive---------16-----------------------#2021-04-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bf29" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">潜入大脑的咸水海洋，更接近激发我们的人工智能系统并使你的想法成为可能的实体。探索如何理解人工和生物神经元之间的差异可能会给我们提供线索，如何走向更灵活的人工智能。</h2></div><figure class="ki kj kk kl gt km"><div class="bz fp l di"><div class="kn ko l"/></div><p class="kp kq gj gh gi kr ks bd b be z dk translated">“百万灵魂的声音”，2分钟的艺术献礼，向我们的皮层列和新皮层中的数十亿神经元致敬(8K质量)。将youtube设置为4K或8K分辨率+全屏，以获得最佳体验。“当我们接近宏伟的圆柱时，神秘的图案用百万灵魂的声音从远处召唤我们……我感觉最明亮的太阳被压缩在那些微小的奇迹中..变成了在我们意识中产生共鸣的梦的织锦..我听到你笑了..我听到你坠落…我听到你的眼泪摧毁了地平线..直到我们在寂静等待的圆柱中心汇合..寂静，然后百万个太阳向着一个新存在的觉醒刺来..抱紧我..让我们直入主题，直入专栏的中心..你和我在寂静中合而为一..”/作者Javier Ideami创作的视频艺术作品和诗歌|<a class="ae kt" href="https://ideami.com" rel="noopener ugc nofollow" target="_blank">https://ideami.com</a></p></figure><p id="4c9d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">你的每一个想法</strong>都是通过你的生物神经元实现的<strong class="kw iu">。许多最有用的人工智能架构背后都有一个受其启发的实体。神经元</strong>处于处理过程的<strong class="kw iu">中心</strong>，这些处理过程支撑着智能系统产生的复杂性。好奇想知道更多关于你的想法的引擎，以及它们与它们的人工对应物相比如何？我们开始吧！</p><p id="3625" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">人工智能神经元</strong>最初受到我们生物神经元的启发，然而它们<strong class="kw iu">非常不同。</strong>为什么不应该呢？有许多方法可以到达同一个目的地，就像人类的飞行受到启发，但不会一部分一部分地模仿鸟类的飞行方式一样，我们的人工神经元也只是部分地受到了生物神经元的启发。</p><p id="a29d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">然而</strong>，我们的生物神经元比我们的人工神经元要复杂得多，包含了如此丰富的细节和如此多的秘密。即使我们不需要复制生物神经元的工作方式，<strong class="kw iu">理解两个实体之间的不同</strong><strong class="kw iu">也可以给我们新的线索</strong>关于如何走向更灵活的人工智能形式。</p><p id="20a0" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">在本文中，我们将回顾这些差异，以及与这些神经元嵌入的网络有关的一些差异。我们还将考虑这些差异如何为即将到来的未来提供新的可能性。</p><p id="be22" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">注</strong> : A.I是一个广阔的领域。在这个地区有各种各样的外来物种。在进行比较时，我将专门参考一些当今最典型和最流行的深度学习架构。</p><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi lq"><img src="../Images/0ce68fab9860fdd0dda32cefdc4fe3de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D_UdWMcEYYKxMKgweVdmpQ.jpeg"/></div></div><p class="kp kq gj gh gi kr ks bd b be z dk translated">神经泉瀑布，作者Javier Ideami |【https://ideami.com T2】</p></figure><h1 id="b101" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">神经元:盐水和电</h1><p id="9844" class="pw-post-body-paragraph ku kv it kw b kx mp ju kz la mq jx lc ld mr lf lg lh ms lj lk ll mt ln lo lp im bi translated">在我们深入研究神经元功能的细节之前，让我们先快速了解一下内部。这要变咸了！</p><ul class=""><li id="ab8f" class="mu mv it kw b kx ky la lb ld mw lh mx ll my lp mz na nb nc bi translated">把你的大脑想象成一个装有盐水海洋的容器。在那片海洋中，你有很多我们称之为<strong class="kw iu">神经元</strong>的细胞和很多<strong class="kw iu">离子</strong>。<strong class="kw iu">离子</strong>是正负电荷不相等的原子。而你大脑中的<strong class="kw iu">主要离子</strong>有:<strong class="kw iu">钠</strong>(Na+)<strong class="kw iu">钾</strong>(K+)<strong class="kw iu">钙</strong> (Ca++)和<strong class="kw iu">氯</strong> (Cl-)。这很好地提醒了我们为什么这些矿物质如此重要！</li><li id="3af2" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">所以，生物<strong class="kw iu">神经元</strong>和大多数细胞一样，基本上是由盐水组成的<strong class="kw iu">，氯离子和钠离子漂浮在</strong>周围。并且<strong class="kw iu">一个神经元所做的一切都可以用电学</strong> : <strong class="kw iu">电压</strong>(例如，存在于神经元细胞膜上的电势/电压)和<strong class="kw iu">电流</strong>(带电离子进出神经元的流动)来解释。</li><li id="f29d" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated"><strong class="kw iu">人工神经元</strong>是用<strong class="kw iu">计算机代码</strong>创建的，该代码在执行时创建由数字字节组成的<strong class="kw iu">数据结构，以此类推。人工神经元所做的大部分事情都可以用数据的<strong class="kw iu">计算</strong>、<strong class="kw iu">线性和非线性转换</strong>来理解。</strong></li></ul><p id="c325" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">咸咸的海洋vs硅的仙境。</strong>什么更高效？让我们来看看。</p><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi lq"><img src="../Images/97feb120c83ef183f70ff79a142d4f1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BCAkHO6lXqjIGkD_UyZHNA.jpeg"/></div></div><p class="kp kq gj gh gi kr ks bd b be z dk translated">作者Javier Ideami |<a class="ae kt" href="https://ideami.com" rel="noopener ugc nofollow" target="_blank">https://ideami.com</a>绘画</p></figure><h1 id="aaf3" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">能源:20瓦的优质服务</h1><p id="abc2" class="pw-post-body-paragraph ku kv it kw b kx mp ju kz la mq jx lc ld mr lf lg lh ms lj lk ll mt ln lo lp im bi translated"><strong class="kw iu">信息处理需要能量，</strong>所以这些神经网络内的能量消耗很重要。它为可能的事情设定了界限，我们的大脑非常高效。</p><ul class=""><li id="66e1" class="mu mv it kw b kx ky la lb ld mw lh mx ll my lp mz na nb nc bi translated">现在，当想法在你的头脑中移动时，你的<strong class="kw iu">大脑只消耗了大约20瓦的能量</strong>，勉强够点亮一个灯泡。它能够做到这一点<strong class="kw iu">，即使你禁食或睡觉</strong>，并保持<strong class="kw iu">大约37摄氏度</strong>度的适度温度。</li><li id="7e70" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">我们的深度学习系统经常使用的强大的<strong class="kw iu">GPU</strong>每单位可以消耗<strong class="kw iu">数百瓦，</strong>比我们的大脑多得多，并且<strong class="kw iu">它们释放大量热量</strong>，达到大约70或80摄氏度的温度。</li></ul><p id="8a72" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最新消息:让人工智能系统更加节能的研究正在进行中。连接和激活的稀疏性可能有助于这些系统接近我们大脑的巨大效率，在任何时候，只有一小部分神经元是活跃的(通常在0.5%和2%之间)。</p><h1 id="e5fe" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">功能:全方位检测</h1><p id="d96d" class="pw-post-body-paragraph ku kv it kw b kx mp ju kz la mq jx lc ld mr lf lg lh ms lj lk ll mt ln lo lp im bi translated">我们已经审查了<strong class="kw iu">环境</strong>和<strong class="kw iu">能源</strong>消耗。是时候放大这些实体之一了。神经元在做什么？</p><ul class=""><li id="8225" class="mu mv it kw b kx ky la lb ld mw lh mx ll my lp mz na nb nc bi translated">它<strong class="kw iu">检测来自它接收的许多输入</strong>的模式。我们的大脑中有大约1000亿个神经元(估计各不相同，一些专家说实际数字大约是860亿)，每个神经元接收来自大约10000个其他神经元的输入(有人说大约7000个，有人说8000个，总之有数千个)。</li><li id="415a" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">生物神经元有一个<strong class="kw iu">阈值</strong>和<strong class="kw iu">，当超过该阈值时，它会发出一个信号</strong>，我们称之为<strong class="kw iu">动作电位或尖峰</strong>。该信号沿着其<strong class="kw iu">轴突</strong>(神经元的输出)向其他神经元的<strong class="kw iu">突触</strong>传播。</li><li id="3009" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated"><strong class="kw iu">突触</strong>是一种允许一个神经元向另一个神经元传递信号的结构。突触位于神经元的<strong class="kw iu">树突</strong>(其分支)。</li><li id="3c43" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">因此<strong class="kw iu">可以用这个过程总结生物神经元</strong>:<strong class="kw iu">接收</strong>输入，<strong class="kw iu">对它们进行积分，<strong class="kw iu">判断</strong>该积分的结果是否足够强以触发输出信号。</strong></li><li id="d3ef" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated"><strong class="kw iu">另一方面，人工神经元</strong>、<strong class="kw iu">执行计算</strong>、<strong class="kw iu">将其输入与其各自的权重</strong>相结合(权重是指定该神经元与其每个输入之间的连接强度的数值)。</li><li id="152b" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">结果是<strong class="kw iu">通过激活函数</strong>(计算其输入的非线性变换的函数。这使得网络能够学习输入和输出之间的非线性映射)。</li><li id="8da6" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">所以你可以<strong class="kw iu">用这个过程来概括人工神经元</strong>:<strong class="kw iu">接收</strong>输入，<strong class="kw iu">将这些输入中的每一个乘以它们与该神经元的连接强度</strong>的结果相加，<strong class="kw iu">通过非线性函数</strong>传递那个计算<strong class="kw iu">的结果</strong>。</li></ul><p id="3100" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">怎么了:</strong>注意一个关键区别。<strong class="kw iu">生物神经元有一个阈值</strong>，这个阈值<strong class="kw iu">使它们保持沉默，直到被超过</strong>。深度学习系统中的大多数人工神经元产生主动输出(一些可能输出0。例如，如果输入小于0，ReLU激活函数将神经元的输出设置为0)。正如我们稍后将强调的那样，在任何时候，我们大约0.5%至2%的生物神经元是活跃的，而在典型的人工深度学习系统中，这一比例约为50%。</p><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi lq"><img src="../Images/94b6258cff0ca2cbe9264a58a733b1eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rKYr32PGJrpOJNTo1YsS4g.jpeg"/></div></div><p class="kp kq gj gh gi kr ks bd b be z dk translated">作者Javier Ideami |<a class="ae kt" href="https://ideami.com" rel="noopener ugc nofollow" target="_blank">https://ideami.com</a>绘画</p></figure><h1 id="b2a5" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">到处都有电</h1><p id="d495" class="pw-post-body-paragraph ku kv it kw b kx mp ju kz la mq jx lc ld mr lf lg lh ms lj lk ll mt ln lo lp im bi translated">让我们暂时回到生物学领域的电的一面。我们经常听说我们的<strong class="kw iu">神经元通过电脉冲</strong>进行交流。让我们深入到电的维度，以便我们能够更好地内化正在发生的事情。</p><ul class=""><li id="c5ea" class="mu mv it kw b kx ky la lb ld mw lh mx ll my lp mz na nb nc bi translated"><strong class="kw iu">电压</strong>是<strong class="kw iu">一个地方的电荷和另一个地方的电荷之间的<strong class="kw iu">比较</strong>(相对而言)。</strong></li><li id="7090" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">神经元中的<strong class="kw iu">电压通常用<strong class="kw iu">毫伏</strong>来测量。毫伏是千分之一伏。我们的<strong class="kw iu">微小神经元</strong>使用<strong class="kw iu">微量的电</strong>来进行它们的操作。</strong></li><li id="cf88" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">要强调的一个关键点是神经元的<strong class="kw iu">膜电位(电压)。这是神经元相对于其外部空间的<strong class="kw iu">电压</strong>。我们称之为<strong class="kw iu">膜电位</strong>是因为它位于神经元的膜上，也就是<strong class="kw iu">一层薄薄的脂肪</strong>。</strong></li><li id="93e5" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated"><strong class="kw iu">当两个区域之间的电荷存在差异</strong>时，电荷<strong class="kw iu">将倾向于流动，以补偿</strong>的差异并均衡情况。</li><li id="f5c6" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">神经元<strong class="kw iu">的<strong class="kw iu">膜</strong>充当了内部电流和外部电流</strong>之间的屏障。我们称之为<strong class="kw iu">的离子通道</strong>就像屏障中的小隧道<strong class="kw iu">，允许物质以可控的方式流动</strong>。</li><li id="5a68" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated"><strong class="kw iu">电导</strong>，开口的<strong class="kw iu">大小，决定了这些离子流入和流出膜的速度。</strong></li><li id="b0dc" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">因此<strong class="kw iu">当电压(电势)存在时，两个区域之间的电荷存在相对差异，离子流动以平衡事物</strong>。但是为什么呢？因为<strong class="kw iu">异性电荷相吸，同性电荷相斥的普遍原理</strong>。例如，在某个特定的环境中，正电荷比负电荷多，就会形成一个电流来平衡这种情况，将更多的负电荷带入该区域。</li><li id="30c2" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated"><strong class="kw iu">触发阈值</strong>(或<strong class="kw iu">动作电位阈值</strong>)，是神经元<strong class="kw iu">膜上必须达到的<strong class="kw iu">电压，以使神经元通过其轴突触发</strong>输出信号(动作电位)。该<strong class="kw iu">阈值</strong>为<strong class="kw iu">通常</strong>在<strong class="kw iu">约-50mv(毫伏)</strong>处。</strong></li><li id="08a7" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">神经元的所谓<strong class="kw iu">静息电位在<strong class="kw iu"> -70mv，</strong>，<strong class="kw iu">低于触发阈值</strong>，因此<strong class="kw iu">默认情况下神经元不会触发。</strong></strong></li><li id="b39a" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">这个门槛的重要性不能被夸大。由于它的存在，<strong class="kw iu">只有最强烈的激活水平通过神经元的轴突(其输出)进行交流</strong>。这使得<strong class="kw iu">信息以一种非常紧凑和有效的方式被编码。</strong></li><li id="d426" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">还记得我们开头提到的那些<strong class="kw iu"> 20瓦</strong>的能耗吗？<strong class="kw iu">生物</strong> <strong class="kw iu">神经元只交流相关</strong>和关键信息。其余的，他们保持沉默。</li></ul><p id="c5d9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们提醒自己，我们没有理由复制或模仿生物神经元的复杂性。我们可以创造出以完全不同的方式展示灵活的智能形式的系统。而且<strong class="kw iu">方式越简单越好。</strong>但是<strong class="kw iu">深入了解我们的生物神经元可以给我们一些想法</strong>，这些想法可以丰富我们在使用人工神经元时的实验和策略。</p><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi lq"><img src="../Images/2ab2885405ecf79ec11f269ef7cd706f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XMCOG7iXahOZEnIFNlelzA.jpeg"/></div></div><p class="kp kq gj gh gi kr ks bd b be z dk translated">作者Javier Ideami |<a class="ae kt" href="https://ideami.com" rel="noopener ugc nofollow" target="_blank">https://ideami.com</a>绘画</p></figure><h1 id="60d7" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">是时候了:扣球还是不扣球？</h1><p id="0252" class="pw-post-body-paragraph ku kv it kw b kx mp ju kz la mq jx lc ld mr lf lg lh ms lj lk ll mt ln lo lp im bi translated">两种神经元之间的一个非常重要的差异(T3)与时间维度(T4)有关。</p><ul class=""><li id="7664" class="mu mv it kw b kx ky la lb ld mw lh mx ll my lp mz na nb nc bi translated"><strong class="kw iu">生物神经元在<strong class="kw iu">非常短暂的</strong>时刻发出</strong>信号。它们发出<strong class="kw iu">尖峰</strong>，持续时间非常短(通常在1毫秒左右)。由神经元传输的<strong class="kw iu">信息</strong>在那些尖峰的时序中被<strong class="kw iu">编码。<strong class="kw iu">尖峰序列</strong>是一系列<strong class="kw iu">尖峰和静默。</strong></strong></li><li id="10bb" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated"><strong class="kw iu">在每一个尖峰</strong>之后，神经元的<strong class="kw iu">膜电位返回到一个小值</strong>(它甚至可以低于其静止电位)。为了使<strong class="kw iu">再次达到峰值</strong>，电压<strong class="kw iu">需要回到高于</strong>点火阈值<strong class="kw iu">的水平。</strong></li><li id="315e" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">当<strong class="kw iu">学习</strong>过程发生时，一个神经元有助于激活其他神经元 <strong class="kw iu">的<strong class="kw iu">效率可以</strong>动态<strong class="kw iu">改变</strong>通过<strong class="kw iu">长时程增强(LTP)</strong>等过程，这些过程对我们学习和创建记忆的方式至关重要(长期抑郁是LTP的相反过程)。</strong></li><li id="c93d" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated"><strong class="kw iu">大多数人工神经元</strong>都是<strong class="kw iu">一般不断地产生输出</strong>(在每个执行周期中)，向线下的下一个神经元发送连续的信号(有时它们的激活函数可能会将它们的输出设置为0)。</li><li id="481c" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">所以，在大多数人工深度学习网络中，<strong class="kw iu">时间维度是不相关的</strong>。在我们的生物网络中，它的使用方式本身没有门槛。我们的人工系统要简单得多。但是有时候越简单越好。今天深度学习系统的发展方向足以将我们带到专家们喜欢称之为AGI(人工通用智能)的地方吗？或者更灵活的人工智能？。关于那件事，还没有定论。</li></ul><p id="572e" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">近况:</strong><strong class="kw iu">冯诺依曼</strong>架构是我们今天使用的大多数硬件背后的<strong class="kw iu">。为了更接近大脑的功能，一些研究人员开始研究其他类型的结构。神经形态计算</strong>就是一个例子。这种架构允许<strong class="kw iu">更多的并行处理和健壮性</strong>。最重要的是，它可以与<strong class="kw iu">脉冲神经网络</strong>一起工作，后者处理<strong class="kw iu">空间和时间维度</strong>，就像大脑一样。像IBM或英特尔这样的公司已经生产出了神经形态芯片。这一研究领域面临着许多重大挑战，既包括研究前沿，也包括必须应对与冯·诺依曼模型高度适应的现有生态系统。</p><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi lq"><img src="../Images/29347974fa9cf17c643d8b52ea256d4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3DjDxN1HLQSejKth7v6b9A.jpeg"/></div></div><p class="kp kq gj gh gi kr ks bd b be z dk translated">作者Javier Ideami |<a class="ae kt" href="https://ideami.com" rel="noopener ugc nofollow" target="_blank">https://ideami.com</a></p></figure><h1 id="9d24" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">请留点空间:稀疏的魔力</h1><p id="2c45" class="pw-post-body-paragraph ku kv it kw b kx mp ju kz la mq jx lc ld mr lf lg lh ms lj lk ll mt ln lo lp im bi translated">当神经元相互结合时，在任何时刻激活的神经元的数量和它们之间的连接数量在能量消耗、弹性、健壮性和其他相关因素方面会产生很多后果。</p><ul class=""><li id="524e" class="mu mv it kw b kx ky la lb ld mw lh mx ll my lp mz na nb nc bi translated">当想法流过你的大脑时，平均只有大约2%的神经元在放电。大部分都是沉默。因为在任何时候只有一小部分神经元是活跃的，所以噪音和其他失真很难干扰这些网络的模式检测过程。稀疏性使我们的生物网络具有弹性和强健性。</li><li id="0aab" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">相反，在我们的<strong class="kw iu">人工深度学习网络</strong>中，大多数神经元都在<strong class="kw iu">中持续产生输出</strong>(一些可能通过它们的激活函数将其输出设置为0)。这就是为什么深度学习系统<strong class="kw iu">通常非常脆弱，对</strong>我们称之为<strong class="kw iu">对抗性攻击/例子</strong>非常敏感的潜在原因之一。<strong class="kw iu">对抗性攻击</strong>是<strong class="kw iu">微妙的</strong>，网络输入的最小<strong class="kw iu">变化(通常我们的感知不可见)，即<strong class="kw iu">在其输出</strong>中产生剧烈且不正确的变化。深度学习网络的<strong class="kw iu">非稀疏特性</strong>使得它们<strong class="kw iu">对其输入的变化</strong>更加敏感。当大部分权重都是相关的并且一直在发挥作用时，<strong class="kw iu">任何变化都会产生戏剧性的后果。</strong></strong></li><li id="ec69" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">但是<strong class="kw iu">稀疏性</strong>超越了激活。Numenta是一家著名的研究公司，一群才华横溢的科学家和工程师将神经科学和机器智能研究结合在一起。他们的团队由杰夫·霍金斯和苏布泰·艾哈迈德领导。努门塔的团队已经非常深入地探索了大脑稀疏性的问题，以及与我们的新皮层如何运作相关的其他领域。我们可以从他们的工作、研究和出版物中学到的一件事是，我们的新大脑皮层在两个层面上是稀疏的。</li><li id="4eff" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">首先，如上所述，就激活而言，<strong class="kw iu">最好的估计是，在任何时候，我们的生物神经元中有0.5%到2%是活跃的。</strong></li><li id="c33b" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">然后，我们也有<strong class="kw iu">神经元之间连接</strong>的稀疏性。<strong class="kw iu">当一层神经元投射到另一层神经元上时，Numenta的团队告诉我们，目前的估计表明有1%到10%的可能神经元之间的连接存在。</strong></li><li id="6503" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">相比之下，<strong class="kw iu">现在大多数深度学习系统都非常密集。</strong> <strong class="kw iu">就连通性而言100%密集，</strong>通常。大约50%的激活率。</li></ul><p id="e04d" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">最新消息:</strong>利用稀疏连接和稀疏激活的新架构是一个正在进行的研究领域。不管A .我不一定要复制大脑做什么，稀疏作为一种策略，在构建更有弹性和更健壮的系统的任务中非常有意义。</p><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi lq"><img src="../Images/47ebc90675e5940812aa7908a68d8bde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cg7YQskDvbb-xq2YKOD4Yw.jpeg"/></div></div><p class="kp kq gj gh gi kr ks bd b be z dk translated">作者Javier Ideami |<a class="ae kt" href="https://ideami.com" rel="noopener ugc nofollow" target="_blank">https://ideami.com</a>绘画</p></figure><h1 id="4df6" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">输入和参数</h1><p id="d03c" class="pw-post-body-paragraph ku kv it kw b kx mp ju kz la mq jx lc ld mr lf lg lh ms lj lk ll mt ln lo lp im bi translated">最终我们想用这些神经元来学习。所以现在让我们更深入地了解这些网络为了产生学习而调整的<strong class="kw iu">旋钮</strong>。之后，我们将研究学习算法本身。我们从比较两种神经网络的<strong class="kw iu">输入和参数</strong>开始。</p><ul class=""><li id="d44f" class="mu mv it kw b kx ky la lb ld mw lh mx ll my lp mz na nb nc bi translated">在<strong class="kw iu">生物网络</strong>中，你可以有<strong class="kw iu"> 3种类型的输入</strong> : <strong class="kw iu">兴奋性</strong>(使接收神经元更容易激发)<strong class="kw iu">抑制性</strong>(做相反的事情)和<strong class="kw iu">泄漏</strong>(与抑制性的功能相似)。</li><li id="c708" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">如前所述，这些<strong class="kw iu">输入通过<strong class="kw iu">突触</strong>、<strong class="kw iu">发送和接收神经元</strong>之间的连接点与接收神经元</strong>对接。大多数突触位于接收神经元的树突上。</li><li id="93f3" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated"><strong class="kw iu">树突是从神经元上分出的分支</strong>(树突来自希腊语- <strong class="kw iu"> dendro </strong> -，意为树)。在树突处，不同的<strong class="kw iu">输入信号被整合</strong>。这些树突上有小刺。正是在那里<strong class="kw iu">来自发送神经元(轴突)的输出接口(突触)</strong>建立与其他神经元的连接。</li><li id="a041" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">在<strong class="kw iu">人工网络中，</strong>通常有<strong class="kw iu">单一类型的输入，</strong>通常有多个相关联的<strong class="kw iu">权重</strong>(表示该输入与连接到它的多个其他神经元之间的连接强度的数字，每个连接一个权重)。</li><li id="8cee" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">那些权重<strong class="kw iu">保存可以是负的或正的连续值。</strong>每个权重值，结合在神经元上执行的计算，实际上将有助于使接收神经元或多或少地活跃(类似于前面描述的兴奋-抑制动态)。</li><li id="a4f4" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">因此，<strong class="kw iu">在生物神经元中，</strong>有这种<strong class="kw iu">兴奋和抑制信号之间的战斗</strong>。这场战斗的结果决定了细胞膜上的电压。为了让神经元<strong class="kw iu">激发</strong>，膜电压<strong class="kw iu">需要超过动作电位阈值</strong>。</li><li id="fef6" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">在<strong class="kw iu">人工神经元中，</strong>事情更简单。没有明确的阈值和<strong class="kw iu">，每个<strong class="kw iu">权重的不同强度</strong>(正或负)组合起来或多或少地刺激</strong>接收神经元。</li></ul><p id="6e82" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">让我们<strong class="kw iu">更接近两个实体的参数</strong>。</p><ul class=""><li id="26f5" class="mu mv it kw b kx ky la lb ld mw lh mx ll my lp mz na nb nc bi translated">在<strong class="kw iu">生物网络</strong>中，我们有<strong class="kw iu">突触权重的概念，</strong>它<strong class="kw iu">决定了来自发送神经元</strong>的信号通过其突触连接对接收神经元 <strong class="kw iu">的影响。</strong></li><li id="8f29" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">更近一点来说，这种<strong class="kw iu">影响所代表的</strong>是<strong class="kw iu">发送神经元的动作电位释放神经递质</strong>、<strong class="kw iu">以及这些神经递质打开接收侧突触通道</strong>的能力。</li><li id="251e" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">在<strong class="kw iu">人工网络中，</strong>我们有<strong class="kw iu">权重</strong>来决定发送和接收神经元之间每个连接的强度。而那些<strong class="kw iu">重量就是</strong>简单来说就是<strong class="kw iu">数字</strong>。它们可以是浮点数、整数、单比特等。</li><li id="f9f9" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">因此，尽管<strong class="kw iu">人工重量是一个简单的数字，</strong>生物<strong class="kw iu">突触重量取决于许多因素</strong>。这些因素包括，例如，可以释放到突触中并在另一侧被吸收的神经递质的<strong class="kw iu">数量(在这里，特定种类的受体和离子的数量开始发挥作用)，信号在轴突中移动的程度(轴突中的<strong class="kw iu">髓鞘化</strong>对此有影响)，信号传播的<strong class="kw iu">效率</strong>以及轴突和接收神经元的树突之间的<strong class="kw iu">数量</strong>。正如我们所见，这远远超出了一个简单的数字。</strong></li></ul><p id="ebf9" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">这些就是这些网络的参数。还有<strong class="kw iu">把</strong>拉远一点，<strong class="kw iu">他们代表着什么</strong>？</p><ul class=""><li id="e80e" class="mu mv it kw b kx ky la lb ld mw lh mx ll my lp mz na nb nc bi translated">一般来说，这些权重代表<strong class="kw iu">一个神经元对什么敏感，</strong> <strong class="kw iu">它正在检测什么</strong>。如果一个<strong class="kw iu">权重值很大，</strong>它<strong class="kw iu">意味着相关神经元对它正在接收的输入非常敏感</strong>。</li><li id="b1c5" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">因此，我们可以感觉到<strong class="kw iu">学习过程，在两种情况下</strong>，都与<strong class="kw iu">改变和调整这些权重有关，</strong>随着学习的进展，在网络中产生不同的模式。</li><li id="2750" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">所以，为此做好准备:<strong class="kw iu">你的每一个想法和记忆都由突触权重模式</strong>来代表。类似的事情也发生在我们的人工网络中，数字权重模式代表不同抽象层次的信息，这些模式在整个学习过程中不断进化。</li></ul><p id="73d3" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们得到了结构、输入、参数和输出。该学习了！</p><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi lq"><img src="../Images/b21225f430ae1cd8ac9fa8a5ba150f15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jUXsRVbm9rVyKJHfWi-CEA.jpeg"/></div></div><p class="kp kq gj gh gi kr ks bd b be z dk translated">作者Javier Ideami的绘画|【https://ideami.com T2】T3</p></figure><h1 id="9316" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">学习:反向传播和超越</h1><p id="03f6" class="pw-post-body-paragraph ku kv it kw b kx mp ju kz la mq jx lc ld mr lf lg lh ms lj lk ll mt ln lo lp im bi translated">为了让<strong class="kw iu">学会</strong>，我们需要<strong class="kw iu">调整那些权重</strong>，那些参数，并且朝着正确的方向去做。但是怎么做呢？学习算法是什么？</p><ul class=""><li id="781c" class="mu mv it kw b kx ky la lb ld mw lh mx ll my lp mz na nb nc bi translated">在我们的<strong class="kw iu">人工深度学习网络</strong>、<strong class="kw iu">反向传播、</strong>结合<strong class="kw iu">梯度下降、</strong>是<strong class="kw iu">典型的</strong> <strong class="kw iu">算法</strong>的选择。</li><li id="2f6e" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">例如，在受监督的系统中(我们提供一个带有标签的数据集，比如一些带有标识动物种类的标签的动物图像)，我们运行网络，然后计算它的<strong class="kw iu">性能、它的损失值</strong>或误差(我们正在获得的和我们想要获得的之间的差异)。</li><li id="475e" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">然后，从网络的输出开始<strong class="kw iu">并向其输入</strong> ( <strong class="kw iu">反向传播</strong>当前损耗值<strong class="kw iu">，这就是为什么我们称之为反向传播)，我们使用<strong class="kw iu">演算</strong>及其<strong class="kw iu">链式法则</strong>到<strong class="kw iu">的力量计算网络的每个参数对最终损耗值的影响。</strong>我们能够做到这一点，因为在神经网络的不同层执行的所有计算都是可微分的。</strong></li><li id="ef42" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">一旦我们知道调整每个权重将如何影响网络的最终损失值，我们就可以继续<strong class="kw iu">调整每个参数</strong> <strong class="kw iu">使最终损失值最小化，</strong>我们的目标和我们在每个时刻的位置之间的差异。</li><li id="3a07" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">如果我们<strong class="kw iu">继续重复这个过程</strong>，沿着这些梯度向下移动<strong class="kw iu">，</strong>我们将<strong class="kw iu">到达一个地方，在这个地方，我们所有权重的组合产生的计算在我们的目标值和我们当前的网络输出之间产生一个非常小的差异</strong>。学习已经发生了。</li><li id="25ed" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">那么我们的生物网络呢？大脑中是否存在类似于反向传播的现象？围绕这个有<strong class="kw iu">争议</strong>。<strong class="kw iu">一些专家</strong>认为<strong class="kw iu">大脑中可能有</strong>正在进行的事情，虽然<strong class="kw iu">与</strong>不同，但可能与反向传播有相似之处。<strong class="kw iu">其他人认为</strong>我们的生物网络学习方式<strong class="kw iu">与之无关。</strong>因此，<strong class="kw iu">评审团仍未确定</strong>，这一领域有很多活跃的研究。</li></ul><p id="8c89" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">怎么了:反向传播</strong>是一个很棒的学习算法。然而，像任何算法一样，它有优点也有缺点。如果我们超越了在大脑中寻找类似反向传播的东西，并考虑其他选择，会怎么样？研究员Ben Goertzel是AGI领域的专家，他认为我们最终会超越反向传播，使用其他种类的学习算法，以更好的方式适应未来AGI系统的需求。这些可能包括应用于复杂神经架构的<strong class="kw iu"> CMA-ES </strong>类型的<strong class="kw iu">进化算法</strong>。</p><p id="44d7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Ben告诉我们,<strong class="kw iu">如果我们使用这些进化算法</strong>，我们就可以，例如，使用<strong class="kw iu">推理进行适应性估计</strong>和<strong class="kw iu">其他策略来指导进化学习</strong>过程，这些策略在我们使用反向传播时更难实施。</p><p id="53aa" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">Ben提出了一个非常有趣的问题:<strong class="kw iu">有多少神经架构仅仅因为不适合与反向传播算法一起工作而被丢弃？这是一个很好的提醒，让我们的选择和思维对新的可能性保持开放。</strong></p><p id="9c18" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">我们的<strong class="kw iu">大脑</strong>受益于<strong class="kw iu">两种学习过程:</strong> <strong class="kw iu">进化过程</strong>将<strong class="kw iu">编码在我们的基因</strong>中，而<strong class="kw iu">则发生在我们一生中的神经网络</strong>中。在我们的人工网络中结合这两种方法可以打开新进展的大门。</p><h1 id="506c" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">进化:运动中的一切</h1><p id="8d3a" class="pw-post-body-paragraph ku kv it kw b kx mp ju kz la mq jx lc ld mr lf lg lh ms lj lk ll mt ln lo lp im bi translated">这就把我们带到了这些神经结构的进化方面。</p><ul class=""><li id="1c56" class="mu mv it kw b kx ky la lb ld mw lh mx ll my lp mz na nb nc bi translated">现在，你大脑皮层中的神经网络已经和几个小时前不同了。他们<strong class="kw iu">从未停止进化</strong>。</li><li id="7577" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">总的来说，缩小来看，在我们的生物网络中，有许多不同层次的<strong class="kw iu">优化过程在进行</strong>，不仅在参数方面，而且在网络本身、结构、算法等层面(例如，与我们的基因组相关)。</li><li id="04b9" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">当处于活动状态时，<strong class="kw iu">我们典型且最受欢迎的人工深度学习系统</strong> <strong class="kw iu">通过使用反向传播来优化它们的参数</strong>(权重)，而<strong class="kw iu">就是这样。</strong>架构本身保持静态，除了我们不时对其超参数进行的更改(手动或通过<strong class="kw iu"> autoML </strong>、<strong class="kw iu">网格搜索</strong>和其他类似选项)。</li></ul><p id="ca7b" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">最新进展:</strong>关于<strong class="kw iu">自我优化机制的研究</strong>这可能允许深度学习架构<strong class="kw iu">随着学习过程的进行，转变和进化它们的结构并优化它们的策略</strong>，这可能使我们的人工网络更具适应性和灵活性。像Kenneth Stanley教授这样的研究人员已经对朝着这个方向发展的动态系统产生了非常有趣的结果。</p><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi lq"><img src="../Images/f83007ddd32e14915e6627526b34c82e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y0Wi-xlIfRy6c1-Qzn8tjQ.jpeg"/></div></div><p class="kp kq gj gh gi kr ks bd b be z dk translated">神经森林，作者Javier Ideami绘制|<a class="ae kt" href="https://ideami.com" rel="noopener ugc nofollow" target="_blank">https://ideami.com</a></p></figure><h1 id="7080" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">何时:持续学习</h1><p id="c507" class="pw-post-body-paragraph ku kv it kw b kx mp ju kz la mq jx lc ld mr lf lg lh ms lj lk ll mt ln lo lp im bi translated">这些学习过程需要多长时间？</p><ul class=""><li id="44f7" class="mu mv it kw b kx ky la lb ld mw lh mx ll my lp mz na nb nc bi translated">在我们<strong class="kw iu">当前最典型的深度学习网络</strong>中，训练过程有<strong class="kw iu">开始和结束</strong>。我们首先训练，完成学习过程，然后执行我们所谓的<strong class="kw iu">推理</strong>。我们在一个单独的过程中将这种学习应用于以前未见过的数据。</li><li id="90f9" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">随着新数据的到来，我们可以不断地重新训练我们的网络，并迭代地更新我们的模型。</li><li id="0a5a" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">在我们的大脑中，学习过程从不睡觉。持续的学习正在进行。当我们思考、行动甚至睡觉时，我们突触连接的强度也会发生变化。</li></ul><p id="ef7f" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">最近怎么样:持续学习是人工智能社区的热门话题。我们知道，如果我们最终要达到一种更加灵活和强大的人工智能，学习需要有更多的连续性。这一领域正在进行大量的研究。</p><h1 id="9d90" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">让我们社交吧</h1><p id="f160" class="pw-post-body-paragraph ku kv it kw b kx mp ju kz la mq jx lc ld mr lf lg lh ms lj lk ll mt ln lo lp im bi translated">生物神经元比我们的人造神经元更加社会化。这是什么意思？</p><ul class=""><li id="5fa8" class="mu mv it kw b kx ky la lb ld mw lh mx ll my lp mz na nb nc bi translated">在典型的深度学习网络中，<strong class="kw iu">人工神经元</strong>在朝向下一层的单一方向上进行<strong class="kw iu">通信(反向传播计算反向进行)，并且它们只连接到前一层和下一层。也有例外，但我们在这里谈论的是最典型的深度学习网络。</strong></li><li id="1e1a" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated"><strong class="kw iu">生物神经元</strong>可能在多个方向上进行交流，并拥有<strong class="kw iu">更广泛和更丰富的连接</strong>(同时也是更稀疏的那些)<strong class="kw iu">。</strong>一些神经元在皮质柱上下沟通。其他人有横向联系。我前面提到的时间方面为这个过程引入了一个更丰富的方面。</li></ul><p id="2ec4" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">进展如何:</strong>利用更灵活和更丰富的连接形式的新型人工智能架构是另一个活跃的研究领域。例如，<strong class="kw iu">研究人员</strong>正在研究<strong class="kw iu">图形神经网络</strong>和其他使用<strong class="kw iu">超图</strong>和<strong class="kw iu">元图的架构。</strong>由<strong class="kw iu"> Ben Goertzel、</strong>创立的<strong class="kw iu"> singularityNET </strong>项目、<strong class="kw iu"> </strong>在这方面做了很多工作。它将<strong class="kw iu">区块链</strong>技术与人工智能服务相结合，产生了一个<strong class="kw iu">去中心化</strong> <strong class="kw iu"> A.I网络</strong>。最近，该项目与<strong class="kw iu"> Cardano </strong>生态系统合作，加速其向全球分散AGI系统的进展。</p><figure class="ki kj kk kl gt km gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ni"><img src="../Images/08284987878dce61f615d697fc27ab43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sKGUBmAxsWTGBV6O8tJWmA.jpeg"/></div></div><p class="kp kq gj gh gi kr ks bd b be z dk translated">作者Javier Ideami |<a class="ae kt" href="https://ideami.com" rel="noopener ugc nofollow" target="_blank">https://ideami.com</a>绘画</p></figure><h1 id="538b" class="lx ly it bd lz ma mb mc md me mf mg mh jz mi ka mj kc mk kd ml kf mm kg mn mo bi translated">放大</h1><p id="d30a" class="pw-post-body-paragraph ku kv it kw b kx mp ju kz la mq jx lc ld mr lf lg lh ms lj lk ll mt ln lo lp im bi translated">为了完成我们的旅程，让我们暂时缩小一下。</p><ul class=""><li id="9cf7" class="mu mv it kw b kx ky la lb ld mw lh mx ll my lp mz na nb nc bi translated">参与我们智力高级部分的神经元位于我们的新大脑皮层。</li><li id="7ea8" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">我们的大脑皮层由<strong class="kw iu">微柱构成，</strong>每个微柱由大约<strong class="kw iu"> 100个神经元</strong>组成，这些神经元处理相似类型的数据。</li><li id="de5d" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">许多<strong class="kw iu">微柱根据皮层柱</strong>构成，我们的新皮层中有<strong class="kw iu">大约150000个柱。</strong></li><li id="0723" class="mu mv it kw b kx nd la ne ld nf lh ng ll nh lp mz na nb nc bi translated">如果你有兴趣更深入地了解这些皮质专栏中发生的事情，我建议你去看看科学家兼企业家杰夫·霍金斯的书<strong class="kw iu">“一千个大脑:一种新的智力理论】</strong>，这是一本真正的杰作，他在书中剖析了他的团队在<strong class="kw iu"> Numenta进行的最新研究。</strong></li></ul><p id="5ee7" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated">如果你想进一步探索最近的神经科学研究如何为实现更具弹性、一致性和灵活性的人工智能指明方向，比如杰夫·霍金斯和他的团队<strong class="kw iu">所做的研究</strong>，你可以看看下面我写的另一篇关于这个主题的文章</p><div class="nj nk gp gr nl nm"><a rel="noopener follow" target="_blank" href="/towards-the-end-of-deep-learning-and-the-beginning-of-agi-d214d222c4cb"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd iu gy z fp nr fr fs ns fu fw is bi translated">走向深度学习的终点和AGI的起点</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">如何最近的研究指出了战胜对立的例子，并实现更有弹性，一致…</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">towardsdatascience.com</p></div></div><div class="nv l"><div class="nw l nx ny nz nv oa lv nm"/></div></div></a></div><p id="8b10" class="pw-post-body-paragraph ku kv it kw b kx ky ju kz la lb jx lc ld le lf lg lh li lj lk ll lm ln lo lp im bi translated"><strong class="kw iu">百万灵魂的声音诗</strong></p><blockquote class="ob"><p id="6ce8" class="oc od it bd oe of og oh oi oj ok lp dk translated"><em class="ol">“当我们接近宏伟的圆柱时，</em></p><p id="7667" class="oc od it bd oe of og oh oi oj ok lp dk translated"><em class="ol">神秘的图案用百万灵魂的声音从远处呼唤着我们……</em></p><p id="21c5" class="oc od it bd oe of og oh oi oj ok lp dk translated">我感觉到最明亮的太阳被压缩在那些奇妙的小点中..</p><p id="0b52" class="oc od it bd oe of og oh oi oj ok lp dk translated">简化成一幅在我们意识中产生共鸣的梦的织锦..</p><p id="abec" class="oc od it bd oe of og oh oi oj ok lp dk translated">我听到你的笑声..我听到你坠落…我听到你的眼泪摧毁了地平线..</p><p id="d11f" class="oc od it bd oe of og oh oi oj ok lp dk translated">直到我们合并在纵队的中央，那里静候着我们..</p><p id="f4e6" class="oc od it bd oe of og oh oi oj ok lp dk translated">寂静，然后百万个太阳向着一个新存在的觉醒刺来..</p><p id="bdf8" class="oc od it bd oe of og oh oi oj ok lp dk translated">抱紧我..让我们直入主题，直入专栏的中心..</p><p id="95fb" class="oc od it bd oe of og oh oi oj ok lp dk translated">你和我在寂静中合而为一..”</p><p id="c01f" class="oc od it bd oe of og oh oi oj ok lp dk translated"><em class="ol"> —作者哈维尔·伊达米</em></p></blockquote></div></div>    
</body>
</html>