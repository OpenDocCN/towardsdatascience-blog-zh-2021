<html>
<head>
<title>Linking Images and Text with OpenAI CLIP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用OpenAI剪辑链接图像和文本</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/linking-images-and-text-with-openai-clip-abb4bdf5dbd2?source=collection_archive---------12-----------------------#2021-06-24">https://towardsdatascience.com/linking-images-and-text-with-openai-clip-abb4bdf5dbd2?source=collection_archive---------12-----------------------#2021-06-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="7323" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="57e3" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">什么是剪辑以及如何使用它</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/ea26b1b5364fe6e83334991f7960ed19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*shkGxeltzKYM3Jt5TBRbFA.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由来自Unsplash的<a class="ae lh" href="https://unsplash.com/@laughayette" rel="noopener ugc nofollow" target="_blank"> Marten Newhall </a>拍摄。</p></figure><h1 id="f1fa" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">介绍</h1><p id="f3a9" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">尽管深度学习已经彻底改变了计算机视觉和自然语言处理，但使用当前最先进的方法仍然很困难，并且需要相当多的专业知识。</p><p id="3bb2" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">OpenAI方法，如对比语言图像预训练(CLIP)，旨在降低这种复杂性，从而使开发人员专注于实际案例。</p><p id="8597" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">CLIP是一个在大量(400M)图像和文本对上训练的神经网络。作为这种多模态训练的结果，CLIP可以用于找到最能代表给定图像的文本片段，或者给定文本查询的最合适的图像。</p><p id="0606" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这使得CLIP对于开箱即用的图像和文本搜索非常有用。</p></div><div class="ab cl nb nc hx nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="im in io ip iq"><h1 id="7f0d" class="li lj it bd lk ll ni ln lo lp nj lr ls ki nk kj lu kl nl km lw ko nm kp ly lz bi translated">它是如何工作的？</h1><p id="a5b3" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">对CLIP进行训练，使得给定一幅图像，它预测该图像在训练数据集中与32768个随机采样的文本片段中的哪一个配对。这个想法是，为了解决这个任务，模型需要从图像中学习多个概念。</p><p id="05f8" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这种方法与传统的图像任务有很大的不同，在传统的图像任务中，通常需要模型来从一大组类别中识别出一个类别(例如ImageNet)。</p><p id="7a32" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">总之，CLIP联合训练一个图像编码器(像ResNet50)和一个文本编码器(像BERT)来预测一批图像和文本的正确配对。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nn"><img src="../Images/fb7697809109cacf384118bb2c8daf38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OVi8blLZw_wf2rrxdlfbdg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">剪辑训练(1)和零镜头学习(2) (3)的原始插图可从<a class="ae lh" href="https://arxiv.org/abs/2103.00020" rel="noopener ugc nofollow" target="_blank">从自然语言监督</a>论文中学习可转移视觉模型获得。(1)在N维向量中处理和编码图像和文本片段。该模型通过最小化正确图像-文本对(N个真实对)之间的余弦距离，同时最大化不正确对(N -N)之间的余弦距离来训练。(2)为了使用剪辑模型进行零镜头学习，类值被编码在文本片段中。(3)将每个类别值的文本嵌入与图像嵌入进行比较，并通过相似性进行排序。如需详细描述，请阅读回形针。</p></figure><p id="edce" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">如果希望使用该模型进行分类，可以通过文本编码器嵌入类别，并与图像进行匹配。这个过程通常被称为零射击学习。</p></div><div class="ab cl nb nc hx nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="im in io ip iq"><h1 id="d9e9" class="li lj it bd lk ll ni ln lo lp nj lr ls ki nk kj lu kl nl km lw ko nm kp ly lz bi translated">入门指南</h1><p id="fffd" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">以下部分解释了如何在Google Colab中设置CLIP，以及如何使用CLIP进行图像和文本搜索。</p><h2 id="e070" class="no lj it bd lk np nq dn lo nr ns dp ls mj nt nu lu mn nv nw lw mr nx ny ly iz bi translated">装置</h2><p id="ebad" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">要使用CLIP，我们首先需要安装一组依赖项。为了方便起见，我们将通过Conda安装它们。此外，谷歌Colab将用于使复制更容易。</p><ol class=""><li id="c276" class="nz oa it mc b md mw mg mx mj ob mn oc mr od mv oe of og oh bi translated"><em class="oi">打开Google Colab </em></li></ol><p id="fd46" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">在浏览器中打开以下网址:<a class="ae lh" href="https://research.google.com/colaboratory/" rel="noopener ugc nofollow" target="_blank">https://research.google.com/colaboratory/</a></p><p id="6891" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">然后，点击屏幕下方的<strong class="mc jd">新PYTHON 3笔记本</strong>链接。</p><p id="51ed" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">你可能注意到了，笔记本的界面和Jupyter提供的很像。有一个代码窗口，您可以在其中输入Python代码。</p><p id="a79d" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated"><em class="oi"> 2。检查Colab中的Python</em></p><p id="035c" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">为了安装正确的Conda版本，使其看起来可以与Colab一起工作，我们首先需要知道Colab使用的是哪个Python版本。在colab类型的第一个单元格中这样做</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="fff1" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这应该会返回类似于</p><pre class="ks kt ku kv gt ol om on oo aw op bi"><span id="ca9f" class="no lj it om b gy oq or l os ot">/usr/local/bin/python <br/>Python 3.7.10</span></pre><p id="9a3a" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated"><em class="oi"> 3。安装康达</em></p><p id="4917" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">在你的浏览器中打开以下网址:<br/><a class="ae lh" href="https://repo.anaconda.com/miniconda/" rel="noopener ugc nofollow" target="_blank">https://repo.anaconda.com/miniconda/</a></p><p id="0c6a" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">然后复制与上面输出中指示的主要Python版本相对应的miniconda版本名称。miniconda版本应该类似于“miniconda 3-py { VERSION }-Linux-x86 _ 64 . sh”。</p><p id="fb57" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">最后，在colab的新单元格中键入以下代码片段，确保conda_version变量设置正确。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="c402" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">再次确认Python主版本仍然是相同的</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="5cee" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这应该会返回类似于</p><pre class="ks kt ku kv gt ol om on oo aw op bi"><span id="e002" class="no lj it om b gy oq or l os ot">/usr/local/bin/python <br/>Python 3.7.10</span></pre><p id="0bcc" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated"><em class="oi"> 4。安装剪辑+依赖关系</em></p><p id="a897" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">康达现在应该很有起色了。下一步是使用Conda安装剪辑模型的依赖项(pytorch、torchvision和cudatoolkit ),然后安装剪辑库本身。</p><p id="801a" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">为此，将下面的代码片段复制到Colab中。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="7cfa" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这一步可能需要一段时间，因为所需的库很大。</p><p id="d5d1" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated"><em class="oi"> 5。将conda路径附加到系统</em></p><p id="04f5" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">使用CLIP之前的最后一步是将conda site-packages路径附加到sys。否则，在Colab环境中可能无法正确识别已安装的软件包。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oj ok l"/></div></figure><h2 id="44f2" class="no lj it bd lk np nq dn lo nr ns dp ls mj nt nu lu mn nv nw lw mr nx ny ly iz bi translated">文本和图像</h2><p id="dfd9" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">我们的环境现在可以使用了。</p><ol class=""><li id="08b0" class="nz oa it mc b md mw mg mx mj ob mn oc mr od mv oe of og oh bi translated"><em class="oi">导入剪辑模型</em></li></ol><p id="b6ab" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">通过导入所需的库并加载模型来使用CLIP start。为此，将下面的代码片段复制到Colab。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="9cc9" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这应该显示类似下面的返回，表明模型被正确加载。</p><pre class="ks kt ku kv gt ol om on oo aw op bi"><span id="9bfd" class="no lj it om b gy oq or l os ot">100%|███████████████████████████████████████| 354M/354M [00:11&lt;00:00, 30.1MiB/s]</span></pre><p id="fc1a" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated"><em class="oi"> 2。提取图像嵌入</em></p><p id="8045" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">现在让我们使用下面的示例图像来测试模型</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ou"><img src="../Images/816a322348c868e62078f1644d264c98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q9jFN84IwCFuiZxG5bOX9Q.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片来自Pexels的Artem Beliaikin</p></figure><p id="0578" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">为此，将下面的代码片段复制到Colab。这段代码将首先使用PIL加载图像，然后使用剪辑模型对其进行预处理。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="3598" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这将显示样本图像，然后是经过处理的图像张量。</p><pre class="ks kt ku kv gt ol om on oo aw op bi"><span id="2bd2" class="no lj it om b gy oq or l os ot">Tensor shape: <br/>torch.Size([1, 3, 224, 224])</span></pre><p id="c7b8" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">现在可以通过从剪辑模型中调用“encode_image”方法来提取图像特征，如下所示</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="a73a" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这应该返回图像特征的张量大小</p><pre class="ks kt ku kv gt ol om on oo aw op bi"><span id="135f" class="no lj it om b gy oq or l os ot">torch.Size([1, 512])</span></pre><p id="7dfb" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated"><em class="oi"> 3。提取文本嵌入内容</em></p><p id="2274" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">让我们创建一组文本片段，其中不同的类值以如下方式嵌入:“一个#CLASS#的照片”。</p><p id="93d0" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">然后我们可以运行clip tokeniser来预处理代码片段。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="6802" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这将返回文本张量形状</p><pre class="ks kt ku kv gt ol om on oo aw op bi"><span id="2f1d" class="no lj it om b gy oq or l os ot">torch.Size([3, 77])</span></pre><p id="0aa1" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">现在可以通过从剪辑模型中调用“encode_text”方法来提取文本特征，如下所示</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="4a70" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated"><em class="oi"> 4。比较图像嵌入和文本嵌入</em></p><p id="d1b7" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">因为我们现在有了图像和文本嵌入，我们可以比较每个组合，并根据相似性对它们进行排序。</p><p id="a24d" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">为此，我们可以简单地调用两个嵌入的模型，并计算softmax。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oj ok l"/></div></figure><p id="2367" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这应该会返回以下输出。</p><pre class="ks kt ku kv gt ol om on oo aw op bi"><span id="3706" class="no lj it om b gy oq or l os ot">Label probs: [[0.9824866 0.00317319 0.01434022]]</span></pre><p id="0618" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">正如所料，我们可以观察到“一张狗的照片”文本片段与样本图像具有最高的相似性。</p><p id="0623" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">现在，您可以让文本查询包含更多的上下文，并查看它们之间的比较。例如，如果你添加“一张狗在草地上跑的照片”，你会想象现在的排名会是什么样子？</p><h2 id="bca5" class="no lj it bd lk np nq dn lo nr ns dp ls mj nt nu lu mn nv nw lw mr nx ny ly iz bi translated">完整脚本</h2><p id="f88a" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">如需完整的脚本，请点击以下链接进入我的github页面:</p><div class="ov ow gp gr ox oy"><a href="https://github.com/andreRibeiro1989/medium/blob/ed800bad2c636049ea789dfd77598a8b72e3e42f/clip_getting_started.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd jd gy z fp pd fr fs pe fu fw jc bi translated">安德里贝罗1989/中号</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">clip_getting_started.ipynb。</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">github.com</p></div></div><div class="ph l"><div class="pi l pj pk pl ph pm lb oy"/></div></div></a></div><p id="f74f" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">或者通过以下链接直接访问Google Colab笔记本:</p><div class="ov ow gp gr ox oy"><a href="https://colab.research.google.com/github/andreRibeiro1989/medium/blob/main/clip_getting_started.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="oz ab fo"><div class="pa ab pb cl cj pc"><h2 class="bd jd gy z fp pd fr fs pe fu fw jc bi translated">谷歌联合实验室</h2><div class="pf l"><h3 class="bd b gy z fp pd fr fs pe fu fw dk translated">clip_getting_started.ipynb</h3></div><div class="pg l"><p class="bd b dl z fp pd fr fs pe fu fw dk translated">colab.research.google.com</p></div></div><div class="ph l"><div class="pn l pj pk pl ph pm lb oy"/></div></div></a></div></div><div class="ab cl nb nc hx nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="im in io ip iq"><h1 id="1ac1" class="li lj it bd lk ll ni ln lo lp nj lr ls ki nk kj lu kl nl km lw ko nm kp ly lz bi translated">结论</h1><p id="8b17" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">CLIP是一个非常强大的图像和文本嵌入模型，可用于查找最能代表给定图像的文本片段(如在经典分类任务中)，或给定文本查询的最合适图像(如图像搜索)。</p><p id="91f0" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">CLIP不仅功能强大，而且非常易于使用。该模型可以很容易地嵌入到API中，并通过AWS lambda函数等方式提供。</p></div><div class="ab cl nb nc hx nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="im in io ip iq"><p id="8500" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">[1] Openai。【https://openai.com/blog/clip/#rf36】剪辑:连接文字和图像<br/>T5</p><p id="9add" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">[2]亚历克·拉德福德，琼·金旭，克里斯·哈拉西等.<em class="oi">从自然语言监督中学习可转移的视觉模型</em><br/><a class="ae lh" href="https://arxiv.org/abs/2103.00020" rel="noopener ugc nofollow" target="_blank">arXiv:2103.00020</a></p></div></div>    
</body>
</html>