<html>
<head>
<title>Taking Keras and TensorFlow to the Next Level</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">让Keras和TensorFlow更上一层楼</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/taking-keras-and-tensorflow-to-the-next-level-c73466e829d3?source=collection_archive---------7-----------------------#2021-09-24">https://towardsdatascience.com/taking-keras-and-tensorflow-to-the-next-level-c73466e829d3?source=collection_archive---------7-----------------------#2021-09-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e40e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">充分利用Keras和TensorFlow的11个技巧和诀窍</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/215d34e12e2ad3c45c0dbaa422d8f2d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7L7x5Xo1jJ0-loTJ"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">保罗·赫顿在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="cd64" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated">时代是一个美丽的项目。虽然TensorFlow和PyTorch过去一直在竞争最先进的技术，但Keras的目标是<em class="me"> us </em>，需要完成工作的专业人士。我们可以使用去年的模型，而不是打赌下一个变形金刚杀手或ResNet的崇拜者。作为TensorFlow的一部分，Keras已经有近两年的时间了，这表明该框架支持易用性和生产率，而不是速度。作为TensorFlow和Keras的用户，我在本文中分享了我多年来开发的一组技巧和诀窍，以将这些框架提升到一个新的水平。</p><p id="31d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我强调这不是一个优化指南。相反，这些技巧旨在使开发模型和实现定制功能变得更容易——所有这些都不会牺牲Keras最可爱的方面，它为您完成了大部分工作。</p><p id="7f8d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">开始了。</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h1 id="3113" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">始终使用函数式API</h1><p id="7a44" class="pw-post-body-paragraph kz la it lb b lc ne ju le lf nf jx lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">使用Keras定义模型有三种主要方法:作为层列表(顺序方法)，作为功能组合(函数方法)，或者通过继承模型(子类化方法)。从树中，<em class="me">总是选择功能方法</em>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nj nk l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用Keras功能API的一个简单例子</p></figure><p id="bd03" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的例子中，我们定义了一个64维的输入层，然后定义了一个256个神经元的隐藏层，最后是一个两类输出层。功能方面在第4行和第5行的末尾。隐藏层是处理输入的函数。同样，输出是处理隐藏层的函数。第6行定义的模型本身只是这个函数调用链的起点和终点。</p><p id="ecb0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在Keras文档上阅读更多关于<a class="ae ky" href="https://keras.io/guides/functional_api/" rel="noopener ugc nofollow" target="_blank">功能API的内容。</a></p><p id="235a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用函数式API的原因很简单:它有最好的支持，没有限制。相比之下，顺序方法只支持前馈模型(不允许跳过连接或分支)，而子类化API对保存/加载模型和分布式训练的支持有限。此外，其他不太常见的特性，如web或移动平台的移植模型，在使用函数式API时也能更好地工作。</p><h1 id="f64f" class="mm mn it bd mo mp nl mr ms mt nm mv mw jz nn ka my kc no kd na kf np kg nc nd bi translated">使用闭包创建块</h1><p id="add9" class="pw-post-body-paragraph kz la it lb b lc ne ju le lf nf jx lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">使用函数式API时，最大的问题是如何在坚持函数式语法的同时创建可重用的代码片段。解决办法？关闭。</p><p id="70c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">闭包的一般定义是指在它之外定义的变量的函数。在计算机科学词汇中，它“捕捉”了这些变量。在Python中，这与嵌套函数(在其他函数中定义的函数)的概念密切相关，但并不局限于此。下面是一个例子:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nj nk l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在Keras中使用闭包定义简单ResBlock的示例</p></figure><p id="d31e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的代码片段展示了如何创建一个函数(resblock ),该函数返回另一个函数(_block ),该函数捕获“n_filters”参数。当通过一个层被调用时，这个第二个函数将通过添加一个典型的ResBlock的所有步骤来继续函数调用链，如在原始论文(<a class="ae ky" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank"> 2015 </a>、<a class="ae ky" href="https://arxiv.org/abs/1603.05027" rel="noopener ugc nofollow" target="_blank"> 2016 </a>)中所定义的。</p><p id="7b45" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用这种语法的好处在于，它允许您将超参数(定义为第一个函数的参数)与调用该层的先决条件(定义为内部函数的参数)分开。此外，它允许您在两个级别上都放置自定义逻辑。例如，上面的示例展示了如何预处理“start”参数，以确保使用1v1卷积至少有“n_filter”通道。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nj nk l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用闭包在Keras中实现2D自我关注</p></figure><p id="0c28" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">对于高级用户:</strong>展示闭包在一个更复杂的例子中，我们实现了一个2D瓶颈查询-值-键关注块，类似于<a class="ae ky" href="https://arxiv.org/abs/1805.08318" rel="noopener ugc nofollow" target="_blank">自我关注甘斯</a>上所描述的。首先，我们定义了超参数<em class="me"> n_filters </em>，在第一个函数的范围内，我们将瓶颈的程度指定为<em class="me"> n_filters </em>的八分之一。然后，在内部函数中，我们使用1v1卷积对查询和值项进行瓶颈处理，并将所有三个输入重新整形为1D向量。接下来，我们将注意力得分计算为查询和值之间的最大点积，最后，将其与关键术语相结合以获得输出。</p><p id="f4ac" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从第27行开始，我们创建了第二个闭包来简化我们刚刚编码到2D自我关注块中的2D关注，该块同时将单个输入映射为查询、值和键。这些实现共同展示了如何使用闭包的外部和内部范围，以及如何组合闭包以进一步扩展它们的功能。</p><p id="4be4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">人们可以将第18-21行替换为内置的<a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Attention" rel="noopener ugc nofollow" target="_blank"> tf.keras.layers .注意力层，以获得更少的从头开始的解决方案</a>。我刚刚意识到，在编码了这个例子之后:x</p><h1 id="2df0" class="mm mn it bd mo mp nl mr ms mt nm mv mw jz nn ka my kc no kd na kf np kg nc nd bi translated">将操作用于自定义代码</h1><p id="5c46" class="pw-post-body-paragraph kz la it lb b lc ne ju le lf nf jx lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">在层链中，虽然很明显只有层可以链接在一起，但事实是大多数TensorFlow操作都可以用作层。我知道的唯一先决条件是它接受并输出张量。使用原始ops代替纯层的优势有几个。我强调它的可读性更强，更轻量级，也更透明。另外，使用ops有完全的控制权。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nj nk l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">尽可能使用TensorFlow ops修改2D注意力示例</p></figure><p id="1a40" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的代码片段中，计算注意力分数和输出所需的操作被转换为简单的TensorFlow操作。这种语法更接近于你在Numpy、PyTorch甚至JAX中看到的语法。</p><p id="daeb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用ops的缺点是当你需要增加重量时。为此，最好尽可能使用内置层。在上面的例子中，我们坚持使用卷积层，其余的使用ops。对于那些熟悉注意力公式的人来说，有时它包括一个最终的比例参数(gamma)。坦白地说，我不会打扰。</p><p id="1f8c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">好吧，对于那些有麻烦的人，需要权重的简单缩放和移位操作可以使用1v1卷积来模拟，或者更好的是，批量范数或层范数。</p><h1 id="713d" class="mm mn it bd mo mp nl mr ms mt nm mv mw jz nn ka my kc no kd na kf np kg nc nd bi translated">为简洁的摘要包装自定义代码</h1><p id="fa44" class="pw-post-body-paragraph kz la it lb b lc ne ju le lf nf jx lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">使用ops的另一个缺点是在运行model.summary()时会变得很难看。例如，上面的代码片段产生了以下内容:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/42fe1d15a94501783d354d2f251c1e6e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*80kwc6e-MMpB6hOJZzPK7Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">调用model.summary()时，ops会有多难看</p></figure><p id="ad3f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有两种方法可以解决这个问题:</p><ol class=""><li id="fe13" class="nr ns it lb b lc ld lf lg li nt lm nu lq nv lu nw nx ny nz bi translated">实现从<a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer" rel="noopener ugc nofollow" target="_blank">层</a>继承的功能</li><li id="e883" class="nr ns it lb b lc oa lf ob li oc lm od lq oe lu nw nx ny nz bi translated">实现从<a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Wrapper" rel="noopener ugc nofollow" target="_blank">包装器</a>继承的功能</li><li id="212d" class="nr ns it lb b lc oa lf ob li oc lm od lq oe lu nw nx ny nz bi translated">将一切封装在另一个<a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/keras/Model" rel="noopener ugc nofollow" target="_blank">模型</a>中</li></ol><p id="3764" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一种方法是最麻烦的，因为您需要处理Keras内部。然而，当您想用其他东西来扩充现有的层实现时，第二种方法非常有效。一个完美的用例是<a class="ae ky" href="https://arxiv.org/abs/1802.05957" rel="noopener ugc nofollow" target="_blank">光谱归一化</a>的<a class="ae ky" href="https://www.tensorflow.org/addons/api_docs/python/tfa/layers/SpectralNormalization" rel="noopener ugc nofollow" target="_blank">插件的实现</a>。然而，第三个是最实际的(有一些警告)。</p><p id="8fc1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个想法是你可以使用模型作为层(就像你可以使用大多数操作作为层一样)。如果您有一组重复的层(如上面的Attention实现)，您可以将其打包到一个模型中，并在您的网络中使用该模型。这样，您可以将整个子网命名为一个实体。</p><p id="fa21" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">警告是两个:(1)模型没有”。“形状”属性，以及(2)模型不会自动命名。如果自定义块试图访问前一层的形状(如上面的ResBlock实现)，前者会咬你一口，如果是模型，它会失败。后者将触发一个Keras异常，要求所有层都有唯一的名称。因此，对于<strong class="lb iu">高级用户</strong>，我留给您一个<em class="me">的hacky </em>装饰器，它会弄乱唯一id的全局名称空间，并用shape属性对创建的模型进行猴子式修补🙈。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nj nk l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用自动命名和形状属性将自定义块包装为模型的装饰器</p></figure><p id="6ea8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">理解这个片段是留给读者的练习。</p><h1 id="d735" class="mm mn it bd mo mp nl mr ms mt nm mv mw jz nn ka my kc no kd na kf np kg nc nd bi translated">关于自定义训练循环的注释</h1><p id="3287" class="pw-post-body-paragraph kz la it lb b lc ne ju le lf nf jx lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">当TensorFlow 2.0发布时，要创建定制的训练例程，如GANs的对抗性训练，需要完全放弃model.fit()调用，并从头开始重新实现它。这样做是不小的壮举。你必须做你的纪元，有效地得到下一批，写训练和测试步骤，运行你的回调和度量，建立一个进度条，等等。混合精度或分布式培训的加分。这是关于定制训练循环的<a class="ae ky" href="https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch#end-to-end_example_a_gan_training_loop_from_scratch" rel="noopener ugc nofollow" target="_blank">官方文档</a>。</p><p id="3cd6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">TensorFlow 2.2给了我们一个使用子类化模型的好理由:你可以在使用<em class="me"> model.fit() </em>的同时覆盖<em class="me"> train_step </em>、<em class="me"> test_step、</em>和<em class="me"> predict_step </em>调用来定制训练。对于胆小的人来说，重新执行这些呼叫仍然不是一件事情。然而，你不需要重新发明整个战车，只需要轮子。</p><p id="2b9d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">到目前为止，我所知道的实现这些调用的最佳参考是源代码本身(它被认为是文档)。你可以在这里找到它。</p><p id="44f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">到目前为止，我建议在所有情况下都坚持使用函数式API。为了继续，您可以对<em class="me"> train_step </em>、<em class="me"> test_step、</em>和<em class="me"> predict_step </em>调用进行猴子修补。语法有点问题，monkey patching是一个相当糟糕的解决方案，但是它很有效:)。毕竟，如果您正在编写定制的训练循环，那么您已经对Python的魔力了如指掌。</p><p id="f6ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">无论如何，这里是维基百科关于猴子补丁的<a class="ae ky" href="https://en.wikipedia.org/wiki/Monkey_patch#Pitfalls" rel="noopener ugc nofollow" target="_blank">陷阱部分</a></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nj nk l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Monkey修补使用函数式API定义的模型的训练步骤的示例</p></figure><p id="023c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个愚蠢的片段展示了如何对<em class="me"> train_step </em>调用打猴子补丁，以计算它被调用的次数。注意，第19行激活了eager mode，强制Keras不要跟踪这个函数到graph-mode，这将导致它只被调用两次。现实世界的用例不需要使用渴望模式，只需要像这样愚蠢的演示。</p><h1 id="eb0c" class="mm mn it bd mo mp nl mr ms mt nm mv mw jz nn ka my kc no kd na kf np kg nc nd bi translated">知道如何遍历模型</h1><p id="e2ae" class="pw-post-body-paragraph kz la it lb b lc ne ju le lf nf jx lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">您是否曾经问过自己如何迭代所有的模型层，并在每个层上做一些事情？当您需要对每个图层应用一些东西或收集内部数据(如每个图层的平均权重)时，遍历会非常方便。这里的技巧是不要忘记迭代子模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nj nk l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">向模型的所有层添加权重衰减</p></figure><p id="f358" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个如何将权重衰减添加到模型所有层的示例。该算法是对所有层的深度优先扫描，使用递归来深入研究子模型、子模型的子模型等。注意这段代码只支持少数层，但是很容易扩展到其他层。</p><h1 id="747d" class="mm mn it bd mo mp nl mr ms mt nm mv mw jz nn ka my kc no kd na kf np kg nc nd bi translated">使用正确的数据类型</h1><p id="478c" class="pw-post-body-paragraph kz la it lb b lc ne ju le lf nf jx lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated">使用正确的数据类型可以节省50%(比如从float64到float32)到93%的内存(从默认的int64到瘦uint8)。我曾在下面的文章中写过这个问题:</p><div class="of og gp gr oh oi"><a rel="noopener follow" target="_blank" href="/memory-efficient-data-science-types-53423d48ba1d"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd iu gy z fp on fr fs oo fu fw is bi translated">高效内存数据科学:类型</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">通过在正确的时间使用正确的浮点和整数，节省高达90%的内存。</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">towardsdatascience.com</p></div></div><div class="or l"><div class="os l ot ou ov or ow ks oi"/></div></div></a></div><p id="3795" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">特别是对于TensorFlow和Keras来说，有一件事人们通常会不假思索地坚持:一键编码和交叉熵。经验法则是用一键编码对标签进行编码，并为每个类输出一个值。这种做法通常是安全和正确的。然而，对于大规模数据集或具有大量类的问题，使用稀疏标注和稀疏交叉熵可能会改变生活。</p><p id="7840" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">稀疏标签是以索引形式存储的标签:例如，第七个类由数字7表示，而不是由第七个位置设置为1的零向量表示。对于1000个类的问题，这是更紧凑的数量级。一个uint16数对一千个浮点数。默认情况下，大多数数据集使用稀疏标签进行打包，我们强制将它们一次性打包。</p><p id="3fa6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy" rel="noopener ugc nofollow" target="_blank">稀疏交叉熵</a>是<a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy" rel="noopener ugc nofollow" target="_blank">分类交叉熵</a>的一个变种，它将你的模型输出以一个热点格式与真实标签以索引格式进行比较。结合这种损失和稀疏标签，你节省了大量的内存，否则将花费在英亩的零。</p><p id="345f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">“关于定制训练循环的注释”的片段展示了一个使用稀疏交叉熵的例子。</p><h1 id="a48b" class="mm mn it bd mo mp nl mr ms mt nm mv mw jz nn ka my kc no kd na kf np kg nc nd bi translated">额外提示</h1><p id="3b26" class="pw-post-body-paragraph kz la it lb b lc ne ju le lf nf jx lh li ng lk ll lm nh lo lp lq ni ls lt lu im bi translated"><strong class="lb iu">不要忘记急切模式:</strong> TensorFlow在开始抱怨形状和不兼容数据类型的内部错误时可能会很棘手。大多数这些在渴望模式下很容易被发现或者不会发生。记住使用渴望模式有助于追踪bug的源头，或者在将bug缩小到图形模式问题(比如使用正确的数据类型)之前关注全局。</p><p id="a2ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">对tf.data也使用渴望模式:</strong>使用数据API是一件痛苦的事情，因为它总是将一切追溯到图形模式。从TensorFlow 2.6开始，您可以为数据API激活调试模式。这项功能仍处于试验阶段，但会非常方便。<a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/data/experimental/enable_debug_mode" rel="noopener ugc nofollow" target="_blank">下面是怎么做的</a>。</p><p id="1c3e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">使用Lambda回调:</strong>当使用昂贵的指标跟踪训练进度时，使用<a class="ae ky" href="https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LambdaCallback" rel="noopener ugc nofollow" target="_blank"> Lambda回调</a>类注入要在epoch end上运行的代码是值得的。如果您的度量非常慢，您可以使用epoch参数跳过在奇数个时期执行代码，或者每十个时期执行一次。</p><p id="0f71" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">让自己成为一个库:</strong>当你使用深度学习时，你将不得不实现不同的层和架构。使用所有内置的东西作为构建模块，有些很容易实现，有些则不容易。因此，保留一个方便的layers.py文件来保存多年来您必须编写的所有自定义内容是值得的。</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="c18d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated">他的一切都是为了现在。如果您对本文有任何问题，请随时发表评论或与我联系。</p><p id="786b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你是中新，我强烈推荐<a class="ae ky" href="https://ygorserpa.medium.com/membership" rel="noopener">订阅</a>。对于数据和IT专业人员来说，中型文章是StackOverflow的完美搭档，对于新手来说更是如此。注册时请考虑使用<a class="ae ky" href="https://ygorserpa.medium.com/membership" rel="noopener">我的会员链接。</a></p><p id="daee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读:)</p></div></div>    
</body>
</html>