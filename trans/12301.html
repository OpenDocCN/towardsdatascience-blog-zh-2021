<html>
<head>
<title>Text Data Augmentation Using the GPT-2 Language Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用GPT-2语言模型的文本数据扩充</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/text-data-augmentation-using-gpt-2-language-model-e5384e15b550?source=collection_archive---------24-----------------------#2021-12-14">https://towardsdatascience.com/text-data-augmentation-using-gpt-2-language-model-e5384e15b550?source=collection_archive---------24-----------------------#2021-12-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ccfe" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用预训练语言模型GPT-2在自然语言处理中生成合成数据</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b9527bbfe466687c4998afe604a62ffd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y9cRQyO95bcXZh2sZrT0-A.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://unsplash.com/photos/n6B49lTx7NM" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="d4af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di"> D </span>数据增强是深度学习从业者大量使用的一种技术，用于增加他们训练数据集中的多样性和规模，以设计健壮的机器学习系统。每个工程师都希望他们的模型不仅在训练集上表现良好，还能很好地推广到未知场景。因此，除了<a class="ae ky" href="https://en.wikipedia.org/wiki/Overfitting" rel="noopener ugc nofollow" target="_blank">过拟合</a>和<a class="ae ky" rel="noopener" target="_blank" href="/regularization-an-important-concept-in-machine-learning-5891628907ea">正则化</a>之外，决定模型泛化的另一个重要因素是它在训练时间内看到的相关数据的数量和种类。到今天为止，有很多经过测试的<a class="ae ky" href="https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/" rel="noopener ugc nofollow" target="_blank">转换</a>可以在图像上进行增强，在低资源设置下效果惊人。但是对于文本数据就没那么容易了。很简单，因为自然语言封装了不同层次的句法和语义信息。</p><h1 id="e8b1" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">一些现有的方法和问题</h1><p id="ef0a" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">在这一领域，过去的工作集中在使用<a class="ae ky" href="https://wordnet.princeton.edu/" rel="noopener ugc nofollow" target="_blank"> WordNet </a>、<a class="ae ky" href="http://jalammar.github.io/illustrated-word2vec/" rel="noopener ugc nofollow" target="_blank"> Word2Vec </a>等方法对语法中某些特殊类型的单词进行<strong class="lb iu">同义词替换</strong>。这种方法是一个很好的起点，但是在提供可变性方面并没有给我们的模型增加多少价值。此外，这些系统本质上非常脆弱。例如，WordNet有一组固定的单词，通常会导致<a class="ae ky" href="https://medium.com/@shabeelkandi/handling-out-of-vocabulary-words-in-natural-language-processing-based-on-context-4bbba16214d5" rel="noopener">超出词汇表</a>，然而，将预先训练的Word2Vec的最近邻居视为同义词在实践中并不总是能得到想要的结果。举个例子——和“神奇”这个词最近的邻居是“蜘蛛”。考虑到《神奇蜘蛛侠》是一部电影，这在某种意义上也是正确的。</p><p id="8c1c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个领域中一篇有趣的论文是<a class="ae ky" rel="noopener" target="_blank" href="/eda-easy-data-augmentation-techniques-for-boosting-performance-on-text-classification-tasks-3e61a56d1332"> EDA:提高文本分类任务性能的简单数据扩充技术</a>。在本文中，作者讨论了三种扩充文本数据的方法，这些方法已被证明能改善文本分类任务的结果。<em class="nb">请随时关注贴有标签的帖子，了解更多关于此事的详细信息。</em></p><blockquote class="nc nd ne"><p id="0fba" class="kz la nb lb b lc ld ju le lf lg jx lh nf lj lk ll ng ln lo lp nh lr ls lt lu im bi translated">今天，在这个博客中，我们将看到如何使用<strong class="lb iu"> GPT2进行高质量的文本增强</strong>。</p></blockquote><p id="78e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们跳到代码和方法之前，我想花点时间用一两段话解释一下GPT新协议。GPT-2值得拥有自己的独立博客，<em class="nb">你可以关注</em> <a class="ae ky" href="http://jalammar.github.io/illustrated-gpt2/" rel="noopener ugc nofollow" target="_blank"> <em class="nb">图示的GPT-2(可视化变压器语言模型)</em> </a></p><h1 id="262d" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">GPT-2语言模型</h1><p id="8a71" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated"><a class="ae ky" href="http://jalammar.github.io/illustrated-gpt2/" rel="noopener ugc nofollow" target="_blank"> GPT-2 </a>本质上是一个复杂的<a class="ae ky" href="https://en.wikipedia.org/wiki/Language_model" rel="noopener ugc nofollow" target="_blank">语言模型</a>，它基于<strong class="lb iu"> Transformer架构</strong>并在40GBs的网络文本上训练。这是一个由多个解码器单元相互叠加而成的堆栈，支持一些先进的学习概念，如<a class="ae ky" href="https://medium.com/@mekarahul/what-are-self-attention-models-69fb59f6b5f8" rel="noopener">掩蔽自我注意</a>、多头、<a class="ae ky" rel="noopener" target="_blank" href="/residual-blocks-building-blocks-of-resnet-fd90ca15d6ec">剩余连接</a>、<a class="ae ky" href="https://mlexplained.com/2018/11/30/an-overview-of-normalization-methods-in-deep-learning/" rel="noopener ugc nofollow" target="_blank">层标准化</a>等。GPT-2语言模型试图优化的是在看到过去的单词后，本质上预测给定序列中的下一个单词。下图，即从<a class="ae ky" href="http://jalammar.github.io/illustrated-gpt2/" rel="noopener ugc nofollow" target="_blank">借用的图解GPT-2(可视化变压器语言模型)</a>直观地给出了一个清晰的画面</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/fd340b9b752209505fb7691a58fce24d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sHipi7G2jB6TUq92Z4kHVw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">GPT-2文本生成|修改后的图像来自<a class="ae ky" href="http://jalammar.github.io/illustrated-gpt2/" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="0b9d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，绿色文本充当前缀，一旦看到提示($)，模型就开始以自回归的方式一次生成一个单词，直到到达标记的末尾。智能手机上的自动完成功能，Gmail中的自动撰写功能，本质上是基于类似的概念。</p><p id="a2c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练如此庞大的模型将需要相当多的GPU日和大量数据。幸运的是，这个模型的版本是开源的，让我们不用从头开始训练这些模型。我们现在可以通过给它们适当大小的特定于领域的数据，在我们的任务中直接微调这些模型。现在让我们看看如何在实践中做到这一点。</p><p id="20e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nb">让我们继续前进，在我们的分类数据集上微调GPT-2模型，看看它能为给定类别生成真实的示例。</em></p><h1 id="4c21" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">资料组</h1><p id="592f" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">出于实验目的，我们将使用<a class="ae ky" href="https://github.com/prakhar21/TextAugmentation-GPT2/blob/master/data/SMSSpamCollection" rel="noopener ugc nofollow" target="_blank">垃圾邮件数据集</a>。。我们的数据集中有大约5500个样本。下图显示了相同的片段-</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/bfca1f4e3314d63b146f9646f10f9f19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I-spBQnzssD2ECl6WNyYxg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">垃圾邮件数据集数据片段|作者图片</p></figure><h1 id="0529" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">方法</h1><p id="d926" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">我们将使用GPT-2模型为<strong class="lb iu">学习关于类Ham和Spam的单词分布、语义、句法结构</strong>。一旦训练完成，我们将为这些类别中的每一个生成合成样本，您将看到这些样本足够好，可以添加回我们的数据集，用于训练具有更大数据的分类模型。</p><p id="2ce3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">预训练的GPT-2模型成为这项任务的合适候选，因为我们在数据集中对于每个垃圾邮件/垃圾邮件类别都只有很少的样本。理想情况下，我们想要一个已经知道很多关于自然语言的句法、语法、语义结构的模型。作为实施的一部分，我们将在我们的垃圾邮件/业余爱好者数据集上微调GPT2，并期望它学习这些电子邮件的单词用法和语言结构。</p><p id="b584" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们首先通过将标签和文本相互连接来创建输入样本，并将其传递给GPT-2模型用于<strong class="lb iu">学习单词-单词和标签-文本依赖关系</strong>。将标签添加到实际的示例中，将有助于我们稍后引导模型控制文本的生成，并使其特定于给定的标签/主题。</p><p id="ec17" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练GPT-2就像训练任何其他语言模型一样简单，我们一次传递一个单词，并在另一端预测下一个单词，然后将生成的单词循环回输入，依此类推，在每一步，我们都计算交叉熵损失。<em class="nb">您可以在这里</em>  <em class="nb">访问培训代码</em> <a class="ae ky" href="https://github.com/prakhar21/TextAugmentation-GPT2/blob/master/train.py" rel="noopener ugc nofollow" target="_blank"> <em class="nb">。</em></a></p><p id="a239" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦模型被训练和转储，我们就可以生成样本了。您可以在<a class="ae ky" href="https://github.com/prakhar21/TextAugmentation-GPT2/blob/master/generate.py" rel="noopener ugc nofollow" target="_blank"> finetune_gpt_generate找到使用训练好的模型生成样本的代码。</a>我们采用<strong class="lb iu"> Top-k，Top-p采样策略</strong>作为我们选择的<a class="ae ky" rel="noopener" target="_blank" href="/5-text-decoding-techniques-that-every-nlp-enthusiast-must-know-6908e72f8df9">文本解码技术</a>。</p><h1 id="6094" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">结果</h1><p id="37c5" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">以下是我们训练过的模型生成的一些新颖样本。正如所承诺的那样，我们可以看到生成的样本看起来非常真实，具有足够的可变性，应该有望帮助我们的模型提高精确度，并更好地进行概括。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/32dcf378428a88dafecf10878fa7ade7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*USLRo4SfeWpgxM6aP9NBng.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">生成的样本|作者图片</p></figure><p id="a9d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nb">所有代码和预训练的垃圾邮件/业余邮件生成器模型可在— </em>找到</p><div class="nk nl gp gr nm nn"><a href="https://github.com/prakhar21/TextAugmentation-GPT2" rel="noopener  ugc nofollow" target="_blank"><div class="no ab fo"><div class="np ab nq cl cj nr"><h2 class="bd iu gy z fp ns fr fs nt fu fw is bi translated">GitHub-prak har 21/text augmentation-gp T2:针对定制主题的微调预训练gp T2…</h2><div class="nu l"><h3 class="bd b gy z fp ns fr fs nt fu fw dk translated">针对特定主题文本生成的微调预训练GPT2。这种系统可用于文本扩充。git…</h3></div><div class="nv l"><p class="bd b dl z fp ns fr fs nt fu fw dk translated">github.com</p></div></div><div class="nw l"><div class="nx l ny nz oa nw ob ks nn"/></div></div></a></div><p id="7359" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">如果你愿意，你也可以在</strong><a class="ae ky" href="https://www.youtube.com/watch?v=9O9scQb4sNo&amp;list=PLsAqq9lZFOtUg63g_95OuV-R2GhV1UiIZ" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"/></a><strong class="lb iu">查看一些最近在NLP数据增强领域的研究。</strong></p><p id="9bf5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望你喜欢读这篇文章。如果你愿意支持我成为一名作家，可以考虑注册<a class="ae ky" href="https://prakhar-mishra.medium.com/membership" rel="noopener">成为一名媒体成员</a>。每月只需5美元，你就可以无限制地使用Medium。</p><h1 id="97d4" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">参考</h1><ol class=""><li id="dd98" class="oc od it lb b lc mw lf mx li oe lm of lq og lu oh oi oj ok bi translated">所使用的数据集是公开的，可以在<a class="ae ky" href="https://www.kaggle.com/venky73/spam-mails-dataset" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/venky73/spam-mails-dataset</a>访问</li></ol><p id="ad5a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢您的宝贵时间！</p></div></div>    
</body>
</html>