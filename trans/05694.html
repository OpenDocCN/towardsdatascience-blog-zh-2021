<html>
<head>
<title>Augment Your Small Dataset Using Transformers and Synonym Replacement for Sentiment Analysis (Part 1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用情感分析的转换器和同义词替换来扩充您的小型数据集(第1部分)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/augment-your-small-dataset-using-transformers-synonym-replacement-for-sentiment-analysis-part-1-87a838cd0baa?source=collection_archive---------11-----------------------#2021-05-21">https://towardsdatascience.com/augment-your-small-dataset-using-transformers-synonym-replacement-for-sentiment-analysis-part-1-87a838cd0baa?source=collection_archive---------11-----------------------#2021-05-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9b10" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何增加数据集的大小，以便稍后用于NLP分类任务。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/43d07b649a19c0354b25a744b655702c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*1Zsl-qPzgjgApNWByBjASA.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作家创造的形象。</p></figure><p id="17de" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在是2021年1月，一场风暴正在Reddit上酝酿。一群个人投资者，即sub Reddit '<a class="ae lr" href="https://en.wikipedia.org/wiki/R/wallstreetbets" rel="noopener ugc nofollow" target="_blank"><em class="ls">wall street bets</em></a>的成员，开始谈论一家公司，行业专业人士和机构投资者认为这家公司正在走下坡路。而且理由很充分——他们的业务正在被在线零售商侵蚀。</p><p id="d3ad" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">但这些“流氓”个人投资者并不认同业内专业人士的观点。许多人用有意义的分析支持的金融术语来表达他们的观点。散户投资者对机构投资者前所未有的异议，以及Reddit用户的集体交易行为，导致<strong class="kx ir"> <em class="ls"> GameStop </em> </strong>股票暴涨。短短几天内，股价从17.25美元飙升至500多美元，创造了历史上最大的“空头挤压”之一。</p><p id="5df5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为什么个人投资者战胜了大机构，为什么没有人预见到这一天的到来？</p><p id="df79" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因为没人在听。</p><h2 id="8a55" class="lt lu iq bd lv lw lx dn ly lz ma dp mb le mc md me li mf mg mh lm mi mj mk ml bi translated">输入情感分析:</h2><p id="18ae" class="pw-post-body-paragraph kv kw iq kx b ky mm jr la lb mn ju ld le mo lg lh li mp lk ll lm mq lo lp lq ij bi translated">情感分析是<em class="ls">自然语言处理</em>(<strong class="kx ir"><em class="ls"/></strong>)的一种形式。它识别和量化文本数据、情感状态以及其中的主题、人物和实体的主观信息。<strong class="kx ir">情感分析</strong>使用NLP方法和算法，这些方法和算法或者是基于规则的，或者是混合的，或者是依靠机器学习技术从<strong class="kx ir">数据集中学习数据。</strong>因此，it最具挑战性的一个方面是在大规模范围内寻找并标记有意义的数据。</p><p id="e5d8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">回到我们最初的案子。如果机构将人气作为分析指标，GameStop的反弹不会让任何人感到意外。但是，要实现这一目标，还需要克服一些挑战。</p><h2 id="fddc" class="lt lu iq bd lv lw lx dn ly lz ma dp mb le mc md me li mf mg mh lm mi mj mk ml bi translated">魔鬼就在数据中:</h2><p id="eb2b" class="pw-post-body-paragraph kv kw iq kx b ky mm jr la lb mn ju ld le mo lg lh li mp lk ll lm mq lo lp lq ij bi translated">首先，存在金融术语独特的问题。这个行业充满了术语，大多数情绪分析工具训练的数据不一定能够理解。然后是它的领域限制的本质和整体缺乏可用的结构化数据(分类)的问题。让我们现实一点，用IMDb电影评论或推特数据集的公共数据来训练NLP模型是不会成功的。综上所述，这些挑战导致大多数用于金融的现成NLP工具表现参差不齐。</p><p id="60a5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们需要的是来自声誉良好的金融领域来源的标记数据。尽管有大量的非结构化数据，但没有足够多的标记源来训练一个监督模型，以解决植根于金融情绪分析的内在挑战。</p><p id="7197" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">传统上，解决这一需求意味着手动通读成千上万的文档，对它们进行分类并相应地进行标记。这将是一个漫长而痛苦的过程，并不能保证很快给我们想要的结果。</p><p id="a9e7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">幸运的是<strong class="kx ir">变形金刚</strong>及其通过迁移学习执行<strong class="kx ir">数据扩充和分类的能力可以改变这一切。</strong></p><p id="45a0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这篇文章中，我们将基于我从网上收集和标记的金融文章的小样本建立一个大型数据集，我们随后将使用它来建立一个情感分析模型，该模型可以测量金融数据的情感。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/8f7757018a9fce372ef70ab03c73fe1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hZXQ-9Z4a9IBAszp"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae lr" href="https://unsplash.com/@nzrhan?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">罗文·辛普森</a>在<a class="ae lr" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h2 id="11c2" class="lt lu iq bd lv lw lx dn ly lz ma dp mb le mc md me li mf mg mh lm mi mj mk ml bi translated">什么是变形金刚和迁移学习？</h2><p id="65b6" class="pw-post-body-paragraph kv kw iq kx b ky mm jr la lb mn ju ld le mo lg lh li mp lk ll lm mq lo lp lq ij bi translated">简而言之<strong class="kx ir"> Transformers </strong>是预先训练好的机器学习模型，主要用于文本数据。谷歌在2017年的论文'<em class="ls">中首次介绍了注意力是你所需要的'</em>，它们已经取代了NLP任务中的其他神经网络框架。</p><p id="1523" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">变形金刚的性能远远超过其他型号，这正是我们手头任务所需要的。在这个练习中，我们将使用<strong class="kx ir"> HuggingFace </strong>(一个提供用于下载预训练模型的API的包)。</p><p id="aab9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">迁移学习</strong>是一种<em class="ls">监督学习，</em>将为特定任务训练的现有模型应用于一组不同的类别，这些类别中的数据要少得多。理论上，原始模型已经从原始数据中学到了足够的东西，通过重新训练它的一部分，它可以应用于我们的新任务。迁移学习是通过用与我们的任务相关的新数据训练所述模型的最终层来完成的，从而使模型适应新的领域。</p><h2 id="ad21" class="lt lu iq bd lv lw lx dn ly lz ma dp mb le mc md me li mf mg mh lm mi mj mk ml bi translated">挑战:</h2><p id="cac1" class="pw-post-body-paragraph kv kw iq kx b ky mm jr la lb mn ju ld le mo lg lh li mp lk ll lm mq lo lp lq ij bi translated">对金融新闻等领域进行情感分析面临多重挑战。在金融领域可能有负面含义的词在其他领域不一定被认为是不好的。</p><p id="48b0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">处理财务数据时需要注意的另一个因素是在一个句子中处理多个身份。例如，常见的句子有<em class="ls">‘在该机构的历史性裁决中，X机构对Y公司罚款数百万美元’。这使得任何对情绪进行分类的尝试都变得困难，因为这取决于我们关注的是哪个实体。</em></p><p id="bab8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这个问题有很多有趣的解决方法；然而，在这篇文章中，我已经精心挑选了数据来避免这个问题，所以我们不会涉及它。如果你想了解更多如何处理这个问题，可以在<a class="ae lr" href="https://www.acl-bg.org/proceedings/2017/RANLP%202017/pdf/RANLP094.pdf" rel="noopener ugc nofollow" target="_blank">这里</a>找到一个方法。</p><p id="737f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这个练习中，我使用了手动标记的352条财经新闻，我将它加载到一个熊猫数据框架中，删除了所有数字字符，单词被压缩(“s to is”)，每篇文章的情绪要么被标记为积极，要么被标记为消极。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/ba88ee9e00198a30283f8065874459d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*rvPSEma4nXeWjnzfzK4sjw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">数据视图</p></figure><h2 id="3ec2" class="lt lu iq bd lv lw lx dn ly lz ma dp mb le mc md me li mf mg mh lm mi mj mk ml bi translated">我们写点代码吧！</h2><p id="fe9d" class="pw-post-body-paragraph kv kw iq kx b ky mm jr la lb mn ju ld le mo lg lh li mp lk ll lm mq lo lp lq ij bi translated">数据扩充是一种创建新数据的方法，方法是对原始源进行多处小的更改，从而扩展可用于训练模型的数据。它广泛应用于图像分类任务，也已经成功地应用于文本分类任务。</p><p id="4dfd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在我们能够扩充数据之前，我们需要解决使用转换器的分类任务的一个主要瓶颈，它对可以训练的文本大小的限制，一些文章和金融文档可能有多页长，试图对它们训练模型可能会导致内存问题。为了解决这个问题，我们将使用名为T5的transformer模型来总结我们的文本数据。T5是在非监督和监督任务的多任务混合上预先训练的编码器-解码器模型，并且对于该模型，每个任务被转换成文本到文本格式。T5是开箱即用的总结任务的最佳表现者之一(无需对我们的数据进行训练),因此我们将在这里使用它。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="63f7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">函数'<strong class="kx ir">summary _ article</strong>'将我们的文章编码、汇总并解码回比源材料小得多的人类可读文本，超参数'<strong class="kx ir"> <em class="ls"> min_length </em> </strong>'和'<strong class="kx ir"> <em class="ls"> max_length </em> </strong>'将确保摘要的大小在标准阈值之间，同时保持其信息完整。一旦完成，输出将被传递到我们的增强管道的下一个步骤。</p><pre class="kg kh ki kj gt mv mw mx my aw mz bi"><span id="fe2c" class="lt lu iq mw b gy na nb l nc nd"><strong class="mw ir"><em class="ls">Before Summarization:</em></strong><br/><em class="ls">len(data_source['text'].max().lower().split()) = </em><strong class="mw ir"><em class="ls">520</em></strong><br/><strong class="mw ir"><em class="ls">After Summarization:</em></strong><br/><em class="ls">len(data_source['text'].max().lower().split()) = </em><strong class="mw ir"><em class="ls">60</em></strong></span></pre><p id="ae88" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">有多种方法来为NLP执行数据扩充。有些技术比其他技术更复杂，而且都有优缺点。NLP绝不是一门精确的科学，当涉及到扩充数据时，理解你的领域和任务是至关重要的。</p><h2 id="0cfb" class="lt lu iq bd lv lw lx dn ly lz ma dp mb le mc md me li mf mg mh lm mi mj mk ml bi translated"><strong class="ak">“简约是极致</strong>世故<strong class="ak">——老子:</strong></h2><p id="9f14" class="pw-post-body-paragraph kv kw iq kx b ky mm jr la lb mn ju ld le mo lg lh li mp lk ll lm mq lo lp lq ij bi translated">我发现关于数据增强的一篇论文很有趣，是的Jason Wei的<a class="ae lr" href="https://arxiv.org/abs/1901.11196" rel="noopener ugc nofollow" target="_blank"><em class="ls">【EDA:提高文本分类任务性能的简单数据增强技术】</em> </a>。</p><p id="51e0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这篇文章中，Jason和Kai探讨了<strong class="kx ir">同义词替换<em class="ls"> (SR) </em>、随机插入<em class="ls"> (RI) </em>、随机交换<em class="ls"> (RS) </em>和随机删除<em class="ls"> (RD) </em> </strong>如何成为轻量级且高效的数据扩充方式，何时以及如何实现，以及与其他方法相比，它们在NLP任务中的表现如何。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/f0f62ca0c3d6cd6daf277d81c20885dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*_RvBRw2hjIUnNMOTXOq5hg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来自Jason Wei论文的子集。</p></figure><h2 id="559c" class="lt lu iq bd lv lw lx dn ly lz ma dp mb le mc md me li mf mg mh lm mi mj mk ml bi translated">飞马座的飞行；</h2><p id="b539" class="pw-post-body-paragraph kv kw iq kx b ky mm jr la lb mn ju ld le mo lg lh li mp lk ll lm mq lo lp lq ij bi translated"><em class="ls">“用提取的间隙句进行抽象概括的预训练”</em>，又名<strong class="kx ir"> <em class="ls">飞马</em> </strong>。它的独特性在于它的“自我监督”，预训练目标架构。与其他通过提取句子的小部分来推断句子意思的模型不同，Pegasus完全“屏蔽”了句子，并试图通过阅读句子前后的文本来找到它。</p><p id="f909" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Pegasus确实擅长数据总结，但也非常擅长转述句子。该模型非常易于使用，不需要很多依赖关系，只需几行代码，我们就可以为训练准备好我们的扩充数据集。</p><h2 id="8091" class="lt lu iq bd lv lw lx dn ly lz ma dp mb le mc md me li mf mg mh lm mi mj mk ml bi translated"><strong class="ak">任务:</strong></h2><p id="1ee2" class="pw-post-body-paragraph kv kw iq kx b ky mm jr la lb mn ju ld le mo lg lh li mp lk ll lm mq lo lp lq ij bi translated">为了能够有效地利用我们的小数据集，我们将执行文本释义和同义词替换，以获得足够大和唯一的数据集来训练我们的情感分析模型。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="a0c8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">型号名称为'<strong class="kx ir"> pegasus_paraphrase </strong>'，更多信息请点击此处。首先，我们将获得我们的依赖项，并下载模型和‘tokenizer’。记号赋予器将句子分解成更小的块，这些块的大小和形状取决于任务和模型。</p><p id="8fbe" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们需要做的就是编写一个方法，使用我们的记号化器将我们的文本转换为一个序列，它将截断长句，填充小句，并返回一个张量结构，模型将使用它来生成释义的句子。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="21be" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">每个句子都将通过模型，超参数值是通过试错法选择的。这里看到的超参数是:</p><ul class=""><li id="270b" class="nf ng iq kx b ky kz lb lc le nh li ni lm nj lq nk nl nm nn bi translated"><strong class="kx ir"><em class="ls">【num _ beam】</em></strong>，模型将在一个序列中搜索最优后续词的次数</li><li id="ce1b" class="nf ng iq kx b ky no lb np le nq li nr lm ns lq nk nl nm nn bi translated"><strong class="kx ir"><em class="ls">‘num _ return _ sequences’</em></strong>模型将生成的句子数量(我们将超过50)</li><li id="b275" class="nf ng iq kx b ky no lb np le nq li nr lm ns lq nk nl nm nn bi translated"><strong class="kx ir"> <em class="ls">【温度】</em> </strong> <em class="ls"> </em>调节高概率词出现的机会，减少低概率词在世代中出现的机会。</li></ul><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="11cb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Pegasus将生成新的句子，这些句子将作为新的系列返回到我们的数据帧中，其中每一行都是生成的文本列表。然后，我们将通过使用Pandas的“explode”功能将列表中的每个元素变成一行来使它们变平。</p><p id="d566" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在模型遍历了每个句子之后，我们最终得到了一个比源材料大11倍的新数据集。然而，这个数据有一个问题。每一行中大约有一半(见下图)以非常相似的文本开始，忽略这一点可能会导致我们稍后付出代价，因为我们可能会过度拟合模型，破坏我们的努力。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/8363e993d5295a1d7ca03f7b35c809ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*taQwA83mijiKJOx_9BpIVw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">项目中间输出:大多数作品都以非常相似的措辞开始</p></figure><p id="9320" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，我们将执行数据扩充任务的第二部分。同义词替换。这个过程将扩展文本，并进一步推广它。</p><p id="ebe0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">首先，扩充的数据通过一个名为'<strong class="kx ir"> find_synonym </strong>的方法传递，在这个方法中，在前半部分文本中找到的任何停用词和数值都被过滤掉。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="ba29" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然后把剩下的单词一分为二地分组。每一个新句子将会把每一组中的两个单词替换成它们各自的同义词。为了找到同义词，我们将利用一个名为自然语言工具包的包，更好的说法是'<strong class="kx ir"><em class="ls">【nltk '，</em> </strong>一套用于符号和统计自然语言处理的库和程序。具体来说它的语料库阅读器'<strong class="kx ir"> <em class="ls"> wordnet </em> </strong>'包含了浩如烟海的词汇信息和同义词。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="3893" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后，在函数遍历了数据集的每一行之后，我们将确保没有任何重复或空值，新的语料库将被保存以备后用。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="7538" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在模型遍历了每个句子并且数据被正确格式化之后，我们最终得到了一个比源材料大50多倍的新数据集，它仍然保留了原始数据的含义、风格和上下文。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/f82efff39522bf6d67b9f7c79825d7a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*KRmawVX2dL0p91_g6C4ikg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">项目最终产出</p></figure><h2 id="e730" class="lt lu iq bd lv lw lx dn ly lz ma dp mb le mc md me li mf mg mh lm mi mj mk ml bi translated">结论:</h2><p id="434a" class="pw-post-body-paragraph kv kw iq kx b ky mm jr la lb mn ju ld le mo lg lh li mp lk ll lm mq lo lp lq ij bi translated">在本文中，我们介绍了如何使用转换器和同义词替换技术来执行数据扩充，使用少量数据带来的挑战以及如何克服这些挑战，以及如何创建一个由足够大的带标签的金融文章组成的数据集来执行有意义的分析。</p><p id="87a9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你好奇如何使用这些数据进行情感分析，点击<a class="ae lr" href="https://medium.com/p/70fbf70a6137/edit" rel="noopener">这里</a>跳转到本系列的第二部分:<a class="ae lr" href="https://medium.com/p/70fbf70a6137/edit" rel="noopener"> <em class="ls">如何使用变形金刚进行情感分析&amp;迁移学习</em> </a></p><h2 id="da3d" class="lt lu iq bd lv lw lx dn ly lz ma dp mb le mc md me li mf mg mh lm mi mj mk ml bi translated">来源:</h2><ul class=""><li id="97a7" class="nf ng iq kx b ky mm lb mn le nv li nw lm nx lq nk nl nm nn bi translated">本项目回购:<a class="ae lr" href="https://github.com/cmazzoni87/SentimentAnalysis" rel="noopener ugc nofollow" target="_blank">https://github.com/cmazzoni87/ComputerVisionRegression</a></li><li id="0747" class="nf ng iq kx b ky no lb np le nq li nr lm ns lq nk nl nm nn bi translated"><a class="ae lr" href="https://arxiv.org/abs/1901.11196" rel="noopener ugc nofollow" target="_blank"><em class="ls">【EDA:提升文本分类任务性能的简易数据增强技术】</em> </a>贾森·魏，。</li><li id="c85d" class="nf ng iq kx b ky no lb np le nq li nr lm ns lq nk nl nm nn bi translated">飞马变形金刚文档:<a class="ae lr" href="https://huggingface.co/transformers/model_doc/pegasus.html" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/transformers/model_doc/pegasus.html</a></li><li id="10df" class="nf ng iq kx b ky no lb np le nq li nr lm ns lq nk nl nm nn bi translated">T5变压器文档:<a class="ae lr" href="https://huggingface.co/transformers/model_doc/t5.html" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/transformers/model_doc/t5.html</a></li></ul></div></div>    
</body>
</html>