<html>
<head>
<title>How GANs Learn: A Simple Introduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">甘如何学习:一个简单的介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-gans-learn-a-simple-introduction-6d21081773bd?source=collection_archive---------35-----------------------#2021-11-01">https://towardsdatascience.com/how-gans-learn-a-simple-introduction-6d21081773bd?source=collection_archive---------35-----------------------#2021-11-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="0146" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">深度学习基础</h2><div class=""/><div class=""><h2 id="d8df" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">生成对抗网络的学习、理论和应用</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/82c44ce17009654901441fd973c06d27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*cSuv9U4ZRkS5NipGZ1GNZg.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</p></figure><h1 id="25f2" class="la lb iq bd lc ld le lf lg lh li lj lk kf ll kg lm ki ln kj lo kl lp km lq lr bi translated"><strong class="ak">简介</strong></h1><p id="a750" class="pw-post-body-paragraph ls lt iq lu b lv lw ka lx ly lz kd ma mb mc md me mf mg mh mi mj mk ml mm mn ij bi translated">在我以前的文章中，我谈到了一般的生成方法，以及是什么让生成器网络如此强大。在这篇文章中，我想更深入地研究生成性对抗网络(GANs)的内部运作。为了确保您完全理解它们，我将浏览GANs论文的原始伪代码并解释GANs的损失函数，然后我将向您展示我自己实现的结果。最后，我将解释如何通过DCGANs论文的建议来改进GANs，这是该研究领域的一篇关键论文。</p><h1 id="a398" class="la lb iq bd lc ld le lf lg lh li lj lk kf ll kg lm ki ln kj lo kl lp km lq lr bi translated"><strong class="ak">甘斯人如何学习</strong></h1><p id="d078" class="pw-post-body-paragraph ls lt iq lu b lv lw ka lx ly lz kd ma mb mc md me mf mg mh mi mj mk ml mm mn ij bi translated">生成方法是一个非常强大的工具，可以用来解决许多问题。他们的目标是生成可能属于训练数据集的新数据样本。生成方法可以通过两种方式做到这一点，通过学习数据空间的近似分布，然后从中采样，或者通过学习生成可能属于该数据空间的样本(避免近似数据分布的步骤)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mo"><img src="../Images/3fcba68778ee32056fda54da1d9fd01d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DpujPSB4s7VeR7NdaSlMOA.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</p></figure><p id="f6a0" class="pw-post-body-paragraph ls lt iq lu b lv mt ka lx ly mu kd ma mb mv md me mf mw mh mi mj mx ml mm mn ij bi translated">上面你可以看到GANs的架构图。GANs由两个网络(生成器和鉴别器)组成，这两个网络本质上是相互竞争的；这两个网络有对立的目标。</p><p id="f558" class="pw-post-body-paragraph ls lt iq lu b lv mt ka lx ly mu kd ma mb mv md me mf mw mh mi mj mx ml mm mn ij bi translated">生成器试图最大化欺骗鉴别器的概率，使其认为生成的图像是真实的。鉴别器的目标是正确地将真实数据分类为真实的，将生成的数据分类为假的。这些目标用网络的损失函数来表示，这些损失函数将在训练期间被优化。</p><p id="31dd" class="pw-post-body-paragraph ls lt iq lu b lv mt ka lx ly mu kd ma mb mv md me mf mw mh mi mj mx ml mm mn ij bi translated">在GANs中，发电机的损失函数最小化，而鉴别器的损失函数最大化。生成器试图最大化鉴别器的误报样本数，鉴别器试图最大化其分类精度。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi my"><img src="../Images/082ff69aaa8d3299d8a50db2f42d061c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*irndtX3ZVpTX5Mq2zMQKQQ.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">伪代码由<strong class="bd mz">伊恩·j·古德菲勒等人编写，</strong> <a class="ae na" href="https://arxiv.org/abs/1406.2661" rel="noopener ugc nofollow" target="_blank"> <strong class="bd mz">生成对抗网络</strong></a><strong class="bd mz">【1】</strong></p></figure><p id="0bda" class="pw-post-body-paragraph ls lt iq lu b lv mt ka lx ly mu kd ma mb mv md me mf mw mh mi mj mx ml mm mn ij bi translated">在上面的伪代码中，对于每个时期，对于每个批次，计算鉴别器和发生器的梯度。鉴别器的损失由真实数据集的正确分类样本数的对数和虚假数据集的正确分类样本数组成。我们希望最大化这一点。生成器的损失函数由鉴别器正确分类假图像的次数组成，我们希望最小化这一点。</p><p id="3e53" class="pw-post-body-paragraph ls lt iq lu b lv mt ka lx ly mu kd ma mb mv md me mf mw mh mi mj mx ml mm mn ij bi translated">目标自然是相反的，因此用于训练网络的梯度也是如此。这可能会成为一个问题，我将在后面讨论这个问题。</p><p id="ae29" class="pw-post-body-paragraph ls lt iq lu b lv mt ka lx ly mu kd ma mb mv md me mf mw mh mi mj mx ml mm mn ij bi translated">一旦训练结束，发电机就是我们唯一关心的了。生成器能够接收随机噪声向量，然后它将输出最可能属于训练数据空间的图像。请记住，即使这有效地学习了随机变量(z)和图像数据空间之间的映射，也不能保证两个空间之间的映射是平滑的。gan不学习数据的分布，他们学习如何生成类似于属于训练数据的样本。</p><h1 id="5600" class="la lb iq bd lc ld le lf lg lh li lj lk kf ll kg lm ki ln kj lo kl lp km lq lr bi translated"><strong class="ak">应用</strong></h1><p id="34b4" class="pw-post-body-paragraph ls lt iq lu b lv lw ka lx ly lz kd ma mb mc md me mf mg mh mi mj mk ml mm mn ij bi translated">让我们来看一个简单GAN的应用。训练数据由来自MNIST数据集的手写数字组成。假设我需要更多的手写数字来训练其他机器学习/统计模型，那么可以使用GANs来尝试生成更多的数据。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/b08706e118fef08366141239bd452a8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*pdkhJ1niT872SsdKE1wcxA.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</p></figure><p id="6db4" class="pw-post-body-paragraph ls lt iq lu b lv mt ka lx ly mu kd ma mb mv md me mf mw mh mi mj mx ml mm mn ij bi translated">看看这个训练数据。正如你所看到的，有些数字对人类来说甚至很难读懂。机器真的很难处理非结构化数据(典型的例子:图像和文本)。计算机只看到像素值，很难教会它一个手写数字是什么排列顺序组成的。</p><h2 id="ee53" class="nc lb iq bd lc nd ne dn lg nf ng dp lk mb nh ni lm mf nj nk lo mj nl nm lq iw bi translated">生成随机样本</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/5f55a9afffbd43140baac541a16922b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:984/format:webp/1*yEDexv0TADoc4VAJmuh46A.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</p></figure><p id="fbe8" class="pw-post-body-paragraph ls lt iq lu b lv mt ka lx ly mu kd ma mb mv md me mf mw mh mi mj mx ml mm mn ij bi translated">如您所见，生成的数据看起来像手写的数字。该模型已经学习了手写数字图像中的一些模式，并由此能够生成新的数据。</p><p id="4840" class="pw-post-body-paragraph ls lt iq lu b lv mt ka lx ly mu kd ma mb mv md me mf mw mh mi mj mx ml mm mn ij bi translated">我们可以在模型学习时查看生成的样本的GIF(GIF可能不会在medium应用程序上显示，所以我建议使用浏览器来查看)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi no"><img src="../Images/abf7581b1e684cdc2805be99c8add790.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/1*lUn19qcnTqXeWGc3lAXvtQ.gif"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">作者GIF</p></figure><p id="0fea" class="pw-post-body-paragraph ls lt iq lu b lv mt ka lx ly mu kd ma mb mv md me mf mw mh mi mj mx ml mm mn ij bi translated">生成器模型从随机权重开始，生成的图像看起来像随机噪声。随着损失函数的优化，生成器在欺骗鉴别器方面变得越来越好，最终产生的图像看起来更像手写数字，而不是随机噪声。</p><h2 id="ec48" class="nc lb iq bd lc nd ne dn lg nf ng dp lk mb nh ni lm mf nj nk lo mj nl nm lq iw bi translated">图像间采样</h2><p id="e757" class="pw-post-body-paragraph ls lt iq lu b lv lw ka lx ly lz kd ma mb mc md me mf mg mh mi mj mk ml mm mn ij bi translated">可以做的一件有趣的事情是拍摄两张生成的图像，并对它们之间的空间进行采样。这两个生成的图像具有两个相应的随机向量，这两个随机向量已经被馈送到生成器。我可以将这两个向量之间的空间离散化，本质上通过这两个向量之间的分布画一条线。然后我可以对这条线进行采样，并将这些点输入到我的生成器中。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/8aa67245282fde114cffe9dae6fd702e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*PgP1SKPrMlPDF5aGi7sC8A.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</p></figure><p id="3bf4" class="pw-post-body-paragraph ls lt iq lu b lv mt ka lx ly mu kd ma mb mv md me mf mw mh mi mj mx ml mm mn ij bi translated">在这里，我看到了一个生成的“4”和一个生成的“8”之间的空间。开始的数字是左上角的“4 ”,这个数字实质上会转化为右下角的“8”。</p><p id="715f" class="pw-post-body-paragraph ls lt iq lu b lv mt ka lx ly mu kd ma mb mv md me mf mw mh mi mj mx ml mm mn ij bi translated">尽管生成器不能保证在输入空间和数据空间之间产生平滑的映射(我们生成的样本不逼近密度函数)，但您可以看到两幅图像之间的过渡仍然非常平滑。有趣的是，在“4”变成“8”之前，它首先转变为“9”，这意味着在数据空间中，数字“9”位于“4”和“8”之间。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/4f7e85d935314f4bbf4b974dfdf01e52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*KIQOcVVT_NNe256HRM7VTA.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</p></figure><p id="d98a" class="pw-post-body-paragraph ls lt iq lu b lv mt ka lx ly mu kd ma mb mv md me mf mw mh mi mj mx ml mm mn ij bi translated">这是另一个例子，这次数字“6”变成了“3”。您可以简单地看到生成的样本看起来像一个“5”。</p><h1 id="a37a" class="la lb iq bd lc ld le lf lg lh li lj lk kf ll kg lm ki ln kj lo kl lp km lq lr bi translated"><strong class="ak">甘斯的弊端</strong></h1><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi np"><img src="../Images/1d62066a49a8890d118c0c45c724e664.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*DaEGm_pg7mmZfqcQlBELhg.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</p></figure><p id="8ae4" class="pw-post-body-paragraph ls lt iq lu b lv mt ka lx ly mu kd ma mb mv md me mf mw mh mi mj mx ml mm mn ij bi translated">GANs的一个主要缺点是，如前所述，鉴别器和发生器具有相反的目标，因此具有相反的符号梯度。可以看出，当优化GAN时，将不会达到最小值。相反，优化算法将在鞍点结束。</p><p id="4457" class="pw-post-body-paragraph ls lt iq lu b lv mt ka lx ly mu kd ma mb mv md me mf mw mh mi mj mx ml mm mn ij bi translated">GANs的另一个常见问题是，在训练这些模型时，鉴别器很容易压倒生成器。鉴别器只是变得太好太快，生成器无法学习如何生成欺骗鉴别器的图像。直觉上这是有意义的，分类任务总是比生成器学习如何生成新样本的任务更容易。</p><h1 id="9613" class="la lb iq bd lc ld le lf lg lh li lj lk kf ll kg lm ki ln kj lo kl lp km lq lr bi translated"><strong class="ak"> DCGANs和消失渐变</strong></h1><p id="3390" class="pw-post-body-paragraph ls lt iq lu b lv lw ka lx ly lz kd ma mb mc md me mf mg mh mi mj mk ml mm mn ij bi translated">深度卷积gan是解决这一问题的一种方法。第一个主要建议是使用LeakyReLU作为鉴别器的激活函数。这有助于解决渐变消失的问题。当训练任何种类的神经网络时，都会出现消失梯度问题，如果梯度太小，它们可能会“陷入”这种消失状态，并且由于它们接近于零，因此很难在训练中使用它们。LeakyReLU通过始终更新模型的权重来减少这种情况的发生，即使激活很小。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi nq"><img src="../Images/e17f4da47f4733f6f76dcde41e5aa93d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0CA4o-qbLHiJ5TbAfVM5nA.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</p></figure><p id="babb" class="pw-post-body-paragraph ls lt iq lu b lv mt ka lx ly mu kd ma mb mv md me mf mw mh mi mj mx ml mm mn ij bi translated">DCGANs论文中的其他技巧已经成为训练神经网络时的常见做法，例如在卷积层之后使用批量归一化层，以及避免使用过多的密集层，而使用卷积层。</p><h1 id="0f31" class="la lb iq bd lc ld le lf lg lh li lj lk kf ll kg lm ki ln kj lo kl lp km lq lr bi translated">结论</h1><p id="d89a" class="pw-post-body-paragraph ls lt iq lu b lv lw ka lx ly lz kd ma mb mc md me mf mg mh mi mj mk ml mm mn ij bi translated">在这篇文章中，我介绍了GANs如何工作以及他们如何学习的理论。然后，我展示了用python实现GANs的简单结果。最后，我强调了GANs的一些缺点，以及如何解决这些缺点。</p><h2 id="99eb" class="nc lb iq bd lc nd ne dn lg nf ng dp lk mb nh ni lm mf nj nk lo mj nl nm lq iw bi translated">支持我</h2><p id="c5c8" class="pw-post-body-paragraph ls lt iq lu b lv lw ka lx ly lz kd ma mb mc md me mf mg mh mi mj mk ml mm mn ij bi translated">希望这对你有所帮助，如果你喜欢它，你可以<a class="ae na" href="https://medium.com/@diegounzuetaruedas" rel="noopener">跟随我！ </a></p><p id="219a" class="pw-post-body-paragraph ls lt iq lu b lv mt ka lx ly mu kd ma mb mv md me mf mw mh mi mj mx ml mm mn ij bi translated">您也可以成为<a class="ae na" href="https://diegounzuetaruedas.medium.com/membership" rel="noopener"> <strong class="lu ja">中级会员</strong> </a> <strong class="lu ja"> </strong>使用我的推荐链接，访问我的所有文章以及更多:<a class="ae na" href="https://diegounzuetaruedas.medium.com/membership" rel="noopener">https://diegounzuetaruedas.medium.com/membership</a></p><h1 id="cdc8" class="la lb iq bd lc ld le lf lg lh li lj lk kf ll kg lm ki ln kj lo kl lp km lq lr bi translated">你可能喜欢的其他文章</h1><p id="8951" class="pw-post-body-paragraph ls lt iq lu b lv lw ka lx ly lz kd ma mb mc md me mf mg mh mi mj mk ml mm mn ij bi translated"><a class="ae na" rel="noopener" target="_blank" href="/differentiable-generator-networks-an-introduction-5a9650a24823">可微发电机网络:简介</a></p><p id="2b7a" class="pw-post-body-paragraph ls lt iq lu b lv mt ka lx ly mu kd ma mb mv md me mf mw mh mi mj mx ml mm mn ij bi translated"><a class="ae na" rel="noopener" target="_blank" href="/fourier-transforms-an-intuitive-visualisation-ba186c7380ee">傅立叶变换:直观的可视化</a></p><h1 id="d872" class="la lb iq bd lc ld le lf lg lh li lj lk kf ll kg lm ki ln kj lo kl lp km lq lr bi translated">参考</h1><p id="00c0" class="pw-post-body-paragraph ls lt iq lu b lv lw ka lx ly lz kd ma mb mc md me mf mg mh mi mj mk ml mm mn ij bi translated">[1] Goodfellow，I .、Pouget-Abadie，j .、Mirza，m .、Xu，b .、Warde-Farley，d .、Ozair，s .、a .和Bengio，y .，2014年。生成性对抗网络。<em class="nr">康乃尔大学</em>https://arxiv.org/abs/1406.2661 T2。</p></div></div>    
</body>
</html>