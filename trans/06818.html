<html>
<head>
<title>Intro to DeepMind’s Reinforcement Learning Framework “Acme”</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DeepMind的强化学习框架“Acme”简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deepminds-reinforcement-learning-framework-acme-87934fa223bf?source=collection_archive---------9-----------------------#2021-06-20">https://towardsdatascience.com/deepminds-reinforcement-learning-framework-acme-87934fa223bf?source=collection_archive---------9-----------------------#2021-06-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/f8d05205659eccfe2539a3a788b3d9da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MxAaEXIQ1cgqBMoD"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">paweczerwi ski在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class=""/><div class=""><h2 id="d0ed" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">如何用Acme实现RL代理</h2></div><p id="3938" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi lr translated"><span class="l ls lt lu bm lv lw lx ly lz di"> A </span> cme是基于Python的强化学习研究框架，由谷歌的DeepMind于2020年开源。它旨在简化新型RL试剂的开发并加速RL研究。根据他们自己的声明，Acme在DeepMind每天都被使用，deep mind是强化学习和人工智能研究的先锋。</p><p id="a25b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了最近的一个大学项目，我决定学习Acme并使用它来实现不同的RL算法。我发现它很棒，我真的很喜欢和它一起工作。</p><p id="d7c1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">开始使用Acme也相对容易。这是因为有几个不同复杂程度的入口点。换句话说，该框架不仅适用于高级研究人员，也允许初学者实现甚至简单的算法——有点类似于初学者和专家使用TensorFlow和PyTorch的方式。</p><p id="53a6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，不利的一面是，由于该框架仍然很新，所以没有真正全面的文档可用，也没有任何优秀的教程。</p><p id="265c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这篇博文有望成为朝着正确方向迈出的一步。它并不意味着是或取代一个完整的文档，而是一个简明、实用的Acme介绍。最重要的是，它应该给你一个框架下的设计选择的概念，以及这对RL算法的实现意味着什么。</p><p id="85b0" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">特别是，我将讨论我的两个简单算法的实现，<strong class="kx jh"> SARSA </strong>和<strong class="kx jh"> Q learning </strong>，目标是玩21点游戏。一旦你理解了什么是<em class="ma"> actors </em>和<em class="ma"> agents </em>以及它们在Acme中是如何设计的，我毫不怀疑你会很快明白如何实现(可能)任何你能想到的强化学习算法。</p><h1 id="40a6" class="mb mc jg bd md me mf mg mh mi mj mk ml km mm kn mn kp mo kq mp ks mq kt mr ms bi translated">Acme的基本构建模块</h1><p id="7981" class="pw-post-body-paragraph kv kw jg kx b ky mt kh la lb mu kk ld le mv lg lh li mw lk ll lm mx lo lp lq ij bi translated">让我们深入一个实际的例子。正如已经提到的，我们希望我们的代理人玩21点。</p><h2 id="8588" class="my mc jg bd md mz na dn mh nb nc dp ml le nd ne mn li nf ng mp lm nh ni mr nj bi translated">环境</h2><p id="902f" class="pw-post-body-paragraph kv kw jg kx b ky mt kh la lb mu kk ld le mv lg lh li mw lk ll lm mx lo lp lq ij bi translated">Acme代理不是为与健身房环境交互而设计的。相反，DeepMind有自己的<a class="ae jd" href="https://github.com/deepmind/dm_env" rel="noopener ugc nofollow" target="_blank"> RL环境API </a>。区别主要在于如何表示时间步长。</p><p id="3b97" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，幸运的是，您仍然可以利用体育馆环境，因为Acme的开发人员已经为此提供了包装器函数。</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="5226" class="my mc jg np b gy nt nu l nv nw">env = acme.wrappers.GymWrapper(gym.make('Blackjack-v0'))</span></pre><p id="caa2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">二十一点有32 x 11 x 2种状态，尽管并非所有这些状态都会在游戏中出现，并且有两个动作，“打”和“棒”。关于这些维度的描述以及为什么不是所有的状态都会发生，你可以在GitHub <a class="ae jd" href="https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py" rel="noopener ugc nofollow" target="_blank">上查看环境。我知道这一开始并不明显。</a></p><h2 id="4d06" class="my mc jg bd md mz na dn mh nb nc dp ml le nd ne mn li nf ng mp lm nh ni mr nj bi translated">演员、学习者和代理</h2><p id="796b" class="pw-post-body-paragraph kv kw jg kx b ky mt kh la lb mu kk ld le mv lg lh li mw lk ll lm mx lo lp lq ij bi translated">理解<em class="ma">行动者、学习者</em>和<em class="ma">代理</em>之间的区别至关重要。演员与环境互动。也就是说，它们观察状态并根据一些动作选择策略采取动作。下图说明了这一点。</p><figure class="nk nl nm nn gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nx"><img src="../Images/c74ea09b860e075298272252487928a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IPjN2jm3qIayiEN8DXcmvw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated"><strong class="bd ny">图1 </strong>:只有演员的简单环境循环。霍夫曼等人(2020年)</p></figure><p id="3cc2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">学习者使用参与者收集的数据来学习或改进策略，通常以迭代的在线方式进行。例如，学习可能包括对神经网络参数的更新。新的参数被传递给执行元，然后执行元根据更新后的策略进行操作。</p><p id="e260" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">代理简单地结合了表演和学习组件，但通常不需要实现额外的强化学习逻辑。下图包括所有三个组件。</p><figure class="nk nl nm nn gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nx"><img src="../Images/6b2483e925efcbbf61f000dd969590a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*du2UwbLKiwc2vTovrRcPxQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated"><strong class="bd ny">图2 </strong>:包括演员和学习者的环境循环。霍夫曼等人(2020年)</p></figure><p id="e2fe" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这种分解成行动者、学习者和代理的主要原因之一是为了促进分布式强化学习。但是，如果我们不关心这个问题，并且算法足够简单，那么只实现actor并简单地将学习步骤集成到actor的<code class="fe nz oa ob np b">update</code>方法中就足够了。为了简单起见，这也是我在这里采用的方法。</p><p id="8d2c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">例如，下面的随机代理继承自<code class="fe nz oa ob np b">acme.Actor</code>类。开发者(你)必须实现的方法有<code class="fe nz oa ob np b">select_action</code>、<code class="fe nz oa ob np b">observe_first</code>、<code class="fe nz oa ob np b">observe</code>、<code class="fe nz oa ob np b">update</code>。正如刚才提到的，后者是在没有额外的学习者组件的情况下进行学习的地方。注意，这个代理在没有子类化<code class="fe nz oa ob np b">acme.Actor</code>的情况下也会以同样的方式工作。基类只是决定了必须重写的方法。这也确保了代理按照预期与其他Acme组件集成，比如我将在下面介绍的环境循环。</p><figure class="nk nl nm nn gt is"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="e9f9" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这个代理使用一个简单地随机选择点击或粘贴的策略，但是一般来说，这个框架允许你在如何实现策略方面有很大的灵活性。稍后，您将看到一个ε-贪婪策略。在其他情况下，该策略可能包括一个可以用TensorFlow、PyTorch或JAX实现的神经网络。从这个意义上说，Acme是框架不可知的，因此您可以将它与您喜欢的任何机器学习库相结合。</p><p id="2e72" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在<code class="fe nz oa ob np b">update</code>方法中，参与者通常只从学习者那里获取最近的参数。然而，如果你不使用单独的学习器，RL逻辑会进入到<code class="fe nz oa ob np b">update</code>方法中(稍后你会看到)。</p><h2 id="7c0b" class="my mc jg bd md mz na dn mh nb nc dp ml le nd ne mn li nf ng mp lm nh ni mr nj bi translated">环境环路</h2><p id="b693" class="pw-post-body-paragraph kv kw jg kx b ky mt kh la lb mu kk ld le mv lg lh li mw lk ll lm mx lo lp lq ij bi translated">如果你已经知道一些关于强化学习的知识，并且已经实现了一个RL算法，那么下面的循环对你来说将是非常熟悉的。每一集由四个步骤组成，这些步骤一直重复，直到达到终点状态。</p><ol class=""><li id="5964" class="oe of jg kx b ky kz lb lc le og li oh lm oi lq oj ok ol om bi translated">观察一个国家</li><li id="fc19" class="oe of jg kx b ky on lb oo le op li oq lm or lq oj ok ol om bi translated">根据行为策略采取行动</li><li id="9805" class="oe of jg kx b ky on lb oo le op li oq lm or lq oj ok ol om bi translated">遵守奖励</li><li id="76e2" class="oe of jg kx b ky on lb oo le op li oq lm or lq oj ok ol om bi translated">更新策略</li></ol><p id="f5cb" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在代码中…</p><figure class="nk nl nm nn gt is"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="158f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">有时候你可能需要实现一个这样的循环，特别是当你想要定制它的时候。但大多数情况下，这个循环总是完全一样的。</p><p id="d8f3" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">方便的是，Acme中有一个快捷方式:<code class="fe nz oa ob np b">EnvironmentLoop</code>，它执行的步骤与上面看到的几乎完全一样。您只需传递您的环境和代理实例，然后您就可以用一行代码运行单个或多个剧集。也有一些记录器可以跟踪重要的指标，如每集的步数和收集的奖励。</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="d720" class="my mc jg np b gy nt nu l nv nw"># init Acme's environment loop<br/>loop = EnvironmentLoop(env, agent, logger=InMemoryLogger())</span><span id="ad50" class="my mc jg np b gy os nu l nv nw"># run a single episode<br/>loop.run_episode()</span><span id="ab05" class="my mc jg np b gy os nu l nv nw"># or run multiple episodes<br/>loop.run(10)</span></pre><p id="c8ce" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">你也可以查看第三方日志程序，比如这个<a class="ae jd" href="https://neptune.ai/blog/logging-in-reinforcement-learning-frameworks" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="883a" class="mb mc jg bd md me mf mg mh mi mj mk ml km mm kn mn kp mo kq mp ks mq kt mr ms bi translated">实现SARSA和Q学习代理</h1><p id="e61d" class="pw-post-body-paragraph kv kw jg kx b ky mt kh la lb mu kk ld le mv lg lh li mw lk ll lm mx lo lp lq ij bi translated">当然，一个随机的代理不是很有用。我的承诺是展示如何实现一些实际的强化学习算法。所以我们开始吧。</p><p id="6334" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">顺便说一下，如果你对RL一点都不熟悉，可以看看萨顿和巴尔托(2018)的书<em class="ma">强化学习:简介</em>。你通常首先学习的两个算法是SARSA和Q学习，无论是在书中还是在大学里学习RL课程。</p><h2 id="50f4" class="my mc jg bd md mz na dn mh nb nc dp ml le nd ne mn li nf ng mp lm nh ni mr nj bi translated">萨尔萨特工</h2><p id="f74d" class="pw-post-body-paragraph kv kw jg kx b ky mt kh la lb mu kk ld le mv lg lh li mw lk ll lm mx lo lp lq ij bi translated">到目前为止，您已经知道Acme代理(或参与者)是如何设计的。让我们看看如何在Acme中实现SARSA算法。</p><p id="59f2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">SARSA是一种基于策略的算法，其更新取决于状态、动作、奖励、下一个状态和下一个动作(因此得名)。因为这不是一个理论上的RL教程，所以我在这里不深入算法本身的细节。</p><p id="8592" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">首先，在代理的<code class="fe nz oa ob np b">__init__</code>方法中，我们初始化Q、状态-动作值矩阵和行为策略，这里是一个ε贪婪策略。还要注意，这个代理必须总是存储它的上一个时间步长、动作和下一个时间步长，因为在<code class="fe nz oa ob np b">update</code>步骤中需要它们。所以我们也初始化它们。</p><figure class="nk nl nm nn gt is"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="3beb" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在<code class="fe nz oa ob np b">observe</code>，你通常不用做太多事情。在这种情况下，我们只需存储观察到的时间步长和采取的操作。然而，这并不总是必要的。例如，有时您可能希望将时间步长(和整个轨迹)存储在数据集或重放缓冲区中。为此，Acme还提供了数据集和加法器组件。事实上，还有一个由DeepMind开发的库。它叫做<strong class="kx jh">混响</strong>(见GitHub <a class="ae jd" href="https://github.com/deepmind/reverb" rel="noopener ugc nofollow" target="_blank">此处</a>)。</p><p id="d696" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">上面的<code class="fe nz oa ob np b">transform_state</code>方法只是一个帮助函数，用于将状态转换成正确的格式，以便正确地索引Q矩阵。</p><p id="7d50" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后，要在500，000集的环境中训练SARSA，只需运行</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="c99a" class="my mc jg np b gy nt nu l nv nw">agent = SarsaAgent()</span><span id="a527" class="my mc jg np b gy os nu l nv nw">loop = EnvironmentLoop(env, agent, logger=InMemoryLogger())<br/>loop.run(500000)</span></pre><h2 id="5820" class="my mc jg bd md mz na dn mh nb nc dp ml le nd ne mn li nf ng mp lm nh ni mr nj bi translated">q学习代理</h2><p id="e33d" class="pw-post-body-paragraph kv kw jg kx b ky mt kh la lb mu kk ld le mv lg lh li mw lk ll lm mx lo lp lq ij bi translated">下面的Q学习代理与SARSA代理非常相似。它们的区别仅在于如何更新Q矩阵。这是因为Q学习是一种偏离策略的算法。</p><figure class="nk nl nm nn gt is"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="4f17" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">要在500，000集的环境中训练Q learning agent，请运行</p><pre class="nk nl nm nn gt no np nq nr aw ns bi"><span id="fdf9" class="my mc jg np b gy nt nu l nv nw">agent = QLearningAgent()</span><span id="d551" class="my mc jg np b gy os nu l nv nw">loop = EnvironmentLoop(env, agent, logger=InMemoryLogger())<br/>loop.run(500000)</span></pre><h1 id="1846" class="mb mc jg bd md me mf mg mh mi mj mk ml km mm kn mn kp mo kq mp ks mq kt mr ms bi translated">结论</h1><p id="8727" class="pw-post-body-paragraph kv kw jg kx b ky mt kh la lb mu kk ld le mv lg lh li mw lk ll lm mx lo lp lq ij bi translated">我认为Acme是一个非常棒的强化学习框架，因为你不必从头开始开发你的算法。因此，你可以依靠DeepMind中无疑非常聪明的研究人员和开发人员，他们已经为你完成了这些工作，而不是自己去弄清楚如何编写可读和可复制的RL代码。</p><p id="8896" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Acme允许你实现任何强化学习算法，你可以将它与任何其他机器学习框架结合，包括TensorFlow、PyTorch和JAX。</p><p id="aa8a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你想了解更多关于Acme的知识，你可以阅读DeepMind的<a class="ae jd" href="https://arxiv.org/abs/2006.00979" rel="noopener ugc nofollow" target="_blank">研究论文</a>，查看他们的<a class="ae jd" href="https://github.com/deepmind/acme" rel="noopener ugc nofollow" target="_blank"> GitHub资源库</a>。</p><p id="ce0d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这里，您还可以找到一些常见算法的实现，如深度Q网络(DQN)、深度确定性策略梯度(DDPG)、蒙特卡罗树搜索(MCTS)、行为克隆(BC)、IMPALA等等。</p><p id="ffea" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">不管你是高级研究员还是对强化学习感兴趣的初学者，我都鼓励你尝试一下。</p><p id="7e03" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">感谢您的阅读。如果你有任何问题，让我知道。</p></div><div class="ab cl ot ou hu ov" role="separator"><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy"/></div><div class="ij ik il im in"><h2 id="55e5" class="my mc jg bd md mz na dn mh nb nc dp ml le nd ne mn li nf ng mp lm nh ni mr nj bi translated">链接</h2><p id="ba16" class="pw-post-body-paragraph kv kw jg kx b ky mt kh la lb mu kk ld le mv lg lh li mw lk ll lm mx lo lp lq ij bi translated">我的Jupyter笔记本包含的代码可以在这里找到<a class="ae jd" href="https://github.com/astoeffelbauer/blogging/blob/master/Acme%20RL%20Medium%20June%202021.ipynb" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="b716" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你有兴趣，也可以看看我的<a class="ae jd" href="https://github.com/astoeffelbauer/MSc-Data-Science/blob/main/ST449%20Deep%20Learning%20and%20Artificial%20Intelligence/Course%20Project/ST449%20Project%20Notebook.ipynb" rel="noopener ugc nofollow" target="_blank">强化学习课程项目</a>。除了SARSA和Q-learning，我还实现了dyna-Q、优先扫描和Mote Carlo树搜索代理。</p><h2 id="efba" class="my mc jg bd md mz na dn mh nb nc dp ml le nd ne mn li nf ng mp lm nh ni mr nj bi translated"><strong class="ak">参考文献</strong></h2><p id="69a4" class="pw-post-body-paragraph kv kw jg kx b ky mt kh la lb mu kk ld le mv lg lh li mw lk ll lm mx lo lp lq ij bi translated">霍夫曼等人(2020): <em class="ma"> Acme:分布式强化学习的研究框架</em>。<a class="ae jd" href="https://arxiv.org/abs/2006.00979" rel="noopener ugc nofollow" target="_blank"> ArXiv </a>。</p><p id="8d56" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">萨顿和巴尔托(2018): <em class="ma">强化学习:导论。</em></p></div><div class="ab cl ot ou hu ov" role="separator"><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy"/></div><div class="ij ik il im in"><div class="nk nl nm nn gt pa"><a href="https://github.com/astoeffelbauer" rel="noopener  ugc nofollow" target="_blank"><div class="pb ab fo"><div class="pc ab pd cl cj pe"><h2 class="bd jh gy z fp pf fr fs pg fu fw jf bi translated">astoeffelbauer -概述</h2><div class="ph l"><h3 class="bd b gy z fp pf fr fs pg fu fw dk translated">数据科学学生@伦敦政治经济学院Block或Report六月七月八月九月十月十一月十二月一月…</h3></div><div class="pi l"><p class="bd b dl z fp pf fr fs pg fu fw dk translated">github.com</p></div></div><div class="pj l"><div class="pk l pl pm pn pj po ix pa"/></div></div></a></div></div></div>    
</body>
</html>