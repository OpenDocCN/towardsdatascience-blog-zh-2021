<html>
<head>
<title>Supercharge your Image Search with Transfer Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过迁移学习增强您的图像搜索能力</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/supercharge-your-image-search-with-transfer-learning-75dfb5d29ceb?source=collection_archive---------21-----------------------#2021-07-27">https://towardsdatascience.com/supercharge-your-image-search-with-transfer-learning-75dfb5d29ceb?source=collection_archive---------21-----------------------#2021-07-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="206a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用迁移学习和近似最近邻的反向图像搜索</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/30fd1f397b802bd2fdc28fb8e20cdd54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0VMK8m21vFJolgFp4CU3xA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来源:<a class="ae kv" href="https://unsplash.com/@beccatapert" rel="noopener ugc nofollow" target="_blank">贝卡·泰普特</a>在<a class="ae kv" href="https://unsplash.com/photos/GnY_mW1Q6Xc?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditShareLink" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><h1 id="bfe9" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">图像相似度</h1><p id="d992" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi mk translated"><span class="l ml mm mn bm mo mp mq mr ms di">正如</span>你可能知道的，逆向图像搜索的问题是找到相似的图像。但是如何定义图像相似性呢？给定两幅样本图像，从视觉上我们可以很容易地确定它们是否相似。我们如何以编程方式做到这一点？</p><p id="d079" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx mv lz ma mb mw md me mf mx mh mi mj ij bi translated">一种非常幼稚的方法是基于与图像相关联的元数据。换句话说，我们可以将图像大小、RGB值、类别标签等元数据信息与每张图像相关联。许多web应用程序仍在利用这种方式进行反向图像搜索。此类应用程序通常将此类元数据存储在优化的文本搜索平台中，如ElasticSearch或Solr。这种方法很简单，但有许多缺陷。最明显的一个问题是标记每张图片需要大量的人工劳动。</p><p id="65b1" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx mv lz ma mb mw md me mf mx mh mi mj ij bi translated">一种更复杂的基于计算机视觉的技术是从图像本身提取不同的特征。传统的计算机视觉算法，如<strong class="lq ir">比例不变特征变换</strong> (SIFT)和<strong class="lq ir">加速稳健特征</strong> (SURF)在提取特征方面非常稳健，可以通过比较来识别相似的图像。SIFT、SURF和许多其他这样的传统方法擅长从输入图像中提取关键特征。<em class="my">图1 </em>描绘了如何利用基于SIFT的特征来寻找相似图像。SIFT识别每个候选人身上突出显示为彩色圆圈的关键特征，然后比较它们以找到相似的特征。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mz"><img src="../Images/17dedd98dea587d8f576d24c453396ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RyrgDlBqNFzVuwvJ35qb1A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1:基于SIFT的图像相似性特征。它非常适用于刚性或不变的对象。这个例子展示了两张从不同角度拍摄的自由女神像的照片。两幅图像之间的前50个匹配特征已经用彩色线示出。图片来源:作者</p></figure><p id="efc7" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx mv lz ma mb mw md me mf mx mh mi mj ij bi translated">这些是快速而强大的方法，在我们有刚性物体的情况下非常有用。例如，用矩形盒子(如包装纸盒)或圆形圆盘(如表盘)来识别图像，这种方法非常有效。另一方面，不太坚硬的物体很难匹配。例如，两个不同的人体模特以不同的姿势展示同一件衬衫/连衣裙，这对于传统技术来说可能是难以处理的。</p><p id="26e5" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx mv lz ma mb mw md me mf mx mh mi mj ij bi translated">您可能知道，可以尝试使用基于元数据的方法，甚至使用传统的基于计算机视觉的技术来识别相似的图像。两者都有各自的优势和缺陷。现在让我们从深度学习的角度来探索图像相似性。</p><h1 id="f63e" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">图像特征与迁移学习</h1><p id="bb35" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">可以在特征提取、微调和预训练设置中利用迁移学习。在CNN的情况下，重新回顾特征提取，我们知道来自预训练模型的初始层集合如何变得善于理解图像。在迁移学习的帮助下，我们可以训练高性能的分类器，它由一个预先训练好的网络作为特征提取器，再加上几个浅层组成。</p><blockquote class="na"><p id="6e79" class="nb nc iq bd nd ne nf ng nh ni nj mj dk translated">反向图像搜索用例是对预训练模型(迁移学习)的特征提取属性的巧妙利用。</p></blockquote><p id="2e98" class="pw-post-body-paragraph lo lp iq lq b lr nk jr lt lu nl ju lw lx nm lz ma mb nn md me mf no mh mi mj ij bi translated">反向图像搜索用例是对这种特征提取特性本身的巧妙利用。让我们利用预先训练的ResNet-50将图像转换为特征。这种特征通常是密集的一维向量。这个将输入转换成矢量的过程也被称为<strong class="lq ir"> <em class="my">矢量化</em> </strong>。我们首先需要准备一个预训练的ResNet-50模型的实例。接下来，我们需要一个效用函数来预处理输入图像并获得矢量输出。<em class="my">代码清单1 </em>展示了相同的实现。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代码清单1:使用预先训练的ResNet-50将图像转换成特征向量</p></figure><p id="8896" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx mv lz ma mb mw md me mf mx mh mi mj ij bi translated">如果我们仔细观察代码清单1，高级的<code class="fe nr ns nt nu b">tensorflow.keras</code> API使得准备预训练模型的实例并将其用于我们预期的特征提取变得非常容易。如图所示，该函数将图像输入转换为2048维的密集矢量输出。2048维向量是从ResNet模型的平均池层获得的，该模型在没有顶级分类层的情况下被实例化。</p><blockquote class="nv nw nx"><p id="63b3" class="lo lp my lq b lr mt jr lt lu mu ju lw ny mv lz ma nz mw md me oa mx mh mi mj ij bi translated"><strong class="lq ir">注意:</strong>我们利用<code class="fe nr ns nt nu b">ResNet-50</code>获得图像向量，但我们可以自由使用任何其他预训练模型。我们只需要记住一些方面，例如预训练网络的域、用于反向图像搜索的数据集以及输出向量的维度。例如，如果我们用<code class="fe nr ns nt nu b">VGG-16</code>代替<code class="fe nr ns nt nu b">ResNet-50</code>，特征向量将会减少到512维。当我们在接下来的小节中讨论一些高级的反向图像搜索方法时，我们将进一步讨论这些选择的影响。</p></blockquote><p id="24a3" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx mv lz ma mb mw md me mf mx mh mi mj ij bi translated">在我们进入反向图像搜索的实际任务之前，我们需要一个数据集。为了便于说明，我们将使用<strong class="lq ir">加州理工学院101 </strong>数据集。这个数据集是由费-李非和他的团队在2003年收集的，包含101个不同物体类别的大约40到800张图像。该数据集具有相当高质量的图像，非常适合我们的理解目的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/bdf609e284abfb035aaa6a22de24ee46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*51rjdKQDFY4Hh7mMh1dQRQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Caltech-101数据集中的样本图像</p></figure><p id="db46" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx mv lz ma mb mw md me mf mx mh mi mj ij bi translated">我们已经准备了一个实用程序来将图像转换为矢量，让我们利用相同的函数来获得参考数据集中图像的矢量表示。这也有助于形象化这些向量来更好地理解它们。想法是，相似的物体/图像应该有邻近的向量。但是一个人如何想象一个2048维的空间呢？为此，我们将采用一种叫做<strong class="lq ir">t-分布式随机邻居</strong>或t-SNE的降维技术。无需深入细节，可以将t-SNE视为将高维向量转换到低维空间，同时保留重要特征的一种方式。一个非常简单的例子是一个很长的购物清单作为我们的起点(高维向量)。将清单上的单个项目分成几类，如蔬菜、水果、奶制品等。可以被看作是向低维空间的转换，同时仍然保持重要的特征。在我们目前的情况下，我们将使用t-SNE变换2048维向量到2或3维。通过减少维度，我们可以很容易地将它们可视化。<em class="my">代码清单2 </em>展示了我们的参考数据集到向量的转换，然后为了可视化的目的对它们进行降维。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代码清单2:矢量化Caltech-101数据集和基于t-SNE的降维</p></figure><pre class="kg kh ki kj gt oc nu od oe aw of bi"><span id="5f06" class="og kx iq nu b gy oh oi l oj ok"># output<br/>Num images   =  9144<br/>Shape of feature_list =  (9144, 2048)<br/>Time taken in sec =  30.623387098312378</span></pre><p id="7d93" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx mv lz ma mb mw md me mf mx mh mi mj ij bi translated">我们正在为反向图像搜索准备一个解决方案。下一步使用这些特征来识别相似的图像。</p><h1 id="4437" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">讨厌:超快的邻居搜索</h1><p id="a72e" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">骚扰或<strong class="lq ir">近似最近邻哦是的</strong>是一个高度优化的超快速实现最近邻搜索从Spotify⁴.asury由Spotify开发，用于为用户提供音乐推荐。因为每天有数百万用户在他们的手机和网络上使用它，所以实现的重点是速度和准确性。用Python包装器在C++中开发，是一种基于树的向量相似性搜索方法，它有效地利用了内存和并行处理。它提供了许多距离度量选项，如欧几里德、余弦、汉明等。让我们看一下在<em class="my">代码清单3 </em>中使用ANNOY的反向图像搜索的快速实现。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代码清单3 (a):基于骚扰的反向图像搜索</p></figure><pre class="kg kh ki kj gt oc nu od oe aw of bi"><span id="3e76" class="og kx iq nu b gy oh oi l oj ok"># output<br/>1000 loops, best of 3: 770 µs per loop</span></pre><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="np nq l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代码清单3 (b):测试更大样本的基于骚扰的反向图像搜索的性能</p></figure><pre class="kg kh ki kj gt oc nu od oe aw of bi"><span id="d9e3" class="og kx iq nu b gy oh oi l oj ok"># output<br/>CPU times: user 139 ms, sys: 0 ns, total: 139 ms<br/>Wall time: 142 ms</span></pre><p id="be94" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx mv lz ma mb mw md me mf mx mh mi mj ij bi translated">如清单3所示，使用ANNOY非常简单。效率高，搜索时间快得惊人。搜索结果也相当可观，如图<em class="my">图2 </em>所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/bb30780657aed3e40fdcde9ca66daaa8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4hSAV48P6-H7pGMZWCTJoA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2:基于恼人的反向图像搜索。每行代表一个搜索查询。最左边的列表示查询图像，搜索结果(相似度递减)放在右边。每个匹配结果都用其与查询图像的欧几里德距离来标记。图片来源:作者</p></figure><p id="28d9" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx mv lz ma mb mw md me mf mx mh mi mj ij bi translated">ANNOY在很大程度上克服了与搜索速度和内存需求相关的限制。它还提供了一些额外的选项来进一步提高性能。其中之一与特征向量的大小有关。这也很明显。如果我们减少向量的大小，将直接减少寻找邻居所需的内存量。我们利用了输出2048维向量的<code class="fe nr ns nt nu b">ResNet-50</code>。如果我们使用像<code class="fe nr ns nt nu b">VGG-16</code>、<code class="fe nr ns nt nu b">VGG-19</code>甚至<code class="fe nr ns nt nu b">MobileNet</code>这样的模型，VGG模型的特征向量将减少到512，MobileNet将减少到1024(几乎减少了50%)。除此之外，我们甚至可以利用典型的降维技术，如主成分分析(PCA)或t-SNE，这是一些最广泛使用的技术。这种技术可以帮助我们大幅减少特征向量的大小，从而减少整体计算和内存需求(尽管会有搜索性能的折衷)。</p><p id="125b" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx mv lz ma mb mw md me mf mx mh mi mj ij bi translated">对优化的近似搜索方法的需求如此之大，以至于多年来已经开发了许多可扩展的解决方案。不赘述细节，雅虎的<strong class="lq ir">局部优化产品量化</strong> (LOPQ)⁵和<strong class="lq ir">邻域图和树</strong> (NGT) <em class="my"> ⁶ </em>等实现已经为Flickr等拥有超过10亿个数据点的庞大数据集的商业应用开发出来。同样，<strong class="lq ir"> Facebook人工智能相似性搜索</strong>或FAISS <em class="my"> ⁷ </em>是一个GPU优化的实现，速度快得惊人，即使在Facebook的规模下使用也非常简单。</p><p id="5eb6" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx mv lz ma mb mw md me mf mx mh mi mj ij bi translated">我们在本节中讨论的反向图像搜索的不同方法和实现基于这样的核心思想:首先将图像转换为特征向量，然后是相似性搜索算法。优化的实现更进一步，它关注内存的最佳利用，同时返回结果的速度也非常快。</p><p id="b259" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx mv lz ma mb mw md me mf mx mh mi mj ij bi translated">从这一部分可以明显看出，反向图像搜索是一个活跃的研究领域，研究人员正在利用这种技术不断拓展新的领域。这些方法的成功开辟了越来越多的使用案例，其中反向图像搜索是一个重要方面。</p><p id="4a7f" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx mv lz ma mb mw md me mf mx mh mi mj ij bi translated">既然我们对反向图像搜索有了相当好的理解，我们可以利用这些知识轻松地从电子商务网站上找到类似的产品，甚至从一组照片中删除重复的照片。同样，反向图片搜索也被Flickr等服务广泛用于建议相似的图片，谷歌购物，Pinterest的“相似图钉”功能，亚马逊的产品扫描仪，甚至被专业摄影师用于识别抄袭的作品。不同的场景需要不同的技术，本节讨论的每种方法的快速总结在<em class="my">表1 </em>中提供，以供快速参考。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/b32eb073b8329821d026b651273b1165.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iwRIM_2zAcRtvCuotT6EKg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">表1:各种邻域搜索方法的总结</p></figure></div><div class="ab cl on oo hu op" role="separator"><span class="oq bw bk or os ot"/><span class="oq bw bk or os ot"/><span class="oq bw bk or os"/></div><div class="ij ik il im in"><p id="6f68" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx mv lz ma mb mw md me mf mx mh mi mj ij bi translated">如果你想更多地了解这些有趣的话题，可以看看<a class="ae kv" href="https://www.linkedin.com/in/dipanzan/?originalSubdomain=in" rel="noopener ugc nofollow" target="_blank">迪潘詹·萨卡尔</a>和<a class="ae kv" href="https://raghavbali.github.io" rel="noopener ugc nofollow" target="_blank">拉格哈夫·巴厘</a>的《<strong class="lq ir">迁移学习在行动</strong>》。你可以在曼宁的liveBook平台<a class="ae kv" href="https://livebook.manning.com/book/transfer-learning-in-action?origin=product-look-inside&amp;utm_source=blog&amp;utm_medium=organic&amp;utm_campaign=book_sarkar_transfer_06_18_21&amp;utm_content=tdd" rel="noopener ugc nofollow" target="_blank">这里</a>查看一下。</p><p id="18fe" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx mv lz ma mb mw md me mf mx mh mi mj ij bi translated">在<a class="ae kv" href="https://www.manning.com/?utm_source=blog&amp;utm_medium=organic&amp;utm_campaign=book_sarkar_transfer_06_18_21&amp;utm_content=tdd" rel="noopener ugc nofollow" target="_blank">manning.com</a>结账时，将<strong class="lq ir"> fccsarkar </strong>输入折扣代码框，即可享受40%的折扣<a class="ae kv" href="https://www.manning.com/books/transfer-learning-in-action?utm_source=blog&amp;utm_medium=organic&amp;utm_campaign=book_sarkar_transfer_06_18_21&amp;utm_content=tdd" rel="noopener ugc nofollow" target="_blank"> <em class="my">转移学习行动</em> </a>。</p><h1 id="3aec" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">参考</h1><p id="4a58" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated"><a class="ae kv" href="#_ftnref1" rel="noopener ugc nofollow">【1】</a>弹性搜索:<a class="ae kv" href="https://www.elastic.co/what-is/elasticsearch" rel="noopener ugc nofollow" target="_blank">https://www.elastic.co/what-is/elasticsearch</a></p><p id="045e" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx mv lz ma mb mw md me mf mx mh mi mj ij bi translated"><a class="ae kv" href="#_ftnref2" rel="noopener ugc nofollow">【2】</a>索尔:<a class="ae kv" href="https://lucene.apache.org/solr/" rel="noopener ugc nofollow" target="_blank">https://lucene.apache.org/solr/</a></p><p id="73f0" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx mv lz ma mb mw md me mf mx mh mi mj ij bi translated"><a class="ae kv" href="#_ftnref1" rel="noopener ugc nofollow">【3】</a>加州理工学院-101数据集:【http://www.vision.caltech.edu/Image_Datasets/Caltech101/ T2】</p><p id="f5b0" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx mv lz ma mb mw md me mf mx mh mi mj ij bi translated"><a class="ae kv" href="#_ftnref1" rel="noopener ugc nofollow"/>LOPQ:<a class="ae kv" href="http://image.ntua.gr/iva/files/lopq.pdf" rel="noopener ugc nofollow" target="_blank">http://image.ntua.gr/iva/files/lopq.pdf</a></p><p id="8cf4" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx mv lz ma mb mw md me mf mx mh mi mj ij bi translated"><a class="ae kv" href="#_ftnref2" rel="noopener ugc nofollow">【5】</a>NGT代码:<a class="ae kv" href="https://github.com/yahoojapan/NGT/blob/master/README.md#publications" rel="noopener ugc nofollow" target="_blank">https://github . com/Yahoo Japan/NGT/blob/master/readme . MD # publications</a></p><p id="1d7b" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx mv lz ma mb mw md me mf mx mh mi mj ij bi translated"><a class="ae kv" href="#_ftnref3" rel="noopener ugc nofollow">【6】</a>https://arxiv.org/abs/1702.08734<a class="ae kv" href="https://arxiv.org/abs/1702.08734" rel="noopener ugc nofollow" target="_blank">NGT论文</a></p><p id="a988" class="pw-post-body-paragraph lo lp iq lq b lr mt jr lt lu mu ju lw lx mv lz ma mb mw md me mf mx mh mi mj ij bi translated"><a class="ae kv" href="#_ftnref1" rel="noopener ugc nofollow">【7】</a>惹怒:<a class="ae kv" href="https://github.com/spotify/annoy" rel="noopener ugc nofollow" target="_blank">https://github.com/spotify/annoy</a></p></div></div>    
</body>
</html>