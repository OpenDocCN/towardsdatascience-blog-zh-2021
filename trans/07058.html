<html>
<head>
<title>Predictive Analytics — Model Predictions And Their Interpretability Challenges</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">预测分析—模型预测及其可解释性挑战</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predictive-analytics-model-predictions-and-their-interpretability-challenges-acbb8ff44b3f?source=collection_archive---------20-----------------------#2021-06-26">https://towardsdatascience.com/predictive-analytics-model-predictions-and-their-interpretability-challenges-acbb8ff44b3f?source=collection_archive---------20-----------------------#2021-06-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="beff" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">模型可解释性在模型选择中的关键作用</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/0caa3141cefc25befc38b8991ddb998c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yPZap7X4nhqiLrkQZe-6wQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">弗兰基·查马基在<a class="ae kv" href="https://unsplash.com/s/photos/machine-learning?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="fa52" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">介绍</h1><p id="c400" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">人类通过推理来证明他们的决定。这样的推理有助于辨别决策，并对其充满信心。有时，这种推理逻辑可能不是直截了当的，但可以加入轶事引证、先入为主的直觉、假设的假设和公理化的思维。然而，只要这种判断足够令人信服，足以引起信心，这种决定通常是可以接受的。虽然原因可能并不简单，但任何因果关系的解释程度、与类似事件的任何关联、或潜在因素/环境之间的关系的确立，将导致对此类决定的更高程度的信任。本质上，这就像相信直觉相信判断。例如，患者相信基于医生对疾病的预测、对诊断的解释、基于经验的推理以及引用可比较的示例病例的治疗计划。</p><p id="5572" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">当预测模型预测结果或做出决定时，类似的推理也是可以预期的。然而，有一些关键的区别。虽然几个因果的、环境的、公理的和相关的因素可以证实人类的推理，但是唯一可以认可预测结果的因素是实例观察、特征特性和用于训练预测模型的算法。这些限制对预测模型的可解释性或可解释性提出了挑战。</p><h1 id="e83e" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">可解释性</h1><p id="9e25" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在进入讨论的实质之前，有必要理解可解释性在模型预测的上下文中意味着什么。对于模型的可解释性，业界没有一致的定义。然而，可解释性可以通过以下几个方面来描述:</p><ol class=""><li id="ae2c" class="mp mq iq lq b lr mk lu ml lx mr mb ms mf mt mj mu mv mw mx bi translated">算法清晰性:这涉及到对算法内部如何工作的全面理解，应用了哪些数学直觉，执行了哪些计算来处理和优化输入，处理后的输入如何映射到预测的输出，以及算法如何工作来确保预测的准确性。</li><li id="3fe6" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj mu mv mw mx bi translated">模型可解构性:这意味着将模型分解成粒度模型单元或结构的能力，这些单元或结构解释了模型预测的部分原因。例如，在贷款批准的决策树中，一个决策节点检查申请人的信用评分是否大于阈值。另一个例子是神经网络中的隐藏节点，它为特定的输入变量分配权重。</li><li id="df67" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj mu mv mw mx bi translated">模型综合能力:这意味着在人类可接受的时间框架内，使用模型参数和输入数据手动完成每个计算，以获得模型预测输出的能力。遍历分层线性模型中的每一步来解释对预测结果的固定效应和随机效应是模型综合的一个例子。</li><li id="8904" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj mu mv mw mx bi translated">事后可解释性:这是对上述所有内容的补充，以进一步加强预测推理。虽然事后可解释性并不严格依赖于模型算法的内部工作，但它通过提供额外的支持信息来补充。特设可能采用其他效用模型来支持推理。对于输入图像，使用k-最近邻模型呈现相似的图像来证明深度学习图像分类模型预测特定类别的原因是事后可解释性的一个示例。类似地，post hoc可以使用模型可视化和其他基于实例的图示来进一步解释预测。</li></ol><p id="e767" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">从上面可以推断，计算的复杂性和模型的体系结构极大地影响了模型的可解释性。虽然计算复杂性影响算法的清晰性和模型的可综合性，但模型架构的复杂性影响模型的可解构性。根据可解释性的程度，预测模型可以分为黑盒或白盒模型。</p><h1 id="b19b" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">黑盒与白盒模型</h1><p id="a2f8" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">预测分析主要解决两大类预测问题——分类和回归。在分类中，预测模型将基于输入变量预测不同的类别标签作为输出。在回归的情况下，预测模型将根据输入变量预测定量输出，如产品的价格。预测模型被归类为黑盒或白盒的首要原因是与模型行为的可解释性和模型结果的可解释性有关。</p><h2 id="daa6" class="nd kx iq bd ky ne nf dn lc ng nh dp lg lx ni nj li mb nk nl lk mf nm nn lm no bi translated">黑盒模型</h2><p id="60d6" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">给定一组输入数据，黑盒预测模型预测输出。虽然输入和输出变量之间的关系可以从训练数据集中观察到，但黑盒模型要么缺乏模型结果的可解释性，要么没有提供一种直接的方式来推理模型为什么以及如何预测特定的输出。缺乏可解释性有两个主要原因。第一个原因是底层算法是高度复杂的，具有复杂的数学计算。因此，即使是经验丰富的从业者也很难理解数学工作的模型，并将其转化为支持模型预测的推理。第二个原因是，虽然模型的内部计算很容易理解，但模型的设置和架构在训练时会变得复杂，因为输入数据要经过多次运算和复杂的转换。因此，虽然可以解释算法的基本表面工作，但很难根据输入数据在处理过程中经历的所有计算和转换来推理预测。具有利用多维向量空间中的复杂超平面的算法、利用复杂概率网络的算法以及利用子模型集合的算法的大多数模型通常属于黑盒模型的类别。然而，模型的准确性随着模型复杂性的增加而增加。因此，黑盒模型通常会产生更准确的预测，但代价是模型的可解释性。</p><h2 id="e546" class="nd kx iq bd ky ne nf dn lc ng nh dp lg lx ni nj li mb nk nl lk mf nm nn lm no bi translated">白盒模型</h2><p id="66ea" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">与黑盒模型类似，白盒模型在给定一组输入的情况下预测输出。但是，白盒模型可以解释训练数据集中输入和输出变量之间的关系，并且可以轻松地在预测输出和给定输入数据之间建立类似的关系。利用基于规则的算法、决策树、决策表、模式匹配和从训练实例中学习的算法的预测模型通常属于白盒解决方案的类别。这种算法通过推理模型如何和为什么预测特定输出，通过其内部工作和可解释性提供了清晰度。然而，白盒解决方案可能无法提供黑盒解决方案为复杂问题场景提供的准确性。尽管如此，在某些情况下，可以提高白盒模型的架构复杂性来增加模型的准确性，但代价是模型失去了模型的可解释性。在这种情况下，白盒模型可能会变成更多的黑盒模型。实际上，在黑盒和白盒模型之间进行选择的一个关键驱动因素是准确性和可解释性之间的权衡。</p><h1 id="f561" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">模型的功能类型</h1><p id="1cfd" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">模型类型的功能分类也有助于理解不同模型类型的可解释性是如何变化的。预测模型的功能类型可以大致分类如下:</p><h2 id="41a9" class="nd kx iq bd ky ne nf dn lc ng nh dp lg lx ni nj li mb nk nl lk mf nm nn lm no bi translated">基于超平面的模型</h2><p id="6e34" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">这些模型利用使用训练数据在多维向量空间中拟合超平面的算法。拟合的超平面在高维向量空间中分离输出变量的类别。给定输入数据，模型通过应用所需的转换来处理输入。多维空间中的变换输入向量可以帮助模型确定输入向量位于超平面的哪一侧，并选择输入向量最可能属于的输出类。这种模型属于黑盒解决方案的范畴，因为它们实现了复杂的数学计算，很难解释模型预测的原因。人工神经网络(ANN)、卷积神经网络(CNN)、递归神经网络(RNN)、支持向量机(SVM)、核方法等。是基于超平面的模型的例子。</p><h2 id="5b52" class="nd kx iq bd ky ne nf dn lc ng nh dp lg lx ni nj li mb nk nl lk mf nm nn lm no bi translated">带有子模型的集合</h2><p id="6256" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">集合模型使用多个子模型，在大多数情况下是弱子模型，以基于子模型做出的预测的投票或平均来提高准确性。他们使用boosting或bagging技术来训练每个子模型。子模型独立地或者基于来自前一个子模型的结果从训练数据中学习。这样，集成导致了复杂的决策层次结构，该层次结构很难解释给定输入数据的预测输出。因此，这种集合模型属于黑箱解决方案的范畴。随机森林(RF)、极端梯度提升(XGBoost)、自适应提升(AdaBoost)、贝叶斯自适应采样(BAS)等。是集成算法的例子。</p><h2 id="c19d" class="nd kx iq bd ky ne nf dn lc ng nh dp lg lx ni nj li mb nk nl lk mf nm nn lm no bi translated">概率网络模型</h2><p id="83ee" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">底层算法在概率网络模型中构建特征和输出节点的复杂非循环有向图。每个节点维护连接到它的先前节点的所有组合条件概率的知识。实际上，模型会选择输出结点值作为预测，对于该预测，基于模型遍历的所有先前结点评估的条件概率最大。虽然此类模型采用的概率理论很容易理解，但在现实世界中，这些模型会构建一个复杂的节点图，其中包含内部计算的不同特征节点。因此，遍历模型并解释它为什么以及如何预测特定的输出变得非常困难。因此，这种模型属于黑箱解决方案的范畴。马尔可夫网络和贝叶斯网络就是这种模型的例子。</p><h2 id="f800" class="nd kx iq bd ky ne nf dn lc ng nh dp lg lx ni nj li mb nk nl lk mf nm nn lm no bi translated">规则驱动的、基于模式的、类似决策树的模型</h2><p id="bac4" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在规则驱动、基于模式或类似决策树的模型的情况下，在开发预测模型之前，输出结合输入的前提是众所周知和充分理解的。这种预测模型甚至是在理解了这种前提之后才开发出来的，因为虽然这种规则和模式的排列和组合是有限的，但是它们很难管理。一旦模型从训练数据集学习，模型就承担起这种繁重的工作。因为模型基于特定模式或规则集或通过遍历决策树来进行预测，所以很容易根据模型在考虑每个输入变量时必须做出的中间决策来解释预测。因此，这种模型通常属于白盒解决方案的范畴。然而，如果规则集变得过于层次化，或者决策树变得过于深入和嵌套，则可解释性可能会受到影响，因为由此产生的错综复杂的决策层次结构可能太难解释。在这种情况下，模型变成了一个黑盒解决方案。</p><h2 id="b652" class="nd kx iq bd ky ne nf dn lc ng nh dp lg lx ni nj li mb nk nl lk mf nm nn lm no bi translated">线性模型</h2><p id="e38d" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">线性模型建立了输入和输出变量之间的线性关系。这种关系的强度以作为模型参数的线性系数的形式来估计。参数的量化估计有助于确定每个输入要素相对于输出变量的重要性，并有助于解释模型基于给定输入数据预测输出的原因和方式。因此，线性模型通常属于白盒解决方案的范畴。然而，如果线性模型具有高维输入空间，或者如果在模型开发时附加了几个计算工程变量或虚拟变量，则线性模型可能会失去其可解释性。线性回归和逻辑回归就是这种线性模型的例子。</p><h2 id="8b0f" class="nd kx iq bd ky ne nf dn lc ng nh dp lg lx ni nj li mb nk nl lk mf nm nn lm no bi translated">基于实例的模型</h2><p id="b162" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">在基于实例的模型的情况下，该算法基于给定输入数据和来自训练数据的过去实例之间的局部近似进行预测，该训练数据基于指定的目标函数表现出相似的特征，然后采用策略来确定局部的领域以选择实例。一旦选择了在功能上与给定输入相似的本地实例，该算法就基于投票或平均来挑选预测。因为预测是基于使用相似实例的局部近似，所以该模型表现出高度的可解释性。实际上，本地选择的相似实例的特征可以解释模型的预测行为。因此，基于实例的模型通常属于白盒解决方案的范畴。k近邻(kNN)、自组织映射(SOM)等。是基于实例的模型的例子。</p><h1 id="d1b5" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">型号选择</h1><p id="be1b" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">确定哪种模型解决方案最合适取决于几个因素，包括底层算法主题和技术细节领域之外的因素。例如，当模型预测被认为是准确的，但被理解为超出人类的识别能力时，黑盒解决方案将是正确的。在这种情况下，模型执行复杂的操作来建立人类无法理解的输入和输出变量之间的关系。例如，在互联车辆用例中预测关键道路交叉口的交通量。在这种情况下，模型可能需要使用来自路边传感器的数据、关于附近事故或道路维修的数据、交通高峰时间等。，准确预测交通量。人工神经网络(ANN)或支持向量回归(SVR)或极端梯度推进(XGBoost)等复杂算法的使用是有保证的，因为:</p><ol class=""><li id="ca39" class="mp mq iq lq b lr mk lu ml lx mr mb ms mf mt mj mu mv mw mx bi translated">该解决方案旨在预测交通量，以防止任何道路事故。</li><li id="10c7" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj mu mv mw mx bi translated">没有预设的模式来建立规则，以规定不同输入的什么组合将确定交通量的估计。排列和组合的数量会激增到超出人类理解的程度。</li><li id="35d2" class="mp mq iq lq b lr my lu mz lx na mb nb mf nc mj mu mv mw mx bi translated">模型预测可以很容易地与人类可以预测的相关联，因此模型结果可以被验证。</li></ol><p id="400a" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">同样，在有些情况下，必须采用白盒解决方案。例如，推理和因果关系优先的情况，因为潜在的问题域要求对预测进行这样的推理，或者遵从性法规强制执行这样的推理要求。例如，美国《平等信贷机会法》认为，拒绝向理由模糊或不合理的申请人提供银行贷款是非法的。因此，金融机构可能必须采用白盒模型进行分类，同时提供一种简单的方式来解释预测决策背后令人满意的原因。类似地，在医疗保健和医学领域，决策需要合理的推理和因果关系归属，采用白盒解决方案进行医学预测可能变得至关重要。在所有这些情况下，预测的纯粹准确性不再重要，但可解释性变得至关重要。因此，所选择的模型应该基于这两种考虑之间的折衷。在其他一些场景中，模型的简单性可能会促使选择更直接的白盒解决方案。</p><p id="97ca" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">然而，有些情况下可以利用黑盒和白盒模型的组合来开发预测分析解决方案。在这种情况下，黑盒模型被用来进行初步预测，但事后白盒模型补充了预测的可解释性。例如，用于图像分类的CNN模型的可解释性将是困难的，因为CNN采用了复杂的卷积运算、图像过滤和变换，这些运算和变换缺乏通过人类解释的可识别性。然而，可以使用CNN为每个训练图像提取的复杂特征来开发单独的、更简单的白盒模型，例如决策树或聚类模型。然后，这种白盒模型可以匹配来自训练数据的相似图像，以实现事后推理和可解释性。</p><h1 id="2ff4" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">结论</h1><p id="0c0c" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">当预测模型预测一个结果或做出一个决定时，人类可以理解的推理是被期待的。能够证实预测结果的可解释性的唯一要素是实例观察、特征特性和用于训练预测模型的算法。这些限制对预测模型的可解释性提出了挑战。虽然模型可解释性没有标准的定义，但是底层算法的清晰性、模型的可解构性、模型的可综合性和事后可解释性是整个模型可解释性的特征。</p><p id="4cb3" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">根据可解释性的程度，预测模型可以分为黑盒或白盒模型。黑盒模型缺乏模型结果的可解释性，或者缺乏一种简单的方法来解释为什么以及如何预测特定的输出。相比之下，白盒模型能够解释训练数据集中输入和输出变量之间的关系，并易于在预测输出和给定输入数据之间建立类似的关系。具有利用多维向量空间中的复杂超平面的算法的模型、利用复杂概率网络的算法以及利用子模型集合的算法通常属于黑盒模型的类别。利用基于规则的算法、决策树、决策表、模式匹配和从训练实例中学习的算法的模型通常属于白盒解决方案的类别。</p><p id="8186" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">确定哪种模型解决方案最合适取决于几个因素，包括底层算法主题和技术细节领域之外的因素。黑盒解决方案将是正确的，模型预测预计是准确的，但被理解为超出了人类的识别能力。推理和因果关系优先的用例，因为潜在的问题域要求对预测进行这样的推理，或者合规性法规使用白盒模型强制执行这样的推理要求保证。一些用例利用黑盒和白盒模型的组合来开发预测分析解决方案。在这种情况下，通常采用黑盒模型进行更精确的初步预测，而白盒模型补充了预测的事后可解释性。</p><p id="91d1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">由于模型的准确性会随着模型复杂性的增加而增加，因此黑盒模型通常会产生更准确的预测，但会牺牲模型的可解释性。实际上，在黑盒和白盒模型之间进行选择的一个关键驱动因素是准确性和可解释性之间的权衡。因此，在决定选择模型算法来开发预测分析解决方案之前，数据科学家和机器学习专家必须首先了解问题域、其目标以及底层的合规性和推理要求。在平衡模型准确性的同时，模型的可解释性对于满足所有这些强制性要求变得至关重要。</p></div></div>    
</body>
</html>