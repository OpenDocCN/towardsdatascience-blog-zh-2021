<html>
<head>
<title>Nested Data Types in Spark 3.1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Spark 3.1中的嵌套数据类型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nested-data-types-in-spark-3-1-663e5ed2f2aa?source=collection_archive---------3-----------------------#2021-07-30">https://towardsdatascience.com/nested-data-types-in-spark-3-1-663e5ed2f2aa?source=collection_archive---------3-----------------------#2021-07-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="dc55" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在Spark SQL中使用结构</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3b996443da8a64fe077cc7f42dc4cc8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fa8far5scpxA7TzeAclEaA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">埃利斯·陈嘉炜在<a class="ae ky" href="https://unsplash.com/s/photos/complex?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="3e3d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在之前关于高阶函数的<a class="ae ky" rel="noopener" target="_blank" href="/higher-order-functions-with-spark-3-1-7c6cf591beaa">文章</a>中，我们描述了三种复杂的数据类型:数组、映射和结构，并特别关注了数组。在这篇后续文章中，我们将研究structs，并了解Spark 3.1.1版本中发布的用于转换嵌套数据的两个重要函数。对于代码，我们将使用Python API。</p><h1 id="ead6" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结构体</h1><p id="b17f" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated"><em class="ms"> StructType </em>是一种非常重要的数据类型，允许表示嵌套的层次数据。它可用于将一些字段组合在一起。<em class="ms"> StructType </em>的每个元素被称为<em class="ms"> StructField </em>，它有一个名称，也有一个类型。元素通常也称为字段或子字段，它们通过名称来访问。<em class="ms"> StructType </em>也用于表示整个数据帧的模式。让我们看一个简单的例子</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="c969" class="my lw it mu b gy mz na l nb nc">from pyspark.sql.types import *</span><span id="de5a" class="my lw it mu b gy nd na l nb nc">my_schema = StructType([<br/>    StructField('id', LongType()),<br/>    StructField('country', StructType([<br/>        StructField('name', StringType()),<br/>        StructField('capital', StringType())<br/>    ])),<br/>    StructField('currency', StringType())<br/>])</span><span id="bafa" class="my lw it mu b gy nd na l nb nc">l = [<br/>        (1, {'name': 'Italy', 'capital': 'Rome'}, 'euro'),<br/>        (2, {'name': 'France', 'capital': 'Paris'}, 'euro'),<br/>        (3, {'name': 'Japan', 'capital': 'Tokyo'}, 'yen')<br/>    ]</span><span id="7675" class="my lw it mu b gy nd na l nb nc">df = spark.createDataFrame(l, schema=my_schema)</span><span id="73c0" class="my lw it mu b gy nd na l nb nc">df.printSchema()</span><span id="15c3" class="my lw it mu b gy nd na l nb nc">root<br/> |-- id: long (nullable = true)<br/> |-- country: struct (nullable = true)<br/> |    |-- name: string (nullable = true)<br/> |    |-- capital: string (nullable = true)<br/> |-- currency: string (nullable = true)</span><span id="a0a7" class="my lw it mu b gy nd na l nb nc">df.show()<br/>+---+---------------+--------+<br/>| id|        country|currency|<br/>+---+---------------+--------+<br/>|  1|  {Italy, Rome}|    euro|<br/>|  2|{France, Paris}|    euro|<br/>|  3| {Japan, Tokyo}|     yen|<br/>+---+---------------+--------+</span></pre><p id="1a3d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">创建的DataFrame有一个结构<em class="ms"> country </em>，它有两个子字段:<em class="ms"> name </em>和<em class="ms"> capital </em>。</p><h2 id="016a" class="my lw it bd lx ne nf dn mb ng nh dp mf li ni nj mh lm nk nl mj lq nm nn ml no bi translated">创建结构</h2><p id="52f2" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">至少有四种基本方法可以在数据框中创建一个<em class="ms">结构类型</em>。第一种方法我们已经在上面看到过——从本地集合创建DataFrame。第二种也是最常见的方式是从支持复杂数据结构的数据源读取数据，比如JSON或Parquet。接下来，有一些函数将创建一个结构作为结果。这种转换的一个特殊例子是通过<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.window.html#pyspark.sql.functions.window" rel="noopener ugc nofollow" target="_blank"> <em class="ms">窗口</em> </a>进行分组，这将产生一个具有两个子字段<em class="ms"> start </em>和<em class="ms"> end </em>的结构，正如您在这里看到的:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="e3ad" class="my lw it mu b gy mz na l nb nc">l = [(1, 10, '2021-01-01'), (2, 20, '2021-01-02')]</span><span id="bac9" class="my lw it mu b gy nd na l nb nc">dx = spark.createDataFrame(l, ['id', 'price', 'date'])</span><span id="a8f9" class="my lw it mu b gy nd na l nb nc">(<br/>    dx<br/>    .groupBy(window('date', '1 day'))<br/>    .agg(sum('price').alias('daily_price'))<br/>).printSchema()</span><span id="ca04" class="my lw it mu b gy nd na l nb nc">root<br/> |-- window: struct (nullable = false)<br/> |    |-- start: timestamp (nullable = true)<br/> |    |-- end: timestamp (nullable = true)<br/> |-- daily_price: long (nullable = true)</span></pre><p id="fa12" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第四种创建struct的方法是使用函数<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.struct.html#pyspark.sql.functions.struct" rel="noopener ugc nofollow" target="_blank"><em class="ms">【struct()</em></a>。该函数将从作为参数传递的其他列创建一个<em class="ms"> StructType </em>，并且<em class="ms"> StructFields </em>将与原始列具有相同的名称，除非我们使用<em class="ms"> alias() </em>重命名它们:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="fe5c" class="my lw it mu b gy mz na l nb nc">df.withColumn('my_struct', struct('id', 'currency')).printSchema()</span><span id="0753" class="my lw it mu b gy nd na l nb nc">root<br/> |-- id: long (nullable = true)<br/> |-- country: struct (nullable = true)<br/> |    |-- name: string (nullable = true)<br/> |    |-- capital: string (nullable = true)<br/> |-- currency: string (nullable = true)<br/> |-- my_struct: struct (nullable = false)<br/> |    |-- id: long (nullable = true)<br/> |    |-- currency: string (nullable = true)</span></pre><p id="5d92" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，我们创建了一个列<em class="ms"> my_struct </em>，它有两个子字段，这两个子字段是从数据帧中的两个列派生出来的。</p><h2 id="1c78" class="my lw it bd lx ne nf dn mb ng nh dp mf li ni nj mh lm nk nl mj lq nm nn ml no bi translated">访问元素</h2><p id="5dd3" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">正如我们上面提到的，结构的子字段是通过名称来访问的，这是通过点符号来完成的:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="232a" class="my lw it mu b gy mz na l nb nc">df.select('country.capital').show()</span><span id="e028" class="my lw it mu b gy nd na l nb nc">+-------+<br/>|capital|<br/>+-------+<br/>|   Rome|<br/>|  Paris|<br/>|  Tokyo|<br/>+-------+</span></pre><p id="cbd9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可能不明显的是，这也适用于结构数组。假设我们有一个数组<em class="ms">国家</em>，数组的每个元素都是一个结构体。如果我们只想访问每个结构的<em class="ms">大写</em>子字段，我们可以用完全相同的方法，得到的列将是一个包含所有大写的数组:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="1389" class="my lw it mu b gy mz na l nb nc">my_new_schema = StructType([<br/>    StructField('id', LongType()),<br/>    StructField('countries', ArrayType(StructType([<br/>        StructField('name', StringType()),<br/>        StructField('capital', StringType())<br/>    ])))<br/>])</span><span id="b5c9" class="my lw it mu b gy nd na l nb nc">l = [(1, [<br/>        {'name': 'Italy', 'capital': 'Rome'},<br/>        {'name': 'Spain', 'capital': 'Madrid'}<br/>    ])<br/>]<br/>    <br/>dz = spark.createDataFrame(l, schema=my_new_schema)</span><span id="559f" class="my lw it mu b gy nd na l nb nc"># we have array of structs:<br/>dz.show(truncate=False)</span><span id="c446" class="my lw it mu b gy nd na l nb nc">+---+--------------------------------+<br/>|id |countries                       |<br/>+---+--------------------------------+<br/>|1  |[{Italy, Rome}, {Spain, Madrid}]|<br/>+---+--------------------------------+</span><span id="6763" class="my lw it mu b gy nd na l nb nc"># access all capitals:<br/>dz.select('countries.capital').show(truncate=False)</span><span id="3f1c" class="my lw it mu b gy nd na l nb nc">+--------------+<br/>|capital       |<br/>+--------------+<br/>|[Rome, Madrid]|<br/>+--------------+</span></pre><p id="b89a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于访问数组内部嵌套结构中元素的另一个具体例子，参见<a class="ae ky" href="https://stackoverflow.com/questions/57810876/how-to-check-if-a-spark-data-frame-struct-array-contains-a-specific-value/57812763#57812763" rel="noopener ugc nofollow" target="_blank">这个</a>堆栈溢出问题。</p><h2 id="6224" class="my lw it bd lx ne nf dn mb ng nh dp mf li ni nj mh lm nk nl mj lq nm nn ml no bi translated">添加新元素</h2><p id="92e9" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">从Spark 3.1开始，支持使用函数<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Column.withField.html#pyspark.sql.Column.withField" rel="noopener ugc nofollow" target="_blank"> <em class="ms"> withField() </em> </a>向现有结构添加新的子字段。让我们看看我们的例子，其中我们将列<em class="ms">货币</em>添加到结构<em class="ms">国家:</em></p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="6b60" class="my lw it mu b gy mz na l nb nc">(<br/>  df<br/>  .withColumn(<br/>    'country', <br/>    col('country').withField('currency', col('currency'))<br/>  )<br/>).show(truncate=False)</span><span id="e664" class="my lw it mu b gy nd na l nb nc">+---+---------------------+--------+<br/>|id |country              |currency|<br/>+---+---------------------+--------+<br/>|1  |{Italy, Rome, euro}  |euro    |<br/>|2  |{France, Paris, euro}|euro    |<br/>|3  |{Japan, Tokyo, yen}  |yen     |<br/>+---+---------------------+--------+</span></pre><p id="dfea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在Spark 3.1之前，情况更复杂，通过重新定义整个结构，可以向结构中添加新字段:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="dcb8" class="my lw it mu b gy mz na l nb nc">new_df = (<br/>  df.withColumn('country', struct(<br/>    col('country.name'),<br/>    col('country.capital'),<br/>    col('currency')<br/>  ))<br/>)</span></pre><p id="0e95" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，我们必须列出所有的结构子字段，然后添加新的字段——这可能非常麻烦，尤其是对于具有许多子字段的大型结构。在这种情况下，有一个很好的技巧可以让你一次处理所有的子字段——使用星形符号:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="c695" class="my lw it mu b gy mz na l nb nc">new_df = (<br/>  df.withColumn('country', struct(<br/>    col('country.*'),<br/>    col('currency')<br/>  ))<br/>)</span></pre><p id="efbb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ms">国家中的星号。* </em>将获取原始结构的所有子字段。然而，在下一个我们想要删除字段的例子中，情况会变得更加复杂。</p><h2 id="cd33" class="my lw it bd lx ne nf dn mb ng nh dp mf li ni nj mh lm nk nl mj lq nm nn ml no bi translated">移除元素</h2><p id="5d30" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">从Spark 3.1开始，从结构中删除子字段又是一项简单的任务，因为函数<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Column.dropFields.html#pyspark.sql.Column.dropFields" rel="noopener ugc nofollow" target="_blank"> <em class="ms"> dropFields() </em> </a>已经发布。现在让我们使用修改后的数据框架<em class="ms"> new_df </em>，其中该结构包含三个子字段<em class="ms"> name </em>、<em class="ms"> capital、</em>和<em class="ms"> currency </em>。例如，删除一个子字段<em class="ms">大写</em>可以按如下方式完成:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="008a" class="my lw it mu b gy mz na l nb nc">new_df.withColumn('country',col('country').dropFields('capital')) \<br/>.show(truncate=False)</span><span id="956d" class="my lw it mu b gy nd na l nb nc">+---+--------------+--------+<br/>|id |country       |currency|<br/>+---+--------------+--------+<br/>|1  |{Italy, euro} |euro    |<br/>|2  |{France, euro}|euro    |<br/>|3  |{Japan, yen}  |yen     |<br/>+---+--------------+--------+</span></pre><p id="8250" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们所见，子字段<em class="ms">大写</em>被删除。在Spark 3.1之前的版本中，情况又变得复杂了，我们必须重新定义整个结构，并删除我们想要删除的子字段:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="cd0e" class="my lw it mu b gy mz na l nb nc">(<br/>    new_df<br/>    .withColumn('country', struct(<br/>        col('country.name'),<br/>        col('country.currency')<br/>    ))<br/>)</span></pre><p id="d525" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于大型结构来说，这又是一个繁琐的过程，所以我们可以通过如下列出所有子字段来使其更加可行:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="a722" class="my lw it mu b gy mz na l nb nc"># list all fields in the struct:<br/>subfields = new_df.schema['country'].dataType.fieldNames()</span><span id="4ccc" class="my lw it mu b gy nd na l nb nc"># remove the subfield from the list:<br/>subfields.remove('capital')</span><span id="2fb6" class="my lw it mu b gy nd na l nb nc"># use the new list to recreate the struct:<br/>(<br/>    new_df.withColumn(<br/>        'country',<br/>        struct(<br/>            ['country.{}'.format(x) for x in subfields]<br/>        )<br/>    )<br/>).show()</span><span id="d90d" class="my lw it mu b gy nd na l nb nc">+---+--------------+--------+<br/>| id|       country|currency|<br/>+---+--------------+--------+<br/>|  1| {Italy, euro}|    euro|<br/>|  2|{France, euro}|    euro|<br/>|  3|  {Japan, yen}|     yen|<br/>+---+--------------+--------+</span></pre><p id="51d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，两个函数<em class="ms"> withField </em>和<em class="ms"> dropFields </em>都是<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#column-apis" rel="noopener ugc nofollow" target="_blank"> <em class="ms">列</em> </a>类的成员，因此它们被称为列对象上的方法(要了解更多如何使用列类中的方法，请查看我最近的<a class="ae ky" rel="noopener" target="_blank" href="/a-decent-guide-to-dataframes-in-spark-3-0-for-beginners-dcc2903345a5">文章</a>，在那里我会更详细地讨论它)。</p><h1 id="cea7" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">SQL表达式中的结构</h1><p id="99de" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">当您查看<a class="ae ky" href="https://spark.apache.org/docs/latest/api/sql/index.html" rel="noopener ugc nofollow" target="_blank"> SQL </a>文档时，您会发现有两个函数可用于创建结构，即<a class="ae ky" href="https://spark.apache.org/docs/latest/api/sql/index.html#struct" rel="noopener ugc nofollow" target="_blank"> <em class="ms"> struct() </em> </a>和<a class="ae ky" href="https://spark.apache.org/docs/latest/api/sql/index.html#named_struct" rel="noopener ugc nofollow" target="_blank"><em class="ms">named _ struct()</em></a>，它们在语法上有所不同，因为<em class="ms"> named_struct </em>也要求为每个子字段传递一个名称:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="3b61" class="my lw it mu b gy mz na l nb nc">(<br/>  df<br/>  .selectExpr("struct(id, currency) as my_struct")<br/>).show(truncate=False)</span><span id="64c8" class="my lw it mu b gy nd na l nb nc">+---------+<br/>|my_struct|<br/>+---------+<br/>|{1, euro}|<br/>|{2, euro}|<br/>|{3, yen} |<br/>+---------+</span><span id="53da" class="my lw it mu b gy nd na l nb nc">(<br/>  df.selectExpr(<br/>    "named_struct('id', id, 'currency', currency) as my_struct")<br/>).show()</span><span id="2473" class="my lw it mu b gy nd na l nb nc">+---------+<br/>|my_struct|<br/>+---------+<br/>|{1, euro}|<br/>|{2, euro}|<br/>| {3, yen}|<br/>+---------+</span></pre><h1 id="8546" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="2801" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在本文中，我们继续描述Spark SQL中的复杂数据类型。在之前的<a class="ae ky" rel="noopener" target="_blank" href="/higher-order-functions-with-spark-3-1-7c6cf591beaa">文章</a>中，我们讨论了数组，这里我们关注结构，在未来的文章中，我们将讨论映射。我们已经看到了最近版本3.1.1中发布的两个重要函数<em class="ms"> withField() </em>和<em class="ms"> dropFields() </em>，它们可以在操作现有结构的子字段时大大简化代码。</p></div></div>    
</body>
</html>