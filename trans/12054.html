<html>
<head>
<title>Unsupervised Topic Segmentation of Meetings with BERT Embeddings (Summary)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">具有BERT嵌入的会议的无监督主题分割(摘要)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/unsupervised-topic-segmentation-of-meetings-with-bert-embeddings-summary-46e1b7369755?source=collection_archive---------9-----------------------#2021-12-05">https://towardsdatascience.com/unsupervised-topic-segmentation-of-meetings-with-bert-embeddings-summary-46e1b7369755?source=collection_archive---------9-----------------------#2021-12-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="517a" class="pw-subtitle-paragraph jr is it bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated"><em class="jq"> NLP研究论文演练</em></h2></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi kj"><img src="../Images/6aeebfbced390bc475e8840d4c5a2d4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NL9iJ5Tj54q51b-S0yR47Q.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated"><em class="jq">修改后的图片来自</em> <a class="ae kz" href="https://unsplash.com/photos/W8KTS-mhFUE" rel="noopener ugc nofollow" target="_blank"> <em class="jq">来源</em> </a></p></figure><p id="41ae" class="pw-post-body-paragraph la lb it lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><em class="lw">在这篇博客中，我试着根据我的理解总结了</em> <a class="ae kz" href="https://arxiv.org/pdf/2106.12978.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lw">使用BERT嵌入的会议无监督主题分割</em> </a> <em class="lw">这篇论文。请随时评论你的想法！</em></p><h1 id="dc1c" class="lx ly it bd lz ma mb mc md me mf mg mh ka mi kb mj kd mk ke ml kg mm kh mn mo bi translated"><em class="jq">问题陈述</em></h1><p id="6b7c" class="pw-post-body-paragraph la lb it lc b ld mp jv lf lg mq jy li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">在过去的几年里，各种在线会议工具的使用突然激增，比如Zoom、Google Meet、Microsoft Teams等。几乎所有的时间你都会因为这样或那样的原因记录这些会议，但是因为它们的原始形式和某人操纵会议中讨论的要点所需要的痛苦，它们真的被再次看到或提及吗？我想不会。受这个问题的启发，脸书的研究人员提出了一种<strong class="lc iu">无监督的方法，使用预先训练的BERT嵌入将会议分割成主题</strong>。</p><p id="b978" class="pw-post-body-paragraph la lb it lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">会议背景下的主题分割是预测多人会议记录中主题边界的任务。作者在无人监督的情况下特别针对这项任务，因为大规模手动注释这些会议很难。</p><h1 id="b5be" class="lx ly it bd lz ma mb mc md me mf mg mh ka mi kb mj kd mk ke ml kg mm kh mn mo bi translated"><em class="jq">方法建议</em></h1><p id="26ec" class="pw-post-body-paragraph la lb it lc b ld mp jv lf lg mq jy li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">假设我们有一份会议记录<strong class="lc iu"> M </strong>，其中有<strong class="lc iu"> U </strong>个总话语数和<strong class="lc iu"> T </strong>个总话题边界数需要我们识别。这里T个话题中的每一个都是用U个连续的话语集合来表示的，姑且称之为可以用U(i)到U(i+z)来表示的块。这里，<strong class="lc iu"> z </strong>是块中考虑的连续话语的计数。现在以M和U作为输入，任务是预测长度为U的布尔向量，其中0表示话语U(i)包含在正在进行的主题中，1表示话语U(i)是正在进行的主题的主题转移。</p><p id="c931" class="pw-post-body-paragraph la lb it lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">如前所述，作者利用BERT嵌入来获得会议片段的语义表示。他们为此测试了两种技术，其中一种是max-pooled预训练<a class="ae kz" href="https://huggingface.co/roberta-base" rel="noopener ugc nofollow" target="_blank"> RoBERTa在架构中倒数第二层的嵌入</a>。第二个是<a class="ae kz" href="https://www.sbert.net/docs/pretrained_models.html" rel="noopener ugc nofollow" target="_blank">句子-BERT </a>模型(BERT使用<a class="ae kz" rel="noopener" target="_blank" href="/a-friendly-introduction-to-siamese-networks-85ab17522942">暹罗网络</a>在SNLI数据集上训练)。</p><p id="fea1" class="pw-post-body-paragraph la lb it lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在此之后，他们将整个会议语料库分成大小为<strong class="lc iu"> z </strong>的多个块，并通过<strong class="lc iu">对话语嵌入分层执行最大池操作</strong>来获得每个块的嵌入表示。在获得块表示之后，它们计算对话中相邻块表示之间的<a class="ae kz" href="https://en.wikipedia.org/wiki/Cosine_similarity" rel="noopener ugc nofollow" target="_blank">角距离</a>。最后，基于某个阈值，他们给话语分配0或1值。</p><p id="766a" class="pw-post-body-paragraph la lb it lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><em class="lw">下图直观地说明了这一点— </em></p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/aebaf46d0c8dea195e501c4544fbd02d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*TEzJAXF1SeU1EL7W-C6C-Q.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">提议方法的插图|作者提供的图片</p></figure><h1 id="87ae" class="lx ly it bd lz ma mb mc md me mf mg mh ka mi kb mj kd mk ke ml kg mm kh mn mo bi translated"><em class="jq">我的想法</em></h1><p id="9015" class="pw-post-body-paragraph la lb it lc b ld mp jv lf lg mq jy li lj mr ll lm ln ms lp lq lr mt lt lu lv im bi translated">我觉得这确实是一项有趣、实用、同时又具有挑战性的任务。所提出的方法看起来很有希望，尽管在下一次迭代中，有趣的是看到围绕发言者的身份、会议的主题、专业领域地名词典等添加额外的特征将如何影响当前的分数。此外，观察使用非上下文嵌入时的分数变化也是值得尝试的。下一步可以尝试的一件更有趣的事情是提出一种自动标记这些主题的技术。</p><blockquote class="mv"><p id="8a28" class="mw mx it bd my mz na nb nc nd ne lv dk translated"><em class="jq">如果你愿意，你也可以</em> <a class="ae kz" href="https://medium.com/analytics-vidhya/summarizing-nlp-research-papers-dbd12965aa0a" rel="noopener"> <em class="jq">看看我写的其他研究论文摘要</em> </a> <em class="jq">。</em></p></blockquote><p id="3c6f" class="pw-post-body-paragraph la lb it lc b ld nf jv lf lg ng jy li lj nh ll lm ln ni lp lq lr nj lt lu lv im bi translated"><em class="lw">随意看论文，对作者说“</em> <strong class="lc iu"> <em class="lw"> Hi </em> </strong> <em class="lw">”，感谢他们的贡献。此外，如果你喜欢看视频而不是文字(就像我:D一样)，一定要看看——</em></p><figure class="kk kl km kn gt ko"><div class="bz fp l di"><div class="nk nl l"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated"><a class="ae kz" href="https://www.youtube.com/channel/UCoz8NrwgL7U9535VNc0mRPA" rel="noopener ugc nofollow" target="_blank"> <em class="jq">多看此类研究论文演练</em> </a></p></figure><blockquote class="nm nn no"><p id="0233" class="la lb lw lc b ld le jv lf lg lh jy li np lk ll lm nq lo lp lq nr ls lt lu lv im bi translated"><em class="it">⏩</em> <strong class="lc iu">论文标题</strong>:具有BERT嵌入的会议无监督主题分割</p><p id="e926" class="la lb lw lc b ld le jv lf lg lh jy li np lk ll lm nq lo lp lq nr ls lt lu lv im bi translated">⏩<strong class="lc iu">论文</strong>:【https://arxiv.org/pdf/2106.12978.pdf】T2</p><p id="3449" class="la lb lw lc b ld le jv lf lg lh jy li np lk ll lm nq lo lp lq nr ls lt lu lv im bi translated">⏩<strong class="lc iu">作者</strong> : <a class="ae kz" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Solbiati%2C+A" rel="noopener ugc nofollow" target="_blank">亚历山德罗·索尔比亚蒂</a>，<a class="ae kz" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Heffernan%2C+K" rel="noopener ugc nofollow" target="_blank">凯文·赫夫南</a>，<a class="ae kz" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Damaskinos%2C+G" rel="noopener ugc nofollow" target="_blank">圣乔治·达玛斯基诺</a>，<a class="ae kz" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Poddar%2C+S" rel="noopener ugc nofollow" target="_blank">什瓦尼·波德达尔</a>，<a class="ae kz" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Modi%2C+S" rel="noopener ugc nofollow" target="_blank">舒巴姆·莫迪</a>，<a class="ae kz" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cali%2C+J" rel="noopener ugc nofollow" target="_blank">雅克·卡利</a></p></blockquote></div><div class="ab cl ns nt hx nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="im in io ip iq"><p id="3276" class="pw-post-body-paragraph la lb it lc b ld le jv lf lg lh jy li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我希望你喜欢读这篇文章。如果你愿意支持我成为一名作家，可以考虑注册<a class="ae kz" href="https://prakhar-mishra.medium.com/membership" rel="noopener">成为一名媒体成员</a>。每月只需5美元，你就可以无限制地使用Medium。<em class="lw"> </em>谢谢！</p></div></div>    
</body>
</html>