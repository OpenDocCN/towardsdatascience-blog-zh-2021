<html>
<head>
<title>Non-Deep Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">非深度网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/non-deep-networks-b0b80c65c7c6?source=collection_archive---------23-----------------------#2021-11-22">https://towardsdatascience.com/non-deep-networks-b0b80c65c7c6?source=collection_archive---------23-----------------------#2021-11-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="cfec" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">越少越新吗？</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/98e835adac456ab74c518b877dbb07b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YECeOxlko9KoOJNw8RNm3A.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">艾莉娜·格鲁布尼亚克在<a class="ae kv" href="https://unsplash.com/s/photos/sparse-connections-network?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="cb26" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">介绍</h1><p id="2826" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi mk translated">一个是更深层次的网络用概念、网络、DenseNets等来统治ML空间的时代。现在甚至被巨大的变形金刚模型所取代。这个行业总是朝着越多越好或越深越好的方向发展，它确实发挥了作用，并产生了突破性的结果，但代价是——巨大的计算成本，更大的内存需求，以及最重要的"<em class="mt">碳足迹</em>"通过将巨大的模型训练几天来一起。<strong class="lq ir">非深度网络</strong>(也称为<strong class="lq ir"> ParNet </strong> ) [1]以80%的顶级准确性、96 %的CIFAR-10和81%的仅12层CIFAR-100的惊人性能获得了良好的测量结果！这是一个相当大的壮举，促使我写这篇论文综述。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/1f6beee93b5278ce15d0e90a697268f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*hoZcmPmTl6CmWPaF0KLhzA.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">描述ImageNet性能指标评测的精度与深度性能的图表和绿点令人印象深刻。图片鸣谢——非深度网络<a class="ae kv" href="https://arxiv.org/pdf/2110.07641.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><h1 id="a9c6" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">背景</h1><p id="34cf" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">曾经有几十层被认为是深的日子，现在已经增长到100倍，达到1000层深的网络。当然，准确性和性能伴随着延迟和并行化方面的缺点。我注意到的另一个方面是，现代模型很少是可复制的，因为这些模型的建造成本和规模。我们已经看到，随着SOTA模型的平均深度不断增加，这些架构在ImageNet基准测试中占据了主导地位。事情肯定开始改变了。</p><blockquote class="mv"><p id="9c28" class="mw mx iq bd my mz na nb nc nd ne mj dk translated">ParNet的性能与著名的12层视觉变压器相当！</p></blockquote><p id="3f2b" class="pw-post-body-paragraph lo lp iq lq b lr nf jr lt lu ng ju lw lx nh lz ma mb ni md me mf nj mh mi mj ij bi translated">我们还看到了1989年的一个经典作品，其中网络只是一个单层，具有非常接近函数的sigmoid激活。这里，当缩放时，宽度增加，导致参数增加。另一种方法是网络的深度，它总是更好地逼近函数，而没有参数的巨大增加，并且总是超过“不太”深的网络(即使参数数量也相似)。缩放神经网络通常涉及增加深度、分辨率和宽度。相反，ParNets的作者选择了并行子体系结构作为他们反对传统方法的方法。</p><h1 id="ad6b" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">理念与建筑</h1><p id="aadc" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">直觉是保持12层不变，并在一定程度上使用并行化，他们称该架构为“令人尴尬的并行”。如果我们看一下下面描述的架构，我们会看到一些<em class="mt">流</em>(分支)，其块或多或少类似于VGG模型[2]。这些块被称为ParNet块，其中内部正在发生一些(简单的)事情。他们选择VGG风格是因为它的能力——结构重新参数化。多个3×3卷积分支可以合并成单个3×3卷积，这可以有效地减少推断时间。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/0ef436c764de3b78e98e5981a9f5ba62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aGJOVZwv_FHm_Pwim-Y3MA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">ParNet架构和ParNet块。图片鸣谢——非深度网络<a class="ae kv" href="https://arxiv.org/pdf/2110.07641.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><p id="04b7" class="pw-post-body-paragraph lo lp iq lq b lr nl jr lt lu nm ju lw lx nn lz ma mb no md me mf np mh mi mj ij bi translated">每个ParNet块由3个主要成分组成，然后在下一个块之前合并/融合这些成分，</p><ol class=""><li id="d395" class="nq nr iq lq b lr nl lu nm lx ns mb nt mf nu mj nv nw nx ny bi translated">1x1卷积</li><li id="7b48" class="nq nr iq lq b lr nz lu oa lx ob mb oc mf od mj nv nw nx ny bi translated">3×3卷积和</li><li id="a3f0" class="nq nr iq lq b lr nz lu oa lx ob mb oc mf od mj nv nw nx ny bi translated">SSE ( <strong class="lq ir">跳跃挤压激发</strong>)层也称为RepVGG-SSE区块</li></ol><p id="3ec9" class="pw-post-body-paragraph lo lp iq lq b lr nl jr lt lu nm ju lw lx nn lz ma mb no md me mf np mh mi mj ij bi translated">上图的最右边部分描绘了跳跃-挤压-激发块。它实际上是在不增加深度的情况下增加了感受野，这与传统的挤压激励[3]实施方式相反。为了在浅层网络中引入比ReLU激活更多的非线性，作者选择使用更近的路斯[4]。</p><p id="1065" class="pw-post-body-paragraph lo lp iq lq b lr nl jr lt lu nm ju lw lx nn lz ma mb no md me mf np mh mi mj ij bi translated">接下来是下采样和融合模块，它们分别降低分辨率并组合来自多个流的信息。下采样导致宽度增加，这有利于多尺度处理。这个模块非常简单，有一个挤压激励(SE)层和一个连接到1x1卷积分支的平均池。除了额外的级联层之外，融合与下采样没有什么不同。</p><p id="d936" class="pw-post-body-paragraph lo lp iq lq b lr nl jr lt lu nm ju lw lx nn lz ma mb no md me mf np mh mi mj ij bi translated">为了扩展网络，根据数据集(CIFAR-10、CIFAR-100和ImageNet)对流的宽度、分辨率和数量进行了实验。</p><h1 id="2e93" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">结果和基准</h1><p id="70f4" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">该模型使用SGD优化器在ImageNet上训练了120个时期，该优化器具有几个学习率调度器和批量大小为2048(相当大)的衰减值。如果批量大小不适合内存，学习速率会与批量大小成比例降低。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/abb0a83fdd282a0e778a996fdc20dead.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*U9UllfByqRBA8G1skV0KxQ.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">ParNet vs ResNet，只强调最高的准确性和速度。图片鸣谢——非深度网络<a class="ae kv" href="https://arxiv.org/pdf/2110.07641.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><p id="5d29" class="pw-post-body-paragraph lo lp iq lq b lr nl jr lt lu nm ju lw lx nn lz ma mb no md me mf np mh mi mj ij bi translated">ParNet的结果与ResNets的结果相当或更好(尽管由于ParNet的更大和超大比例版本，参数的数量更多)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/bfe9043c99789f9f796289861618eba7.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*kkkZjx6ju2N7S6Qnm7Gf5Q.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">ParNet与ResNet，重点是深度与精度指标。图片鸣谢——非深度网络<a class="ae kv" href="https://arxiv.org/pdf/2110.07641.pdf" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><p id="fdeb" class="pw-post-body-paragraph lo lp iq lq b lr nl jr lt lu nm ju lw lx nn lz ma mb no md me mf np mh mi mj ij bi translated">这表明，仅用12层，基础ParNet就可以在top-1和top-5精度方面胜过在ParNet中使用相同程序和增强的重新训练的ResNet-34。此外，ParNet-XL的性能优于原始的ResNet-瓶颈101。</p><p id="7e4a" class="pw-post-body-paragraph lo lp iq lq b lr nl jr lt lu nm ju lw lx nn lz ma mb no md me mf np mh mi mj ij bi translated">本文中有一个单独的部分专门讨论并行性及其优势，请阅读该部分以了解更多详细信息。我将把消融研究部分留给您来探索和理解性能是如何提升的。此外，还有关于针对CIFAR-10和CIFAR-100数据集的ResNets、ParNets、DenseNets等的各种结果的详细信息。</p><p id="156f" class="pw-post-body-paragraph lo lp iq lq b lr nl jr lt lu nm ju lw lx nn lz ma mb no md me mf np mh mi mj ij bi translated">作者很想测试ParNet作为对象检测网络的主干，它会提高现有的性能吗？当ParNet在YOLOv4 [7]中取代Darknet53 [6]时，在准确性和延迟方面看起来确实如此。</p><h1 id="6d87" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">我的想法</h1><p id="7692" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">ParNet可以在不需要大规模基础设施的情况下实现(并且结果是可重复的),这是一个受欢迎的变化。它为研究人员开辟了探索这种浅深度并行架构空间的途径，并带来了可能适用于边缘部署场景而不会对性能造成重大影响的强大网络。</p><p id="0f62" class="pw-post-body-paragraph lo lp iq lq b lr nl jr lt lu nm ju lw lx nn lz ma mb no md me mf np mh mi mj ij bi translated">由于它在图像分类和物体检测方面都显示出了前景，我非常渴望看到它在转移到医疗保健等敏感领域时的表现。它能保持相当的性能吗？只有时间能证明一切。</p><h1 id="e744" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">结论</h1><p id="adde" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">帕内特向我们展示了逆势而行是有回报的。由于深度很浅，当爬上ImageNet分类基准测试的排行榜时，它毫不费力[8]。这是第一次一个网络能够出色地在三个著名的数据集上运行——只有12层的CIFAR-10、CIFAR-100 [9]和ImageNet。ParNet的性能随着流、分辨率和宽度的增加而增加，而深度保持不变。作者还观察到，目前的性能尚未饱和，可以进一步扩展。</p><h1 id="c16b" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">参考</h1><p id="ee5c" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">[1]非深度网络:<a class="ae kv" href="https://arxiv.org/pdf/2110.07641.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2110.07641.pdf</a></p><p id="c74d" class="pw-post-body-paragraph lo lp iq lq b lr nl jr lt lu nm ju lw lx nn lz ma mb no md me mf np mh mi mj ij bi translated">[2]https://arxiv.org/pdf/1409.1556.pdf<a class="ae kv" href="https://arxiv.org/pdf/1409.1556.pdf" rel="noopener ugc nofollow" target="_blank">VGG网络</a></p><p id="1eb7" class="pw-post-body-paragraph lo lp iq lq b lr nl jr lt lu nm ju lw lx nn lz ma mb no md me mf np mh mi mj ij bi translated">[3]压缩-激发网络:【https://arxiv.org/pdf/1709.01507.pdf T4】</p><p id="c16a" class="pw-post-body-paragraph lo lp iq lq b lr nl jr lt lu nm ju lw lx nn lz ma mb no md me mf np mh mi mj ij bi translated">[4]https://arxiv.org/pdf/1702.03118.pdf激活:<a class="ae kv" href="https://arxiv.org/pdf/1702.03118.pdf" rel="noopener ugc nofollow" target="_blank">路斯</a></p><p id="6961" class="pw-post-body-paragraph lo lp iq lq b lr nl jr lt lu nm ju lw lx nn lz ma mb no md me mf np mh mi mj ij bi translated">[5]丹塞尼特:<a class="ae kv" href="https://arxiv.org/pdf/1608.06993v5.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1608.06993v5.pdf</a></p><p id="91ea" class="pw-post-body-paragraph lo lp iq lq b lr nl jr lt lu nm ju lw lx nn lz ma mb no md me mf np mh mi mj ij bi translated">[6]约洛夫3中的暗网53:<a class="ae kv" href="https://arxiv.org/pdf/1804.02767v1.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1804.02767v1.pdf</a></p><p id="e4f9" class="pw-post-body-paragraph lo lp iq lq b lr nl jr lt lu nm ju lw lx nn lz ma mb no md me mf np mh mi mj ij bi translated">【7】约洛夫4:【https://arxiv.org/pdf/2004.10934v1.pdf】T2</p><p id="05d6" class="pw-post-body-paragraph lo lp iq lq b lr nl jr lt lu nm ju lw lx nn lz ma mb no md me mf np mh mi mj ij bi translated">[8] ImageNet基准:<a class="ae kv" href="https://paperswithcode.com/sota/image-classification-on-imagenet" rel="noopener ugc nofollow" target="_blank">https://papers with code . com/sota/image-class ification-on-ImageNet</a></p><p id="cfe7" class="pw-post-body-paragraph lo lp iq lq b lr nl jr lt lu nm ju lw lx nn lz ma mb no md me mf np mh mi mj ij bi translated">[9] CIFAR-10和CIFAR-100数据集:<a class="ae kv" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank">https://www.cs.toronto.edu/~kriz/cifar.html</a></p></div></div>    
</body>
</html>