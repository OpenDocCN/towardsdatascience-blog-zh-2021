<html>
<head>
<title>Building a Sentiment Analysis Model using Yelp Reviews and ML Ensemble Methods</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Yelp评论和ML集成方法构建情感分析模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-sentiment-analysis-model-using-yelp-reviews-and-ml-ensemble-methods-80e45db6d0c7?source=collection_archive---------4-----------------------#2021-10-03">https://towardsdatascience.com/building-a-sentiment-analysis-model-using-yelp-reviews-and-ml-ensemble-methods-80e45db6d0c7?source=collection_archive---------4-----------------------#2021-10-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6c90" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><strong class="ak">三星级评审和四星级评审的实际区别是什么？自然语言处理有答案！</strong></h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5a380a0cb9412b6c95a75857d82cd8d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LNvtf0UCCSKsUgYieLKD_g.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由<a class="ae ky" href="https://unsplash.com/@abillion" rel="noopener ugc nofollow" target="_blank">亿万富翁</a>在<a class="ae ky" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="ea92" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你曾经通读过Yelp评论，你可能会注意到评论者之间有很多不一致的地方。一些评论说这种食物“改变生活”，但只留下三颗星，而另一些评论说这种食物“还行”，只留下五颗星。五星评估系统是有缺陷的，因为没有一个明确的方法来定义五星和四星。然而，如果我们忽略明星的数量，而是通读实际的评论，我们应该更好地了解Yelp评论者实际上在说什么。这听起来像是更多的工作，但是通过使用Python的自然语言工具包，我们可以让我们的代码做所有的艰苦工作。</p><p id="a33d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第1部分:获取Yelp评论</strong></p><p id="5fdd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以用几种不同的方法来做这件事。</p><ul class=""><li id="f12c" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">很少有Kaggle数据集像<a class="ae ky" href="https://www.kaggle.com/yelp-dataset/yelp-dataset?select=yelp_academic_dataset_business.json" rel="noopener ugc nofollow" target="_blank">这个</a>一样有大量内容，但可能没有最新的评论。</li><li id="c977" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">还有一些其他的独立数据集，比如来自拥抱脸的<a class="ae ky" href="https://huggingface.co/datasets/yelp_review_full" rel="noopener ugc nofollow" target="_blank">这个</a>。人们使用Yelp评论进行分析已经很多年了，有很多这样的资源可以帮助你开始。</li><li id="2959" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">在我看来，最好的选择是直接从源头获取评论。Yelp有一个<a class="ae ky" href="https://www.yelp.com/dataset" rel="noopener ugc nofollow" target="_blank">公共数据集，包含超过800万条评论</a>，全部存储在一个JSON文件中。你应该能够在一个简单的格式中找到各种各样的评论，旧的和新的。</li></ul><p id="55ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第二部分:清理和准备情绪分析</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mj"><img src="../Images/a902f3785dd5619c7cc92fcd0656d01f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uwAYLeUtVgUqWqGFbIgrbQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="7bab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当处理基于文本的数据时，减少单词和字符的数量是有益的，这样你的代码就不用花几个小时来运行。我们希望这样做的同时仍然保留重要的词，这些词的意思是“好”或“坏”，让我们了解评论者的感受。像‘the’，‘or’和‘is’这样的词，也被称为<a class="ae ky" href="https://en.wikipedia.org/wiki/Stop_word" rel="noopener ugc nofollow" target="_blank">停用词</a>，并不是很重要。没有理由浪费时间和处理能力去处理这些停用词，所以最好是删除它们。Python的自然语言工具包使这一过程变得简单:</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="bcef" class="mp mq it ml b gy mr ms l mt mu"><strong class="ml iu">from</strong> <strong class="ml iu">nltk.corpus</strong> <strong class="ml iu">import</strong> stopwords<br/><strong class="ml iu">from</strong> <strong class="ml iu">nltk.tokenize</strong> <strong class="ml iu">import</strong> word_tokenize<br/><br/><br/>example_sent = """This is a sample sentence,<br/>                  showing off the stop words filtration."""<br/><br/>stop_words = set(stopwords.words('english'))<br/> <br/>word_tokens = word_tokenize(example_sent)<br/> <br/>filtered_sentence = [w <strong class="ml iu">for</strong> w <strong class="ml iu">in</strong> word_tokens <strong class="ml iu">if</strong> <strong class="ml iu">not</strong> w.lower() <strong class="ml iu">in</strong> stop_words]<br/> <br/>filtered_sentence = []<br/> <br/><strong class="ml iu">for</strong> w <strong class="ml iu">in</strong> word_tokens:<br/>    <strong class="ml iu">if</strong> w <strong class="ml iu">not</strong> <strong class="ml iu">in</strong> stop_words:<br/>        filtered_sentence.append(w)<br/><br/>print(word_tokens)<br/>print(filtered_sentence)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/c5a476d20a71cd460e46c529af05144b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YLjU9JN7K6Gxf7aLpG6qBA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="ba2b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你还会注意到我在一些变量中使用了单词“token”。这意味着我把一个句子分解成一系列单词，这个过程被称为标记化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/67265e4ce4b9c091ac564b71ad72a519.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r6hFrTyvt7p7e7QlFnc-8w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片bt作者</p></figure><p id="0438" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在这里看到Yelp评论平均有40-45%的停用词。摆脱他们会节省我们很多时间！</p><p id="0623" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们已经过滤掉了停用词，让我们尝试使用一个叫做<a class="ae ky" href="https://en.wikipedia.org/wiki/Stemming" rel="noopener ugc nofollow" target="_blank">词干</a>的过程来简化数据集。词干化是将所有形式的单词还原为基本形式的过程。如果你有“非常”、“最棒”和“更棒”这样的词，这些词都会被简化为“很棒”，以便于分组和处理。Python的自然语言工具包又一次让这个过程变得非常简单:</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="db13" class="mp mq it ml b gy mr ms l mt mu"><strong class="ml iu">import</strong> <strong class="ml iu">nltk</strong><br/><strong class="ml iu">from</strong> <strong class="ml iu">nltk.stem.snowball</strong> <strong class="ml iu">import</strong> SnowballStemmer<br/><br/>snowBallStemmer = SnowballStemmer("english")<br/><br/>sentence = yelp_data_unpacked[40]<br/>wordList = nltk.word_tokenize(sentence)<br/><br/>stemWords = [snowBallStemmer.stem(word) <strong class="ml iu">for</strong> word <strong class="ml iu">in</strong> wordList]<br/><br/>stemmed = ' '.join(stemWords)<br/>print(sentence)<br/>print('')<br/>print(stemmed)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mx"><img src="../Images/1978440dad8e1e0beeff024f9ca63812.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*uBvF9sfKk7eYZmlaYQFZ_A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="7b84" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看起来变化不大，但是如果我们将这些句子标记化并组合在一起，那么唯一的键值对将会更少。</p><p id="f211" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在项目的这一点上，我们已经清理了数据并准备好进行处理。在我们开始建立分类模型之前，我们必须决定如何对它进行分类。</p><p id="45ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第3部分:分类和超参数</strong></p><p id="2a1b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文的开头，我说过我想通过分析每个评论中的实际内容来改进Yelp的五星评级系统。因此，我不是根据星级将评论分为五组，而是根据情绪将它们分为四组。这些组是“阴性”、“轻微阴性”、“轻微阳性”和“阳性”。为了整理我对训练数据集的评论，我使用了<a class="ae ky" href="https://planspace.org/20150607-textblob_sentiment/" rel="noopener ugc nofollow" target="_blank"> Textblob的极性得分</a>作为基线。</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="8faf" class="mp mq it ml b gy mr ms l mt mu"><strong class="ml iu">from</strong> <strong class="ml iu">textblob</strong> <strong class="ml iu">import</strong> TextBlob</span><span id="006d" class="mp mq it ml b gy my ms l mt mu">yelp_data_s['Polarity'] =<br/>yelp_data_s['Stemmed'].apply(<strong class="ml iu">lambda</strong> x: TextBlob(x).sentiment[0])</span><span id="0ea4" class="mp mq it ml b gy my ms l mt mu">sentiment = []<br/><strong class="ml iu">for</strong> i <strong class="ml iu">in</strong> range(len(yelp_data_s)):<br/>    <strong class="ml iu">if</strong> yelp_data_s['Polarity'][i] &gt;= 0.4:<br/>        sentiment.append('Positive')<br/>    <strong class="ml iu">if</strong> yelp_data_s['Polarity'][i] &gt; 0.2 <strong class="ml iu">and</strong> yelp_data_s['Polarity'][i] &lt; 0.4:<br/>        sentiment.append('Slightly Positive')<br/>    <strong class="ml iu">if</strong> yelp_data_s['Polarity'][i] &lt;= 0.2 <strong class="ml iu">and</strong> yelp_data_s['Polarity'][i] &gt; 0:<br/>        sentiment.append('Slightly Negative')<br/>    <strong class="ml iu">if</strong> yelp_data_s['Polarity'][i] &lt; 0:<br/>        sentiment.append('Negative')</span><span id="385e" class="mp mq it ml b gy my ms l mt mu">yelp_data_s['sentiment'] = sentiment</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/c8ab3c66989fa4a7a16e7898ca8a5b28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*o61-ZRmR0Le6TSXv5MA5vA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="5173" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不过，我想不仅仅使用这个分数来构建我们的模型，所以我决定看看其他一些特性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/d272ac91888285540952a5538e5ab38e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-aDJjC0wv7CkVyXu"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="e462" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于第二个图表，我使用<a class="ae ky" href="https://gist.github.com/mkulakowski2/4289437" rel="noopener ugc nofollow" target="_blank">这个正面情绪词列表</a>来确定什么是“正面词”。</p><p id="6b32" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我有了一个好主意，知道在构建模型时应该寻找什么样的特征。实际选择模型之前的最后一步是通过使用<a class="ae ky" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf#:~:text=In%20information%20retrieval%2C%20tf%E2%80%93idf,in%20a%20collection%20or%20corpus.&amp;text=tf%E2%80%93idf%20is%20one%20of,popular%20term%2Dweighting%20schemes%20today." rel="noopener ugc nofollow" target="_blank">术语频率逆文档频率</a>矩阵(TFIDF)来查看每个评论中的内容</p><p id="d39f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简单来说，这是一种在评论中寻找最独特、最有冲击力的词的方法。术语频率只是告诉您该单词出现的频率，但是一旦添加了逆文档频率参数，您还可以分析该单词在整个数据集中出现的频率。分析这些独特的词很重要，因为它们比每个评论者使用的常用词能告诉你更多。</p><p id="5d7e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了创建训练数据集，我将这些列用作我的列:</p><ul class=""><li id="8f38" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">名字:只是餐馆的名字</li><li id="7bfc" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">评论:Yelp上写的完整评论</li><li id="6b26" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">Polarity:text blob极性得分</li><li id="ce39" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">正面单词P:正面单词的百分比</li><li id="6444" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">情感:指定的情感分数</li><li id="1b81" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">TFIDF:每个唯一的单词是一个列，其频率作为其值列出，在这个数据集中会有很多列(这就是为什么我们首先删除停用词并阻止我们的评论)</li></ul><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="e1c0" class="mp mq it ml b gy mr ms l mt mu"><strong class="ml iu">from</strong> <strong class="ml iu">sklearn.feature_extraction.text</strong> <strong class="ml iu">import</strong> TfidfVectorizer</span><span id="39d8" class="mp mq it ml b gy my ms l mt mu">text = yelp_data_s['Stemmed:Sentence']<br/>Tvectorizer = TfidfVectorizer()<br/>Tvectorizer.fit(text)<br/>Tvector = Tvectorizer.transform(text)</span><span id="4a1b" class="mp mq it ml b gy my ms l mt mu">voc = Tvectorizer.vocabulary_<br/>df = pd.DataFrame(Tvector.toarray(),columns=voc)<br/><br/>col_name1 ='Name'<br/>col_name2 = 'Review'<br/>col_name3 = 'Polarity'<br/>col_name4 = 'Sentiment'<br/>col_name5 = 'Positive_Words_P'<br/><br/>col1 = yelp_data_s['Name']<br/>col2 = yelp_data_s['Review']<br/>col3 = yelp_data_s['Polarity']<br/>col4 = yelp_data_s['sentiment']<br/>col5 = yelp_data_s['Positive_Words_P']<br/><br/>df.insert(0,col_name1,col1)<br/>df.insert(1,col_name2,col2)<br/>df.insert(2,col_name3,col3)<br/>df.insert(3,col_name4,col4)<br/>df.insert(4,col_name5,col5)</span></pre><p id="5411" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用这个训练数据集和一个简单的逻辑回归模型来测试它，我得到了67%准确率的基线分数。我知道我可以通过调整一些超参数来改善这一点，所以我再次查看了TFIDF矩阵。</p><ul class=""><li id="5350" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">最小词频:</strong>虽然查看真正独特的单词很重要，但它可能有助于过度拟合，以提高我们正在处理的单词的阈值。我测试了同一个模型，但是增加了最小词频的超参数，看看它是如何影响准确度的。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi na"><img src="../Images/ad7cf34dbe7d6f3e2d719ba2403c46f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*k58VsHGvbej8K4RRq6ikwg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/e1a74af9115f6a13366794fba13b0384.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/1*iPbnP1iXUu5Bx9N1yKErnw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><ul class=""><li id="4ecd" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">n grams总数:</strong>这基本上告诉我们数据是如何被一个字一个字地处理的。如果您将ngrams设置为1，则单词将会像“this”、“is”、“not”、“lame”等那样逐一处理。如果您将ngrams设置为2，您可以处理单词对，例如“this is”、“is not”、“not lame”。您可能会看到这将如何影响准确性，当ngrams设置为1时，模型只会看到“蹩脚”并将其作为负值处理，而当ngrams设置为2时，模型会看到“不蹩脚”并将其作为正值处理。这反映在这张精度图上:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/8682926200f46e6beb2769dd1cbef105.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*Gbx0yeaLJAtCnnGwH_36yA.png"/></div></figure><p id="fb02" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有了这两个超参数，我们将基线准确率提高到了71%！这可能看起来不多，但当我们开始使用其他模型时，知道我们有最佳的超参数是很重要的。</p><p id="a262" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第四部分:选型</strong></p><p id="7ece" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，是选择模型的时间。对于这个项目，我想尝试许多不同的分类模型，整个过程如下所示:</p><ul class=""><li id="d305" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">选择模型类型</li><li id="afaa" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">定义一个管道，使用Gridsearch找到最佳参数</li><li id="21ae" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">保存模型，这样我们就不必多次运行它</li><li id="c517" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">加载模型并运行准确度和f1分数</li><li id="7d43" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">对其他模型重复该过程</li></ul><p id="733a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我还想将我的模型选择分成两部分，前四部分将分析数字成分(阳性词百分比、极性得分等)。)并且最后两个将分析TFIDF矩阵。以下是每个型号的表现:</p><ul class=""><li id="7740" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu"> Logistic回归:</strong> F1值为71%，lr__C: 0.1</li></ul><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="2e4d" class="mp mq it ml b gy mr ms l mt mu"><strong class="ml iu">from</strong> <strong class="ml iu">sklearn.linear_model</strong> <strong class="ml iu">import</strong> LogisticRegression<br/><strong class="ml iu">from</strong> <strong class="ml iu">sklearn.model_selection</strong> <strong class="ml iu">import</strong> GridSearchCV<br/><strong class="ml iu">from</strong> <strong class="ml iu">sklearn.metrics</strong> <strong class="ml iu">import</strong> make_scorer, accuracy_score, f1_score<br/><strong class="ml iu">from</strong> <strong class="ml iu">sklearn.model_selection</strong> <strong class="ml iu">import</strong> train_test_split<br/><strong class="ml iu">from</strong> <strong class="ml iu">sklearn.pipeline</strong> <strong class="ml iu">import</strong> Pipeline<br/><strong class="ml iu">from</strong> <strong class="ml iu">sklearn.preprocessing</strong> <strong class="ml iu">import</strong> StandardScaler</span><span id="96f4" class="mp mq it ml b gy my ms l mt mu">steps = [('scaler', StandardScaler()), ('lr', LogisticRegression(solver = 'lbfgs'))] <br/>pipeline = Pipeline(steps)<br/>parameters = {'lr__C':[0.01, 0.1, 1, 10, 100]}<br/><br/>clf = GridSearchCV(pipeline, parameters, cv = 10, scoring="accuracy") <br/>clf.fit(X_train, y_train)<br/>clf.best_params_</span></pre><ul class=""><li id="039f" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">随机森林:</strong> F1值为66%，最大特征:“sqrt”，n估计值:50</li></ul><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="ac6a" class="mp mq it ml b gy mr ms l mt mu"><strong class="ml iu">from</strong> <strong class="ml iu">sklearn.ensemble</strong> <strong class="ml iu">import</strong> RandomForestClassifier</span><span id="ea81" class="mp mq it ml b gy my ms l mt mu">steps = [('scaler', StandardScaler()), ('rf', RandomForestClassifier())] <br/>pipeline = Pipeline(steps)</span><span id="e3f0" class="mp mq it ml b gy my ms l mt mu">parameters = {'rf__n_estimators':[10 , 20, 30, 40, 50], 'rf__max_features':['auto','sqrt']}<br/>clf = GridSearchCV(pipeline, parameters, cv = 10, scoring="accuracy") <br/>clf.fit(X_train, y_train)<br/>clf.best_params_</span></pre><ul class=""><li id="d60d" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">支持向量分类(SVC): </strong> F1得分为70%，svc__C: 0.01</li></ul><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="644d" class="mp mq it ml b gy mr ms l mt mu"><strong class="ml iu">from</strong> <strong class="ml iu">sklearn.svm</strong> <strong class="ml iu">import</strong> SVC<br/><br/>steps = [('scaler', StandardScaler()), ('svc', SVC(probability=<strong class="ml iu">False</strong>,kernel='linear',gamma='auto'))] <br/>pipeline = Pipeline(steps) </span><span id="2808" class="mp mq it ml b gy my ms l mt mu">parameters = {'svc__C':[0.01, 0.1, 1]}<br/>clf = GridSearchCV(pipeline, parameters, cv = 3, scoring="accuracy") <br/>clf.fit(X_train, y_train)<br/><br/>clf.best_params_</span></pre><ul class=""><li id="9ed6" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">梯度增强分类器:</strong> F1值为72%，学习率为0.15，n估计量为500</li></ul><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="90ed" class="mp mq it ml b gy mr ms l mt mu"><strong class="ml iu">from</strong> <strong class="ml iu">sklearn.ensemble</strong> <strong class="ml iu">import</strong> GradientBoostingClassifier<br/><br/>steps = [('scaler', StandardScaler()), ('gbc', GradientBoostingClassifier(max_features='sqrt'))] <br/>pipeline = Pipeline(steps) </span><span id="584d" class="mp mq it ml b gy my ms l mt mu">parameters = {'gbc__n_estimators':[10, 50, 100, 200, 500], 'gbc__learning_rate': [0.05, 0.1, 0.15, 0.2, 0.25]}<br/>clf = GridSearchCV(pipeline, parameters, cv = 10, scoring="accuracy") <br/>clf.fit(X_train, y_train)<br/><br/><br/>steps = [('scaler', StandardScaler()), ('gbc', GradientBoostingClassifier(learning_rate = 0.15, max_features = 'sqrt', n_estimators = 500))]  clf = Pipeline(steps)  clf.fit(X_train, y_train)</span></pre><ul class=""><li id="712b" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">带TFIDF的朴素贝叶斯:</strong> F1分数为64%，alpha: 1，min_df: 10</li></ul><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="e7fe" class="mp mq it ml b gy mr ms l mt mu"><strong class="ml iu">from</strong> <strong class="ml iu">sklearn.feature_extraction.text</strong> <strong class="ml iu">import</strong> TfidfVectorizer<br/><strong class="ml iu">from</strong> <strong class="ml iu">sklearn.naive_bayes</strong> <strong class="ml iu">import</strong> MultinomialNB<br/><br/>steps = [('vec', TfidfVectorizer(stop_words = 'english', ngram_range = (1, 2))), ('nb', MultinomialNB())] <br/>pipeline = Pipeline(steps) </span><span id="5fe8" class="mp mq it ml b gy my ms l mt mu">parameters = {'vec__min_df':[0.01, 0.1, 1, 10, 100], 'nb__alpha':[0.01, 0.1, 1, 10, 100]}<br/>clf = GridSearchCV(pipeline, parameters, cv = 10, scoring="accuracy") <br/>clf.fit(X_train, y_train)<br/><br/>clf.best_params_</span></pre><ul class=""><li id="f083" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">【TFIDF梯度增强分类器: F1值为57%，学习率为0.25</li></ul><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="7ade" class="mp mq it ml b gy mr ms l mt mu">steps = [('vec', TfidfVectorizer(stop_words = 'english', ngram_range = (1, 2))), <br/>         ('gbc', GradientBoostingClassifier(max_features='sqrt',n_estimators=500))] <br/>pipeline = Pipeline(steps) </span><span id="0193" class="mp mq it ml b gy my ms l mt mu">parameters = {'gbc__learning_rate': [0.05, 0.1, 0.15, 0.2, 0.25]}<br/>clf = GridSearchCV(pipeline, parameters, cv = 3, scoring="accuracy") <br/>clf.fit(X_train, y_train)<br/><br/>clf.best_params_</span></pre><p id="b516" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不幸的是，这些模型的分数都在60-70%之间，不是最好的。有一种方法可以从最好的模型中提取性能并组合它们来改善结果，这就是所谓的<a class="ae ky" href="https://www.toptal.com/machine-learning/ensemble-methods-machine-learning#:~:text=Ensemble%20methods%20are%20techniques%20that,winning%20solutions%20used%20ensemble%20methods." rel="noopener ugc nofollow" target="_blank">集成方法</a>。对于这个项目，我采用了性能最好的基于文本的模型(带TFIDF的朴素贝叶斯)及其性能作为性能最好的数值模型(梯度增强分类器)的一个特征。这部分有很多步骤，但代码看起来像这样:</p><pre class="kj kk kl km gt mk ml mm mn aw mo bi"><span id="0dc3" class="mp mq it ml b gy mr ms l mt mu"># define train/test split </span><span id="c769" class="mp mq it ml b gy my ms l mt mu">X = yelp_data.Review # word tokens<br/>y = yelp_data.Sentiment # assigned sentiment<br/>indices = yelp_data.index<br/><br/>X_train, X_test, y_train, y_test, i_train, i_test = train_test_split(X, y, indices, train_size = 0.8, random_state = 7)</span><span id="0793" class="mp mq it ml b gy my ms l mt mu"># define Naive Bayes TFIDF model</span><span id="9da9" class="mp mq it ml b gy my ms l mt mu">steps = [('vec', TfidfVectorizer(ngram_range = (1, 2))), ('nb', MultinomialNB())] <br/>pipeline = Pipeline(steps)<br/>parameters = {'vec__min_df':10, 'nb__alpha':0.1}<br/>clf = GridSearchCV(pipeline, parameters, cv = 10, scoring="accuracy") <br/>clf.fit(X_train, y_train)</span><span id="09a8" class="mp mq it ml b gy my ms l mt mu"># turn model scores into feature in dataframe (prediction probability)</span><span id="1d8a" class="mp mq it ml b gy my ms l mt mu">Xtrain_proba = pd.DataFrame(clf.predict_proba(X_train), index = i_train) Xtest_proba = pd.DataFrame(clf.predict_proba(X_test), index = i_test)</span><span id="f500" class="mp mq it ml b gy my ms l mt mu"># define new train/test split</span><span id="d84e" class="mp mq it ml b gy my ms l mt mu">X = yelp_data.iloc[0:,4:] # first four columns are text, review, positive word percentage, and polarity<br/>y = yelp_data.Sentiment # assigned sentiment<br/>indices = yelp_data.index<br/><br/>X_train, X_test, y_train, y_test, itrain, itest = train_test_split(X,y,indices,train_size=0.8,random_state=7)</span><span id="4ad1" class="mp mq it ml b gy my ms l mt mu"># Adding prediction probability</span><span id="cdd8" class="mp mq it ml b gy my ms l mt mu">Xtrain_combined = pd.merge(X_train, Xtrain_proba, left_index=<strong class="ml iu">True</strong>, right_index=<strong class="ml iu">True</strong>) Xtest_combined = pd.merge(X_test, Xtest_proba, left_index=<strong class="ml iu">True</strong>, right_index=<strong class="ml iu">True</strong>)</span><span id="4e1d" class="mp mq it ml b gy my ms l mt mu"># define Gradient Boosted Classifier</span><span id="2454" class="mp mq it ml b gy my ms l mt mu">steps = [('scaler', StandardScaler()), ('gbc', GradientBoostingClassifier(max_features='sqrt'))] <br/>pipeline = Pipeline(steps) </span><span id="518d" class="mp mq it ml b gy my ms l mt mu">parameters = {'gbc__n_estimators':[10, 50, 100, 200, 500], 'gbc__learning_rate': [0.05, 0.1, 0.15, 0.2, 0.25]}<br/>clf = GridSearchCV(pipeline, parameters, cv = 10, scoring="accuracy") <br/>clf.fit(X_train, y_train)</span><span id="6196" class="mp mq it ml b gy my ms l mt mu"># with best features</span><span id="7093" class="mp mq it ml b gy my ms l mt mu">steps = [('scaler', StandardScaler()), ('gbc', GradientBoostingClassifier(learning_rate = 0.2, max_features = 'sqrt', n_estimators = 500))] <br/>clf = Pipeline(steps) <br/>clf.fit(X_train, y_train)</span></pre><p id="a30e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，通过这种堆叠模式，我们的F1得分提高了92%！为了向您展示这有多准确，让我们来看看原始逻辑回归的混淆矩阵(准确率为71%):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/d4570e692e71ad2bf504b81118ee3bf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*w-w1jgwdnC6XldQlXEKAbQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="443e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对比我们的堆叠模型的混淆矩阵(准确率92%):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/a9ddfc98bb7483e10cc59629aea974f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1330/format:webp/1*8KT8d2MQfAcZ-UpGORtnSQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="1239" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如你所见，集合方法对我们的准确性有巨大的影响。定义所有这些模型并将两者结合在一起可能需要一些额外的时间，但对于21%的准确性提升，我认为这是值得的。我强烈推荐阅读更多关于集成方法的内容，这对你从事的几乎任何机器学习项目都会很有帮助！</p><p id="b72e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">第5部分:结果</strong></p><p id="c263" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们来测试我们的模型。我去Yelp上找了一家意大利餐厅的随机三星级评论。三星级的评论并不能真正告诉你评论者的感受，我也不想阅读整个评论，所以我让模型做所有的工作。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/5c8b0169b68650db6b3096493b70c0a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*aDNCP-FXzXvDm-ofcNMqKA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="197e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从这个简单的输出中，我可以更好地了解评论者对这家意大利餐厅的感受。情感下面的单词和数字显示了对分数影响最大的15个单词，这些单词通常是独特的，接近于情感单词，如“好”或“坏”。从这个结果，我知道这个评论家有一个稍微正面的体验，扇贝可能真的很好。我也知道这个地方可能有点潮湿，这位评论家被餐厅里的一些东西吓坏了。如果我是这家餐厅的老板，我真的会从中受益，因为我可以看到哪些词有助于积极的评论，哪些词有助于消极的评论。</p><p id="eeb2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">自然语言处理有着广泛的用途，Yelp评论只是我们可以用来建立情感分析模型的许多基于文本的数据形式之一。我从构建这个模型中学到了很多，类似这样的东西将是对使用Python进行NLP的很好的介绍。如果你有兴趣了解更多，我强烈建议你通读科尔·霍华德、汉尼斯·哈普克和霍布森·莱恩所著的《自然语言处理实践》一书。</p><p id="7b06" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望这个项目演练能启发您创建自己的情感分析模型或研究集成方法！感谢您的阅读！</p><p id="c5c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">完整项目:【https://github.com/Bench-amblee/yelp_sentiment_analysis T2】</p></div></div>    
</body>
</html>