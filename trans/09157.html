<html>
<head>
<title>Differentially Private Federated Learning with Flower and Opacus</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于花和不透明的差分私有联合学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/differentially-private-federated-learning-with-flower-and-opacus-e14fb0d2d229?source=collection_archive---------15-----------------------#2021-08-24">https://towardsdatascience.com/differentially-private-federated-learning-with-flower-and-opacus-e14fb0d2d229?source=collection_archive---------15-----------------------#2021-08-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="322f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">了解如何使用Flower和Opacus框架通过DP-SGD训练联邦模型</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/640a1581f291aa941e4d2344c74e1338.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c3uMKigCxVYIqbsZ-MNulg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://unsplash.com/@lensinkmitchel?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">米切尔·林辛克</a>在<a class="ae kv" href="https://unsplash.com/@lensinkmitchel?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的原始照片。由作者编辑。</p></figure><p id="8995" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在之前的<a class="ae kv" rel="noopener" target="_blank" href="/scaling-flower-with-multiprocessing-a0bc7b7aace0#a341-919044032aa0">帖子</a>中，我已经描述了什么是联合学习，并给出了一个如何在Flower框架中使用它的例子。我还展示了如何使用多处理来扩展您的实验，以避免GPU内存混乱。这个教程是基于上一个的，所以我建议如果你是第一次接触<a class="ae kv" href="https://flower.dev/" rel="noopener ugc nofollow" target="_blank"> Flower </a>的话，先快速回顾一下，获取更多的背景知识。</p><h1 id="af47" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">差分隐私的(真正的)简短介绍:</h1><p id="19de" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">通过将实体之间交换的信息流从数据本身转移到仅模型参数，联合学习(FL)比机器学习中的普通方法实现了更高程度的隐私。事实证明，这对于以移动应用为目标的用例非常有用[ <a class="ae kv" href="http://arxiv.org/abs/1811.03604" rel="noopener ugc nofollow" target="_blank"> 1 </a> ]，因为大量参与者可能有时间接受培训，并提供了在其他情况下无法获得的数据上进行培训的可能性。然而，FL本身不足以实现完全隐私，因为它表明模型可以记住给出的例子，而不仅仅是学习如何使用呈现的特征来完成任务。这带来了问题，因为成员推理攻击可以被执行并恢复用于训练模型的原始样本。</p><p id="56b0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">解决这个问题的常见方法是在将模型的权重发送给另一个实体(可能是服务器或另一个对等体)之前对其进行加密。然而，这些技术会给工作流带来更高的计算和通信成本。一种替代方法是使用差分隐私(DP)，它分析由给定机制引起的潜在隐私泄露。DP最初是由Dwork [ <a class="ae kv" href="https://doi.org/10.1007/11787006_1" rel="noopener ugc nofollow" target="_blank"> 2 </a> ]在2006年提出的，它基于相邻数据库的概念，即数据库最多只有一个元素不同，定义如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/272dd6458a0da18b363f21e7527cab1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JpGmBZ4k0lR9hRZGKle6Rg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">与D和D '相邻的数据库。图片取自[ <a class="ae kv" href="https://doi.org/10.1109/CSF.2017.11" rel="noopener ugc nofollow" target="_blank"> 3 </a> ]。</p></figure><p id="e5fc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">DP保证当在数据库中添加或删除一个给定的元素时，来自我们机制的答案不会改变一定的量:exp(ε) + δ。通过添加独立的高斯噪声来实现机制的差分私有的普通方式，其中方差与机制的灵敏度成比例。为了评估随机机制的隐私预算，必须进行分析，以便我们可以跟踪隐私损失。这种分析包括雷尼差分隐私(RDP) [ <a class="ae kv" href="https://doi.org/10.1109/CSF.2017.11" rel="noopener ugc nofollow" target="_blank"> 3 </a>，并具有到(ε，δ)-DP的转换。DP最有利的特性之一是它对后处理是健壮的，这意味着无论对获得的结果做什么，隐私保证都将保持不变。</p><p id="b8fb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">DP是一个很大的主题，许多其他人已经写了关于它的文章，所以我在这里将保持简短，但是，如果你想更好地了解它的细节，你可以看看媒体上的这个<a class="ae kv" rel="noopener" target="_blank" href="/understanding-differential-privacy-85ce191e198a">其他帖子</a>。</p><p id="7269" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，人们可以决定直接对原始数据应用DP来保护它。然而，噪声的增加是以精度为代价的，在我们的工作流程中应用DP越晚越好。这就是为什么ML模型的常见方法是使用由Abaddi等人[ <a class="ae kv" href="https://doi.org/10.1145/2976749.2978318" rel="noopener ugc nofollow" target="_blank"> 4 </a> ]介绍的SGD优化器的DP版本。该算法不将噪声应用于数据或模型参数，而是在梯度下降的每一步直接应用于梯度。它还使用裁剪来限制单个元素对训练过程的影响。这是我们今天要使用的算法，在PyTorch的团队<a class="ae kv" href="https://opacus.ai/" rel="noopener ugc nofollow" target="_blank"> Opacus </a>库中实现，并使用RDP作为隐私分析。</p><h2 id="038e" class="mq lt iq bd lu mr ms dn ly mt mu dp mc lf mv mw me lj mx my mg ln mz na mi nb bi translated">设置环境</h2><p id="5457" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">该代码可从GitHub 上获得，并详细说明了如何安装这些库。我推荐用一个用<a class="ae kv" href="https://python-poetry.org/" rel="noopener ugc nofollow" target="_blank">诗</a>搭建的虚拟环境，Flower的团队也在用。</p><h1 id="2dfa" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">助手文件</h1><p id="8b9e" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">和第一篇文章一样，我们将从编写一个名为<code class="fe nc nd ne nf b">flower_helpers.py</code>的助手文件开始，我们的大多数函数都将位于这个文件中。从进口开始:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="d47a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所有需要的库都在这里:Flower (flwr)、Torch + Torchivision、Numpy和Opacus。还有一些是打字问题。您可以注意到我们从Flower导入了FedAvg，这是库用来定义如何在联邦过程中更新权重的策略。我们需要制定策略来适应DP案例。从Opacus只导入了两件东西:PrivacyEngine和sampler。该引擎将允许我们将其附加到任何torch优化器，以在其上执行DP-SGD步骤。至于采样器，我们将在一段时间内看到它。下一步是为模型定义我们的设备:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="3f87" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">之前，我们也使用了CIFAR10作为数据集，但为了简单起见，我们保持了它的完整性。然而，为了更好地理解DP对我们工作流的贡献，我们将创建自己的联邦数据集。大多数情况下，集中式数据集受益于完全相同且独立分布(IID)，这意味着每个类都是平衡的，并且其中的样本共享相同的要素分布。然而，在联邦环境中，由于数据来自不同的实体，找到完美的分布式本地数据集的机会几乎为零。以MNIST为例，如果我们根据参与创作的每个人来划分，我们会注意到，每个人抽取的数字可能不同，但他们的写作风格也不同。这是一个非IID的例子，但是数据集可以有很多非IID的方式，这是联邦应用程序的主要关注点，因为我们无法看到我们正在使用的数据。在我们的教程中，我们将保持简单，仅在N个客户端中分离CIFAR10数据集，它们可以保存的样本数量有所不同。从分割功能开始:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="5126" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每个分割最初包含相同数量的样本，然后我们通过交替地从一个分割中移除样本并将它们添加到另一个分割中来调整它。我们用一个比率来确定一次分割可以拥有的最小样本数。75%的比率意味着我们最小的可能分割包含平均值的25%。接下来，我们加载数据:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="1073" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要加载客户端数据，我们首先加载CIFAR10并对其进行规范化，然后根据我们想要使用的客户端数量创建拆分。然后，在将子集提供给DataLoader类之前，创建一个子集。这不是最佳方式，但由于CIFAR10是一个非常小的数据集，我们负担得起。你在这里注意到了uniformwithrecreplacementsampler的使用，它也出现在所有Opacus的教程中，但是他们从来没有解释过为什么要使用它。这是非常小的一步，但却是非常重要的一步，因为这个采样器允许我们修改批处理的创建方式。就训练表现而言，我们甚至看不到区别，但是没有区别，DP保证就不成立。如果什么都不做，将按顺序选择批，直到dataloader中没有元素。问题是DP的保证是基于这样一个事实，即对于每一批，每个样本都有相同的概率<em class="ni"> p </em>被抽样。在一个时期内，一个样本可以被多次看到，并且批次甚至大小不一。uniformwithrecreplacementsampler正是这样做的，它以概率<em class="ni"> p </em>(由<code class="fe nc nd ne nf b">sample_rate</code>参数定义)获取一个样本，并将其放回下一批的数据集中。如果不使用，Opacus给出的最终隐私预算不会改变，但不会是真实的。</p><p id="86e6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来是定义我们的模型:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="81cd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该模型基于“<a class="ae kv" href="https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html" rel="noopener ugc nofollow" target="_blank">py torch:60分钟闪电战</a>”教程。当使用DP-SGD时，我们必须在建立模型时考虑它，因为算法引起的噪声和削波会极大地影响模型的行为。例如，建议通过Tanh [ <a class="ae kv" href="https://openreview.net/forum?id=rJg851rYwH" rel="noopener ugc nofollow" target="_blank"> 5 </a> ]等有界函数切换ReLU激活函数，这些函数是无界的。这使得模型支持更好的渐变裁剪并提高了性能。我们这里没有，但是放弃辍学也是一个不错的选择，因为DP已经是一个强大的规范器。如果在您的网络中使用任何BatchNormalization层，您应该用其他层替换它们(如GroupNormalization或LayerNormalization)，因为批处理方式的计算操作不是DP友好的，并且不受Opacus支持。最后一个好的选择，也是尚未被证明的[ <a class="ae kv" href="https://openreview.net/forum?id=rJg851rYwH" rel="noopener ugc nofollow" target="_blank"> 5 </a> ]，可以尝试减少你的模型的容量(它的参数数量)。DP的过度拟合会妨碍你的模型，所以尽量避免比平时多一倍。同样，超参数应针对差压设置进行微调。</p><p id="091e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来是一些与Flower相关的函数，用于设置和获取模型的权重:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="84fe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为每个客户端计算目标δ的函数:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="da52" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">δ表示ε给出的隐私保证不成立的概率，因此理想情况下，它应该低于数据集大小的倒数。它通常被设置得低一个数量级。</p><p id="12d4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来是我们的培训功能:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="a23b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将在这里回顾一些重要的步骤。首先，您可以注意到有两个批量大小:<code class="fe nc nd ne nf b">vbatch_size</code> (v代表虚拟)和<code class="fe nc nd ne nf b">batch_size</code>。<code class="fe nc nd ne nf b">vbatch_size</code>是我们的目标批量，而<code class="fe nc nd ne nf b">batch_size</code>将是数据加载器使用的实际批量。使用这些方法可以将PrivacyEngine的优化步骤分解为两个部分:计算梯度，然后应用噪声和梯度下降。这给了我们两个好处，首先，我们之前创建的采样器给出的实际批量大小并不完全是我们给出的。因此，累积多个批次的梯度可以让我们趋向于我们的初始目标值。其次，使用DP-SGD增加了训练过程的计算开销，随着批量的增长需要更多的内存。正如Opacus在他们的教程中所说:</p><p id="5699" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ni"> "Opacus计算并存储每样本渐变，因此对于每个法线渐变，Opacus将在每一步上存储</em> <code class="fe nc nd ne nf b"><em class="ni">n=batch_size</em></code> <em class="ni">每样本渐变，从而至少增加</em> <code class="fe nc nd ne nf b"><em class="ni">O(batch_size)</em></code> <em class="ni">的内存占用。然而，实际上，与非私有模型相比，峰值内存需求是</em> <code class="fe nc nd ne nf b"><em class="ni">O(batch_size^2)</em></code> <em class="ni">。这是因为每样本梯度计算中的一些中间步骤涉及对两个矩阵的操作，每个矩阵都将batch_size作为其中一个维度。”</em></p><p id="d55d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这意味着通过保持虚拟步骤的小批量，我们可以有效地管理任何批量的内存需求。在代码中，它转化为确定所需的累积步骤数(<code class="fe nc nd ne nf b"><em class="ni">n_acc_steps</em></code>)，稍后当我们计算优化器步骤时，我们执行虚拟步骤或正常步骤，并刷新梯度:</p><pre class="kg kh ki kj gt nj nf nk nl aw nm bi"><span id="7c9d" class="mq lt iq nf b gy nn no l np nq">if ((i + 1) % n_acc_steps == 0) or ((i + 1) == len(train_loader)):</span><span id="b54d" class="mq lt iq nf b gy nr no l np nq">    optimizer.step()  # real step</span><span id="4f09" class="mq lt iq nf b gy nr no l np nq">    optimizer.zero_grad()</span><span id="0cad" class="mq lt iq nf b gy nr no l np nq">else:</span><span id="296f" class="mq lt iq nf b gy nr no l np nq">    optimizer.virtual_step()  # take a virtual step</span></pre><p id="22ce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后你会注意到我们正在训练函数中加载数据和模型。这是因为我们将使用多重处理来允许每次选择新客户端时释放GPU内存。如果我们在真实的环境中，客户端会在连接到服务器之前加载它们。您还可以注意到优化器从SGD with momentum更改为Adam。Opacus可以与任何优化器一起工作，但我个人发现Adam更容易使用DP设置。</p><p id="893e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">alpha列表包含由Opacus执行的RDP分析的订单。这个列表取自Opacus的教程，一般来说，你不需要修改它，但是如果你看到在你的例子中选择的alpha一直是一个极值，你可能想要调整它。</p><p id="7705" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来是PrivacyEngine，它是来自Opacus的自定义对象，允许执行DP-SGD。使用它的方法是将它附加到任何优化器上。它以模型、采样速率、阶数α、噪声乘数(添加的噪声量)、最大梯度范数(削波)和目标δ值为参数。通过给它们这些参数，它将执行RDP分析，并为训练过程的当前状态提供ε值。该值是在给定数据集上执行的总步骤数的情况下计算的，这些步骤存储在引擎的<code class="fe nc nd ne nf b">state_dict</code>中，可以通过调用相关方法获得。这就是我们通过联合培训跟踪客户隐私预算的方式。每次选择一个客户时，他们将存储产生的状态字典，以便在下次参与时恢复它。</p><p id="9d40" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我想指出的最后一点是，我们只是在为一个单一的本地时代进行训练。原因是因为FedAvg被证明在非IID数据集上不收敛[ <a class="ae kv" href="http://arxiv.org/abs/1907.02189" rel="noopener ugc nofollow" target="_blank"> 6 </a> ]。</p><p id="e834" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">培训后，我们定义不包括任何DP步骤的测试:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="0a83" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我们需要为服务器定义一个行为来管理客户端。我们可以只使用标准的联合平均策略，但如果我们的目标是尊重所需的隐私预算，我们需要处理客户的意见:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="8caf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们只添加一个名为<code class="fe nc nd ne nf b">max_epsilon</code>的参数来跟踪全球隐私预算。由于合成定理，独立机制的组合所产生的隐私预算将等于最大值。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="6135" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每个客户端将在其度量字典中返回他们的隐私预算是否超过目标值。如果是这样的话，服务器会断开他们的连接，这样他们就不必再参加培训了。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="b2ed" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，最初的<code class="fe nc nd ne nf b">FedAvg</code>类中的<code class="fe nc nd ne nf b">configure_evaluate</code>在每一轮都被调用，如果最初没有评估函数被传递给策略，它将选择一些客户端来执行评估。同样，在最后一轮，它将对每个可用的客户端进行评估。当我们在服务器端执行评估时，我们希望避免这两种行为，但是如果没有足够的客户端可用于训练，我们会打印一条消息，因为我们断开了其中一些客户端。</p><h1 id="fb8e" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">客户端文件</h1><p id="c9a4" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">我们所有有用的函数都被定义了，然后我们可以编写我们的客户端文件。从进口开始:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="b1b1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在main函数中，我们从脚本的参数开始:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="27c2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，我们通过从NumpyClient类继承来定义我们的客户端</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="270d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">遵循fit方法，首先是训练过程:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="ac77" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">获得结果:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="fadd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后是评估函数(我们在这个例子中没有用到):</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="fb0f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后要做的是启动客户端并运行main:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><h1 id="5ed8" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">服务器文件</h1><p id="2203" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">只剩下服务器文件。导入和全局变量:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="3e8c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">服务器端的评估函数:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="26dd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">主要功能和启动服务器:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><h1 id="3c69" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">Bash文件</h1><p id="b50b" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">我们所有的核心元素都已经编写好了，现在我们可以使用bash脚本组合并启动它们:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="d3cc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在将脚本转换为可执行文件后，在您的终端中运行<code class="fe nc nd ne nf b">./run.sh</code>将默认运行2个客户端3轮。您可以根据需要修改参数。</p><p id="c8ec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于只有两个客户端和一个稍微非IID的数据集，该模型应该执行close(如果不使用FL的话)。经过一段时间的训练后，你可能会发现网络卡在40%以下，很难再提高。这是因为如果没有DP(55%左右)，我们正在使用的模型本身就不能表现得那么好。您可以通过使用更好的模型(增加每个卷积层的滤波器数量)甚至使用迁移学习来缓解这一问题。随意实验！</p><h1 id="0794" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结论和展望</h1><p id="e017" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">通过这篇文章，您了解了如何使用Flower和Opacus在联邦模型上执行DP-SGD。然而，有些部分我们没有涉及到。例如，没有解决为联合DP-SGD寻找超参数的问题。人们可以选择在对每个客户端进行训练之前直接计算它们，这导致了额外的隐私成本，或者他们可以选择预先对类似的本地数据集进行微调，并使用为所有客户端找到的超参数。我们也可能想知道DP-SGD对我们的模型的真正影响，以及(ε，δ)-DP预算在保护方面的真正含义，[ <a class="ae kv" href="http://arxiv.org/abs/2006.07709" rel="noopener ugc nofollow" target="_blank"> 7 </a>对此提供了一些答案，但这仍然是一个开放的话题。寻找对增加的噪声更具弹性的架构，从而减少应用DP引起的性能下降也很重要。</p><p id="8f94" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你想了解更多关于如何将Opacus应用于不同情况的知识，你可以查看他们的<a class="ae kv" href="https://opacus.ai/tutorials/" rel="noopener ugc nofollow" target="_blank">教程</a>部分，这里有两个额外的NLP示例。同样的道理也适用于<a class="ae kv" href="https://github.com/adap/flower/tree/main/examples" rel="noopener ugc nofollow" target="_blank">花</a>，虽然我们在这里用PyTorch使用它，但它并不限于此，你可以用任何你喜欢的框架进行实验，甚至是<a class="ae kv" href="https://flower.dev/blog/2021-07-21-federated-scikit-learn-using-flower" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a>。同样，我们只涵盖了我们案例的一个基本数据集，但是我们可以使用一些专门为联邦案例设计的数据集，比如<a class="ae kv" href="https://leaf.cmu.edu/" rel="noopener ugc nofollow" target="_blank"> LEAF </a>基准。</p><p id="3c5e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本教程到此结束。代码可在<a class="ae kv" href="https://github.com/matturche/flower_opacus_example" rel="noopener ugc nofollow" target="_blank">这里</a>获得。我希望这将是有用的，并帮助你们中的一些人更好地理解如何花和Opacus可以一起工作。不要犹豫留下反馈和/或问题！</p><h1 id="a8b9" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">参考</h1><pre class="kg kh ki kj gt nj nf nk nl aw nm bi"><span id="1653" class="mq lt iq nf b gy nn no l np nq">[1] A. Hard <em class="ni">et al.</em>, “<a class="ae kv" href="http://arxiv.org/abs/1811.03604" rel="noopener ugc nofollow" target="_blank">Federated Learning for Mobile Keyboard Prediction</a>,” <em class="ni">arXiv:1811.03604 [cs]</em></span><span id="faf6" class="mq lt iq nf b gy nr no l np nq">[2] C. Dwork, “<a class="ae kv" href="https://doi.org/10.1007/11787006_1" rel="noopener ugc nofollow" target="_blank">Differential Privacy</a>,” in <em class="ni">Automata, Languages and Programming</em>, Berlin, Heidelberg, 2006, pp. 1–12. doi: 10.1007/11787006_1.</span><span id="0166" class="mq lt iq nf b gy nr no l np nq">[3] I. Mironov, “<a class="ae kv" href="https://doi.org/10.1109/CSF.2017.11" rel="noopener ugc nofollow" target="_blank">Renyi Differential Privacy</a>,” <em class="ni">2017 IEEE 30th Computer Security Foundations Symposium (CSF)</em>, pp. 263–275, Aug. 2017, doi: 10.1109/CSF.2017.11.</span><span id="a26a" class="mq lt iq nf b gy nr no l np nq">[4] M. Abadi <em class="ni">et al.</em>, “<a class="ae kv" href="https://doi.org/10.1145/2976749.2978318" rel="noopener ugc nofollow" target="_blank">Deep Learning with Differential Privacy</a>,” <em class="ni">Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security</em>, pp. 308–318, Oct. 2016, doi: 10.1145/2976749.2978318.</span><span id="6596" class="mq lt iq nf b gy nr no l np nq">[5] N. Papernot, S. Chien, S. Song, A. Thakurta, and U. Erlingsson, “<a class="ae kv" href="https://openreview.net/forum?id=rJg851rYwH" rel="noopener ugc nofollow" target="_blank">Making the Shoe Fit: Architectures, Initializations, and Tuning for Learning with Privacy</a>,” OpenReview</span><span id="4543" class="mq lt iq nf b gy nr no l np nq">[6] X. Li, K. Huang, W. Yang, S. Wang, and Z. Zhang, “<a class="ae kv" href="http://arxiv.org/abs/1907.02189" rel="noopener ugc nofollow" target="_blank">On the Convergence of FedAvg on Non-IID Data</a>,” <em class="ni">arXiv:1907.02189 [cs, math, stat]</em></span><span id="40c1" class="mq lt iq nf b gy nr no l np nq">[7] M. Jagielski, J. Ullman, and A. Oprea, “<a class="ae kv" href="http://arxiv.org/abs/2006.07709" rel="noopener ugc nofollow" target="_blank">Auditing Differentially Private Machine Learning: How Private is Private SGD?</a>,” <em class="ni">arXiv:2006.07709 [cs]</em></span></pre></div></div>    
</body>
</html>