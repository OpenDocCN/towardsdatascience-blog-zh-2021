<html>
<head>
<title>Essential guide to perform Feature Binning using a Decision Tree Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用决策树模型执行功能宁滨的基本指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/essential-guide-to-perform-feature-binning-using-a-decision-tree-model-90bcc66d61f9?source=collection_archive---------9-----------------------#2021-09-02">https://towardsdatascience.com/essential-guide-to-perform-feature-binning-using-a-decision-tree-model-90bcc66d61f9?source=collection_archive---------9-----------------------#2021-09-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c93d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何为数字特征宁滨找到最佳存储桶</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0527a5b34aa0681a7de76ada4433c33d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QUyAVShHSA6hygdkhkqRnQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://pixabay.com/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=3839456" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>的<a class="ae ky" href="https://pixabay.com/users/thedigitalartist-202249/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=3839456" rel="noopener ugc nofollow" target="_blank">皮特·林福思</a></p></figure><p id="ec51" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">特征工程是机器学习模型开发管道的重要组成部分。机器学习模型只理解数字向量，因此数据科学家需要设计这些特征来训练一个健壮的机器学习模型。</p><p id="cc33" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">宁滨或离散化用于将连续变量或数值变量编码成分类变量。有时，数字或连续特征不适用于非线性模型。因此，连续变量的宁滨会在数据中引入非线性，并有助于提高模型的性能。</p><p id="b00f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有各种技术来执行特征宁滨，包括宁滨的<strong class="lb iu">非监督和监督方法。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/567e9083ce23aa03ab1d47cfcfc92b70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5JDlohoNuzQx4lYdVYrzlA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)，宁滨表演技巧</p></figure><p id="8b56" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">阅读我以前的<a class="ae ky" rel="noopener" target="_blank" href="/feature-engineering-deep-dive-into-encoding-and-binning-techniques-5618d55a6b38">文章</a>，深入了解特性编码策略</p><div class="lw lx gp gr ly lz"><a rel="noopener follow" target="_blank" href="/feature-engineering-deep-dive-into-encoding-and-binning-techniques-5618d55a6b38"><div class="ma ab fo"><div class="mb ab mc cl cj md"><h2 class="bd iu gy z fp me fr fs mf fu fw is bi translated">特征工程——深入研究编码和宁滨技术</h2><div class="mg l"><h3 class="bd b gy z fp me fr fs mf fu fw dk translated">特征编码和特征宁滨技术图解</h3></div><div class="mh l"><p class="bd b dl z fp me fr fs mf fu fw dk translated">towardsdatascience.com</p></div></div><div class="mi l"><div class="mj l mk ml mm mi mn ks lz"/></div></div></a></div><p id="3f6f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将讨论使用决策树模型绑定数字特征的最佳策略之一。</p><h1 id="d399" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">宁滨的特色是什么？</h1><p id="9021" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">特征宁滨指的是将数字或连续特征转换或存储为分类变量的技术。</p><p id="f928" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Pandas提出了一个<code class="fe nl nm nn no b"><strong class="lb iu">pd.qcut(x, q)</strong></code> <strong class="lb iu"> </strong>函数，该函数将连续特征分成<code class="fe nl nm nn no b"><strong class="lb iu">q</strong></code>个桶，具有相等的百分位数差异。</p><pre class="kj kk kl km gt np no nq nr aw ns bi"><span id="2326" class="nt mp it no b gy nu nv l nw nx"><strong class="no iu">df['Sepallength_quartle'] = pd.qcut(df['SepalLengthCm'], 4)</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/d9a17dff21032217f368f17c3b7d64b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*mD8w4jKgupjDz9ss4Cx5iQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)，分桶结果示例</p></figure><p id="4f58" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">或者也可以使用<code class="fe nl nm nn no b"><strong class="lb iu">pd.cut(x, bins, labels)</strong></code> <strong class="lb iu"> </strong>函数，并传递自定义的箱柜和标签进行装箱。</p><p id="c566" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在计算仓位时，这些类型的分桶策略不涉及目标变量。因此，箱夜与目标变量没有任何关联。特征宁滨的目的是在数据中引入非线性，这可以进一步改善模型的性能。</p><h1 id="dbbf" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">想法:</h1><p id="6683" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">通过执行随机、计数或基于四分位数的存储桶，无法实现功能宁滨的主要目的。想法是使用决策树模型找到最佳的桶或箱集合，该模型将涉及与目标变量的相关性。</p><blockquote class="nz oa ob"><p id="48ab" class="kz la oc lb b lc ld ju le lf lg jx lh od lj lk ll oe ln lo lp of lr ls lt lu im bi translated">数据来源:我们将对从<a class="ae ky" href="https://www.kaggle.com/uciml/iris" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>下载的虹膜数据集中的“萼片长度”特征进行宁滨处理。</p></blockquote><p id="c0e1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">按照以下步骤确定最佳“萼片长度”箱:</p><ul class=""><li id="e39c" class="og oh it lb b lc ld lf lg li oi lm oj lq ok lu ol om on oo bi translated">用特征“萼片长度”作为训练数据，用“物种”作为目标变量，训练一个最适合的决策树模型。</li></ul><pre class="kj kk kl km gt np no nq nr aw ns bi"><span id="da8f" class="nt mp it no b gy nu nv l nw nx"><strong class="no iu">X = df[['SepalLengthCm']]<br/>y = df['Species']</strong></span><span id="849e" class="nt mp it no b gy op nv l nw nx"><strong class="no iu">params = {'max_depth':[2,3,4], 'min_samples_split':[2,3,5,10]}<br/>clf_dt = DecisionTreeClassifier()<br/>clf = GridSearchCV(clf_dt, param_grid=params, scoring='accuracy')<br/>clf.fit(X, y)</strong></span><span id="f9fe" class="nt mp it no b gy op nv l nw nx"><strong class="no iu">clf_dt = DecisionTreeClassifier(clf.best_params_)</strong></span></pre><ul class=""><li id="73a6" class="og oh it lb b lc ld lf lg li oi lm oj lq ok lu ol om on oo bi translated">使用Graphviz绘制决策树。</li></ul><pre class="kj kk kl km gt np no nq nr aw ns bi"><span id="ffbe" class="nt mp it no b gy nu nv l nw nx"><strong class="no iu">tree.plot_tree(clf_dt, filled=True, feature_names = list(X.columns), class_names=['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'])<br/>plt.show()</strong></span></pre><ul class=""><li id="4026" class="og oh it lb b lc ld lf lg li oi lm oj lq ok lu ol om on oo bi translated">通过观察图节点来决定特征桶。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/9f436fdd5fe8dbe4542d46f3d4ecaff9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*YXjUZyU__13OYE2aHcASZA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)，使用Graphviz的决策树表示</p></figure><p id="cc27" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从决策树模型的上述图形表示中，我们可以得出结论，鸢尾属的大多数点的萼片长度≤ 5.45，而鸢尾属的大多数数据点的萼片长度&gt; 6.15。大多数鸢尾-杂色花数据点的萼片长度在4.86-6.15之间。</p><p id="81d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">利用上述观察，我们可以准备桶来收集“萼片长度”连续特征。</p><pre class="kj kk kl km gt np no nq nr aw ns bi"><span id="73fc" class="nt mp it no b gy nu nv l nw nx">bins = [0, 5.45, 6.15, 10]<br/>labels = ['Less Than 5.45','4.86 - 6.15', 'Above 6.16']<br/>df['SepalLengthBucket'] = pd.cut(df['SepalLengthCm'], bins=bins, labels=labels)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/93c82b77d9a9ea83514327edef210660.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*QRe3hD3BEL3njBuXqQeN9A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)，分桶连续特征(“SepalLengthBucket”)和目标变量(“物种”)之间的相关性</p></figure><p id="cd86" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">分桶萼片长度特征与目标变量有很好的相关性，并且将比随机、计数或基于四分位数的分桶策略更好地训练稳健得多的模型。</p><h1 id="585c" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">结论:</h1><p id="1aad" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">连续变量的宁滨会在数据中引入非线性，并有助于提高模型的性能。在执行要素宁滨时，基于决策树规则的分桶策略是决定要挑选的最佳要素分桶集的一种简便方法。</p><p id="e54f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">必须记住，不要训练深度较大的决策树模型，因为解释特征桶会变得很困难。</p><h1 id="8b7d" class="mo mp it bd mq mr ms mt mu mv mw mx my jz mz ka na kc nb kd nc kf nd kg ne nf bi translated">参考资料:</h1><p id="8608" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">[1] Scikit-learn文档:【https://scikit-learn.org/stable/modules/tree.html T2】</p><blockquote class="os"><p id="dcc7" class="ot ou it bd ov ow ox oy oz pa pb lu dk translated">感谢您的阅读</p></blockquote></div></div>    
</body>
</html>