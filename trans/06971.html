<html>
<head>
<title>NodePiece: Tokenizing Knowledge Graphs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">节点:标记知识图</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nodepiece-tokenizing-knowledge-graphs-6dd2b91847aa?source=collection_archive---------20-----------------------#2021-06-24">https://towardsdatascience.com/nodepiece-tokenizing-knowledge-graphs-6dd2b91847aa?source=collection_archive---------20-----------------------#2021-06-24</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="899f" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想与理论</a>，构图性&amp;图形</h2><div class=""/><div class=""><h2 id="ef17" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">将每个节点映射到一个嵌入向量会产生非常大的嵌入矩阵。有没有一种类似于“子词单元”的固定大小的令牌词汇表的方法？</h2></div><p id="787a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="lk">这篇博文是与Etienne Denis和Paul Wu共同撰写的，基于我们的论文“</em> <a class="ae ll" href="https://arxiv.org/abs/2106.12144" rel="noopener ugc nofollow" target="_blank"> <em class="lk">节点件:大型知识图的组合和参数高效表示</em> </a> <em class="lk">”。</em></p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lm"><img src="../Images/9c8a91343b938a45b96bf572438f7d6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yYt_5YzZHpfMu4T1j1tXyg.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">语言模型有固定大小的词汇表和表达性编码器。浅KG嵌入大多只使用所有节点的词汇表。我们能不能有一个固定大小的词汇表，带有一个嵌入KGs的编码器？作者图片</p></figure><p id="6209" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi mc translated"><span class="l md me mf bm mg mh mi mj mk di">K</span>knowledge graphs(KG)不断变得越来越大:<a class="ae ll" href="https://www.wikidata.org/wiki/Wikidata:Main_Page" rel="noopener ugc nofollow" target="_blank"> Wikidata </a>拥有大约1亿个节点(实体)，<a class="ae ll" href="https://yago-knowledge.org/getting-started" rel="noopener ugc nofollow" target="_blank"> YAGO 4 </a>拥有大约5000万个节点，而像<a class="ae ll" href="https://developers.google.com/knowledge-graph" rel="noopener ugc nofollow" target="_blank"> Google KG </a>或<a class="ae ll" href="https://www.diffbot.com/" rel="noopener ugc nofollow" target="_blank"> Diffbot </a>这样的定制图要大几个数量级📈。</p><p id="a4b2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">对它们执行任何机器学习(ML)或图形表示学习(GRL)任务通常意味着<strong class="kq ja">通过将每个实体和关系映射到一个<strong class="kq ja">唯一向量</strong>(或有时许多向量，或整个唯一矩阵)来将</strong>实体和关系嵌入潜在空间。我们通常把那些称为<strong class="kq ja"> <em class="lk">浅嵌入</em> </strong>。</p></div><div class="ab cl ml mm hu mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="ij ik il im in"><h1 id="d14b" class="ms mt iq bd mu mv mw mx my mz na nb nc kf nd kg ne ki nf kj ng kl nh km ni nj bi translated"><strong class="ak">浅埋:局限性</strong></h1><blockquote class="nk"><p id="0153" class="nl nm iq bd nn no np nq nr ns nt lj dk translated">几乎所有现有的KG嵌入算法(如TransE，RotatE，DistMult等)都很肤浅。</p></blockquote><p id="aa26" class="pw-post-body-paragraph ko kp iq kq b kr nu ka kt ku nv kd kw kx nw kz la lb nx ld le lf ny lh li lj ij bi mc translated"><span class="l md me mf bm mg mh mi mj mk di"> E </span>具有可学习的实体嵌入矩阵的基于GNN的模型，例如<a class="ae ll" href="https://openreview.net/pdf?id=BylA_C4tPr" rel="noopener ugc nofollow" target="_blank"> CompGCN </a>，在消息传递之前具有浅查找层。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi nz"><img src="../Images/88bed7f038efcaeb6227ac63e8c808bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GineV8gy0INnmGm9t50Cxw.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">浅嵌入:所有节点被映射到各自唯一的向量。作者图片</p></figure><p id="bfe6" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">本质上，shallow意味着嵌入查找:对于包含<em class="lk"> |E| </em>个实体的词汇表，在嵌入矩阵中必须有<em class="lk"> |E| </em> <em class="lk"> d </em>维的行(🖼👈).也就是说，100个实体= 100行，10K实体= 10K行，100M实体= 100M行，依此类推。现在，将行数乘以嵌入维数，以获得可学习参数的大致数量(关系数<em class="lk"> |R| </em>比<em class="lk"> |E| </em>小得多，因此我们主要关注实体)。并且整个嵌入矩阵驻留在珍贵的GPU存储器中💾。</p><p id="727e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">顺便说一下，像<a class="ae ll" href="https://arxiv.org/pdf/1607.00653.pdf" rel="noopener ugc nofollow" target="_blank"> node2vec </a>这样的经典图嵌入算法也是浅层的，因为它们也将每个节点映射到一个唯一的向量。</p><p id="b0d2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">从<a class="ae ll" href="https://ogb.stanford.edu/" rel="noopener ugc nofollow" target="_blank">开放图形基准(OGB) </a>扩展到一个只有250万个实体的<a class="ae ll" href="https://ogb.stanford.edu/docs/linkprop/#ogbl-wikikg2" rel="noopener ugc nofollow" target="_blank"> WikiKG 2 </a>图会发生什么？嗯，…🤔</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi oa"><img src="../Images/a2d14d7a773fff68ee7bf2d6fcb75598.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SuRd5GIu8u4v6QAwIM3Z0Q.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">当前OGB维基2排行榜。来源:<a class="ae ll" href="https://ogb.stanford.edu/docs/leader_linkprop/" rel="noopener ugc nofollow" target="_blank"> <strong class="bd ob">开图基准</strong> </a></p></figure><p id="a431" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">因此，模型大小在500M-1.25B参数之间变化(并且您需要具有45 GB VRAM的🛩-expensive GPU或减少嵌入维度)👀</p><p id="c664" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">🛑等等，革命性的NLP的<a class="ae ll" href="https://www.aclweb.org/anthology/N19-1423.pdf" rel="noopener ugc nofollow" target="_blank"> BERT或者被认为</a><a class="ae ll" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank">“太危险而不能公开发布”</a>的GPT-2有多大？340M和1.5B参数。所以你可能会有一个自然出现的问题:</p><blockquote class="nk"><p id="236c" class="nl nm iq bd nn no np nq nr ns nt lj dk translated">我们是否有效地使用了我们的参数预算？为什么1.25B WikiKG嵌入没有那么<em class="oc">危险</em>？</p></blockquote><p id="9ed5" class="pw-post-body-paragraph ko kp iq kq b kr nu ka kt ku nv kd kw kx nw kz la lb nx ld le lf ny lh li lj ij bi translated">(嗯，除了训练上的能耗)</p><p id="90e4" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我相信你比我更清楚:</p><ul class=""><li id="c74c" class="od oe iq kq b kr ks ku kv kx of lb og lf oh lj oi oj ok ol bi translated">LMs有一个小的固定大小的词汇表，并在这个词汇表之上有一个强大的编码器(当然是Transformer的变体)；</li><li id="fedf" class="od oe iq kq b kr om ku on kx oo lb op lf oq lj oi oj ok ol bi translated">固定大小的小词汇量允许处理任何单词，甚至是那些在训练中看不到的单词；</li><li id="6d9e" class="od oe iq kq b kr om ku on kx oo lb op lf oq lj oi oj ok ol bi translated">这个固定大小的小词汇表仍然足够宽(768–1024d)。</li></ul><p id="e414" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">另一方面，我们在浅嵌入模型中有什么？(这里我们来到了<strong class="kq ja">标题图片</strong> 🖼👆)</p><ul class=""><li id="5398" class="od oe iq kq b kr ks ku kv kx of lb og lf oh lj oi oj ok ol bi translated">只有实体和关系的词汇表，没有通用的编码器；</li><li id="700d" class="od oe iq kq b kr om ku on kx oo lb op lf oq lj oi oj ok ol bi translated">庞大的词汇表吞噬了所有的参数预算。要么是词汇表变得太大，要么是你必须减少维数以保持总的大小合理。</li><li id="43d8" class="od oe iq kq b kr om ku on kx oo lb op lf oq lj oi oj ok ol bi translated">纯转导设置—没有能力构建看不见的节点的表示，它们是OOV(不在词汇表中)。</li></ul><p id="df36" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">那些浅薄的属性让你想起什么了吗？🤨欢迎回到2014年！</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi or"><img src="../Images/5afc2b176faf26a309101901cead48e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*njheGLmI6qcoqOXNNZTWzA.gif"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">这里需要一些时间旅行。来源:<a class="ae ll" href="http://gph.is/14n1OSJ" rel="noopener ugc nofollow" target="_blank"> Giphy </a></p></figure><h1 id="dbad" class="ms mt iq bd mu mv os mx my mz ot nb nc kf ou kg ne ki ov kj ng kl ow km ni nj bi translated">迂回:从word2vec到字节对编码</h1><p id="ef8c" class="pw-post-body-paragraph ko kp iq kq b kr ox ka kt ku oy kd kw kx oz kz la lb pa ld le lf pb lh li lj ij bi translated">在<em class="lk">史前</em>前变形金刚时代，NLP的人做了什么？</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi pc"><img src="../Images/4c96a7bc2102339d87e19590648e4cb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hdrYYnrmhFti9buWXCkbGQ.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">来源:<a class="ae ll" href="https://jalammar.github.io/illustrated-word2vec/" rel="noopener ugc nofollow" target="_blank">杰·阿拉玛的插图文字2 vec</a></p></figure><p id="8b44" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi mc translated"><span class="l md me mf bm mg mh mi mj mk di">W</span>T8】ord2 vec和<a class="ae ll" href="https://github.com/stanfordnlp/GloVe" rel="noopener ugc nofollow" target="_blank"> GloVe </a>提供了<strong class="kq ja"> 400K - 3M </strong>预训练单词嵌入(有时还有短语)的矩阵。任何超过40万到300万的单词都属于OOV(不在词汇表中🤷‍♀️)类别。类似地，如果在推理时你收到一个看不见的单词——你必须用&lt; UNK &gt;令牌替换它。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi pd"><img src="../Images/0053d1b95996bbbdb69cac449ba662ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JhVxouagp63VBiZzd2WTAg.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">字节对编码词汇。来源:<a class="ae ll" href="https://www.aclweb.org/anthology/P16-1162.pdf" rel="noopener ugc nofollow" target="_blank">森里奇等人</a>和<a class="ae ll" href="http://web.stanford.edu/class/cs224n/readings/cs224n-2019-notes06-NMT_seq2seq_attention.pdf" rel="noopener ugc nofollow" target="_blank"> CS224N </a></p></figure><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi pe"><img src="../Images/69c0f411ed246b297f186a3725dfc3ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YSSxrxh29725ZPHQW0LSJw.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">机器翻译中的“单词块”。<a class="ae ll" href="https://www.aclweb.org/anthology/P16-1162.pdf" rel="noopener ugc nofollow" target="_blank">资料来源:森里奇等人</a></p></figure><p id="f493" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">最终，无论我们如何扩展<strong class="kq ja">单词</strong>词汇，由于语言的复合性，我们永远无法完全掌握所有的单词🏗。很快，社区意识到我们需要一个更具组合性的机制来代表任何单词，无论是看得见的还是看不见的，使用同一套原子🧱。2015年有很多这样的方法:</p><ul class=""><li id="a91f" class="od oe iq kq b kr ks ku kv kx of lb og lf oh lj oi oj ok ol bi translated"><a class="ae ll" href="https://arxiv.org/pdf/1508.06615.pdf" rel="noopener ugc nofollow" target="_blank">基于字符的模型</a>其中原子本质上是字母表🔡用一些辅助符号；</li><li id="e15a" class="od oe iq kq b kr om ku on kx oo lb op lf oq lj oi oj ok ol bi translated">在<a class="ae ll" href="https://www.aclweb.org/anthology/P16-1162.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja"> Sennrich等人</strong> </a>的开创性工作中引入了“词块”🧩或“子词单位”模型，其中原子是最常见的n元语法。将这些子字单元实现为<a class="ae ll" href="https://github.com/rsennrich/subword-nmt" rel="noopener ugc nofollow" target="_blank">子字-nmt </a>或<a class="ae ll" href="https://github.com/google/sentencepiece" rel="noopener ugc nofollow" target="_blank">字段/句子段</a>中的<a class="ae ll" href="https://github.com/rsennrich/subword-nmt" rel="noopener ugc nofollow" target="_blank">字节对编码(BPE) </a>的算法成为所有现代神经语言模型中事实上的标准预处理步骤。</li></ul><p id="ee37" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在，使用<strong class="kq ja">有限</strong>的原子词汇表，🧱就有可能构建几乎无限<strong class="kq ja">数量的组合词，任何看不见的词都可以被标记成一系列字符或词块，而且不存在OOV问题！</strong></p><p id="c5e9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">此外，您不需要存储3M大的词汇矩阵作为输入嵌入。使用单词块，典型的词汇表要小得多。🤏</p><ul class=""><li id="b4c5" class="od oe iq kq b kr ks ku kv kx of lb og lf oh lj oi oj ok ol bi translated">伯特:<strong class="kq ja"> 30K </strong>文字块令牌</li><li id="5872" class="od oe iq kq b kr om ku on kx oo lb op lf oq lj oi oj ok ol bi translated">GPT-2和GPT-3: <strong class="kq ja"> 50K </strong> BPE代币</li></ul><p id="1877" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">那么我们为什么不把这个用于图的ML域呢？🤔</p><blockquote class="nk"><p id="ec4c" class="nl nm iq bd nn no np nq nr ns nt lj dk translated">如果节点是“词”，是否可以为图设计一个固定大小的“子词”(子节点)单位的词汇？</p></blockquote><h1 id="25d7" class="ms mt iq bd mu mv os mx my mz ot nb nc kf pf kg ne ki pg kj ng kl ph km ni nj bi translated">用节点标记kg</h1><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi pi"><img src="../Images/f37ea5cc5b6dee5971586144cbe07e52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-kmNcYeJ_fMHKXlvScEIOQ.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">在自然语言处理和图ML方法中定位节点。图片作者。</p></figure><p id="1bc3" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">🎯我们在最近的工作中解决了这个研究问题，我们提出了<strong class="kq ja"> NodePiece </strong>作为一种组合的“节点标记化”方法来减少大型kg中的词汇大小。与其存储庞大的实体嵌入矩阵，我们建议学习一个由原子组成的<strong class="kq ja">固定大小的词汇表</strong>和一个简单的编码器，它将能够从原子序列中引导任何节点的表示，就像从子词单元序列中构造词一样。</p><p id="e053" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">🧱在NodePiece中，词汇原子(或者“节点片”，或者“令牌”，如果你喜欢的话)是<strong class="kq ja">锚节点</strong>和<strong class="kq ja">关系类型</strong>。锚节点是图中现有节点的子集，可以随机选择，也可以根据某些中心性度量(如PageRank或节点度)来选择。您不需要很多锚节点(因为您不需要令牌词汇表中所有可能的n元语法)，我们发现，通常1–10%就足够了。有时你甚至不需要锚节点在所有🧙‍♂️！</p><p id="068f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">所有的关系类型都包含在词汇表中，因为与节点(实体)的总数相比，它们通常并不多。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi pj"><img src="../Images/49a07ac0b307207f11dc7ada645c3fb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nki888pCVjXcLzaqwFzedQ.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">节点标记化策略。红色节点用3个最近锚点和3个传出关系类型来标记。作者图片</p></figure><p id="87a5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">那么我们如何标记图中的任意节点呢？让我们按照插图🖼👆凭着一种基本的直觉。</p><p id="9b3b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">假设我们想要标记红色节点🔴给定锚点和几种关系类型。</p><ol class=""><li id="5b9b" class="od oe iq kq b kr ks ku kv kx of lb og lf oh lj pk oj ok ol bi translated">我们使用BFS找到<em class="lk"> k </em> <strong class="kq ja">个最近的锚</strong>，并按照它们距离<strong class="kq ja">到</strong>的升序排列它们🔴(<em class="lk"> k </em> =20如果足够经常的话)。</li><li id="f5a7" class="od oe iq kq b kr om ku on kx oo lb op lf oq lj pk oj ok ol bi translated">锚<strong class="kq ja">距离</strong>是类似于<em class="lk">位置编码</em>的标量，有助于定位(或“三角测量”)🔴节点，并将其归属于锚节点。类似于位置编码，我们将<em class="lk"> k </em>锚距离嵌入添加到<em class="lk"> k </em>各自最近的锚节点。</li><li id="5a53" class="od oe iq kq b kr om ku on kx oo lb op lf oq lj pk oj ok ol bi translated">在大型kg中，一些“哈希冲突”仍然是可能的，因此第三个成分🧂是节点<strong class="kq ja">关系上下文</strong>的<em class="lk"> m </em>唯一传出关系🔴(注意，向图中添加反向边的标准预处理步骤非常有帮助，因为我们保持每个节点的完全可达性)。根据公斤的大小和密度，我们将m从3到15不等。</li><li id="2fb6" class="od oe iq kq b kr om ku on kx oo lb op lf oq lj pk oj ok ol bi translated">现在，我们有了一个<em class="lk"> k+m </em>令牌序列，<strong class="kq ja">唯一地标识</strong>任何节点。任何内射池(像MLP或Transformer这样的序列编码器)都可以用来将<em class="lk"> k+m </em>序列编码成唯一的嵌入向量，这就是我们一直在寻找的节点嵌入。</li></ol><p id="df00" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">🔧让我们用维基百科的术语来标记<a class="ae ll" href="https://www.wikidata.org/wiki/Q937" rel="noopener ugc nofollow" target="_blank">阿尔伯特·爱因斯坦</a>，看看这个序列会是什么样子。假设我们已经预先选择了一些锚，我们可以使用三个最接近的:<code class="fe pl pm pn po b">Ulm</code>、<code class="fe pl pm pn po b">Nobel Prize</code>、<code class="fe pl pm pn po b">Theoretical Physics</code>。所有这些都是在爱因斯坦的1跳邻域中找到的，所以它们的锚距离是1。作为关系上下文，我们随机抽取4种独特的传出关系类型，例如，<code class="fe pl pm pn po b">place of birth</code>、<code class="fe pl pm pn po b">award received</code>、<code class="fe pl pm pn po b">occupation</code>、<code class="fe pl pm pn po b">academic degree</code>。因此，我们的<em class="lk">爱因斯坦实体</em>是用7个原子🧱.表征的自然地，KG越大，你想要使用的锚就越多，以确保序列更独特，尽管我们发现即使在百万节点图中，每个节点也有一个大约20个锚的饱和点。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi pp"><img src="../Images/172fe630191f5cafb208c0d498566bed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e9d3iJAx7R0r7piSYDWOzQ.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">阿尔伯特·爱因斯坦可能的“记号化”将乌尔姆、诺贝尔奖和理论物理作为KG中的锚节点和外向关系的子集。谓词有Wikidata QIDs。为简洁起见，省略了锚定距离。作者图片</p></figure><p id="3377" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">将编码器函数<em class="lk"> f </em>应用于符号化的序列，并且我们重建爱因斯坦的唯一节点嵌入。</p><p id="c9ca" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">📩这种节点嵌入可以在任何下游任务中发送到您最喜欢的模型，例如，节点分类、链接预测或关系预测，或者在直推或归纳设置中的其他东西。</p><h2 id="874d" class="pq mt iq bd mu pr ps dn my pt pu dp nc kx pv pw ne lb px py ng lf pz qa ni iw bi translated">NodePiece的直接好处是什么？</h2><ul class=""><li id="4ae6" class="od oe iq kq b kr ox ku oy kx qb lb qc lf qd lj oi oj ok ol bi translated">戏剧词汇和嵌入大小缩减📉:仅<em class="lk"> |A| </em>锚节点，而非所有<em class="lk"> |E| </em>节点。根据不同的任务，我们以10倍、100倍、1000倍的缩减率进行实验，并且仍然观察到竞争结果(更多内容见下文)；</li><li id="ee60" class="od oe iq kq b kr om ku on kx oo lb op lf oq lj oi oj ok ol bi translated">节省的参数预算现在可以投入到一个表达性编码器中，该编码器构建节点表示；</li><li id="ace0" class="od oe iq kq b kr om ku on kx oo lb op lf oq lj oi oj ok ol bi translated"><strong class="kq ja">感应</strong>开箱即用的能力！任何附加到可见图的新的不可见输入节点都可以使用相同的词汇进行“标记化”,并通过相同的编码器进行编码。也就是说，没有必要为此发明复杂的方法或评分函数——您仍然可以使用TransE/ComplEx/RotatE/(put your fav model)作为评分函数！</li></ul><h2 id="f195" class="pq mt iq bd mu pr ps dn my pt pu dp nc kx pv pw ne lb px py ng lf pz qa ni iw bi translated">实验:一些数字🧪</h2><p id="1131" class="pw-post-body-paragraph ko kp iq kq b kr ox ka kt ku oy kd kw kx oz kz la lb pa ld le lf pb lh li lj ij bi translated">我们进行了大量的实验，包括在FB15k-237和YAGO 3–10等标准基准上进行的直推式和感应式链路预测，但这里让我们专注于特别有趣的现象。</p><p id="9247" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">🤔首先，如果我们将词汇量减少10倍会发生什么？也就是说，我们只保留10%的原始节点作为锚(享受10倍的参数缩减)，其余的通过MLP从它们重建。我们使用节点件词汇表和编码器以及几个最新的特定于任务的解码器模型(例如，分别是<strong class="kq ja">关系预测</strong>上的<a class="ae ll" href="https://openreview.net/forum?id=HkgEQnRqYQ" rel="noopener ugc nofollow" target="_blank"> RotatE </a>和<strong class="kq ja">节点分类</strong>上的<a class="ae ll" href="https://arxiv.org/abs/1911.03082" rel="noopener ugc nofollow" target="_blank"> CompGCN </a></p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi qe"><img src="../Images/4745fcc9827e810403b9d2561ae8ffd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FpIHI2APmq9Nx6ztkxwqhg.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">节点件扩充模型的关系预测比较。作者图片</p></figure><p id="b2e4" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在👆关系预测，<em class="lk"> 10x </em>节点词汇缩减实际上在所有数据集上都很有竞争力，在Hits@10方面达到或优于基线。</p><p id="51ca" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">👀消融研究揭示了一个有趣的现象——拥有<strong class="kq ja"> 0个锚</strong>不会影响像FB15k-237和YAGO 3–10这样的关系丰富的图上的性能，所以<em class="lk">根本不需要</em>节点/锚嵌入🚮！对于一个非常好的性能来说，与编码器的关系上下文中的一些关系就足够了。</p><p id="32db" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这是关系预测任务的独特之处吗？🤨</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi qf"><img src="../Images/58e7a408b454bd629cbe3b7384ca59d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5UsfJKq2tnhgFCU6GN07TQ.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">一个节点片增强模型的节点分类比较。作者图片</p></figure><p id="3b3d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">事实证明，并不是！在节点分类中👆(这里是一个大约有500个标签的多类多标签问题)，我们将词汇量减少了大约<strong class="kq ja">1000倍</strong>(46K节点图中有50个锚点)，并观察到了实际的改进📈以及所有可能暗示基线已经被过度参数化的度量。</p><p id="aa27" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">同样，如果我们完全放弃具有<strong class="kq ja"> 0锚</strong>的节点嵌入词汇表，数字会稍微好一点。也就是说，对于节点分类来说，仅仅一个关系上下文似乎就足够了！</p><p id="448a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这一发现很好地符合最近开始的研究趋势:</p><blockquote class="qg qh qi"><p id="61d3" class="ko kp lk kq b kr ks ka kt ku kv kd kw qj ky kz la qk lc ld le ql lg lh li lj ij bi translated">在KG表征学习中，关系的连续性仍然被低估。</p></blockquote><p id="5b66" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">kg是多关系图，这一点尚未得到充分利用。我们认为在不久的将来会出现更多的关系感知编码方法(它们是1跳上下文、关系路径或新的<a class="ae ll" href="https://arxiv.org/abs/2106.06935" rel="noopener ugc nofollow" target="_blank">神经贝尔曼-福特框架</a>)。</p></div><div class="ab cl ml mm hu mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="ij ik il im in"><h1 id="6038" class="ms mt iq bd mu mv mw mx my mz na nb nc kf nd kg ne ki nf kj ng kl nh km ni nj bi translated">外卖和资源</h1><ul class=""><li id="2ac4" class="od oe iq kq b kr ox ku oy kx qb lb qc lf qd lj oi oj ok ol bi translated">类似于“标记化”的实体组合编码对kg有效！</li><li id="ef2d" class="od oe iq kq b kr om ku on kx oo lb op lf oq lj oi oj ok ol bi translated">组合编码在设计上是归纳性的——我们可以从<strong class="kq ja">有限的</strong>词汇表中构建出<strong class="kq ja">无限的</strong>数量的组合(实体)。</li><li id="0f62" class="od oe iq kq b kr om ku on kx oo lb op lf oq lj oi oj ok ol bi translated">词汇减少允许将更多的参数投入到强大的编码器中。</li><li id="8530" class="od oe iq kq b kr om ku on kx oo lb op lf oq lj oi oj ok ol bi translated">节点令牌化可以增强任何现有的下游KG任务。</li><li id="f1d4" class="od oe iq kq b kr om ku on kx oo lb op lf oq lj oi oj ok ol bi translated">几乎没有性能下降:整体性能水平与大得多的浅嵌入模型相当。</li><li id="2a78" class="od oe iq kq b kr om ku on kx oo lb op lf oq lj oi oj ok ol bi translated">在关系丰富的图中，对于某些任务来说，仅仅几个与<strong class="kq ja"> 0 </strong>锚的关系就足够了。</li></ul><p id="50bb" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">纸张</strong>:<a class="ae ll" href="https://arxiv.org/abs/2106.12144" rel="noopener ugc nofollow" target="_blank">Arxiv上的预打印</a> <br/> <strong class="kq ja">代码</strong> : <a class="ae ll" href="https://github.com/migalkin/NodePiece" rel="noopener ugc nofollow" target="_blank"> Github repo </a></p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div class="gh gi qm"><img src="../Images/7c07ec546f1294290d6d379ee5fab5e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/1*ZSJJ-vk2XBH5hOSMXwlReA.gif"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">是时候吃一块了！资料来源:<a class="ae ll" href="https://giphy.com/gifs/end-full-bAlYQOugzX9sY" rel="noopener ugc nofollow" target="_blank">吉菲</a></p></figure></div></div>    
</body>
</html>