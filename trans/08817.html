<html>
<head>
<title>Accelerate your Hyperparameter optimization with scikit-optimize</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用scikit-optimize加速超参数优化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/accelerate-your-hyperparameter-optimization-with-scikit-optimize-e72426e60bf1?source=collection_archive---------14-----------------------#2021-08-14">https://towardsdatascience.com/accelerate-your-hyperparameter-optimization-with-scikit-optimize-e72426e60bf1?source=collection_archive---------14-----------------------#2021-08-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7493" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">训练另一个模型，为您的模型找到最佳超参数，避免繁琐的网格搜索</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6f4b5c626a8c69923cba510580616978.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3lwsNotm5JtmuJyNWrnh4A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者照片</p></figure><p id="bc43" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">尽管是创建模型的最后阶段之一，超参数优化(“HPO”)可以使好的模型(概括得很好)或难看的过度拟合(在训练数据中表现很好，但在验证集上表现差得多)之间产生很大差异。</p><p id="1b46" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于流行的基于树的模型，如Random Forest、XGBoost或CatBoost，尤其如此。通常，基本模型会严重过度拟合您的数据。另一方面，试图通过在RandomForest中设置一些超参数(如“max_depth”或“max_features ”)来手动增加偏差通常会导致严重的欠拟合。可能的超参数的搜索空间有如此多的维度和值，以至于您需要一种方便的方法来找到最佳点。</p><h1 id="b6b4" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">1.介绍</h1><p id="a9d6" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">最简单的方法是网格搜索——基本上是一种强力方法，使用一组定义的参数和范围的所有可能组合来重新训练模型。这种方法的巨大缺点是，您将花费大部分时间探索效果不佳的参数组合，只有一小部分接近最佳点。</p><p id="7691" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们可以有另一个模型，它评估超参数的每次迭代的结果，并试图将它们向一个方向移动，从而提高基本模型的性能，会怎么样？幸运的是，scikit-optimize (SKOPT)正是这样做的。</p><p id="29ca" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我将演示如何使用scikit启动HPO，使用RandomForest和XGBoost优化，并使用25万德国租金的样本数据来预测租金价格。数据是<a class="ae mr" href="https://www.kaggle.com/corrieaar/apartment-rental-offers-in-germany" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu"> Kaggle </strong> </a>数据集的转换版本，它和用于文章的代码可以在<a class="ae mr" href="https://github.com/Jan-Majewski/Medium_articles/tree/master/05_HOPT" rel="noopener ugc nofollow" target="_blank"> <strong class="la iu"> GitHub上找到。</strong> </a></p><h1 id="57ff" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">2.SKOPT入门</h1><p id="68fc" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">SKOPT通过创建另一个模型，使您的超参数优化变得更加容易，该模型试图通过改变其超参数来最小化您的初始模型损失。我们将首先为一个简单的RandomForestRegressor模型设置HPO。</p><p id="8c85" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，你需要准备三样东西:</p><ul class=""><li id="e551" class="ms mt it la b lb lc le lf lh mu ll mv lp mw lt mx my mz na bi translated"><strong class="la iu">搜索空间</strong></li></ul><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="a3de" class="ng lv it nc b gy nh ni l nj nk">search_space = [<br/>         skopt.space.Integer(4, 12, name='max_depth'),<br/>         skopt.space.Integer(50, 200, name='n_estimators'),<br/>         skopt.space.Integer(5, 20, name='max_features'),<br/>         skopt.space.Real(0.0, 1.0, name='min_impurity_decrease'),<br/>         skopt.space.Categorical(categories = [True, False],name="bootstrap")<br/>         ]</span></pre><p id="9245" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">搜索空间定义了您想要在搜索中探索的超参数以及探索边界。大多数参数要么是整型，要么是实型(浮点型)，要么是分类型。您可以使用skopt.space类为每个参数定义搜索空间。</p><ul class=""><li id="c48d" class="ms mt it la b lb lc le lf lh mu ll mv lp mw lt mx my mz na bi translated"><strong class="la iu">超参数优化参数</strong></li></ul><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="f1c4" class="ng lv it nc b gy nh ni l nj nk">HPO_params = {<br/>              'n_calls':100,<br/>              'n_random_starts':20,<br/>              'base_estimator':'ET',<br/>              'acq_func':'EI',<br/>             }</span></pre><p id="46a8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我知道这有点令人困惑——为什么我们需要另一组参数来寻找最佳超参数？HPO参数定义了我们将用来寻找最佳参数的过程的一些基本属性。</p><ul class=""><li id="1a30" class="ms mt it la b lb lc le lf lh mu ll mv lp mw lt mx my mz na bi translated"><em class="nl"> n_calls </em>定义您想要进行多少次参数迭代</li><li id="c5d1" class="ms mt it la b lb nm le nn lh no ll np lp nq lt mx my mz na bi translated"><em class="nl"> n_random_starts </em>定义在开始寻找最佳点之前，模型将进行随机迭代的次数，以获得搜索空间的更广泛探索。在这种情况下，我们让我们的模型在开始寻找最佳区域之前，随机搜索搜索空间20次迭代</li><li id="023d" class="ms mt it la b lb nm le nn lh no ll np lp nq lt mx my mz na bi translated"><em class="nl"> base_estimator </em>选择用于优化初始模型超参数的模型，“ET”代表额外应力回归量</li><li id="a538" class="ms mt it la b lb nm le nn lh no ll np lp nq lt mx my mz na bi translated"><em class="nl"> acq_func </em>定义了最小化的函数，“EI”意味着我们期望损失度量的减少作为改进</li><li id="2547" class="ms mt it la b lb nm le nn lh no ll np lp nq lt mx my mz na bi translated"><strong class="la iu">设定我们的模型试图最小化的目标</strong></li></ul><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="d3c4" class="ng lv it nc b gy nh ni l nj nk"><a class="ae mr" href="http://twitter.com/skopt" rel="noopener ugc nofollow" target="_blank">@skopt</a>.utils.use_named_args(search_space)<br/>def objective(**params):<br/>    return (evaluator.evaluate_params(model, params))</span></pre><p id="ba2c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们的HPO模型使用目标函数来衡量每次迭代在提高基础模型性能方面的有效性。它将每次迭代中选择的超参数组合作为其输入，并输出我们的基本模型性能(隐藏在evaluator类中)。</p><p id="00dc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们使用@skopt.utils.used_named_args包装器来转换我们的目标函数，以便它接受列表参数(默认由优化器传递),同时保留特性名称。</p><h1 id="f9b7" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">3.设置评估员类别</h1><p id="c2ae" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">需要解释的最后一部分是evaluator类的evaluate_params函数。为了更好的可读性，我创建了一个类来将所有与模型训练和评估相关的代码从HPO中分离出来</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nr ns l"/></div></figure><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="4a6b" class="ng lv it nc b gy nh ni l nj nk">evaluator = Params_Evaluate(X_train, X_val, y_train, y_val)<br/>evaluator.select_model(model = RandomForestRegressor(n_jobs=4))</span></pre><p id="7b1f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们从训练和验证数据开始，选择我们想要评估的模型，然后我们搜索最佳参数集，以最小化我们的验证集上的RMSE。我还在每次迭代后添加了一个print语句，这使得跟踪进度更加容易。</p><h1 id="102e" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated"><strong class="ak"> 4。执行超参数优化</strong></h1><p id="de3f" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">现在，我们已经做好了一切准备，可以用最后一行代码开始搜索最佳超参数了:</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="36f6" class="ng lv it nc b gy nh ni l nj nk">results = skopt.forest_minimize(objective,search_space,**HPO_params)</span></pre><p id="0a00" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了得到结果，我们使用skopt.forest_minimize函数，它使用了我们已经准备好的3个参数——目标、搜索空间和HPO参数。</p><p id="62c1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于在<em class="nl"> HPO参数中定义的迭代次数，</em>在<em class="nl">搜索空间中选择一组参数，</em>将它们提供给<em class="nl">目标</em>函数，该函数使用我们的evaluator.evaluate_params函数来检查这些参数在我们的模型中的表现。由于我在evaluate_params函数中添加了print语句，我们可以跟踪每次迭代之间的进度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/a11a8953234b45a33fc7c0ae53f9d83f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dHKWrcdAGnxzlDdi0Jz9lw.png"/></div></div></figure><h1 id="8a01" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">5.评估结果</h1><p id="b379" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">得到结果可能需要一段时间，因为我们正在训练我们的模型100次，但一旦他们准备好了，就开始有趣了。我们可以使用SKOPT来可视化我们的超参数搜索。</p><p id="07a9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以从评估收敛结果开始，看看我们的模型在每次迭代中的最佳性能是如何提高的</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="c56d" class="ng lv it nc b gy nh ni l nj nk">skopt.plots.plot_convergence(results)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/4b97a0610e0ecddc9e06929bc9c5da35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*_E3LV2v5h-Pbdq5Fd8zxDQ.png"/></div></figure><p id="45f3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们还可以看到我们的搜索空间的哪些区域被更精确地评估了——这意味着它们给出了更好的结果，并且可能接近最优</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="97eb" class="ng lv it nc b gy nh ni l nj nk">skopt.plots.plot_evaluations(results)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/d2cc9a582b299e9dc3fb95e6e5111ea5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U96m37BEUUqUvee0hRh2bw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">每个特征的搜索区域和频率</p></figure><p id="57f1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">查看图表，我们可以看到搜索主要集中在max_depth和max_features最大值附近，这表明我们可以扩大搜索范围以获得更好的结果。</p><p id="e036" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Plot_objective函数允许我们评估与我们的目标相关的超参数之间的相关性</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="513b" class="ng lv it nc b gy nh ni l nj nk">skopt.plots.plot_objective(results)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/b11822e0020f58001b537d2f240fcf35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hwwVds8QJkgvWdsDrdCy1A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">搜索区域和最佳结果</p></figure><p id="5855" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该图证实了对max_depth和max_features参数的高度依赖性，因为所有最佳结果(浅绿色)都集中在它们的顶部边界周围。</p><p id="b2dd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">基于对上述结果的评估，我们可能应该修改我们的search_space以获得更好的结果，但是为了这个练习，我对当前的结果很满意。</p><p id="7810" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以通过调用results.x来获得最佳参数——不幸的是，它们是作为一个没有param_name的列表给出的，我创建了一个helper函数来将其转换成一个更易于阅读的字典。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nr ns l"/></div></figure><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="8718" class="ng lv it nc b gy nh ni l nj nk">best_params = to_named_params(results, search_space)<br/>best_params</span><span id="28a1" class="ng lv it nc b gy nx ni l nj nk">{'max_depth': 12,<br/> 'n_estimators': 137,<br/> 'max_features': 20,<br/> 'min_impurity_decrease': 0.03287287860139721,<br/> 'bootstrap': False}</span></pre><p id="4851" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">将具有最佳参数的模型与初始模型(具有n_estimators=100，max_depth=5，max_features=20的RandomForestRegressor)进行比较，我们可以看到R2得分显著增加:</p><ul class=""><li id="41ba" class="ms mt it la b lb lc le lf lh mu ll mv lp mw lt mx my mz na bi translated">基本型号为0.80</li><li id="cab6" class="ms mt it la b lb nm le nn lh no ll np lp nq lt mx my mz na bi translated">0.87，适用于带有调谐超参数的模型</li></ul><h1 id="4492" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">6.为XGBoost重组工具</h1><p id="8006" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">为新型号重新装备需要3个简单的步骤:</p><ul class=""><li id="6900" class="ms mt it la b lb lc le lf lh mu ll mv lp mw lt mx my mz na bi translated">在评估器类中选择新模型</li></ul><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="13e6" class="ng lv it nc b gy nh ni l nj nk">evaluator.select_model(model =  XGBRegressor())</span></pre><ul class=""><li id="e25c" class="ms mt it la b lb lc le lf lh mu ll mv lp mw lt mx my mz na bi translated">重新定义搜索空间</li></ul><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="5f0d" class="ng lv it nc b gy nh ni l nj nk">search_space_xgb= [<br/>         skopt.space.Integer(4, 5, name='max_depth'),<br/>         skopt.space.Real(0.0, 1.0, name='eta'),<br/>         skopt.space.Real(0.0, 1.0, name='subsample'),<br/>         skopt.space.Categorical(categories = ["gbtree", "dart"],name="booster")<br/>         ]</span></pre><ul class=""><li id="7259" class="ms mt it la b lb lc le lf lh mu ll mv lp mw lt mx my mz na bi translated">重组目标函数，使包装器使用更新的search_space名称</li></ul><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="7139" class="ng lv it nc b gy nh ni l nj nk"><a class="ae mr" href="http://twitter.com/skopt" rel="noopener ugc nofollow" target="_blank">@skopt</a>.utils.use_named_args(search_space_xgb)<br/>def objective(**params):<br/>    return  evaluator.evaluate_params(params)</span></pre><p id="52bc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们准备在同一个数据集上对XGBoost模型进行超参数优化</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="58d5" class="ng lv it nc b gy nh ni l nj nk"><br/>results=skopt.forest_minimize(objective, search_space_xgb,**HPO_params)</span></pre><h1 id="513d" class="lu lv it bd lw lx ly lz ma mb mc md me jz mf ka mg kc mh kd mi kf mj kg mk ml bi translated">7.摘要</h1><p id="d9c1" class="pw-post-body-paragraph ky kz it la b lb mm ju ld le mn jx lg lh mo lj lk ll mp ln lo lp mq lr ls lt im bi translated">SKOPT是我最喜欢的超参数优化工具，它结合了易用性和可视化来分析结果。</p><p id="d83e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该库也是非常通用的，因为我们可以自由地设置我们的目标函数，我们可以使用它来评估任何模型和任何超参数集。</p><p id="57d8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我希望我的技巧可以帮助你尝试SKOPT，这样你就不需要在这些乏味的网格搜索上浪费时间，而是利用另一个模型来找到适合你的超参数最佳点。</p></div></div>    
</body>
</html>