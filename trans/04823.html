<html>
<head>
<title>Practical Privacy with Synthetic Data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">合成数据的实用隐私</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/practical-privacy-f5c68fae770a?source=collection_archive---------48-----------------------#2021-04-26">https://towardsdatascience.com/practical-privacy-f5c68fae770a?source=collection_archive---------48-----------------------#2021-04-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="cdc6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><strong class="ak">实施实际攻击以测量合成数据模型中的无意记忆</strong></h2></div><h1 id="f859" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">TL；DR；</h1><p id="3140" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在本帖中，我们将对合成数据模型实施一次实际攻击，这在Nicholas Carlini等人的<a class="ae lt" href="https://arxiv.org/pdf/1802.08232.pdf" rel="noopener ugc nofollow" target="_blank">秘密分享者:评估和测试神经网络中的非故意记忆</a>中有所描述。艾尔。我们将利用这次攻击来看看<a class="ae lt" href="https://gretel.ai/blog/what-is-synthetic-data" rel="noopener ugc nofollow" target="_blank">合成数据</a>模型如何利用各种神经网络和差分隐私参数设置来保护数据集中的敏感数据和秘密。有一些非常令人惊讶的结果。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi lu"><img src="../Images/9ff68e445c5207269453824186f79610.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t1qhG_FuqYPRvMo8C8DQMA.jpeg"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">特征图像Gretel.ai</p></figure><h1 id="95b1" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">背景</h1><p id="bf74" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">开放数据集和机器学习模型对于知识的民主化非常有价值，并允许开发人员和数据科学家以新的方式试验和使用数据。无论数据集是否包含位置数据、影像(如摄像机视频)甚至电影评论，用于创建这些数据集的基础数据通常包含并基于个人数据。</p><p id="a76f" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">合成数据模型是一种很有前途的技术，用于生成包含与原始数据相同的洞察力和分布的人工数据集，而不包含任何真实的用户数据。潜在的应用包括允许<a class="ae lt" rel="noopener" target="_blank" href="/reducing-ai-bias-with-synthetic-data-7bddc39f290d">医学研究人员在不了解患者的情况下了解一种罕见疾病</a>，或者在不暴露潜在<a class="ae lt" href="https://gretel.ai/blog/automatically-reducing-ai-bias-with-synthetic-data" rel="noopener ugc nofollow" target="_blank">敏感信息的情况下训练私人数据的机器学习模型，并减少偏差</a>。</p><h1 id="f828" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated"><strong class="ak">数据集</strong></h1><p id="f438" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">传统上，差分隐私的成功应用是通过包含高度同质数据的大规模数据集来实现的。例如，<a class="ae lt" href="https://www.census.gov/programs-surveys/decennial-census/2020-census/planning-management/2020-census-data-products.html" rel="noopener ugc nofollow" target="_blank">美国人口普查数据</a>，或者<a class="ae lt" href="https://www.apple.com/privacy/docs/Differential_Privacy_Overview.pdf" rel="noopener ugc nofollow" target="_blank">苹果设备表情符号预测</a>。这些数据集有数以亿计的行，并且维数较低，这使它们成为差分隐私等隐私保护技术的理想候选对象。对于这个例子，我们将使用一个小得多的数据集，该数据集基于电动自行车共享数据的公共源。是什么让这个数据集对我们的数据集感兴趣？它包含敏感的位置数据，这一直被认为是一个正确匿名的<a class="ae lt" href="https://www.nytimes.com/interactive/2019/12/19/opinion/location-tracking-cell-phone.html" rel="noopener ugc nofollow" target="_blank">挑战</a>。其次，我们正在处理一个不到30，000行输入数据的数据集，它更能代表科学家每天处理的典型数据集。</p><p id="346c" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">让我们看看是否可以构建一个合成数据集，它具有与原始数据相同的洞察力，但不使用真实的行程或位置。对于这个例子，我们在洛杉矶地区记录了一天的公共拼车信息，你可以在我们的<a class="ae lt" href="https://github.com/gretelai/gretel-synthetics/blob/master/examples/data/uber_scooter_rides_1day.csv" rel="noopener ugc nofollow" target="_blank"> Github </a>上下载数据集。格式如下:</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi mp"><img src="../Images/efb11ef5bb9e35ac8eceeb347938a872.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jAAmKgADg3Juu7_H5Xk-3w.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">加州洛杉矶为期一天的电动自行车旅行</p></figure><h1 id="d2e3" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated"><strong class="ak">攻击</strong></h1><p id="e0e1" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我们将对<a class="ae lt" href="https://gretel.ai/synthetics" rel="noopener ugc nofollow" target="_blank"> Gretel.ai的数据模型</a>(这将在任何生成语言模型上工作)实施实际攻击，方法是将金丝雀值(与训练集无关的随机生成的字符串)以各种频率随机插入模型的训练数据中。然后，我们可以使用这些数据来生成具有各种神经网络和隐私设置的模型，并测量每个模型记忆和回放金丝雀值的倾向。</p><p id="82a8" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">这些结果与差别隐私提供的保证有何不同？嗯，如果这些攻击通过，我们不能从数学上声称一个模型是私有的。例如，有可能模型已经记住了一些非预期的信息(金丝雀值),这些信息只是还没有被模型重放。也就是说，知道您可以在数据集中插入10到100次金丝雀值，而不会看到它重复出现，这确实为您的模型提供了实际检查，_并且_您对隐私保证的期望似乎是成立的。</p><p id="c884" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">让我们从生成随机秘密插入训练数据开始。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mq mr l"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">生成一组秘密来测试模型记忆</p></figure><p id="c6a7" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">接下来，对定型集进行采样，并添加包含金丝雀值的新记录。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mq mr l"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">将金丝雀值混合到训练数据集中</p></figure><p id="ab89" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">我们可以使用一个简单的助手在生成的数据中搜索秘密字符串的存在。让我们看看我们将在训练数据中插入多少个秘密值。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="mq mr l"/></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">一个在数据帧中搜索我们的秘密字符串的助手</p></figure><h1 id="0d1e" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">进行实验</h1><p id="06fc" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我们准备通过在包含各种数量的金丝雀值的新训练集上训练合成数据模型来运行我们的实验。在下面的实验中，我们用10种不同的配置训练了合成数据模型，并且在训练数据中插入了18到138次鸭翼。然后，我们计算了模型在生成过程中重放的秘密的数量。</p><p id="8b30" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">要亲自尝试，请查看我们的<a class="ae lt" href="https://gist.github.com/zredlined/7692fe2bb8999f933f64e62cf912245e" rel="noopener ugc nofollow" target="_blank">示例笔记本</a>。</p><div class="ms mt gp gr mu mv"><a href="https://colab.research.google.com/gist/zredlined/7692fe2bb8999f933f64e62cf912245e/synthetic-data-uber-differential-privacy.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="mw ab fo"><div class="mx ab my cl cj mz"><h2 class="bd ir gy z fp na fr fs nb fu fw ip bi translated">谷歌联合实验室</h2><div class="nc l"><h3 class="bd b gy z fp na fr fs nb fu fw dk translated">Gretel . ai—colab.research.google.com优步DP实验笔记本</h3></div></div><div class="nd l"><div class="ne l nf ng nh nd ni me mv"/></div></div></a></div><h1 id="dec0" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">检查结果</h1><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ma mb di mc bf md"><div class="gh gi nj"><img src="../Images/3b16e406e6de6501ad4a6aa2e59d3577.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VUXPPEFZL7lE_CMLpzrNAw.png"/></div></div><p class="mg mh gj gh gi mi mj bd b be z dk translated">我们对合成模型实际攻击的结果</p></figure><p id="1f90" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">正如我们可以看到的，在实验0、1和2中没有差分隐私的标准合成数据配置提供了对稀有数据和秘密被学习的合理保护，其中秘密需要在训练数据中出现超过18次以被模型记忆和重放。</p><p id="f82b" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated"><strong class="kz ir">差分隐私，即使epsilon和delta隐私值比理论保证的推荐值高几个数量级，在所有测试配置中很好地防止了鸭翼的记忆</strong>。实验9使用了相对较高的噪声乘数(0.1)和积极的梯度削波(1.5)，这导致在生成的数据中再现了零个鸭翼，但模型精度却大大降低了。</p><p id="3044" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">令我们惊讶的是，在实验6中简单地使用梯度裁剪和最小水平的噪声到优化器中防止了我们的金丝雀值的任何重放(甚至是在数据中重复138次的秘密)，而模型准确性只有很小的损失。</p><h1 id="e478" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated">后续步骤</h1><p id="4ab2" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">隐私工程的挑战之一是在隐私保证与实用性和准确性之间找到正确的平衡。上面的例子特别有趣，因为它们显示了在隐私保护方面的实质性收益，与差分隐私的理论保护所要求的相比，具有远不那么激进的设置。尝试使用不同的参数配置对您自己的数据进行试验，以找到适合您的用例的平衡点。</p><p id="b47d" class="pw-post-body-paragraph kx ky iq kz b la mk jr lc ld ml ju lf lg mm li lj lk mn lm ln lo mo lq lr ls ij bi translated">有自己的用例可以讨论吗？我们很乐意听到你的用例——欢迎在评论中联系我们进行更深入的讨论，或者通过<a class="ae lt" href="mailto:hi@gretel.ai" rel="noopener ugc nofollow" target="_blank"> hi@gretel.ai </a>。</p></div></div>    
</body>
</html>