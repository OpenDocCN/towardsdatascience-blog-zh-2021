<html>
<head>
<title>Decision Tree Algorithm in Python From Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的决策树算法从零开始</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/decision-tree-algorithm-in-python-from-scratch-8c43f0e40173?source=collection_archive---------5-----------------------#2021-04-14">https://towardsdatascience.com/decision-tree-algorithm-in-python-from-scratch-8c43f0e40173?source=collection_archive---------5-----------------------#2021-04-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1590" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用Python编写只使用NumPy和Pandas的流行算法，并解释其中的内容</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/5df3480096be32940d06e1b8fc9bb73d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*7wRAwQjvOGXNUx753F_-Vg.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">决策树架构；按作者分类的图表</p></figure></div><div class="ab cl ku kv hx kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="im in io ip iq"><p id="4f72" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">本文的目的是通过遍历实现该算法的代码，使决策树分类器的所有部分变得清晰。代码只使用了NumPy、Pandas和标准python库。</p><p id="199a" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">完整的代码可以通过<a class="ae lx" href="https://github.com/Eligijus112/decision-tree-python" rel="noopener ugc nofollow" target="_blank">https://github.com/Eligijus112/decision-tree-python</a>访问</p><p id="0147" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">到目前为止，当目标变量是二进制并且特征是数字时，代码创建一个决策树。这完全足以理解算法。</p><p id="3efc" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">用python构建决策树的黄金标准是scikit-learn实现:</p><div class="ly lz gp gr ma mb"><a href="https://scikit-learn.org/stable/modules/tree.html" rel="noopener  ugc nofollow" target="_blank"><div class="mc ab fo"><div class="md ab me cl cj mf"><h2 class="bd iu gy z fp mg fr fs mh fu fw is bi translated">1.10.决策树-scikit-了解0.24.1文档</h2><div class="mi l"><h3 class="bd b gy z fp mg fr fs mh fu fw dk translated">决策树(DTs)是一种用于分类和回归的非参数监督学习方法。目标是…</h3></div><div class="mj l"><p class="bd b dl z fp mg fr fs mh fu fw dk translated">scikit-learn.org</p></div></div><div class="mk l"><div class="ml l mm mn mo mk mp ko mb"/></div></div></a></div><p id="2fc1" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">当我测试我的代码时，我想确保结果与scikit-learn实现相同。</p></div><div class="ab cl ku kv hx kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="im in io ip iq"><p id="fc20" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">本文使用的数据是著名的泰坦尼克号幸存者数据集。我们将使用两个数字变量——乘客的年龄和票价——来预测乘客是否幸存。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/e360146ec842de14ed8b3a3f61bcc264.png" data-original-src="https://miro.medium.com/v2/resize:fit:1282/format:webp/1*x6m1X3M8oRKd2IxabgxbdA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">年龄+票价~生存；按作者分类的图表</p></figure><p id="b8ab" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">目标是创建数字变量的<strong class="ld iu">【最佳】</strong>分割。目测数据，我们可以猜测一个好的分割方法是将数据分成两部分:年龄为10的观察值和年龄≥ 10的观察值:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/0d5ecbb638fcf83fde837324fcdd84ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*ppAoSGEKlS9c10g77fyh2w.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">拆分数据集；按作者分类的图表</p></figure><p id="d53f" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">现在，一些迫在眉睫的问题可能会出现:</p><p id="2b24" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><em class="ms">这是一个好的分裂吗？</em></p><p id="9d2e" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">也许以200英镑的价格分开会更好？</p><p id="2e3c" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><em class="ms">我们如何量化拆分的“好处”？</em></p><p id="e208" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><em class="ms">计算机如何寻找最佳分割？</em></p><p id="0e63" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">所有这些问题都将在本文结束时得到解答。</p></div><div class="ab cl ku kv hx kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="im in io ip iq"><p id="0f80" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">决策树算法(简称DT)是一种机器学习算法，用于在给定一组输入特征的情况下对观察值进行分类。该算法在不同的决策级别创建一组规则，从而优化某个度量。</p><p id="75de" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">目标变量将表示为<strong class="ld iu"> Y = {0，1} </strong>，特征矩阵将表示为<strong class="ld iu"> X. </strong></p><p id="6e78" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">要扩展的关键词:</p><p id="0f6a" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><strong class="ld iu">节点</strong></p><p id="82d5" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><strong class="ld iu">基尼系数</strong>(我们正在优化的指标)</p><p id="c179" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><strong class="ld iu">级别</strong></p><p id="75ee" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><strong class="ld iu">分裂</strong></p></div><div class="ab cl ku kv hx kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="im in io ip iq"><p id="301f" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">一个<strong class="ld iu">节点</strong>是决策树中的构建块。当查看决策树的典型模式(如标题图片中的模式)时，节点是向下连接到其他节点的矩形或气泡。</p><p id="263d" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">每个节点都有以下主要属性:</p><p id="9d6a" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><strong class="ld iu"> <em class="ms">基尼杂质分数</em> </strong></p><p id="2fe6" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><strong class="ld iu"> <em class="ms">观察次数</em> </strong></p><p id="7361" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><strong class="ld iu"> <em class="ms">属于每个二元目标类的观测值的数量。</em>T11】</strong></p><p id="e9ad" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><strong class="ld iu"> <em class="ms">特征矩阵X表示落入节点的观察值。</em>T15】</strong></p><p id="e59c" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">python中的自定义<strong class="ld iu">节点</strong>类(我写的):</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="2b7a" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">决策树的第一个节点叫做<strong class="ld iu"> <em class="ms">根。</em> </strong>树底部的节点叫做<strong class="ld iu"> <em class="ms">树叶</em> </strong>。</p><p id="8a9b" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">如果满足拆分标准，那么每个节点都有两个链接节点:左节点和右节点。例如，一个非常简单的具有一个根和两个叶的决策树可能如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mv"><img src="../Images/963d86fbbcb88a661face9f14db7d1af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V77auK0mjKya6fWdU-ensg.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">示例决策树；按作者分类的图表</p></figure><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="9539" class="nf ng it nb b gy nh ni l nj nk"><strong class="nb iu">n —</strong> number of observations</span><span id="becb" class="nf ng it nb b gy nl ni l nj nk"><strong class="nb iu">y1 —</strong> number of first class elements</span><span id="61c0" class="nf ng it nb b gy nl ni l nj nk"><strong class="nb iu">y2 —</strong> number of second class elements </span><span id="2a48" class="nf ng it nb b gy nl ni l nj nk"><strong class="nb iu">X —</strong> numeric feature for the observations</span></pre><p id="c898" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">如果特征X小于5，则输入将转到左侧节点。大于或等于5的特征值将去往正确的节点。</p></div><div class="ab cl ku kv hx kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="im in io ip iq"><p id="0dc0" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">如上所述，每个节点都有一个GINI杂质分数。为了计算GINI杂质，所需要的只是目标变量在节点中的分布，或者简单地说，节点中有多少Y=1和Y=0观测值。</p><p id="7c3d" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">GINI杂质的正式定义如下:</p><p id="9437" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><em class="ms"> Gini杂质是对从集合中随机选择的元素被错误标记的频率的度量，如果根据子集中标签的分布对其进行随机标记的话。</em></p><p id="8d38" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">代数定义:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi nm"><img src="../Images/08efe17542147d4b95d683f5b78076ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lyWA34J48lu3fTVK-PsEHQ.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">GINI杂质公式；公式取自作者的笔记本</p></figure><p id="4ee3" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">例如，如果在一个节点中有10个观测值是幸存者，5个观测值不是幸存者，则:</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="54a5" class="nf ng it nb b gy nh ni l nj nk">p(survivors) = 10 /(10 + 5) = 0.66..<br/>p(non-survivors) = 5 /(10 + 5) = 0.33..<br/></span></pre><p id="214d" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">因此，GINI杂质可以通过平方这两个数字，将它们相加，然后从1中减去来计算:</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="9341" class="nf ng it nb b gy nh ni l nj nk">gini impurity = 1 - (0.66..^2 + 0.33..^2) = 0.44.. </span></pre><p id="dd7f" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">在二进制情况下，最大基尼系数等于0.5，最小基尼系数等于0。值越低，节点越“纯”。无论我们有多少个观察值，我们都可以根据整个数据集计算其中一个类的份额，并绘制一个关系:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/ac487c80c1f60ea8c2cd47971fea6fa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:802/format:webp/1*FAnsLE0Kc6IrlADMMlooxw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">基尼不纯度与数据集中的类别份额；作者图片</p></figure><p id="a567" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">我们总是寻找GINI杂质最低的分裂。 </p><p id="681a" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><em class="ms">别的不说，最基本的直觉是，一个节点中的一类观测值越多，其杂质就越低。</em></p></div><div class="ab cl ku kv hx kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="im in io ip iq"><p id="9f92" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">level属性定义在创建节点之前进行了多少次拆分。例如，根节点的级别为0，那么左右节点的级别为1，依此类推。</p><p id="c861" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">在自定义节点类中，树的最大深度可以通过超参数<strong class="ld iu"> max_depth来调节。</strong></p></div><div class="ab cl ku kv hx kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="im in io ip iq"><p id="68bc" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">分裂过程是一个过程，其中我们在每个节点中搜索哪个特征和哪个特征值是最好的，以将数据分成两个更小的部分。</p><p id="7e90" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">在分类的情况下，我们希望最大化<strong class="ld iu">基尼增益。</strong>回到之前的例子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi mv"><img src="../Images/963d86fbbcb88a661face9f14db7d1af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V77auK0mjKya6fWdU-ensg.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">示例决策树；按作者分类的图表</p></figure><p id="5916" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">根节点的基尼系数为0.48。左节点得分为0.27，右节点得分为0.5。<strong class="ld iu"> <em class="ms">我们“获得”了多少基尼？</em> </strong></p><p id="022a" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">计算公式如下:</p><p id="803e" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">Gini gain =父节点Gini杂质减去左右节点的Gini杂质的加权平均值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi no"><img src="../Images/8c44d1544c311776096c07b87d95b733.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d-E_mzonG3HqWfgx0nAU2Q.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">基尼增益公式；来自作者的笔记本</p></figure><p id="f993" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">在上面的例子中，插入我们将得到的所有值:</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="0d23" class="nf ng it nb b gy nh ni l nj nk">0.48 — (0.27 * 30 /100 + 0.5 * 70/100) =<strong class="nb iu"> 0.049</strong></span></pre><p id="bae8" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">GINI增益等于0.049。任何正的基尼系数都是一种进步。这意味着我们的决定会使节点更加“纯净”。</p><p id="9683" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">该算法如何在数字列中搜索最佳拆分？</p><p id="04b7" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">对于每个特征，我们对特征值进行排序，并得到两个相邻值的平均值。</p><p id="abbc" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">例如，假设我们的特征1如下:</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="82ea" class="nf ng it nb b gy nh ni l nj nk">feature_1 = [7, 5, 9, 1, 2, 8]</span></pre><p id="3891" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">已排序:</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="3596" class="nf ng it nb b gy nh ni l nj nk">feature_1 = [1, 2, 5, 7, 8, 9]</span></pre><p id="ab1e" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">邻居的方式:</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="c465" class="nf ng it nb b gy nh ni l nj nk">feature_1 = [1.5, 3.5, 6.5, 7.5, 8.5]</span></pre><p id="cecd" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">然后，我们检查来自上述向量的每个值的GINI增益是多少。而且，我们对数据集中的所有要素都执行此操作。</p><p id="6e55" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated"><strong class="ld iu"> <em class="ms">最终分割值和分割特征是具有最高GINI增益的一个。</em>T11】</strong></p></div><div class="ab cl ku kv hx kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="im in io ip iq"><p id="fa0d" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">增长树的自定义节点对象的超参数(将来会添加更多)是<strong class="ld iu"> max_depth: int </strong>和<strong class="ld iu"> min_samples_split: int </strong>变量。</p><p id="578b" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">max_depth整数定义了树应该增长到多深。在max_depth深度处，停止搜索最佳分割特征和分割特征值。</p><p id="22b0" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">min_samples_split整数定义了开始最佳分割搜索时节点中的最小观察次数。例如，如果节点有51个观察值，但min_samples_split = 55，则树的生长停止。</p></div><div class="ab cl ku kv hx kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="im in io ip iq"><p id="2a8d" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">那么，代码是如何工作的呢？</p><p id="a22c" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">首先，读取数据:</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="5fa8" class="nf ng it nb b gy nh ni l nj nk"># Loading data<br/>d = pd.read_csv(‘data/train.csv’)</span><span id="c420" class="nf ng it nb b gy nl ni l nj nk"># Dropping missing values<br/>dtree = d[[‘Survived’, ‘Age’, ‘Fare’]].dropna().copy()</span><span id="16b0" class="nf ng it nb b gy nl ni l nj nk"># Defining the X and Y matrices<br/>Y = dtree[‘Survived’].values<br/>X = dtree[[‘Age’, ‘Fare’]]</span><span id="f9c5" class="nf ng it nb b gy nl ni l nj nk"># Saving the feature list <br/>features = list(X.columns)</span></pre><p id="913d" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">然后我们定义超参数字典。</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="53c5" class="nf ng it nb b gy nh ni l nj nk">hp = {<br/> ‘max_depth’: 3,<br/> ‘min_samples_split’: 50<br/>}</span></pre><p id="4b02" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">然后我们启动根节点:</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="e1b3" class="nf ng it nb b gy nh ni l nj nk">root = Node(Y, X, **hp)</span></pre><p id="0a75" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">主要的树构建函数是<strong class="ld iu"> grow_tree() </strong>函数。</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="000a" class="nf ng it nb b gy nh ni l nj nk">root.grow_tree()</span></pre><p id="712f" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">就是这样！</p><p id="0c25" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">为了查看结果，我们可以调用<strong class="ld iu"> print_tree() </strong>函数。</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="1f81" class="nf ng it nb b gy nh ni l nj nk">root.print_tree()</span></pre><p id="39d4" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">结果是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mw mx di my bf mz"><div class="gh gi np"><img src="../Images/7e40cf54afe4fdac852d7405ef8d1545.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mzzTA_V6Mf-OyAJ74JmOCA.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">全决策树；作者摘录</p></figure><p id="4590" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">从scikit-learn实现中获得的决策树是相同的:</p><pre class="kj kk kl km gt na nb nc nd aw ne bi"><span id="c437" class="nf ng it nb b gy nh ni l nj nk">|--- Fare &lt;= 52.28<br/>|   |--- Fare &lt;= 10.48<br/>|   |   |--- Age &lt;= 32.50<br/>|   |   |   |--- class: 0<br/>|   |   |--- Age &gt;  32.50<br/>|   |   |   |--- class: 0<br/>|   |--- Fare &gt;  10.48<br/>|   |   |--- Age &lt;= 6.50<br/>|   |   |   |--- class: 1<br/>|   |   |--- Age &gt;  6.50<br/>|   |   |   |--- class: 0<br/>|--- Fare &gt;  52.28<br/>|   |--- Age &lt;= 63.50<br/>|   |   |--- Age &lt;= 29.50<br/>|   |   |   |--- class: 1<br/>|   |   |--- Age &gt;  29.50<br/>|   |   |   |--- class: 1<br/>|   |--- Age &gt;  63.50<br/>|   |   |--- class: 0</span></pre><p id="ded5" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">尽管如此，scikit-learn实现输出的信息比我的实现少。</p><p id="8fa0" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">事实证明，最佳的第一次初始拆分是值为52.28的票价特征，而不是值为10的建议年龄特征。</p></div><div class="ab cl ku kv hx kw" role="separator"><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz la"/><span class="kx bw bk ky kz"/></div><div class="im in io ip iq"><p id="e447" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">我写的代码构建了与scikit-learn实现相同的树，并且预测是相同的。但是scikit-learn算法的训练时间要快得多。但是我的目标不是让树长得更快。我的目标是为任何机器学习爱好者编写一个可理解的代码，以便更好地理解正在发生的事情。</p><p id="213e" class="pw-post-body-paragraph lb lc it ld b le lf ju lg lh li jx lj lk ll lm ln lo lp lq lr ls lt lu lv lw im bi translated">如果您发现任何错误或只想添加功能，请在此回购<a class="ae lx" href="https://github.com/Eligijus112/decision-tree-python" rel="noopener ugc nofollow" target="_blank">https://github.com/Eligijus112/decision-tree-python</a>中随意创建一个拉取请求。</p></div></div>    
</body>
</html>