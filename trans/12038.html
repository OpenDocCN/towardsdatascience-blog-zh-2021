<html>
<head>
<title>Understanding How a Gradient Boosted Tree Does Binary Classification</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解梯度增强树如何进行二叉分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-how-a-gradient-boosted-tree-does-binary-classification-c215967600fe?source=collection_archive---------10-----------------------#2021-12-04">https://towardsdatascience.com/understanding-how-a-gradient-boosted-tree-does-binary-classification-c215967600fe?source=collection_archive---------10-----------------------#2021-12-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f953" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用银行券数据集，在 LightGBM 和 R 中从数据到预测的逐步重新计算</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1e3676d47f24b2c13d537f11f32489e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XPqkzZLn1GwfV5PDmtXepA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@eprouzet?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Eric Prouzet </a>在<a class="ae ky" href="https://unsplash.com/s/photos/banknote?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="70b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">几周前，我深入研究了梯度增强模型如何工作。我提出的主要观点是，人们不必对梯度增强模型(如 XGBoost 和 LightGBM)的“黑盒”感到恐惧，理解模型正在做什么实际上是相当简单的。</p><p id="27b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我的例子中，我使用泊松回归作为目标，现在我认为用不同的、<strong class="lb iu">更常用的目标:二元分类</strong>来重复这个练习会很有趣。</p><p id="f347" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我们将:</p><ol class=""><li id="e802" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">训练二元分类模型，</li><li id="0f3a" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">重新计算二进制日志丢失和二进制错误度量，</li><li id="9706" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">检查预测概率和原始模型结果之间的联系，</li><li id="f09b" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">根据梯度和 hessian 公式重新计算模型结果。</li></ol><p id="9374" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们不会深入到每一个小细节，相反，我们将把重点放在二元分类目标的特殊性上。我推荐我之前关于这个话题的<a class="ae ky" rel="noopener" target="_blank" href="/this-weeks-unboxing-gradient-boosted-models-black-box-138c3a0c6d80">帖子</a>作为起点。</p><h1 id="e0e8" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">环境</h1><p id="6070" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">我们将在 LightGBM 包中工作，编程在 r 中完成。(不用说，我们讨论的所有内容都应该适用于 Python，以及其他梯度增强包，如 XGBoost。)</p><p id="decb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">完整脚本请参考我的<a class="ae ky" href="https://github.com/MatePocs/quick_projects/blob/main/lightgbm_binary.R" rel="noopener ugc nofollow" target="_blank"> GitHub </a>。</p><p id="3269" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将要使用的数据是钞票数据集，可以在<a class="ae ky" href="https://archive.ics.uci.edu/ml/datasets/banknote+authentication" rel="noopener ugc nofollow" target="_blank"> UCI 机器学习库</a>上免费获得。该数据包含 1372 张钞票的信息，目标是找出它们是否是伪造的。</p><h1 id="3dcc" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">数据</h1><p id="f8a3" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">假设数据存储在<code class="fe ng nh ni nj b">data</code>文件夹中，为 LightGBM 准备数据的脚本:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="f9d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们有一个 LightGBM 可以使用的<code class="fe ng nh ni nj b">dtrain</code>对象。</p><h1 id="5c7e" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">训练模型</h1><p id="946e" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">我们现在可以这样训练模型:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="7350" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意事项:</p><ul class=""><li id="71f4" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu nm mb mc md bi translated">在现实生活中，<strong class="lb iu">你永远不会训练出这样的分类模型</strong>。我们有连续的特征，这意味着模型可以完美地过度拟合，除非我们用某种形式的正则化来限制它。然而，在这个项目中，我们不关心这个，我们只是想理解计算。</li><li id="42a0" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nm mb mc md bi translated">超参数应该非常简单，除了<code class="fe ng nh ni nj b">sigmoid</code>。这是一个特定于二进制分类的参数，我们将很快看到它是如何在计算中使用的。</li></ul><h1 id="78b7" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">预言</h1><p id="1acf" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">我们将需要使用下面的模型预测，让我们来解决这个问题。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="88c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意事项:</p><ul class=""><li id="da0b" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu nm mb mc md bi translated"><code class="fe ng nh ni nj b">num_iteration</code>定义我们希望包含在预测中的树的数量。通过将它设置为 1，我们在仅一轮后就获得了模型预测的结果。</li><li id="afe1" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nm mb mc md bi translated"><code class="fe ng nh ni nj b">rawscore = TRUE</code>在将原始模型结果转换为预测之前返回它们。在二元分类的情况下，这意味着返回的是对数几率而不是概率。</li><li id="ddf3" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nm mb mc md bi translated">来自分类模型的预测将是概率，而不是实际的标签。现在，我们将通过选择 0.5 作为阈值来进行预测，即 1 的概率超过 0.5 的每个观察值都将被预测为 1。在实际项目中，您会花很多心思来选择正确的阈值。</li></ul><h1 id="9255" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">误差度量</h1><p id="2f36" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">我们在模型中要求两个错误度量:二进制错误和二进制日志丢失。这些指标是在模型训练期间打印出来的。我们也可以使用<code class="fe ng nh ni nj b">lgb.get.eval.result</code>函数来获取它们(用<code class="fe ng nh ni nj b">booster = bst</code>、<code class="fe ng nh ni nj b">data_name = “train”</code>、<code class="fe ng nh ni nj b">eval_name = “binary_logloss”</code>或<code class="fe ng nh ni nj b">eval_name = “binary_error”</code>)。</p><h2 id="0b6a" class="nn mk it bd ml no np dn mp nq nr dp mt li ns nt mv lm nu nv mx lq nw nx mz ny bi translated">二进制错误</h2><p id="0f3c" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">第一轮后的<code class="fe ng nh ni nj b">binary_error</code>度量是<code class="fe ng nh ni nj b">0.08454810</code>，第二轮后是<code class="fe ng nh ni nj b">0.07069971</code>。这只是数据中不正确预测的比率。例如，在第二轮中，这是实际与预测表:</p><pre class="kj kk kl km gt nz nj oa ob aw oc bi"><span id="9350" class="nn mk it nj b gy od oe l of og">   predict label   N<br/>1:       0     0 734<br/>2:       0     1  69<br/>3:       1     0  28<br/>4:       1     1 541</span></pre><p id="28ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以将误差计算为:</p><pre class="kj kk kl km gt nz nj oa ob aw oc bi"><span id="a9cc" class="nn mk it nj b gy od oe l of og"> (69 + 28) / 1372 = 0.07069971</span></pre><p id="1058" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们希望这个度量是一个低值，如果我们让模型训练更多轮次，最终，它将达到 0。(同样，这是因为我们现在没有使用任何规范化。)</p><h2 id="e8cd" class="nn mk it bd ml no np dn mp nq nr dp mt li ns nt mv lm nu nv mx lq nw nx mz ny bi translated">binary_logloss</h2><p id="b674" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">该指标的值在第一轮后为<code class="fe ng nh ni nj b">0.5102119</code>，在第二轮后为<code class="fe ng nh ni nj b">0.3966159</code>。<code class="fe ng nh ni nj b">binary_logloss</code>计算为观察值的单个二元对数损失的负平均值。观察值的二进制对数损失就是目标值为 1 的观察值的预测概率的对数，以及 1 的对数减去目标值为 0 的观察值的预测概率。</p><p id="d7c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以将二进制对数丢失列添加到<code class="fe ng nh ni nj b">data.table</code>中，就像第二轮那样:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="0ff4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后像这样计算 binary_logloss:</p><pre class="kj kk kl km gt nz nj oa ob aw oc bi"><span id="8f1b" class="nn mk it nj b gy od oe l of og">-sum(dt[,binary_logloss_nround2])/dt[,.N] = 0.3966159</span></pre><p id="ac85" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当然，第一轮结果也可以重复同样的过程。</p><h1 id="78b6" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">预测的叶分数</h1><p id="344c" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">让我们来看看叶子分数是如何变成预测的！如果我们在第一轮后打印出独特的预测(原始分数和概率)，我们会得到以下结果:</p><pre class="kj kk kl km gt nz nj oa ob aw oc bi"><span id="2cd9" class="nn mk it nj b gy od oe l of og">   predict_proba_nround1 predict_rawscore_nround1   N<br/>1:             0.3373846               -0.9642444 679<br/>2:             0.3702293               -0.7589048 105<br/>3:             0.5472543                0.2708327  37<br/>4:             0.5905585                0.5232494 551</span></pre><p id="acd9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一列包含实际预测，即概率，第二列是原始分数。</p><h2 id="4edc" class="nn mk it bd ml no np dn mp nq nr dp mt li ns nt mv lm nu nv mx lq nw nx mz ny bi translated">公式</h2><p id="3777" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">为了将原始分数转换为预测值，我们使用所谓的<strong class="lb iu"> sigmoid 函数</strong>:</p><pre class="kj kk kl km gt nz nj oa ob aw oc bi"><span id="c185" class="nn mk it nj b gy od oe l of og">1 / (1 + exp(-0.5232494 * 0.7)) = 0.5905585</span></pre><p id="3eda" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与此相反的是<strong class="lb iu"> logit 函数</strong>:</p><pre class="kj kk kl km gt nz nj oa ob aw oc bi"><span id="0114" class="nn mk it nj b gy od oe l of og">log(0.5905585 / (1 - 0.5905585)) / 0.7 = 0.5232495</span></pre><p id="0cc1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意事项:</p><ul class=""><li id="00a3" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu nm mb mc md bi translated">上面公式中的<code class="fe ng nh ni nj b">0.7</code>是我们在训练模型时设置的<code class="fe ng nh ni nj b">sigmoid</code>参数。它在 LightGBM <a class="ae ky" href="https://lightgbm.readthedocs.io/en/latest/Parameters.html" rel="noopener ugc nofollow" target="_blank">参数文档</a>中被称为“sigmoid”，但是我找不到关于这个参数的任何数学背景。<strong class="lb iu">如果你知道这个参数来自哪里，请给我发消息。嗯，我们知道它的用法，这很了不起！</strong></li></ul><h2 id="34ea" class="nn mk it bd ml no np dn mp nq nr dp mt li ns nt mv lm nu nv mx lq nw nx mz ny bi translated">术语摘要</h2><p id="04f0" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">我发现这里的术语有些混乱，让我快速回顾一下:</p><ul class=""><li id="fe18" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu nm mb mc md bi translated"><strong class="lb iu">概率:</strong> <code class="fe ng nh ni nj b">p</code></li><li id="d097" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nm mb mc md bi translated"><strong class="lb iu">赔率</strong> : <code class="fe ng nh ni nj b">(p/(1–p))</code></li><li id="2f92" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nm mb mc md bi translated"><strong class="lb iu">对数赔率</strong> : <code class="fe ng nh ni nj b">log((p/(1–p)))</code></li></ul><p id="6d7b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">模型中的原始分数是对数比除以 sigmoid 参数。</p><h2 id="a981" class="nn mk it bd ml no np dn mp nq nr dp mt li ns nt mv lm nu nv mx lq nw nx mz ny bi translated">在树形图中查找分数</h2><p id="5c0b" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">这些都很好，但是这些价值实际上来自哪里呢？</p><p id="554a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们查看<code class="fe ng nh ni nj b">lgb.model.dt.tree(bst)</code>表，可以很容易地通过叶子计数得到它们:这些是第一棵树中的值(<code class="fe ng nh ni nj b">tree_index = 0</code>)。打印出树形图表格的一部分，我们得到这个:</p><pre class="kj kk kl km gt nz nj oa ob aw oc bi"><span id="64ef" class="nn mk it nj b gy od oe l of og">   tree_index leaf_index leaf_value leaf_count<br/>1:          0          0  0.5232494        551<br/>2:          0          2 -0.7589048        105<br/>3:          0          1  0.2708327         37<br/>4:          0          3 -0.9642444        679</span></pre><p id="5295" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这与我们之前看到的原始分数一致。</p><h1 id="bc51" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">叶分数计算</h1><p id="f748" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">现在我们知道了指标是如何计算的，分数是从哪里来的，以及分数是如何转换成概率的。我们唯一缺少的是分数本身。</p><h2 id="32e1" class="nn mk it bd ml no np dn mp nq nr dp mt li ns nt mv lm nu nv mx lq nw nx mz ny bi translated">出发点</h2><p id="c87a" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">在模型甚至开始考虑分裂之前，它将计算一个基线预测，所有观察都将从该基线预测开始。</p><p id="53fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">运行<code class="fe ng nh ni nj b">lgb.train</code>功能后，您可以在终端输出中注意到这一行:</p><pre class="kj kk kl km gt nz nj oa ob aw oc bi"><span id="29d4" class="nn mk it nj b gy od oe l of og">[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.444606 -&gt; initscore=-0.317839</span></pre><p id="a1d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们需要的线索差不多就这些了。训练数据中标签的平均值为<code class="fe ng nh ni nj b">0.444606</code>，因此预测将从以下位置开始:</p><pre class="kj kk kl km gt nz nj oa ob aw oc bi"><span id="80c3" class="nn mk it nj b gy od oe l of og">log(0.4446064 / (1 - 0.4446064)) / 0.7 = 0.3178395</span></pre><p id="8668" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我认为这很简单:在做任何分割之前，任何一个单独的观察都有一个属于第一组的<code class="fe ng nh ni nj b">0.4446064</code>概率。哦，<code class="fe ng nh ni nj b">0.7</code>又是那个讨厌的<code class="fe ng nh ni nj b">sigmoid</code>参数。</p><h2 id="643e" class="nn mk it bd ml no np dn mp nq nr dp mt li ns nt mv lm nu nv mx lq nw nx mz ny bi translated">第一轮目标</h2><p id="8496" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">如果我们观察第一棵树，我们会看到它在:</p><ul class=""><li id="e12b" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu nm mb mc md bi translated"><code class="fe ng nh ni nj b">variance = 0.30942</code></li><li id="6d28" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nm mb mc md bi translated"><code class="fe ng nh ni nj b">skewness = 7.60565</code></li><li id="437c" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nm mb mc md bi translated"><code class="fe ng nh ni nj b">curtosis = -4.48625</code></li></ul><p id="c641" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以在第一轮之后，我们会有四组不同的观察，每组都有不同的预测，基于上面的三个分裂。让我们试着重现这些叶子是如何得分的！</p><p id="ba5f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一轮有点特殊，因为所有的观察仍然有相同的起始分数:我们在上一节看到的<code class="fe ng nh ni nj b">0.3178395</code>。</p><p id="f216" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">先说<code class="fe ng nh ni nj b">variance &lt;= 0.30942</code>和<code class="fe ng nh ni nj b">skewness &lt;= 7.60565</code>所在的组。该组有<code class="fe ng nh ni nj b">551</code>个观察值，其中<code class="fe ng nh ni nj b">512</code>的标签为 1，<code class="fe ng nh ni nj b">39</code>的标签为 0。我们还知道这组得到了一个<code class="fe ng nh ni nj b">0.5232494</code>的预测。这就是我们想要复制的数字。</p><h2 id="4682" class="nn mk it bd ml no np dn mp nq nr dp mt li ns nt mv lm nu nv mx lq nw nx mz ny bi translated">第一轮梯度和 Hessian</h2><p id="5bfb" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">我们不能再推迟了，我们必须钻研梯度和黑森计算。你可以从理论的角度试一试，但是我有点困惑。梯度和 hessian 被认为是目标函数相对于预测的一阶和二阶导数，但是我很难将<code class="fe ng nh ni nj b">sigmoid</code>参数放入混合中...</p><p id="2667" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以我认为<strong class="lb iu">最直接的方法就是看一下 LightGBM </strong>的 <a class="ae ky" href="https://github.com/microsoft/LightGBM/blob/d517ba12f2e7862ac533908304dddbd770655d2b/src/objective/binary_objective.hpp" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">源代码</strong> </a> <strong class="lb iu">。我们需要的位在<code class="fe ng nh ni nj b">GetGradients</code>函数中。(一要小心，<code class="fe ng nh ni nj b">label</code>这里跟你原来的目标不一样。对于目标为 1 的观测值，<code class="fe ng nh ni nj b">label</code>为 1，但对于目标为 0 的观测值，<code class="fe ng nh ni nj b">label</code>为-1。为了避免混淆，我将我们想要预测的值称为目标。)</strong></p><p id="5ea8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个术语中，这个分数是当前的预测，将是我们在第一轮所有观察中的<code class="fe ng nh ni nj b">0.3178395</code>。梯度和 hessian 的主要构件是所谓的<code class="fe ng nh ni nj b">response</code>值。</p><ul class=""><li id="3377" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu nm mb mc md bi translated">对于<strong class="lb iu">目标为 1 </strong>的观测值，<code class="fe ng nh ni nj b">response</code>计算如下:</li></ul><pre class="kj kk kl km gt nz nj oa ob aw oc bi"><span id="a94d" class="nn mk it nj b gy od oe l of og">-1 * 0.7 / (1 + exp(1 * 0.7 * -0.317839)) = - 0.3887755</span></pre><ul class=""><li id="bdaf" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu nm mb mc md bi translated">对于<strong class="lb iu">目标为 0 </strong>的观察值，响应计算如下:</li></ul><pre class="kj kk kl km gt nz nj oa ob aw oc bi"><span id="9909" class="nn mk it nj b gy od oe l of og">1 * 0.7 / (1 + exp(-1 * 0.7 * -0.317839)) = 0.3112245</span></pre><p id="1ce8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从这些值中，我们可以很容易地计算出这个组的梯度和 hessian:我们只需将观察值的各个梯度和 hessian 相加。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="af0c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据这些值，新的叶子分数是这样计算的:</p><pre class="kj kk kl km gt nz nj oa ob aw oc bi"><span id="34cf" class="nn mk it nj b gy od oe l of og">- (gradient / hessian) * 0.3 + (-0.317839) = 0.5232497</span></pre><p id="396c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意:</p><ul class=""><li id="fb07" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu nm mb mc md bi translated">上面公式中的<code class="fe ng nh ni nj b">0.3</code>就是<code class="fe ng nh ni nj b">learning_rate</code>。</li><li id="60a9" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nm mb mc md bi translated"><code class="fe ng nh ni nj b">512</code>和<code class="fe ng nh ni nj b">39</code>是受检组中目标值为 1 和 0 的观察次数。</li><li id="b29e" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nm mb mc md bi translated">注意我们是如何将起始共享预测<code class="fe ng nh ni nj b">-0.317839</code>添加到<code class="fe ng nh ni nj b">leaf_score</code>中的。这可能是特定于 LightGBM 的事情…本质上，总分数因此在第一轮中被滚动到叶分数中。在随后的回合中，不需要公式的这一部分。我们将为第 2 轮做一个例子来看看区别。</li><li id="33c2" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nm mb mc md bi translated">如果我们有权重，这些公式会更长。我不想让事情变得不必要的复杂，请参考<a class="ae ky" href="https://github.com/microsoft/LightGBM/blob/d517ba12f2e7862ac533908304dddbd770655d2b/src/objective/binary_objective.hpp" rel="noopener ugc nofollow" target="_blank">源代码</a>进行计算。请记住，这里有两个可能的权重:<code class="fe ng nh ni nj b">label_weight</code>是您对正确预测目标 1 或 0 的重视程度，而<code class="fe ng nh ni nj b">weights[i]</code>是指您通过观察定义的权重。</li></ul><h2 id="698a" class="nn mk it bd ml no np dn mp nq nr dp mt li ns nt mv lm nu nv mx lq nw nx mz ny bi translated">第二轮目标</h2><p id="1e77" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">看看能不能把这个知识转移到第二棵树上。如果我们观察树的结构，我们可以看到，与第一个不同，这个结构是不对称的:第一次分裂的一个节点变成了一片叶子。我们会观察这个群体。通过打印<code class="fe ng nh ni nj b">variance &gt; 0.77605</code>所在组中唯一的<code class="fe ng nh ni nj b">predict_rawscore_nround1</code>和<code class="fe ng nh ni nj b">predict_rawscore_nround2</code>，这就是我们得到的表格(具体如何得到表格，请参考我的<a class="ae ky" href="https://github.com/MatePocs/quick_projects/blob/main/lightgbm_binary.R" rel="noopener ugc nofollow" target="_blank"> GitHub repo </a>):</p><pre class="kj kk kl km gt nz nj oa ob aw oc bi"><span id="8d9e" class="nn mk it nj b gy od oe l of og">   label predict_rawscore_nround1 predict_rawscore_nround2   N<br/>1:     0               -0.9642444               -1.4947746 575<br/>2:     0                0.2708327               -0.2596975   8<br/>3:     1               -0.9642444               -1.4947746  23<br/>4:     1                0.2708327               -0.2596975  20</span></pre><p id="604e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一列和第二列之间的差异是一个常数，<code class="fe ng nh ni nj b">-0.5305303</code>，这就是我们在树形图中看到的该组的叶分数。这就是我们想要复制的数字。</p><h2 id="8872" class="nn mk it bd ml no np dn mp nq nr dp mt li ns nt mv lm nu nv mx lq nw nx mz ny bi translated">第二轮渐变和 Hessian</h2><p id="2023" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">与第一轮计算的唯一区别是，我们不再有一个共同的预测(如上表所示)，因此计算时间有点长。请注意，我按组重新计算一切，以更好地展示正在发生的事情，但从技术上讲，您可以单独计算每个观察值的梯度和 hessian，然后将它们相加。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nk nl l"/></div></figure><p id="3403" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">根据这些值，叶分数值计算如下:</p><pre class="kj kk kl km gt nz nj oa ob aw oc bi"><span id="2155" class="nn mk it nj b gy od oe l of og">- (gradient / hessian) * 0.3 = -0.5305302</span></pre><h1 id="9b0f" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">摘要</h1><p id="619d" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">在这篇文章中，我们重新计算了 LightGBM 在进行二进制分类时计算的指标、得分和预测。</p><p id="1424" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我认为从这样的练习中得到的主要收获是<strong class="lb iu">对超参数如何影响模型训练的更深理解</strong>。对<code class="fe ng nh ni nj b">learning_rate</code>做了什么有一个大概的了解是一回事，自己重新计算这些结果是完全不同的另一回事。</p><p id="f7cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">我不是说像 XGBoost，LightGBM 等型号。在任何方面都很简单。我们没有触及非常重要的一点:分割点是如何确定的。这些决定是最重要的，这些车型在这方面实现的<strong class="lb iu">性能提升确实令人惊讶</strong>。然而，我的观点是，一旦这些分裂点被确定，大部分的工作基本上只是一长串的算法，基于你，用户设置的超参数。</strong></p><div class="oh oi gp gr oj ok"><a href="https://matepocs.medium.com/membership" rel="noopener follow" target="_blank"><div class="ol ab fo"><div class="om ab on cl cj oo"><h2 class="bd iu gy z fp op fr fs oq fu fw is bi translated">加入我的推荐链接-伴侣概念</h2><div class="or l"><h3 class="bd b gy z fp op fr fs oq fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="os l"><p class="bd b dl z fp op fr fs oq fu fw dk translated">matepocs.medium.com</p></div></div><div class="ot l"><div class="ou l ov ow ox ot oy ks ok"/></div></div></a></div></div></div>    
</body>
</html>