<html>
<head>
<title>Transformers, Explained: Understand the Model Behind GPT-3, BERT, and T5</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变形金刚，解释:理解GPT 3，伯特和T5背后的模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/transformers-explained-understand-the-model-behind-gpt-3-bert-and-t5-cdbf3fc8a40a?source=collection_archive---------1-----------------------#2021-05-06">https://towardsdatascience.com/transformers-explained-understand-the-model-behind-gpt-3-bert-and-t5-cdbf3fc8a40a?source=collection_archive---------1-----------------------#2021-05-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/bfb0116e443eceb289e33cd4137ff9b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hLlS1zbkzsbV3U_hlVthcA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">戴尔·马科维茨原创图片</p></figure><h2 id="457e" class="jd je jf bd b dl jg jh ji jj jk jl dk jm translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><p id="b11a" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">你知道那句话吗，<em class="lj">当你有一把锤子时，一切看起来都像钉子？嗯，在机器学习中，似乎我们真的发现了一把神奇的锤子，事实上，对它来说一切都是钉子，它们被称为变形金刚。变形金刚是可以被设计成翻译文本，写<a class="ae lk" href="https://www.gwern.net/GPT-3" rel="noopener ugc nofollow" target="_blank">诗和专栏</a>，甚至<a class="ae lk" href="https://www.wired.com/story/ai-latest-trick-writing-computer-code/" rel="noopener ugc nofollow" target="_blank">生成计算机代码</a>的模型。事实上，我写的许多关于daleonai.com的惊人研究都是建立在变形金刚之上的，比如从基因序列预测蛋白质结构的模型<a class="ae lk" href="https://daleonai.com/how-alphafold-works" rel="noopener ugc nofollow" target="_blank"> AlphaFold 2 </a>，以及强大的自然语言处理(NLP)模型，比如<a class="ae lk" href="https://daleonai.com/how-alphafold-works" rel="noopener ugc nofollow" target="_blank"> GPT-3 </a>，BERT，T5，Switch，Meena等等。你可能会说他们不仅仅是满足了…唉，算了吧。</em></p><p id="9422" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">如果你想在机器学习，特别是NLP方面保持领先，你至少要知道一点关于变形金刚的知识。所以在这篇文章中，我们将谈论它们是什么，它们是如何工作的，以及为什么它们如此有影响力。</p><p id="db8a" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">转换器是一种神经网络体系结构。概括地说，神经网络是一种非常有效的模型，用于分析复杂的数据类型，如图像、视频、音频和文本。但是有不同类型的神经网络针对不同类型的数据进行了优化。例如，为了分析图像，我们通常会使用<a class="ae lk" href="https://en.wikipedia.org/wiki/Convolutional_neural_network" rel="noopener ugc nofollow" target="_blank">卷积神经网络</a>或“CNN”隐约地，它们模仿了人类大脑处理视觉信息的方式。</p><figure class="lm ln lo lp gt is gh gi paragraph-image"><div class="gh gi ll"><img src="../Images/0ec8f56274a80f2d41c02e0f2388b42c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/0*XFxFBu-uVuEd4axc.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated"><em class="lq">卷积神经网络，图片来自Wikicommons的Renanar2。</em></p></figure><p id="eb6a" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">大约从2012年开始，我们已经相当成功地用CNN解决了视觉问题，比如识别照片中的物体、识别人脸和阅读手写数字。但是在很长一段时间里，对于语言任务(翻译、文本摘要、文本生成、命名实体识别等)来说，没有什么可以与之相比的。这很不幸，因为语言是我们人类交流的主要方式。</p><p id="2104" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在2017年推出变形金刚之前，我们使用深度学习来理解文本的方式是一种叫做递归神经网络或RNN的模型，看起来像这样:</p><figure class="lm ln lo lp gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi lr"><img src="../Images/4ee0dbb76ffe2e84a0833480441dad51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*XLnfvLAhp3vg782H.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">来自<em class="lq">维基媒体</em> <a class="ae lk" href="https://commons.wikimedia.org/wiki/User:Ixnay" rel="noopener ugc nofollow" target="_blank"> fdeloche </a>的一张RNN图片。</p></figure><p id="8fd3" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">假设你想把一个句子从英语翻译成法语。RNN会把一个英语句子作为输入，一次处理一个单词，然后依次吐出它们的法语对应物。这里的关键词是“顺序”在语言中，单词的顺序很重要，你不能把它们混在一起。句子:</p><p id="9066" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">"简自找麻烦。"</p><p id="049c" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">意思和这句话很不一样:</p><p id="e6d1" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">“荷生去找简了”</p><p id="1017" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">因此，任何理解语言的模型都必须捕捉词序，递归神经网络通过按顺序一次处理一个词来做到这一点。</p><p id="6bb2" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">但是RNNs有问题。首先，他们很难处理大型文本序列，比如长段落或短文。当一个段落结束时，他们会忘记开头发生的事情。例如，一个基于RNN的翻译模型可能很难记住一个长段落主题的性别。</p><p id="17c5" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">更糟糕的是，注册护士很难训练。众所周知，他们容易受到所谓的<a class="ae lk" rel="noopener" target="_blank" href="/the-exploding-and-vanishing-gradients-problem-in-time-series-6b87d558d22">消失/爆炸梯度问题</a>(有时你只需重新开始训练并交叉手指)。更有问题的是，因为它们顺序处理单词，rnn很难并行化。这意味着你不能仅仅通过向它们扔更多的GPU来加速训练，这反过来意味着你不能在那么多数据上训练它们。</p><h1 id="bc75" class="ls lt jf bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">输入变压器</h1><p id="73bf" class="pw-post-body-paragraph kl km jf kn b ko mq kq kr ks mr ku kv kw ms ky kz la mt lc ld le mu lg lh li ij bi translated">这就是变形金刚改变一切的地方。它们是由谷歌和多伦多大学的研究人员在2017年开发的，最初旨在做翻译。但与递归神经网络不同，变压器可以非常有效地并行化。这意味着，有了合适的硬件，你可以训练一些真正的大模型。</p><p id="5146" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">多大？</p><p id="5153" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">非常大。</p><p id="d42c" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">GPT-3是一个特别令人印象深刻的文本生成模型，它写得几乎和人类一样好，它是在一些45 TB的文本数据上训练出来的，包括几乎所有的公共网络。</p><p id="dd2b" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">因此，如果你还记得关于变形金刚的任何事情，就让它这样吧:将一个可扩展的模型与一个巨大的数据集结合起来，结果可能会让你大吃一惊。</p><h1 id="7f3f" class="ls lt jf bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">变形金刚是怎么工作的？</h1><figure class="lm ln lo lp gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mv"><img src="../Images/7120896c066db93b6ec0eb940105d872.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*JWBxqi-JS9DclnYW.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated"><em class="lq">变压器图来自原图:</em><a class="ae lk" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1706.03762</a></p></figure><p id="43fa" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">虽然来自<a class="ae lk" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">原始论文</a>的图表有点吓人，但变形金刚背后的创新可以归结为三个主要概念:</p><ol class=""><li id="3c7b" class="mw mx jf kn b ko kp ks kt kw my la mz le na li nb nc nd ne bi translated">位置编码</li><li id="20d3" class="mw mx jf kn b ko nf ks ng kw nh la ni le nj li nb nc nd ne bi translated">注意力</li><li id="c166" class="mw mx jf kn b ko nf ks ng kw nh la ni le nj li nb nc nd ne bi translated">自我关注</li></ol><h2 id="522b" class="nk lt jf bd lu nl nm dn ly nn no dp mc kw np nq mg la nr ns mk le nt nu mo jl bi translated">位置编码</h2><p id="b028" class="pw-post-body-paragraph kl km jf kn b ko mq kq kr ks mr ku kv kw ms ky kz la mt lc ld le mu lg lh li ij bi translated">让我们从第一个开始，位置编码。假设我们试图将文本从英语翻译成法语。请记住，RNNs，旧的翻译方法，通过顺序处理单词来理解单词顺序。但这也是它们难以并行化的原因。</p><p id="8c59" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">变形金刚通过一种叫做位置编码的创新技术绕过了这个障碍。这个想法是获取输入序列中的所有单词——在这个例子中是一个英语句子——并给每个单词附加一个它的顺序号。所以，你给你的网络输入一个序列:</p><p id="05de" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><code class="fe nv nw nx ny b">[("Dale", 1), ("says", 2), ("hello", 3), ("world", 4)]</code></p><p id="2922" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">从概念上讲，你可以把理解词序的负担从神经网络的结构转移到数据本身。</p><p id="abed" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">首先，在转换器接受任何数据训练之前，它不知道如何解释这些位置编码。但随着模型看到越来越多的句子及其编码的例子，它学会了如何有效地使用它们。</p><p id="bf7a" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我在这里做了一点过度简化——最初的作者使用正弦函数来提出位置编码，而不是简单的整数1、2、3、4——但要点是相同的。将词序存储为数据，而不是结构，你的神经网络变得更容易训练。</p><h2 id="3039" class="nk lt jf bd lu nl nm dn ly nn no dp mc kw np nq mg la nr ns mk le nt nu mo jl bi translated">注意力</h2><p id="27db" class="pw-post-body-paragraph kl km jf kn b ko mq kq kr ks mr ku kv kw ms ky kz la mt lc ld le mu lg lh li ij bi translated">变形金刚的下一个重要部分叫做注意力。</p><p id="1def" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">呵。</p><p id="aff9" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">注意力是一种神经网络结构，这些天你会在机器学习中到处听到。事实上，2017年介绍变形金刚的论文标题并不叫<em class="lj">我们向你呈现变形金刚。</em>取而代之的是叫做<a class="ae lk" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a>。</p><p id="a4bf" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><a class="ae lk" href="https://arxiv.org/pdf/1409.0473.pdf" rel="noopener ugc nofollow" target="_blank">关注</a>是在两年前的2015年，在翻译的语境下引入的。要理解它，请参考原文中的例句:</p><p id="d238" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">欧洲经济区协议于1992年8月签署。</p><p id="70bf" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">现在想象一下，试着把这句话翻译成法语的对等物:</p><p id="7dc9" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">《欧洲经济区协定》于1992年8月签署。</p><p id="09a6" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">尝试翻译这句话的一个糟糕的方法是仔细阅读英语句子中的每个单词，并尝试一次一个单词地吐出它的法语对应词。出于几个原因，这不会很好地工作，但其中一个原因是，法语翻译中的一些单词被颠倒了:在英语中是“欧洲经济区”，但在法语中是“欧洲经济区”。此外，法语是一种带有性别词汇的语言。形容词“économique”和“européenne”必须是阴性形式，以匹配阴性宾语“la zone”</p><p id="cfe7" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">注意力是一种机制，它允许文本模型在决定如何翻译输出句子中的单词时“查看”原始句子中的每个单词。这是来自最初的关注文件的一个很好的可视化:</p><figure class="lm ln lo lp gt is gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/c1f9878b7a35d9e811ec5698341920c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/0*274WDwgRhJTajmiY.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated"><em class="lq">图自论文《联合学习对齐和翻译的神经机器翻译(2015)》，</em>【https://arxiv.org/abs/1409.0473】T2</p></figure><p id="95f5" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">这是一种热图，显示了当模型输出法语句子中的每个单词时，它“注意”了哪里。正如您所料，当模型输出单词“européenne”时，它会同时关注输入单词“European”和“Economic”</p><p id="cd7c" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">模型如何知道在每个时间点应该“注意”哪些单词？这是从训练数据中学到的东西。通过查看数千个法语和英语句子的例子，该模型了解了哪些类型的单词是相互依赖的。它学习如何尊重性别、多元化和其他语法规则。</p><p id="976d" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">自2015年被发现以来，注意力机制一直是自然语言处理的一个非常有用的工具，但在其最初的形式中，它是与递归神经网络一起使用的。因此，2017年《变形金刚》论文的创新部分是完全抛弃了RNNs。这就是为什么2017年的论文被称为“注意力是你所需要的全部”</p><h2 id="a2db" class="nk lt jf bd lu nl nm dn ly nn no dp mc kw np nq mg la nr ns mk le nt nu mo jl bi translated">自我关注</h2><p id="614d" class="pw-post-body-paragraph kl km jf kn b ko mq kq kr ks mr ku kv kw ms ky kz la mt lc ld le mu lg lh li ij bi translated">《变形金刚》的最后一部分(可能也是最有影响力的部分)是注意力的扭曲，叫做“自我关注”</p><p id="75c4" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">我们刚刚谈到的“香草”注意力有助于在英语和法语句子中对齐单词，这对翻译很重要。但是，如果你不是试图翻译单词，而是建立一个理解语言潜在意义和模式的模型——一种可以用来完成任何数量的语言任务的模型——会怎么样呢？</p><p id="ac13" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">总的来说，使神经网络强大、令人兴奋和酷的是，它们经常自动建立对它们接受训练的数据的有意义的内部表示。例如，当你检查视觉神经网络的层时，你会发现一组神经元“识别”边缘、形状，甚至像眼睛和嘴这样的高级结构。根据文本数据训练的模型可能会自动学习词性、语法规则以及单词是否同义。</p><p id="a971" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">神经网络学习的语言的内部表示越好，它在任何语言任务中就越好。事实证明，如果注意力被输入的文本本身所吸引，它会是一种非常有效的方式。</p><p id="0a6a" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">例如，以这两句话为例:</p><p id="ae04" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">"服务员，能给我结账吗？"</p><p id="518f" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">“看来我刚刚让服务器崩溃了。”</p><p id="9076" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">服务器这个词在这里意味着两个非常不同的东西，我们人类可以通过查看周围的单词来轻松消除歧义。自我注意允许神经网络在周围单词的上下文中理解一个单词。</p><p id="03a2" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">因此，当一个模型处理第一句话中的单词“服务器”时，它可能会“注意”单词“检查”，这有助于区分人工服务器和金属服务器。</p><p id="645e" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">在第二句话中，模型可能会注意到单词“崩溃”来确定<em class="lj">这个</em>“服务器”指的是一台机器。</p><p id="5ea4" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">自我关注帮助神经网络消除单词歧义，进行词性标注，实体解析，学习语义角色和<a class="ae lk" href="https://arxiv.org/abs/1905.05950" rel="noopener ugc nofollow" target="_blank">等等</a>。</p><p id="f680" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">所以，我们到了。:变压器，在10，000英尺的高度解释，归结为:</p><p id="a669" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">如果你想要更深入的技术解释，我强烈推荐看看杰伊·阿拉玛的博客文章<a class="ae lk" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">《变形金刚》</a>。</p><h2 id="4345" class="nk lt jf bd lu nl nm dn ly nn no dp mc kw np nq mg la nr ns mk le nt nu mo jl bi translated">变形金刚能做什么？</h2><p id="46d1" class="pw-post-body-paragraph kl km jf kn b ko mq kq kr ks mr ku kv kw ms ky kz la mt lc ld le mu lg lh li ij bi translated">最流行的基于变压器的模型之一被称为BERT，是“来自变压器的双向编码器表示”的缩写它是在我2018年加入谷歌时由谷歌的研究人员推出的，很快就进入了几乎所有的NLP项目——包括<a class="ae lk" href="https://blog.google/products/search/search-language-understanding-bert/" rel="noopener ugc nofollow" target="_blank">谷歌搜索</a>。</p><p id="622c" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">BERT指的不仅仅是模型架构，而是经过训练的模型本身，你可以在这里免费下载和使用。它由谷歌研究人员在大规模文本语料库上训练，已经成为NLP的通用小刀。它可以扩展解决一系列不同的任务，例如:</p><p id="3278" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">-文本摘要</p><p id="dfc2" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">-回答问题</p><p id="625d" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">-分类</p><p id="caea" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">-命名实体解析</p><p id="e348" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">-文本相似性</p><p id="0248" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">-攻击性信息/亵渎检测</p><p id="30a3" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">-了解用户查询</p><p id="16a7" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">-多得多</p><p id="1848" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">BERT证明了你可以在未标记的数据上建立非常好的语言模型，比如从维基百科和Reddit上搜集的文本，然后这些大型“基础”模型可以根据特定领域的数据进行调整，以适应许多不同的用例。</p><p id="34ae" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">最近，由OpenAI创建的模型<a class="ae lk" href="https://daleonai.com/gpt3-explained-fast" rel="noopener ugc nofollow" target="_blank"> GPT-3 </a>以其生成逼真文本的能力震撼了人们的心灵。<a class="ae lk" href="https://ai.googleblog.com/2020/01/towards-conversational-agent-that-can.html" rel="noopener ugc nofollow" target="_blank"> Meena </a>，由Google Research去年推出，是一个基于Transformer的聊天机器人(akhem，“对话代理”)，它可以就几乎任何话题进行引人注目的对话(作者曾经花了20分钟与Meena争论对人类来说意味着什么)。</p><p id="e4c6" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">变形金刚也在自然语言处理之外兴风作浪，通过<a class="ae lk" href="https://magenta.tensorflow.org/music-transformer" rel="noopener ugc nofollow" target="_blank">作曲</a>，<a class="ae lk" href="https://daleonai.com/dalle-5-mins" rel="noopener ugc nofollow" target="_blank">从文本描述生成图像</a>，以及<a class="ae lk" href="https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology" rel="noopener ugc nofollow" target="_blank">预测蛋白质结构</a>。</p><h1 id="d97a" class="ls lt jf bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">如何使用变形金刚？</h1><p id="77d3" class="pw-post-body-paragraph kl km jf kn b ko mq kq kr ks mr ku kv kw ms ky kz la mt lc ld le mu lg lh li ij bi translated">既然你已经被变压器的力量所吸引，你可能想知道如何在你自己的应用中使用它们。没问题。</p><p id="39e4" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">你可以从<a class="ae lk" href="https://tfhub.dev/" rel="noopener ugc nofollow" target="_blank"> TensorFlow Hub </a>下载常见的基于变压器的模型，比如BERT。关于代码教程，请查看我写的这篇关于构建语义语言驱动的应用的文章。</p><p id="f27f" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">但是如果你想真正时髦并且你写Python，我强烈推荐由HuggingFace公司维护的流行的“变形金刚”库。这个平台允许你以一种对开发者非常友好的方式训练和使用大多数当今流行的NLP模型，比如BERT，Roberta，T5，GPT-2。</p><p id="78bd" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated">暂时就这样吧！</p><p id="5676" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><em class="lj">特别感谢路易斯/古斯·古斯塔沃、卡尔·魏因梅斯特和亚历克斯·Ku审阅了本文的初稿！</em></p></div><div class="ab cl oa ob hu oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="ij ik il im in"><p id="db7e" class="pw-post-body-paragraph kl km jf kn b ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li ij bi translated"><em class="lj">原载于2021年5月6日https://daleonai.com</em><em class="lj">T21</em><a class="ae lk" href="https://daleonai.com/transformers-explained" rel="noopener ugc nofollow" target="_blank">。</a></p></div></div>    
</body>
</html>