<html>
<head>
<title>An in-depth EfficientNet tutorial using TensorFlow — How to use EfficientNet on a custom dataset.</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用TensorFlow的深入EfficientNet教程—如何在自定义数据集上使用EfficientNet。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-in-depth-efficientnet-tutorial-using-tensorflow-how-to-use-efficientnet-on-a-custom-dataset-1cab0997f65c?source=collection_archive---------5-----------------------#2021-03-31">https://towardsdatascience.com/an-in-depth-efficientnet-tutorial-using-tensorflow-how-to-use-efficientnet-on-a-custom-dataset-1cab0997f65c?source=collection_archive---------5-----------------------#2021-03-31</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7f5e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用Tensorflow在具有挑战性的Kaggle数据集上训练效率网</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a2fb150a053a794182517e6115d15668.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LuOLJwetbCP4Z0Yw"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">桑德·韦特林在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><blockquote class="kz la lb"><p id="b41c" class="lc ld le lf b lg lh ju li lj lk jx ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">卷积神经网络(ConvNets)通常在固定的资源预算下开发，然后如果有更多的资源可用，则按比例增加以获得更好的准确性。在本文中，我们系统地研究了模型缩放，发现仔细平衡网络深度、宽度和分辨率可以获得更好的性能。基于这一观察，我们提出了一种新的缩放方法，该方法使用简单而高效的复合系数来统一缩放深度/宽度/分辨率的所有维度。我们证明了这种方法在扩展MobileNets和ResNet上的有效性。</p></blockquote><p id="c220" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">来源:<a class="ae ky" href="https://arxiv.org/pdf/1905.11946.pdf" rel="noopener ugc nofollow" target="_blank"> arxiv </a></p><p id="c7d0" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">EfficientNet 是目前最先进的图像分类网络中相当强大的一个。我可以看到它在Kaggle的图像分类比赛中被大量使用，AUC为0.90以上，我想我应该把我们的教程放在这里，因为网上没有那么多。</p><p id="7d44" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">我将不会讨论EfficientNet的理论部分，因为有大量的在线资源，相反，我将讨论编码部分。你可以使用<a class="ae ky" href="https://github.com/lukemelas/EfficientNet-PyTorch" rel="noopener ugc nofollow" target="_blank"> efficientNet-pytorch </a>，然而，我通常发现TensorFlow更快更容易使用。</p><p id="d53a" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">我们将在此使用的数据集是来自Kaggle竞赛<a class="ae ky" href="https://www.kaggle.com/c/vinbigdata-chest-xray-abnormalities-detection" rel="noopener ugc nofollow" target="_blank"> VinBigData </a>的胸部x光数据集。我们将使用512x512图像的调整版本，因为原始图像非常大(2k+)。你可以在这里找到调整后的版本<a class="ae ky" href="https://www.kaggle.com/awsaf49/vinbigdata-512-image-dataset" rel="noopener ugc nofollow" target="_blank"/>。无论如何，本教程的主要目的是让你在自定义数据集上使用它。</p><p id="abf3" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">除了图像之外，我们还有一个数据帧，用于指定每个图像的class_id:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mc"><img src="../Images/c79ed8ee24dd7c334a422912ff0e9d96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Hppumt6jaFAVLENtKokUzw.png"/></div></div></figure><h2 id="f4e0" class="md me it bd mf mg mh dn mi mj mk dp ml lz mm mn mo ma mp mq mr mb ms mt mu mv bi translated">包装和进口</h2><p id="4773" class="pw-post-body-paragraph lc ld it lf b lg mw ju li lj mx jx ll lz my lo lp ma mz ls lt mb na lw lx ly im bi translated">你想做的第一件事就是跑</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="7022" class="md me it nc b gy ng nh l ni nj">!pip  install tensorflow-gpu</span></pre><p id="b0ae" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">这将允许您在GPU上训练您的模型(如果您有一个)。接下来要导入几个包:</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="8bad" class="md me it nc b gy ng nh l ni nj">from tensorflow.keras.applications import * #Efficient Net included here<br/>from tensorflow.keras import models<br/>from tensorflow.keras import layers<br/>from keras.preprocessing.image import ImageDataGenerator<br/>import os<br/>import shutil<br/>import pandas as pd<br/>from sklearn import model_selection<br/>from tqdm import tqdm<br/>from tensorflow.keras import optimizers<br/>import tensorflow as tf</span><span id="2549" class="md me it nc b gy nk nh l ni nj">#Use this to check if the GPU is configured correctly<br/>from tensorflow.python.client import device_lib<br/>print(device_lib.list_local_devices())</span></pre><h2 id="9f62" class="md me it bd mf mg mh dn mi mj mk dp ml lz mm mn mo ma mp mq mr mb ms mt mu mv bi translated">设置效率网络:</h2><blockquote class="kz la lb"><p id="2642" class="lc ld le lf b lg lh ju li lj lk jx ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了更进一步，我们使用神经架构搜索来设计一个新的基线网络，并将其放大以获得一系列模型，称为EfficientNets，它比以前的ConvNets实现了更好的准确性和效率。特别是，我们的EfficientNet-B7在ImageNet上实现了最先进的84.3%的顶级准确性，同时在推理上比现有的最佳ConvNet小8.4倍，快6.1倍。我们的EfficientNets也能很好地进行迁移，并在CIFAR-100 (91.7%)、Flowers (98.8%)和其他3个迁移学习数据集上实现了最先进的准确性，并且参数数量级更少。源代码在https://github . com/tensor flow/TPU/tree/master/models/official/efficient net</p></blockquote><p id="9860" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">来源:<a class="ae ky" href="https://arxiv.org/pdf/1905.11946.pdf" rel="noopener ugc nofollow" target="_blank"> arxiv </a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/c21ca30c27fde9b4a97e3be64ad98784.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mNsaNzPfITcr4sev"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@lucabravo?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">卢卡·布拉沃</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="64ab" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">好了，接下来我们需要做的是建立一个有效的网络，并安装预先训练好的砝码</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="5d99" class="md me it nc b gy ng nh l ni nj"># Options: EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3, ... up to  7<br/># Higher the number, the more complex the model is. and the larger resolutions it  can handle, but  the more GPU memory it will need</span><span id="cf04" class="md me it nc b gy nk nh l ni nj"># loading pretrained conv base model</span><span id="c37f" class="md me it nc b gy nk nh l ni nj">#input_shape is (height, width, number of channels) for images<br/>conv_base = EfficientNetB6(weights="imagenet", include_top=False, input_shape=input_shape)</span></pre><p id="de8e" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">Weights="imagenet "允许我们进行迁移学习，但如果您愿意，可以将其设置为None(您可能不应该这样做)。include_top=False允许我们轻松地将最终图层更改为自定义数据集。</p><p id="806b" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">安装模型后，我们希望做一点配置，使它适合我们的自定义数据集:</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="7a35" class="md me it nc b gy ng nh l ni nj">model = models.Sequential()<br/>model.add(conv_base)<br/>model.add(layers.GlobalMaxPooling2D(name="gap"))<br/></span><span id="f008" class="md me it nc b gy nk nh l ni nj">#avoid overfitting<br/>model.add(layers.Dropout(dropout_rate=0.2, name="dropout_out"))<br/></span><span id="a4ed" class="md me it nc b gy nk nh l ni nj"># Set NUMBER_OF_CLASSES to the number of your final predictions.<br/>model.add(layers.Dense(NUMBER_OF_CLASSES, activation="softmax", name="fc_out"))<br/>conv_base.trainable = False</span></pre><h2 id="472f" class="md me it bd mf mg mh dn mi mj mk dp ml lz mm mn mo ma mp mq mr mb ms mt mu mv bi translated">准备数据集:</h2><p id="bfb5" class="pw-post-body-paragraph lc ld it lf b lg mw ju li lj mx jx ll lz my lo lp ma mz ls lt mb na lw lx ly im bi translated">模型准备好了。现在我们需要准备数据集。我们将使用flow_from_directory和Keras的<a class="ae ky" href="https://keras.io/api/preprocessing/image/" rel="noopener ugc nofollow" target="_blank"> ImageDataGenerator </a>。此方法需要培训和验证目录。在每个目录中，每个类都应该有一个单独的目录，该目录下有相应的图像。</p><p id="2205" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">首先，让我们为培训和验证下的每个类创建一个目录。</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="2788" class="md me it nc b gy ng nh l ni nj">TRAIN_IMAGES_PATH = './vinbigdata/images/train' #12000<br/>VAL_IMAGES_PATH = './vinbigdata/images/val' #3000<br/>External_DIR = '../input/vinbigdata-512-image-dataset/vinbigdata/train' # 15000<br/>os.makedirs(TRAIN_IMAGES_PATH, exist_ok = True)<br/>os.makedirs(VAL_IMAGES_PATH, exist_ok = True)</span><span id="06ef" class="md me it nc b gy nk nh l ni nj">classes = [ 'Aortic enlargement',<br/>            'No Finding']</span><span id="c8b5" class="md me it nc b gy nk nh l ni nj"># Create directories for each class.</span><span id="3e45" class="md me it nc b gy nk nh l ni nj">for class_id in [x for x in range(len(classes))]:<br/>    os.makedirs(os.path.join(TRAIN_IMAGES_PATH, str(class_id)), exist_ok = True)<br/>    os.makedirs(os.path.join(VAL_IMAGES_PATH, str(class_id)), exist_ok = True)</span></pre><p id="6a22" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">接下来要做的是将每个图像复制到其正确的目录中:</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="8457" class="md me it nc b gy ng nh l ni nj">Input_dir = '/kaggle/input/vinbigdata-512-image-dataset/vinbigdata/train'</span><span id="9f42" class="md me it nc b gy nk nh l ni nj">def preproccess_data(df, images_path):<br/>    for column, row in tqdm(df.iterrows(), total=len(df)):<br/>        class_id = row['class_id']<br/>        shutil.copy(os.path.join(Input_dir, f"{row['image_id']}.png"), os.path.join(images_path, str(class_id)))</span><span id="d3c3" class="md me it nc b gy nk nh l ni nj">df = pd.read_csv('../input/vinbigdata-512-image-dataset/vinbigdata/train.csv')<br/>df.head()</span><span id="f1b3" class="md me it nc b gy nk nh l ni nj">#Split the dataset into 80% training and 20% validation<br/>df_train, df_valid = model_selection.train_test_split(df, test_size=0.2, random_state=42, shuffle=True)</span><span id="89aa" class="md me it nc b gy nk nh l ni nj">#run the  function on each of them<br/>preproccess_data(df_train, TRAIN_IMAGES_PATH)<br/>preproccess_data(df_valid, VAL_IMAGES_PATH)</span></pre><p id="c5c2" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">现在，您可以检查数据集目录，所有图像都应该复制到它们正确的子目录中。下一步是将数据集传递给生成器，然后开始训练:</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="ba40" class="md me it nc b gy ng nh l ni nj"># I love the  ImageDataGenerator class, it allows us to specifiy whatever augmentations we want so easily...<br/>train_datagen = ImageDataGenerator(<br/>    rescale=1.0 / 255,<br/>    rotation_range=40,<br/>    width_shift_range=0.2,<br/>    height_shift_range=0.2,<br/>    shear_range=0.2,<br/>    zoom_range=0.2,<br/>    horizontal_flip=True,<br/>    fill_mode="nearest",<br/>)</span><span id="cb60" class="md me it nc b gy nk nh l ni nj"># Note that the validation data should not be augmented!</span><span id="153d" class="md me it nc b gy nk nh l ni nj">#and a very important step is to normalise the images through  rescaling<br/>test_datagen = ImageDataGenerator(rescale=1.0 / 255)</span><span id="39e6" class="md me it nc b gy nk nh l ni nj">train_generator = train_datagen.flow_from_directory(<br/>    # This is the target directory<br/>    TRAIN_IMAGES_PATH,<br/>    # All images will be resized to target height and width.<br/>    target_size=(height, width),<br/>    batch_size=batch_size,<br/>    # Since we use categorical_crossentropy loss, we need categorical labels<br/>    class_mode="categorical",<br/>)<br/>validation_generator = test_datagen.flow_from_directory(<br/>    VAL_IMAGES_PATH,<br/>    target_size=(height, width),<br/>    batch_size=batch_size,<br/>    class_mode="categorical",<br/>)<br/>model.compile(<br/>    loss="categorical_crossentropy",<br/>    optimizer=optimizers.RMSprop(lr=2e-5),<br/>    metrics=["acc"],<br/>)</span></pre><p id="37f1" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">如果一切按计划进行，您应该会得到与此类似的消息:</p><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="2dec" class="md me it nc b gy ng nh l ni nj">Found X images belonging to x classes.<br/>Found Y images belonging to x classes.</span></pre><h2 id="5d28" class="md me it bd mf mg mh dn mi mj mk dp ml lz mm mn mo ma mp mq mr mb ms mt mu mv bi translated">训练模型:</h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="fbc1" class="md me it nc b gy ng nh l ni nj">history = model.fit_generator(<br/>    train_generator,<br/>    steps_per_epoch=NUMBER_OF_TRAINING_IMAGES // batch_size,<br/>    epochs=epochs,<br/>    validation_data=validation_generator,<br/>    validation_steps=NUMBER_OF_VALIDATION_IMAGES // batch_size,<br/>    verbose=1,<br/>    use_multiprocessing=True,<br/>    workers=4,<br/>)</span></pre><h2 id="5daa" class="md me it bd mf mg mh dn mi mj mk dp ml lz mm mn mo ma mp mq mr mb ms mt mu mv bi translated">预测</h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="4f21" class="md me it nc b gy ng nh l ni nj">y_pred = model.predict(X_test)<br/>score = model.evaluate(X_test, y_test,verbose=1)</span></pre><h2 id="e279" class="md me it bd mf mg mh dn mi mj mk dp ml lz mm mn mo ma mp mq mr mb ms mt mu mv bi translated">评估:</h2><pre class="kj kk kl km gt nb nc nd ne aw nf bi"><span id="31a6" class="md me it nc b gy ng nh l ni nj"># Import the modules from `sklearn.metrics` <br/>from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score# Confusion matrix </span><span id="25e7" class="md me it nc b gy nk nh l ni nj">confusion_matrix(y_test, y_pred) <br/><br/>precision_score(y_test, y_pred) <br/><br/>recall_score(y_test, y_pred) <br/><br/>f1_score(y_test,y_pred) </span></pre><p id="3a6f" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">下一步是进一步评估模型，有很多资源可以做这件事，因为你可能会对探索大量不同的指标感兴趣，使用Keras应该很容易。</p><p id="254f" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">对本教程的一些可能的改进是通过创建几个折叠，然后组合最终的预测来使用交叉验证。此外，您可以使用更高级的数据增强技术，如混合、分割和抖动。</p></div></div>    
</body>
</html>