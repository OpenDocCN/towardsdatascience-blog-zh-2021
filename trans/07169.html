<html>
<head>
<title>Training T5 for paraphrase generation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于释义生成的训练T5</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/training-t5-for-paraphrase-generation-ab3b5be151a2?source=collection_archive---------14-----------------------#2021-06-29">https://towardsdatascience.com/training-t5-for-paraphrase-generation-ab3b5be151a2?source=collection_archive---------14-----------------------#2021-06-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div class="gh gi jn"><img src="../Images/b289703c2b012e40633a40c401b1c172.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/0*SwOEuUjkNgp0ZOH5.jpg"/></div><p class="ju jv gj gh gi jw jx bd b be z dk translated">使用<a class="ae jy" href="https://imgflip.com/memegenerator" rel="noopener ugc nofollow" target="_blank"> Imgflip </a>生成的图像</p></figure><p id="7788" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">在我之前的博客<a class="ae jy" rel="noopener" target="_blank" href="/textgenie-augmenting-your-text-dataset-with-just-2-lines-of-code-23ce883a0715">中</a>谈到了<a class="ae jy" href="https://github.com/hetpandya/textgenie" rel="noopener ugc nofollow" target="_blank"> TextGenie </a>，我提到了我在从零开始收集文本数据和使用T5(文本到文本转换转换器)生成的释义作为扩充文本数据的方法之一时所面临的问题。看过模型的运行后，让我们来体验一下培训过程😉</p><p id="fccf" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">如果您希望全程跟随我，您可以在我的Github repo上的<a class="ae jy" href="https://github.com/hetpandya/paraphrase-datasets-pretrained-models/blob/main/examples/t5_paraphrase_model_training_example.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到培训笔记本。</p><p id="521b" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated"><strong class="kb ir">提示:</strong>如果你没有GPU，我建议使用<a class="ae jy" href="https://colab.research.google.com" rel="noopener ugc nofollow" target="_blank"> Google Colaboratory </a>来训练模型。</p><h1 id="3bd6" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">安装依赖项</h1><p id="3d17" class="pw-post-body-paragraph jz ka iq kb b kc lv ke kf kg lw ki kj kk lx km kn ko ly kq kr ks lz ku kv kw ij bi translated">在继续之前，让我们准备好所有需要的包，使用:</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="5071" class="mj ky iq mf b gy mk ml l mm mn">pip install simpletransformers datasets tqdm pandas</span></pre><h1 id="76d1" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">资料组</h1><p id="2bea" class="pw-post-body-paragraph jz ka iq kb b kc lv ke kf kg lw ki kj kk lx km kn ko ly kq kr ks lz ku kv kw ij bi translated">我们将使用<a class="ae jy" href="https://huggingface.co/datasets/tapaco" rel="noopener ugc nofollow" target="_blank"> TaPaCo </a>数据集来完成我们的任务。该数据集由73种语言的总共190万个句子组成，我们将从中提取<code class="fe mo mp mq mf b">English</code>语言的句子。</p><h2 id="cc39" class="mj ky iq bd kz mr ms dn ld mt mu dp lh kk mv mw ll ko mx my lp ks mz na lt nb bi translated">预处理数据集(可选)</h2><p id="ad4f" class="pw-post-body-paragraph jz ka iq kb b kc lv ke kf kg lw ki kj kk lx km kn ko ly kq kr ks lz ku kv kw ij bi translated">在将数据集输入模型之前，需要将其转换成成对的输入句子和目标句子。预处理的代码可以在<a class="ae jy" href="https://github.com/hetpandya/paraphrase-datasets-pretrained-models/tree/main/datasets/tapaco#storing-original-dataset-as-csv" rel="noopener ugc nofollow" target="_blank">这里</a>以及笔记本中找到。</p><h2 id="67ba" class="mj ky iq bd kz mr ms dn ld mt mu dp lh kk mv mw ll ko mx my lp ks mz na lt nb bi translated">下载已经预处理的数据集</h2><p id="4756" class="pw-post-body-paragraph jz ka iq kb b kc lv ke kf kg lw ki kj kk lx km kn ko ly kq kr ks lz ku kv kw ij bi translated">如果您不想对数据进行预处理，我已经为您完成了任务。你可以直接从<a class="ae jy" href="https://github.com/hetpandya/paraphrase-datasets-pretrained-models/raw/main/datasets/tapaco/tapaco_paraphrases_dataset.csv" rel="noopener ugc nofollow" target="_blank">这里</a>下载数据集的预处理版本。</p><h2 id="aaaa" class="mj ky iq bd kz mr ms dn ld mt mu dp lh kk mv mw ll ko mx my lp ks mz na lt nb bi translated">加载数据集</h2><p id="88a0" class="pw-post-body-paragraph jz ka iq kb b kc lv ke kf kg lw ki kj kk lx km kn ko ly kq kr ks lz ku kv kw ij bi translated">完成后，您可以按以下方式加载数据集:</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="1c1f" class="mj ky iq mf b gy mk ml l mm mn">import pandas as pd</span><span id="67e1" class="mj ky iq mf b gy nc ml l mm mn">dataset_df = pd.read_csv("tapaco_paraphrases_dataset.csv",sep="\t")</span></pre><p id="44d3" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">加载后，需要重命名数据的列。另外，我们需要给每个句子加一个前缀。这里，前缀可以是作为列添加的任何文本，每行具有相同的值。</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="dfb1" class="mj ky iq mf b gy mk ml l mm mn"># Renaming the columns<br/>dataset_df.columns = ["input_text","target_text"]</span><span id="094a" class="mj ky iq mf b gy nc ml l mm mn"># Adding a prefix. Here we shall keep "paraphrase" as a prefix.<br/>dataset_df["prefix"] = "paraphrase"</span></pre><h2 id="52d0" class="mj ky iq bd kz mr ms dn ld mt mu dp lh kk mv mw ll ko mx my lp ks mz na lt nb bi translated">分割数据集</h2><p id="daa3" class="pw-post-body-paragraph jz ka iq kb b kc lv ke kf kg lw ki kj kk lx km kn ko ly kq kr ks lz ku kv kw ij bi translated">我们将以90%-10%的比例分割数据集</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="1f81" class="mj ky iq mf b gy mk ml l mm mn">from sklearn.model_selection import train_test_split</span><span id="d097" class="mj ky iq mf b gy nc ml l mm mn">train_data,test_data = train_test_split(dataset_df,test_size=0.1)</span></pre><h1 id="a4d1" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">训练模型</h1><p id="240e" class="pw-post-body-paragraph jz ka iq kb b kc lv ke kf kg lw ki kj kk lx km kn ko ly kq kr ks lz ku kv kw ij bi translated">该模型需要调整某些参数，如下所示:</p><figure class="ma mb mc md gt jr"><div class="bz fp l di"><div class="nd ne l"/></div></figure><p id="2462" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">从<code class="fe mo mp mq mf b">simpletransformers</code>初始化<code class="fe mo mp mq mf b">T5Model</code>类对象:</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="a179" class="mj ky iq mf b gy mk ml l mm mn">from simpletransformers.t5 import T5Model<br/>import sklearn</span><span id="c0a4" class="mj ky iq mf b gy nc ml l mm mn">model = T5Model("t5","t5-small", args=args)</span></pre><p id="88f0" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">我们现在将采用<code class="fe mo mp mq mf b">t5-small</code>模式。让我们继续培训:</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="dea2" class="mj ky iq mf b gy mk ml l mm mn">model.train_model(train_data, eval_data=test_data, use_cuda=True,acc=sklearn.metrics.accuracy_score)</span></pre><h1 id="0836" class="kx ky iq bd kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu bi translated">使用训练好的模型进行加载和预测</h1><p id="2a3c" class="pw-post-body-paragraph jz ka iq kb b kc lv ke kf kg lw ki kj kk lx km kn ko ly kq kr ks lz ku kv kw ij bi translated">模型训练可能需要几个小时。一旦培训完成，您可能会在<code class="fe mo mp mq mf b">outputs</code>目录中找到最终的模型。它可以加载为:</p><h2 id="cd0c" class="mj ky iq bd kz mr ms dn ld mt mu dp lh kk mv mw ll ko mx my lp ks mz na lt nb bi translated">加载已训练的模型</h2><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="4021" class="mj ky iq mf b gy mk ml l mm mn">from simpletransformers.t5 import T5Model<br/>import os</span><span id="599b" class="mj ky iq mf b gy nc ml l mm mn">root_dir = os.getcwd()<br/>trained_model_path = os.path.join(root_dir,"outputs")</span><span id="38da" class="mj ky iq mf b gy nc ml l mm mn">args = {<br/>"overwrite_output_dir": True,<br/>"max_seq_length": 256,<br/>"max_length": 50,<br/>"top_k": 50,<br/>"top_p": 0.95,<br/>"num_return_sequences": 5<br/>}</span><span id="bb08" class="mj ky iq mf b gy nc ml l mm mn">trained_model = T5Model("t5",trained_model_path,args=args)</span></pre><h2 id="ca59" class="mj ky iq bd kz mr ms dn ld mt mu dp lh kk mv mw ll ko mx my lp ks mz na lt nb bi translated">使用所训练的模型生成释义</h2><p id="13d9" class="pw-post-body-paragraph jz ka iq kb b kc lv ke kf kg lw ki kj kk lx km kn ko ly kq kr ks lz ku kv kw ij bi translated">让我们看看模型在我们的自定义输入下表现如何:</p><pre class="ma mb mc md gt me mf mg mh aw mi bi"><span id="ef45" class="mj ky iq mf b gy mk ml l mm mn">prefix = "paraphrase"<br/>pred = trained_model.predict([f"{prefix}: The house will be cleaned by me every Saturday."])</span><span id="fa6f" class="mj ky iq mf b gy nc ml l mm mn">print(pred)</span><span id="7077" class="mj ky iq mf b gy nc ml l mm mn">#Output:</span><span id="d4fe" class="mj ky iq mf b gy nc ml l mm mn">[['My home will be cleaned on Saturdays.',   <br/>'I will clean the house every Saturday.',   <br/>'The house is going to be clean every Saturday.',   <br/>"I'll clean the house every Saturday.",   <br/>'I will clean the house every Saturday.']]</span></pre><p id="2257" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">而且很管用！！耶！</p><p id="bfd1" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">T5车型培训到此为止。我已经开源了预训练模型和预处理数据集，以便在我的<a class="ae jy" href="https://github.com/hetpandya/paraphrase-datasets-pretrained-models" rel="noopener ugc nofollow" target="_blank"> Github repo </a>上解释，如果你想探索它们的话。</p><p id="d882" class="pw-post-body-paragraph jz ka iq kb b kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw ij bi translated">感谢您的阅读😄</p></div></div>    
</body>
</html>