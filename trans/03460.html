<html>
<head>
<title>A Journey through XGBoost: Milestone 4</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">XGBoost之旅:里程碑4</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-journey-through-xgboost-milestone-4-303545a40fb0?source=collection_archive---------20-----------------------#2021-03-19">https://towardsdatascience.com/a-journey-through-xgboost-milestone-4-303545a40fb0?source=collection_archive---------20-----------------------#2021-03-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2208" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通过交叉验证评估XGBoost模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/9cf451e90b1e12b3eef6b76a782c60d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dvWa4zyEn_2zitSobFo7-w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">马丁·亚当斯在<a class="ae ky" href="https://unsplash.com/s/photos/drive?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="83a9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你好。这是我们XGBoost旅程的第四个里程碑。到目前为止，我们基本上讨论了用XGBoost执行回归和分类任务。如今，通过交叉验证的XGBoost模型评估得到了更多的重视。还记得在“用XGBoost分类”(里程碑2)的文章中，如果为<strong class="lb iu"> train_test_split() </strong>函数中的<strong class="lb iu"> random_state </strong>参数设置不同的整数值，会得到略有不同的准确率分数吗？今天，我们将通过交叉验证来解决这个问题。</p><p id="c163" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基本上，我们将使用兼容Scikit-learn和不兼容Scikit-learn的API执行交叉验证。但是，最后一件事！在继续之前，请务必阅读以下文章，它们是今天内容的先决条件。那些文章中讨论的概念非常重要，我强烈推荐你阅读它们。</p><h2 id="884b" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">先决条件</h2><ul class=""><li id="efd9" class="mo mp it lb b lc mq lf mr li ms lm mt lq mu lu mv mw mx my bi translated">【XGBoost之旅:里程碑1(设置背景)</li><li id="ff48" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu mv mw mx my bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/a-journey-through-xgboost-milestone-2-f3410109be5a">XGBoost之旅:里程碑2(用XGBoost分类)</a></li><li id="4758" class="mo mp it lb b lc mz lf na li nb lm nc lq nd lu mv mw mx my bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/k-fold-cross-validation-explained-in-plain-english-659e33c0bc0">用简单的英语解释k倍交叉验证</a></li></ul><p id="40ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们开始吧！</p><h2 id="5aff" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">使用Scikit-learn兼容API评估XGBoost模型</h2><p id="814e" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">这里，我们使用相同的“心脏病”数据集(<a class="ae ky" href="https://drive.google.com/file/d/19s5qMRjssBoohFb2NY4FFYQ3YW2eCxP4/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">在此下载</a>)和在“<a class="ae ky" rel="noopener" target="_blank" href="/a-journey-through-xgboost-milestone-2-f3410109be5a">XGBoost之旅:里程碑2(用XGBoost分类)</a>”文章中构建的分类模型。</p><p id="44b4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">“sci kit-learn compatible”</strong>表示这里我们可以使用sci kit-learn<strong class="lb iu">cross _ val _ score()</strong>函数与XGBoost进行交叉验证。</p><p id="9108" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们来写Python代码。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等到加载Python代码！(代码片段-1)</p></figure><p id="d62a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输出是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/95e7b4d678780d9a80ac57a7cab37683.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*XrqGAI8Kjjn7mbkTIzpoxw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">代码片段-1的输出(图片由作者提供)</p></figure><p id="789e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">0.818的平均精度更稳健，是我们模型的良好性能估计。请注意，这里我们执行了10重交叉验证，并为我们的模型获得了10个不同的精度值。</p><p id="d185" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里我们只输入<strong class="lb iu"> X </strong>和<strong class="lb iu"> y </strong>。我们不需要把数据集拆分为<strong class="lb iu"> X_train </strong>、<strong class="lb iu"> y_train </strong>、<strong class="lb iu"> X_test </strong>、<strong class="lb iu"> y_test </strong>。在交叉验证中，分割是根据我们在<strong class="lb iu"> cv </strong>中指定的折叠数在内部完成的(这里是10)。使用交叉验证可以保证我们的XGBoost模型的准确性分数不会受到随机数据分割过程的太大影响。如果我们只使用<strong class="lb iu"> train_test_split() </strong>函数<em class="nk">而不进行</em> t交叉验证，那么根据我们在<strong class="lb iu"> train_test_split() </strong>函数中提供的<strong class="lb iu"> random_state </strong>，准确率得分会有很大的差异。在交叉验证中，使用10次(cv=10)这样迭代的平均值来计算精确度！</p><p id="95fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在k-fold交叉验证中，我们假设数据集中的所有观察值都以数据没有偏差的方式很好地分布。这就是为什么我们首先使用<strong class="lb iu">混洗</strong>函数混洗数据集。</p><p id="61d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们使用XGBoost非Scikit-learn兼容的API执行相同的交叉验证过程。</p><h2 id="5628" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">使用XGBoost非Scikit-learn兼容API评估XGBoost模型</h2><p id="ee5d" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">用XGBoost执行交叉验证的另一种方法是使用XGBoost自己的非Scikit-learn兼容API。<strong class="lb iu">“与Scikit-learn不兼容”</strong>意味着这里我们不使用Scikit-learn<strong class="lb iu">cross _ val _ score()</strong>函数，而是使用XGBoost的<strong class="lb iu"> cv() </strong>函数和显式创建的<strong class="lb iu">d matrix</strong>。A <strong class="lb iu"> DMatrix </strong>是由<strong class="lb iu"> XGBoost </strong>使用的内部数据结构。</p><p id="45c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们来写Python代码。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等到加载Python代码！(代码片段-2)</p></figure><p id="c153" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输出是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/3f79e6f936e675157345beea97ca7a60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1090/format:webp/1*OMdgbwYT2_TBl_zb219BBw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">交叉验证结果(作者图片)</p></figure><p id="8027" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，您将得到一个格式良好的输出，其中包含许多细节。XGBoost的<strong class="lb iu"> cv() </strong>函数返回每个折叠的训练和测试错误。你可以用1个误差得到精度。这里的平均准确度分数与之前的分数非常接近。有一些小的不同，因为在混排数据时涉及到随机性。</p><h1 id="f387" class="nm lw it bd lx nn no np ma nq nr ns md jz nt ka mg kc nu kd mj kf nv kg mm nw bi translated">摘要</h1><p id="908e" class="pw-post-body-paragraph kz la it lb b lc mq ju le lf mr jx lh li ne lk ll lm nf lo lp lq ng ls lt lu im bi translated">今天，我们讨论了交叉验证过程，它可以用来以一种更好的方式评估我们的XGBoost。我们已经用XGBoost的两个不同的API进行了交叉验证。我们在两个API中获得了相同的准确度分数，但是XGBoost自己的非Scikit-learn兼容API获得了更多的细节。</p><p id="51d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在下一篇文章中，我们将讨论如何调优XGBoost的超参数。交叉验证也包括在内。因此，如果你仍然不太熟悉交叉验证，请阅读我的“<a class="ae ky" rel="noopener" target="_blank" href="/k-fold-cross-validation-explained-in-plain-english-659e33c0bc0">用简单英语解释的k重交叉验证</a>”文章。下次见！</p><p id="a31d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读！</p><p id="780d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本教程由<a class="ae ky" href="https://www.linkedin.com/in/rukshan-manorathna-700a3916b/" rel="noopener ugc nofollow" target="_blank"> <em class="nk">鲁克山·普拉莫迪塔</em></a><em class="nk"/><a class="ae ky" href="https://medium.com/data-science-365" rel="noopener">数据科学365博客</a>作者设计创作。</p><p id="a260" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">阅读我在https://rukshanpramoditha.medium.com<a class="ae ky" href="https://rukshanpramoditha.medium.com/" rel="noopener">的其他文章</a></p><p id="f460" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi">2021–03–19</p></div></div>    
</body>
</html>