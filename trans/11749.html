<html>
<head>
<title>The Importance and Reasoning behind Initialisation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">初始化背后的重要性和原因</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-importance-and-reasoning-behind-initialisation-829d8f6e9f43?source=collection_archive---------28-----------------------#2021-11-22">https://towardsdatascience.com/the-importance-and-reasoning-behind-initialisation-829d8f6e9f43?source=collection_archive---------28-----------------------#2021-11-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d10f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">神经网络的一个重要组成部分</h2></div><p id="a4f3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">神经网络通过学习哪些参数最适合数据集来预测输出。为了学习最佳参数，ML工程师必须初始化，然后使用训练数据优化它们。你可能会考虑一些初始化的选项，尽管错误的选择会导致网络变慢甚至无用！</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/f268729f6dee6b60cc75f0f06fdc4917.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*e5tyPhJMTj9ESxQZ"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">布拉登·科拉姆在<a class="ae lr" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><h1 id="ca3d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">为什么我们不能将权重初始化为零</h1><p id="72c9" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">这是一个诱人的选择，尽管它不起作用。如果你将所有的权重初始化为零，或者任何相同的数字，网络的所有节点都以相同的方式运行。下图解释了一个例子。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi mp"><img src="../Images/5872020f54fb5af5bab19dd53c439b3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6P23nU8J3VZ36KULizjVmQ.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">具有相同初始权重的简单3层网络。(图片由作者提供)</p></figure><p id="498e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里，所有的权重和偏差已经分别初始化为相同的w和b。在这种情况下，我们假设任何单层中的激活函数都是相同的。无论一个示例数据点通过什么路径到达输出，它都会产生相同的输出。因此，它将产生相同的成本。在反向传播期间，同一层中的参数将经历相同的梯度下降，并在训练后达到相同的最优值。所以每层中的节点彼此之间没有任何不同！当一个新的数据点通过网络时，无论它采用什么路径，都会产生相同的输出！我们需要“打破节点的对称性”…</p><h1 id="ed02" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">为什么我们不能完全随机地初始化权重</h1><p id="5ba2" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">虽然这将“打破对称”，但它带来了渐变消失和爆炸的问题。每个节点输出的平均值应该为零。我们说我们希望激活函数以零为中心。这在我的文章介绍激活函数中有解释，链接如下。</p><div class="mq mr gp gr ms mt"><a rel="noopener follow" target="_blank" href="/the-importance-and-reasoning-behind-activation-functions-4dc00e74db41"><div class="mu ab fo"><div class="mv ab mw cl cj mx"><h2 class="bd ir gy z fp my fr fs mz fu fw ip bi translated">激活函数背后的重要性和推理</h2><div class="na l"><h3 class="bd b gy z fp my fr fs mz fu fw dk translated">神经网络的一个重要组成部分</h3></div><div class="nb l"><p class="bd b dl z fp my fr fs mz fu fw dk translated">towardsdatascience.com</p></div></div><div class="nc l"><div class="nd l ne nf ng nc nh ll mt"/></div></div></a></div><p id="aa6d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">考虑下面的多层网络。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ni"><img src="../Images/1ae9f2810eae74559dd8fa1b3af437b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NuJ1HZw8CiccQSADJIpFUw.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">作者图片</p></figure><p id="7128" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在产生输出y之前，数据点X必须经过几个节点。如果它所乘以的权重都很大(或都很小)，这将导致最终层的输出非常大(或非常小)。成本的梯度将因此爆炸(或消失)。在爆炸梯度的情况下，成本将永远不会达到它的最小值，因为它将一直错过它并围绕它振荡。这是因为每个梯度下降步骤太大。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/bc8a2695fca968ab8447f4bab65dafb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*M9q0z12pLzwvfb8xkhwd_A.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">爆炸梯度问题。(图片由作者提供)</p></figure><p id="b51f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在梯度消失的情况下，成本永远不会达到最小值，因为接近它的步骤非常小。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/78014adbc13b0ca8bfc4c73ca5a6dfad.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*O91i4E3YvJ4kBmzSF_G9Fw.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">消失梯度问题。(图片由作者提供)</p></figure><h1 id="a48e" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">什么是最常见的初始化技术，为什么</h1><p id="5aff" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">为了防止梯度变得过大或过小并减缓梯度下降，我们需要确保满足两个条件。任何图层输出的平均值为0，并且在整个图层中方差相对恒定。解决方案是随机化权重，但是将它们乘以一个值，该值取决于正在初始化的层中的节点数量。不同的激活功能的值是不同的。</p><h2 id="89e3" class="nl lt iq bd lu nm nn dn ly no np dp mc ko nq nr me ks ns nt mg kw nu nv mi nw bi translated">他初始化</h2><p id="fc67" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">对于具有ReLU或泄漏ReLU激活函数的节点，我们将随机数乘以前一层中节点数量的函数。公式如下所示。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/045ca0bbe4a0f681525ba869e0c4a73c.png" data-original-src="https://miro.medium.com/v2/resize:fit:188/format:webp/1*Da387WpNaVDvPXf-GBigIA.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">作者图片</p></figure><h2 id="29c7" class="nl lt iq bd lu nm nn dn ly no np dp mc ko nq nr me ks ns nt mg kw nu nv mi nw bi translated">Xavier初始化</h2><p id="2bec" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">对于具有Tanh激活函数的节点，我们使用Xavier初始化。这非常类似于初始化，但是我们用随机数乘以的值在分子中是1而不是2。公式如下所示。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/c20b9e77e0abf2bcc9cd45e215034f82.png" data-original-src="https://miro.medium.com/v2/resize:fit:200/format:webp/1*kb-stWHOwL38-keuWe35_A.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">作者图片</p></figure><h1 id="9fac" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">摘要</h1><p id="50b1" class="pw-post-body-paragraph kf kg iq kh b ki mk jr kk kl ml ju kn ko mm kq kr ks mn ku kv kw mo ky kz la ij bi translated">这是对初始化方法的简短介绍。这两种初始化技术保持跨层的方差相同，并且如果我们使用有效的激活函数，我们可以确保每个节点的输出的平均值为零，从而保持梯度下降时间合理。</p></div></div>    
</body>
</html>