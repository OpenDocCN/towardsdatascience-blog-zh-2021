<html>
<head>
<title>Overparameterized but generalized ~ Neural Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">过度参数化但一般化的神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/overparameterized-but-generalized-neural-network-420fe646c54c?source=collection_archive---------20-----------------------#2021-03-29">https://towardsdatascience.com/overparameterized-but-generalized-neural-network-420fe646c54c?source=collection_archive---------20-----------------------#2021-03-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5e5d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">为什么你的深度神经网络在测试数据上表现很好？</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/1d8ba0811c966bd96b5367399eaf7796.png" data-original-src="https://miro.medium.com/v2/resize:fit:1316/format:webp/1*QNWl7hitkR3s1npOw46FwA.jpeg"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">信用:Maximalfocus Unsplash</p></figure><p id="37a3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">深度学习已经从只是参数化的非线性函数发展到用于主要的计算机视觉和自然语言处理任务。分段非线性网络能够形成数据的非平凡表示。尽管这些网络已经非常成功，但是在他们对为什么通过找到问题的近似最优解而表现如此之好的理解之间还有许多差距。</p><blockquote class="ln lo lp"><p id="300e" class="kr ks lq kt b ku kv jr kw kx ky ju kz lr lb lc ld ls lf lg lh lt lj lk ll lm ij bi translated">这些模型给出的训练误差为<em class="iq"> 0，因此<em class="iq">高度过拟合训练数据</em>，但仍然能够给出良好的测试性能。</em></p></blockquote><p id="c9d8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这种良性的过度拟合似乎与公认的统计学智慧相矛盾，统计学智慧坚持在模型的复杂性及其与数据的拟合之间进行<em class="lq">权衡。因此，有必要研究这一有趣的现象来填补空白。</em></p><h2 id="171e" class="lu lv iq bd lw lx ly dn lz ma mb dp mc la md me mf le mg mh mi li mj mk ml mm bi translated">什么是泛化？</h2><p id="d16e" class="pw-post-body-paragraph kr ks iq kt b ku mn jr kw kx mo ju kz la mp lc ld le mq lg lh li mr lk ll lm ij bi translated">假设我们的任务是将一组数据点<em class="lq"> X </em>映射到它们的标签<em class="lq">Y。</em>我们定义一个函数<em class="lq"> f: X- &gt; Y </em>。这里的基本假设是数据是独立同分布的，这意味着每个数据点都是彼此独立随机生成的。</p><p id="49ee" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">选择<em class="lq"> f </em>可以有很多种方式。其中一种方法是让数据分布<em class="lq"> X </em>决定<em class="lq"> f </em>应该是什么，假设这是<em class="lq"> f^ </em>。现在，我们希望<em class="lq"> f^ </em>给出好的预测，这样<em class="lq">f^<em class="lq">的</em>风险</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/ba761b44fefef753d9c495aa1f1ad46d.png" data-original-src="https://miro.medium.com/v2/resize:fit:452/format:webp/1*089eT_QGBGiExE7QgN2vXQ.png"/></div></figure><p id="9e5f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">是最小值。<strong class="kt ir"> <em class="lq">风险</em> </strong>基本上就是损失的<em class="lq">预期。有办法保证这一点吗</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/de21a7e91dac52d16b6a92729d59f52a.png" data-original-src="https://miro.medium.com/v2/resize:fit:276/format:webp/1*4uOxX9LFcBti9BjsyAlP3g.png"/></div></figure><p id="1799" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">对于大样本量n，输出较小。这里<em class="lq"> f* </em>是描述数据的最佳函数。</p><p id="1014" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这给了我们一个衡量我们的模型在未知样本上表现如何的标准。这里需要注意的一点是，如果我们有足够数量的样本，这两项将会收敛。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/b33da757cb2e1f167bc4c8ddf8947c9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*BL-92cHEMaFnAP_qw0yfqw.png"/></div></figure><p id="5508" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">但在现实生活中并非如此。我们需要知道在一个特定的模型上，我们的模型在n个样本的情况下会犯多少错误，因此我们的模型会有多好。</p><blockquote class="ln lo lp"><p id="fed7" class="kr ks lq kt b ku kv jr kw kx ky ju kz lr lb lc ld ls lf lg lh lt lj lk ll lm ij bi translated">泛化为我们提供了一个在没有看到数据的情况下，我们的模型将执行的最高错误数的界限。</p></blockquote><p id="2611" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">你不觉得这对<em class="lq">量产</em>的车型超级重要吗？我们可以找出我们的模型可能犯的最多的错误！</p><h2 id="c506" class="lu lv iq bd lw lx ly dn lz ma mb dp mc la md me mf le mg mh mi li mj mk ml mm bi translated">概括方法</h2><p id="1e17" class="pw-post-body-paragraph kr ks iq kt b ku mn jr kw kx mo ju kz la mp lc ld le mq lg lh li mr lk ll lm ij bi translated">传统的一般化方法采用<strong class="kt ir">模型容量</strong>的观点，其中一般化的能力由假设类<em class="lq"> H </em>的复杂性(容量)来建模。其中假设类是所有可能符合数据的函数<em class="lq"> f </em>(见第一节)。一些传统的方法包括VC维、Rademacher复杂度和PAC-Bayes界。</p><ol class=""><li id="fdf2" class="mv mw iq kt b ku kv kx ky la mx le my li mz lm na nb nc nd bi translated"><strong class="kt ir"> VC dimension- </strong>衡量算法可以学习的一组函数的能力。它由算法能够<em class="lq">粉碎的最大点集来衡量。f </em>被认为分解了一组数据点，如果对于这些点的所有标签分配，存在一个函数使得<em class="lq"> f </em>在对这些点求值时没有错误。</li><li id="306e" class="mv mw iq kt b ku ne kx nf la ng le nh li ni lm na nb nc nd bi translated"><strong class="kt ir"> Rademacher complexity- </strong>一类实值函数丰富性的度量，因此是一个假设类拟合二元标签的能力。</li><li id="8904" class="mv mw iq kt b ku ne kx nf la ng le nh li ni lm na nb nc nd bi translated"><strong class="kt ir">大概正确- </strong>如【3】所述</li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="nk nl di nm bf nn"><div class="gh gi nj"><img src="../Images/e629107a52bbe2708a9976e6c90312d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*58xFQ-MghwcM9yniyGPIyg.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">来自[3]的定义</p></figure><p id="9cd6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">简单来说:PAC的主要目标是分析一个模型是否以及在什么条件下可能输出一个近似正确的分类器。在这里，我们可以通过以下方式定义近似的</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/168a282f193e4d49aedfa34ec6bfc95b.png" data-original-src="https://miro.medium.com/v2/resize:fit:224/format:webp/1*iDpYfBH8CdsXoXR4pPnyww.png"/></div></figure><p id="7068" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">其中D是输入的分布，h是假设。接下来，我们如何定义“大概”？如果我们的模型将以概率1-δ输出这样一个分类器，其中0≤δ≤1/2，我们称该分类器<em class="lq">可能</em>近似正确。知道目标概念是PAC可学习的，允许您限制可能学习近似正确的分类器所需的样本大小。</p><h1 id="199a" class="np lv iq bd lw nq nr ns lz nt nu nv mc jw nw jx mf jz nx ka mi kc ny kd ml nz bi translated">神经网络中的泛化</h1><h2 id="7319" class="lu lv iq bd lw lx ly dn lz ma mb dp mc la md me mf le mg mh mi li mj mk ml mm bi translated"><strong class="ak">实证研究的方法</strong></h2><p id="5dc3" class="pw-post-body-paragraph kr ks iq kt b ku mn jr kw kx mo ju kz la mp lc ld le mq lg lh li mr lk ll lm ij bi translated">社区提出了各种理论和经验结果。[2] <strong class="kt ir">通过系统地改变超参数对40个测量值进行了研究。</strong>他们训练了大约10000个卷积网络，以了解泛化和不同超参数设置之间的关系。他们发现了PAC-Bayesian界限的<strong class="kt ir">潜力</strong>和基于规范的措施的失败。</p><h2 id="e8e1" class="lu lv iq bd lw lx ly dn lz ma mb dp mc la md me mf le mg mh mi li mj mk ml mm bi translated"><strong class="ak">通过随机测试探索普遍性</strong></h2><p id="38e7" class="pw-post-body-paragraph kr ks iq kt b ku mn jr kw kx mo ju kz la mp lc ld le mq lg lh li mr lk ll lm ij bi translated">张等[1]的论文进行了<strong class="kt ir"> </strong>随机测试，用随机标签代替真实标签。理想情况下，模型的性能应该不会很好。但令他们惊讶的是，他们发现~深度神经网络很容易以0的训练误差拟合随机标签。</p><blockquote class="ln lo lp"><p id="4ab0" class="kr ks lq kt b ku kv jr kw kx ky ju kz lr lb lc ld ls lf lg lh lt lj lk ll lm ij bi translated">仅通过随机化标签，我们就可以在不改变模型、其大小、超参数或优化器的情况下，迫使模型的泛化误差大幅上升。</p></blockquote><p id="47c4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">他们还用完全随机的像素(例如高斯噪声)代替真实图像，并观察到卷积神经网络继续以零训练误差拟合数据。<em class="lq">这表明，尽管它们的结构不同，</em> <strong class="kt ir"> <em class="lq">卷积神经网络可以拟合随机噪声。</em> </strong></p><p id="9f34" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">他们进一步改变随机化的量，在无噪声和完全噪声的情况之间平滑地插值，以观察当我们增加噪声水平时泛化误差的稳定恶化。这表明神经网络能够捕捉数据中的剩余信号，同时使用蛮力拟合噪声部分。</p><h2 id="cc24" class="lu lv iq bd lw lx ly dn lz ma mb dp mc la md me mf le mg mh mi li mj mk ml mm bi translated"><strong class="ak">表征学习视角</strong></h2><p id="d315" class="pw-post-body-paragraph kr ks iq kt b ku mn jr kw kx mo ju kz la mp lc ld le mq lg lh li mr lk ll lm ij bi translated">周Z-H[4]对这个问题给出了一个非常不同的视角。他对为什么过度参数化不会过度拟合的回答导致了一个事实-</p><blockquote class="ln lo lp"><p id="491f" class="kr ks lq kt b ku kv jr kw kx ky ju kz lr lb lc ld ls lf lg lh lt lj lk ll lm ij bi translated">深度网络将<strong class="kt ir"> <em class="iq">特征学习</em> </strong>与<strong class="kt ir">分类器训练</strong>相结合。所有传统的学习理论主要关注学习者的训练，或者更具体地说，来自特征空间的分类器的训练，但是很少关注特征空间本身的构造。</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/fad335405a9a92d1205efdc84eb4143a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*x1I4qFXRTVhDWdWb9epG2g.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">深度学习结合了分类器构造和特征学习</p></figure><p id="dda5" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">因此，传统的学习理论可以用来理解概括的行为，<em class="lq">但是当它被应用到表征学习的时候就必须小心了</em>。因此，需要对表征学习的一般化做特殊的工作。</p><h2 id="61a3" class="lu lv iq bd lw lx ly dn lz ma mb dp mc la md me mf le mg mh mi li mj mk ml mm bi translated"><strong class="ak">实证方法</strong></h2><p id="3205" class="pw-post-body-paragraph kr ks iq kt b ku mn jr kw kx mo ju kz la mp lc ld le mq lg lh li mr lk ll lm ij bi translated">NeurIPS竞赛关于<em class="lq">预测深度学习中的泛化(NeurIPS 2020)的获奖方案【5】。</em>声称理论上的界限通常被证明在实际设置中没有用。他们在不查看测试数据的情况下给出了训练数据的泛化错误。他们利用表征的两个特征- <strong class="kt ir">一致性</strong>和<strong class="kt ir">稳健性</strong>来衡量泛化。他们对他们的方法有如下的看法-</p><blockquote class="ln lo lp"><p id="7239" class="kr ks lq kt b ku kv jr kw kx ky ju kz lr lb lc ld ls lf lg lh lt lj lk ll lm ij bi translated">如果深度神经网络将相同的标签分配给两幅图像，它们必须在网络的某个阶段收敛到相似的表示。</p></blockquote><p id="c681" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">测量网络内部表示的一致性将会告诉我们这个网络的泛化能力。</p><blockquote class="ln lo lp"><p id="4116" class="kr ks lq kt b ku kv jr kw kx ky ju kz lr lb lc ld ls lf lg lh lt lj lk ll lm ij bi translated">模型对输入空间中似是而非的扰动的鲁棒性是泛化能力的另一个标志。</p></blockquote><p id="7eb6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">为了测量网络表示对有效或似是而非的扰动的鲁棒性，他们使用了Mixup。<strong class="kt ir">在Mixup中，他们检查相同标签内输入样本线性组合的模型性能</strong>。</p><blockquote class="ln lo lp"><p id="4b0b" class="kr ks lq kt b ku kv jr kw kx ky ju kz lr lb lc ld ls lf lg lh lt lj lk ll lm ij bi translated">样本离被归入另一类有多远。</p></blockquote><p id="2ec0" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">他们证明了<strong class="kt ir">扩大的边际分布作为泛化的度量</strong>，可以考虑过度拟合和异常样本。</p><h2 id="b88b" class="lu lv iq bd lw lx ly dn lz ma mb dp mc la md me mf le mg mh mi li mj mk ml mm bi translated"><strong class="ak">优化视角到泛化</strong></h2><p id="4fe3" class="pw-post-body-paragraph kr ks iq kt b ku mn jr kw kx mo ju kz la mp lc ld le mq lg lh li mr lk ll lm ij bi translated">Google[7]给出了泛化的在线优化视角。他们给出了一个<strong class="kt ir">深度引导框架</strong>，其中他们将数据有限的训练世界与数据无限的理想世界进行了比较。<em class="lq">先验</em>，人们可能期望现实世界和理想世界可能彼此无关，因为在现实世界中，模型从分布中看到有限数量的例子，而在理想世界中，模型看到整个分布。但在实践中，他们发现真实模型和理想模型实际上有相似的测试误差。</p><blockquote class="ln lo lp"><p id="0b0d" class="kr ks lq kt b ku kv jr kw kx ky ju kz lr lb lc ld ls lf lg lh lt lj lk ll lm ij bi translated">在存在无限训练数据(理想世界)和存在有限数据(现实世界)的场景中的训练给出相似的测试误差，直到现实世界收敛。</p></blockquote><p id="247a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">因此，人们可以通过研究模型在理想世界中的相应行为来研究现实世界中的模型。好的模型和训练程序是那些(1) <strong class="kt ir">在理想世界中快速优化</strong>和(2) <strong class="kt ir">在现实世界中不太快速优化</strong>的模型和训练程序。每当一个人做出影响现实世界中的普遍性的改变时(架构、学习速度等)。)，应该考虑它对(1)测试误差的理想世界优化(越快越好)和(2)训练误差的现实世界优化(越慢越好)的影响。</p><h1 id="aeee" class="np lv iq bd lw nq nr ns lz nt nu nv mc jw nw jx mf jz nx ka mi kc ny kd ml nz bi translated">结论</h1><p id="bf31" class="pw-post-body-paragraph kr ks iq kt b ku mn jr kw kx mo ju kz la mp lc ld le mq lg lh li mr lk ll lm ij bi translated">从统计学的角度来看，神经网络中的过度参数化使它们变得有趣。这篇文章简单介绍了传统的测量泛化能力的方法，这些方法并不直接适用于深度学习。已经从表示学习和优化的角度提供了不同的视角来解释神经网络所实现的高测试性能。</p><p id="964c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">[1] <a class="ae ob" href="http://export.arxiv.org/pdf/1611.03530" rel="noopener ugc nofollow" target="_blank">理解深度学习需要重新思考泛化(arxiv.org)</a></p><p id="0b61" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">[2]<a class="ae ob" href="https://arxiv.org/pdf/1912.02178.pdf" rel="noopener ugc nofollow" target="_blank">1912.02178.pdf(arxiv.org)</a></p><p id="bd38" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">[3]<a class="ae ob" href="https://www.cs.princeton.edu/courses/archive/spring14/cos511/scribe_notes/0211.pdf" rel="noopener ugc nofollow" target="_blank">0211.pdf(princeton.edu)</a></p><p id="f305" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">[4]<a class="ae ob" href="https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/sciChina20over.pdf" rel="noopener ugc nofollow" target="_blank">sciChina20over.pdf(nju.edu.cn)</a></p><p id="01d8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">[5]<a class="ae ob" href="https://arxiv.org/abs/2012.02775#:~:text=Title%3ARepresentation%20Based%20Complexity%20Measures%20for%20Predicting%20Generalization%20in,on%20these%20viewpoints%2C%20such%20as%20norm-based%2C%20PAC%20" rel="noopener ugc nofollow" target="_blank">【2012.02775】基于表征的复杂性度量用于预测深度学习中的泛化(arxiv.org)</a></p><p id="4c5e" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">[6]【arxiv.org T2】【2010.08127】深度引导框架:好的在线学习者是好的离线概括者</p><p id="b0be" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">【2103.09177.pdf(arxiv.org)<a class="ae ob" href="https://arxiv.org/pdf/2103.09177.pdf" rel="noopener ugc nofollow" target="_blank"/></p></div></div>    
</body>
</html>