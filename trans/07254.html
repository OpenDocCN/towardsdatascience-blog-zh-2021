<html>
<head>
<title>LSTMs for Music Generation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">音乐生成的LSTMs</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lstms-for-music-generation-8b65c9671d35?source=collection_archive---------29-----------------------#2021-07-01">https://towardsdatascience.com/lstms-for-music-generation-8b65c9671d35?source=collection_archive---------29-----------------------#2021-07-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="845b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">让我们随着人工智能产生的音乐跳舞吧</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/1ea3a9e92e1b31c6072f9a51485632fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7pFwrIo5EuQb8_W3OZ4Zog.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd kv">图片来源:</strong> <a class="ae kw" href="https://www.istockphoto.com/portfolio/AndSus?mediatype=illustration" rel="noopener ugc nofollow" target="_blank">安德烈·苏斯洛夫</a>上<a class="ae kw" href="https://www.istockphoto.com/vector/smart-speaker-iot-gm1072477006-287004793" rel="noopener ugc nofollow" target="_blank">伊斯托克</a></p></figure><p id="8415" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi lt translated">udio是一个领域，来自计算机视觉和NLP领域的思想的交叉融合拓宽了视野。音频生成并不是一个新领域，但多亏了深度学习领域的研究，这个领域近年来也取得了一些巨大的进步。音频生成有几种应用。现在最突出最流行的是一系列智能助手(谷歌助手，苹果Siri，亚马逊Alexa，等等)。这些虚拟助手不仅试图理解自然语言查询，还会用非常像人类的声音做出回应。</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="cbc4" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><em class="mj">本文摘自本书，</em> <a class="ae kw" href="https://www.amazon.com/Generative-AI-Python-TensorFlow-Create/dp/1800200889?maas=maas_adg_6BDCD121BC86911E075F8723702C51F5_afap_abs&amp;ref_=aa_maas" rel="noopener ugc nofollow" target="_blank"> <strong class="kz ir"> <em class="mj">用Python和TensorFlow生成AI 2</em></strong></a><strong class="kz ir"><em class="mj">。这本书是约瑟夫·巴布科克和Raghav Bali(我自己)</em> </strong> <em class="mj">写的。我们的目的是为读者创建一个实用的指南，使他们能够用VAEs、GANs、LSTMs、GPT模型等创建图像、文本和音乐。这本书帮助了许多Python程序员、经验丰富的建模师和机器学习工程师理解深度生成模型背后的理论，并用实际例子进行实验。</em></p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><p id="5f9b" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">音乐是一个连续的信号，是各种乐器的声音和声音的组合。另一个特征是我们在听的时候注意到的结构性循环模式的存在。换句话说，每首音乐作品都有自己特有的连贯性、节奏和流动。</p><p id="2870" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">在这篇文章中，我们将以一种非常简单的方式来处理音乐生成的任务。我们将利用和扩展堆叠的LSTM网络来完成音乐创作的任务。这种设置类似于文本生成的情况(这是下一篇文章的主题)。为了使事情简单和易于实现，我们将关注于一个<em class="mj">单乐器/单声道</em>音乐生成任务。</p><p id="18de" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">以下是我们此次演练的工作流程概要:</p><ul class=""><li id="4c2e" class="mk ml iq kz b la lb ld le lg mm lk mn lo mo ls mp mq mr ms bi translated">了解数据集</li><li id="bf55" class="mk ml iq kz b la mt ld mu lg mv lk mw lo mx ls mp mq mr ms bi translated">为音乐生成准备数据集</li><li id="857d" class="mk ml iq kz b la mt ld mu lg mv lk mw lo mx ls mp mq mr ms bi translated">基于LSTMs的音乐生成模型(<em class="mj">我们说注意了</em>！)</li><li id="a250" class="mk ml iq kz b la mt ld mu lg mv lk mw lo mx ls mp mq mr ms bi translated">模特培训</li><li id="dcde" class="mk ml iq kz b la mt ld mu lg mv lk mw lo mx ls mp mq mr ms bi translated">听着节拍！让我们听听我们的模型生成的几个样本</li></ul><p id="fa7e" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">让我们首先了解更多关于数据集的知识，并思考我们将如何为我们的音乐生成任务准备它。</p><p id="80d4" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><em class="mj">本文使用的代码可以通过Github库</em><a class="ae kw" href="https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2" rel="noopener ugc nofollow" target="_blank"><em class="mj">【1】</em></a><em class="mj">和</em><a class="ae kw" href="https://github.com/raghavbali/generative_ai_with_tensorflow" rel="noopener ugc nofollow" target="_blank"><em class="mj">【2】</em></a><em class="mj">获得。更有趣的是，这被很好地打包在一个</em><strong class="kz ir"><em class="mj">Google-Colab</em></strong><em class="mj">启用的jupyter笔记本中，你可以简单地点击并使用它。</em></p><h1 id="cb4c" class="my mz iq bd na nb nc nd ne nf ng nh ni jw nj jx nk jz nl ka nm kc nn kd no np bi translated">数据集</h1><p id="7baa" class="pw-post-body-paragraph kx ky iq kz b la nq jr lc ld nr ju lf lg ns li lj lk nt lm ln lo nu lq lr ls ij bi lt translated">IDI是一种易于使用的格式，它帮助我们提取文件中包含的音乐的符号表示。在本次讨论/演练中，我们将利用reddit用户<em class="mj"> u/midi_man </em>收集和共享的大规模公共MIDI数据集的子集，该数据集可通过以下链接获得:<a class="ae kw" href="https://www.reddit.com/r/WeAreTheMusicMakers/comments/3ajwe4/the_largest_midi_collection_on_the_internet/" rel="noopener ugc nofollow" target="_blank"> r/WeAreTheMusicMakers </a></p><p id="a9c9" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我们将利用这个数据集本身的一个子集。该子集基于伟大音乐家如贝多芬、巴赫、巴托克等的古典钢琴曲。子集可以在压缩文件夹<code class="fe nv nw nx ny b">midi_dataset.zip</code>中找到，还有这个<a class="ae kw" href="https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2/tree/master/Chapter_11/lstm_attention" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>中的代码。</p><p id="3370" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我们将利用<code class="fe nv nw nx ny b">music21</code>来处理这个数据集的子集，并为训练模型准备我们的数据。由于音乐是来自各种乐器和人声/歌手的声音的集合，为了这个练习的目的，我们将首先使用<code class="fe nv nw nx ny b">chordify()</code>功能从歌曲中提取和弦。下面的代码片段帮助我们获得所需格式的MIDI乐谱列表。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/4e8762285c103ed68e242f432fd2d6d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nSb7Xz1dwWcdwQVLzU1npA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来自Google-Colab enabled<a class="ae kw" href="https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2/blob/master/Chapter_11/lstm_attention/lstm_attention.ipynb" rel="noopener ugc nofollow" target="_blank"><strong class="bd kv">jupyter笔记本</strong> </a>的代码片段</p></figure><p id="166f" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">一旦我们有了乐谱列表，下一步就是提取音符和它们相应的计时信息。为了提取这些细节，<code class="fe nv nw nx ny b">music21</code>有简单易用的接口，如<code class="fe nv nw nx ny b">element.pitch</code>和<code class="fe nv nw nx ny b">element.duration</code>。下面的代码片段帮助我们从MIDI文件中提取这样的信息，并准备两个并行列表。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/1958a5e50a3ec119d9fa2765ced88d34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*af1X2OAv2qj9DiZV-qqBRg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来自Google-Colab enabled<a class="ae kw" href="https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2/blob/master/Chapter_11/lstm_attention/lstm_attention.ipynb" rel="noopener ugc nofollow" target="_blank"><strong class="bd kv">jupyter笔记本</strong> </a>的代码片段</p></figure><p id="80cd" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我们采取一个额外的步骤来降低维度。虽然这是一个可选步骤，但我们建议这样做，以保持任务的可处理性，并将模型培训要求保持在限制范围内。以下片段将音符/和弦列表和持续时间缩减为只有<em class="mj"> C大调</em>(您也可以选择任何其他调)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/2dd292fddb86c85e4549e01cd698728a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BKMZHQkWgQqQ2z0RF_uLXw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来自Google-Colab enabled<a class="ae kw" href="https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2/blob/master/Chapter_11/lstm_attention/lstm_attention.ipynb" rel="noopener ugc nofollow" target="_blank"><strong class="bd kv">jupyter笔记本</strong> </a>的代码片段</p></figure><p id="6b2d" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">现在我们已经预处理了我们的数据集，下一步是将音符/和弦和持续时间相关的信息转换成可消费的形式。一个简单的方法是创建符号到整数的映射。一旦转换成整数，我们可以将它们作为模型嵌入层的输入，在训练过程中进行微调。下面的代码片段准备了映射，并提供了一个示例输出。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/3d68a5509fcf6712aad879837f6d4417.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bUnjHhwIAxpgtQABE9ppMg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">启用Google-Colab的代码片段<a class="ae kw" href="https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2/blob/master/Chapter_11/lstm_attention/lstm_attention.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="bd kv"> jupyter笔记本</strong> </a></p></figure><p id="6d21" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我们现在已经准备好了地图。在下面的代码片段中，我们将训练数据集准备为长度为32的序列，并将它们对应的目标作为序列中的下一个标记。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/588de79e9f448a4de1e0bea2583b286c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZEArYMvCfU40GKqPRBhFQQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来自Google-Colab enabled<a class="ae kw" href="https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2/blob/master/Chapter_11/lstm_attention/lstm_attention.ipynb" rel="noopener ugc nofollow" target="_blank"><strong class="bd kv">jupyter笔记本</strong> </a>的代码片段</p></figure><p id="bf87" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">正如我们所看到的，除了与MIDI文件处理相关的一些细微差别，数据集准备阶段基本上是简单明了的。生成的序列及其对应的目标显示在下面的输出片段中，以供参考。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oc"><img src="../Images/8967d2cc53467f444c82f0aa7eb977bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6bwkmdzOWE8ikxpvNj2_Mg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代码片段来自Google-Colab enabled<a class="ae kw" href="https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2/blob/master/Chapter_11/lstm_attention/lstm_attention.ipynb" rel="noopener ugc nofollow" target="_blank"><strong class="bd kv">jupyter笔记本</strong> </a></p></figure><p id="549b" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">转换后的数据集现在是一个数字序列，就像文本生成的情况一样。列表上的下一项是模型本身。</p><h1 id="02c5" class="my mz iq bd na nb nc nd ne nf ng nh ni jw nj jx nk jz nl ka nm kc nn kd no np bi translated">音乐生成的LSTM模型</h1><p id="f17a" class="pw-post-body-paragraph kx ky iq kz b la nq jr lc ld nr ju lf lg ns li lj lk nt lm ln lo nu lq lr ls ij bi lt translated">与文本生成(使用Char-RNN)一样，我们通常只有少量的输入符号(小写和大写字母、数字)，而音乐生成的符号数量很大(约500)。在此符号列表中添加一些时间/持续时间相关信息所需的额外符号。对于这个更大的输入符号列表，模型需要更多的训练数据和学习能力(根据LSTM单元的数量、嵌入大小等的能力)。</p><p id="a44a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我们需要注意的下一个明显的变化是模型在每个时间步接受两个输入的能力。换句话说，模型应该能够在每个时间步长将音符和持续时间信息作为输入，并生成具有相应持续时间的输出音符。为此，我们利用功能性的<code class="fe nv nw nx ny b">tensorflow.keras</code> API来准备一个多输入多输出架构。</p><p id="3210" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">堆叠的LSTM在能够学习比具有单个LSTM层的网络更复杂的特征方面具有明显的优势。除此之外，<strong class="kz ir">注意机制</strong>有助于缓解RNNs固有的问题，例如处理远程依赖性的困难。由于音乐是由节奏和连贯性形式的局部和整体结构组成的，注意力机制肯定会产生影响。</p><p id="bfc1" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">以下代码片段以所讨论的方式准备了一个多输入堆叠LSTM网络。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/bd5e202b6312561bf3007e060b8db099.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fQwk1vnAVumjVZBIjSVqpw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">启用Google-Colab的代码片段<a class="ae kw" href="https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2/blob/master/Chapter_11/lstm_attention/lstm_attention.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="bd kv"> jupyter笔记本</strong> </a></p></figure><p id="fdcd" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">使用上述片段准备的模型是一个多输入网络(音符和持续时间各一个输入)。下面概括介绍了模型设置:</p><ul class=""><li id="7ddf" class="mk ml iq kz b la lb ld le lg mm lk mn lo mo ls mp mq mr ms bi translated">使用各自的嵌入层将每个输入转换成矢量。</li><li id="5726" class="mk ml iq kz b la mt ld mu lg mv lk mw lo mx ls mp mq mr ms bi translated">两个输入被连接</li><li id="bb1b" class="mk ml iq kz b la mt ld mu lg mv lk mw lo mx ls mp mq mr ms bi translated">级联的输入然后通过几个LSTM层，接着是一个简单的注意机制。</li><li id="26db" class="mk ml iq kz b la mt ld mu lg mv lk mw lo mx ls mp mq mr ms bi translated">在这一点之后，模型再次分成两个输出(一个用于下一个音符，另一个用于该音符的持续时间)。鼓励读者使用<code class="fe nv nw nx ny b">keras</code>实用程序来可视化网络。</li></ul><h1 id="8cd5" class="my mz iq bd na nb nc nd ne nf ng nh ni jw nj jx nk jz nl ka nm kc nn kd no np bi translated">模型训练和音乐生成</h1><p id="43c9" class="pw-post-body-paragraph kx ky iq kz b la nq jr lc ld nr ju lf lg ns li lj lk nt lm ln lo nu lq lr ls ij bi translated">训练这个模型就像在<code class="fe nv nw nx ny b">keras</code>模型对象上调用<code class="fe nv nw nx ny b">fit()</code>函数一样简单。我们训练模型大约100个时期。下图描述了模型在不同时期的学习进度。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/60a27c858725491218a7d9d7b673b4d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6WMzLsitzOrp09bSdWEMUQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来自Google-Colab enabled<a class="ae kw" href="https://github.com/PacktPublishing/Hands-On-Generative-AI-with-Python-and-TensorFlow-2/blob/master/Chapter_11/lstm_attention/lstm_attention.ipynb" rel="noopener ugc nofollow" target="_blank"><strong class="bd kv">jupyter笔记本</strong> </a>的代码片段</p></figure><p id="95c0" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">如图所示，该模型能够在生成的音乐样本中学习一些重复模式。这里，我们使用基于温度的采样作为我们的解码策略[ <a class="ae kw" href="https://github.com/raghavbali/text_generation/blob/master/notebooks/text_generation_02.ipynb" rel="noopener ugc nofollow" target="_blank"> <em class="mj">参考链接</em> </a> ]。</p><h1 id="4096" class="my mz iq bd na nb nc nd ne nf ng nh ni jw nj jx nk jz nl ka nm kc nn kd no np bi translated">让我们听听艾的声音</h1><p id="1d31" class="pw-post-body-paragraph kx ky iq kz b la nq jr lc ld nr ju lf lg ns li lj lk nt lm ln lo nu lq lr ls ij bi translated">以下是我们的模型分别在5个和50个时期后生成的两个样本。他们并不完美，但肯定很有前途，就像一个即将到来的音乐家。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="of og l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">5个时期后生成的样本音乐</p></figure><p id="5bc8" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">观察这两个样本之间的改进。50个时期之后的输出比仅仅5个时期之后的输出更加连贯和有节奏。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="of og l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">50个时代后生成的样本音乐</p></figure><h1 id="6da7" class="my mz iq bd na nb nc nd ne nf ng nh ni jw nj jx nk jz nl ka nm kc nn kd no np bi translated"><strong class="ak">总结</strong></h1><p id="00f5" class="pw-post-body-paragraph kx ky iq kz b la nq jr lc ld nr ju lf lg ns li lj lk nt lm ln lo nu lq lr ls ij bi translated">这是一个使用深度学习模型的音乐生成的非常简单的实现。我们用文本生成的概念进行了类比。在这篇文章所基于的书中，我们进一步使用其他高级技术进行了一些音乐生成，包括MuseGANs等生成对抗网络。</p></div><div class="ab cl mc md hu me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="ij ik il im in"><h1 id="37e0" class="my mz iq bd na nb oh nd ne nf oi nh ni jw oj jx nk jz ok ka nm kc ol kd no np bi translated"><strong class="ak">关于作者</strong></h1><p id="10d9" class="pw-post-body-paragraph kx ky iq kz b la nq jr lc ld nr ju lf lg ns li lj lk nt lm ln lo nu lq lr ls ij bi translated"><strong class="kz ir"> Joseph Babcock </strong>在电子商务、数字流媒体和量化金融领域从事大数据和人工智能工作已经超过十年。在他的职业生涯中，他致力于推荐系统、Pb级云数据管道、A/B测试、因果推理和时间序列分析。他在约翰霍普金斯大学完成了博士研究，将机器学习应用于药物发现和基因组学领域。</p><p id="2ec0" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated"><a class="ae kw" href="https://raghavbali.github.io/" rel="noopener ugc nofollow" target="_blank"> <strong class="kz ir"> Raghav Bali </strong> </a>是<a class="ae kw" href="https://www.amazon.com/Raghav-Bali/e/B07K2PKCT9?ref=sr_ntt_srch_lnk_1&amp;qid=1625124559&amp;sr=8-1" rel="noopener ugc nofollow" target="_blank">多部畅销书籍</a>的作者，也是世界上最大的医疗保健组织之一的高级数据科学家。他的工作包括研发基于机器学习、深度学习和自然语言处理的企业级解决方案，用于医疗保健和保险相关的用例。他之前的经历包括在英特尔和美国运通工作。Raghav拥有孟加拉国国际信息技术学院的硕士学位(金牌获得者)。</p></div></div>    
</body>
</html>