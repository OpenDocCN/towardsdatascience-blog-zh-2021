<html>
<head>
<title>Bayesian Statistics 101</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">贝叶斯统计101</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bayesian-statistics-101-4c4bc5fde1e1?source=collection_archive---------1-----------------------#2021-10-18">https://towardsdatascience.com/bayesian-statistics-101-4c4bc5fde1e1?source=collection_archive---------1-----------------------#2021-10-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/97cf0b0b938465e2e27b614d7e15a97e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*p1NRJANnzEwwjgMl"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">照片由<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae jg" href="https://unsplash.com/@madebyjens?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">延斯·勒列</a>拍摄</p></figure><div class=""/><div class=""><h2 id="c939" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">不管你喜不喜欢，你都不会再以同样的方式看待统计数据了</h2></div><h1 id="097a" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">介绍</h1><p id="059f" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">贝叶斯统计与经典统计(也称为频率统计)的基本区别在于它对概率的解释。前者将其视为“<em class="mm">信任度</em>”，而后者将其视为“<em class="mm">在多次试验中观察到的相对频率</em>”。</p><p id="c513" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这种差异可能看起来过于抽象，但它对后来开发的方法有很大的实际影响。</p><p id="a6fe" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">频率主义者的观点是最常见的，因此影响了大量的统计技术。然而，许多现代方法依赖于贝叶斯方法，可以产生很好的结果，当没有太多数据可用时，为您提供替代方案。</p><p id="16b9" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们现在将看到的方法将为您提供一个正式的框架，通过该框架，您可以将主观判断添加到您的数据科学问题中，这在您没有太多可用数据时，或者在您知道数据存在某种缺陷时会特别有用。它还将帮助您理解一些著名的机器学习算法的起源，如朴素贝叶斯分类器和贝叶斯神经网络。</p><p id="8944" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们将从快速浏览贝叶斯定理(贝叶斯统计的核心)开始，然后继续学习由此衍生的一些技术，以及如何使用它们来解决各种统计问题。顺便说一下，我们将使用Python。</p><h1 id="1b4f" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">贝叶斯定理</h1><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/ed528960de5c9710cb77b3811f6f2d85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1274/format:webp/1*k0HlZUEjg2C-sZ5jJudmiw.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">贝叶斯定理</p></figure><p id="bb06" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">上面的等式很简单，但是理解它需要知道一些概率论的符号:</p><ul class=""><li id="723a" class="mx my jj ls b lt mn lw mo lz mz md na mh nb ml nc nd ne nf bi translated"><strong class="ls jk"> P(A) </strong>:事件发生的概率A</li><li id="18da" class="mx my jj ls b lt ng lw nh lz ni md nj mh nk ml nc nd ne nf bi translated"><strong class="ls jk"> P(A|B) </strong>:假设事件B发生，事件A发生<em class="mm">的概率</em></li></ul><p id="778e" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">一个事件可以是任何事情。例如，<strong class="ls jk"> P(A|B) </strong>可以表示“假设(<strong class="ls jk"> | </strong>)你的聚合酶链式反应测试检测结果为阳性(<strong class="ls jk"> B </strong>)，你得到COVID ( <strong class="ls jk"> A </strong>)的概率(<strong class="ls jk"> P </strong>)。要使用上述等式计算该概率，我们需要:</p><ul class=""><li id="4392" class="mx my jj ls b lt mn lw mo lz mz md na mh nb ml nc nd ne nf bi translated"><strong class="ls jk"> P(A) </strong>:具有COVID的概率(不考虑测试结果)</li><li id="4d92" class="mx my jj ls b lt ng lw nh lz ni md nj mh nk ml nc nd ne nf bi translated"><strong class="ls jk"> P(B) </strong>:聚合酶链式反应测试出现阳性结果的概率(无论您是否有COVID)</li><li id="d486" class="mx my jj ls b lt ng lw nh lz ni md nj mh nk ml nc nd ne nf bi translated">P(B|A) :给定COVID，聚合酶链式反应测试出现阳性结果的概率</li></ul><p id="04e8" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这个定理可以很容易地从条件概率的定义中推导出来，它并不专门用于贝叶斯统计，也不是经常性的争论。到目前为止，这只是基本的概率论。</p><h1 id="dd50" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">贝叶斯统计</h1><p id="c1f1" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">贝叶斯统计从现在开始所做的就是把这个定理变成下面的咒语:</p><blockquote class="nl nm nn"><p id="b6db" class="lq lr mm ls b lt mn kk lv lw mo kn ly no mp mb mc np mq mf mg nq mr mj mk ml im bi translated">你对世界如何运转有一个先验的信念。一旦你得到数据，你就相应地更新这种信念。</p></blockquote><p id="2d05" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这意味着您通过定义一个先验分布来开始解决任何问题(这一步非常主观)，然后您使用观察数据的可能性来更新该先验，创建一个后验分布。</p><p id="293e" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">不清楚？让我们看一个问题解决方案的例子，你会看到这在实践中是如何发生的。</p><h2 id="7a81" class="nr kz jj bd la ns nt dn le nu nv dp li lz nw nx lk md ny nz lm mh oa ob lo oc bi translated">蒙蒂·霍尔问题</h2><p id="d380" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><em class="mm">让我们做个交易</em>是一个流行的电视游戏节目，始于60年代的美国，其最初的主持人叫蒙蒂·霍尔。基于它的一个著名的概率谜题后来出名了，格式如下:</p><blockquote class="nl nm nn"><p id="182b" class="lq lr mm ls b lt mn kk lv lw mo kn ly no mp mb mc np mq mf mg nq mr mj mk ml im bi translated">你在游戏节目的舞台上，这里有三扇门。其中一个后面有一辆汽车，另外两个后面有一只山羊。你必须选择其中一扇门，你的奖品将是门后的任何东西(显然你想要这辆车)。</p><p id="7191" class="lq lr mm ls b lt mn kk lv lw mo kn ly no mp mb mc np mq mf mg nq mr mj mk ml im bi translated">你选1号门。然后主人打开3号门，发现门后有一只山羊。然后，他给你选择坚持1号门还是转2号门。你是做什么的？</p></blockquote><p id="fe72" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这就是这个问题引起了很多愤怒的地方:我们的直觉告诉我们，换车门或坚持我们的选择之间没有区别，两者都有50%的可能性把车藏起来。然而，仔细观察就会发现，换门确实是有益的——它实际上会让你的胜算翻倍。</p><p id="7002" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">让我们用贝叶斯观点来看这个问题。</p><p id="01f1" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们从每个门的1/3的先验概率分布开始。这意味着每扇门都有1/3的机会成为“正确的”门(有3扇门，我们没有理由认为其中一扇门比另一扇门更有可能)。先验是贝叶斯定理中的P(A)。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi od"><img src="../Images/cfcd252eb3ca7126b74a61eb643fd130.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/format:webp/1*ErRhOZeP0bjWeLY3qK3Auw.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">我们的前科</p></figure><p id="05c0" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">然后，考虑到我们拥有的新数据，我们继续计算可能性。如果汽车在后面，主人打开3号门(如他所做的)的可能性有多大:</p><ul class=""><li id="9d0e" class="mx my jj ls b lt mn lw mo lz mz md na mh nb ml nc nd ne nf bi translated"><strong class="ls jk">1号门</strong>:主持人会在2号门和3号门之间随机选择，所以这种情况下他打开3号门的概率是<strong class="ls jk"> 1/2 </strong></li><li id="088c" class="mx my jj ls b lt ng lw nh lz ni md nj mh nk ml nc nd ne nf bi translated"><strong class="ls jk">门2 </strong>:主持人必须打开门3(因为汽车在门2后面，而你已经选择了门1)，所以在这种情况下他打开门3的概率是<strong class="ls jk"> 1 </strong></li><li id="efa2" class="mx my jj ls b lt ng lw nh lz ni md nj mh nk ml nc nd ne nf bi translated"><strong class="ls jk">3号门</strong>:如果后面有车，主人不可能打开3号门，所以这种情况下主人打开3号门的概率是<strong class="ls jk"> 0 </strong></li></ul><p id="bf18" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这给我们留下了:</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/6d743f7f7e74dd1ed3c1d252fde93956.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*KGn_1T7RrlDF0OkplY3EHQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">每个门的先验和可能性</p></figure><p id="2569" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">似然性相当于贝叶斯定理中的P(B|A)。现在，让我们计算“先验”和“可能性”的乘积:</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi of"><img src="../Images/b38c09bcac643827b5dcf987003f703f.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*HPpqaYlRulZsyTHnmitf4g.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">先验*可能性是前两列的乘积</p></figure><p id="6e52" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">您会注意到3个值(1/6、1/3和0)加起来不等于1。这是因为我们遗漏了贝叶斯定理的最后一个元素:P(B)。这恰好是这三个值的总和。通过将每一个除以总和，我们将以它们加到1的方式缩放它们。在我们的问题中，3个值相加等于1/2 (1/6 + 1/3 = 1/2)。为了找到我们的后验概率，我们只需将最后一列除以1/2:</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi og"><img src="../Images/8cd6e9109f3a10d530d450171b38939d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/1*L-JZuSoab0qTFDd4m8E5wg.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">最后，我们的后路！</p></figure><p id="5dd5" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">正如所料，汽车在3号门后面的概率是0，因为主机已经打开了它。我们还看到，门2隐藏汽车的可能性是门1的两倍。</p><p id="d7ff" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如果你对这个问题特别感兴趣，或者你觉得结果太反直觉，这里有一整篇文章<a class="ae jg" rel="noopener" target="_blank" href="/the-monty-hall-problem-9c4053ef0640">关于它</a>。</p><p id="bdb7" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">好吧，我知道这是一个玩具问题，我们使用了一个非常简单的分布作为我们的先验。当我们面对现实生活中的复杂问题时，会发生什么呢？这些问题有许多变量，先验不容易定义，可能性很难计算。幸运的是，有一个Python库可以帮助我们:PyMC3。</p><p id="46b0" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">使用PyMC3，您可以对许多事情使用贝叶斯推理，包括估计回归和分类的参数。您可以使用该库附带的内置函数来完成这项工作，或者使用它从头开始构建定制模型。第一个选项更简单，但灵活性较差，而第二个选项在选择参数时会给你更多的自由。</p><p id="f795" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">让我们来看看如何将这两个选项用于回归和分类等经典问题。</p><h1 id="4690" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">回归</h1><h2 id="6898" class="nr kz jj bd la ns nt dn le nu nv dp li lz nw nx lk md ny nz lm mh oa ob lo oc bi translated">线性回归“从零开始”</h2><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/86fdc3c3934286cbcd923f661c893b37.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/1*_tqLkkoPW-x0-xyKnuXqnQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">贝叶斯线性回归定义</p></figure><p id="c6cd" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如果你熟悉线性回归，你会发现它与传统模型有一些不同。主要的一点是，在贝叶斯回归中，我们不把参数α、β1、β2和σ看作固定值，而是看作服从分布的变量。</p><p id="5797" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们将尝试估计这些分布，使用和以前一样的方法:定义一个先验分布并用数据更新它，得到一个后验分布。</p><p id="42c4" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在写这一部分时，我在使用真实世界的数据还是生成虚假数据来运行回归之间犹豫不决。我决定使用假数据，因为这将使我们能够将我们的结果与真实的地面真相进行比较:我们用来生成数据的参数。</p><pre class="mt mu mv mw gt oi oj ok ol aw om bi"><span id="a548" class="nr kz jj oj b gy on oo l op oq"><strong class="oj jk">[IN]:<br/></strong>import arviz as az<br/>import matplotlib.pyplot as plt<br/>import numpy as np<br/>import pandas as pd<br/>import pymc3 as pm</span><span id="9374" class="nr kz jj oj b gy or oo l op oq"><strong class="oj jk"># Setting a random seed for reproducibility</strong><br/>np.random.seed(23)</span><span id="2e1b" class="nr kz jj oj b gy or oo l op oq"><strong class="oj jk"># True parameter values</strong><br/>alpha = -0.8<br/>beta = [-1, 1.5]<br/>sigma = 1</span><span id="6e66" class="nr kz jj oj b gy or oo l op oq"><strong class="oj jk"># Number of observations</strong><br/>size = 500</span><span id="f66a" class="nr kz jj oj b gy or oo l op oq"><strong class="oj jk"># Predictor variables</strong><br/>X1 = np.random.randn(size)<br/>X2 = np.random.randn(size) * 0.8</span><span id="0b0a" class="nr kz jj oj b gy or oo l op oq"><strong class="oj jk"># Outcome variable</strong><br/>Y = alpha + beta[0] * X1 + beta[1] * X2 + np.random.randn(size) * sigma</span><span id="14a2" class="nr kz jj oj b gy or oo l op oq"><strong class="oj jk"># Putting our original data into a dataframe (we'll use it later on)</strong><br/>df = pd.DataFrame(<br/>    data = np.array([X1, X2, Y]),<br/>    index=['X1', 'X2', 'Y']<br/>).T</span></pre><p id="0e3b" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">目前，除了导入所需的库和生成数据之外，我们还没有做太多工作。现在让我们使用PyMC3创建第一个模型:</p><pre class="mt mu mv mw gt oi oj ok ol aw om bi"><span id="5946" class="nr kz jj oj b gy on oo l op oq"><strong class="oj jk">[IN]:</strong><br/>with pm.Model() as model_1:<br/>    <strong class="oj jk"># Data</strong><br/>    X_1 = pm.Data('X1', X1)<br/>    X_2 = pm.Data('X2', X2)</span><span id="4679" class="nr kz jj oj b gy or oo l op oq">    <strong class="oj jk"># Priors</strong><br/>    alpha = pm.Normal("alpha", mu=0, sigma=10)<br/>    beta = pm.Normal("beta", mu=0, sigma=10, shape=2)<br/>    sigma = pm.HalfNormal("sigma", sigma=10)</span><span id="5804" class="nr kz jj oj b gy or oo l op oq">    <strong class="oj jk"># Likelihood</strong><br/>    mu = alpha + beta[0] * X_1 + beta[1] * X_2<br/>    Y_obs = pm.Normal("Y_obs", mu=mu, sigma=sigma, observed=Y)<br/>    <br/>    <strong class="oj jk"># Posterior</strong><br/>    trace = pm.sample(100, return_inferencedata=False, chains=4)<br/>    az.plot_trace(trace)</span></pre><p id="1c97" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">首先，请注意模型构造的格式，这是PyMC3的标准格式:您将所有内容(数据、先验、似然和后验)放在一个“with”语句中，在这里命名您的模型(在我们的例子中是“model_1”)。稍后可以访问这个模型。</p><p id="4c06" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">你可能会问为什么这些前科？在这种特定情况下，对于α和β，我们使用以0为中心的正态分布，因为我们没有任何先验知识可以表明X和y之间的强关系。关于每个先验中的σ= 10，它设置了正态分布的标准偏差，因此该数字越高，我们的先验中的方差越大，因此它们的信息量越少(如果您不太确定这些先验，这可能是一件好事)。西格玛参数的<em class="mm">半正态</em>分布将保持其正值。</p><p id="ad32" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">尝试测试不同的先验，看看最终结果变化有多大，这实际上是一件好事，看看你的模型对不良先验有多稳健(变化越大，你越依赖你的先验)。</p><p id="b8db" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">让我们来看看我们模型的输出:</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi os"><img src="../Images/429f4a9ed410736997596ef8751e2ad0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s599E0SIA811Wl4Cdr8Rrw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">第一个模型输出</p></figure><p id="9032" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">该图中的每一“行”代表我们的一个参数(α、β、σ)，左边部分代表后验分布，右边部分代表它们随时间的收敛。</p><p id="7502" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">以左上角的图为例:它显示了alpha的估计后验分布。但是等等，实际上有4个发行版，而不是一个！这是因为pm.sample()有一个名为<em class="mm">链</em>的参数，设置为2或系统中CPU的数量，取其中较大的一个。在我的例子中，它等于4，但是为了便于说明，我决定显式地设置它。<em class="mm">链</em>将设置并行运行的独立链的数量，这允许我们有一些收敛统计。</p><p id="c6f5" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">您还可以在右侧的图中看到链的概念，每个链有一条线。然而，这些线条显示的是什么？你看到pm.sample()中的100了吗？也就是设置要抽取的样本数量。这些线条基本上显示了100个样本中，4个链中每个链的参数的最可能估计值。对于alpha，您可以看到它在-0.8附近变化(尽管稍微偏向较低的值)。</p><p id="0ed6" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在中间的左、右图中，beta 1和beta 2的信息相同(可以通过不同的颜色区分它们)。</p><p id="0976" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如你所见，估计的分布或多或少与基本事实一致(一些偏差可能来自我们设定的先验)。</p><p id="bee7" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们有后验分布和它们的图，但是你可能想知道我们如何用它做预测。PyMC3有一个名为<em class="mm">fast _ sample _ posterior _ predictive</em>的函数，它允许我们这样做:</p><pre class="mt mu mv mw gt oi oj ok ol aw om bi"><span id="b7b4" class="nr kz jj oj b gy on oo l op oq"><strong class="oj jk">[IN]:</strong><br/>with model_1:</span><span id="8815" class="nr kz jj oj b gy or oo l op oq">pm.set_data({<br/>        'X1': [0.6, 0.02],<br/>        'X2': [-1.3, 0.3]<br/>    })<br/>    y_test = pm.fast_sample_posterior_predictive(trace, samples=100)</span><span id="8d39" class="nr kz jj oj b gy or oo l op oq">print(y_test['Y_obs'].mean(axis=0))</span><span id="3ffa" class="nr kz jj oj b gy or oo l op oq"><strong class="oj jk">[OUT]:</strong><br/>[-3.27941019 -0.31231568]</span></pre><p id="4642" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">输出是我们放入示例中的2个观察值的预测，通过使用我们的后验分布获取100个样本来生成。</p><p id="9fe7" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">此外，如果您需要点估计而不仅仅是分布，您可以通过运行<em class="mm"> find_MAP </em>并使用这些参数构建一个经典的线性方程来获得:</p><pre class="mt mu mv mw gt oi oj ok ol aw om bi"><span id="1bb0" class="nr kz jj oj b gy on oo l op oq"><strong class="oj jk">[IN}:</strong><br/>pm.find_MAP(model=model_1)</span><span id="aa56" class="nr kz jj oj b gy or oo l op oq"><strong class="oj jk">[OUT]:</strong><br/>{'alpha': array(-0.85492716),<br/> 'beta': array([-1.05603871,  1.48859634]),<br/> 'sigma_log__': array(0.04380838),<br/> 'sigma': array(1.04478214)}</span></pre><p id="9d91" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">尽管PyMC3可能需要一段时间才能运行，但它确实让我们的生活变得轻松了许多。但是它到底是做什么的呢？</p><p id="e840" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">PyMC3的工作方式与我们构建Monty Hall示例的方式并不完全相同，因为它并不计算精确的后验分布。相反，它做出了一个聪明的举动，实际上是<em class="mm">从后验概率中采样</em>数据，以便对其进行估计(这就是为什么你会在我们代码的“后验”部分看到“样本”函数)。</p><p id="c34c" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">虽然这不会产生我们正在寻找的精确分布，但它在合理的时间内给了我们一个很好的近似值。它使用主要基于MCMC(马尔可夫链蒙特卡罗)和VI(变分推理)算法的方法来拟合模型。如果你想要更多关于这两种方法的数学细节，我建议你阅读<a class="ae jg" rel="noopener" target="_blank" href="/bayesian-inference-algorithms-mcmc-and-vi-a8dad51ad5f5">这篇文章</a>。</p><h2 id="459e" class="nr kz jj bd la ns nt dn le nu nv dp li lz nw nx lk md ny nz lm mh oa ob lo oc bi translated">使用GLM的线性回归</h2><p id="226d" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">现在，让我们看看如何使用PyMC3中的GLM类来获得我们的后验概率:</p><pre class="mt mu mv mw gt oi oj ok ol aw om bi"><span id="59cd" class="nr kz jj oj b gy on oo l op oq"><strong class="oj jk">[IN]:</strong><br/>from pymc3.glm import GLM</span><span id="13e1" class="nr kz jj oj b gy or oo l op oq"># Creating our model<br/>with pm.Model() as model_glm:<br/>    GLM.from_formula('Y ~ X1 + X2', df)<br/>    trace = pm.sample(100)<br/>    pm.traceplot(trace)<br/>    plt.show()</span><span id="68e6" class="nr kz jj oj b gy or oo l op oq"><strong class="oj jk">[OUT]:</strong></span></pre><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ot"><img src="../Images/a7de14ed7016aa79b6b6aa1bb2c8764e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QFskWnfAFuC68kEHhSPuIw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">GLM产量</p></figure><p id="f6ed" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这个模型的构建几乎是不言自明的，也就是说，你所要做的就是写出GLM的公式，剩下的工作和以前差不多。显然，区别在于运行代码时幕后发生了什么。例如，你会注意到，这一次，你没有为你的参数设置先验。相反，PyMC3使用默认的、无信息的先验，例如宽均匀和扩散正态分布，试图添加尽可能少的先验信息。</p><p id="5ff0" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">对结果的解释与前面的例子相同。</p><h1 id="862c" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">分类</h1><h2 id="551e" class="nr kz jj bd la ns nt dn le nu nv dp li lz nw nx lk md ny nz lm mh oa ob lo oc bi translated">逻辑回归“从零开始”</h2><p id="484d" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">对于这个例子，让我们像以前一样重复使用相同的数据，但是为Y创建两个类:高于平均值的类和不高于平均值的类。</p><pre class="mt mu mv mw gt oi oj ok ol aw om bi"><span id="3765" class="nr kz jj oj b gy on oo l op oq"><strong class="oj jk">[IN]:</strong><br/><strong class="oj jk"># Creating a binary variable</strong><br/>df['Y_bin']=df['Y']&gt;df['Y'].mean()</span></pre><p id="3d0f" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们现在将应用与线性回归相似的方法，但使用不同的可能性函数:</p><pre class="mt mu mv mw gt oi oj ok ol aw om bi"><span id="929b" class="nr kz jj oj b gy on oo l op oq"><strong class="oj jk">[IN]:</strong><br/>with pm.Model() as model_log:</span><span id="50b3" class="nr kz jj oj b gy or oo l op oq">    <strong class="oj jk"># Priors</strong><br/>    alpha = pm.Normal("alpha", mu=0, sigma=10)<br/>    beta = pm.Normal("beta", mu=0, sigma=10, shape=2)</span><span id="6c3d" class="nr kz jj oj b gy or oo l op oq">    <strong class="oj jk"># Likelihood </strong>   <br/>    p = pm.Deterministic('p', pm.math.sigmoid(alpha + beta[0] * X1 + beta[1] * X2))<br/>    Y_obs = pm.Bernoulli("Y_obs", p, observed=df['Y_bin'])<br/>    <br/>    <strong class="oj jk"># Posterior</strong><br/>    trace = pm.sample(100, return_inferencedata=False, chains=4)<br/>    az.plot_trace(trace)</span><span id="e0a5" class="nr kz jj oj b gy or oo l op oq"><strong class="oj jk">[OUT]:</strong></span></pre><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ot"><img src="../Images/c7dbd00a6db328f7be8090440510990e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uSTEsSns7loUq6dSAlHyUg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">我们第一次逻辑回归的输出</p></figure><p id="fbe2" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">首先，请记住，我们在这里估计的参数与我们为线性回归创建的参数不同，尽管它们有相同的名称。在第一种情况下，我们直接使用它们来创建Y，而这里我们是在估计逻辑回归中的参数，所以我们不应该使用我们为alpha和beta设置的初始值作为这些参数的基础。</p><h2 id="98dc" class="nr kz jj bd la ns nt dn le nu nv dp li lz nw nx lk md ny nz lm mh oa ob lo oc bi translated">使用GLM的逻辑回归</h2><p id="5cf2" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">让我们再次尝试解决上述分类问题，但使用GLM:</p><pre class="mt mu mv mw gt oi oj ok ol aw om bi"><span id="90e7" class="nr kz jj oj b gy on oo l op oq"><strong class="oj jk">[IN]:</strong><br/>from pymc3.glm.families import Binomial</span><span id="e5b9" class="nr kz jj oj b gy or oo l op oq">with pm.Model() as model_glm_logistic:<br/>    GLM.from_formula('Y_bin ~ X1 + X2', df, family=Binomial())<br/>    trace = pm.sample(100)<br/>    pm.traceplot(trace)<br/>    plt.show()</span></pre><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ot"><img src="../Images/a592555b40e3412153e9d0e793639c89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jTyN0IYfnx2YX18tgvYfHg.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">逻辑回归的输出-GLM版本</p></figure><p id="82f2" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这个版本(逻辑回归)与第一个GLM(线性回归)之间的差异主要在于<em class="mm"> family=Binomial() </em>参数，显然至少在构建代码方面是如此。</p><h1 id="89b6" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">结论</h1><p id="32a3" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">正如你所看到的，贝叶斯推理背后的直觉很简单:你的后验概率将介于你的先验(主观)信念和你观察到的数据之间，这意味着不同的人将得到不同的分布。然而，由此产生的方法并不那么简单，因为它们要求我们以不同的方式思考:总是根据分布而不是固定参数进行推理。</p><p id="7c5e" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这只是对基本贝叶斯统计和推理的介绍，但这个主题要深入得多，并且有更多有趣的方法源于它:朴素贝叶斯分类器、贝叶斯神经网络、贝叶斯假设测试、贝叶斯分层建模和贝叶斯强化学习。希望我很快会在另一篇文章中写这些话题。</p><h1 id="e336" class="ky kz jj bd la lb lc ld le lf lg lh li kp lj kq lk ks ll kt lm kv ln kw lo lp bi translated">更进一步</h1><p id="0e3f" class="pw-post-body-paragraph lq lr jj ls b lt lu kk lv lw lx kn ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">如果你想更深入地研究贝叶斯统计，这里列出了一些额外的信息来源:</p><div class="is it gp gr iu ou"><a href="https://allendowney.github.io/ThinkBayes2" rel="noopener  ugc nofollow" target="_blank"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd jk gy z fp oz fr fs pa fu fw ji bi translated">想想贝叶斯</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">认为贝叶斯是一本免费的书。它在知识共享署名-非商业性使用-类似共享4.0下可用…</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">allendowney.github.io</p></div></div></div></a></div><div class="is it gp gr iu ou"><a href="https://docs.pymc.io/en/stable/pymc-examples/examples/getting_started.html" rel="noopener  ugc nofollow" target="_blank"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd jk gy z fp oz fr fs pa fu fw ji bi translated">PyMC3 - PyMC3 3.11.4文档入门</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">作者:John Salvatier，Thomas V. Wiecki，Christopher Fonnesbeck注:本文基于PeerJ CS出版物…</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">docs.pymc.io</p></div></div><div class="pd l"><div class="pe l pf pg ph pd pi ja ou"/></div></div></a></div><div class="is it gp gr iu ou"><a rel="noopener follow" target="_blank" href="/stan-vs-pymc3-vs-edward-1d45c5d6da77"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd jk gy z fp oz fr fs pa fu fw ji bi translated">斯坦vs PyMc3 (vs爱德华)</h2><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">towardsdatascience.com</p></div></div><div class="pd l"><div class="pj l pf pg ph pd pi ja ou"/></div></div></a></div><p id="d33b" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">最后，本文使用的代码可从以下网址获得:</p><div class="is it gp gr iu ou"><a href="https://github.com/arthurmello/statistics/blob/master/6.%20Bayesian%20statistics/Bayesian%20statistics.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd jk gy z fp oz fr fs pa fu fw ji bi translated">统计/贝叶斯统计. ipynb at master Arthur Mello/统计</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">教程的统计方法说明。通过创建帐户为arthurmello/statistics的发展做出贡献…</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">github.com</p></div></div><div class="pd l"><div class="pk l pf pg ph pd pi ja ou"/></div></div></a></div></div><div class="ab cl pl pm hx pn" role="separator"><span class="po bw bk pp pq pr"/><span class="po bw bk pp pq pr"/><span class="po bw bk pp pq"/></div><div class="im in io ip iq"><p id="1f47" class="pw-post-body-paragraph lq lr jj ls b lt mn kk lv lw mo kn ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如果你喜欢这篇文章，你可能也会喜欢这些:</p><div class="is it gp gr iu ou"><a href="https://medium.com/@arthurmello_/the-science-of-why-a-brief-introduction-ed92060e6f0d" rel="noopener follow" target="_blank"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd jk gy z fp oz fr fs pa fu fw ji bi translated">为什么的科学:简介</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">理解因果关系是如何工作的，以及如何处理它</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">medium.com</p></div></div><div class="pd l"><div class="ps l pf pg ph pd pi ja ou"/></div></div></a></div><div class="is it gp gr iu ou"><a rel="noopener follow" target="_blank" href="/xgboost-theory-and-practice-fb8912930ad6"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd jk gy z fp oz fr fs pa fu fw ji bi translated">XGBoost:理论与实践</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">了解最流行的算法之一是如何工作的，以及如何使用它</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">towardsdatascience.com</p></div></div><div class="pd l"><div class="pt l pf pg ph pd pi ja ou"/></div></div></a></div><div class="is it gp gr iu ou"><a rel="noopener follow" target="_blank" href="/k-nearest-neighbors-theory-and-practice-7f6f6ee48e56"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd jk gy z fp oz fr fs pa fu fw ji bi translated">k近邻:理论与实践</h2><div class="pb l"><h3 class="bd b gy z fp oz fr fs pa fu fw dk translated">了解如何使用KNN，这是最直观的分类和回归算法之一</h3></div><div class="pc l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">towardsdatascience.com</p></div></div><div class="pd l"><div class="pu l pf pg ph pd pi ja ou"/></div></div></a></div></div><div class="ab cl pl pm hx pn" role="separator"><span class="po bw bk pp pq pr"/><span class="po bw bk pp pq pr"/><span class="po bw bk pp pq"/></div><div class="im in io ip iq"><blockquote class="nl nm nn"><p id="9557" class="lq lr mm ls b lt mn kk lv lw mo kn ly no mp mb mc np mq mf mg nq mr mj mk ml im bi translated">如果你想进一步讨论，请随时在<a class="ae jg" href="https://www.linkedin.com/in/melloarthur/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上联系我，这将是我的荣幸(老实说)。</p></blockquote></div></div>    
</body>
</html>