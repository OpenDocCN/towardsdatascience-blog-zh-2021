<html>
<head>
<title>RLS: Learning on the Fly</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">RLS:动态学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/recursive-least-squares-learning-on-the-fly-f8bb878eb270?source=collection_archive---------17-----------------------#2021-09-17">https://towardsdatascience.com/recursive-least-squares-learning-on-the-fly-f8bb878eb270?source=collection_archive---------17-----------------------#2021-09-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="f45e" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">简单在线学习算法</h2><div class=""/><div class=""><h2 id="1e21" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">一个简单的模型，当新的数据进来时，它可以动态地学习更新它的权重。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/f9d62247f18e6ec65c5fb0d62bc7d532.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TENmHx1YSZUAPZBnlu4o6g.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><h1 id="1e3c" class="le lf iq bd lg lh li lj lk ll lm ln lo kf lp kg lq ki lr kj ls kl lt km lu lv bi translated"><strong class="ak">为什么要递归最小二乘法？</strong></h1><p id="0d28" class="pw-post-body-paragraph lw lx iq ly b lz ma ka mb mc md kd me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">在线学习是人工智能研究领域中一个蓬勃发展的研究领域。当今世界的许多问题需要机器动态学习，并在收集新信息时进行改进或适应。</p><p id="713b" class="pw-post-body-paragraph lw lx iq ly b lz ms ka mb mc mt kd me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">在本文中，我将解释如何适应最小二乘回归，以便在新数据到来时递归地计算最佳权重，从而使其适合在线学习应用程序。</p><h1 id="a196" class="le lf iq bd lg lh li lj lk ll lm ln lo kf lp kg lq ki lr kj ls kl lt km lu lv bi translated"><strong class="ak">最小二乘法</strong></h1><p id="bc1f" class="pw-post-body-paragraph lw lx iq ly b lz ma ka mb mc md kd me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">有了足够的数据，回归问题可以通过最小化由一组权重和目标做出的预测的平方误差来解决。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/fbadc59919e2ccc9f85be1b4281ddd8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*z1dEIxa2Pg8uxQDPBk53xQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">其中E是误差的平方，y是目标，X是预测数据，w是模型的权重</p></figure><p id="e352" class="pw-post-body-paragraph lw lx iq ly b lz ms ka mb mc mt kd me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">这有一个解析解，可以通过称为伪逆的线性代数来获得:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi my"><img src="../Images/b595b07e5db2681ef994cbf8f3446f0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:524/format:webp/1*wjHn8susdSeD1XNB67EVSA.png"/></div></figure><p id="ea9a" class="pw-post-body-paragraph lw lx iq ly b lz ms ka mb mc mt kd me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">最小化平方误差的最佳权重集由上面的等式给出。</p><p id="c5c1" class="pw-post-body-paragraph lw lx iq ly b lz ms ka mb mc mt kd me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">对于高维问题，计算上式中的逆运算可能非常昂贵。<strong class="ly ja">逆运算与数据维度的平方成比例</strong>。在在线设置中，每次有新的数据点时都必须执行该操作，这是不可行的。这种方法的另一个问题是没有考虑数据的年龄。在在线环境中，每秒钟都会有新数据出现，更新的数据可能比多年前获得的数据更重要。</p><p id="5b8a" class="pw-post-body-paragraph lw lx iq ly b lz ms ka mb mc mt kd me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">因此，该方法必须适于在在线环境中实施。</p><h1 id="1075" class="le lf iq bd lg lh li lj lk ll lm ln lo kf lp kg lq ki lr kj ls kl lt km lu lv bi translated"><strong class="ak">秩一更新，逆序更新</strong></h1><p id="56d7" class="pw-post-body-paragraph lw lx iq ly b lz ma ka mb mc md kd me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">秩1更新允许通过使用使用矩阵A的逆矩阵及其向量的表达式来更新矩阵的逆矩阵加上向量乘以自身。首先，要求逆的项被分解为一个矩阵和一个向量加上它的转置的和。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/f326224934b7fa1cff8c070f8861f7a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*m6035ocpuL3S9-Pdv2PFhQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">其中xn是进入数据集的新数据点，Rn是我们希望求逆的新矩阵</p></figure><p id="be9d" class="pw-post-body-paragraph lw lx iq ly b lz ms ka mb mc mt kd me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">在每个时间步长，当接收到新的数据点时，Rn可以表示为如上。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi na"><img src="../Images/20b836064875927a657bc3dc7b40f741.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*9R2C3Dbu6wsGPzFwJqWvUQ.png"/></div></figure><p id="fb2b" class="pw-post-body-paragraph lw lx iq ly b lz ms ka mb mc mt kd me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">然后可以使用秩一更新来更新权重。如你所见，Rn的更新逆矩阵已经被计算出来，而不需要执行矩阵的逆矩阵。更新的权重方程的完整推导不会显示，但是，这些方程足以实现您自己版本的递归最小二乘算法。</p><p id="5a34" class="pw-post-body-paragraph lw lx iq ly b lz ms ka mb mc mt kd me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">值得注意的是<strong class="ly ja">更新的权重等于权重的先前估计减去增益项乘以预测误差。</strong>这个结果在机器学习领域经常见到(看看随机梯度下降的更新公式，卡尔曼滤波，强化学习中的Q-learning等)。</p><h1 id="1538" class="le lf iq bd lg lh li lj lk ll lm ln lo kf lp kg lq ki lr kj ls kl lt km lu lv bi translated"><strong class="ak">RLS击败SGD的地方</strong></h1><p id="b977" class="pw-post-body-paragraph lw lx iq ly b lz ma ka mb mc md kd me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">上面显示的算法是用python实现的，并与非常流行的随机梯度下降(SGD)算法进行了比较。</p><p id="a493" class="pw-post-body-paragraph lw lx iq ly b lz ms ka mb mc mt kd me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">随机生成由500个数据点组成的具有某种相关性的合成数据集。利用随机初始化的权重，可以应用SGD来逼近最佳拟合线。这可以与RLS算法进行比较。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nb"><img src="../Images/6d22cd8acb428c925003289a59b69ce6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gmpgcn3TOee3OwLMr1IyDg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="c1c2" class="pw-post-body-paragraph lw lx iq ly b lz ms ka mb mc mt kd me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">正如您所看到的，RLS算法只需要大约100个数据点就可以达到完美的解决方案，而SGD算法需要大约40，000次迭代才能达到相同的解决方案(即使它可以访问所有数据)。不仅如此，RLS还在动态学习，每次接收到一个新点时都会逼近伪逆解。</p><h1 id="a850" class="le lf iq bd lg lh li lj lk ll lm ln lo kf lp kg lq ki lr kj ls kl lt km lu lv bi translated"><strong class="ak">与其他实现相比</strong></h1><p id="9d3a" class="pw-post-body-paragraph lw lx iq ly b lz ma ka mb mc md kd me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">最后，这个实现可以与公开提供的python库进行比较。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/f9d62247f18e6ec65c5fb0d62bc7d532.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TENmHx1YSZUAPZBnlu4o6g.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="6b57" class="pw-post-body-paragraph lw lx iq ly b lz ms ka mb mc mt kd me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">该算法的实现与statsmodels.api(一个流行的python库)提供的实现完全相同。上图显示了两种算法的三个权重的收敛，所有权重达到相同的值。</p><h1 id="dae3" class="le lf iq bd lg lh li lj lk ll lm ln lo kf lp kg lq ki lr kj ls kl lt km lu lv bi translated"><strong class="ak">结论</strong></h1><p id="e39c" class="pw-post-body-paragraph lw lx iq ly b lz ma ka mb mc md kd me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">RLS算法能够根据最小二乘解来估计最佳权重，而无需显式计算伪逆中的逆运算。这使得它成为在线学习应用的强大算法，在在线学习应用中，需要即时更新估计值。该模型与SGD进行了比较，结果表明，在适当的情况下，该算法可以大大优于更受欢迎的同类算法。</p><h2 id="9681" class="nc lf iq bd lg nd ne dn lk nf ng dp lo mf nh ni lq mj nj nk ls mn nl nm lu iw bi translated">支持我👏</h2><p id="83d0" class="pw-post-body-paragraph lw lx iq ly b lz ma ka mb mc md kd me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated">希望这对你有所帮助，如果你喜欢，你可以<a class="ae nn" href="https://medium.com/@diegounzuetaruedas" rel="noopener"> <strong class="ly ja">关注我！</strong>T9】</a></p><p id="1628" class="pw-post-body-paragraph lw lx iq ly b lz ms ka mb mc mt kd me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">您也可以成为<a class="ae nn" href="https://diegounzuetaruedas.medium.com/membership" rel="noopener"> <strong class="ly ja">中级会员</strong> </a> <strong class="ly ja"> </strong>使用我的推荐链接，访问我的所有文章以及更多:<a class="ae nn" href="https://diegounzuetaruedas.medium.com/membership" rel="noopener">https://diegounzuetaruedas.medium.com/membership</a></p><h2 id="74f1" class="nc lf iq bd lg nd ne dn lk nf ng dp lo mf nh ni lq mj nj nk ls mn nl nm lu iw bi translated">你可能喜欢的其他文章</h2><p id="4a40" class="pw-post-body-paragraph lw lx iq ly b lz ma ka mb mc md kd me mf mg mh mi mj mk ml mm mn mo mp mq mr ij bi translated"><a class="ae nn" rel="noopener" target="_blank" href="/differentiable-generator-networks-an-introduction-5a9650a24823">可微发电机网络:简介</a></p><p id="2519" class="pw-post-body-paragraph lw lx iq ly b lz ms ka mb mc mt kd me mf mu mh mi mj mv ml mm mn mw mp mq mr ij bi translated">傅立叶变换:直观的可视化</p></div></div>    
</body>
</html>