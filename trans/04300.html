<html>
<head>
<title>Face Mask Detection using darknet’s YOLOv3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用darknet的YOLOv3进行面具检测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/face-mask-detection-using-darknets-yolov3-84cde488e5a1?source=collection_archive---------9-----------------------#2021-04-12">https://towardsdatascience.com/face-mask-detection-using-darknets-yolov3-84cde488e5a1?source=collection_archive---------9-----------------------#2021-04-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="129e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">新冠肺炎:如何使用YOLOv3构建面具检测器的教程。*为了便于推断，视频流和图像都可以使用。</h2></div><p id="9e2b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi lb translated">这篇文章旨在为那些想要训练来自YOLO家族的物体探测器的人提供完整的指南(一步一步)。由于疫情，这样的任务似乎很热门。</p><p id="a5cb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于本教程，我将使用<a class="ae lk" href="https://github.com/AlexeyAB/darknet" rel="noopener ugc nofollow" target="_blank"> YOLOv3 </a>，这是YOLO家族最常用的版本之一，它包含了用于实时场景的最先进的对象检测系统，它的准确性和速度令人惊讶。YOLOv4、YOLOv5等新版本可能会获得更好的结果，在我的下一篇文章中，我还将尝试这些架构，并与您分享我的发现。</p><p id="1bc8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设你已经掌握了使用深度学习技术的物体检测，特别是你知道关于YOLO的基础知识，让我们开始我们的任务…</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/378cd20c2726201656f8a1fcc7fcd592.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*U1DLBNSG5RoIqE-8"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">在<a class="ae lk" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae lk" href="https://unsplash.com/@anshu18?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">安舒A </a>的照片</p></figure><p id="4c18" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可以在我的Github <a class="ae lk" href="https://github.com/skanelo/Face-Mask-Detection" rel="noopener ugc nofollow" target="_blank"> repo </a>上找到这个上传的项目。</p><h1 id="e66f" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">环境🌠</h1><p id="30d6" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">为了实现这个项目，我利用了Google Colab的资源。我的第一个预处理步骤实验是在我的笔记本电脑上进行的，因为它们的计算成本不高，但是模型是在Colab上使用<strong class="kh ir"> GPU </strong>进行训练的。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi my"><img src="../Images/99223ee638a3bbc2d565231622f22367.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*W81V8TV5sThucvIglyox4A.png"/></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">在Colab上通过<strong class="bd mz">编辑</strong> - &gt; <strong class="bd mz">笔记本设置</strong>有人可以激活<strong class="bd mz"> GPU，</strong>我<em class="na">法师作者</em></p></figure><h1 id="1f57" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">资料组📚</h1><p id="7664" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">首先，为了构建一个掩膜检测器，我们需要相关数据。此外，由于YOLO的性质，我们需要带边界框的注释数据。一种选择是通过从网上或通过拍摄朋友/熟人的照片收集图像来建立我们自己的数据集，并使用特定的程序如<a class="ae lk" href="https://github.com/tzutalin/labelImg" rel="noopener ugc nofollow" target="_blank"> LabelImg </a>手工注释它们。然而，这两种想法都非常乏味和耗时(尤其是后者)。另一个选项，也是目前为止对我来说最可行的，是使用公开可用的数据集。我从Kaggle 中选择了<a class="ae lk" href="https://www.kaggle.com/andrewmvd/face-mask-detection" rel="noopener ugc nofollow" target="_blank">人脸面具检测数据集，并将其直接下载到我的Google Drive中(你可以在这里</a><a class="ae lk" href="https://laptrinhx.com/how-to-download-kaggle-datasets-into-google-colab-via-google-drive-1107891156/" rel="noopener ugc nofollow" target="_blank">查看如何做</a>)。下载的数据集包含两个文件夹:</p><ul class=""><li id="0a5e" class="nb nc iq kh b ki kj kl km ko nd ks ne kw nf la ng nh ni nj bi translated"><strong class="kh ir">图像</strong>，包含853。png文件</li><li id="64ff" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated"><strong class="kh ir">注解</strong>，包含853个对应。xml注释。</li></ul><p id="2f4d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下载数据集后，我们需要将。xml文件转换成。更准确地说，我们需要创建YOLO格式来训练我们的模型。下面显示了一个示例:</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="np nq l"/></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">假设这是一个只包含3个边界框的图像的注释(这可以从中的<object> … </object>跨度的数量看出)。xml格式。</p></figure><p id="8634" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了创建一个. txt文件，我们需要从每一个5件事。xml文件。对于an中的每个<em class="nr"> &lt;对象&gt;…&lt;/对象&gt; </em>。xml文件取数<strong class="kh ir">类</strong>(即<em class="nr">名称&gt;…&lt;/名称&gt; </em>字段)，以及<strong class="kh ir">边界框</strong>的坐标(即<em class="nr">&lt;bnd box&gt;…&lt;/bnd box&gt;</em>中的4个属性)。理想的格式如下所示:</p><p id="6c76" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe ns nt nu nv b">&lt;class_name&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;</code></p><p id="c6cf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，为了实现这一点，我创建了一个<a class="ae lk" href="https://github.com/skanelo/Face-Mask-Detection/blob/main/xml_to_yolo.py" rel="noopener ugc nofollow" target="_blank">脚本</a>来获取每个对象的上述5个属性。xml文件并创建相应的。txt文件。<em class="nr"> ( </em> <strong class="kh ir"> <em class="nr">注:</em> </strong> <em class="nr">更多关于转换的方法的解析步骤可以在我的脚本中找到)。</em></p><p id="eb13" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">例如，<code class="fe ns nt nu nv b">image1.jpg</code>必须有一个关联的<code class="fe ns nt nu nv b">image1.txt</code>，包含:</p><pre class="lm ln lo lp gt nw nv nx ny aw nz bi"><span id="f128" class="oa mc iq nv b gy ob oc l od oe">1 0.18359375 0.337431693989071 0.05859375 0.10109289617486339<br/>0 0.4013671875 0.3333333333333333 0.080078125 0.12021857923497267<br/>1 0.6689453125 0.3155737704918033 0.068359375 0.13934426229508196</span></pre><p id="1bdd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">而这正是上面的转换。xml文件转换为. txt文件。<em class="nr"> ( </em> <strong class="kh ir"> <em class="nr">注:</em> </strong> <em class="nr">将对应的图像分组到同一个文件夹中至关重要。txt注释)。</em></p><p id="84f0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当然，在继续训练之前，我们需要绝对确定转换是正确的，并且我们将为我们的网络提供有效的数据。为此，我创建了一个<a class="ae lk" href="https://github.com/skanelo/Face-Mask-Detection/blob/main/show_bb.py" rel="noopener ugc nofollow" target="_blank">脚本</a>，它获取一个图像及其对应的。txt注释，并显示带有基本事实边界框的图像。对于上述示例，图像如下所示:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi of"><img src="../Images/036741fa74c2dd8204070901761d2b00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*uHqlK9f1ZzvC3vNxOj-UWQ.png"/></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">maksssksksss0.png来自<a class="ae lk" href="https://www.kaggle.com/andrewmvd/face-mask-detection" rel="noopener ugc nofollow" target="_blank"> Kaggle的</a>公开发布的面具检测数据集</p></figure><p id="67db" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是当我们知道到目前为止我们做得很好，但让我们继续下去…</p><h1 id="4307" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">火车测试分裂❇️</h1><p id="a55f" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">为了在训练阶段训练和验证我们的模型，我们必须将数据分成两组，训练组和验证组。比例分别为<strong class="kh ir"><em class="nr">90–10%</em></strong>。因此，我创建了两个新文件夹，将86张图片及其相应的注释放入test_folder，将其余767张图片放入train_folder。</p></div><div class="ab cl og oh hu oi" role="separator"><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol"/></div><div class="ij ik il im in"><p id="1108" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nr">再忍耐一下，我们需要一些最后的润色，我们已经准备好训练我们的模型了</em> <strong class="kh ir">😅</strong></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi on"><img src="../Images/b54c03cdbace0162a32384d1d4799422.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0ZkjoENcR9PtLRYy"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">布鲁斯·马斯在<a class="ae lk" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure></div><div class="ab cl og oh hu oi" role="separator"><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol"/></div><div class="ij ik il im in"><h1 id="2c21" class="mb mc iq bd md me oo mg mh mi op mk ml jw oq jx mn jz or ka mp kc os kd mr ms bi translated">克隆暗网框架⬇️</h1><p id="16d3" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">下一步是通过运行以下命令克隆<a class="ae lk" href="https://github.com/AlexeyAB/darknet" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">暗网</strong>回购</a>:</p><pre class="lm ln lo lp gt nw nv nx ny aw nz bi"><span id="bc25" class="oa mc iq nv b gy ob oc l od oe">!git clone <a class="ae lk" href="https://github.com/AlexeyAB/darknet" rel="noopener ugc nofollow" target="_blank">https://github.com/AlexeyAB/darknet</a></span></pre><p id="a8e3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">之后，我们需要下载预训练模型的权重，以便应用迁移学习，而不是从头开始训练模型。</p><pre class="lm ln lo lp gt nw nv nx ny aw nz bi"><span id="6047" class="oa mc iq nv b gy ob oc l od oe">!wget https://pjreddie.com/media/files/darknet53.conv.74</span></pre><p id="6a3d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> darknet53.conv.74 </strong>是YOLOv3网络的主干，它最初是为ImageNet数据集上的分类而训练的，并扮演提取器的角色。为了使用这一点进行检测，在训练之前，随机初始化YOLOv3网络中存在的附加权重。但是当然，他们会在训练阶段得到他们应有的价值观。</p><h1 id="b718" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">最后一步🚨</h1><p id="a6ff" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">我们需要创建5个文件来完成我们的准备工作，并开始训练模型。</p><ol class=""><li id="0d4b" class="nb nc iq kh b ki kj kl km ko nd ks ne kw nf la ot nh ni nj bi translated"><strong class="kh ir"> face_mask.names </strong>:创建文件_ <em class="nr">。包含问题类别的名称</em>。在我们的例子中，原始Kaggle数据集有3个类别:<em class="nr">带_掩码</em>、<em class="nr">不带_掩码</em>和<em class="nr">掩码_磨损_不正确。为了简化任务，我将后两个类别合并成一个。因此，对于我们的任务，我们根据某人是否恰当地佩戴了她的/他的面具，将<strong class="kh ir">分为两个</strong>类别:<em class="nr">好</em>和<em class="nr">坏</em>。</em></li></ol><pre class="lm ln lo lp gt nw nv nx ny aw nz bi"><span id="dadc" class="oa mc iq nv b gy ob oc l od oe">1. Good<br/>2. Bad</span></pre><p id="9a82" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2.<strong class="kh ir"> face_mask.data </strong>:创建一个_ <em class="nr">。数据</em>文件，包含与我们的问题相关的信息，将在程序中使用:</p><pre class="lm ln lo lp gt nw nv nx ny aw nz bi"><span id="123d" class="oa mc iq nv b gy ob oc l od oe">classes = 2<br/>train = data/train.txt<br/>valid  = data/test.txt<br/>names = data/face_mask.names<br/>backup = backup/</span></pre><p id="ae46" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="nr">注:</em> </strong> <em class="nr">如果没有备份文件夹，就创建一个，因为每1000次迭代后都会保存权重。这些实际上是你的检查点，以防意外中断，从那里你可以继续训练过程。</em></p><p id="e92c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">3.<strong class="kh ir"> face_mask.cfg </strong>:这个配置文件必须根据我们的问题进行调整，即我们需要复制yolov3.cfg，将其重命名为<em class="nr"> _。cfg </em>并应用如下所述的修正:</p><ul class=""><li id="26b6" class="nb nc iq kh b ki kj kl km ko nd ks ne kw nf la ng nh ni nj bi translated">将生产线批次更改为<a class="ae lk" href="https://gist.github.com/skanelo/794ce78fdd136bff336a478644c0e736#file-face_mask-cfg-L6" rel="noopener ugc nofollow" target="_blank">批次=64 </a></li><li id="d711" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated">将行细分改为<a class="ae lk" href="https://gist.github.com/skanelo/794ce78fdd136bff336a478644c0e736#file-face_mask-cfg-L7" rel="noopener ugc nofollow" target="_blank">细分=16 </a> ( <strong class="kh ir"> <em class="nr">注:</em> </strong> <em class="nr">如果出现内存不足问题，将该值增加到32或64 </em>)</li><li id="0ae8" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated">将输入尺寸更改为默认的<em class="nr">宽度=416 </em>，<em class="nr">高度=416。(</em> <strong class="kh ir"> <em class="nr">注</em> </strong> <em class="nr">:就我个人而言，我从这个分辨率开始，对我的模型进行了4000次迭代的训练，但为了实现更准确的预测，我</em> <a class="ae lk" href="https://github.com/AlexeyAB/darknet#how-to-improve-object-detection" rel="noopener ugc nofollow" target="_blank"> <em class="nr">提高了分辨率</em> </a> <em class="nr">，并继续了3000次迭代的训练过程</em>)。</li><li id="b87a" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated">将行<a class="ae lk" href="https://gist.github.com/skanelo/794ce78fdd136bff336a478644c0e736#file-face_mask-cfg-L20" rel="noopener ugc nofollow" target="_blank"> max_batches </a>改为(#classes * 2000)，这样我们的任务<em class="nr"> ( </em> <strong class="kh ir"> <em class="nr">注</em> </strong> <em class="nr">)就有4000次迭代:如果你只有一个</em><strong class="kh ir"><em class="nr"/></strong><em class="nr">类别，你不应该只训练你的模型2000次迭代。建议4000次迭代是模型的最小迭代次数)</em>。</li><li id="04ab" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated">将生产线步长更改为<a class="ae lk" href="https://gist.github.com/skanelo/794ce78fdd136bff336a478644c0e736#file-face_mask-cfg-L22" rel="noopener ugc nofollow" target="_blank"> max_batches </a>的80%和90%。对于我们的例子，80/100 * 4000 = 3200，90 / 100 * 4000 = 3600。</li><li id="bf06" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated">使用ctrl+F并搜索单词“yolo”。这将把你直接带到yolo_layers，在那里你想做两件事。更改<strong class="kh ir">类别数量</strong>(对于我们的案例类别=2)并更改[yolo]线上方两个变量的过滤器数量。这个变化必须是<strong class="kh ir">过滤器=(类+ 5) * 3 </strong>，也就是说，对于我们的任务，过滤器= (2 + 5) * 3 = 21。在我们的。cfg文件，有3个yolo_layers，因此你应该做上述的变化3次。</li></ul><p id="2e75" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">4.<strong class="kh ir"> train.txt </strong>例如，我的train.txt文件的一个片段如下所示:</p><pre class="lm ln lo lp gt nw nv nx ny aw nz bi"><span id="19d2" class="oa mc iq nv b gy ob oc l od oe">/content/gdrive/MyDrive/face_mask_detection/mask_yolo_train/maksssksksss734.png<br/>/content/gdrive/MyDrive/face_mask_detection/mask_yolo_train/maksssksksss735.png<br/>/content/gdrive/MyDrive/face_mask_detection/mask_yolo_train/maksssksksss736.png<br/>/content/gdrive/MyDrive/face_mask_detection/mask_yolo_train/maksssksksss737.png<br/>/content/gdrive/MyDrive/face_mask_detection/mask_yolo_train/maksssksksss738.png<br/>...</span></pre><p id="4a55" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nr"> ( </em> <strong class="kh ir"> <em class="nr">注:</em> </strong> <em class="nr">正如我前面提到的。png文件应该与它们对应的。txt注解</em></p><p id="d834" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，我们的项目结构如下:</p><pre class="lm ln lo lp gt nw nv nx ny aw nz bi"><span id="dd51" class="oa mc iq nv b gy ob oc l od oe"><strong class="nv ir">MyDrive</strong><br/>├──<strong class="nv ir">darknet</strong><br/>      ├──...<br/>      ├──<strong class="nv ir">backup</strong><br/>      ├──...<br/>      ├──<strong class="nv ir">cfg</strong><br/>            ├──face_mask.cfg</span><span id="3e2f" class="oa mc iq nv b gy ou oc l od oe">      ├──...<br/>      ├──<strong class="nv ir">data</strong><br/>            ├──face_mask.data<br/>            ├──face_mask.names<br/>            ├──train.txt<br/>            ├──test.txt</span><span id="bfe2" class="oa mc iq nv b gy ou oc l od oe">├──<strong class="nv ir">face_mask_detection</strong><br/>      ├──<strong class="nv ir">annotations       </strong><em class="nr">(contains original .xml files)<br/>      </em>├──<strong class="nv ir">images            </strong><em class="nr">(contains the original .png images)<br/>      </em>├──<strong class="nv ir">mask_yolo_test    </strong><em class="nr">(contains .png % .txt files for testing)<br/>      </em>├──<strong class="nv ir">mask_yolo_train   </strong><em class="nr">(contains .png % .txt files for training)</em><br/>      ├── show_bb.py<br/>      └── xml_to_yolo.py</span></pre><h1 id="77bd" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">让我们开始训练吧📈</h1><p id="4e6a" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">在我们<a class="ae lk" href="https://github.com/AlexeyAB/darknet#how-to-compile-on-linux-using-make" rel="noopener ugc nofollow" target="_blank">编译</a>模型之后，我们需要更改相关的权限，如下所示:</p><pre class="lm ln lo lp gt nw nv nx ny aw nz bi"><span id="19f8" class="oa mc iq nv b gy ob oc l od oe">!chmod +x ./darknet</span></pre><p id="70e3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们可以从跑步开始训练:</p><pre class="lm ln lo lp gt nw nv nx ny aw nz bi"><span id="757f" class="oa mc iq nv b gy ob oc l od oe">!./darknet detector train data/face_mask.data cfg/face_mask.cfg backup/face_mask_last.weights -dont_show -i 0 -map</span></pre><p id="68c1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">标志<code class="fe ns nt nu nv b">-map</code> <em class="nr"> </em>会通过打印出平均丢失、精度、召回、平均精度(AP)、平均精度(mAP)等重要指标来通知我们训练的进度。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ov"><img src="../Images/dde1fcc9ce9211a01f4c9a99644050b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HjiqlcdobS45Ch4rdMudLw.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">作者图片</p></figure><p id="0ed8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，控制台中的mAP指示器被认为是比损耗更好的指标，所以在mAP增加时进行训练。</p><p id="9158" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nr"> ( </em> <strong class="kh ir"> <em class="nr">注:</em> </strong> <em class="nr">训练过程可能需要</em> <strong class="kh ir"> <em class="nr">多个小时</em></strong><em class="nr">……这是正常的。对于这个项目，为了训练我的模型到这一点，我需要大约15个小时。但是我在大约7个小时内完成了4000个步骤的训练，对这个模型有了第一印象</em>。</p><h1 id="d164" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">测试(和讨论)时间到了🎉</h1><p id="fb3a" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">是的…模型已经准备好展示了！！！让我们尝试一些它从未见过的图像。为此，我们需要运行:</p><pre class="lm ln lo lp gt nw nv nx ny aw nz bi"><span id="65a3" class="oa mc iq nv b gy ob oc l od oe">!./darknet detector test data/face_mask.data cfg/face_mask.cfg backup/<strong class="nv ir">face_mask_best.weights</strong></span></pre><p id="64f3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你是否注意到我们使用了<strong class="kh ir"> face_mask_best.weights </strong>而不是<em class="nr">face _ mask _ final . weights</em>？<strong class="kh ir"> </strong>幸运的是，我们的模型将最佳权重(<strong class="kh ir">map . 5</strong><strong class="kh ir"><em class="nr">87.16%</em></strong>被达到)保存在备份文件夹中，以防我们对其进行比它应该的更多的时期的训练(这可能会导致过度拟合)。</p><p id="d597" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面显示的示例取自<a class="ae lk" href="https://www.pexels.com/" rel="noopener ugc nofollow" target="_blank">像素</a>，是高分辨率图像，用肉眼看，我可以说它们与来自不同点的训练/测试数据集非常不同，因此它们具有不同的分布。我选择这样的图片是为了看看这个模型的概括能力有多强。</p></div><div class="ab cl og oh hu oi" role="separator"><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol"/></div><div class="ij ik il im in"><div class="lm ln lo lp gt ab cb"><figure class="ow lq ox oy oz pa pb paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><img src="../Images/a38c5fbdfca1eda0f61d596883f97787.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*Mr9cvpCzyBDxJQi-kY7tWA.jpeg"/></div></figure><figure class="ow lq ox oy oz pa pb paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><img src="../Images/941c10217882b25336e3728940efa5ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*WfRns8wgyah9b6Hi6rZEhQ.jpeg"/></div></figure><figure class="ow lq ox oy oz pa pb paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><img src="../Images/aaba67c8a98ab92e32180033b9683b6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*o5-9Unhkrd0YONqukcYffQ.jpeg"/></div><p class="lx ly gj gh gi lz ma bd b be z dk pc di pd pe translated"><strong class="bd mz">(左)</strong>模特对一张照片的预测<a class="ae lk" href="https://www.pexels.com/@charlotte-may?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">夏洛特梅</a>来自<a class="ae lk" href="https://www.pexels.com/photo/crop-attractive-asian-woman-putting-on-mask-on-street-5965831/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">派克斯</a> | <strong class="bd mz">(中)</strong>模特对一张照片的预测<a class="ae lk" href="https://www.pexels.com/@tim-douglas?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">蒂姆道格拉斯</a>来自<a class="ae lk" href="https://www.pexels.com/photo/women-in-masks-with-shopping-bags-walking-on-street-during-coronavirus-6567212/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">派克斯</a> | <strong class="bd mz">(右)</strong>模特对一张照片的预测来自<a class="ae lk" href="https://www.pexels.com/@shvetsa?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">安娜什韦茨</a>来自<a class="ae lk" href="https://www.pexels.com/photo/photo-of-person-wearing-protective-wear-while-holding-globe-4167541/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">派克斯</a></p></figure></div><p id="f3d0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上述示例中，模型是准确的，并且对其预测相当有信心。值得注意的是，右边的图像没有将模型与地球上面具的存在混淆。它揭示了这些预测不仅仅是基于面具的存在，还基于它周围的环境。</p><div class="lm ln lo lp gt ab cb"><figure class="ow lq pf oy oz pa pb paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><img src="../Images/9af5ab65315c1000d1e25f3e9d559f77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/format:webp/1*CF8BNt8O8xweS2VYaF6FSQ.jpeg"/></div></figure><figure class="ow lq pg oy oz pa pb paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><img src="../Images/22c9b3b0fffc78ca4ebb1ed04f45c6b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*SGjo9cAzi2twWkXNV9pVgg.jpeg"/></div><p class="lx ly gj gh gi lz ma bd b be z dk ph di pi pe translated"><strong class="bd mz">(左)</strong>模特对一张照片的预测<a class="ae lk" href="https://www.pexels.com/@kindelmedia?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">金德尔传媒</a>发自<a class="ae lk" href="https://www.pexels.com/photo/people-smiling-on-the-poolside-7294264/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">佩克斯</a> | <strong class="bd mz">(右)</strong>模特对一张照片的预测<a class="ae lk" href="https://www.pexels.com/@apasaric?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">亚历山大·帕萨里克</a>发自<a class="ae lk" href="https://www.pexels.com/photo/woman-wearing-black-monokini-2038014/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">佩克斯</a></p></figure></div><p id="dc36" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这两个例子清楚地表明，被描绘的人没有戴面具，而且模特也很容易识别这一点。</p><div class="lm ln lo lp gt ab cb"><figure class="ow lq pj oy oz pa pb paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><img src="../Images/bb581904a572aebbe530cafa83c0fd75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*KFBoYtfnS-LSe-YSDXuqNw.jpeg"/></div></figure><figure class="ow lq pj oy oz pa pb paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><img src="../Images/da73d7a18c1d4c47fcc3737318109c0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*2V7w6wHuLM2sWTRoItRZ9A.jpeg"/></div><p class="lx ly gj gh gi lz ma bd b be z dk pk di pl pe translated"><strong class="bd mz">(左)</strong>模特对一张照片的预测<a class="ae lk" href="https://www.pexels.com/@norma-mortenson?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">诺玛·莫滕森</a>来自<a class="ae lk" href="https://www.pexels.com/photo/men-putting-food-on-a-thermal-bag-4393665/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">派克斯</a> | <strong class="bd mz">(右)</strong>模特对一张照片的预测<a class="ae lk" href="https://www.pexels.com/@life-matters-3043471?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">生活要紧</a>来自<a class="ae lk" href="https://www.pexels.com/photo/man-with-raised-fist-in-a-protest-4614144/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">派克斯</a></p></figure></div><p id="52ba" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上面的两个例子中，我们可以在两个类别都出现的情况下测试模型的性能。该模型甚至可以在模糊的背景中识别人脸，这一事实令人钦佩。我还观察到，与其后面的预测(在模糊区域上为100%)相比，其不太确定的最前面的预测(在清晰区域上仅为38%)可能与被训练的数据集的质量有关，因此它似乎在一定程度上受到影响(至少它不是不准确的😅 ).</p><h2 id="b305" class="oa mc iq bd md pm pn dn mh po pp dp ml ko pq pr mn ks ps pt mp kw pu pv mr pw bi translated">最后一次测试🐵</h2><p id="0f03" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">当然，YOLO的一大优势是它的速度。出于这个原因，我还想向您展示它在输入视频时是如何工作的:</p><pre class="lm ln lo lp gt nw nv nx ny aw nz bi"><span id="08e9" class="oa mc iq nv b gy ob oc l od oe">!./darknet detector demo data/face_mask.data cfg/face_mask.cfg backup/face_mask_best.weights -dont_show vid1.mp4 -i 0 -out_filename res1.avi</span></pre><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi px"><img src="../Images/f3d20a8c767be061f4647574d59dfcb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*OvEqLI2bHTT91cFYit3lrw.gif"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">作者对视频流、图像的推断</p></figure></div><div class="ab cl og oh hu oi" role="separator"><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol"/></div><div class="ij ik il im in"><h1 id="5502" class="mb mc iq bd md me oo mg mh mi op mk ml jw oq jx mn jz or ka mp kc os kd mr ms bi translated">结论👏</h1><p id="51ee" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">这是我的第一个分步教程，介绍如何在自定义数据集上使用YOLOv3构建自己的检测器。希望你觉得有用。请随时给我反馈或询问任何相关问题。</p><p id="e096" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">非常感谢您的宝贵时间！一会儿见…😜</p></div></div>    
</body>
</html>