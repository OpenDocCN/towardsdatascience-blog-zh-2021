<html>
<head>
<title>Supervised Learning algorithms cheat sheet</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">监督学习算法备忘单</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/supervised-learning-algorithms-cheat-sheet-40009e7f29f5?source=collection_archive---------10-----------------------#2021-09-21">https://towardsdatascience.com/supervised-learning-algorithms-cheat-sheet-40009e7f29f5?source=collection_archive---------10-----------------------#2021-09-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9d03" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">你应该知道的所有监督机器学习算法的完整备忘单，包括优点、缺点和超参数</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3a9adfbca04843e5e4329efe6ac78ff9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l0uEwdu9ordtJ4i8lIT3eg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">监督机器学习算法的本质。作者图片</p></figure><p id="0505" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">本文提供了不同的监督学习机器学习概念和算法的备忘单。这不是一个教程，但它可以帮助你更好地理解机器学习的结构，或者刷新你的记忆。</p><p id="b19b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">要了解更多关于特定算法的信息，只需谷歌一下或者在<a class="ae lu" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> <em class="lv"> sklearn文档</em> </a>中查找。</p><p id="a449" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">接下来，将回顾或提及以下算法:</p><ul class=""><li id="9acb" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated"><em class="lv">线性回归</em></li><li id="eadf" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated"><em class="lv">逻辑回归</em></li><li id="ae7a" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated"><em class="lv">支持向量机</em></li><li id="fcd1" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated"><em class="lv">k-最近邻</em></li><li id="c367" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated"><em class="lv">决策树</em></li><li id="fe0f" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated"><em class="lv">装袋</em>和<em class="lv">粘贴</em></li><li id="fe8d" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated"><em class="lv">随机森林</em>和<em class="lv">多余的树木</em></li><li id="175c" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated"><em class="lv">增压</em></li><li id="5d63" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated"><em class="lv">堆垛</em>和<em class="lv">混合</em></li></ul><p id="43ff" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这篇文章卷帙浩繁，不建议你一次看完。将这篇文章添加到阅读列表中以便稍后回来，通过GitLab 阅读章节<a class="ae lu" href="https://gitlab.com/Winston-90/supervised_algorithms" rel="noopener ugc nofollow" target="_blank">或下载这篇文章的pdf版本并打印出来(可在同一位置获得)。</a></p><div class="mk ml gp gr mm mn"><a href="https://gitlab.com/Winston-90/supervised_algorithms" rel="noopener  ugc nofollow" target="_blank"><div class="mo ab fo"><div class="mp ab mq cl cj mr"><h2 class="bd iu gy z fp ms fr fs mt fu fw is bi translated">Dmytro Nikolaiev /监督算法</h2><div class="mu l"><h3 class="bd b gy z fp ms fr fs mt fu fw dk translated">监督学习算法备忘单</h3></div><div class="mv l"><p class="bd b dl z fp ms fr fs mt fu fw dk translated">gitlab.com</p></div></div><div class="mw l"><div class="mx l my mz na mw nb ks mn"/></div></div></a></div></div><div class="ab cl nc nd hx ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="im in io ip iq"><h2 id="2260" class="nj nk it bd nl nm nn dn no np nq dp nr lh ns nt nu ll nv nw nx lp ny nz oa ob bi translated">介绍</h2><p id="4a14" class="pw-post-body-paragraph ky kz it la b lb oc ju ld le od jx lg lh oe lj lk ll of ln lo lp og lr ls lt im bi translated"><em class="lv">监督学习</em>是基于示例输入输出对学习将输入映射到输出的函数的机器学习任务。监督学习算法分析训练数据并产生推断的函数，该函数可以在以后用于映射新的示例。</p><p id="3d2e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最受欢迎的监督学习任务是<em class="lv">回归</em>和<em class="lv">分类</em>。</p><p id="2c75" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">解决<em class="lv">回归</em>任务的结果是一个可以进行<em class="lv">数值预测</em>的模型。例如:</p><ul class=""><li id="af63" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated">房地产价值预测</li><li id="8ae0" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">预测贵公司明年的收入</li></ul><p id="f357" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">解决<em class="lv">分类</em>任务的结果是一个可以进行<em class="lv">分类预测</em>的模型。例如:</p><ul class=""><li id="e1e6" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated">垃圾邮件检测</li><li id="75d7" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">新闻文章分类</li><li id="bec0" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">根据验血预测患癌概率(0到1之间)。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/f21cb479efcb5b7a76118f18517c84fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OGkIPmqe5uuCoshzxyQQVw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">分类与回归。作者图片</p></figure><p id="84c4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">分类算法也可以分为<strong class="la iu">硬</strong>和<strong class="la iu">软</strong>:</p><ul class=""><li id="5848" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated"><strong class="la iu">硬分类算法</strong>预测数据点是否属于特定类别<strong class="la iu">，而不产生概率估计</strong>。</li><li id="33b5" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated"><strong class="la iu">软分类算法</strong>反过来也估计了类别条件<strong class="la iu">概率</strong>。</li></ul><p id="7b81" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">分类算法也可以按照要分类的类的数量来划分:</p><ul class=""><li id="4a07" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated"><strong class="la iu">二元分类</strong>——只有两类。</li><li id="b777" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated"><strong class="la iu">多类分类</strong> —两个以上的类。</li><li id="d027" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated"><strong class="la iu">多标记分类</strong>(multi label-multi class)——多个类，但类是二元的(图像中人的存在)。结果— [0，0，1]或[1，0，1]。</li><li id="0446" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated"><strong class="la iu">多输出分类</strong> (Multioutput-multiclass)又称<strong class="la iu">多任务分类</strong> —多个类，但类不是二进制的(预测项数)。结果— [5，0，1]或[7，0，0]。</li></ul><p id="89bc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有些算法是专为二元分类问题设计的(例如<em class="lv"> SVM </em>)。因此，它们不能直接用于多类分类任务。相反，可以使用启发式方法将多类分类问题分成多个二进制分类数据集，并分别训练一个二进制分类模型:</p><ul class=""><li id="b088" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated"><strong class="la iu">OvR</strong>(one-vs-rest)——有时<strong class="la iu">OvA</strong>(one-vs-all)——你必须为N个类训练N个分类器，但是是在完整的数据集上。</li><li id="d583" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated"><strong class="la iu"> OvO </strong>(一对一)——你必须为N个类训练N*(N-1)/2个分类器，但是是在你的数据集中的子样本上。更适合不平衡的数据。</li></ul><p id="a638" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注意，<em class="lv">除了<em class="lv">线性回归(仅回归)</em>和<em class="lv">逻辑回归(仅分类)</em>之外，所有接下来的算法都解决分类和回归任务</em>。</p><h1 id="809b" class="oi nk it bd nl oj ok ol no om on oo nr jz op ka nu kc oq kd nx kf or kg oa os bi translated">简单的算法</h1><p id="491f" class="pw-post-body-paragraph ky kz it la b lb oc ju ld le od jx lg lh oe lj lk ll of ln lo lp og lr ls lt im bi translated">我使用短语<em class="lv">简单算法</em>并不是指它们实现起来简单(尽管它们中的一些确实很简单)，而是指这些是独立的算法，而不是我们稍后将看到的<em class="lv">集成学习</em>。</p><h2 id="9a62" class="nj nk it bd nl nm nn dn no np nq dp nr lh ns nt nu ll nv nw nx lp ny nz oa ob bi translated">线性回归</h2><p id="f98a" class="pw-post-body-paragraph ky kz it la b lb oc ju ld le od jx lg lh oe lj lk ll of ln lo lp og lr ls lt im bi translated">在最简单的情况下，回归任务是通过数据点画一条线，使得这条线(预测)和真实值之间的误差最小。一般来说，这是<em class="lv">最小化损失函数</em>的问题，所以<em class="lv">优化问题</em>。通常损失函数是<em class="lv"> MSE —均方误差</em>(因为<em class="lv">最大似然估计</em>)，优化算法是<em class="lv">梯度下降</em>。总之，可以使用优化算法的任何其他损失函数。</p><p id="db6e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">线性回归的一个重要特性是，最优参数(根据<em class="lv"> MSE </em>，同样是因为<em class="lv">最大似然估计</em>)可以用简单的<strong class="la iu">正态方程</strong>计算出来。但是这种方法不能很好地适应大量的特征，因此可以应用任何其他优化方法来代替。</p><p id="5011" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果数据相关性比直线更复杂，我们可以添加每个特征的幂作为新特征(可以使用来自<em class="lv"> sklearn </em>的<em class="lv">多项式特征</em>类),然后训练线性回归模型。这种技术被称为<strong class="la iu">多项式回归</strong>。创造新特征的过程(如xⁿ，或log(x)，eˣ等)。)被称为<em class="lv">特征工程</em>，可以显著提高线性模型性能。</p><p id="4cfe" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这种算法的另一个流行版本是<strong class="la iu">贝叶斯线性回归</strong>，它通过建立<em class="lv">置信区间</em>，不仅预测数值，还预测概率。多亏了贝叶斯定理，这才成为可能。</p><p id="7d54" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">避免过度拟合和离群值影响回归的最有效方法之一是<strong class="la iu">正则化</strong>。正则化项<em class="lv">被添加到损失函数中，因此回归系数必须尽可能小。</em></p><ul class=""><li id="ee1f" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated"><strong class="la iu">套索回归</strong> —实现L1正则化，+ |coeff|。</li><li id="36c3" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated"><strong class="la iu">岭回归</strong> —实现L2正则化，+ coeff。又称<em class="lv">吉洪诺夫正规化</em>。</li><li id="aacc" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated"><strong class="la iu">弹性网络回归</strong> —实现L1和L2正则化。</li></ul><p id="ecb0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正则化回归也可以像<em class="lv">特征选择</em>工具一样使用。例如，由于某些属性，LASSO回归可以删除无关紧要的要素(将其系数设置为零)。</p><p id="c8e4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如前所述，<em class="lv">线性回归只解决回归任务</em>。</p><p id="fdfa" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">主要超参数</strong>:</p><ul class=""><li id="90a8" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated"><em class="lv">特征工程</em>和<em class="lv">特征选择</em></li><li id="f6ab" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">正则化类型和参数</li><li id="e6e9" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">求解器—优化算法</li></ul><p id="46f9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">优点</strong>:</p><ul class=""><li id="b58f" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated">参数少，学习速度快</li><li id="5a60" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">可使用<em class="lv">随机梯度下降</em>进行配置，无需将所有样本存储在存储器中，并可用于<em class="lv">在线学习</em></li><li id="9e8c" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">比复杂的模型更容易解释</li><li id="8642" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">非常适用于具有少量数据点和大量要素的问题</li><li id="67a2" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">非常适合稀疏数据</li></ul><p id="79d8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">缺点</strong>:</p><ul class=""><li id="9cfc" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated">复杂依赖关系恢复不佳</li><li id="dec2" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">需要数据预处理</li></ul></div><div class="ab cl nc nd hx ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="im in io ip iq"><h2 id="e818" class="nj nk it bd nl nm nn dn no np nq dp nr lh ns nt nu ll nv nw nx lp ny nz oa ob bi translated">逻辑回归</h2><p id="09fc" class="pw-post-body-paragraph ky kz it la b lb oc ju ld le od jx lg lh oe lj lk ll of ln lo lp og lr ls lt im bi translated">与线性回归模型一样，逻辑回归(也称为<strong class="la iu"> logit regression </strong>)计算输入要素(加上偏差)的加权和，但它不是直接输出该结果，而是输出结果的<em class="lv">逻辑</em>。<em class="lv">逻辑</em>是一个<em class="lv"> sigmoid函数</em>，它输出一个介于0和1之间的数字，因此逻辑回归是一个<strong class="la iu">软二元分类器</strong>，它估计实例属于正类的概率。根据某些阈值，可以获得不同的准确度/召回率值。可以使用与线性回归中相同类型的正则化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/a6ed372210cbd93631a3db4b06f4d949.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m8DOa404kORstVA0QvOe3w.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">乙状结肠函数。<a class="ae lu" href="https://en.wikipedia.org/wiki/Sigmoid_function#/media/File:Logistic-curve.svg" rel="noopener ugc nofollow" target="_blank">公共领域</a></p></figure><p id="45d0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">非常相似的<strong class="la iu">概率单位回归</strong>使用了稍微不同的函数——概率单位函数，而不是sigmoid。</p><p id="d3d0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">可以将逻辑回归模型推广到直接支持多个类别，而无需训练多个分类器。这被称为<strong class="la iu"> Softmax回归</strong>(或<em class="lv">多项逻辑回归</em>)。该模型计算每个类别的分数，然后通过应用<em class="lv"> softmax函数</em>(也称为<em class="lv">归一化指数</em>)来估计每个类别的概率。</p><p id="c7ef" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如前所述，<em class="lv">逻辑回归只解决分类任务</em>。</p><p id="55e5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">基于线性回归，因此继承了该算法的所有超参数、优点和缺点。需要单独说明的是，该算法的<em class="lv">解释</em>水平较高，因此通常被广泛用于<em class="lv">信用评分</em>任务和<em class="lv">医疗诊断</em>。</p></div><div class="ab cl nc nd hx ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="im in io ip iq"><h2 id="78ec" class="nj nk it bd nl nm nn dn no np nq dp nr lh ns nt nu ll nv nw nx lp ny nz oa ob bi translated">支持向量机</h2><p id="07b9" class="pw-post-body-paragraph ky kz it la b lb oc ju ld le od jx lg lh oe lj lk ll of ln lo lp og lr ls lt im bi translated">支持向量机算法基于<em class="lv">支持向量</em>概念——极值点(图中用黑色圈出)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/7ff926c469b72768f122d635a5c9278f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5JGnvKLb0P513oqtNAcjkg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">支持向量。<a class="ae lu" href="https://en.wikipedia.org/wiki/Support-vector_machine#/media/File:SVM_margin.png" rel="noopener ugc nofollow" target="_blank">公共领域</a></p></figure><p id="3837" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在<em class="lv">分类任务</em>的情况下，它试图在类别之间画一条分隔线，使得<em class="lv">支持向量</em>位于离这条线尽可能远的地方(在一般情况下是分离超平面):</p><ul class=""><li id="6140" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated"><em class="lv">硬边界分类</em> —假设相同类别的实例无例外地位于分离超平面的同一侧。</li><li id="a14d" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated"><em class="lv">软边界分类</em>-允许违反决策边界，由正则化参数控制。</li></ul><p id="d3c5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在<em class="lv">回归任务</em>的情况下，相反，它试图画一条线来适应边界内尽可能多的实例，“在街上”。</p><p id="7ee9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于SVM要求计算点与点之间的距离，它也要求<em class="lv">特征缩放</em>。</p><p id="4174" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">SVM最重要也是数学上最优雅的特征是，<em class="lv">对偶问题</em>(这是SVM的基础)的解不直接依赖于特征(作为向量)，而<em class="lv">只依赖于它们的两两标量积</em>。这就允许我们用某个函数<em class="lv"> k(a，b) </em>来代替标量积，这个函数叫做<em class="lv">内核</em>。事实上，内核是另一个空间中的<em class="lv">标量积。这个过程允许你在不增加新特征的情况下建立非线性分类器(在更大的维度空间中实际上是线性的),被称为<strong class="la iu">内核技巧</strong>。</em></p><p id="e6b3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">不同内核的使用允许该算法在<em class="lv">分类</em>和<em class="lv">回归</em>任务中恢复非常复杂的相关性。最受欢迎的内核是:</p><ul class=""><li id="ed65" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated">多项式</li><li id="db2a" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">RBF —高斯径向基函数</li><li id="99f8" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">乙状结肠和其他</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/32cc1fe1908391e78debdfc0c8849a7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5lLgaW2Z_qrLYD2_jgx3Ig.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">具有不同内核和默认参数的SVM。作者图片</p></figure><p id="d085" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一级SVM也可以用于<em class="lv">异常检测</em>问题。</p><p id="c3e8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">主要超参数</strong>:</p><ul class=""><li id="883a" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated">内核类型</li><li id="f525" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">正则化参数C——对每个错误分类的数据点的惩罚(通常为0.1 &lt; C &lt; 100)</li><li id="a3d5" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">regularization parameter gamma — controls regions separating different classes. Large gamma leads to too specific class regions (overfitting). (usually 0.0001 &lt; gamma &lt; 10)</li></ul><p id="7ee0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">赞成</strong>):</p><ul class=""><li id="ae07" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated">最强大和最灵活的模型之一</li><li id="270a" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">由于线性模型继承了线性回归的优点</li></ul><p id="99f1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">缺点</strong>:</p><ul class=""><li id="0937" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated">需要数据预处理</li><li id="12d1" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">它适用于许多要素，但不适用于样本，因此仅适用于小型和中型数据集</li></ul></div><div class="ab cl nc nd hx ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="im in io ip iq"><h2 id="c02a" class="nj nk it bd nl nm nn dn no np nq dp nr lh ns nt nu ll nv nw nx lp ny nz oa ob bi translated">k-最近邻</h2><p id="7377" class="pw-post-body-paragraph ky kz it la b lb oc ju ld le od jx lg lh oe lj lk ll of ln lo lp og lr ls lt im bi translated">最近邻算法作为<em class="lv">度量方法</em>的代表，对数据分布做了两个假设:</p><ul class=""><li id="c178" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated"><em class="lv">回归</em>的连续性假设——相近的对象对应相近的答案，并且</li><li id="3c59" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated"><em class="lv">分类</em>的紧密性假设——接近的对象对应同一类。</li></ul><p id="4a5d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于一个新的对象，我们必须找到k个最近的邻居。最近的<em class="lv">的定义取决于我们想要使用的距离度量(曼哈顿、欧几里德等。).</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/2ce3102b8f6ebb011d341f21802a7a80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tMlNFFtvkzIsesMDvv9MAA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">k-最近邻算法。结果可能因k. <strong class="bd ox"> </strong> <a class="ae lu" href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm#/media/File:KnnClassification.svg" rel="noopener ugc nofollow" target="_blank">公共域</a>而异</p></figure><p id="422a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最重要的超参数是相邻点的数量——k。k的一个好的初始近似值是将<em class="lv"> k设置为数据点数</em>的平方根，但是，当然，k可以通过<em class="lv">交叉验证</em>找到。<em class="lv">分类</em>然后根据每个点的最近邻居的简单多数投票计算，<em class="lv">回归</em>——根据每个点的最近邻居的平均值。</p><p id="6e38" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">主要超参数</strong>:</p><ul class=""><li id="883d" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated">k —邻居的数量</li><li id="e66b" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">距离度量</li></ul><p id="a2ee" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">优点</strong>:</p><ul class=""><li id="605f" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated"><em class="lv">懒惰学习</em>——我们只需将数据载入内存</li><li id="26e1" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">简单解读</li><li id="ebe6" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">简单实现</li></ul><p id="89a6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">缺点</strong>:</p><ul class=""><li id="4856" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated">需要数据预处理</li><li id="a7ce" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">复杂的依赖关系恢复不佳(高度重叠数据的分类)</li><li id="cb44" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">任何度量算法在处理稀疏的高维数据时都表现不佳</li><li id="b75f" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">由于任何基于<em class="lv">实例的算法</em>都必须将所有训练数据存储在内存中</li></ul></div><div class="ab cl nc nd hx ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="im in io ip iq"><h2 id="7a2c" class="nj nk it bd nl nm nn dn no np nq dp nr lh ns nt nu ll nv nw nx lp ny nz oa ob bi translated">决策树</h2><p id="2eea" class="pw-post-body-paragraph ky kz it la b lb oc ju ld le od jx lg lh oe lj lk ll of ln lo lp og lr ls lt im bi translated">在每一步，训练集被分成两个(或更多)部分，这取决于特定的选择。通常，树构建算法是<em class="lv">贪婪</em>，这意味着，它们在特定步骤寻找<em class="lv">局部</em>最优解。构建树的常用算法有:</p><ul class=""><li id="3d08" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated"><strong class="la iu"> ID3 </strong>(最古老的算法之一，<em class="lv">迭代二分法3 </em>是由<em class="lv">罗斯·昆兰</em>发明的)，</li><li id="3fe8" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated"><strong class="la iu"> C4.5，c 5.0</strong>(ID3算法的扩展，它们是由同一个人开发的，并且在于<em class="lv">在使用ID3之后修剪</em>树)，</li><li id="27ae" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated"><strong class="la iu"> CART </strong> ( <em class="lv">分类和回归树</em>针对分类(<em class="lv">基尼系数</em>作为度量)和回归(<em class="lv"> MSE </em>作为度量)树进行了优化，并在<em class="lv"> scikit-learn </em>中实现。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/ba156209eb9075bf17c68ed87dcc63d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*63sERa9Tqsckqst0ck2m6w.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">决策树分类器和回归器。作者图片</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/e664dd83b7cbc7740ba006e08e0810d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:920/format:webp/1*cHpTBmL4Om_XknBgJwk8hA.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用上述示例决策树分类器的决策树结构。作者图片</p></figure><p id="8bea" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">可以使用不同的方法来计算<em class="lv">信息增益</em>。然后决策树算法使用信息增益来拆分特定节点:</p><ul class=""><li id="1670" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated"><em class="lv">熵</em>——无序的一种度量。</li><li id="718f" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated"><em class="lv">基尼杂质</em>。</li></ul><p id="155d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所谓的<strong class="la iu">决策树剪枝</strong>比单纯限制树的长度更能说明问题。这是当我们建立一个完整深度的树时的过程，在此之后，我们移除树的无关紧要的节点。然而，这一过程需要更多的资源。</p><p id="e7ee" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">主要超参数</strong>:</p><ul class=""><li id="fca4" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated">树的最大深度:越小-过度拟合越小，通常为10–20</li><li id="1e89" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">一个叶中的最小对象数:越大-过度拟合越少，通常为20+</li></ul><p id="359e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">优点</strong>:</p><ul class=""><li id="b61d" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated">简单解读</li><li id="8f72" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">简单实现</li><li id="fc68" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">计算简单性</li><li id="e473" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">不需要特征预处理，可以处理缺失值</li><li id="1782" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">可以使用信息增益来计算特征重要性</li></ul><p id="ce27" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">缺点</strong>:</p><ul class=""><li id="0765" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated">不稳定且易变(对贪婪算法的研究)-输入数据的微小变化会完全改变树的结构</li><li id="6fbb" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">对训练集的内容和噪声高度敏感</li><li id="ef30" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">复杂(非线性)依赖关系的恢复较差</li><li id="b493" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">在树的较大深度处过度拟合的趋势</li><li id="715e" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">与线性模型不同，它们不是外推的(它们只能预测从训练集的最小值到最大值范围内的值)</li></ul></div><div class="ab cl nc nd hx ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="im in io ip iq"><h1 id="bc75" class="oi nk it bd nl oj oz ol no om pa oo nr jz pb ka nu kc pc kd nx kf pd kg oa os bi translated">集成方法</h1><p id="e058" class="pw-post-body-paragraph ky kz it la b lb oc ju ld le od jx lg lh oe lj lk ll of ln lo lp og lr ls lt im bi translated">集成方法(也称为<strong class="la iu">集成学习</strong>)是创建多个模型，然后将它们组合起来以产生改进结果的技术。集合方法通常比单一模型产生更精确的解。</p><h2 id="6665" class="nj nk it bd nl nm nn dn no np nq dp nr lh ns nt nu ll nv nw nx lp ny nz oa ob bi translated">制袋材料</h2><p id="9aa4" class="pw-post-body-paragraph ky kz it la b lb oc ju ld le od jx lg lh oe lj lk ll of ln lo lp og lr ls lt im bi translated"><strong class="la iu">装袋</strong>代表<em class="lv">引导汇总</em>。</p><p id="0500" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当我们有一个训练集<code class="fe pe pf pg ph b">X_train (N x M) N data points and M features</code>时，我们在<code class="fe pe pf pg ph b">X</code>上训练<code class="fe pe pf pg ph b">n</code>个模型，其中<code class="fe pe pf pg ph b">X (N x M)</code>是<code class="fe pe pf pg ph b">X_train</code>的一个相同大小的随机子样本。当<code class="fe pe pf pg ph b">X</code>成型时<strong class="la iu">带替换</strong>的算法称为<strong class="la iu">装袋</strong>，当<code class="fe pe pf pg ph b">X</code>成型时<strong class="la iu">不带替换</strong>的算法称为<strong class="la iu">粘贴</strong>。当这个模型进行预测时，实际上，它从<code class="fe pe pf pg ph b">n</code>个不同的模型中获得<code class="fe pe pf pg ph b">n</code>个预测，并汇总它们。<em class="lv">分类</em>通过模型的简单多数投票计算，回归<em class="lv">通过模型预测的平均值计算。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pi"><img src="../Images/e2807397cf61fd6b91a229ef15edab5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PU1O6QkqJKjvJp9alQH0_g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">装袋。作者图片</p></figure><p id="c41e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">粘贴</strong>最初是为<strong class="la iu">大数据集</strong>计算能力有限时设计的。<strong class="la iu">另一方面，Bagging </strong>可以多次使用相同的子集，这对于较小的样本量非常有用，它提高了稳健性。</p><p id="e84e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这种方法允许<strong class="la iu">保持相同的偏差，但是由于<em class="lv">中心极限定理</em>而减少了方差</strong>。算法的变量越多，其预测的相关性就越低，因此，CLT的效果就越好(决策树是一个很好的选择)。</p><p id="0bbf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们使用装袋，有可能一个样本永远不会被选中，而其他样本可能会被多次选中。一般来说，对于一个大数据集，37%的样本从未被选择，我们可以用它来测试我们的模型。这被称为<strong class="la iu">出袋得分</strong>，或<strong class="la iu"> OOB得分</strong>。</p><p id="fecf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">主要超参数</strong>:</p><ul class=""><li id="8bb3" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated">型号类型</li><li id="9dc4" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">n_estimators —集合中模型的数量</li><li id="2fb9" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">max _ samples从训练集中获取的用于训练每个基本模型的样本数</li><li id="8c89" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">max _ features从训练集中提取的用于训练每个基础模型的特征数</li></ul><p id="ee64" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">优点</strong>:</p><ul class=""><li id="19ef" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated">质量非常好</li><li id="1030" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">训练过程可以简单地并行化，因为模型相互独立地学习</li><li id="2615" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">不需要特征预处理和特征重要性的内置评估(在树的情况下)</li><li id="e4cf" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">抗过度配合</li><li id="76e4" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">抵抗异常值</li><li id="2b30" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated"><em class="lv"> OOB评分</em>允许使用完整数据集，而无需将其分为训练和验证</li></ul><p id="5c5f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">缺点</strong>:</p><ul class=""><li id="e0b7" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated">解释的复杂性</li><li id="be51" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">无法很好地处理大量要素或稀疏数据</li><li id="239b" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">训练和预测速度明显慢于线性模型</li></ul></div><div class="ab cl nc nd hx ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="im in io ip iq"><h2 id="ebc2" class="nj nk it bd nl nm nn dn no np nq dp nr lh ns nt nu ll nv nw nx lp ny nz oa ob bi translated">随机森林</h2><p id="b667" class="pw-post-body-paragraph ky kz it la b lb oc ju ld le od jx lg lh oe lj lk ll of ln lo lp og lr ls lt im bi translated">尽管<em class="lv">装袋</em>可以应用于所有类型的算法，但是<strong class="la iu">装袋超过决策树</strong>已经变得很普遍。因为它们是不稳定的和可变的，所以获得了好的结果。实际上，<strong class="la iu">随机森林</strong>就是用随机子空间方法对决策树进行<strong class="la iu">打包。</strong></p><p id="fb25" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当我们有一个训练集<code class="fe pe pf pg ph b">X_train N x M (N data points and M features)</code>时，我们在<code class="fe pe pf pg ph b">X</code>上训练<code class="fe pe pf pg ph b">n</code>树，其中<code class="fe pe pf pg ph b">X (N x m)</code>是带有替换的<code class="fe pe pf pg ph b">X_train</code>的随机子样本，但是我们也取<code class="fe pe pf pg ph b">m (m &lt; M)</code>特征的随机子集。这被称为<em class="lv">随机子空间方法</em>。当这个模型进行预测时，实际上，它从<code class="fe pe pf pg ph b">n</code>个不同的模型中获得<code class="fe pe pf pg ph b">n</code>个预测，并汇总它们。<em class="lv">分类</em>通过模型的简单多数投票计算得出，而<em class="lv">回归</em>通过模型预测的平均值计算得出。</p><p id="ca1e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这种方法允许<strong class="la iu">保持相同的偏差，但是由于<em class="lv">中心极限定理</em>而减少了方差</strong>。</p><p id="d7f9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如已知的<strong class="la iu">隔离森林</strong>算法也可用于<em class="lv">异常检测</em>问题。</p><p id="b1cd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">继承套袋的利弊</strong>。</p><p id="fdc1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">主要超参数:</p><ul class=""><li id="ae0d" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated">n _ estimators——集合中树的数量——越多越好</li><li id="7c2d" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">max_features —从训练集中提取的特征数，用于训练每个基树—建议使用<code class="fe pe pf pg ph b">n/3</code>进行回归，使用<code class="fe pe pf pg ph b">sqrt(n)</code>进行分类</li><li id="7612" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">max _ depth树的最大深度</li><li id="949d" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">min_sample_leaf —分割内部节点所需的最小样本数</li></ul></div><div class="ab cl nc nd hx ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="im in io ip iq"><h2 id="8b61" class="nj nk it bd nl nm nn dn no np nq dp nr lh ns nt nu ll nv nw nx lp ny nz oa ob bi translated">额外的树</h2><p id="0b11" class="pw-post-body-paragraph ky kz it la b lb oc ju ld le od jx lg lh oe lj lk ll of ln lo lp og lr ls lt im bi translated">额外的树与广泛使用的随机森林算法有关。</p><ul class=""><li id="e1d4" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated"><strong class="la iu">与bagging和random forest </strong>从训练数据集的引导样本中训练每个决策树不同，<em class="lv">额外树</em>算法在<strong class="la iu">整个训练数据集</strong>上训练每个决策树。</li><li id="74c4" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated"><strong class="la iu">像随机森林</strong>一样，<em class="lv">额外树</em>算法将<strong class="la iu">在决策树的每个分裂点随机采样特征</strong>。</li><li id="813a" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated"><strong class="la iu">与随机森林</strong>使用贪婪算法选择最佳分割点不同，<em class="lv">额外树</em>算法<strong class="la iu">随机选择分割点</strong>。</li></ul><p id="3a62" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">它通常可以实现与随机森林算法一样好或更好的性能，尽管它使用更简单的算法来构建用作集成成员的决策树，因此<strong class="la iu">工作更快</strong>。</p><p id="424c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">继承所有超参数，随机森林的利弊</strong>。</p></div><div class="ab cl nc nd hx ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="im in io ip iq"><h2 id="4c6b" class="nj nk it bd nl nm nn dn no np nq dp nr lh ns nt nu ll nv nw nx lp ny nz oa ob bi translated">助推</h2><p id="e9e2" class="pw-post-body-paragraph ky kz it la b lb oc ju ld le od jx lg lh oe lj lk ll of ln lo lp og lr ls lt im bi translated">A boosting是顺序训练<strong class="la iu">的弱算法</strong>(预测精度略好于随机算法)的集合，每个后续算法考虑前一个算法的误差。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pj"><img src="../Images/3741b15939a3efe8be03087b7c396d33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s-afiTU9mwPZSN4is50KCQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">助推。作者图片</p></figure><p id="65f1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">升压的一般思想可以以不同的方式实现。三种最受欢迎的升压类型是:</p><ul class=""><li id="f54d" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated"><strong class="la iu"> AdaBoost </strong></li></ul><p id="d4bc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">AdaBoost代表<em class="lv">自适应增压</em>。这是一个贪婪的迭代算法。在每一步，它识别错误分类的数据点，并调整权重以最小化训练误差。<br/>这个版本的boosting对异常值很敏感。</p><ul class=""><li id="adf0" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated"><strong class="la iu">梯度推进</strong></li></ul><p id="3b40" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">梯度推进也叫<em class="lv">梯度推进机— GBM </em>。与任何升压实现一样，在每一步，该算法都试图最小化前面步骤中产生的误差。但是GBM不是改变权重(像AdaBoost)，而是根据前一个模型的残差训练下一个模型。GDM的实现之一是<a class="ae lu" href="https://lightgbm.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> LightGBM </a>。</p><ul class=""><li id="a754" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated"><strong class="la iu"> XGBoost </strong></li></ul><p id="e6c8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae lu" href="https://xgboost.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>代表<em class="lv">极限梯度提升</em>。这种实现旨在提高速度和性能，它与GPU和Hadoop并行工作。XGBFIR 是一个非常棒的XGBoost特性重要性分析库。</p><p id="d0c3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于boosting的主要实现仍然使用决策树作为基本模型，因此boosting与随机森林一样，决定了特征的重要性。但是boosting的流行创造了许多允许您进行更详细分析的库(例如，XGBFIR库允许您不仅分析一个特性的重要性，还可以分析它们的双重甚至三重组合)。</p><p id="0598" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">主要超参数</strong>:</p><ul class=""><li id="62d3" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated">模型的类型及其相互作用的方式</li></ul><p id="4977" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">优点</strong>:</p><ul class=""><li id="d9d2" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated">质量非常好，通常比随机森林好</li><li id="8c17" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">功能重要性的内置评估</li></ul><p id="cf16" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">缺点</strong>:</p><ul class=""><li id="826e" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated">学习比随机森林慢，因为学习过程必须严格按顺序进行(尽管像XGBoost或LightGBM这样的实现对此有争议)</li><li id="e13f" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">倾向于过度拟合</li><li id="48c9" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">仅适用于足够大的数据集</li></ul></div><div class="ab cl nc nd hx ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="im in io ip iq"><h2 id="1515" class="nj nk it bd nl nm nn dn no np nq dp nr lh ns nt nu ll nv nw nx lp ny nz oa ob bi translated">堆垛</h2><p id="bffc" class="pw-post-body-paragraph ky kz it la b lb oc ju ld le od jx lg lh oe lj lk ll of ln lo lp og lr ls lt im bi translated">堆叠模型的架构涉及<strong class="la iu">两个或更多基础模型</strong>，通常称为<em class="lv">0级模型</em>，以及一个组合了基础模型预测的<strong class="la iu">元模型，称为<em class="lv">1级模型</em>。</strong></p><ul class=""><li id="9971" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated"><em class="lv">0级模型(基础模型)</em>:训练数据被分成<code class="fe pe pf pg ph b">K</code>个折叠。然后<code class="fe pe pf pg ph b">K</code>个模特在<code class="fe pe pf pg ph b">K-1</code>上训练折叠。</li><li id="77a6" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated"><em class="lv">一级模型(元模型)</em>:学习如何以最佳方式组合基础模型预测的模型。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pk"><img src="../Images/efb74434eb2bfcb407926925cea5bbb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jxnv6tb_-mH84cXK8xy0UA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">堆叠。作者图片</p></figure><p id="cc44" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">与增压的区别:</p><ul class=""><li id="a7a9" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated"><strong class="la iu">与装袋</strong>不同，在堆叠中，<strong class="la iu">模型通常与</strong>不同(例如，并非所有决策树)。</li><li id="f167" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated"><strong class="la iu">与增强</strong>不同，在堆叠中，<strong class="la iu">使用单个模型来学习如何最好地组合来自贡献模型</strong>的预测(例如，代替校正先前模型的预测的模型序列)。</li></ul><p id="4045" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用一个简单的线性模型作为元模型通常会给叠加起一个通俗的名字<strong class="la iu">混合</strong>。</p><p id="894f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">主要超参数</strong>:</p><ul class=""><li id="e094" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated">模型的类型及其相互作用的方式</li></ul><p id="388a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">优点</strong>:</p><ul class=""><li id="3472" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated">在没有其他帮助的情况下提高模型的质量</li><li id="02b9" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">允许您有效地混合不同类别的模型，结合它们的优势</li><li id="aadf" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">帮助你在Kaggle上赢得金牌</li></ul><p id="6cae" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">缺点</strong>:</p><ul class=""><li id="e5dd" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated">高计算复杂度</li><li id="474e" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">解释的复杂性</li><li id="4007" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">很容易出现信息泄露</li><li id="bcf9" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">仅适用于足够大的数据集</li></ul></div><div class="ab cl nc nd hx ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="im in io ip iq"><h1 id="1c18" class="oi nk it bd nl oj oz ol no om pa oo nr jz pb ka nu kc pc kd nx kf pd kg oa os bi translated">结论</h1><p id="88dc" class="pw-post-body-paragraph ky kz it la b lb oc ju ld le od jx lg lh oe lj lk ll of ln lo lp og lr ls lt im bi translated">这里描述了最流行的监督学习算法(当然，还有其他算法)。总之，我想描述选择一种算法来解决一个典型的监督学习任务(分类或回归)的过程。很简单——你只需要回答两个问题。</p><p id="d4a5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你的数据稀疏吗？如果是，那么你将不得不使用线性方法。这通常是一个<em class="lv"> SVM </em>，使用不同的内核，它将允许你恢复复杂的依赖关系。请记住，线性方法需要数据预处理，这在某些情况下可能会有问题。</p><p id="e499" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你的数据密集，那么你就比较幸运。现在一切都取决于他们的数量。如果有很多，那就用一个<em class="lv">助推</em>，否则——<em class="lv">随机森林</em>。这两种算法都很强大，抗噪声，会给你看很好的质量，但是需要很长时间的学习和预测。此外，请记住，升压容易过度拟合。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pl"><img src="../Images/2d29ee7b19c49cef39980f9ab7a85ad5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eZ2CxoP1sRWixJhCGwe9nQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">算法选择框图。作者图片</p></figure><p id="21b0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lv">大量数据</em>是什么意思？多少钱？通常，谈论一个阈值<em class="lv"> 10万个样本</em>，但是无论如何，你可以(并且很可能会)尝试不同的算法。</p><p id="5861" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这只是一个建议，您应该使用不同的超参数尝试不同的算法，以最佳方式解决您的任务。</p></div><div class="ab cl nc nd hx ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="im in io ip iq"><p id="95f1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您可能还对以下内容感兴趣:</p><div class="mk ml gp gr mm mn"><a rel="noopener follow" target="_blank" href="/unsupervised-learning-algorithms-cheat-sheet-d391a39de44a"><div class="mo ab fo"><div class="mp ab mq cl cj mr"><h2 class="bd iu gy z fp ms fr fs mt fu fw is bi translated">无监督学习算法备忘单</h2><div class="mu l"><h3 class="bd b gy z fp ms fr fs mt fu fw dk translated">你应该知道的所有无监督机器学习算法的完整备忘单</h3></div><div class="mv l"><p class="bd b dl z fp ms fr fs mt fu fw dk translated">towardsdatascience.com</p></div></div><div class="mw l"><div class="pm l my mz na mw nb ks mn"/></div></div></a></div></div><div class="ab cl nc nd hx ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="im in io ip iq"><h1 id="ee27" class="oi nk it bd nl oj oz ol no om pa oo nr jz pb ka nu kc pc kd nx kf pd kg oa os bi translated">感谢您的阅读！</h1><ul class=""><li id="728a" class="lw lx it la b lb oc le od lh pn ll po lp pp lt mb mc md me bi translated">我希望这些材料对你有用。<a class="ae lu" href="https://medium.com/@andimid" rel="noopener">在Medium上关注我</a>获取更多类似的文章。</li><li id="289b" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">如果您有任何问题或意见，我将很高兴得到任何反馈。在评论里问我，或者通过<a class="ae lu" href="https://www.linkedin.com/in/andimid/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或者<a class="ae lu" href="https://twitter.com/dimid_ml" rel="noopener ugc nofollow" target="_blank"> Twitter </a>联系。</li><li id="6725" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">为了支持我作为一名作家，并获得数以千计的其他媒体文章，使用<a class="ae lu" href="https://medium.com/@andimid/membership" rel="noopener">我的推荐链接</a>获得媒体会员资格(不收取额外费用)。</li></ul><h1 id="c912" class="oi nk it bd nl oj ok ol no om on oo nr jz op ka nu kc oq kd nx kf or kg oa os bi translated">参考</h1><p id="5963" class="pw-post-body-paragraph ky kz it la b lb oc ju ld le od jx lg lh oe lj lk ll of ln lo lp og lr ls lt im bi translated">[1] Aurélien Géron，<a class="ae lu" href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/" rel="noopener ugc nofollow" target="_blank">使用Scikit-Learn、Keras和TensorFlow进行机器学习，第二版</a> (2019)，奥赖利媒体公司</p></div></div>    
</body>
</html>