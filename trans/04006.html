<html>
<head>
<title>Practical Introduction to PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PySpark实用介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/practical-introduction-to-pyspark-fd04b48b9672?source=collection_archive---------15-----------------------#2021-04-04">https://towardsdatascience.com/practical-introduction-to-pyspark-fd04b48b9672?source=collection_archive---------15-----------------------#2021-04-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="06b1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">简单如Python，强大如Spark</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3f696494afd19791646888b0368e5e7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S_62IajCWBVoSAD9DbK43Q.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Marc-Olivier Jodoin 在<a class="ae ky" href="https://unsplash.com/s/photos/fast?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="463b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Spark是一个用于大规模数据处理的分析引擎。它让您可以将数据和计算分散到集群上，从而实现显著的性能提升。</p><p id="b6ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于收集和存储数据变得越来越容易，成本也越来越低，当我们处理现实生活中的问题时，可能会有大量的数据。因此，像Spark这样的分布式引擎正在成为数据科学生态系统中的主要工具。</p><p id="f2be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PySpark是Spark的Python API。它结合了Python的简单性和Spark的高效性，这种合作得到了数据科学家和工程师的高度赞赏。</p><p id="5db0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将通过几个例子来介绍PySpark的SQL模块，它用于处理结构化数据。</p><p id="439d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们首先需要创建一个SparkSession，作为Spark SQL的入口点。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="118a" class="ma mb it lw b gy mc md l me mf">from pyspark.sql import SparkSession</span><span id="02c4" class="ma mb it lw b gy mg md l me mf">sc = SparkSession.builder.getOrCreate()<br/>sc.sparkContext.setLogLevel("WARN")</span><span id="e370" class="ma mb it lw b gy mg md l me mf">print(sc)<br/>&lt;pyspark.sql.session.SparkSession object at 0x7fecd819e630&gt;</span></pre><p id="60a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用这个SparkSession对象与Spark SQL的函数和方法进行交互。让我们通过读取csv文件来创建一个spark数据帧。我们将使用Kaggle上的墨尔本房产<a class="ae ky" href="https://www.kaggle.com/anthonypino/melbourne-housing-market" rel="noopener ugc nofollow" target="_blank">数据集</a>。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="8742" class="ma mb it lw b gy mc md l me mf">df = sc.read.option("header", "true").csv(<br/>    "/home/sparkuser/Desktop/melb_housing.csv"<br/>)</span></pre><p id="0dbc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将通过仅选择5列来缩小数据框。可以使用“df.columns”方法查看列列表。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="4378" class="ma mb it lw b gy mc md l me mf">df = df.select("Type", "Landsize", "Distance", "Regionname", "Price")</span><span id="ed6e" class="ma mb it lw b gy mg md l me mf">df.show(5)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/0c49f13a20642d54315ffe11dac0f31d.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*bl-BMs6F_6x841-07FabJw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="dbd8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们将在示例中看到的，PySpark语法看起来像是Pandas和SQL的混合。因此，如果您已经在使用这些工具，那么学习PySpark将会相对容易。</p><p id="71b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以使用orderBy函数对数据框中的行进行排序。让我们来看看数据集中最贵的5栋房子。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="7a76" class="ma mb it lw b gy mc md l me mf">df.orderBy("Price", ascending=False).show(5)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/83f7ab0ccb3cc6d6b315e99775d3dae6.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*aNJxgO1cdj0O9lyQhN_7Lw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="de2b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">PySpark的SQL模块有许多函数可用于数据分析和操作。我们可以一次或分别导入它们。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="88b5" class="ma mb it lw b gy mc md l me mf"># import all functions<br/>from pyspark.sql import functions as F</span></pre><p id="5bb5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据集中有3种不同类型的房屋。我们可以使用groupby函数计算每种类型的平均价格。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="a56d" class="ma mb it lw b gy mc md l me mf">df.groupby("Type").agg(F.round(F.mean("Price")).alias("Avg_price")).show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/d350d9e92a0c88a605377a1c0d42e17d.png" data-original-src="https://miro.medium.com/v2/resize:fit:242/format:webp/1*PbFfPH5pRpgZ6tBzh5jTKw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="762a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们来详细说明一下语法。我们按照类型列对观察值(即行)进行分组。然后我们计算每组的平均价格。round函数用于向上舍入小数点。alias方法更改聚合列的名称。它类似于SQL中的“as”关键字。</p><p id="de55" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">距离栏显示到中央商务区的距离。当我们离开市中心时，房价通常会下降。我们可以通过计算距离超过3英里的房屋的平均房价来证实我们的预测。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="b233" class="ma mb it lw b gy mc md l me mf">df.filter(F.col("Distance") &gt; 3).groupby("Type").agg(<br/>    F.round(F.mean("Price")).alias("Avg_price")<br/>).show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/e9f2ca4c0e1709d5ac23336e4454232b.png" data-original-src="https://miro.medium.com/v2/resize:fit:248/format:webp/1*fXwEkxusHWszQDC0VgaHQQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="89e4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们使用filter函数对距离列应用一个条件。结果证实了我们的预期。每种类型的平均价格都下降了。</p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><p id="750c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另外两个有用的函数是count和countDistinct，它们分别计算一个组中的总观察值和不同观察值(即行)的数量。例如，以下代码返回每种类型的房屋数量以及不同价格的数量。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="94f9" class="ma mb it lw b gy mc md l me mf">df.groupby("Type").agg(<br/>    F.count("Price").alias("n_houses"),<br/>    F.countDistinct("Price").alias("n_distinct_price")<br/>).show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/826bcca862b322e764163c7c7594d919.png" data-original-src="https://miro.medium.com/v2/resize:fit:456/format:webp/1*2x5sdVNWf-Dmj2dUsh46_g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><p id="6bd4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据分析中的一个典型任务是基于现有列派生新列。这可能是你的特征工程过程的一部分。关于住房数据集，我们可以创建一个新要素来表示每单位土地面积的价格。</p><p id="40d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">若要完成此任务，可以按如下方式使用withColumn函数:</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="f172" class="ma mb it lw b gy mc md l me mf">df = df.withColumn(<br/>    "Price_per_size", <br/>    F.round(F.col("Landsize")*1000 / F.col("Price"), 2)<br/>)</span></pre><p id="d382" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">新列的名称是“价格/大小”。下一行指定了导出值的步骤。我们把土地面积乘以1000，再除以房价。结果四舍五入到小数点后两位。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/58f0734ef3113bd89c220a44fa16abc1.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*lc0MyN-LrqLBfD9xT_-2WA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><h2 id="81e0" class="ma mb it bd mu mv mw dn mx my mz dp na li nb nc nd lm ne nf ng lq nh ni nj nk bi translated">结论</h2><p id="dec6" class="pw-post-body-paragraph kz la it lb b lc nl ju le lf nm jx lh li nn lk ll lm no lo lp lq np ls lt lu im bi translated">我们已经介绍了PySpark的数据分析和操作。PySpark的SQL模块提供了更多的函数和方法来执行高效的数据分析。</p><p id="94bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">需要注意的是，Spark针对大规模数据进行了优化。因此，在处理小规模数据时，您可能看不到任何性能提升。事实上，在处理小数据集时，Pandas可能会比PySpark表现得更好。</p><p id="83e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢您的阅读。如果您有任何反馈，请告诉我。</p></div></div>    
</body>
</html>