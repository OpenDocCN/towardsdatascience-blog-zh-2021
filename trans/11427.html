<html>
<head>
<title>Text Classification with BERT in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch中基于BERT的文本分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f?source=collection_archive---------0-----------------------#2021-11-10">https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f?source=collection_archive---------0-----------------------#2021-11-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e342" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何利用来自拥抱脸的预先训练的BERT模型来分类新闻文章的文本</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1987d10e070cc36b00252186df8d968b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Gzqs1XjVgH-URnan5kwQDg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/s/photos/news?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae ky" href="https://unsplash.com/@freegraphictoday?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">absolute vision</a>拍摄</p></figure><p id="3c30" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">早在2018年，谷歌就为NLP应用开发了一个强大的基于Transformer的机器学习模型，该模型在不同的基准数据集上都优于以前的语言模型。这个模型叫做伯特。</p><p id="4e39" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我们将使用来自拥抱脸的预训练BERT模型进行文本分类任务。您可能已经知道，文本分类任务中模型的主要目标是将文本分类到一个预定义的标签或标记中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/09f9f355682387ffcfae98e5a2299ec8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qzpJIgODil4oyh5e9BNOiQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自作者</p></figure><p id="5573" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">具体来说，很快我们将使用预先训练好的伯特模型来分类一篇新闻文章的文本是否可以归类为<em class="lw">体育</em>、<em class="lw">政治</em>、<em class="lw">商业</em>、<em class="lw">娱乐</em>或<em class="lw">科技</em>类别。</p><p id="925b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是在我们深入研究实现之前，让我们简单地讨论一下BERT背后的概念。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="6993" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">伯特是什么？</h1><p id="5b98" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">BERT是<strong class="lb iu">B</strong>I directional<strong class="lb iu">E</strong>n coder<strong class="lb iu">R</strong>的缩写，代表来自<strong class="lb iu"> T </strong>变压器。这个名字本身就给了我们几个线索，让我们知道伯特是怎么回事。</p><p id="7067" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">BERT架构由几个堆叠在一起的变压器编码器组成。每个Transformer编码器封装了两个子层:自我关注层和前馈层。</p><p id="4b42" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有两种不同的BERT模型:</p><ol class=""><li id="5a01" class="nb nc it lb b lc ld lf lg li nd lm ne lq nf lu ng nh ni nj bi translated">BERT <strong class="lb iu"> base </strong>是一个BERT模型，由12层变压器编码器、12个注意头、768个隐藏尺寸和110M个参数组成。</li><li id="bc66" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">BERT <strong class="lb iu">大</strong>是一个BERT模型，由24层变压器编码器，16个注意头，1024个隐藏尺寸，340个参数组成。</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/31cc208e6d7eb9ae7d8be4d5648b4d16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4bet66zMfpkDaaOQdKF-mg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自作者</p></figure><p id="ab26" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">BERT是一个强大的语言模型至少有两个原因:</p><ol class=""><li id="3378" class="nb nc it lb b lc ld lf lg li nd lm ne lq nf lu ng nh ni nj bi translated">它根据从拥有8亿单词的BooksCorpus和拥有2500万单词的Wikipedia中提取的未标记数据进行预训练。</li><li id="7ef8" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">顾名思义，它是利用编码器堆栈的双向特性进行预训练的。这意味着伯特不仅从左到右，而且从右到左从一系列单词中学习信息。</li></ol></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="f567" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">伯特输入和输出</h1><p id="566e" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">BERT模型期望将一系列标记(单词)作为输入。在每个记号序列中，有两个特殊的记号是BERT期望的输入:</p><ol class=""><li id="bd11" class="nb nc it lb b lc ld lf lg li nd lm ne lq nf lu ng nh ni nj bi translated"><strong class="lb iu">【CLS】</strong>:这是每个序列的第一个记号，代表分类记号。</li><li id="bb25" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated"><strong class="lb iu">【SEP】</strong>:这是让BERT知道哪个令牌属于哪个序列的令牌。这个特殊标记主要对于下一个句子预测任务或问答任务是重要的。如果我们只有一个序列，那么这个标记将被附加到序列的末尾。</li></ol><p id="46b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了使它更清楚，让我们说我们有一个由下面的短句组成的文本:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/dbc9944b65cbe007c34deb3def912d8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*p8lCLYbzfbcDnnnqUqRjKw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自作者</p></figure><p id="be55" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一步，我们需要把这个句子转换成一系列的标记(单词)，这个过程叫做<em class="lw">标记化</em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/ed900c9a5173d801d55462e14821f5f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/1*SQiPqx4z0Vuav71T6vwuNw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自作者</p></figure><p id="f9a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">尽管我们已经对输入句子进行了标记，但我们还需要做一步。我们需要通过添加<strong class="lb iu">【CLS】</strong>和<strong class="lb iu">【SEP】</strong>记号来重新格式化记号序列，然后将它用作我们的BERT模型的输入。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/e7b4854e4a732c5c7274542ec26b0052.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qkszUyGzFe-bHWdbbVfqiw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自作者</p></figure><p id="836e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">幸运的是，我们只需要一行代码就可以将输入的句子转换成BERT期望的符号序列，正如我们在上面看到的。我们将使用<code class="fe nt nu nv nw b">BertTokenizer</code>来完成这项工作，稍后您可以看到我们是如何完成的。</p><p id="0bba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还需要注意的是，可以输入BERT模型的最大令牌大小是512。如果一个序列中的令牌少于512个，我们可以用<strong class="lb iu">【PAD】</strong>令牌填充未使用的令牌槽。如果序列中的令牌长度超过512，那么我们需要进行截断。</p><p id="67e1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这就是伯特所期望的输入。</p><p id="0eb5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，BERT模型将在每个记号中输出大小为768的嵌入向量。我们可以使用这些向量作为不同类型的自然语言处理应用的输入，无论是文本分类、下一句预测、命名实体识别(NER)还是问答。</p><p id="86d1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于文本分类任务，我们将注意力集中在从特殊的<strong class="lb iu">【CLS】</strong>记号输出的嵌入向量上。这意味着我们将使用来自<strong class="lb iu">【CLS】</strong>令牌的大小为768的嵌入向量作为我们分类器的输入，然后它将输出一个大小为我们分类任务中类别数量的向量。</p><p id="8c43" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是BERT模型的输入和输出的图示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/57254948dbeff3220bec1b05830cd00f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CUE12SdxQ31xhvsTAPsg8g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自作者</p></figure></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="a7bb" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">基于BERT的文本分类</h1><p id="f6fa" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">现在我们要跳转到我们的主题，用BERT对文本进行分类。在本帖中，我们将使用<em class="lw"> BBC新闻分类</em>数据集。如果你想跟进，你可以<a class="ae ky" href="https://www.kaggle.com/sainijagjit/bbc-dataset" rel="noopener ugc nofollow" target="_blank">在Kaggle </a>下载数据集。</p><p id="4f9d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个数据集已经是CSV格式的，它有2126个不同的文本，每个文本都被标记为5个类别之一:<em class="lw">娱乐</em>、<em class="lw">体育</em>、<em class="lw">科技</em>、<em class="lw">商业</em>或<em class="lw">政治</em>。</p><p id="bae3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看数据集是什么样子的。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="d3dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，dataframe只有两列，即作为标签的<em class="lw">类别</em>和作为BERT输入数据的<em class="lw">文本</em>。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h2 id="2303" class="oa mf it bd mg ob oc dn mk od oe dp mo li of og mq lm oh oi ms lq oj ok mu ol bi translated">预处理数据</h2><p id="e03b" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">从上一节中您可能已经知道，我们需要通过添加<strong class="lb iu">【CLS】</strong>和<strong class="lb iu">【SEP】</strong>标记来将我们的文本转换成BERT期望的格式。我们可以用拥抱脸的<code class="fe nt nu nv nw b">BertTokenizer</code>类轻松做到这一点。</p><p id="dc19" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们需要通过pip安装Transformers库:</p><pre class="kj kk kl km gt om nw on oo aw op bi"><span id="9b48" class="oa mf it nw b gy oq or l os ot">pip install transformers</span></pre><p id="5a70" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了让我们更容易理解从<code class="fe nt nu nv nw b">BertTokenizer</code>得到的输出，让我们用一个简短的文本作为例子。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="6221" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是上面对<code class="fe nt nu nv nw b">BertTokenizer </code>参数的解释:</p><ul class=""><li id="ee2c" class="nb nc it lb b lc ld lf lg li nd lm ne lq nf lu ou nh ni nj bi translated"><code class="fe nt nu nv nw b">padding</code>:将每个序列填充到您指定的最大长度。</li><li id="b723" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ou nh ni nj bi translated"><code class="fe nt nu nv nw b">max_length</code>:每个序列的最大长度。在本例中，我们使用10，但对于我们的实际数据集，我们将使用512，这是BERT允许的最大序列长度。</li><li id="843a" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ou nh ni nj bi translated"><code class="fe nt nu nv nw b">truncation</code>:如果为真，则每个序列中超过最大长度的记号将被截断。</li><li id="5669" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ou nh ni nj bi translated"><code class="fe nt nu nv nw b">return_tensors</code>:将要返回的张量的类型。既然我们使用Pytorch，那么我们使用<code class="fe nt nu nv nw b">pt</code>。如果用Tensorflow，那么就需要用<code class="fe nt nu nv nw b">tf</code>。</li></ul><p id="181a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您从上面的<code class="fe nt nu nv nw b">bert_input</code>变量中看到的输出是我们稍后的BERT模型所必需的。但是这些输出意味着什么呢？</p><ol class=""><li id="3936" class="nb nc it lb b lc ld lf lg li nd lm ne lq nf lu ng nh ni nj bi translated">第一行是<code class="fe nt nu nv nw b">input_ids</code>，是每个令牌的id表示。我们实际上可以将这些输入id解码成实际的令牌，如下所示:</li></ol><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="41c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，<code class="fe nt nu nv nw b">BertTokenizer</code>负责对输入文本进行所有必要的转换，这样它就可以用作我们的BERT模型的输入了。自动添加<strong class="lb iu">【CLS】</strong><strong class="lb iu">【SEP】</strong><strong class="lb iu">【PAD】</strong>令牌。既然我们指定最大长度为10，那么最后只有两个<strong class="lb iu">【PAD】</strong>记号。</p><p id="19c1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2.第二行是<code class="fe nt nu nv nw b">token_type_ids</code>，这是一个二进制掩码，标识一个令牌属于哪个序列。如果我们只有一个序列，那么所有的令牌类型id都将是0。对于文本分类任务，<code class="fe nt nu nv nw b">token_type_ids</code>是我们的BERT模型的可选输入。</p><p id="09a1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">3.第三行是<code class="fe nt nu nv nw b">attention_mask</code>，这是一个二进制掩码，用来标识一个令牌是真实的单词还是仅仅是填充。如果令牌包含<strong class="lb iu">【CLS】</strong><strong class="lb iu">【SEP】</strong>或任何真实单词，那么掩码将是1。同时，如果令牌只是填充符或<strong class="lb iu">【填充符】</strong>，那么掩码将为0。</p><p id="60bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如你可能注意到的，我们使用来自<code class="fe nt nu nv nw b">bert-base-cased </code>模型的预训练<code class="fe nt nu nv nw b">BertTokenizer</code>。如果数据集中的文本是英文，这个预先训练好的标记器可以很好地工作。</p><p id="8b92" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您有来自不同语言的数据集，您可能想要使用<code class="fe nt nu nv nw b">bert-base-multilingual-cased</code>。具体来说，如果数据集是德语、荷兰语、中文、日语或芬兰语，您可能希望使用专门针对这些语言预训练的标记器。您可以在这里查看对应的预训练分词器<a class="ae ky" href="https://huggingface.co/transformers/pretrained_models.html" rel="noopener ugc nofollow" target="_blank">的名称。</a></p><p id="62e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">综上所述，下面是<code class="fe nt nu nv nw b">BertTokenizer</code>对我们输入句的说明。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/d19969d6ce9a6b52e0e84917c8c64aca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EZU3f1nSXhJ9Io0o2lOqow.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自作者</p></figure></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h2 id="cd94" class="oa mf it bd mg ob oc dn mk od oe dp mo li of og mq lm oh oi ms lq oj ok mu ol bi translated">数据集类</h2><p id="6eb2" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">现在我们知道了从<code class="fe nt nu nv nw b">BertTokenizer</code>将得到什么样的输出，让我们为新闻数据集构建一个<code class="fe nt nu nv nw b">Dataset</code>类，作为生成新闻数据的类。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="d901" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的实现中，我们定义了一个名为<code class="fe nt nu nv nw b">labels</code>的变量，这是一个将dataframe中的<em class="lw">类别</em>映射到我们标签的id表示中的字典。注意，我们还在上面的<code class="fe nt nu nv nw b">__init__</code>函数中调用了<code class="fe nt nu nv nw b">BertTokenizer</code>来将我们的输入文本转换成BERT期望的格式。</p><p id="8715" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">定义数据集类后，让我们将数据帧分成训练集、验证集和测试集，比例为80:10:10。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h2 id="8271" class="oa mf it bd mg ob oc dn mk od oe dp mo li of og mq lm oh oi ms lq oj ok mu ol bi translated">模型结构</h2><p id="d501" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">到目前为止，我们已经构建了一个数据集类来生成我们的数据。现在，让我们使用预训练的BERT基本模型构建实际模型，该模型具有12层Transformer编码器。</p><p id="e058" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您的数据集不是英文的，最好使用<code class="fe nt nu nv nw b">bert-base-multilingual-cased</code>模型。如果您的数据是德语、荷兰语、中文、日语或芬兰语，则可以使用专门为这些语言预先训练的模型。你可以在这里查看对应预训模特<a class="ae ky" href="https://huggingface.co/transformers/pretrained_models.html" rel="noopener ugc nofollow" target="_blank">的名字。</a></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="34ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上面的代码可以看出，BERT模型输出两个变量:</p><ul class=""><li id="5ffe" class="nb nc it lb b lc ld lf lg li nd lm ne lq nf lu ou nh ni nj bi translated">在上面的代码中，我们命名为<code class="fe nt nu nv nw b">_</code>的第一个变量包含序列中所有标记的嵌入向量。</li><li id="167d" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ou nh ni nj bi translated">我们命名为<code class="fe nt nu nv nw b">pooled_output</code>的第二个变量包含<strong class="lb iu">【CLS】</strong>令牌的嵌入向量。对于文本分类任务，使用这种嵌入作为我们的分类器的输入就足够了。</li></ul><p id="d4dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后我们将<code class="fe nt nu nv nw b">pooled_output </code>变量传递给一个带有ReLU激活函数的线性层。在线性层的末端，我们有一个大小为5的向量，每个向量对应于我们标签的类别(<em class="lw">体育、商业</em>、<em class="lw">政治、</em>、<em class="lw">娱乐</em>和<em class="lw">科技</em>)。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h2 id="0307" class="oa mf it bd mg ob oc dn mk od oe dp mo li of og mq lm oh oi ms lq oj ok mu ol bi translated">训练循环</h2><p id="6bee" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">现在是我们训练模型的时候了。训练循环将是标准PyTorch训练循环。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="6039" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们对模型进行5个时期的训练，并使用Adam作为优化器，而学习率设置为<em class="lw"> 1e-6 </em>。我们还需要使用分类交叉熵作为损失函数，因为我们正在处理多类分类。</p><p id="c66c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">建议您使用GPU来训练模型，因为BERT基本模型包含1.1亿个参数。</p><p id="26e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用上述配置5个时期后，您将获得以下输出作为示例:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/d92fb460e989a842459cd7ed7471a228.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y0_wLkQSrB-99G27StOcmw.png"/></div></div></figure><p id="fd7e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">显然，由于训练过程的随机性，您可能不会获得与上面截图类似的损失和准确性值。如果你在5个周期后没有得到一个好的结果，试着增加周期到，比如说，10个或者调整学习速率。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h2 id="7fa6" class="oa mf it bd mg ob oc dn mk od oe dp mo li of og mq lm oh oi ms lq oj ok mu ol bi translated">根据测试数据评估模型</h2><p id="bd9d" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">既然我们已经训练了模型，我们可以使用测试数据来评估模型在看不见的数据上的性能。下面是在测试集上评估模型性能的函数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="0b87" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">运行上面的代码后，我从测试数据中得到了0.994的准确率。由于训练过程中的随机性，你得到的准确度显然会与我的略有不同。</p></div><div class="ab cl lx ly hx lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="im in io ip iq"><h1 id="0a4b" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">结论</h1><p id="131e" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">现在你知道了我们如何利用来自拥抱脸的预先训练的BERT模型进行文本分类任务的步骤。我希望这篇文章能帮助你开始使用BERT。</p><p id="dab9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">需要记住的一点是，我们不仅可以使用来自BERT的嵌入向量来执行句子或文本分类任务，还可以执行更高级的NLP应用，如问题回答、下一句预测或命名实体识别(NER)任务。</p><p id="0f98" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在t <a class="ae ky" href="https://github.com/marcellusruben/All_things_medium/blob/main/Text_Classification_BERT/bert_medium.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">的笔记本</strong> </a>中找到这篇文章中演示的所有代码片段。</p></div></div>    
</body>
</html>