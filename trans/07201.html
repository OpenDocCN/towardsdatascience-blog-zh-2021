<html>
<head>
<title>Getting started with PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyTorch入门</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/getting-started-with-pytorch-2819d7aeb87c?source=collection_archive---------9-----------------------#2021-06-30">https://towardsdatascience.com/getting-started-with-pytorch-2819d7aeb87c?source=collection_archive---------9-----------------------#2021-06-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="53fb" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/getting-started" rel="noopener" target="_blank">入门</a></h2><div class=""/><div class=""><h2 id="936c" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">通过示例学习基础知识</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/36216a8572a0befee5257f9e74e84fd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*06QT9w9D_SYm82I-9J6lHA.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由来自Pexels的<a class="ae lh" href="https://www.pexels.com/@jeremy-bishop-1260133" rel="noopener ugc nofollow" target="_blank">杰瑞米·毕夏普</a>拍摄</p></figure><h1 id="ba71" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">介绍</h1><p id="12a8" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">在我之前的帖子“<a class="ae lh" rel="noopener" target="_blank" href="/getting-started-with-tensorflow-e33999defdbf">Tensorflow</a>入门”中，我提到如果你想构建小型或大型深度学习解决方案，tensor flow和PyTorch都是很好的选择。这两个平台都广泛应用于学术界和工业界，维护良好，是开源的，提供简单的API和高级功能。</p><p id="a79d" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">在这里，我们将探索PyTorch API。在“【Tensorflow入门”的同时，我们将讨论PyTorch是如何产生的，以及如何将其用于深度学习。</p></div><div class="ab cl nb nc hx nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="im in io ip iq"><h1 id="3a46" class="li lj it bd lk ll ni ln lo lp nj lr ls ki nk kj lu kl nl km lw ko nm kp ly lz bi translated">PyTorch是什么时候开发的？</h1><p id="16c9" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">大约在PyTorch 0.1.1版本于2016年9月发布时，有多个深度学习框架可用，为构建和训练复杂模型提供了低级和高级包装器。</p><p id="d6fa" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">Caffe、Chainer、Theano、Tensorflow、CNTK、MXNet和Torch只是研究人员用来构建日益复杂的网络的几个低级库。</p><p id="ded7" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">另一方面，Lasagne和Keras专注于创建可以使用许多低级库之一的高级包装器。这种方法得到了从业者的大力支持，因为它允许他们从Tensorflow、CNTK或Theano等语言的低级语法中进行抽象。</p><p id="d368" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">在接下来的几年里，随着frameworks⁴:的放弃和巩固，情况发生了巨大的变化</p><ul class=""><li id="f43e" class="nn no it mc b md mw mg mx mj np mn nq mr nr mv ns nt nu nv bi translated">Keras被并入Tensorflow，大大改变了方向；</li><li id="9c50" class="nn no it mc b md nw mg nx mj ny mn nz mr oa mv ns nt nu nv bi translated">Caffe 2被合并到PyTorch中，取代了Torch祖先的部分原始Lua实现；</li><li id="5113" class="nn no it mc b md nw mg nx mj ny mn nz mr oa mv ns nt nu nv bi translated">Theano、CNTK和Chainer的开发和支持已停止。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ob"><img src="../Images/3e51b1faa74f47e61ddab37a4fa7422e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YeaX76PgCpTOiRWKmzYbNA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">谷歌在“机器学习和人工智能”类别下搜索2017年1月至2021年7月之间的顶级低/高级深度学习框架(Tensorflow、Keras、PyTorch)。为了进行比较，还显示了合并的Tensorflow+Keras和PyTorch+Caffe2。图片由作者提供。</p></figure><p id="234f" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">到2021年6月，超过99%的谷歌搜索主要深度学习框架包含Tensorflow/Keras或PyTorch/Caffe，其中第一个仍然明显更受欢迎(见图表)。</p></div><div class="ab cl nb nc hx nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="im in io ip iq"><h1 id="2879" class="li lj it bd lk ll ni ln lo lp nj lr ls ki nk kj lu kl nl km lw ko nm kp ly lz bi translated">通过例子学习</h1><p id="4010" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">在下面的小节中，我将介绍在PyTorch中构建两个简单神经网络的关键概念(一个用于回归，一个用于分类)。</p><p id="e551" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">为了使本教程与我之前的帖子“<a class="ae lh" rel="noopener" target="_blank" href="/getting-started-with-tensorflow-e33999defdbf">tensor flow</a>入门”相媲美，我将重复使用相同的模拟数据集。为了使本教程自成一体，我将在这里再次重新介绍它们。</p><h2 id="b0e3" class="oc lj it bd lk od oe dn lo of og dp ls mj oh oi lu mn oj ok lw mr ol om ly iz bi translated">线性回归</h2><p id="ef62" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">在此示例中，我们将在PyTorch中构建一个简单的1层神经网络来解决线性回归问题。这将解释如何初始化你的权重(又名回归系数)，以及如何通过反向传播来更新它们。</p><p id="2d8e" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我们首先需要的是一个数据集。这里，我们将模拟一个噪声线性模型，如下所示</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi on"><img src="../Images/ec737bbfb915fbbbfc3295a5d4ab673c.png" data-original-src="https://miro.medium.com/v2/resize:fit:322/0*6lJaa7emJ96TkbEJ"/></div></figure><p id="09bd" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">其中Y是我们的目标，X是我们的输入，w是我们要确定的系数，N是高斯分布噪声变量。为此，在笔记本的第一个单元格中粘贴并运行以下代码片段。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="8bc3" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这将显示X和Y之间关系的散点图，清楚地表明在一些高斯噪声下的线性相关性。在这里，我们期望一个合理的模型来估计2作为理想的回归系数。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/efd6dff9bac11d305248dad4bb3bab4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*O-gJbzzuaf9Qvu-cji6Bag.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">代表高斯噪声下X和Y之间线性相关性的模拟数据。</p></figure><p id="db4e" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">在PyTorch中，我们通常使用张量来表示我们的输入、目标和回归系数(这里称为权重)。张量是由火炬表示的多维元素阵列。张量的对象。张量只有一种数据类型和一种形状。</p><p id="4bfb" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">现在让我们将输入(x)和目标(y)转换成火炬张量。为此，在Colab中复制并运行以下代码片段。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="7d99" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这将返回类(火炬。张量)、输入和目标张量的形状、大小和值。</p><pre class="ks kt ku kv gt or os ot ou aw ov bi"><span id="4c2d" class="oc lj it os b gy ow ox l oy oz">Describing the features...<br/>&lt;class 'torch.Tensor'&gt;<br/>torch.float64<br/>torch.Size([1000])<br/>tensor([0.0000, 0.0010, 0.0020, 0.0030, 0.0040, 0.0050, 0.0060, 0.0070, 0.0080,<br/>        0.0090], dtype=torch.float64)<br/></span><span id="8647" class="oc lj it os b gy pa ox l oy oz">Describing the target...<br/>&lt;class 'torch.Tensor'&gt;<br/>torch.float64<br/>torch.Size([1000])<br/>tensor([ 0.1587, -0.6984,  0.1692,  0.1368,  0.1386,  0.0854,  0.2807,  0.2895,<br/>         0.5358,  0.3550], dtype=torch.float64)</span></pre><p id="a421" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">现在让我们用一个常数(0.1)开始我们的权重。为此，我们调用“torch.tensor ”,缺省值为0.1，需要的数据类型(float)和存储张量的设备。在本例中，我们将在“cpu”中执行所有操作。为了提高大型神经网络模型的性能，我们可以将张量移至“gpu”。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="48f8" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">运行这个代码片段将输出权重张量的类、形状、类型和值。注意，这个张量有一个“[]”形状，表示它是一个0维向量。</p><pre class="ks kt ku kv gt or os ot ou aw ov bi"><span id="2c29" class="oc lj it os b gy ow ox l oy oz">Describing the weights… <br/>&lt;class ‘torch.Tensor’&gt; <br/>torch.float32 torch.Size([]) <br/>tensor(0.1000, requires_grad=True)</span></pre><p id="f810" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">给定初始化的权重张量和输入X，为了获得预测的Y (Yhat ),我们可以简单地调用Yhat = x * w _ tensor.detach()。numpy()'。的’。分离()。“numpy()”用于将权重向量转换为numpy数组。复制并运行下面的代码片段，看看初始化后的重量如何符合数据。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="a4e9" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">正如你所观察到的,‘w _ tensor’的当前值与理想值相差甚远。回归线完全符合数据。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/ff32e1e2c8c292da4dc639133dfc97a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*edownMHLZHbT--VLVcrrtg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">训练前模型拟合的表示。</p></figure><p id="72d3" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">为了找到‘w _ tensor’的最佳值，我们需要定义一个损失度量和一个优化器。这里，我们将使用均方误差(MSE)作为我们的损失度量，随机梯度下降(SGD)作为我们的优化器。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="3a35" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我们现在有了优化我们的“w _张量”的所有部分。优化循环需要自定义“前进”步骤的定义，从我们的损失度量调用“后退”步骤方法，从我们的优化器调用“步骤”方法。</p><p id="cb9e" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">自定义的“前进”步骤告诉模型如何将输入与权重张量相结合，以及如何计算我们的目标与预测目标之间的误差(下面代码片段中的第5行到第8行)。在一个更复杂的例子中，这将是一组定义从输入X到目标y的计算图的指令。</p><p id="1cd9" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">“向后”步骤告诉模型将错误反向传播到网络中的每一层(下面代码片段中的第22行)。</p><p id="7fac" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">最后，“optimizer.step()”告诉模型计算并应用这次迭代的权重变化(下面代码片段中的第23行)。注意，在大多数情况下，您需要在调用优化器步骤之前清除梯度(下面代码片段中的第21行)。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="59f8" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">在训练结束时，你的体重应该接近2(理想值)。要使用该模型进行推理(即，在给定X值的情况下预测Y变量)，只需执行“Yhat = x * w_tensor.detach()。numpy()'。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/992e5a2de24f9bc67036b328f0245107.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*7Cotpg6meRQLEX_WRyWgyA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">训练后模型拟合的表示。</p></figure><h2 id="2d49" class="oc lj it bd lk od oe dn lo of og dp ls mj oh oi lu mn oj ok lw mr ol om ly iz bi translated">分类问题</h2><p id="16f1" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">在本例中，我们将引入PyTorch NN顺序模型定义来创建一个更复杂的神经网络。如果你习惯Keras，这个模块看起来会很熟悉。我们将把这个模型应用于一个线性可分的分类问题。</p><p id="e84d" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">像以前一样，让我们从构建数据集开始。在下面的代码片段中，我们为第一个聚类创建了两个以(0.2，0.2)为中心的点聚类，为第二个聚类创建了两个以(0.8，0.8)为中心的点聚类。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="108b" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我们可以很快观察到，用一条与两个聚类距离相等的线将两个数据集线性分离的模型是理想的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/7b99b2a74482e695201fb5e6d2b81325.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*N2HY_cey8QEKm6i5MGPVIA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">模拟数据表示两个数据聚类，第一个聚类以(0.2，0.2)为中心，第二个聚类以(0.8，0.8)为中心。</p></figure><p id="08ab" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">让我们从定义自定义NN模型来解决这个问题开始。我们将定义一个5层神经网络如下:</p><ol class=""><li id="6e21" class="nn no it mc b md mw mg mx mj np mn nq mr nr mv pb nt nu nv bi translated">有10个节点的线性层。这将有一个2 x 10(输入形状x图层大小)的形状。</li><li id="8a2a" class="nn no it mc b md nw mg nx mj ny mn nz mr oa mv pb nt nu nv bi translated">批量标准化层。这一层将对每一批的第一层的输出进行归一化，避免爆炸/消失梯度。</li><li id="4432" class="nn no it mc b md nw mg nx mj ny mn nz mr oa mv pb nt nu nv bi translated">Relu激活层。这一层将为我们的网络提供非线性能力。请注意，我们仅以此为例。对于这个问题，relu层是不必要的，因为它是线性可分离的。</li><li id="2a1a" class="nn no it mc b md nw mg nx mj ny mn nz mr oa mv pb nt nu nv bi translated">具有两个节点的线性层。这将有一个10 x 2(层大小x输出形状)的形状。</li><li id="38b5" class="nn no it mc b md nw mg nx mj ny mn nz mr oa mv pb nt nu nv bi translated">Softmax层。该层会将第4层的输出转换为softmax。</li></ol><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="4047" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">和以前一样，让我们也将x和y numpy数组转换成张量，使它们对PyTorch可用，然后定义我们的损失度量和优化器。在这个例子中，我们应该使用分类损失度量，例如交叉熵。</p><p id="81be" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">对于优化器，我们可以像以前一样使用SGD。然而，普通SGD的收敛速度慢得令人难以置信。相反，我们将使用最近的自适应梯度下降方法(RMSProp)。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="2141" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">和以前一样，我们先来检查一下我们的网络在训练前表现如何。要使用该模型进行推理，我们可以简单地键入“yhat = model(x)”。现在，复制下面的片段来可视化网络输出。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="37f3" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">你可以确认网络一点也不好。显然需要一些训练来正确区分这两个类别。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/690bdbf6cae97330a0f35ad6cb9eb67e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*6o9V-azJLKkvjYvDVhlO6g.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">训练前模型拟合的表示。</p></figure><p id="6dd5" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">为了训练顺序PyTorch模型，我们遵循与第一个示例相同的步骤，但是用模型调用替换了自定义的“前进”步骤。将下面的片段添加到您的笔记本中，以训练模型。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="a127" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">在训练循环结束时，你的网络应该能够很好地分离这两个类。要使用该模型进行推理(即，在给定X值的情况下预测Y变量)，您可以简单地执行“yhat = model(x)”。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/b312be952307479005fa8ca4e7544828.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*fLpcRv8SmyOMhnicv7eekA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">训练后模型拟合的表示。</p></figure><h2 id="8145" class="oc lj it bd lk od oe dn lo of og dp ls mj oh oi lu mn oj ok lw mr ol om ly iz bi translated">完整脚本</h2><p id="43e1" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">如需完整的脚本，请点击以下链接进入我的github页面:</p><div class="pd pe gp gr pf pg"><a href="https://github.com/andreRibeiro1989/medium/blob/main/pytorch_getting_started.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="ph ab fo"><div class="pi ab pj cl cj pk"><h2 class="bd jd gy z fp pl fr fs pm fu fw jc bi translated">安德里贝罗1989/中号</h2><div class="pn l"><h3 class="bd b gy z fp pl fr fs pm fu fw dk translated">pytorch_getting_started.ipynb</h3></div><div class="po l"><p class="bd b dl z fp pl fr fs pm fu fw dk translated">github.com</p></div></div><div class="pp l"><div class="pq l pr ps pt pp pu lb pg"/></div></div></a></div><p id="705e" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">或者通过以下链接直接访问Google Colab笔记本:</p><div class="pd pe gp gr pf pg"><a href="https://colab.research.google.com/github/andreRibeiro1989/medium/blob/main/pytorch_getting_started.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="ph ab fo"><div class="pi ab pj cl cj pk"><h2 class="bd jd gy z fp pl fr fs pm fu fw jc bi translated">谷歌联合实验室</h2><div class="pn l"><h3 class="bd b gy z fp pl fr fs pm fu fw dk translated">pytorch_getting_started.ipynb</h3></div><div class="po l"><p class="bd b dl z fp pl fr fs pm fu fw dk translated">colab.research.google.com</p></div></div><div class="pp l"><div class="pv l pr ps pt pp pu lb pg"/></div></div></a></div></div><div class="ab cl nb nc hx nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="im in io ip iq"><h1 id="ac1f" class="li lj it bd lk ll ni ln lo lp nj lr ls ki nk kj lu kl nl km lw ko nm kp ly lz bi translated">结论</h1><p id="cc8f" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">PyTorch是目前开发定制深度学习解决方案的最佳深度学习框架之一(另一个是Tensorflow)。在这篇博客中，我介绍了在PyTorch中构建两个简单NN模型的关键概念。</p><p id="f442" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated"><strong class="mc jd">警告！！！</strong>正如我在《【Tensorflow入门中提到的，你的学习才刚刚开始。要变得更好，你需要不断练习。<a class="ae lh" href="https://pytorch.org/tutorials/" rel="noopener ugc nofollow" target="_blank">官方Pytorch </a>网站提供了从初学者到专家级别的示例，以及Pytorch软件包的官方文档。祝你好运！</p></div><div class="ab cl nb nc hx nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="im in io ip iq"><p id="5794" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">[1]钦塔拉，苏密特。<em class="pw"> PyTorch Alpha-1发布</em>(2016年9月)<br/><a class="ae lh" href="https://github.com/pytorch/pytorch/releases/tag/v0.1.1" rel="noopener ugc nofollow" target="_blank">https://github.com/pytorch/pytorch/releases/tag/v0.1.1</a></p><p id="8304" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">[2]因陀罗·登·贝克。"<em class="pw">深度学习框架之战—第一部分:2017，更多框架和接口"</em><br/><a class="ae lh" rel="noopener" target="_blank" href="/battle-of-the-deep-learning-frameworks-part-i-cff0e3841750">https://towardsdatascience . com/Battle-of-the-Deep-Learning-frameworks-Part-I-cff0e 3841750</a></p><p id="1d08" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">[3]麦迪逊·梅。"<em class="pw">Python深度学习框架概述"</em><br/><a class="ae lh" href="https://www.kdnuggets.com/2017/02/python-deep-learning-frameworks-overview.html" rel="noopener ugc nofollow" target="_blank">https://www . kdnugges . com/2017/02/Python-Deep-Learning-Frameworks-Overview . html</a></p><p id="09b1" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">[4]伊莱·史蒂文斯、卢卡·安提卡、托马斯·维赫曼。"<em class="pw">用PyTorch进行深度学习"</em><br/><a class="ae lh" href="https://pytorch.org/assets/deep-learning/Deep-Learning-with-PyTorch.pdf" rel="noopener ugc nofollow" target="_blank">https://py torch . org/assets/Deep-Learning/Deep-Learning-with-py torch . pdf</a></p></div></div>    
</body>
</html>