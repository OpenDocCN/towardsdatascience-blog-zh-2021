<html>
<head>
<title>Probabilistic Deep Learning for Wind Turbines</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">风力涡轮机的概率深度学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/probabilistic-deep-learning-for-wind-turbines-b8ea00fabe30?source=collection_archive---------28-----------------------#2021-11-01">https://towardsdatascience.com/probabilistic-deep-learning-for-wind-turbines-b8ea00fabe30?source=collection_archive---------28-----------------------#2021-11-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d8d4" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何对大数据应用高斯过程</h2></div><p id="f1d7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在大型数据集上，模型速度可能是一个决定性因素。利用实证研究，我们将着眼于两种降维技术，以及它们如何应用于高斯过程。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi lb"><img src="../Images/12df6f30534c3445f9177088820c8cc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DK4seQk4sTK1ZMzj_b6ifQ.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图 1:方法概述。CNN 是卷积神经网络，GPR/VGPR 是不同的高斯过程回归。图片作者。</p></figure><p id="b760" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">关于该方法的实现，任何熟悉条件概率基础的人都可以开发高斯过程模型。然而，要充分利用框架的功能，需要相当深入的知识。高斯过程的计算效率也不是很高，但是它们的灵活性使它们成为小生境回归问题的常见选择。</p><p id="7b6f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">事不宜迟，我们开始吧。</p><h1 id="4025" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">技术 TLDR</h1><p id="0322" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">高斯过程(GPs)是非参数贝叶斯模型。对数据点之间的协方差进行建模，因此对于大于 10，000 个数据点的数据集是不切实际的。然而，他们的灵活性是无与伦比的。</p><p id="5344" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了在大型数据集上运行 GPs，我们概述了两种要素缩减方法。第一种是学习潜在特征的卷积神经网络。该论文引用了将 2501 个特征减少到 4 个，这使得该问题在计算上易于处理。在此基础上，他们利用稀疏高斯过程减少了所需的数据点数量，进一步提高了建模速度。</p><p id="6dbe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">总体而言，高斯过程观察到最高的准确性，但是稀疏高斯过程表现出相似的准确性和更快的运行时间。</p><p id="021b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是纸。</p><h1 id="a50c" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">但是实际上是怎么回事呢？</h1><p id="6a46" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">让我们稍微慢下来，讨论一下特征约简方法是如何工作的。</p><h2 id="3dbc" class="mp ls iq bd lt mq mr dn lx ms mt dp mb ko mu mv md ks mw mx mf kw my mz mh na bi translated">1 —背景</h2><p id="c370" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">当创建风电场时，它只是一系列的风力涡轮机，涡轮机的位置非常重要。从涡轮机折射的空气会显著影响后续涡轮机的效率。为了优化这种配置，我们求助于计算流体动力学。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nb"><img src="../Images/49af39fa8187238d822fa851b2ad9385.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5KpOJqcwRaV68zEK"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">由<a class="ae mo" href="https://unsplash.com/@nrdoherty?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">尼古拉斯·多尔蒂</a>在<a class="ae mo" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="5cd1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">大多数流体(空气、水等。)模拟依赖于经验数据和物理方程。然而，许多数据集包含大量嘈杂和复杂的数据。在我们的案例中，论文中引用的数据集是从德克萨斯州的一个风电场收集的，历时两年，包含每个涡轮机的 2501 个特征。行数没有透露。</p><p id="0c0c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们希望开发实时优化，我们必须简化我们的数据。这就是自动编码器的用武之地。</p><h2 id="fe9d" class="mp ls iq bd lt mq mr dn lx ms mt dp mb ko mu mv md ks mw mx mf kw my mz mh na bi translated">2 .1 —自动编码器</h2><p id="98c9" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">自动编码器是编码器-解码器框架的一个子集，用于维护特征结构。简单来说，我们希望执行两个步骤。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nc"><img src="../Images/19fefcaac0ab9f656a294dfa1ceda718.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IbKbayoVGQXMhxeU95x1Ng.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图 2:自动编码器框架。关键是数据输入和输出结构是相同的。图片作者。</p></figure><p id="b7ee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，我们将数据编码成少量的潜在变量。这些潜在变量可用于训练我们的模型，从而大幅减少训练时间。在这篇论文中，作者能够将 2501 个原始特征编码成 4 个潜在特征。</p><p id="cea2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第二，<strong class="kh ir">我们将那些潜在变量解码回我们数据的原始结构。</strong>我们需要这一步，因为我们无法处理潜在变量，我们必须有真实的预测。</p><p id="bc69" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意，这里我们使用自动编码器来减少特征，但是还有许多其他用例，其中一些包括<a class="ae mo" rel="noopener" target="_blank" href="/extreme-rare-event-classification-using-autoencoders-in-keras-a565b386f098">异常检测</a>和<a class="ae mo" rel="noopener" target="_blank" href="/deep-inside-autoencoders-7e41f319999f">数据“清理”</a>。</p><h2 id="cedc" class="mp ls iq bd lt mq mr dn lx ms mt dp mb ko mu mv md ks mw mx mf kw my mz mh na bi translated">2.2—卷积自动编码器</h2><p id="25e0" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">最流行的自动编码器框架之一利用了卷积神经网络(CNN)。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nd"><img src="../Images/66b0008e9564d4493d4616f61f5e43fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zCBq_3WxOgwxi7Ic36FBbg.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图 3:卷积示例— <a class="ae mo" rel="noopener" target="_blank" href="/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1"> src </a>。图片作者。</p></figure><p id="69ee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">CNN 是一个简单的神经网络，其中“窗口”在数据集上重复移动。它们很受<a class="ae mo" rel="noopener" target="_blank" href="/convolutional-autoencoders-for-image-noise-reduction-32fce9fc1763">图像分类</a>的欢迎，因为它们很有效，也很容易理解。然而，当扩大到二维数据之外时，CNN 仍然非常有效。</p><p id="de45" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将 CNN 自动编码器框架应用于我们的风数据集，最终得到一个明显更小的数据集。我们现在已经准备好去适应“真实的”模型了。</p><h2 id="45ba" class="mp ls iq bd lt mq mr dn lx ms mt dp mb ko mu mv md ks mw mx mf kw my mz mh na bi translated">3 —高斯概率回归</h2><p id="071d" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">在本节中，我们将提供高斯概率(GP)模型的高级解释。该论文还实现了一个多层感知器和主动学习模型，但都显示出较低的准确性或计算效率相对于 GP 模型。</p><p id="9581" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">最精确的方法——精确高斯过程</strong></p><p id="02da" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">精确高斯过程是非参数贝叶斯模型。我们从关于数据的先验假设开始，然后利用数据点之间的协方差来更新我们的先验假设。最后，根据我们的潜在特征(X ),我们得到了因变量(Y)的概率估计。这个概率也叫后验概率。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ne"><img src="../Images/26791f408efb2f515405eb64f8eab775.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s8hvIE2thNBFdYDuHD77dQ.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图 4:按组件分解的贝叶斯定理。图片作者。</p></figure><p id="01d5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于我们的示例应用程序，我们希望找到在给定 4 个潜在特征的情况下观察到风力涡轮机流量的概率。图 4 中的贝叶斯定理分解了整个概率。</p><p id="e842" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">右侧的三个组件都可以估计，但是需要一些工程设计的主要组件是先验。</p><p id="07da" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">先验概率分布，通常被称为“先验”，是在查看 X 值之前，Y 变量的概率分布。为了计算这个基线，我们只需假设它是正态分布的，并使用我们的 Y 估计平均值和标准差。</p><p id="6f48" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对其他两个组件的评估超出了本文的范围，但是可以查看评论中一些有用的链接。</p><p id="078a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">从性能的角度来看，精确的高斯过程具有时间复杂度，精确的高斯过程的时间复杂度是<a class="ae mo" href="http://krasserm.github.io/2020/12/12/gaussian-processes-sparse/" rel="noopener ugc nofollow" target="_blank"> <em class="nf"> O(n ) </em> </a>，其中<em class="nf"> n </em>是我们数据中的行数。实际上，这将我们的数据集大小限制在<a class="ae mo" href="https://arxiv.org/abs/1903.08114" rel="noopener ugc nofollow" target="_blank"> ~10，000 个数据点</a> <em class="nf">。</em>下面我们概述了一种将运行时间复杂度降低到<em class="nf"> O(nm ) </em>的方法，其中<em class="nf"> m </em>是从我们的原始数据集中提取的一组稀疏特征。</p><p id="d1ba" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">最快的方法——稀疏高斯过程</strong></p><p id="7920" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了提高模型拟合的速度，作者实现了稀疏高斯过程(SGP)。</p><p id="dfa2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">简而言之，SGP 利用一组最接近我们观察到的数据的<em class="nf"> m </em>个数据点。然后我们可以用它们来拟合我们的模型。虽然这些精度变化高度依赖于数据集，但对于风力涡轮机数据集，4 个潜在特征中的每一个都分别表现出 7%、20%、14%和 5%的精度下降。</p><p id="804c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，如果这些模型的目的是实时优化，稀疏高斯过程可能是一个可靠的替代方案。</p><p id="2b32" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们来快速看一下这些<em class="nf"> m </em>数据点是怎么选出来的。不幸的是，论文中引用的数据是不公开的，所以我们将创建自己的数据。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi ng"><img src="../Images/b420b709674f14c12f982ed2e3e39556.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*STxOk-Zckxo_taUXdYnVmA.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图 5:1000 个数据点的训练集(蓝色)，大致遵循我们的潜在数据生成函数(黑色)——<a class="ae mo" href="http://krasserm.github.io/2020/12/12/gaussian-processes-sparse/" rel="noopener ugc nofollow" target="_blank">src</a>。图片作者。</p></figure><p id="aaef" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在上面的图 5 中，我们可以看到用蓝色 X 表示的训练数据。这些是使用潜在函数(黑色实线)和一些随机噪声生成的。在 1000 个训练点中，我们的目标是估计出最接近我们训练数据的<em class="nf"> m=30 </em>个点。</p><p id="9277" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过仅拟合 30 个点，我们希望在不牺牲准确性的情况下显著降低运行时间的复杂性。最佳的 30 个点如下图 6 所示。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="lh li di lj bf lk"><div class="gh gi nh"><img src="../Images/25fd09a235bf1b03cf90ed879b96b027.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KM-vQ_qQ1iKxc5hGshMc8A.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图 6:m = 30 时的最佳诱导变量— <a class="ae mo" href="http://krasserm.github.io/2020/12/12/gaussian-processes-sparse/" rel="noopener ugc nofollow" target="_blank"> src </a>。图片作者。</p></figure><p id="b715" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在不太专业的情况下，我们希望用平均值和协方差来描述我们的<em class="nf"> m </em>数据点。在找到每个的最佳值后，我们可以从正态分布<em class="nf"> N(mean_m，cov_m) </em>中采样 30 个数据点。如果你想深入了解，这里有一个<a class="ae mo" href="http://krasserm.github.io/2020/12/12/gaussian-processes-sparse/" rel="noopener ugc nofollow" target="_blank">极好的资源</a>。</p><p id="d19c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们有了更少的数据点，我们可以更快地拟合我们的模型，而不会牺牲太多的准确性。</p><p id="238f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，这里有一个优化过程的有趣动画。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/e26fff12d2af9d0bcec04cc59831b95c.png" data-original-src="https://miro.medium.com/v2/resize:fit:816/1*wO3VwJRevD56yLYMjdEmYg.gif"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">图 7:稀疏高斯过程优化的动画— <a class="ae mo" href="http://krasserm.github.io/2020/12/12/gaussian-processes-sparse/" rel="noopener ugc nofollow" target="_blank"> src </a>。图片作者。</p></figure><p id="8bb8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就是你所知道的 GPs 的高层次概述，以及如何将它们应用于 CNN 衍生的潜在特征。</p><h1 id="4d28" class="lr ls iq bd lt lu lv lw lx ly lz ma mb jw mc jx md jz me ka mf kc mg kd mh mi bi translated">摘要</h1><p id="263d" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">在这篇文章中，我们讨论了如何简化计算复杂的模型。我们首先使用卷积神经网络来寻找减少数据集中列数的潜在特征。我们还拟合了一个稀疏高斯过程来减少数据集中的行数。</p><p id="666e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">精确高斯过程表现出最高的准确性，但是在需要实时优化的情况下，稀疏选项可能更好。</p></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><p id="2459" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nf">感谢阅读！我会再写 29 篇文章，把学术研究带到 DS 行业。查看我的评论，链接到这篇文章的主要来源和一些有用的资源。</em></p></div></div>    
</body>
</html>