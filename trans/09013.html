<html>
<head>
<title>A Comprehensive Case-Study of GraphSage with Hands-on-Experience using PyTorchGeometric Library and Open-Graph-Benchmark’s Amazon Product Recommendation Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用PyTorchGeometric库和Open-Graph-Benchmark的Amazon产品推荐数据集对GraphSage进行全面的案例研究，并提供实际操作经验</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-comprehensive-case-study-of-graphsage-algorithm-with-hands-on-experience-using-pytorchgeometric-6fc631ab1067?source=collection_archive---------6-----------------------#2021-08-20">https://towardsdatascience.com/a-comprehensive-case-study-of-graphsage-algorithm-with-hands-on-experience-using-pytorchgeometric-6fc631ab1067?source=collection_archive---------6-----------------------#2021-08-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="4c04" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><h1 id="470f" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">概述</strong></h1><p id="586d" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇博客文章提供了对<a class="ae ls" href="https://arxiv.org/abs/1706.02216" rel="noopener ugc nofollow" target="_blank"><strong class="kw ja">graph sage</strong></a><strong class="kw ja"/>的理论和实践理解的全面研究，这是一种归纳的图形表示学习算法。对于实际应用，我们将使用流行的PyTorch几何库和<a class="ae ls" href="https://ogb.stanford.edu/" rel="noopener ugc nofollow" target="_blank"><strong class="kw ja">Open-Graph-Benchmark</strong></a>数据集。我们使用<a class="ae ls" href="https://ogb.stanford.edu/docs/nodeprop/#ogbn-products" rel="noopener ugc nofollow" target="_blank"><strong class="kw ja">ogbn-products</strong></a><strong class="kw ja"/>数据集，这是一个无向图和未加权图，代表一个<strong class="kw ja">亚马逊产品共同购买网络，来预测购物偏好</strong>。节点表示亚马逊上销售的产品，两个产品之间的边表示这两个产品是一起购买的。目标是在多类别分类设置中预测产品的类别，其中47个顶级类别用于目标标签，使其成为<strong class="kw ja">节点分类任务</strong>。</p><p id="57d8" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">简单来说，这是博客的轮廓:</p><ul class=""><li id="5b93" class="ly lz iq kw b kx lt lb lu lf ma lj mb ln mc lr md me mf mg bi translated">什么是GraphSage</li><li id="aeb6" class="ly lz iq kw b kx mh lb mi lf mj lj mk ln ml lr md me mf mg bi translated">邻近抽样的重要性</li><li id="500e" class="ly lz iq kw b kx mh lb mi lf mj lj mk ln ml lr md me mf mg bi translated">获得使用GraphSage和PyTorch几何库的实践经验</li><li id="fcd9" class="ly lz iq kw b kx mh lb mi lf mj lj mk ln ml lr md me mf mg bi translated">Open-Graph-Benchmark的亚马逊产品推荐数据集</li><li id="7d1d" class="ly lz iq kw b kx mh lb mi lf mj lj mk ln ml lr md me mf mg bi translated">创建和保存模型</li><li id="57c0" class="ly lz iq kw b kx mh lb mi lf mj lj mk ln ml lr md me mf mg bi translated">生成图形嵌入、可视化和观察</li></ul><blockquote class="mm"><p id="35ad" class="mn mo iq bd mp mq mr ms mt mu mv lr dk translated">启动电源！！</p><p id="2109" class="mn mo iq bd mp mq mw mx my mz na lr dk translated">我和PyTorch Geometric，NVIDIA Triton，ArangoDB一起做过一个专题为“<a class="ae ls" href="https://www.youtube.com/watch?v=HVGRpuLD5zM&amp;t=2608s" rel="noopener ugc nofollow" target="_blank"> <strong class="ak">关于图的机器学习:超越欧氏空间的思考</strong></a><strong class="ak">”</strong>的工作坊。本次研讨会深入探讨了图形数据结构的重要性、Graph ML的应用、图形表示学习背后的动机、如何通过Nvidia Triton推理服务器在生产中使用Graph ML以及使用真实应用程序的ArangoDB。</p></blockquote><h1 id="5438" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh nb kj kk kl nc kn ko kp nd kr ks kt bi translated">什么是图形表征学习？</h1><p id="0f44" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一旦在合并了图的所有实体(节点)之间有意义的关系(边)之后创建了图。想到的下一个问题是找到一种方法来将关于图结构的信息(例如，关于图中节点的全局位置或其局部邻域结构的信息)集成到机器学习模型中。从图中提取结构信息的一种方式是使用节点度、聚类系数、核函数或手工设计的特征来计算其图统计，以估计局部邻域结构。然而，使用这些方法，我们不能执行端到端的学习，即在训练过程中不能借助损失函数来学习特征。<br/>为了解决上述问题，已经采用表示学习方法将关于图的结构信息编码到欧几里德空间(向量/嵌入空间)。</p><p id="b096" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">图形表示学习背后的关键思想是学习一个映射函数，它将节点或整个(子)图(来自非欧几里德)作为低维向量空间中的点嵌入(到嵌入空间)。目的是优化这种映射，使得在原始网络中邻近的节点在嵌入空间(向量空间)中也应该保持彼此靠近，同时将未连接的节点推开。因此，通过这样做，我们可以通过学习映射函数来保持嵌入空间内的原始网络的几何关系。下图描述了映射过程，编码器enc将节点<em class="ne"> u </em>和<em class="ne"> v </em>映射到低维向量<em class="ne"> zu </em>和<em class="ne"> zv </em>:</p><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi nf"><img src="../Images/1103eb11b5e948d254546245a56d7360.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7fUF3v1IHjpUT1o5HiKF1g.png"/></div></div><p class="nr ns gj gh gi nt nu bd b be z dk translated">图形表示学习中的映射过程(src: <a class="ae ls" href="https://snap-stanford.github.io/cs224w-notes/machine-learning-with-networks/node-representation-learning" rel="noopener ugc nofollow" target="_blank"> stanford-cs224w </a></p></figure><p id="76e1" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">让我们用Zachary空手道俱乐部社交网络的图结构中的一个有趣的例子来更直观地理解这一点。在该图中，节点表示人，并且如果两个人是朋友，则在他们之间存在边。图表中的颜色代表不同的社区。图A)表示扎卡里空手道俱乐部社交网络，图B)示出了使用<a class="ae ls" href="https://arxiv.org/abs/1403.6652" rel="noopener ugc nofollow" target="_blank"> <strong class="kw ja"> DeepWalk </strong> </a>方法从空手道图创建的节点嵌入的2D可视化。如果你分析这两个图，你会发现从一个图结构(非欧几里德或不规则域)到一个嵌入空间(图B)的节点映射是以这样一种方式完成的，即嵌入空间中的节点之间的距离反映了原始图中的接近度(保留了节点的邻域结构)。例如，在空手道图中，标记为紫色和绿色的人的群体与彼此远离的紫色和海绿色的群体相比，共享很近的距离。当在空手道图上应用深度行走方法时(为了学习节点嵌入)，当学习的节点嵌入在2D空间中可视化时，我们可以观察到相同的邻近行为。</p><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi nv"><img src="../Images/424e8957503aef5b6eb4c4188fc7aa31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WwyB3nArMo-aXQVkAGUuGg.png"/></div></div><p class="nr ns gj gh gi nt nu bd b be z dk translated">图像学分:<a class="ae ls" href="https://arxiv.org/pdf/1709.05584.pdf" rel="noopener ugc nofollow" target="_blank">图形表示学习</a></p></figure><h2 id="cd2b" class="nw jx iq bd jy nx ny dn kc nz oa dp kg lf ob oc kk lj od oe ko ln of og ks iw bi translated">我们可以将这些学习到的节点嵌入用于各种机器学习下游任务:</h2><p id="4766" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">1)它可以用作下游ML任务的特征输入(例如，经由节点分类或链接预测的社区检测)</p><p id="780b" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">2)我们可以从嵌入中构造一个KNN/余弦相似图。该图表可用于推荐(如产品推荐)</p><p id="9c51" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">3)通过使用U-Map、t-SNE算法(例如执行聚类)将数据减少到2或3维，对数据进行可视化探索。</p><p id="91c7" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">4)数据集比较</p><p id="4d6a" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">5)迁移学习</p><h1 id="6ba0" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">GraphSage动机！！</h1><p id="3f15" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇博客文章/笔记本中，我们将介绍一种<a class="ae ls" href="https://arxiv.org/pdf/1706.02216.pdf" rel="noopener ugc nofollow" target="_blank"> GraphSage </a>(样本和聚合)算法，这是一种<strong class="kw ja">归纳</strong>(它可以推广到看不见的节点)深度学习方法，由Hamilton，Ying和Leskovec (2017)开发，用于为节点生成低维向量表示的图形。这与以前的图形机器学习方法形成对比，如<a class="ae ls" href="https://arxiv.org/abs/1609.02907" rel="noopener ugc nofollow" target="_blank">图形卷积网络</a>或DeepWalk，它们是固有的<strong class="kw ja">直推式</strong>，即它们只能在训练期间为固定图形中出现的节点生成嵌入。<br/>这意味着，如果将来图形发展并且新节点(在训练期间不可见)进入图形，那么我们需要重新训练整个图形，以便计算新节点的嵌入。这一限制使得直推式方法无法有效地应用于不断进化的图形(如社交网络、蛋白质-蛋白质网络等)，因为它们无法在看不见的节点上进行推广。直推式方法(主要是DeepWalk或Node2Vec)的另一个主要限制是，它们不能利用节点特征，例如文本属性、节点概要信息、节点度等。<br/>另一方面，GraphSage算法同时利用丰富的节点特征和每个节点邻域的拓扑结构来有效地生成新节点的表示，而无需重新训练。</p><h1 id="6ff8" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">一些流行的GraphSage用例:</h1><p id="04ae" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">1)动态图表:这些图表随着时间的推移而演变，就像来自脸书、Linkedin或Twitter的社交网络图表，或者Reddit上的帖子，Youtube上的用户和视频。</p><p id="dea4" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">2)通过无监督损失函数生成的节点嵌入可用于各种下游机器学习任务，如节点分类、聚类和链接预测。</p><p id="643a" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">3)需要为其子图计算嵌入的真实世界应用</p><p id="db5a" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">4)蛋白质-蛋白质相互作用图:在这里，经过训练的嵌入生成器可以预测在新物种/有机体上收集的数据的节点嵌入</p><p id="b80b" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">5) UberEats:它利用Graph ML的力量向用户建议他们下一步可能喜欢的菜肴、餐馆和美食。为了提出这些建议，优步·艾特斯使用了GraphSAGE算法，因为它具有归纳的性质和扩展到十亿个节点的能力</p><p id="70f3" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">6) Pinterest:它使用pin sage(graph sage的另一个版本)的功能进行视觉推荐(pin是视觉书签，例如用于购买衣服或其他产品)。PinSage是一种基于随机游走的GraphSage算法，它学习网络规模图中节点(以十亿计)的嵌入。</p><h1 id="6da2" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">GraphSage的工作原理</h1><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/b3a8a9a81e8423a01a71760f5201ca0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1278/format:webp/1*J-gDVY-bAUQliRQNHB4h5A.png"/></div><p class="nr ns gj gh gi nt nu bd b be z dk translated">来源:<a class="ae ls" href="https://arxiv.org/pdf/1706.02216.pdf" rel="noopener ugc nofollow" target="_blank">大型图上的归纳表示学习</a></p></figure><p id="9eac" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">GraphSage的工作过程主要分为两步，第一步是对输入图进行<strong class="kw ja">邻域采样</strong>，第二步是<strong class="kw ja">在每个搜索深度学习聚合函数</strong>。我们将详细讨论这些步骤中的每一个步骤，从执行节点邻域采样的需求开始。之后，我们将讨论学习聚合器函数的重要性，这些函数基本上帮助GraphSage算法实现了其属性<strong class="kw ja">归纳性</strong>。</p><h1 id="3477" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">邻域抽样的重要性是什么？</h1><p id="8f7a" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们从下面描述的图形卷积网络图(GCNs)的角度来理解这一点。GCNs是一种算法，它可以利用图形拓扑信息(即节点的邻域)和节点特征，然后提取这些信息以生成节点表示或密集矢量嵌入。下图直观地展示了GCNs的工作过程。在左侧，我们有一个样本输入图，其中的节点由它们相应的特征向量(例如，节点度或文本嵌入等)表示。我们首先定义一个搜索深度(K ),它通知算法从目标节点的邻域收集信息应该达到什么深度。这里，K是一个超参数，它也描述了GCNs中使用的层数。</p><p id="b013" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">在K=0时，GCNs将所有节点嵌入初始化为它们的原始特征向量。现在，假设我们想要计算目标节点<em class="ne"> 0 </em>在层K=1处的嵌入，然后我们<strong class="kw ja">聚集</strong>(它是其邻居的置换不变函数)与节点<em class="ne"> 0 </em>相距1跳距离的所有节点(包括其自身)的特征向量(在该时间步或层，我们正在聚集K=0处的节点的原始特征表示)。对于目标节点<em class="ne"> 0 </em>，GCNs使用均值聚合器来计算相邻节点特征及其自身特征的均值(自循环)。在K=1之后，目标节点<em class="ne"> 0 </em>现在知道了关于其紧邻的信息；这一过程如下图所示。我们对图中的所有节点重复这一过程(即，对于每个节点，我们在1跳邻域上聚合),以便在每一层找到每个节点的新表示。</p><p id="2838" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated"><strong class="kw ja">注意:</strong>随着搜索深度的增加，目标节点从其局部邻域聚集特征的范围也增加。例如，在K=1时，目标节点知道关于其1跳距离的本地邻居的信息，在K=2时，目标节点知道关于其1跳距离的本地邻居的信息，以及1跳距离即高达2跳距离的节点的邻居的信息。</p><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi oi"><img src="../Images/a47661a2806b42bcaa2c56c61af7327f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HcKIzPkiZWSA5l6djW7EnQ.png"/></div></div></figure><h1 id="0ecb" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">GCN方法的问题</h1><p id="61be" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如上所述，GCNs使用邻域聚合来计算节点表示。出于训练目的，我们可以将目标节点的k跳邻域表示为计算图，并且以小批量方式发送这些计算图，以便学习网络的权重(即，应用随机梯度下降)。下图说明了目标节点0到2跳邻域的计算图。现在，这个问题是:</p><p id="0e11" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">1) <strong class="kw ja">计算量大</strong>:因为对于每个节点，我们需要生成完整的K跳邻域计算图，然后需要从其周围聚集大量信息。随着我们深入到邻域(大K ),计算图变得指数级大。这可能会导致在GPU内存中拟合这些大型计算图形时出现问题。</p><p id="a924" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">2) <strong class="kw ja">枢纽节点或名人节点的诅咒</strong>:枢纽节点是那些在图中度数非常高的节点，例如一个非常受欢迎的名人拥有数百万个连接。如果是这种情况，那么我们需要聚集来自数百万个节点的信息，以便计算中枢节点的嵌入。因此，为中枢节点生成的计算图非常庞大。这个问题图示如下(r.h.s)。</p><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi oj"><img src="../Images/8fb0d3eb290ece84eaea33508619e707.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ifCiUqCY2aNvZ-uP2dFMAA.png"/></div></div><p class="nr ns gj gh gi nt nu bd b be z dk translated"><a class="ae ls" href="http://web.stanford.edu/class/cs224w/slides/17-scalable.pdf" rel="noopener ugc nofollow" target="_blank">图像信用</a></p></figure><p id="6972" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">因此，想法不是获取目标节点的整个K跳邻域，而是从K跳邻域中随机选择几个节点，以便生成计算图。这一过程被称为邻域采样，它为GraphSage算法提供了独特的能力，可以将图中的节点<strong class="kw ja">扩展到十亿个</strong>。因此，使用这种方法，如果我们遇到任何中枢节点，那么我们不会采用它的整个K跳邻域，而是从每层或搜索深度K中随机选择几个节点。现在，生成的计算图由GPU处理更有效。下图显示了在每一跳最多采样2个邻居的过程。</p><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/bdc09bb7cb980d8b92b71533c99c2ba4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*EFDGLO0P81K-AKsG1Te_2Q.png"/></div><p class="nr ns gj gh gi nt nu bd b be z dk translated"><a class="ae ls" href="http://web.stanford.edu/class/cs224w/slides/17-scalable.pdf" rel="noopener ugc nofollow" target="_blank">图像信用</a></p></figure><h1 id="6074" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">为什么GraphSage被称为归纳表示学习算法？</h1><p id="4018" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">GraphSage是GCNs的归纳版本，这意味着它在学习期间不需要整个图结构，并且它可以很好地推广到看不见的节点。它是图形神经网络的一个分支，通过从多个搜索深度或跳跃中采样和聚集邻居来学习节点表示。其归纳属性基于这样的前提，即我们不需要学习每个节点的嵌入，而是学习一个聚集函数(可以是任何可微分函数，如均值、池或lstm ),当给定来自节点的局部邻域的信息(或特征)时，它知道如何聚集这些特征(通过随机梯度下降进行学习),使得节点<strong class="kw ja"> v </strong>的聚集特征表示现在包括关于其局部环境或邻域的信息。</p><p id="6133" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">GraphSage在两个方面不同于GCNs:即1)graph sage不是获取目标节点的整个K跳邻域，而是首先采样或修剪K跳邻域计算图，然后在这个采样图上执行特征聚集操作，以便为目标节点生成嵌入。2)在学习过程中，为了生成节点嵌入；GraphSage学习聚合器函数，而gcn利用对称归一化图拉普拉斯算子。</p><p id="9d38" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">下图说明了GraphSage节点<strong class="kw ja"> 0 </strong>如何在搜索深度K=1处从其采样的本地邻居聚集信息。如果我们观察r.h.s图，我们将发现在K=1时，目标节点<strong class="kw ja"> 0 </strong>现在具有关于其周围直到1跳的信息。</p><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi ol"><img src="../Images/4ec2ce10ee41e372bd858a19d735ff5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pauLBetIACV2KXNORVsw-Q.png"/></div></div></figure><h1 id="1811" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">GraphSage的形式解释</h1><p id="e58c" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如上所述，GraphSage的关键概念是学习如何从节点的本地邻域聚集特征信息。现在，让我们更正式地理解GraphSage如何使用前向传播在每一层(K)生成节点嵌入。我们借助视觉来理解这一点，然后将这种理解映射到<a class="ae ls" href="https://arxiv.org/pdf/1706.02216.pdf" rel="noopener ugc nofollow" target="_blank"> GraphSage论文</a>中提到的伪代码。但在此之前，让我们定义一些本文中使用的符号。</p><p id="b30a" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated"><strong class="kw ja">定义符号:</strong></p><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/10bb45301b45696d6d715f540e58bbbe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*X4v2lEiNGlWsT1cTRg9IWA.png"/></div></figure><p id="5974" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">如上图所示，在K=1时，目标节点<strong class="kw ja"> 0 </strong>聚合来自其本地邻居的信息(特征),最多1跳。类似地，在k=2时，目标节点<strong class="kw ja"> 0 </strong>聚集来自其本地邻居的信息直到2跳，即现在它知道在它的邻居中有什么直到2跳。因此，我们可以重复这个过程，其中目标节点<strong class="kw ja"> 0 </strong>从图的更远的范围递增地获得越来越多的信息。我们为原始图(∀v ∈ V)中的每个节点收集信息。让我们添加一些视觉效果来更直观地理解这个迭代过程:</p><p id="b1b9" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">下图描绘了层K=0处的目标节点<strong class="kw ja"> 0 </strong>的计算图，此时图中的所有节点都被初始化为其原始特征向量。我们的目标是通过迭代的局部邻域信息收集过程，在层K=2找到节点<strong class="kw ja"> 0 </strong>(即z0)的最终表示。这个迭代过程有时也被称为<strong class="kw ja">消息传递方法</strong>。</p><p id="9139" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">因此，我们可以将此步骤正式表示为:</p><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/5bea4d298ae034b2a73fab22eaf768a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*e1iWQ9Gp2O-Pf6IQQuKBAg.png"/></div></figure><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi oo"><img src="../Images/4c8342810620daa6b7a3aa19610a68da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zjdghgb5T2bITiIQiMp5Dw.png"/></div></div></figure><p id="4805" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated"><strong class="kw ja">注:</strong>由于medium不支持下标，我就把隐藏层(h)表示写成(上标)h(下标)。</p><p id="c433" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">上标表示-&gt; <em class="ne">第k</em>层</p><p id="8aba" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">下标表示-&gt;节点id</p><h2 id="53a3" class="nw jx iq bd jy nx ny dn kc nz oa dp kg lf ob oc kk lj od oe ko ln of og ks iw bi translated">邻域聚合(K=1)</h2><p id="74cf" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于节点是从图的更深处逐渐收集信息的，所以我们从搜索深度1…K开始迭代过程。在K=1时，我们将目标节点<strong class="kw ja"> 0 </strong> (1h0)的相邻节点表示聚合到单个向量中，即位于前一层(K-1h2和K-1h3)的节点2和3表示。这里1h0是聚合表示。在同一时间步，节点2、3和9也将聚集来自它们各自的局部邻域的特征向量，直到1跳的距离。现在，在这个时间点上，计算图中的每个节点都知道它们的周围环境中有什么样的信息。</p><p id="d413" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">因此，我们可以将此步骤正式表示为:</p><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div class="gh gi op"><img src="../Images/9b5495a5049e9e8188b7e6eba3ce313c.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/format:webp/1*inBFS7tpnyMa4-K0FUj-fw.png"/></div></figure><h1 id="2c67" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">更新</strong></h1><p id="4baa" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一旦我们实现了聚合表示，即1h0，下一步将是将该聚合表示与其先前的层表示(0h0)连接或组合。然后通过将它乘以一个权重矩阵<strong class="kw ja"> WK </strong>将变换应用于这个连接的输出，你可以认为这个过程类似于将卷积核(可学习的权重矩阵)应用于图像，以便从中提取特征。最后，我们对这个转换后的输出应用非线性激活函数，使其能够学习和执行更复杂的任务。</p><p id="6e1c" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated"><strong class="kw ja">重要提示</strong>:graph sage算法在每个搜索深度K单独学习权重矩阵，或者你也可以说它学习如何在每个搜索深度从节点的邻域聚集信息。</p><p id="7e4b" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">因此，我们可以将此步骤正式表示为:</p><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/72b97a84b7ae3fa0f2e0598a422de0d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*B0CXIAjOC8QITPF3PtjKPw.png"/></div></figure><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi or"><img src="../Images/2a9a9e6a548932f358654f49cc9c9c85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R7Ll95M2wLsDLwWt-DWBgw.png"/></div></div></figure><h1 id="e3c7" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">规范化节点嵌入</h1><p id="dc46" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">随后，对节点表示khv(或此时的步骤1h0)应用归一化，这有助于算法保持节点嵌入的一般分布。该步骤计算如下:</p><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/3e26b6050cc3dc3ba2841136ca76a644.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/format:webp/1*kFgEo79jqPkj_rObqm4mUA.png"/></div></figure><h1 id="8ff2" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">K=2时的节点嵌入</h1><p id="54ce" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">完成了从K=1的节点的局部邻域收集信息。在K=2时，节点探索图的更远的范围，即超出它们的直接邻居，并且查看跳距离2。我们再次执行节点的本地邻居聚合，但是这一次目标节点<strong class="kw ja"> 0 </strong>现在将具有其在1跳和2跳距离的邻居的信息。然后，我们再次重复搜索深度K=2的更新和归一化过程。由于为了理解GraphSage算法的流程，我们已经设置了K=2的值，因此，我们将在此停止。在K=2之后，计算图中的每个节点由它们各自的最终节点嵌入表示，即<strong class="kw ja"> zv </strong>。</p><p id="b88b" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">此工作流程如下图所示:</p><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi ot"><img src="../Images/0402e059cc2ccd8765b8dc2de7c1c269.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YKrRcKe7UoyqosyX8ywZOg.png"/></div></div></figure><h1 id="8a75" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated"><strong class="ak">现在，我们可以很容易地将我们的理解映射到论文中的以下GraphSage算法:</strong></h1><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi ou"><img src="../Images/cc78c0cb7d7aacf7696dddf1329fc14a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vcj0SryNot5AtlERbH0M0g.png"/></div></div><p class="nr ns gj gh gi nt nu bd b be z dk translated"><a class="ae ls" href="https://arxiv.org/pdf/1706.02216.pdf" rel="noopener ugc nofollow" target="_blank">算法信用</a></p></figure><h1 id="b807" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">损失函数:学习参数</h1><p id="65d5" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作者在论文中使用两种不同类型的损失函数记录了结果，如下所示:</p><p id="3553" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated"><strong class="kw ja">无监督情况</strong>:如图形表示学习部分所述，目标是优化映射，使得原始网络中邻近的节点在嵌入空间(向量空间)中也应该保持彼此靠近，同时将未连接的节点推开。</p><p id="cbad" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated"><strong class="kw ja">监督案例</strong>:作者使用常规交叉熵损失来执行节点分类任务。</p><p id="bf4c" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">下面是本文中使用的无监督损失函数:</p><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/2e7efcddf7f648d1882f3af01532a948.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*dCWxhSn8bGMv2llcYSsoPw.png"/></div></figure><h1 id="261f" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">使用PyTorch几何库和OGB基准数据集对GraphSage进行实际操作！</h1><p id="4056" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将借助来自<a class="ae ls" href="https://ogb.stanford.edu/" rel="noopener ugc nofollow" target="_blank"><strong class="kw ja">Open Graph Benchmark</strong></a>(OGB)数据集的真实数据集，更详细地理解GraphSage的工作过程。OGB是由斯坦福大学开发的一个现实的、大规模的、多样化的图表机器学习基准数据集的集合。</p><h1 id="a181" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">注意了。！！</h1><blockquote class="ow ox oy"><p id="8dd3" class="ku kv ne kw b kx lt kz la lb lu ld le oz lv lh li pa lw ll lm pb lx lp lq lr ij bi translated">前面有很多代码，如果你有兴趣接触这些代码，我真的鼓励你这样做，那么我已经准备了一个<a class="ae ls" href="https://colab.research.google.com/github/sachinsharma9780/interactive_tutorials/blob/master/notebooks/example_output/Comprehensive_GraphSage_Guide_with_PyTorchGeometric_Output.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="kw ja"> google colab笔记本</strong> </a> <strong class="kw ja"> </strong>供你使用…</p></blockquote><h1 id="dc56" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">资料组</h1><p id="00a9" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们使用<a class="ae ls" href="https://ogb.stanford.edu/docs/nodeprop/#ogbn-products" rel="noopener ugc nofollow" target="_blank"> obgn-products </a>数据集，这是一个无向图，表示亚马逊产品联合购买网络。节点表示亚马逊上销售的产品，两个产品之间的边表示这两个产品是一起购买的。节点特征表示取自产品描述的词汇特征。目标是在多类别分类设置中预测产品的类别，其中47个顶级类别用于目标标签，使其成为<strong class="kw ja">节点分类任务</strong>。</p><h1 id="5377" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">让我们从下载必要的库开始</h1><pre class="ng nh ni nj gt pc pd pe pf aw pg bi"><span id="2f00" class="nw jx iq pd b gy ph pi l pj pk"><em class="ne"># Installing Pytorch Geometric </em><br/>%%capture<br/>!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html<br/>!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html<br/>!pip install -q torch-cluster -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html<br/>!pip install -q torch-geometric<br/>!pip install ogb<br/>!pip install umap-learn</span></pre><p id="114f" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">导入必要的库</p><pre class="ng nh ni nj gt pc pd pe pf aw pg bi"><span id="1db5" class="nw jx iq pd b gy ph pi l pj pk"><strong class="pd ja">import</strong> <strong class="pd ja">torch</strong><br/><strong class="pd ja">import</strong> <strong class="pd ja">torch.nn.functional</strong> <strong class="pd ja">as</strong> <strong class="pd ja">F</strong><br/><strong class="pd ja">from</strong> <strong class="pd ja">tqdm</strong> <strong class="pd ja">import</strong> tqdm<br/><strong class="pd ja">from</strong> <strong class="pd ja">torch_geometric.data</strong> <strong class="pd ja">import</strong> NeighborSampler<br/><strong class="pd ja">from</strong> <strong class="pd ja">torch_geometric.nn</strong> <strong class="pd ja">import</strong> SAGEConv<br/><strong class="pd ja">import</strong> <strong class="pd ja">os.path</strong> <strong class="pd ja">as</strong> <strong class="pd ja">osp</strong><br/><strong class="pd ja">import</strong> <strong class="pd ja">pandas</strong> <strong class="pd ja">as</strong> <strong class="pd ja">pd</strong><br/><strong class="pd ja">import</strong> <strong class="pd ja">numpy</strong> <strong class="pd ja">as</strong> <strong class="pd ja">np</strong><br/><strong class="pd ja">import</strong> <strong class="pd ja">collections</strong><br/><strong class="pd ja">from</strong> <strong class="pd ja">pandas.core.common</strong> <strong class="pd ja">import</strong> flatten<br/><em class="ne"># importing obg datatset</em><br/><strong class="pd ja">from</strong> <strong class="pd ja">ogb.nodeproppred</strong> <strong class="pd ja">import</strong> PygNodePropPredDataset, Evaluator<br/><strong class="pd ja">from</strong> <strong class="pd ja">pandas.core.common</strong> <strong class="pd ja">import</strong> flatten<br/><strong class="pd ja">import</strong> <strong class="pd ja">seaborn</strong> <strong class="pd ja">as</strong> <strong class="pd ja">sns</strong><br/><strong class="pd ja">import</strong> <strong class="pd ja">matplotlib.pyplot</strong> <strong class="pd ja">as</strong> <strong class="pd ja">plt</strong><br/>sns.set(rc={'figure.figsize':(16.7,8.27)})<br/>sns.set_theme(style="ticks")<br/><strong class="pd ja">import</strong> <strong class="pd ja">collections</strong><br/><strong class="pd ja">from</strong> <strong class="pd ja">scipy.special</strong> <strong class="pd ja">import</strong> softmax<br/><strong class="pd ja">import</strong> <strong class="pd ja">umap</strong></span></pre><p id="35ab" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">下载并加载数据集</p><pre class="ng nh ni nj gt pc pd pe pf aw pg bi"><span id="cc6b" class="nw jx iq pd b gy ph pi l pj pk"><em class="ne"> </em><br/>root = osp.join(osp.dirname(osp.realpath('./')), 'data', 'products')<br/>dataset = PygNodePropPredDataset('ogbn-products', root)</span></pre><p id="25f3" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">获取训练、验证和测试索引</p><pre class="ng nh ni nj gt pc pd pe pf aw pg bi"><span id="2ecd" class="nw jx iq pd b gy ph pi l pj pk"><em class="ne"># split_idx contains a dictionary of train, validation and test node indices</em><br/>split_idx = dataset.get_idx_split()<br/><em class="ne"># predefined ogb evaluator method used for validation of predictions</em><br/>evaluator = Evaluator(name='ogbn-products')</span></pre><p id="4be0" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">让我们检查一下训练、验证和测试节点的划分。</p><pre class="ng nh ni nj gt pc pd pe pf aw pg bi"><span id="e81b" class="nw jx iq pd b gy ph pi l pj pk"><em class="ne"># lets check the node ids distribution of train, test and val</em><br/>print('Number of training nodes:', split_idx['train'].size(0))<br/>print('Number of validation nodes:', split_idx['valid'].size(0))<br/>print('Number of test nodes:', split_idx['test'].size(0))</span><span id="681c" class="nw jx iq pd b gy pl pi l pj pk">Number of training nodes: 196615<br/>Number of validation nodes: 39323<br/>Number of test nodes: 2213091</span></pre><p id="f73e" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">加载数据集</p><pre class="ng nh ni nj gt pc pd pe pf aw pg bi"><span id="1ea4" class="nw jx iq pd b gy ph pi l pj pk">data = dataset[0]</span></pre><p id="2028" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">图表统计</p><pre class="ng nh ni nj gt pc pd pe pf aw pg bi"><span id="b250" class="nw jx iq pd b gy ph pi l pj pk"><em class="ne"># lets check some graph statistics of ogb-product graph</em><br/>print("Number of nodes in the graph:", data.num_nodes)<br/>print("Number of edges in the graph:", data.num_edges)<br/>print("Node feature matrix with shape:", data.x.shape) <em class="ne"># [num_nodes, num_node_features]</em><br/>print("Graph connectivity in COO format with shape:", data.edge_index.shape) <em class="ne"># [2, num_edges]</em><br/>print("Target to train against :", data.y.shape) <br/>print("Node feature length", dataset.num_features)</span><span id="be23" class="nw jx iq pd b gy pl pi l pj pk">Number of nodes in the graph: 2449029<br/>Number of edges in the graph: 123718280<br/>Node feature matrix with shape: torch.Size([2449029, 100])<br/>Graph connectivity in COO format with shape: torch.Size([2, 123718280])<br/>Target to train against : torch.Size([2449029, 1])<br/>Node feature length 100</span></pre><p id="f1fd" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated"><em class="ne">检查唯一标签的数量</em></p><pre class="ng nh ni nj gt pc pd pe pf aw pg bi"><span id="498a" class="nw jx iq pd b gy ph pi l pj pk"><em class="ne"># there are 47 unique categories of product</em><br/>data.y.unique()</span></pre><p id="3603" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated"><em class="ne">从数据集内提供的标签映射将整数加载到真实产品类别</em></p><pre class="ng nh ni nj gt pc pd pe pf aw pg bi"><span id="bc91" class="nw jx iq pd b gy ph pi l pj pk">df = pd.read_csv('/data/products/ogbn_products/mapping/labelidx2productcategory.csv.gz')</span></pre><p id="6471" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">让我们看看一些产品类别</p><pre class="ng nh ni nj gt pc pd pe pf aw pg bi"><span id="4434" class="nw jx iq pd b gy ph pi l pj pk">df[:10]</span></pre><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi pm"><img src="../Images/2e8792716cd2021b34bf1cf7eaf4c209.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CTarD5JrWHzYqzRazWdoyg.png"/></div></div></figure><p id="2cd5" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated"><em class="ne">创建产品类别和相应整数标签的字典</em></p><pre class="ng nh ni nj gt pc pd pe pf aw pg bi"><span id="376d" class="nw jx iq pd b gy ph pi l pj pk">label_idx, prod_cat = df.iloc[: ,0].values, df.iloc[: ,1].values<br/>label_mapping = dict(zip(label_idx, prod_cat))</span><span id="83af" class="nw jx iq pd b gy pl pi l pj pk"><em class="ne"># counting the numbers of samples for each category</em><br/>y = data.y.tolist()<br/>y = list(flatten(y))<br/>count_y = collections.Counter(y)<br/>print(count_y)</span></pre><h2 id="5096" class="nw jx iq bd jy nx ny dn kc nz oa dp kg lf ob oc kk lj od oe ko ln of og ks iw bi translated">邻近抽样</h2><p id="f589" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该模块迭代地采样邻居(在每层)并构建模拟GNNs实际计算流程的二分图。</p><p id="39e0" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">sizes:表示我们希望对每层中的每个节点采样多少个邻居。</p><p id="b582" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated"><code class="fe pn po pp pd b">NeighborSampler</code>保存当前:obj: <code class="fe pn po pp pd b">batch_size</code>，参与计算的所有节点的id:obj:<code class="fe pn po pp pd b">n_id</code>，通过元组保存二分图对象列表:obj: <code class="fe pn po pp pd b">(edge_index, e_id, size)</code>，其中:obj: <code class="fe pn po pp pd b">edge_index</code>表示源节点和目标节点之间的二分边，obj: <code class="fe pn po pp pd b">e_id</code>表示完整图中原始边的id，obj: <code class="fe pn po pp pd b">size</code>保存二分图的形状。</p><p id="e4ca" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">然后，实际的计算图以相反的方式返回，这意味着我们将消息从一个较大的节点集传递到一个较小的节点集，直到到达我们最初想要计算嵌入的节点。</p><pre class="ng nh ni nj gt pc pd pe pf aw pg bi"><span id="5e21" class="nw jx iq pd b gy ph pi l pj pk">train_idx = split_idx['train']<br/>train_loader = NeighborSampler(data.edge_index, node_idx=train_idx,<br/>                               sizes=[15, 10, 5], batch_size=1024,<br/>                               shuffle=<strong class="pd ja">True</strong>)</span></pre><h2 id="4ed3" class="nw jx iq bd jy nx ny dn kc nz oa dp kg lf ob oc kk lj od oe ko ln of og ks iw bi translated">GraphSage算法</h2><pre class="ng nh ni nj gt pc pd pe pf aw pg bi"><span id="7125" class="nw jx iq pd b gy ph pi l pj pk"><strong class="pd ja">class</strong> <strong class="pd ja">SAGE</strong>(torch.nn.Module):<br/>    <strong class="pd ja">def</strong> __init__(self, in_channels, hidden_channels, out_channels, num_layers=3):<br/>        super(SAGE, self).__init__()<br/><br/>        self.num_layers = num_layers<br/><br/>        self.convs = torch.nn.ModuleList()<br/>        self.convs.append(SAGEConv(in_channels, hidden_channels))<br/>        <strong class="pd ja">for</strong> _ <strong class="pd ja">in</strong> range(num_layers - 2):<br/>            self.convs.append(SAGEConv(hidden_channels, hidden_channels))<br/>        self.convs.append(SAGEConv(hidden_channels, out_channels))<br/><br/>    <strong class="pd ja">def</strong> reset_parameters(self):<br/>        <strong class="pd ja">for</strong> conv <strong class="pd ja">in</strong> self.convs:<br/>            conv.reset_parameters()<br/><br/>    <strong class="pd ja">def</strong> forward(self, x, adjs):<br/>        <em class="ne"># `train_loader` computes the k-hop neighborhood of a batch of nodes,</em><br/>        <em class="ne"># and returns, for each layer, a bipartite graph object, holding the</em><br/>        <em class="ne"># bipartite edges `edge_index`, the index `e_id` of the original edges,</em><br/>        <em class="ne"># and the size/shape `size` of the bipartite graph.</em><br/>        <em class="ne"># Target nodes are also included in the source nodes so that one can</em><br/>        <em class="ne"># easily apply skip-connections or add self-loops.</em><br/>        <strong class="pd ja">for</strong> i, (edge_index, _, size) <strong class="pd ja">in</strong> enumerate(adjs):<br/>            xs = []<br/>            x_target = x[:size[1]]  <em class="ne"># Target nodes are always placed first.</em><br/>            x = self.convs[i]((x, x_target), edge_index)<br/>            <strong class="pd ja">if</strong> i != self.num_layers - 1:<br/>                x = F.relu(x)<br/>                x = F.dropout(x, p=0.5, training=self.training)<br/>            xs.append(x)<br/>            <strong class="pd ja">if</strong> i == 0: <br/>                x_all = torch.cat(xs, dim=0)<br/>                layer_1_embeddings = x_all<br/>            <strong class="pd ja">elif</strong> i == 1:<br/>                x_all = torch.cat(xs, dim=0)<br/>                layer_2_embeddings = x_all<br/>            <strong class="pd ja">elif</strong> i == 2:<br/>                x_all = torch.cat(xs, dim=0)<br/>                layer_3_embeddings = x_all    <br/>        <em class="ne">#return x.log_softmax(dim=-1)</em><br/>        <strong class="pd ja">return</strong> layer_1_embeddings, layer_2_embeddings, layer_3_embeddings<br/><br/>    <strong class="pd ja">def</strong> inference(self, x_all):<br/>        pbar = tqdm(total=x_all.size(0) * self.num_layers)<br/>        pbar.set_description('Evaluating')<br/><br/>        <em class="ne"># Compute representations of nodes layer by layer, using *all*</em><br/>        <em class="ne"># available edges. This leads to faster computation in contrast to</em><br/>        <em class="ne"># immediately computing the final representations of each batch.</em><br/>        total_edges = 0<br/>        <strong class="pd ja">for</strong> i <strong class="pd ja">in</strong> range(self.num_layers):<br/>            xs = []<br/>            <strong class="pd ja">for</strong> batch_size, n_id, adj <strong class="pd ja">in</strong> subgraph_loader:<br/>                edge_index, _, size = adj.to(device)<br/>                total_edges += edge_index.size(1)<br/>                x = x_all[n_id].to(device)<br/>                x_target = x[:size[1]]<br/>                x = self.convs[i]((x, x_target), edge_index)<br/>                <strong class="pd ja">if</strong> i != self.num_layers - 1:<br/>                    x = F.relu(x)<br/>                xs.append(x)<br/><br/>                pbar.update(batch_size)<br/><br/>            <strong class="pd ja">if</strong> i == 0: <br/>                x_all = torch.cat(xs, dim=0)<br/>                layer_1_embeddings = x_all<br/>            <strong class="pd ja">elif</strong> i == 1:<br/>                x_all = torch.cat(xs, dim=0)<br/>                layer_2_embeddings = x_all<br/>            <strong class="pd ja">elif</strong> i == 2:<br/>                x_all = torch.cat(xs, dim=0)<br/>                layer_3_embeddings = x_all<br/>                <br/>        pbar.close()<br/><br/>        <strong class="pd ja">return</strong> layer_1_embeddings, layer_2_embeddings, layer_3_embeddings</span></pre><p id="4669" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">实例化模型</p><pre class="ng nh ni nj gt pc pd pe pf aw pg bi"><span id="c57a" class="nw jx iq pd b gy ph pi l pj pk">device = torch.device('cuda' <strong class="pd ja">if</strong> torch.cuda.is_available() <strong class="pd ja">else</strong> 'cpu')<br/>model = SAGE(dataset.num_features, 256, dataset.num_classes, num_layers=3)<br/>model = model.to(device)</span></pre><p id="326d" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">加载节点特征矩阵和节点标签</p><pre class="ng nh ni nj gt pc pd pe pf aw pg bi"><span id="d744" class="nw jx iq pd b gy ph pi l pj pk">x = data.x.to(device)<br/>y = data.y.squeeze().to(device)</span></pre><p id="d55c" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">培养</p><pre class="ng nh ni nj gt pc pd pe pf aw pg bi"><span id="044a" class="nw jx iq pd b gy ph pi l pj pk"><strong class="pd ja">def</strong> train(epoch):<br/>    model.train()<br/><br/>    <em class="ne">#pbar = tqdm(total=train_idx.size(0))</em><br/>    <em class="ne">#pbar.set_description(f'Epoch {epoch:02d}')</em><br/><br/>    total_loss = total_correct = 0<br/>    <strong class="pd ja">for</strong> batch_size, n_id, adjs <strong class="pd ja">in</strong> train_loader:<br/>        <em class="ne"># `adjs` holds a list of `(edge_index, e_id, size)` tuples.</em><br/>        adjs = [adj.to(device) <strong class="pd ja">for</strong> adj <strong class="pd ja">in</strong> adjs]<br/>        optimizer.zero_grad()    <br/>        l1_emb, l2_emb, l3_emb = model(x[n_id], adjs)<br/>        <em class="ne">#print("Layer 1 embeddings", l1_emb.shape)</em><br/>        <em class="ne">#print("Layer 2 embeddings", l1_emb.shape)</em><br/>        out = l3_emb.log_softmax(dim=-1)<br/>        loss = F.nll_loss(out, y[n_id[:batch_size]])<br/>        loss.backward()<br/>        optimizer.step()<br/><br/>        total_loss += float(loss)<br/>        total_correct += int(out.argmax(dim=-1).eq(y[n_id[:batch_size]]).sum())<br/>        <em class="ne">#pbar.update(batch_size)</em><br/><br/>    <em class="ne">#pbar.close()</em><br/><br/>    loss = total_loss / len(train_loader)<br/>    approx_acc = total_correct / train_idx.size(0)<br/><br/>    <strong class="pd ja">return</strong> loss, approx_acc</span></pre><p id="20e1" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">划时代！！</p><pre class="ng nh ni nj gt pc pd pe pf aw pg bi"><span id="a393" class="nw jx iq pd b gy ph pi l pj pk">optimizer = torch.optim.Adam(model.parameters(), lr=0.003)<br/><br/><strong class="pd ja">for</strong> epoch <strong class="pd ja">in</strong> range(1, 21):<br/>    loss, acc = train(epoch)<br/>    <em class="ne">print(f'Epoch {epoch:02d}, Loss: {loss:.4f}, Approx. Train: {acc:.4f}')</em></span></pre><h2 id="d274" class="nw jx iq bd jy nx ny dn kc nz oa dp kg lf ob oc kk lj od oe ko ln of og ks iw bi translated">为推理部分保存模型</h2><p id="770c" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们需要保存推理部分的模型，因为由于RAM大小的限制，google colab不能同时创建两个图形加载器。因此，我们首先使用train_loader进行训练，然后使用这个保存的模型对测试数据进行推理。</p><p id="b967" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">在这里，您可以将模型保存在google MyDrive或本地计算机上。</p><pre class="ng nh ni nj gt pc pd pe pf aw pg bi"><span id="f40d" class="nw jx iq pd b gy ph pi l pj pk"><em class="ne">#torch.save(model, '/content/drive/MyDrive/model_weights/graph_embeddings/model.pt')</em><br/><br/><em class="ne"># saving model in mydrive</em><br/><strong class="pd ja">from</strong> <strong class="pd ja">google.colab</strong> <strong class="pd ja">import</strong> drive<br/>drive.mount('/content/drive')<br/>fp = '/content/drive/MyDrive/model.pt'<br/><br/>torch.save(model, './model.pt')<br/>torch.save(model, fp)</span></pre><h1 id="1da2" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">推论:让我们检查GraphSage感应功率！！</h1><p id="e8c8" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这一部分包括利用经过训练的GraphSage模型来计算节点嵌入并对测试数据执行节点类别预测。之后，我们比较了GraphSage的3个不同层的节点嵌入的U-Map可视化，得出了一些有趣的观察结果。</p><blockquote class="ow ox oy"><p id="6632" class="ku kv ne kw b kx lt kz la lb lu ld le oz lv lh li pa lw ll lm pb lx lp lq lr ij bi translated">如果读者在我准备的<a class="ae ls" href="https://colab.research.google.com/github/sachinsharma9780/interactive_tutorials/blob/master/notebooks/example_output/Comprehensive_GraphSage_Guide_with_PyTorchGeometric_Output.ipynb" rel="noopener ugc nofollow" target="_blank"> <strong class="kw ja">谷歌实验室笔记本</strong> </a>中运行GraphSage的推理部分，会对他/她更有用，以便更好地直观了解GraphSage每一层的可视化是如何计算的。</p></blockquote><h1 id="0fc2" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">GraphSage第1层节点嵌入可视化</h1><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi pq"><img src="../Images/197342cb28edf8dd5aaaf4cfc4890c1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3yuU8yQCdkutZ3hn5k89lA.png"/></div></div></figure><h2 id="eca9" class="nw jx iq bd jy nx ny dn kc nz oa dp kg lf ob oc kk lj od oe ko ln of og ks iw bi translated">观察</h2><p id="d74b" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第1层的节点嵌入可视化表明，该模型无法很好地区分产品类别(因为不同产品类别的嵌入非常接近)，因此我们无法以很高的概率预测/估计未来哪两种产品可以一起购买，或者如果某人购买了一种产品，那么他/她可能也会对另一种产品感兴趣。</p><h1 id="9de5" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">GraphSage第2层节点嵌入可视化</h1><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi pr"><img src="../Images/9b4da0fde39fd9166acc18e3a552256c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BWO2T2sSMeQVE7FfSaq7CQ.png"/></div></div></figure><h2 id="68ff" class="nw jx iq bd jy nx ny dn kc nz oa dp kg lf ob oc kk lj od oe ko ln of og ks iw bi translated">观察</h2><p id="4e43" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在第2层，我们可以看到一些独立的产品类别集群正在形成，我们可以从中获得一些有价值的见解，例如电影和电视与CD和黑胶唱片、美容与健康和个人护理、视频游戏与玩具和游戏。然而，书和美簇相距甚远。</p><h1 id="64ee" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">GraphSage第三层节点嵌入可视化</h1><figure class="ng nh ni nj gt nk gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi ps"><img src="../Images/c083bcbfebe9b9c74039069e3485b561.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NknVNXxIu_Bhvo9S9WCwiA.png"/></div></div></figure><h2 id="2158" class="nw jx iq bd jy nx ny dn kc nz oa dp kg lf ob oc kk lj od oe ko ln of og ks iw bi translated">观察</h2><p id="f4a8" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在第3层，节点表示比第2层稍微精细一些，因为我们可以看到一些更远的集群，例如手机和配件与电子产品。</p><h1 id="364c" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">承认</h1><p id="6417" class="pw-post-body-paragraph ku kv iq kw b kx ky kz la lb lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我要感谢<a class="ae ls" href="https://www.arangodb.com/" rel="noopener ugc nofollow" target="_blank"> <strong class="kw ja"> ArangoDB </strong> </a>的整个ML团队为我提供了关于博客的宝贵反馈。</p><p id="f362" class="pw-post-body-paragraph ku kv iq kw b kx lt kz la lb lu ld le lf lv lh li lj lw ll lm ln lx lp lq lr ij bi translated">想联系我:<a class="ae ls" href="https://www.linkedin.com/in/sachin-sharma-4198061a9/?originalSubdomain=de" rel="noopener ugc nofollow" target="_blank"> <strong class="kw ja"> Linkedin </strong> </a></p><h1 id="4d6e" class="jw jx iq bd jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt bi translated">参考资料(更多学习材料)</h1><ol class=""><li id="f8f7" class="ly lz iq kw b kx ky lb lc lf pt lj pu ln pv lr pw me mf mg bi translated">fastgraphml:一个加速graphml模型开发过程的低代码框架</li><li id="50be" class="ly lz iq kw b kx mh lb mi lf mj lj mk ln ml lr pw me mf mg bi translated"><a class="ae ls" href="https://www.arangodb.com/2021/08/a-comprehensive-case-study-of-graphsage-using-pytorchgeometric/?utm_content=176620548&amp;utm_medium=social&amp;utm_source=linkedin&amp;hss_channel=lcp-5289249" rel="noopener ugc nofollow" target="_blank">https://www . arango db . com/2021/08/a-comprehensive-case-study-of-graphsage-using-pytorchgeometric/？UTM _ content = 176620548&amp;UTM _ medium = social&amp;UTM _ source = LinkedIn&amp;HSS _ channel = LCP-5289249</a>(<strong class="kw ja">原博文</strong>)</li><li id="1ab9" class="ly lz iq kw b kx mh lb mi lf mj lj mk ln ml lr pw me mf mg bi translated"><a class="ae ls" href="https://arxiv.org/pdf/1706.02216.pdf" rel="noopener ugc nofollow" target="_blank">大型图上的归纳表示学习</a></li><li id="be7c" class="ly lz iq kw b kx mh lb mi lf mj lj mk ln ml lr pw me mf mg bi translated"><a class="ae ls" href="http://web.stanford.edu/class/cs224w/slides/17-scalable.pdf" rel="noopener ugc nofollow" target="_blank">http://web.stanford.edu/class/cs224w/slides/17-scalable.pdf</a></li><li id="5fed" class="ly lz iq kw b kx mh lb mi lf mj lj mk ln ml lr pw me mf mg bi translated"><a class="ae ls" href="https://medium.com/arangodb/a-voyage-through-graph-machine-learning-universe-motivation-applications-datasets-graph-ml-e573a898b346" rel="noopener">图形机器学习宇宙之旅:动机、应用、数据集、图形ML库、图形数据库</a></li><li id="bd73" class="ly lz iq kw b kx mh lb mi lf mj lj mk ln ml lr pw me mf mg bi translated"><a class="ae ls" href="https://medium.com/pinterest-engineering/pinsage-a-new-graph-convolutional-neural-network-for-web-scale-recommender-systems-88795a107f48" rel="noopener">https://medium . com/Pinterest-engineering/pin sage-a-new-graph-convolutionary-neural-network-for-web-scale-recommender-systems-88795 a107f 48</a></li><li id="f657" class="ly lz iq kw b kx mh lb mi lf mj lj mk ln ml lr pw me mf mg bi translated"><a class="ae ls" href="https://eng.uber.com/uber-eats-graph-learning/" rel="noopener ugc nofollow" target="_blank">https://eng.uber.com/uber-eats-graph-learning/</a></li><li id="c5b9" class="ly lz iq kw b kx mh lb mi lf mj lj mk ln ml lr pw me mf mg bi translated">更多与<a class="ae ls" href="https://github.com/arangodb/interactive_tutorials" rel="noopener ugc nofollow" target="_blank">图ML </a>相关的东西</li></ol></div></div>    
</body>
</html>