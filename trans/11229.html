<html>
<head>
<title>Transformer based 3D Computer Vision Synthesis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于变压器的三维计算机视觉合成</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/transformer-based-3d-computer-vision-synthesis-eaa9677d931f?source=collection_archive---------13-----------------------#2021-11-03">https://towardsdatascience.com/transformer-based-3d-computer-vision-synthesis-eaa9677d931f?source=collection_archive---------13-----------------------#2021-11-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5f95" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用数据做很酷的事情</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7a84378ea9ca5e4e712ded98a04aaf49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nNsgCyId_u_FMGCto39AxA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1:<a class="ae ky" href="https://unsplash.com/@simonppt" rel="noopener ugc nofollow" target="_blank">西蒙·李</a>在<a class="ae ky" href="https://unsplash.com/photos/Ue97JK9S0QE" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="2736" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="ca07" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">3D计算机视觉是一种迷人的现场教学模型，可以像我们一样理解世界。近年来，2D计算机视觉在目标检测、分割、关键点估计等方面取得了巨大的进展。3D计算机视觉涉及理解几何形状和深度，并在许多领域中有应用，包括机器人、自动驾驶、3D跟踪、3D场景重建和AR/VR。</p><p id="12a0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">传统方法仍然非常受3D计算机视觉的欢迎，但深度学习已经取得了令人印象深刻的进展。在这篇博客中，我们将讨论表现3D物体和场景的不同方式。然后，我们将使用Transformer模型进行3D特征生成的实践代码。代码分享在Github <a class="ae ky" href="https://github.com/priya-dwivedi/Deep-Learning/tree/master/Transformer-based-3D" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><h1 id="bd88" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">3D信息的表示</h1><p id="1ac1" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">从摄像机捕获的图像本质上是2D的，并且在过去，多个摄像机被用于理解场景的几何形状。</p><p id="a34b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然而，使用多个相机和激光雷达来生成3D信息是一项昂贵的任务，也是3D计算机视觉更广泛使用的一个障碍。因此，焦点已经转移到深度学习方法上，这些方法可以从单个单目相机中提取关于场景的3D信息。</p><p id="fb9c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">让我们来看看3D信息的不同表现方式。常见的代表包括:</p><ol class=""><li id="26e1" class="ms mt it lt b lu mn lx mo ma mu me mv mi mw mm mx my mz na bi translated">深度图</li><li id="80d1" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">体素表示</li><li id="1c1f" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">点云</li><li id="a146" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">网格表示</li></ol><h2 id="7fed" class="ng la it bd lb nh ni dn lf nj nk dp lj ma nl nm ll me nn no ln mi np nq lp nr bi translated">深度图</h2><p id="1db3" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">深度图允许我们提取深度，并将其显示为一个额外的维度。返回的深度可以是绝对深度，也可以是相对深度。深度可以标准化为0到255的黑白色标，并如下图2所示进行可视化。这里，靠近相机的点用红橙色标度，远离相机的点用蓝色标度。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/7633444667b267099d32fc5c1db26ab7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GjHIEGWkI0hxNvE_iE00KQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2:使用DLT模型的深度估计。从本博客中的代码生成。作者照片</p></figure><p id="06dd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">有几个公共数据集包含图像和相应的深度信息。NYU深度v2 就是这样一个数据集。通常，这些数据集的深度信息是使用Kinect等传感器计算的。</p><h2 id="a965" class="ng la it bd lb nh ni dn lf nj nk dp lj ma nl nm ll me nn no ln mi np nq lp nr bi translated">使用DLT的深度估计</h2><p id="575b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><a class="ae ky" href="https://arxiv.org/abs/2103.13413" rel="noopener ugc nofollow" target="_blank"> DLT模型</a>将<a class="ae ky" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank">视觉转换器</a>扩展到密集预测任务。我是变形金刚模型的忠实粉丝，Vision Transformer是第一个流行的用于视觉任务的变形金刚实现。然而，Vision Transformer是一种仅支持编码器的型号，通常与分类头一起用于分类任务。</p><p id="0200" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">DLT的论文用解码器/融合器扩展了视觉变换器，该解码器/融合器在不同的头部对视觉变换器的输出进行采样，并融合这些表示以获得与输入图像具有相同形状的输出表示。该模型在深度估计和语义分割任务上进行训练，并在这两个方面显示了最先进的结果。</p><p id="f5c3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">DLT模型的模型架构如下图3所示。左侧显示了整体DLT架构，包括视觉转换器编码器模块，然后重组+融合为解码器模块。中间的面板展开了重新组装块，右边的面板解释了融合块。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/34714f4aaffdf510c45166a6cc66ff92.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DYjeJlW-xBx3MHdqRoa4SQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3:他们论文中的DLT模型。来源:<a class="ae ky" href="https://arxiv.org/pdf/2103.13413.pdf" rel="noopener ugc nofollow" target="_blank">链接</a></p></figure><p id="62ed" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我已经使用PyTorch hub 的<a class="ae ky" href="https://pytorch.org/hub/intelisl_midas_v2/" rel="noopener ugc nofollow" target="_blank"> DLT的实现来从单目图像生成深度估计。我写了一个</a><a class="ae ky" href="https://colab.research.google.com/drive/1a3kuY6TeH-TtIiYiTfMTGeNxPPmUlcAP?authuser=2#scrollTo=qd_sOLySBetD" rel="noopener ugc nofollow" target="_blank"> colab笔记本</a>，用于加载DLT模型，并使用样本图像对其进行推理。该代码也可以在Github <a class="ae ky" href="https://github.com/priya-dwivedi/Deep-Learning/tree/master/Transformer-based-3D" rel="noopener ugc nofollow" target="_blank">这里</a>获得。</p><p id="6dd1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">运行DLT模型的管道如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/67a72983989f7a1ef18155f8f97a41c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g1P1tRxl0k7p5_DaTyIqUw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4:来自Colab笔记本的DLT模型代码。作者照片</p></figure><p id="41b3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">深度图很容易计算并且相当精确，但是深度本身并不能提供场景的完整3D表示，因为深度不能从被遮挡的对象中计算出来。例如，上面图1中蒲团珠后面的床被遮挡，因此我们没有对它的深度估计。</p><p id="d002" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">其他表示能够提供物体的完整3D模型。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/97985e17f1858874d509dfd7cbe0e97b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1110/format:webp/1*PSNM1VguNtd_ANUmZXXVNA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图5:点云(左)、体素(中)和网格(右)的3D表示。来源:<a class="ae ky" href="https://www.mdpi.com/2079-9292/8/10/1196/htm" rel="noopener ugc nofollow" target="_blank">链接</a></p></figure><p id="71a0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">3D对象的三种最常见的表示是体素、网格和点云。我真的很喜欢下面的图5，它将一幅2D图像转换成了各种3D图像。</p><h2 id="fe8b" class="ng la it bd lb nh ni dn lf nj nk dp lj ma nl nm ll me nn no ln mi np nq lp nr bi translated">点云表示</h2><p id="faed" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">点云表示涉及将3D空间表示为点的集合，每个点具有3D位置。</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="d300" class="ng la it nx b gy ob oc l od oe">point_cloud = [(x1, y1, z1), (x2, y2, z2),..., (xn, yn, zn)]</span></pre><p id="6b5b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">物体的点云表示非常流行。它比深度估计更好地保留了输入形状。如果物体的RGBD信息是可用的，那么如果相机参数是已知的，它可以用于估计物体的点云。相机参数通常使用称为相机校准的过程来估计。</p><p id="184f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在我们关于<a class="ae ky" href="https://colab.research.google.com/drive/1a3kuY6TeH-TtIiYiTfMTGeNxPPmUlcAP?authuser=2#scrollTo=qd_sOLySBetD" rel="noopener ugc nofollow" target="_blank"> Colab </a>和<a class="ae ky" href="https://github.com/priya-dwivedi/Deep-Learning/tree/master/Transformer-based-3D" rel="noopener ugc nofollow" target="_blank"> Github </a>的代码中，我们使用<a class="ae ky" href="https://kornia.readthedocs.io/en/latest/get-started/introduction.html" rel="noopener ugc nofollow" target="_blank"> Kornia </a>使用来自DLT的RGBD信息计算3D点云。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/d6699182e71c313cf9d8c7cc0181e8a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*THPMLLgCyAYPo4dECT9ZHQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图6:从深度图中提取三维点云的代码。作者照片</p></figure><p id="9acd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们使用depth_to_3d函数将深度图转换为点云。对于尺寸为416x416的图像，点云是形状为416x416x3的NumPy数组。</p><p id="dccf" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然后，我们可以使用<a class="ae ky" href="http://www.open3d.org/" rel="noopener ugc nofollow" target="_blank"> Open3D </a>库将3D点云转换为ply格式文件，并可视化生成的点云。<a class="ae ky" href="https://colab.research.google.com/drive/1a3kuY6TeH-TtIiYiTfMTGeNxPPmUlcAP?authuser=2#scrollTo=qd_sOLySBetD" rel="noopener ugc nofollow" target="_blank"> colab </a>笔记本有运行这个可视化的代码，但是，colab不能打开交互式可视化器。我能够在我的本地机器上运行jupyter笔记本的可视化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/779f6a64ec59767c226a1c4f1065f872.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*MTdYyYYeK0Njg8w4eTfWlQ.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图7:来自Open3D的3D点云甲龙。从本博客中的代码生成。作者照片</p></figure><p id="324e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">从我们的深度图生成的椅子点云如下所示。请使用open3d库可视化椅子点云</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/2de99f11503574304a7cc2a4921ae60e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*2sSEUwhowd_Lb_jK_tSvDg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图8:来自Open3D的椅子的3D点云。从本博客中的代码生成。作者照片</p></figure><h2 id="2c8d" class="ng la it bd lb nh ni dn lf nj nk dp lj ma nl nm ll me nn no ln mi np nq lp nr bi translated">体素表示</h2><p id="69f5" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">体素表示将对象表示为许多3D像素或立方体的组合。体素表示非常类似于真实世界的表示，在真实世界中，物体由微小的部分——原子/分子——组成。如果增加体素网格的分辨率，你会越来越接近真实世界的物体，如下图6所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/70e09766b91625fa8eb73261dc1eb376.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*UYh0Q_je3rualPnuJfzGPg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图9:椅子的体素表示。<a class="ae ky" href="https://www.researchgate.net/figure/Three-expressions-of-3D-data-a-Multi-view-b-point-cloud-and-c-voxels_fig2_338614048" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="4f05" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">基于体素的方法的主要缺点是在高分辨率体素上渲染和运行深度学习模型所需的计算。</p><h2 id="d369" class="ng la it bd lb nh ni dn lf nj nk dp lj ma nl nm ll me nn no ln mi np nq lp nr bi translated">网格表示</h2><p id="2ff3" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">另一种常见的3D表示是网格表示。如图6所示，该网格表示将对象显示为连接在一起的面和顶点。</p><p id="acf8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">来自DeepMind的Polygen model 使用变形器来生成代表3D对象的网格和面。它使用2D图像或体素作为条件输入，为3D对象生成一组可信的顶点。然后，它通过合理的方式连接顶点来生成面。</p><p id="a0a5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">顶点和面模型都是自回归解码器，其中下一个顶点/面是基于在该点之前预测的所有其他东西来预测的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/bbd0edf25933dad08f92ba7864a22af3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1026/format:webp/1*Q5WUEandRFZSr6hVKpWhGQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图10:解释的多边形模型。来源:<a class="ae ky" href="https://arxiv.org/pdf/2002.10880.pdf" rel="noopener ugc nofollow" target="_blank">宝丽金论文</a></p></figure><p id="d968" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">回购协议的作者已经发布了预先训练的模型和相同的Colab代码是<a class="ae ky" href="https://colab.research.google.com/github/deepmind/deepmind-research/blob/master/polygen/sample-pretrained.ipynb#scrollTo=FU3yBmWWebbk" rel="noopener ugc nofollow" target="_blank">在这里</a>。我测试了代码，为两个常见的对象生成网格——桌子和鸟舍。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/8865d5e57ff28f14c334b4676dcb3876.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hXWpbqJIsYVI13_IJzyogA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图11。多边形模型生成的表格网格样本。来源:本博客中的代码。作者照片</p></figure><h1 id="312f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="a759" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这个博客展示了变形金刚模型是如何被用来生成我们世界的3D表现的。我们研究了如何使用变形金刚来生成室内场景的深度图。然后，我们将深度图转换为3D点云，并使用Open3D可视化这些点云。我们还查看了来自Deep Mind的多边形模型，以生成常见对象的网格。</p><p id="a296" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我希望您尝试一下代码，并生成自己的常见场景的3D可视化。请在下面的评论中分享你的经历。</p><p id="d6e8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在<a class="ae ky" href="https://deeplearninganalytics.org/" rel="noopener ugc nofollow" target="_blank">深度学习分析</a>，我们非常热衷于使用机器学习来解决现实世界的问题。我们已经帮助许多企业部署了创新的基于人工智能的解决方案。通过我们的网站<a class="ae ky" href="https://deeplearninganalytics.org/contact-us/" rel="noopener ugc nofollow" target="_blank">这里</a>联系我们，或者如果你看到合作的机会，给我发电子邮件到priya@deeplearninganalytics.org。</p><p id="2ec4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">参考</p><ol class=""><li id="6c08" class="ms mt it lt b lu mn lx mo ma mu me mv mi mw mm mx my mz na bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/generating-3d-models-with-polygen-and-pytorch-4895f3f61a2e"> Polygen模型解释</a></li><li id="c5f2" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated"><a class="ae ky" href="https://arxiv.org/pdf/2103.13413.pdf" rel="noopener ugc nofollow" target="_blank">用于密集预测的视觉转换器</a></li><li id="1503" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated">关于<a class="ae ky" href="https://www.3dflow.net/elementsCV/S4.xhtml" rel="noopener ugc nofollow" target="_blank">立体几何的细节</a></li><li id="6826" class="ms mt it lt b lu nb lx nc ma nd me ne mi nf mm mx my mz na bi translated"><a class="ae ky" href="https://github.com/kornia/kornia" rel="noopener ugc nofollow" target="_blank">Kornia</a>3D点云估算库</li></ol></div></div>    
</body>
</html>