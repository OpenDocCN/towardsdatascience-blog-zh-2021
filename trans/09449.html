<html>
<head>
<title>You Are Missing Out on LightGBM. It Crushes XGBoost in Every Aspect</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你错过了LightGBM。它在各个方面都击败了XGBoost</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-beat-the-heck-out-of-xgboost-with-lightgbm-comprehensive-tutorial-5eba52195997?source=collection_archive---------1-----------------------#2021-09-02">https://towardsdatascience.com/how-to-beat-the-heck-out-of-xgboost-with-lightgbm-comprehensive-tutorial-5eba52195997?source=collection_archive---------1-----------------------#2021-09-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="02bd" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">再也不会了，XGBoost，再也不会了</h2></div><p id="1cae" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这个全面的LightGBM教程中学习如何碾压XGBoost。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/f3b304ef34688583e508d5334bce8c13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cpq9uZdaapAKu2GmckskuA.jpeg"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated"><strong class="bd lu">照片由</strong> <a class="ae lv" href="https://unsplash.com/@grstocks?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> <strong class="bd lu"> GR股票</strong> </a> <strong class="bd lu">上</strong> <a class="ae lv" href="https://unsplash.com/s/photos/win?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> <strong class="bd lu"> Unsplash。</strong> </a> <strong class="bd lu">除特别注明外，所有图片均为作者所有。</strong></p></figure><p id="0f8c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我很困惑。</p><p id="7fbd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如此多的人被XGBoost吸引，就像飞蛾扑火一样。是的，它在著名的比赛中经历了一些辉煌的日子，并且它仍然是最广泛使用的ML库。</p><p id="fca3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是，就性能而言，XGBoost已经4年没有占据头把交椅了。2017年，微软开源了LightGBM(光梯度增强机器)，它以2-10倍的训练速度提供了同样高的精度。</p><p id="2256" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">考虑到大规模百万行数据集的普遍存在，这是一个改变游戏规则的优势。还有其他一些区别使天平向LightGBM倾斜，使它比XGBoost更有优势。</p><p id="4a3b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文结束时，您将了解到这些优势，包括:</p><ul class=""><li id="12f2" class="lx ly it kk b kl km ko kp kr lz kv ma kz mb ld mc md me mf bi translated">如何为分类和回归任务开发LightGBM模型</li><li id="ae13" class="lx ly it kk b kl mg ko mh kr mi kv mj kz mk ld mc md me mf bi translated">XGBoost和LGBM之间的结构差异</li><li id="0cac" class="lx ly it kk b kl mg ko mh kr mi kv mj kz mk ld mc md me mf bi translated">如何使用提前停止和评估集</li><li id="3ea4" class="lx ly it kk b kl mg ko mh kr mi kv mj kz mk ld mc md me mf bi translated">支持强大的分类功能，速度提升高达8倍</li><li id="a72d" class="lx ly it kk b kl mg ko mh kr mi kv mj kz mk ld mc md me mf bi translated">使用LGBM实现成功的交叉验证</li><li id="ff77" class="lx ly it kk b kl mg ko mh kr mi kv mj kz mk ld mc md me mf bi translated">使用Optuna进行超参数调谐(第二部分)</li></ul><div class="ml mm gp gr mn mo"><a href="https://ibexorigin.medium.com/membership" rel="noopener follow" target="_blank"><div class="mp ab fo"><div class="mq ab mr cl cj ms"><h2 class="bd iu gy z fp mt fr fs mu fu fw is bi translated">通过我的推荐链接加入Medium-BEXGBoost</h2><div class="mv l"><h3 class="bd b gy z fp mt fr fs mu fu fw dk translated">获得独家访问我的所有⚡premium⚡内容和所有媒体没有限制。支持我的工作，给我买一个…</h3></div><div class="mw l"><p class="bd b dl z fp mt fr fs mu fu fw dk translated">ibexorigin.medium.com</p></div></div><div class="mx l"><div class="my l mz na nb mx nc lo mo"/></div></div></a></div><p id="f9a6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">获得由强大的AI-Alpha信号选择和总结的最佳和最新的ML和AI论文:</p><div class="ml mm gp gr mn mo"><a href="https://alphasignal.ai/?referrer=Bex" rel="noopener  ugc nofollow" target="_blank"><div class="mp ab fo"><div class="mq ab mr cl cj ms"><h2 class="bd iu gy z fp mt fr fs mu fu fw is bi translated">阿尔法信号|机器学习的极品。艾总结的。</h2><div class="mv l"><h3 class="bd b gy z fp mt fr fs mu fu fw dk translated">留在循环中，不用花无数时间浏览下一个突破；我们的算法识别…</h3></div><div class="mw l"><p class="bd b dl z fp mt fr fs mu fu fw dk translated">alphasignal.ai</p></div></div><div class="mx l"><div class="nd l mz na nb mx nc lo mo"/></div></div></a></div></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="7c29" class="nl nm it bd nn no np nq nr ns nt nu nv jz nw ka nx kc ny kd nz kf oa kg ob oc bi translated">XGBoost与LightGBM</h1><p id="23f8" class="pw-post-body-paragraph ki kj it kk b kl od ju kn ko oe jx kq kr of kt ku kv og kx ky kz oh lb lc ld im bi translated">当LGBM发布时，它在生成决策树的方式上有了突破性的改变。</p><blockquote class="oi oj ok"><p id="91fc" class="ki kj lw kk b kl km ju kn ko kp jx kq ol ks kt ku om kw kx ky on la lb lc ld im bi translated">XGBoost和LightGBM都是ensebmle算法。他们使用一种特殊类型的决策树，也称为弱学习器，来捕捉复杂的非线性模式。</p></blockquote><p id="41b4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在XGBoost(和许多其他库)中，决策树是一次构建一层的:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/c0f2d7adc898794eadf0b5548e2eca2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/0*LJdDYev_aB_u4Wdn.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">图片来自LGBM文档</p></figure><p id="767a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种类型的结构往往会导致不必要的节点和叶子，因为树会继续构建，直到到达<code class="fe op oq or os b">max_depth</code>。这导致了更高的模型复杂性和运行时培训成本。</p><p id="3370" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">相比之下，LightGBM采用了一种基于叶子的方法:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ot"><img src="../Images/882666cdec18adfdba34eed6650d4d4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*E_j4d719Dgh97qWI.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">图片来自LGBM文档</p></figure><p id="81cc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该结构随着最有希望的分支和叶子(具有最大delta损失的节点)继续增长，保持决策叶子的数量不变。(如果这对你来说没有意义，不要多心。这不会妨碍你有效使用LGBM)。</p><p id="5864" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这也是LGBM刚出来的时候在速度上碾压XGBoost的主要原因之一。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="ab gu cl ou"><img src="../Images/4aba93262fd8119a52ea667fd1ad0e72.png" data-original-src="https://miro.medium.com/v2/format:webp/1*NgSNunWiZi9UT4xHCwcJYA.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">图片来自LGBM文档</p></figure><p id="7a0a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面是XGBoost与传统决策树和LGBM与叶式结构(第一列和最后一列)在大约500k-13M样本数据集上的基准比较。说明LGBM比XGB快几个数量级。</p><p id="3841" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">LGBM还使用连续特征的h <a class="ae lv" href="https://lightgbm.readthedocs.io/en/latest/Features.html#optimization-in-speed-and-memory-usage" rel="noopener ugc nofollow" target="_blank"> istogram宁滨</a>，这比传统的梯度提升提供了更高的速度。宁滨数值大大减少了决策树中需要考虑的分裂点的数量，并且它们消除了使用排序算法的需要，排序算法总是计算量很大。</p><p id="cf42" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">受LGBM的启发，XGBoost还引入了直方图宁滨，这带来了巨大的加速，但仍不足以与LGBM相媲美:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="ab gu cl ou"><img src="../Images/dc1dde65caa5768506aaf8ed0af26b5c.png" data-original-src="https://miro.medium.com/v2/format:webp/1*OfCY8lkNw-FjN6Z196g5sg.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">来自LGBM文档的图像<strong class="bd lu">直方图-宁滨比较-第二列和第三列。</strong></p></figure><p id="351c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将在接下来的章节中继续探讨这些差异。</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="bc63" class="nl nm it bd nn no np nq nr ns nt nu nv jz nw ka nx kc ny kd nz kf oa kg ob oc bi translated">模型初始化和目标</h1><p id="950f" class="pw-post-body-paragraph ki kj it kk b kl od ju kn ko oe jx kq kr of kt ku kv og kx ky kz oh lb lc ld im bi translated">和XGBoost一样，LGBM也有两个API——核心学习API和Sklearn兼容API。你知道我是Sklearn的忠实粉丝，所以本教程将重点介绍那个版本。</p><blockquote class="oi oj ok"><p id="2dac" class="ki kj lw kk b kl km ju kn ko kp jx kq ol ks kt ku om kw kx ky on la lb lc ld im bi translated">XGBoost和LGBM的Sklearn兼容API允许您将它们的模型集成到Sklearn生态系统中，以便您可以在管道中结合其他变压器使用它们。</p></blockquote><p id="ebae" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Sklearn API使用熟悉的<code class="fe op oq or os b">fit/predict/predict_proba</code>模式公开<code class="fe op oq or os b">LGBMRegressor</code>和<code class="fe op oq or os b">LGBMClassifier</code>:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="ov ow l"/></div></figure><p id="9d66" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe op oq or os b">objective</code>指定学习任务的类型。除了常见的<code class="fe op oq or os b">binary</code>、<code class="fe op oq or os b">multiclass</code>、<code class="fe op oq or os b">regression</code>任务，还有其他的<code class="fe op oq or os b">poisson</code>、<code class="fe op oq or os b">tweedie</code>回归。完整的目标列表请参见文档的本节。</p><pre class="lf lg lh li gt ox os oy oz aw pa bi"><span id="3910" class="pb nm it os b gy pc pd l pe pf">&gt;&gt;&gt; reg.fit(X, y)</span><span id="688f" class="pb nm it os b gy pg pd l pe pf">LGBMRegressor()</span></pre></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="eb74" class="nl nm it bd nn no np nq nr ns nt nu nv jz nw ka nx kc ny kd nz kf oa kg ob oc bi translated">控制决策树的数量</h1><p id="7cb8" class="pw-post-body-paragraph ki kj it kk b kl od ju kn ko oe jx kq kr of kt ku kv og kx ky kz oh lb lc ld im bi translated">集成中决策树的数量会显著影响结果。您可以在分类器和回归器中使用<code class="fe op oq or os b">n_estimators</code>参数来控制它。下面，我们将在具有1000个决策树的Kaggle TPS March数据集上拟合LGBM二元分类器:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="ov ow l"/></div></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="ab gu cl ou"><img src="../Images/d11781a1225fe0aaf5dd762478376c59.png" data-original-src="https://miro.medium.com/v2/format:webp/1*dnYtp4uN5oSxZz4J6meldw.png"/></div></figure><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="ov ow l"/></div></figure><p id="90ec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">添加更多的树会导致更高的精度，但会增加过度拟合的风险。为了解决这个问题，你可以创建许多树(+2000)并选择一个更小的<code class="fe op oq or os b">learning_rate</code>(稍后会详细介绍)。</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="ov ow l"/></div></figure><p id="26a9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">像在XGBoost中一样，将单个决策树与数据相匹配被称为<strong class="kk iu">提升回合</strong>。</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="2f45" class="nl nm it bd nn no np nq nr ns nt nu nv jz nw ka nx kc ny kd nz kf oa kg ob oc bi translated">提前停止</h1><p id="e387" class="pw-post-body-paragraph ki kj it kk b kl od ju kn ko oe jx kq kr of kt ku kv og kx ky kz oh lb lc ld im bi translated">集合中的每一棵树都建立在最后一棵树的预测上，即每一轮提升都是上一轮的改进。</p><p id="951f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果预测在一系列回合后没有改善，那么停止集合的训练是明智的，即使我们没有在<code class="fe op oq or os b">n_estimators</code>中硬停下来。</p><p id="b584" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为此，LGBM在<code class="fe op oq or os b">fit</code>函数中提供了<code class="fe op oq or os b">early_stopping_rounds</code>参数。例如，将其设置为100意味着如果预测在最后100轮中没有改善，我们就停止训练。</p><p id="c234" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在查看代码示例之前，我们应该学习一些与早期停止相关的概念。</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="4770" class="nl nm it bd nn no np nq nr ns nt nu nv jz nw ka nx kc ny kd nz kf oa kg ob oc bi translated">评估集和指标</h1><p id="65b7" class="pw-post-body-paragraph ki kj it kk b kl od ju kn ko oe jx kq kr of kt ku kv og kx ky kz oh lb lc ld im bi translated">只有当您将一组评估集传递给<code class="fe op oq or os b">fit</code>方法的<code class="fe op oq or os b">eval_set</code>参数时，才能启用提前停止。这些评估集用于跟踪从一轮提升到下一轮提升的预测质量:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="ov ow l"/></div></figure><p id="da73" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在每一轮<code class="fe op oq or os b">n_estimators</code>中，单个决策树被拟合到(<code class="fe op oq or os b">X_train</code>，<code class="fe op oq or os b">y_train</code>)，并且在通过的评估集上进行预测(<code class="fe op oq or os b">X_eval</code>，<code class="fe op oq or os b">y_eval</code>)。预测的质量通过<code class="fe op oq or os b">eval_metric</code>中的合格指标来衡量。</p><p id="c553" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">训练在第738次迭代时停止，因为自第638次迭代以来验证分数没有提高——提前停止100轮。现在，我们可以随心所欲地创建任意多的树，并且可以丢弃不必要的树。</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="5b31" class="nl nm it bd nn no np nq nr ns nt nu nv jz nw ka nx kc ny kd nz kf oa kg ob oc bi translated">建立基线</h1><p id="5172" class="pw-post-body-paragraph ki kj it kk b kl od ju kn ko oe jx kq kr of kt ku kv og kx ky kz oh lb lc ld im bi translated">让我们用目前已知的信息建立一个基准分数。我们将对XGBoost进行同样的操作，以便我们可以比较结果:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="ov ow l"/></div></figure><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="ov ow l"/></div></figure><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="ov ow l"/></div></figure><p id="e928" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">LGBM在大约4倍的运行时间内实现了较小的损失。在我们继续交叉验证之前，让我们看看最后一个LGBM技巧。</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="db22" class="nl nm it bd nn no np nq nr ns nt nu nv jz nw ka nx kc ny kd nz kf oa kg ob oc bi translated">分类和缺失值支持</h1><p id="a7e2" class="pw-post-body-paragraph ki kj it kk b kl od ju kn ko oe jx kq kr of kt ku kv og kx ky kz oh lb lc ld im bi translated">LGBM中的直方图宁滨内置了对缺失值和分类特征的处理支持。TPS March数据集包含19个类别，到目前为止，我们一直在使用一次性编码。</p><p id="7403" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这一次，我们将让LGBM处理类别，并再次将结果与XGBoost进行比较:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="ov ow l"/></div></figure><p id="100b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要指定分类特性，请将它们的索引列表传递给<code class="fe op oq or os b">fit</code>方法中的<code class="fe op oq or os b">categorical_feature</code>参数:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="ov ow l"/></div></figure><blockquote class="oi oj ok"><p id="18af" class="ki kj lw kk b kl km ju kn ko kp jx kq ol ks kt ku om kw kx ky on la lb lc ld im bi translated">如果在使用LGBM时使用<code class="fe op oq or os b">pandas.Categorical</code>数据类型，可以实现高达8倍的速度提升。</p></blockquote><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="ab gu cl ou"><img src="../Images/9d79dfeacb28751eb4b6f3d743bf62d6.png" data-original-src="https://miro.medium.com/v2/format:webp/1*lndK461TgekvKDuSGtUBqA.png"/></div></figure><p id="3db8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该表显示了两个模型的最终得分和运行时间。如您所见，使用默认分类处理的版本在准确性和速度上都胜过了其他版本。干杯！</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="634f" class="nl nm it bd nn no np nq nr ns nt nu nv jz nw ka nx kc ny kd nz kf oa kg ob oc bi translated">使用LightGBM进行交叉验证</h1><p id="09c5" class="pw-post-body-paragraph ki kj it kk b kl od ju kn ko oe jx kq kr of kt ku kv og kx ky kz oh lb lc ld im bi translated">用LGBM做CV最常见的方法是使用Sklearn CV分割器。</p><p id="49aa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我不是在谈论像<code class="fe op oq or os b">cross_validate</code>或<code class="fe op oq or os b">cross_val_score</code>这样的效用函数，而是像<code class="fe op oq or os b">KFold</code>或<code class="fe op oq or os b">StratifiedKFold</code>这样的分裂器和它们的<code class="fe op oq or os b">split</code>方法。这样做CV，让你对整个过程有更多的掌控。</p><blockquote class="oi oj ok"><p id="296c" class="ki kj lw kk b kl km ju kn ko kp jx kq ol ks kt ku om kw kx ky on la lb lc ld im bi translated">我已经多次谈到交叉验证的重要性。你可以阅读<a class="ae lv" rel="noopener" target="_blank" href="/6-sklearn-mistakes-that-silently-tell-you-are-a-rookie-84fa55f2b9dd?source=your_stories_page-------------------------------------">这篇文章</a>了解更多详情。</p></blockquote><p id="6207" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此外，它使您能够在交叉验证期间以一种轻松的方式使用早期停止。下面是TPS三月份数据的样子:</p><figure class="lf lg lh li gt lj"><div class="bz fp l di"><div class="ov ow l"/></div></figure><p id="b4c6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">首先，创建一个CV分解器——我们选择<code class="fe op oq or os b">StratifiedKFold</code>,因为这是一个分类问题。然后，使用<code class="fe op oq or os b">split</code>循环每个训练/测试组。在每一次折叠中，初始化并训练一个新的LGBM模型，并可选地报告分数和运行时间。就是这样！大多数人都是这样做CV的，包括在Kaggle上。</p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="5063" class="nl nm it bd nn no np nq nr ns nt nu nv jz nw ka nx kc ny kd nz kf oa kg ob oc bi translated">结论</h1><p id="b844" class="pw-post-body-paragraph ki kj it kk b kl od ju kn ko oe jx kq kr of kt ku kv og kx ky kz oh lb lc ld im bi translated">在这篇文章中，我们学习了LightGBM的纯建模技术。接下来，我们将探索如何使用Optuna充分利用LGBM模型的性能。</p><p id="1358" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">具体来说，本文的第二部分将包括最重要的LGBM超参数的详细概述，并介绍一个经过良好测试的超参数调优工作流程。它已经出来了—在这里阅读它<a class="ae lv" rel="noopener" target="_blank" href="/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5?source=your_stories_page-------------------------------------">。</a></p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ph"><img src="../Images/f91b8a21f12d3c3b7d428e0f8d5f48be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*KeMS7gxVGsgx8KC36rSTcg.gif"/></div></div></figure></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="2508" class="nl nm it bd nn no np nq nr ns nt nu nv jz nw ka nx kc ny kd nz kf oa kg ob oc bi translated">您可能也会感兴趣…</h1><figure class="lf lg lh li gt lj gh gi paragraph-image"><a href="https://towardsdatascience.com/25-numpy-functions-you-never-knew-existed-p-guarantee-0-85-64616ba92fa8"><div class="gh gi pi"><img src="../Images/d1f61af83178ef33ca100733ae58308c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RUpe8vQVwFgcA3sU7w-xmw.png"/></div></a></figure><figure class="lf lg lh li gt lj gh gi paragraph-image"><a href="https://towardsdatascience.com/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5?source=your_stories_page-------------------------------------"><div class="gh gi pj"><img src="../Images/9ec4cc046e31e09d7f8d3dd2b579cc2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IZ7m0LQQf3igUyUPTeFGIA.png"/></div></a></figure><div class="ml mm gp gr mn mo"><a rel="noopener follow" target="_blank" href="/tired-of-cliché-datasets-here-are-18-awesome-alternatives-from-all-domains-196913161ec9"><div class="mp ab fo"><div class="mq ab mr cl cj ms"><h2 class="bd iu gy z fp mt fr fs mu fu fw is bi translated">厌倦了陈词滥调的数据集？以下是来自所有领域的18个令人敬畏的选择</h2><div class="mv l"><h3 class="bd b gy z fp mt fr fs mu fu fw dk translated">编辑描述</h3></div><div class="mw l"><p class="bd b dl z fp mt fr fs mu fu fw dk translated">towardsdatascience.com</p></div></div><div class="mx l"><div class="pk l mz na nb mx nc lo mo"/></div></div></a></div><div class="ml mm gp gr mn mo"><a rel="noopener follow" target="_blank" href="/love-3blue1brown-animations-learn-how-to-create-your-own-in-python-in-10-minutes-8e0430cf3a6d"><div class="mp ab fo"><div class="mq ab mr cl cj ms"><h2 class="bd iu gy z fp mt fr fs mu fu fw is bi translated">喜欢3Blue1Brown动画？了解如何在10分钟内用Python创建自己的</h2><div class="mv l"><h3 class="bd b gy z fp mt fr fs mu fu fw dk translated">编辑描述</h3></div><div class="mw l"><p class="bd b dl z fp mt fr fs mu fu fw dk translated">towardsdatascience.com</p></div></div><div class="mx l"><div class="pl l mz na nb mx nc lo mo"/></div></div></a></div><div class="ml mm gp gr mn mo"><a rel="noopener follow" target="_blank" href="/7-cool-python-packages-kagglers-are-using-without-telling-you-e83298781cf4"><div class="mp ab fo"><div class="mq ab mr cl cj ms"><h2 class="bd iu gy z fp mt fr fs mu fu fw is bi translated">Kagglers正在使用的7个很酷的Python包</h2><div class="mv l"><h3 class="bd b gy z fp mt fr fs mu fu fw dk translated">编辑描述</h3></div><div class="mw l"><p class="bd b dl z fp mt fr fs mu fu fw dk translated">towardsdatascience.com</p></div></div><div class="mx l"><div class="pm l mz na nb mx nc lo mo"/></div></div></a></div></div></div>    
</body>
</html>