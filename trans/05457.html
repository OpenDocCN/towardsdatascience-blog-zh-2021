<html>
<head>
<title>Exploratory Data Analysis of Text data Including Visualization and Sentiment Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">文本数据的探索性数据分析，包括可视化和情感分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/exploratory-data-analysis-of-text-data-including-visualization-and-sentiment-analysis-e46dda3dd260?source=collection_archive---------17-----------------------#2021-05-14">https://towardsdatascience.com/exploratory-data-analysis-of-text-data-including-visualization-and-sentiment-analysis-e46dda3dd260?source=collection_archive---------17-----------------------#2021-05-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/dbca545459f88cea5abc6a73269e0809.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*yfnpe1_gUgUHw5T4"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">柴坦尼亚·皮拉拉在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class=""/><div class=""><h2 id="208b" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">文本数据的预处理、分析、可视化和情感分析</h2></div><p id="9820" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">文本数据分析一天比一天简单。Python和R等著名的编程语言都有很好的文本数据分析库。曾经有一段时间，人们认为你需要成为编码专家来完成这些复杂的任务。但是随着库版本的不断开发和改进，只需要简单的初级编码知识就可以更容易地执行文本数据分析。</p><p id="0702" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我将使用主要是文本数据集的数据集。该数据集包含亚马逊婴儿产品和评级的客户评论。请随意从<a class="ae jg" href="https://www.kaggle.com/sameersmahajan/reviews-of-amazon-baby-products" rel="noopener ugc nofollow" target="_blank">下载数据集和此链接</a>并跟随。这是一个用于机器学习的很好的数据集，因为它有评论评级。但是本文将只关注探索性的数据分析。我们仍然会讨论情感，但这不是情感分析机器学习。</p><p id="82a3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们跳进来吧！</p><h2 id="1224" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">资料组</h2><p id="1dcd" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">首先，导入数据集，然后我们将进一步讨论它。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="1b58" class="lu lv jj mx b gy nb nc l nd ne">import pandas as pd<br/>import numpy as np</span><span id="d2f3" class="lu lv jj mx b gy nf nc l nd ne">df = pd.read_csv("amazon_baby.csv")<br/>df.head()</span></pre><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/c346ce7ae34a88b2454015635f3bda3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1270/format:webp/1*igYv6a3t_8Vd47UPDkDshQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="0835" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如您所见，我们在数据集中有产品名称、客户评论和评级。我喜欢从检查数据集中有多少行数据开始:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="5eab" class="lu lv jj mx b gy nb nc l nd ne">len(df)</span></pre><p id="1648" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="db63" class="lu lv jj mx b gy nb nc l nd ne">183531</span></pre><p id="3386" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是大量的数据。我决定缩小数据集。因为当我试图处理这个大数据集时，需要花费太多时间来完成一些可视化、计算和数据操作。为了使运行代码更容易、更省时，我决定缩小数据集。一种方法是简单地从数据集中取出5000到10000或任何你想要的数字。</p><p id="7013" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但在此之前，让我们检查每个产品的评论的价值计数。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="4532" class="lu lv jj mx b gy nb nc l nd ne">df['name'].value_counts()</span></pre><p id="81e1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="d01c" class="lu lv jj mx b gy nb nc l nd ne">Vulli Sophie the Giraffe Teether                                            785<br/>Simple Wishes Hands-Free Breastpump Bra, Pink, XS-L                         562<br/>Infant Optics DXR-5 2.4 GHz Digital Video Baby Monitor with Night Vision    561<br/>Baby Einstein Take Along Tunes                                              547<br/>Cloud b Twilight Constellation Night Light, Turtle                          520<br/>                                                                           ... <br/>Mud Pie Baby Stroller Bear Buddy (Set of 4)                                   1<br/>Baby Mod Modena 3 in 1 Convertible Crib, White                                1<br/>Britax Kick Mats                                                              1<br/>Camouflage Camo print Cloth Diaper                                            1<br/>Baby Gear Blue Bear with Dots and Circles Security Blanket Lovey              1<br/>Name: name, Length: 32415, dtype: int64</span></pre><p id="d201" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您可以从输出中看到，有些产品只有一个评论。仅仅通过一次回顾是不可能得出任何结论的。在这里，我选择了至少有20条评论的产品。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="1f28" class="lu lv jj mx b gy nb nc l nd ne">df = df[df.groupby("name")["name"].transform('size') &gt; 20]</span></pre><p id="8439" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我再次检查数据集的长度，发现我们现在有89691个数据。那也很多。这次演示我只需要10000个数据。如果你不担心运行时间，或者如果你有一个更高容量的计算机，请随意使用所有的数据。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="ca06" class="lu lv jj mx b gy nb nc l nd ne">df= df.head(10000)</span></pre><p id="e08c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我有一个包含10000个数据的数据集。</p><h2 id="50c2" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">预处理</h2><p id="5bf4" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">在进入分析之前，一些文本清理和处理是必要的。预处理和数据清理是数据分析的一大部分。首先，我将把“review”列设为字符串格式。它看起来像字符串。但是万一有一些数据不是字符串格式的，我会简单地将整列转换为字符串。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="52a8" class="lu lv jj mx b gy nb nc l nd ne">df['review'] = df['review'].astype(str)</span></pre><p id="7fd7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">评论是这个数据集中的主要信息。如果任何一行遗漏了评审，我们就不需要那一行。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="7e9b" class="lu lv jj mx b gy nb nc l nd ne">df = df[~df["review"].isnull()]</span></pre><p id="d1b1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我还有10000个数据。这意味着review列中没有空值。</p><blockquote class="nh ni nj"><p id="43a2" class="ky kz nk la b lb lc kk ld le lf kn lg nl li lj lk nm lm ln lo nn lq lr ls lt im bi translated">删除特殊字符</p></blockquote><p id="0ab1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">评论可能包含许多对任何分析都没有帮助的特殊字符。一开始就清洗它们是很好的。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="9b5c" class="lu lv jj mx b gy nb nc l nd ne">def clean(txt):<br/>    txt = txt.str.replace("(&lt;br/&gt;)", "")<br/>    txt = txt.str.replace('(&lt;a).*(&gt;).*(&lt;/a&gt;)', '')<br/>    txt = txt.str.replace('(&amp;amp)', '')<br/>    txt = txt.str.replace('(&amp;gt)', '')<br/>    txt = txt.str.replace('(&amp;lt)', '')<br/>    txt = txt.str.replace('(\xa0)', ' ')  <br/>    return txt</span><span id="72ac" class="lu lv jj mx b gy nf nc l nd ne">df['review'] = clean(df['review'])</span></pre><blockquote class="nh ni nj"><p id="eaf5" class="ky kz nk la b lb lc kk ld le lf kn lg nl li lj lk nm lm ln lo nn lq lr ls lt im bi translated">转换为小写</p></blockquote><p id="ce06" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">转换成小写是必要的。否则，它会将带有大写字母的同一个单词视为不同的单词。像‘我’和‘我’会被认为是不同的词。我们不想那样。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="c979" class="lu lv jj mx b gy nb nc l nd ne">df['review1'] = df['review'].apply(lambda x: " ".join(x.lower() for x in x.split()))<br/>df['review1'].head()</span></pre><p id="3eb3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="76b8" class="lu lv jj mx b gy nb nc l nd ne">153    we bought these for our son when he turned two...<br/>154    my son loves stacking cups, so a friend recomm...<br/>155    my son cameron just loves these great little s...<br/>156    my one year old son received these as a birthd...<br/>157    i purchased this toy for my great grandson\'s ...<br/>Name: review1, dtype: object</span></pre><blockquote class="nh ni nj"><p id="85c2" class="ky kz nk la b lb lc kk ld le lf kn lg nl li lj lk nm lm ln lo nn lq lr ls lt im bi translated">删除标点符号</p></blockquote><p id="b280" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这一步是删除标点符号。由于标点符号的原因，一个单词可能会被以不同于原来的方式处理。例如,“use”和“use:”会因为标点而变成不同的单词。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="6d7a" class="lu lv jj mx b gy nb nc l nd ne">df['review1'] = df['review1'].str.replace('[^\w\s]', '')<br/>df['review1'].head()</span></pre><p id="6efe" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="39f8" class="lu lv jj mx b gy nb nc l nd ne">153    we bought these for our son when he turned two...<br/>154    my son loves stacking cups so a friend recomme...<br/>155    my son cameron just loves these great little s...<br/>156    my one year old son received these as a birthd...<br/>157    i purchased this toy for my great grandsons fi...<br/>Name: review1, dtype: object</span></pre><p id="790c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如你所见，标点符号不见了！在第157行，“孙子”一词中有标点符号。我们最终会在词汇化或词干部分去掉“孙子”中的“s”。</p><blockquote class="nh ni nj"><p id="eab8" class="ky kz nk la b lb lc kk ld le lf kn lg nl li lj lk nm lm ln lo nn lq lr ls lt im bi translated">删除停用词</p></blockquote><p id="9b18" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">停用词是一些语法或约束词，如“是”、“the”、“and”、“so”、“my”等。这些是出现频率很高的词。但是可能不会给分析增加任何价值。虽然有争议。有些人认为他们有时很重要。在一些人工智能项目中，停用词可能很重要，但在这个例子中，停用词是不必要的。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="aa59" class="lu lv jj mx b gy nb nc l nd ne">import nltk<br/>nltk.download('stopwords')<br/>from nltk.corpus import stopwords</span><span id="3d98" class="lu lv jj mx b gy nf nc l nd ne">stop = stopwords.words('english')<br/>df['review1'] = df['review1'].apply(lambda x: " ".join(x for x in x.split() if x not in stop))<br/>df['review1'].head()</span></pre><p id="3366" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="2f0d" class="lu lv jj mx b gy nb nc l nd ne">153    bought son turned two seen playmates home love...<br/>154    son loves stacking cups friend recommended toy...<br/>155    son cameron loves great little stacking cars e...<br/>156    one year old son received birthday gift loves ...<br/>157    purchased toy great grandsons first christmas ...<br/>Name: review1, dtype: object</span></pre><p id="6bf6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">看，所有的停用词都没了！</p><blockquote class="nh ni nj"><p id="3e51" class="ky kz nk la b lb lc kk ld le lf kn lg nl li lj lk nm lm ln lo nn lq lr ls lt im bi translated">去掉生僻字</p></blockquote><p id="58b8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有些词只出现过一次。那些生僻字并没有增加任何东西。所以我们可以安全地丢弃它们。首先，找出每个单词的频率，然后找出只出现过一次的单词。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="0c30" class="lu lv jj mx b gy nb nc l nd ne">freq = pd.Series(' '.join(df['review1']).split()).value_counts()<br/>less_freq = list(freq[freq ==1].index)<br/>less_freq</span></pre><p id="36a4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="d620" class="lu lv jj mx b gy nb nc l nd ne">['djgs',<br/> 'now7monthold',<br/> 'joseph',<br/> 'area8',<br/> 'activing',<br/> 'tea',<br/> 'productdespite',<br/> 'worth3the',<br/> 'aroundand',<br/> '80lb',<br/> 'combinedit',<br/> 'hikesnow',<br/> 'bubblesbeing',<br/> 'cheast',<br/> 'inexcusable',<br/> 'heavyeven',</span></pre><p id="0120" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是输出的一部分。这个列表总共有14352个单词。如果你注意到这些单词中的大部分甚至看起来是有线的。大多是错别字或者拼写错误。让我们把它们从评论中去掉:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="6f23" class="lu lv jj mx b gy nb nc l nd ne">df['review1'] = df['review1'].apply(lambda x: " ".join(x for x in x.split() if x not in less_freq))</span></pre><blockquote class="nh ni nj"><p id="cdf7" class="ky kz nk la b lb lc kk ld le lf kn lg nl li lj lk nm lm ln lo nn lq lr ls lt im bi translated">拼写纠正</p></blockquote><p id="d35f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">简单的拼写错误可以使用correct()函数来纠正。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="2211" class="lu lv jj mx b gy nb nc l nd ne">from textblob import TextBlob, Word, Blobber</span><span id="b56b" class="lu lv jj mx b gy nf nc l nd ne">df['review1'].apply(lambda x: str(TextBlob(x).correct()))<br/>df['review1'].head()</span></pre><p id="b6b0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="141c" class="lu lv jj mx b gy nb nc l nd ne">153    He bought these for our son when he turn two H...<br/>154    By son love stick cups so a friend recommend t...<br/>155    By son cameron just love these great little st...<br/>156    By one year old son receive these a a birthday...<br/>157    I purchase the toy for my great grandson first...</span></pre><p id="0bf8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">只是一个警告！运行这段代码需要几个小时。你必须有耐心。如果你愿意，你可以避免这样做以节省时间。</p><blockquote class="nh ni nj"><p id="08a5" class="ky kz nk la b lb lc kk ld le lf kn lg nl li lj lk nm lm ln lo nn lq lr ls lt im bi translated">词干化和词汇化</p></blockquote><p id="7fa4" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">词干将从单词中删去像“ly”、“ing”、“ed”这样的部分。我们之前谈过一点。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="67fc" class="lu lv jj mx b gy nb nc l nd ne">from nltk.stem import PorterStemmer<br/>st = PorterStemmer()<br/>df['review1'] = df['review'].apply(lambda x: " ".join([st.stem(word) for word in x.split()]))</span></pre><p id="9a15" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="36c3" class="lu lv jj mx b gy nb nc l nd ne">153    We bought these for our son when he turn two. ...<br/>154    My son love stack cups, so a friend recommend ...<br/>155    My son cameron just love these great littl sta...<br/>156    My one year old son receiv these as a birthday...<br/>157    I purchas thi toy for my great grandson\' firs...</span></pre><p id="bf90" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">查看输出中的第157行。在‘孙子’这个词后面，标点符号又回来了。别担心，我们以后会处理的。</p><p id="fdaa" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下一步是将单词词根化。您可以选择词干化或词汇化。词干化之后，您可能看不到词汇化的很多变化。我仍然展示它是为了演示的目的。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="993d" class="lu lv jj mx b gy nb nc l nd ne">df['review1'] = df['review1'].apply(lambda x: " ".join([Word(word).lemmatize() for word in x.split()]))<br/>df['review1'].head()</span></pre><p id="296e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="1a45" class="lu lv jj mx b gy nb nc l nd ne">153    We bought these for our son when he turn two. ...<br/>154    My son love stack cups, so a friend recommend ...<br/>155    My son cameron just love these great littl sta...<br/>156    My one year old son receiv these a a birthday ...<br/>157    I purchas thi toy for my great grandson\' firs...</span></pre><p id="4e75" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们应该再次删除标点符号:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="5c56" class="lu lv jj mx b gy nb nc l nd ne">df['review1'] = df['review1'].str.replace('[^\w\s]', '')<br/>df['review1'].head()</span></pre><p id="26da" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="b242" class="lu lv jj mx b gy nb nc l nd ne">153    We bought these for our son when he turn two H...<br/>154    My son love stack cups so a friend recommend t...<br/>155    My son cameron just love these great littl sta...<br/>156    My one year old son receiv these a a birthday ...<br/>157    I purchas thi toy for my great grandson first ...<br/>Name: review1, dtype: object</span></pre><p id="44c5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">实际上，我多做了一步。你可以避免删除开头的标点符号，在这一点上做。那会省去你不必要的一步。</p><h2 id="f141" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">数据分析</h2><p id="eea4" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">让我们从向数据集中添加更多要素开始分析。在这里，我加上评论的长度和每篇评论的字数。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="9d23" class="lu lv jj mx b gy nb nc l nd ne">df['review_len'] = df['review'].astype(str).apply(len)<br/>df['word_count'] = df['review'].apply(lambda x: len(str(x).split()))</span></pre><p id="1b4f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我想再增加一个特性，叫做极性。极性表现了一段文字的情绪。它计算消极和积极的话，并确定极性。该值的范围从-1到1，其中-1代表负面情绪，0代表中性情绪，1代表正面情绪。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="df4c" class="lu lv jj mx b gy nb nc l nd ne">from textblob import TextBlob, Word, Blobber</span><span id="3141" class="lu lv jj mx b gy nf nc l nd ne">df['polarity'] = df['review1'].map(lambda text: TextBlob(text).sentiment.polarity)</span><span id="9510" class="lu lv jj mx b gy nf nc l nd ne">df.head()</span></pre><p id="3556" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">输出:</p><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi no"><img src="../Images/e7e8a78e8f052a85317b6e35d2cdbef4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8m29cokCrTzx3_xjMxNspw.png"/></div></div></figure><blockquote class="nh ni nj"><p id="7a51" class="ky kz nk la b lb lc kk ld le lf kn lg nl li lj lk nm lm ln lo nn lq lr ls lt im bi translated">分布</p></blockquote><p id="ec32" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我想先看一下word_count、review_len和polarity的分布。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="372f" class="lu lv jj mx b gy nb nc l nd ne">df[["review_len", "word_count", "polarity"]].hist(bins=20, figsize=(15, 10))</span></pre><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi np"><img src="../Images/bd4823236d83332060b09f2b1736208f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VTn9i5d8GN338qojHqf4Ag.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="4cc9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">评论长度和字数的分布与预期相似。大多数人口处于较低的范围。严重向右倾斜的分布。极性几乎正常。多数极性大于零。这意味着大多数评论都是正面的。</p><blockquote class="nh ni nj"><p id="ef36" class="ky kz nk la b lb lc kk ld le lf kn lg nl li lj lk nm lm ln lo nn lq lr ls lt im bi translated">极性与额定值</p></blockquote><p id="0491" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因为有一个评级栏可用，我们应该检查极性是否与评级相符。以下是各等级极性的箱线图:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="ee2a" class="lu lv jj mx b gy nb nc l nd ne">plt.figure(figsize = (10, 8))<br/>sns.set_style('whitegrid')<br/>sns.set(font_scale = 1.5)<br/>sns.boxplot(x = 'rating', y = 'polarity', data = df)<br/>plt.xlabel("Rating")<br/>plt.ylabel("Polatiry")<br/>plt.title("Product Ratings vs Polarity")<br/>plt.show()</span></pre><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/d2159d3367ce9302db49b21d35630863.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*W0-fCbA2XtYZTu1wbMpnbQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="9cbc" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">等级越高，平均极性越大。尽管在等级1和5中有很多异常值。也许看看这些数字会更有帮助。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="aa46" class="lu lv jj mx b gy nb nc l nd ne">mean_pol = df.groupby('rating')['polarity'].agg([np.mean])<br/>mean_pol.columns = ['mean_polarity']</span><span id="55ca" class="lu lv jj mx b gy nf nc l nd ne">fig, ax = plt.subplots(figsize=(8, 6))<br/>plt.bar(mean_pol.index, mean_pol.mean_polarity, width=0.3)</span><span id="4060" class="lu lv jj mx b gy nf nc l nd ne">#plt.gca().set_xticklabels(mean_pol.index, fontdict={'size': 14})<br/>    <br/>for i in ax.patches:<br/>    ax.text(i.get_x(), i.get_height()+0.01, str("{:.2f}".format(i.get_height())))</span><span id="8a9f" class="lu lv jj mx b gy nf nc l nd ne">plt.title("Polarity of Ratings", fontsize=22)<br/>plt.ylabel("Polarity", fontsize=16)<br/>plt.xlabel("Rating", fontsize=16)<br/>plt.ylim(0, 0.35)<br/>plt.show()</span></pre><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/7a58541c7c6fcf01a6ffb87e4063e133.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*GE2O_w_xKNttSaS8DRF19g.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="0f76" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我原以为评分1和2的极性接近-1。但是看起来它们更接近于0。这意味着评论中可能没有那么多负面词汇。我只是通过观察极性来猜测。请阅读几篇评级为1的评论以进行复查。</p><blockquote class="nh ni nj"><p id="e3a5" class="ky kz nk la b lb lc kk ld le lf kn lg nl li lj lk nm lm ln lo nn lq lr ls lt im bi translated">每个评分的评论数</p></blockquote><p id="fc85" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面是一个计数图，显示了数据集中每个评分的评论数。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="8486" class="lu lv jj mx b gy nb nc l nd ne">plt.figure(figsize=(8, 6))<br/>sns.countplot(x='rating', data=df)<br/>plt.xlabel("Rating")<br/>plt.title("Number of data of each rating")<br/>plt.show()</span></pre><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/4dd48e806bc92846fa2271a2d9146984.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*-E_BDBZ6ax9ILfyVaoHsHA.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="c4c6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该数据集的大多数评论的评级为5。</p><blockquote class="nh ni nj"><p id="a339" class="ky kz nk la b lb lc kk ld le lf kn lg nl li lj lk nm lm ln lo nn lq lr ls lt im bi translated">评论的长度与评分</p></blockquote><p id="6ffe" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">看看评论的长度是否会随着评分的变化而变化将会很有趣。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="5cec" class="lu lv jj mx b gy nb nc l nd ne">plt.figure(figsize=(10, 6))<br/>sns.pointplot(x = "rating", y = "review_len", data = df)<br/>plt.xlabel("Rating")<br/>plt.ylabel("Review Length")<br/>plt.title("Product Rating vs Review Length")<br/>plt.show()</span></pre><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/0362cbd6eb58b9e606a4ff7d45c205a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1292/format:webp/1*xxoHK2oNdDeK7DP4aotsuQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="1063" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当评分为5时，评论长度显著下降。所以，客户高兴的时候，也没写太多！</p><blockquote class="nh ni nj"><p id="33a3" class="ky kz nk la b lb lc kk ld le lf kn lg nl li lj lk nm lm ln lo nn lq lr ls lt im bi translated">基于极性的20大产品</p></blockquote><p id="2bb3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这些是基于极性的前20个产品</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="a695" class="lu lv jj mx b gy nb nc l nd ne">product_pol = df.groupby('name')['polarity'].agg([np.mean])<br/>product_pol.columns = ['polarity']<br/>product_pol = product_pol.sort_values('polarity', ascending=False)<br/>product_pol = product_pol.head(20)<br/>product_pol</span></pre><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/a05313863f52c6af4f0f53da8e628829.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*F2CT5WPoygrBW2vrothMyw.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><blockquote class="nh ni nj"><p id="a693" class="ky kz nk la b lb lc kk ld le lf kn lg nl li lj lk nm lm ln lo nn lq lr ls lt im bi translated">WordCloud</p></blockquote><p id="0111" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Wordcloud是一种常见且漂亮的文本数据可视化工具，用于绘制单词的频率。如果您还没有wordcloud，您可能需要使用以下命令安装它:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="cb84" class="lu lv jj mx b gy nb nc l nd ne">conda install -c conda-forge wordcloud</span></pre><p id="a857" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我使用Anaconda包。这就是为什么我给出了anaconda包的安装命令。</p><p id="59f1" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了创建单词cloud，我将review1列中的所有文本组合在一起，形成了一个bid文本块。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="2f97" class="lu lv jj mx b gy nb nc l nd ne">text = " ".join(review for review in df.review1)</span></pre><p id="84b8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用这个文本块，我创建了单词cloud。在造云这个词之前，我去掉了一些我认为没有必要的词。如果你愿意，你可以进一步清理。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="9fcf" class="lu lv jj mx b gy nb nc l nd ne">from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator<br/>stopwords = set(STOPWORDS)<br/>stopwords = stopwords.union(["ha", "thi", "now", "onli", "im", "becaus", "wa", "will", "even", "go", "realli", "didnt", "abl"])</span><span id="a0b9" class="lu lv jj mx b gy nf nc l nd ne"><br/>wordcl = WordCloud(stopwords = stopwords, background_color='white', max_font_size = 50, max_words = 5000).generate(text)<br/>plt.figure(figsize=(14, 12))<br/>plt.imshow(wordcl, interpolation='bilinear')<br/>plt.axis('off')<br/>plt.show()</span></pre><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nv"><img src="../Images/e536ee7fdf1ea423f13ba2c08a76c366.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*98rRRM4DPxNMwijFTqDHgw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="0cef" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">文本中出现频率越高的单词越大。</p><blockquote class="nh ni nj"><p id="9e9d" class="ky kz nk la b lb lc kk ld le lf kn lg nl li lj lk nm lm ln lo nn lq lr ls lt im bi translated">频率图表</p></blockquote><p id="e300" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是文本数据分析中制作词频图表的常见做法。这很好地说明了人们在这篇文章中谈论最多的是什么。首先，找出每个单词在数据集的review列中的出现频率。然后根据频率画出前20个单词。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="99b8" class="lu lv jj mx b gy nb nc l nd ne">def get_top_n_words(corpus, n=None):<br/>    vec=CountVectorizer().fit(corpus)<br/>    bag_of_words = vec.transform(corpus)<br/>    sum_words = bag_of_words.sum(axis=0)<br/>    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]<br/>    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)<br/>    return words_freq[:n]</span><span id="b0fd" class="lu lv jj mx b gy nf nc l nd ne">common_words = get_top_n_words(df['review1'], 20)</span><span id="7e12" class="lu lv jj mx b gy nf nc l nd ne">df1 = pd.DataFrame(common_words, columns = ['Review', 'count'])<br/>df1.head()</span></pre><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/5a47c83c1a338ba1a578de54a7b80c2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:284/format:webp/1*l4VdQvVa83K2ONZlW2VIRQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="6d82" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面是前20个单词出现频率的柱状图:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="193e" class="lu lv jj mx b gy nb nc l nd ne">df1.groupby('Review').sum()['count'].sort_values(ascending=False).plot(<br/>    kind='bar',<br/>    figsize=(10, 6),<br/>    xlabel = "Top Words",<br/>    ylabel = "Count",<br/>    title = "Bar Chart of Top Words Frequency"<br/>)</span></pre><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/e5d9df29750c045a22fef2b98d0b2228.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*ZFo-Xoebt3pnqP3YqUUpPQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="3ed2" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这些是评论中出现频率最高的词。但比起只看到一个单词，两个或三个连续的单词更有帮助。它们提供了一些意义。下图显示了最常见的二元模型:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="63a9" class="lu lv jj mx b gy nb nc l nd ne">def get_top_n_bigram(corpus, n=None):<br/>    vec = CountVectorizer(ngram_range=(2,2)).fit(corpus)<br/>    bag_of_words = vec.transform(corpus)<br/>    sum_words = bag_of_words.sum(axis=0)<br/>    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]<br/>    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)<br/>    return words_freq[:n]</span><span id="ee59" class="lu lv jj mx b gy nf nc l nd ne">common_words2 = get_top_n_bigram(df['review1'], 30)<br/>df2 = pd.DataFrame(common_words2, columns=['Review', "Count"])<br/>df2.head()</span></pre><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/afd614320ad318be7065d82f6cd6e754.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*D625nEMnShn0-Vv_cooXDg.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="4ef9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是最高发生二元模型的条形图:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="07c8" class="lu lv jj mx b gy nb nc l nd ne">df2.groupby('Review').sum()['Count'].sort_values(ascending=False).plot(<br/>    kind='bar',<br/>    figsize=(12,6),<br/>    xlabel = "Bigram Words",<br/>    ylabel = "Count",<br/>    title = "Bar chart of Bigrams Frequency"<br/>)</span></pre><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nz"><img src="../Images/7b3df2c419aef081c9a8fc9cf38e25ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*paAL0Tn2aYzvnOMPkZ3G-w.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="6d4f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">看看这些二元模型。它们是一些更有意义的短语。下一个图是三元模型。也许这将为我们提供更多关于人们在评论中说什么的想法。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="6ebe" class="lu lv jj mx b gy nb nc l nd ne">def get_top_n_trigram(corpus, n=None):<br/>    vec = CountVectorizer(ngram_range=(3, 3), stop_words='english').fit(corpus)<br/>    bag_of_words = vec.transform(corpus)<br/>    sum_words = bag_of_words.sum(axis=0)<br/>    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]<br/>    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)<br/>    return words_freq[:n]</span><span id="d7a2" class="lu lv jj mx b gy nf nc l nd ne">common_words3 = get_top_n_trigram(df['review1'], 30)</span><span id="5fd8" class="lu lv jj mx b gy nf nc l nd ne">df3 = pd.DataFrame(common_words3, columns = ['Review' , 'Count'])<br/>df3.groupby('Review').sum()['Count'].sort_values(ascending=False).plot(<br/>    kind='bar',<br/>    figsize=(12,6),<br/>    xlabel = "Trigram Words",<br/>    ylabel = "Count",<br/>    title = "Bar chart of Trigrams Frequency"<br/>)</span></pre><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oa"><img src="../Images/e470c5e1ab0013ea0921dd6f08b0b0d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yX76wYiqg7jswjzz2ejhPQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><blockquote class="nh ni nj"><p id="7872" class="ky kz nk la b lb lc kk ld le lf kn lg nl li lj lk nm lm ln lo nn lq lr ls lt im bi translated">词性标注</p></blockquote><p id="d23f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是一个用词性标注单词的过程，如名词、代词、动词、形容词等。使用TextBlob API很容易做到这一点。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="0457" class="lu lv jj mx b gy nb nc l nd ne">blob = TextBlob(str(df['review1']))<br/>pos_df = pd.DataFrame(blob.tags, columns = ['word', 'pos'])</span><span id="7892" class="lu lv jj mx b gy nf nc l nd ne">pos_df = pos_df.pos.value_counts()[:30]<br/>pos_df.plot(kind='bar',<br/>            xlabel = "Part Of Speech",<br/>           ylabel = "Frequency",<br/>           title = "Bar Chart of the Frequency of the Parts of Speech",<br/>           figsize=(10, 6))</span></pre><figure class="ms mt mu mv gt iv gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/ed60484415d2d026e67d50737aa5db75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*Pg_3bIuTM4kDhllplAMAgA.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><blockquote class="nh ni nj"><p id="6634" class="ky kz nk la b lb lc kk ld le lf kn lg nl li lj lk nm lm ln lo nn lq lr ls lt im bi translated">情感分析</p></blockquote><p id="fb0b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">探索性数据分析也包括机器学习。在这里，您将发现使用相同数据集对产品评论进行的情感分析。</p><div class="is it gp gr iu oc"><a rel="noopener follow" target="_blank" href="/a-complete-sentiment-analysis-algorithm-in-python-with-amazon-product-review-data-step-by-step-2680d2e2c23b"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd jk gy z fp oh fr fs oi fu fw ji bi translated">一个完整的带有亚马逊产品评论数据的Python情感分析算法:一步一步</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">使用Python的Scikit_learn库的NLP项目</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">towardsdatascience.com</p></div></div><div class="ol l"><div class="om l on oo op ol oq ja oc"/></div></div></a></div><h2 id="7537" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">结论</h2><p id="646a" class="pw-post-body-paragraph ky kz jj la b lb mn kk ld le mo kn lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">我试图提出一些很好的方法来理解或从一篇文章中提取信息。在预处理部分，我介绍了几种在机器学习中也很有用的预处理技术。在所有的分析或机器学习中，您可能不会使用我在这里介绍的所有预处理技术。你可以选择任何你觉得合适的。然后，介绍了一些通用的探索性分析技术。还有很多技术可以利用。希望你能在自己的数据集中尝试这些，并做一些很酷的分析。</p><p id="3b47" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">欢迎在推特上关注我，喜欢我的T2脸书页面。</p><h2 id="126d" class="lu lv jj bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">更多阅读</h2><div class="is it gp gr iu oc"><a rel="noopener follow" target="_blank" href="/an-ultimate-cheatsheet-of-data-visualization-in-seaborn-be8ed13a3697"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd jk gy z fp oh fr fs oi fu fw ji bi translated">用Python的Seaborn库实现数据可视化的终极指南</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">对学习者来说也是一个很好的资源</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">towardsdatascience.com</p></div></div><div class="ol l"><div class="or l on oo op ol oq ja oc"/></div></div></a></div><div class="is it gp gr iu oc"><a rel="noopener follow" target="_blank" href="/exploratory-data-analysis-visualization-and-prediction-model-in-python-241b954e1731"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd jk gy z fp oh fr fs oi fu fw ji bi translated">Python中的探索性数据分析、可视化和预测模型</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">在Python中使用Pandas、Matplotlib、Seaborn和Scikit_learn库</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">towardsdatascience.com</p></div></div><div class="ol l"><div class="os l on oo op ol oq ja oc"/></div></div></a></div><div class="is it gp gr iu oc"><a rel="noopener follow" target="_blank" href="/a-full-length-machine-learning-course-in-python-for-free-f2732954f35f"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd jk gy z fp oh fr fs oi fu fw ji bi translated">免费的Python全长机器学习课程</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">吴恩达用Python写的机器学习教程</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">towardsdatascience.com</p></div></div><div class="ol l"><div class="ot l on oo op ol oq ja oc"/></div></div></a></div><div class="is it gp gr iu oc"><a rel="noopener follow" target="_blank" href="/text-files-processing-cleaning-and-classification-of-documents-in-r-394c6b4f1504"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd jk gy z fp oh fr fs oi fu fw ji bi translated">R语言中文本文件的处理、清理和分类</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">使用了一些很棒的包和K近邻分类器</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">towardsdatascience.com</p></div></div><div class="ol l"><div class="ou l on oo op ol oq ja oc"/></div></div></a></div><div class="is it gp gr iu oc"><a rel="noopener follow" target="_blank" href="/an-ultimate-guide-to-time-series-analysis-in-pandas-76a0433621f3"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd jk gy z fp oh fr fs oi fu fw ji bi translated">熊猫时间序列分析终极指南</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">在Pandas中执行时间序列分析所需的所有Pandas功能。您也可以将此用作备忘单。</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">towardsdatascience.com</p></div></div><div class="ol l"><div class="ov l on oo op ol oq ja oc"/></div></div></a></div></div></div>    
</body>
</html>