<html>
<head>
<title>Creating a Powerful COVID-19 Mask Detection Tool with PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用PyTorch创建一个强大的新冠肺炎面具检测工具</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/creating-a-powerful-covid-19-mask-detection-tool-with-pytorch-d961b31dcd45?source=collection_archive---------23-----------------------#2021-06-03">https://towardsdatascience.com/creating-a-powerful-covid-19-mask-detection-tool-with-pytorch-d961b31dcd45?source=collection_archive---------23-----------------------#2021-06-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="150f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">快速建立小而精确的图像分类模型的实例研究</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/119a221e766a9e1b3cd62a9aae065874.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sj7Hr8t1BVJg8Vls"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">马修·韦林在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="e98d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在过去的一年里，新冠肺炎给世界带来了社会、经济和人类的损失。人工智能工具可以<strong class="ky ir">识别正确的面具佩戴，以帮助重新打开世界，防止未来的大流行。</strong></p><p id="9b21" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当然，这有更深层的社会含义，比如隐私和安全的权衡。这些将在随附的arXiv研究论文<a class="ae kv" href="https://arxiv.org/pdf/2105.01816.pdf" rel="noopener ugc nofollow" target="_blank">中讨论。</a></p><p id="40c5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章中，我将带你浏览我们创建的项目。</p><p id="1fa6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">读完这篇文章<em class="ls">，你将有知识创建一个类似的管道，不仅用于遮罩检测，也用于任何类型的图像识别</em>。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="c31b" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">特征工程</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/9e0a72830ba23eaac963b2d63657fce2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fib1_5-NwuhPQnQb4uguMA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">我们的数据集</p></figure><p id="44f2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用<a class="ae kv" href="https://github.com/cabani/MaskedFace-Net" rel="noopener ugc nofollow" target="_blank"> MaskedFace-Net </a>和<a class="ae kv" href="https://github.com/NVlabs/ffhq-dataset" rel="noopener ugc nofollow" target="_blank"> Flickr Faces </a>数据集，我们将图像大小调整为128x128以节省空间。我们将图像移动到三个文件夹中(train、val和test)。每个文件夹都有三个子目录(正确/不正确/无掩码)。这使得他们很容易装载PyTorch的火炬视觉。</p><pre class="kg kh ki kj gt mt mu mv mw aw mx bi"><span id="8b67" class="my mb iq mu b gy mz na l nb nc">batch = 16</span><span id="e796" class="my mb iq mu b gy nd na l nb nc"><br/>train_dataset = torchvision.datasets.ImageFolder('./Data_Sample/train', transform = train_transform)</span><span id="2bf2" class="my mb iq mu b gy nd na l nb nc">val_dataset = torchvision.datasets.ImageFolder('./Data_Sample/val', transform = test_transform)</span><span id="b451" class="my mb iq mu b gy nd na l nb nc">test_dataset = torchvision.datasets.ImageFolder('./Data_Sample/test', transform = test_transform)</span><span id="9a79" class="my mb iq mu b gy nd na l nb nc">train_dataset_loader = torch.utils.data.DataLoader(<br/>train_dataset, batch_size=batch, shuffle=True)</span><span id="1cb1" class="my mb iq mu b gy nd na l nb nc">val_dataset_loader = torch.utils.data.DataLoader(<br/>val_dataset, batch_size=batch, shuffle=True)</span><span id="b0c1" class="my mb iq mu b gy nd na l nb nc">test_dataset_loader = torch.utils.data.DataLoader(<br/>test_dataset, batch_size=batch, shuffle=False)</span></pre><p id="dd2f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果您查看代码片段，您会发现我们使用了torchvision的内置参数来对我们的图像进行转换。</p><p id="e050" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于所有数据，我们将图像转换为张量，并使用一些标准值对其进行归一化。对于训练和验证数据，我们应用了额外的转换。例如，随机对图像进行重新着色、调整大小和模糊处理。这些是应用于每一批的<strong class="ky ir">，意味着模型试图学习一个移动目标</strong>:每次应用不同噪声的训练图像。</p><pre class="kg kh ki kj gt mt mu mv mw aw mx bi"><span id="4dff" class="my mb iq mu b gy mz na l nb nc">import torchvision.transforms as transforms</span><span id="67f4" class="my mb iq mu b gy nd na l nb nc">test_transform = transforms.Compose([<br/>transforms.ToTensor(),<br/>transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),<br/>])</span></pre><p id="862e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">变换使您的模型规范化，使其更好地一般化。因此，它将在真实世界的数据上表现得更好。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="988c" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">基线模型</h1><p id="7571" class="pw-post-body-paragraph kw kx iq ky b kz ne jr lb lc nf ju le lf ng lh li lj nh ll lm ln ni lp lq lr ij bi translated">对于像实时图像分类这样的目标，准确性和速度都是关键。这些通常有一个权衡，因为较大的模型可能具有较高的准确性。为了获得基线，我们从应用更快的机器学习模型开始。</p><p id="b4f4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">好家伙，我们会大吃一惊的！我们意识到，我们的数据集使用合成模型使得预测变得有些琐碎。在时间压力下，并且在我们使用的180，000张图像附近没有数据集，我们<strong class="ky ir">继续研究如何进一步提高性能</strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/456adf5f2fc97e79fb4fc325fea79838.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xwWBvJzTTLY3vgW4Qo_XYw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">基线模型精度</p></figure><p id="dc8a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于这些更简单的模型，我们使用LeakyReLU和AdamW训练了scikit-learn的随机森林、cv2的Haar级联分类器和自定义CNN。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="c3e6" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">高级模型</h1><p id="8ced" class="pw-post-body-paragraph kw kx iq ky b kz ne jr lb lc nf ju le lf ng lh li lj nh ll lm ln ni lp lq lr ij bi translated">接下来，我们开始使用最先进的模型。顶级研究人员使用数百万美元的计算来训练和微调这些模型。对于一般问题，它们往往工作得最好。</p><p id="3203" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以用我们的数据训练它们，看看它们在我们的面具分类任务中表现如何。这种<strong class="ky ir">转移学习</strong>比从头开始训练要快得多，因为权重已经接近最优。我们的训练会使他们变得更好。</p><p id="5f39" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下面的函数用于不同的torchvision模型，看看它们的性能如何。</p><pre class="kg kh ki kj gt mt mu mv mw aw mx bi"><span id="c977" class="my mb iq mu b gy mz na l nb nc"># Takes the model and applies transfer learning<br/># Returns the model and an array of validation accuracies<br/>def train_model(model, dataloaders, optimizer, criterion, scheduler, device, num_epochs=20):<br/> startTime = time.time()<br/> top_model = copy.deepcopy(model.state_dict())<br/> top_acc = 0.0</span><span id="aedd" class="my mb iq mu b gy nd na l nb nc"> for epoch in range(num_epochs):</span><span id="da45" class="my mb iq mu b gy nd na l nb nc">  for data_split in ['Train', 'Validation']:</span><span id="8c88" class="my mb iq mu b gy nd na l nb nc">   if data_split == 'Train':<br/>    scheduler.step()<br/>    model.train()</span><span id="6dc2" class="my mb iq mu b gy nd na l nb nc">   else:<br/>    model.eval()</span><span id="cbff" class="my mb iq mu b gy nd na l nb nc">   # Track running loss and correct predictions<br/>   running_loss = 0.0<br/>   running_correct = 0</span><span id="0026" class="my mb iq mu b gy nd na l nb nc">   # Move to device<br/>   for inputs, labels in dataloaders[data_split]:<br/>    inputs = inputs.to(device)<br/>    labels = labels.to(device)</span><span id="fe96" class="my mb iq mu b gy nd na l nb nc">   # Zero out the gradient<br/>    optimizer.zero_grad()</span><span id="71b3" class="my mb iq mu b gy nd na l nb nc"># Forward pass<br/>   # Gradient is turned on for train<br/>    with torch.set_grad_enabled(data_split == "Train"):<br/>    outputs = model(inputs)<br/>    loss = criterion(outputs, labels)<br/>    _, preds = torch.max(outputs, 1)<br/>    if data_split == "Train":<br/>     loss.backward()<br/>     optimizer.step()<br/>    # Update running loss and correct predictions<br/>    running_loss += loss.item() * inputs.size(0)<br/>    running_correct += torch.sum (labels.data == preds)<br/>   <br/>   epoch_loss = running_loss / dataloader_sizes[data_split]<br/>   epoch_acc = running_correct.double() / dataloader_sizes[data_split]<br/>   print('{} Loss: {:.2f}, Accuracy: {:.2f}'.format(data_split,     epoch_loss, epoch_acc))<br/>   # If this the top model, deep copy it<br/>   if data_split == "Validation" and epoch_acc &gt; top_acc:<br/>   top_acc = epoch_acc<br/>   top_model = copy.deepcopy(model.state_dict())</span><span id="7a47" class="my mb iq mu b gy nd na l nb nc">  print("Highest validation accuracy: {:.2f}".format(top_acc))</span><span id="439d" class="my mb iq mu b gy nd na l nb nc"> # Load best model's weights<br/> model.load_state_dict(top_model)<br/> return model;</span></pre><p id="4bbb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于我们的重点是速度，我们选择了四个较小但性能良好的神经网络，并取得了以下结果:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/66eab61f4d08736bdfd557da5b891d9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6HxspqpOR6qKK8_gdwGtpg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">迁移学习模型的准确性</p></figure><p id="eb77" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">大幅提升！<strong class="ky ir">但是我们能做得更好吗？</strong></p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="5e22" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">蒸馏</h1><p id="8261" class="pw-post-body-paragraph kw kx iq ky b kz ne jr lb lc nf ju le lf ng lh li lj nh ll lm ln ni lp lq lr ij bi translated">这是<strong class="ky ir">的第一步。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/04e67dfc5b718f29e32360f6db6b232c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*l77VRcauFnkP5tvw"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Jan Ranft 在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="d22b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">蒸馏是一项前沿技术，它训练更小的模型来做出更快的预测。它从网络中提取知识。这非常适合我们的用例。</p><p id="5cd5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在蒸馏中，你从教师模型中训练学生。不是根据你的数据训练你的模型，<strong class="ky ir">而是根据另一个模型的预测训练它</strong>。因此，您可以用更小的网络来复制结果。</p><p id="d318" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">蒸馏的实施可能具有挑战性且需要大量资源。幸运的是，<a class="ae kv" href="https://github.com/SforAiDl/KD_Lib" rel="noopener ugc nofollow" target="_blank"> KD_Lib </a> for PyTorch提供了可作为库访问的研究论文的实现。下面的代码片段用于香草蒸馏。</p><pre class="kg kh ki kj gt mt mu mv mw aw mx bi"><span id="3c28" class="my mb iq mu b gy mz na l nb nc">import torch</span><span id="0460" class="my mb iq mu b gy nd na l nb nc">import torch.optim as optim</span><span id="3691" class="my mb iq mu b gy nd na l nb nc">from torchvision import datasets, transforms<br/>from KD_Lib.KD import VanillaKD</span><span id="34ab" class="my mb iq mu b gy nd na l nb nc"># Define models<br/>teacher_model = resnet<br/>student_model = inception</span><span id="1927" class="my mb iq mu b gy nd na l nb nc"># Define optimizers<br/>teacher_optimizer = optim.SGD(teacher_model.parameters(), 0.01)<br/>student_optimizer = optim.SGD(student_model.parameters(), 0.01)</span><span id="5630" class="my mb iq mu b gy nd na l nb nc"># Perform distillation<br/>distiller = VanillaKD(teacher_model, student_model, train_dataset_loader, val_dataset_loader,<br/>teacher_optimizer, student_optimizer, device = 'cuda')<br/>distiller.train_teacher(epochs=5, plot_losses=True, save_model=True)<br/>distiller.train_student(epochs=5, plot_losses=True, save_model=True)<br/>distiller.evaluate(teacher=False)<br/>distiller.get_parameters()</span></pre><p id="10e0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们的DenseNet模型上使用香草蒸馏，我们<strong class="ky ir">用我们的基线CNN达到了99.85%的准确率</strong>。在V100上，CNN <strong class="ky ir">以每秒775次推理的速度超过了所有最先进型号的速度</strong>。CNN用15%的参数做到了这一点。</p><p id="5c08" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="ls">更加赫然，</em> </strong>再次运行蒸馏继续提高精度。例如，NoisyTeacher、SelfTraining和MessyCollab的组合提高了结果。</p><p id="b4f1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls">边注:</em> <a class="ae kv" href="https://arxiv.org/abs/1909.11723" rel="noopener ugc nofollow" target="_blank"> <em class="ls">自我训练</em> </a> <em class="ls">把你的模型既当老师又当学生，多爽！</em></p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><h1 id="21f6" class="ma mb iq bd mc md me mf mg mh mi mj mk jw ml jx mm jz mn ka mo kc mp kd mq mr bi translated">结束语</h1><p id="e5c4" class="pw-post-body-paragraph kw kx iq ky b kz ne jr lb lc nf ju le lf ng lh li lj nh ll lm ln ni lp lq lr ij bi translated">创建一个正确使用面具的分类器既及时又紧迫。</p><p id="1295" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">构建这条管道的<strong class="ky ir">课程对你想做的任何图像分类任务</strong>都是有用的。希望这个管道有助于理解和解决你想要做的任何图像分类任务。<em class="ls">祝你好运！</em></p><blockquote class="nm nn no"><p id="d925" class="kw kx ls ky b kz la jr lb lc ld ju le np lg lh li nq lk ll lm nr lo lp lq lr ij bi translated">相关论文:【https://arxiv.org/pdf/2105.01816.pdf】<a class="ae kv" href="https://arxiv.org/pdf/2105.01816.pdf" rel="noopener ugc nofollow" target="_blank"/></p><p id="ed03" class="kw kx ls ky b kz la jr lb lc ld ju le np lg lh li nq lk ll lm nr lo lp lq lr ij bi translated">相关演示:<a class="ae kv" href="https://www.youtube.com/watch?v=iyf4uRWgkaI" rel="noopener ugc nofollow" target="_blank">https://www.youtube.com/watch?v=iyf4uRWgkaI</a></p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/55d0a98b1ed63cf26afed6e48821b0d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TztU-Cv-P-nSziQAncdcCA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">掩模分类器(单独的YOLOv5管道，在<a class="ae kv" href="https://arxiv.org/pdf/2105.01816.pdf" rel="noopener ugc nofollow" target="_blank">纸</a>中)</p></figure></div></div>    
</body>
</html>