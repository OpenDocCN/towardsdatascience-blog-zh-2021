<html>
<head>
<title>Analyzing Stack Overflow Dataset with Apache Spark 3.0</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Apache Spark 3.0分析堆栈溢出数据集</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/analyzing-stack-overflow-dataset-with-apache-spark-3-0-39786c141829?source=collection_archive---------19-----------------------#2021-12-13">https://towardsdatascience.com/analyzing-stack-overflow-dataset-with-apache-spark-3-0-39786c141829?source=collection_archive---------19-----------------------#2021-12-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="583a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在Amazon EMR上使用Spark SQL和Jupyter进行端到端数据分析</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f73990259c9d828605bc460f6b339b19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*riZsouf2AINzb5Et-AUrQg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">从<a class="ae ky" href="https://pixabay.com/cs/illustrations/otazn%c3%adk-hromada-ot%c3%a1zka-ozna%c4%8dit-1495858/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>下载的图像</p></figure><p id="829d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据科学家和分析师经常选择Python library Pandas进行数据探索和转换，这是很常见的，因为它的API漂亮、丰富且用户友好。Pandas库提供了DataFrame实现，当数据量适合单台机器的内存时，这真的非常方便。对于更大的数据集，这可能不再是最佳选择，而Apache Spark等分布式系统的强大功能变得非常有用。事实上，Apache Spark成为了大数据环境中数据分析的标准。</p><p id="5346" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将看到一步一步地分析特定数据集的方法，即来自堆栈溢出数据库的转储。本文的目标并不是通过一些具体的结论进行详尽的分析，而是展示如何使用Apache Spark和AWS堆栈(EMR、S3、EC2)进行这样的分析。我们将在这里展示所有的步骤，从从堆栈交换存档下载数据，上传到S3，数据的基本预处理，到使用Spark的最终分析，包括使用Matplotlib和Pandas绘制的一些漂亮的图表。</p><p id="e8a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，我们将看到使用Spark和Python生态系统对大数据集进行数据分析的一种非常常见的模式。该模式有三个步骤，第一，用Spark读取数据，第二，做一些减少数据大小的处理——这可能是一些过滤、聚合，甚至是数据的采样，最后将减少的数据集转换成Pandas数据框架，并在Pandas中继续分析，这允许您使用Matplotlib在幕后绘制图表。</p><p id="ce06" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将在本文中使用的所有代码也可以在我的<a class="ae ky" href="https://github.com/davidvrba/Stackoverflow-Data-Analysis" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>中找到，所以请随意查看。</p><p id="454d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文假设读者对AWS有一些基本的了解，比如在那里有一个帐户，能够生成EC2密钥对，等等。<strong class="lb iu">此外，让我在这里强调，这些服务，如EC2、EMR和S3，需要信用卡信息，亚马逊将向您收取使用费用。</strong>因此，请确保在完成后终止EC2实例和EMR集群，并在不再需要时从S3中删除数据(包括日志、笔记本和其他可能在此过程中创建的文件)。</p><p id="94e1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">EC2和集群设置的步骤将通过AWS文档的相关链接进行简要描述，AWS文档非常广泛，其他资源也可以在网上找到。让我们看看我们将执行的所有步骤:</p><ol class=""><li id="93a1" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">从Stack Exchange归档文件下载数据转储(这是一个7z压缩的XML文件)</li><li id="00be" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">解压缩下载的文件</li><li id="9695" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">将文件上传到S3(AWS上的分布式对象存储)</li><li id="f173" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">将XML文件转换为Apache Parquet格式(再次在S3上保存Parquet)</li><li id="69e4" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">分析数据集</li></ol><p id="0e2c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于步骤1–3，我们将使用一个带有更大磁盘的EC2实例。对于第4步和第5步，我们将使用Spark 3.0和JupyterLab在AWS上部署一个EMR集群。</p><p id="3671" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用Spark执行的分析本身独立于运行Spark的云，因此，如果您不想使用AWS，但您在其他地方运行Spark，并且您可以将数据存储到您的存储系统，您仍然可以使用我们将在下面使用的查询。</p><h1 id="4ab1" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">数据集</h1><p id="7877" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated"><a class="ae ky" href="https://stackoverflow.com/" rel="noopener ugc nofollow" target="_blank">栈溢出</a>很受程序员欢迎，你可以在那里提问，也可以回答其他用户的问题。转储本身可以在栈交换<a class="ae ky" href="https://archive.org/download/stackexchange" rel="noopener ugc nofollow" target="_blank">档案</a>中找到，并且可以在知识共享许可cc-by-sa 4.0下获得(更多信息参见<a class="ae ky" href="https://archive.org/details/stackexchange" rel="noopener ugc nofollow" target="_blank">细节</a>)。</p><p id="7714" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据由更多的文件组成，分别是<em class="ng">帖子</em>、<em class="ng">用户</em>、<em class="ng">评论</em>、<em class="ng">徽章</em>、<em class="ng">投票</em>、<em class="ng">标签</em>、<em class="ng">历史记录、</em>和<em class="ng">帖子链接</em>。在本文中，我们将只使用压缩XML中有16.9 GB的<em class="ng"> Posts </em>数据集(截止到2021年12月)。解压缩后，大小增加到85.6 GB。要查看各个表及其列的含义，我建议查看Stack Exchange上的这篇<a class="ae ky" href="https://meta.stackexchange.com/questions/2677/database-schema-documentation-for-the-public-data-dump-and-sede" rel="noopener ugc nofollow" target="_blank">帖子</a>，这篇帖子很好地描述了这一点。我们将处理的帖子包含所有问题及其答案的列表，我们可以根据<em class="ng"> _Post_type_id </em>来区分它们，其中值1代表问题，值2代表答案。</p><h1 id="33ce" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">数据下载和解压缩</h1><p id="578b" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">为了从Stack Exchange归档文件中下载数据，我们将使用EC2实例<em class="ng"> t2.large </em>和200 GB的磁盘空间。为了做到这一点，你需要登录到<a class="ae ky" href="https://aws.amazon.com/" rel="noopener ugc nofollow" target="_blank"> AWS控制台</a>，进入EC2实例，点击<em class="ng">启动实例</em>并选择操作系统，例如，Ubuntu Server 18.04 LTS。接下来，您需要指定实例类型，使用<em class="ng"> t2.large </em>很好，但是其他选项可能也不错。然后单击<em class="ng">配置实例详细信息</em>，并在<em class="ng">添加存储</em>选项卡中添加一个磁盘卷。您需要指定磁盘的大小，我选择200 GB，但是更小的磁盘应该也可以。只需记住，解压缩后数据集将有近90 GB。我在这里添加了一些与描述的步骤相关的截图:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/a3edf675b007ee59ccdcbd4dbaa92e6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BKwhuD0PimHady46xudnmw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者创造的形象。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/91fcb8993c6b57cc161fe376b99750f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8knVm0kQUAg2q7nbrxsqQw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者创造的形象。</p></figure><p id="a451" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，AWS会建议您设置一些安全组来限制可以使用该实例的IP地址，所以您可以随意这样做，并查看<a class="ae ky" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/working-with-security-groups.html#creating-security-group" rel="noopener ugc nofollow" target="_blank"> AWS文档</a>。最后，要启动实例，您需要有一个EC2密钥对，如果您还没有，您可以继续并生成它。同样，要生成新的密钥对，请参见AWS <a class="ae ky" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair" rel="noopener ugc nofollow" target="_blank">文档</a>。在实例运行之后，您可以从终端使用您的密钥(您需要在下载密钥的同一个文件夹中)对它进行ssh:</p><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="6847" class="no mk it nk b gy np nq l nr ns">ssh -i key.pem ubuntu@&lt;the ip address of your instance&gt;</span></pre><p id="450b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，在控制台上安装<em class="ng"> pip3 </em>和<em class="ng"> p7zip </em>(第一个是Python3的包管理系统，我们将用它来安装2个Python包，第二个是用来解压7z文件的):</p><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="4f8b" class="no mk it nk b gy np nq l nr ns">sudo apt update<br/>sudo apt install python3-pip<br/>sudo apt install p7zip-full</span></pre><p id="d146" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来安装Python包<em class="ng">请求</em>和<em class="ng"> boto3 </em>，我们将使用它们进行数据下载和S3上传:</p><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="fd6f" class="no mk it nk b gy np nq l nr ns">pip3 install boto3<br/>pip3 install requests</span></pre><p id="f357" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后使用<em class="ng"> vi data_download.py </em>创建一个新文件，并在其中复制以下Python代码(在vi编辑器中，您需要通过按<em class="ng"> i </em>键切换到插入模式):</p><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="f410" class="no mk it nk b gy np nq l nr ns">url = 'https://archive.org/download/stackexchange/stackoverflow.com-Posts.7z'</span><span id="0549" class="no mk it nk b gy nt nq l nr ns">local_filename =  'posts.7z'<br/>with requests.get(url, stream=True) as r:<br/>  r.raise_for_status()<br/>  with open(local_filename, 'wb') as f:<br/>    for chunk in r.iter_content(chunk_size=8192):<br/>      if chunk:<br/>        f.write(chunk)</span></pre><p id="56a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后保存(按<em class="ng"> esc </em>键并使用<em class="ng"> :wq </em>命令)并使用<em class="ng">python 3 data _ download . py</em>运行。数据下载本身可能需要一段时间，因为如上所述，该文件有近17 GB。接下来使用以下命令解压缩文件</p><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="bc17" class="no mk it nk b gy np nq l nr ns">7z x /home/ubuntu/posts.7z</span></pre><p id="3232" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">之后，创建另一个python文件<em class="ng"> vi upload_to_s3.py，</em>并将数据上传到s3的代码复制到那里:</p><pre class="kj kk kl km gt nj nk nl nm aw nn bi"><span id="3fd9" class="no mk it nk b gy np nq l nr ns">from boto3.session import *<br/>import os</span><span id="0144" class="no mk it nk b gy nt nq l nr ns">access_key = '...'<br/>secret_key = '...'<br/>bucket = '...'<br/>s3_output_prefix = '...'</span><span id="25c2" class="no mk it nk b gy nt nq l nr ns">session = Session(<br/>  aws_access_key_id=access_key, <br/>  aws_secret_access_key=secret_key<br/>)<br/>s3s = session.resource('s3').Bucket(bucket)</span><span id="9a44" class="no mk it nk b gy nt nq l nr ns">local_input_prefix = '/home/ubuntu'<br/>file_name = 'Posts.xml'</span><span id="0c3d" class="no mk it nk b gy nt nq l nr ns">input_path = os.path.join(local_input_prefix, file_name)<br/>output_path = os.path.join(s3_output_prefix, file_name)</span><span id="aba4" class="no mk it nk b gy nt nq l nr ns">s3s.upload_file(input_path, output_path)</span></pre><p id="b098" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如您在代码中看到的，您需要填写S3的凭证(<em class="ng"> access_key </em>和<em class="ng"> secret_key </em>)、您的bucket的名称和<em class="ng"> s3_output_prefix </em>，这是文件上传的最终目的地。如果没有<em class="ng"> access_key </em>和<em class="ng"> secret_key </em>，可以生成(参见<a class="ae ky" href="https://docs.aws.amazon.com/powershell/latest/userguide/pstools-appendix-sign-up.html" rel="noopener ugc nofollow" target="_blank">文档</a>)。如果您没有存储桶，您可以<a class="ae ky" href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-bucket-overview.html" rel="noopener ugc nofollow" target="_blank">创建</a>它。</p><p id="ddc8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我也提一下，还有其他方法如何访问堆栈溢出数据，即有一个公共的<a class="ae ky" href="https://api.stackexchange.com/docs" rel="noopener ugc nofollow" target="_blank"> API </a>允许您查询数据库，还有一个Python <a class="ae ky" href="https://stackapi.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">包装器</a>构建在其上。</p><h1 id="957d" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">数据预处理</h1><p id="6a3f" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">从存档中下载数据并存储在S3上后，我们现在将做一些基本的预处理，为分析查询准备数据。更具体地说，我们将把格式从XML转换为Apache Parquet，并将列重命名/重新键入为更方便的格式。</p><h2 id="3f3c" class="no mk it bd ml nu nv dn mp nw nx dp mt li ny nz mv lm oa ob mx lq oc od mz oe bi translated">正在启动EMR群集</h2><p id="761c" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">对于这个预处理步骤，以及实际的数据分析，我们将使用Spark 3.0和JupyterHub启动一个EMR集群。要启动集群，请转到EMR(在AWS服务中)并点击<em class="ng">创建集群。</em>之后，你就需要指定一堆设置了。转到<em class="ng">高级选项</em>，在那里您可以选择EMR版本并检查集群上将安装哪些应用程序。这里我们使用了<em class="ng"> emr-6.1.0 </em>，我们将需要的关键组件是Hadoop、Spark、JupyterHub和Livy(另见所附截图):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/40afeb904894df808fc8f82573f9c0cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GuBINktVdthoTAwEHAtFzg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者创造的形象。</p></figure><p id="2a34" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在下一步中，您可以配置您的群集将有多少个节点，您可以使用默认设置，但是对于一些查询，您将不得不等待一段时间—它不会有超级响应。换句话说，拥有一个更大的集群会导致更快的查询，但是集群的成本也会更大。在下一步中，您可以为群集选择一个有意义的名称，并取消选中<em class="ng">终止</em>保护。最后，在点击<em class="ng">创建集群</em>之后，它会将您带到一个页面，您会看到您的集群正在启动。接下来，您可以点击<em class="ng">笔记本</em>并创建一个笔记本，您可以从中运行Spark查询。为笔记本选择一个名称，并选择该笔记本将连接到的已创建群集。几秒钟后，您可以点击<em class="ng">打开JupyterLab </em>按钮，打开Jupyter:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/ffd744cc62069916741719637ba1a457.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZrDCpE8ITVCDnuVXCpn_6A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者创造的形象。</p></figure><p id="172c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在JupyterLab中，你会看到你可以打开使用的笔记本。除此之外，您还可以使用菜单中的上传按钮来上传其他笔记本。</p><p id="f10b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于数据预处理和分析，您可以将我的<a class="ae ky" href="https://github.com/davidvrba/Stackoverflow-Data-Analysis" rel="noopener ugc nofollow" target="_blank">存储库</a>克隆到您的本地机器上，并上传这两个笔记本:</p><ul class=""><li id="df69" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu oh mb mc md bi translated">stack overflow-Data-Analysis/Data-prepare/Data-prepare . ipynb</li><li id="972d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu oh mb mc md bi translated">stack overflow-Data-Analysis/Data-Analysis/Posts-General-Analysis . ipynb</li></ul><p id="87c7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在笔记本本身，确保你使用的是正确的内核，你可以在右上角看到(也可以看上面的截图)。出于我们的目的，我们将需要PySpark内核。</p><h2 id="801f" class="no mk it bd ml nu nv dn mp nw nx dp mt li ny nz mv lm oa ob mx lq oc od mz oe bi translated">转换为拼花地板格式</h2><p id="b114" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">让EMR集群运行Spark和Jupyter，我们就可以开始处理数据了。我们要做的第一步是将格式从原始的XML转换成Apache Parquet，这对于Spark中的分析查询来说要方便得多。为了读取Spark SQL中的XML，我们将使用<a class="ae ky" href="https://github.com/databricks/spark-xml" rel="noopener ugc nofollow" target="_blank"> <em class="ng"> spark-xml </em> </a>包，它允许我们指定格式<em class="ng"> xml </em>并将数据读入数据帧</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="5606" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为这个ETL过程的一部分，我们还将所有列重命名为snake-case样式，并将<em class="ng"> creation_date </em>列转换为<em class="ng"> TimestampType </em>。将数据转换为拼花后，大小从85.6 GB减少到30 GB，这是因为拼花压缩，也因为我们没有在最终的拼花中包括所有列。这里我们可以考虑的另一个步骤是按照<em class="ng">创建年份</em>甚至可能是从<em class="ng">创建日期</em>中导出的<em class="ng">创建月份</em>来划分数据。如果我们只想分析数据的某一部分，也许是最后一段时间等等，这将是很方便的。</p><h1 id="f3dd" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">数据分析</h1><p id="cb10" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">最后，我们将进入探索数据并可能揭示其一些有趣属性的部分。我们将回答关于数据集的5个分析问题:</p><h2 id="33d7" class="no mk it bd ml nu nv dn mp nw nx dp mt li ny nz mv lm oa ob mx lq oc od mz oe bi translated">1.计算计数</h2><p id="88ec" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">让我们计算以下有趣的计数:</p><ul class=""><li id="6fa6" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu oh mb mc md bi translated">我们有多少问题</li><li id="f90e" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu oh mb mc md bi translated">有多少个答案</li><li id="0773" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu oh mb mc md bi translated">有多少问题已经接受了答案</li><li id="7031" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu oh mb mc md bi translated">有多少用户提问或回答了一个问题</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="532b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的代码中，我们首先读取了所有的帖子，但是只选择了在进一步的分析中需要的特定列，并且我们缓存了它们，这将非常方便，因为我们将在所有的查询中引用数据集。然后，我们根据<em class="ng"> post_type_id </em>将帖子分成两个数据帧，因为值1代表问题，值2代表答案。帖子总数为53 949 886，其中21 641 802是问题，32 199 928是回答(数据集中还有其他类型的帖子)。当过滤<em class="ng"> accepted_answer_id </em>不为空的问题时，我们得到了具有可接受答案的问题的数量，它是11 138 924。对<em class="ng"> user_id </em>列上的数据集进行重复数据删除，我们得到提问或回答问题的用户总数为5 404 321。</p><h2 id="b35a" class="no mk it bd ml nu nv dn mp nw nx dp mt li ny nz mv lm oa ob mx lq oc od mz oe bi translated">2.计算响应时间</h2><p id="305f" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">我们在这里将响应时间定义为从提出问题到得到被接受的答案所经过的时间。在下面的代码中，您可以看到我们需要将问题与答案连接起来，这样我们就可以将问题的创建日期与其被接受的答案的创建日期进行比较:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="9408" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当根据响应时间对数据进行排序时，我们可以看到最快的可接受答案出现在一秒钟内，您可能会想，为什么有人能如此快速地回答一个问题。我明确地检查了其中的一些问题，发现这些问题是由发布问题的同一个用户回答的，所以显然他/她知道答案，并将其与问题一起发布。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/d8bb6a5d0ce3803275a6721bbb6b5d00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UcXDBGhbmjEAq7MoMap3Hw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者创造的形象。</p></figure><p id="c195" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将响应时间从几秒钟转换为几个小时，并进行汇总，我们可以显示在问题发布后的每个小时内有多少问题得到了回答。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="b941" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里我们将时间限制为24小时，以查看问题发布后第一天内的行为。正如您在图表中看到的，大多数问题都在第一个小时内得到了回答:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/1c77f95cec902013a6302aa4da14dd4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pUvjLJgd4bTSnbBOGPtUWw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者创造的形象。</p></figure><h2 id="fb3b" class="no mk it bd ml nu nv dn mp nw nx dp mt li ny nz mv lm oa ob mx lq oc od mz oe bi translated">3.查看问题/答案的数量如何随着时间的推移而变化</h2><p id="f551" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">观察每个时间单位的问题和答案的数量如何随着时间的推移而变化是非常有趣的。要使用Spark计算聚合，我们可以使用<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.window.html#pyspark.sql.functions.window" rel="noopener ugc nofollow" target="_blank"> <em class="ng"> window() </em> </a>函数进行分组，它有两个参数，第一个是具有时间含义的列的名称，第二个是我们希望对时间维度进行分组的时间段。这里我们选择时间单位为一周。在聚合本身中，我们可以使用<em class="ng"> when </em>条件计算答案和问题，如下所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="620e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">按窗口分组创建了一个具有两个<em class="ng">结构字段的<em class="ng">结构类型</em>,</em><em class="ng">开始、</em>和<em class="ng">结束</em>，因此在这里为了绘图，我们使用了<em class="ng">开始</em>字段，它是每周开始的日期。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/1e41235742dd45b3a097934734e30017.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rPsudFDL1O-NwEOdmmaeuA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者创造的形象。</p></figure><h2 id="3d58" class="no mk it bd ml nu nv dn mp nw nx dp mt li ny nz mv lm oa ob mx lq oc od mz oe bi translated">4.计算标签的数量</h2><p id="5f76" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">每个问题都有一些代表一些主题的标签—在堆栈溢出数据集中，这些通常是用户询问的一些技术。然而，标签是以字符串的形式存储的，格式为<em class="ng">&lt;tag 1&gt;&lt;tag 2&gt;&lt;…&gt;</em>，所以为了分析它们，将这个字符串转换成这些标签的数组并去掉尖括号是很有用的。为此，我们将首先使用<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.split.html#pyspark.sql.functions.split" rel="noopener ugc nofollow" target="_blank"> <em class="ng"> split </em> </a>函数拆分字符串，之后，我们将使用高阶函数<a class="ae ky" rel="noopener" target="_blank" href="/higher-order-functions-with-spark-3-1-7c6cf591beaa"> <em class="ng"> TRANSFORM </em> </a>使用<a class="ae ky" href="https://spark.apache.org/docs/latest/api/sql/index.html#regexp_replace" rel="noopener ugc nofollow" target="_blank"><em class="ng">regexp _ replace</em></a>删除数组中每个元素的尖括号。为了计算所有标签的总非重复计数，我们将最终<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.explode.html#pyspark.sql.functions.explode" rel="noopener ugc nofollow" target="_blank"> <em class="ng">展开</em> </a>该数组并对其进行重复数据删除:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="6bb7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">计数的结果显示61 662个不同的标签。要查看哪些标签被使用的次数最多，我们只需按标签对展开的数据集进行分组，计算每个标签的数量，并按计算出的频率进行排序。这就产生了这个列表，从这个列表中可以看出使用最多的标签是javascript:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/6fbfc38ca21bdde2721c8d4921951c3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-kM26_M7y4Ez7zoqwKgJ_g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者创造的形象。</p></figure><h2 id="eb5d" class="no mk it bd ml nu nv dn mp nw nx dp mt li ny nz mv lm oa ob mx lq oc od mz oe bi translated">5.查看一些特定标签的流行程度</h2><p id="9f47" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">将tags列转换为一个数组后，我们现在可以过滤这个数组并研究特定的标记。这里我们将看看人们用标签<em class="ng"> apache-spark </em>或<em class="ng"> apache-spark-sql </em>问了多少问题。为此，我们可以使用函数<a class="ae ky" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.array_contains.html#pyspark.sql.functions.array_contains" rel="noopener ugc nofollow" target="_blank"> <em class="ng"> array_contains </em> </a>，该函数有两个参数，第一个参数是数组的列，第二个参数是一个特定的元素，我们希望确定它是否包含在数组中。该函数返回<em class="ng">真</em>或<em class="ng">假</em>，因此我们可以在过滤器内部直接使用它。像这样过滤问题后，我们可以根据<em class="ng">创建日期</em>按周对它们进行分组。最后，我们将其转换为熊猫数据帧并绘制成图:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><p id="d95a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最终的曲线图显示，在Stack Overflow上，人们在2014年开始对Spark感兴趣，每周被问及最多的问题是在2017年和2018年:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/fa7eec601e5019cebbd42f7a94052cc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6jAEHGCAJ5HkKoic9wP90w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者创造的形象。</p></figure><h1 id="292d" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">结论</h1><p id="18d6" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">在本文中，我们看到了如何使用Spark来分析公开可用的数据集，但是，我们首先需要下载、解压缩并存储在某个分布式存储系统中，这样我们就可以使用Spark有效地访问它。</p><p id="bb69" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">堆栈溢出数据集包含大量有趣的信息，我这样说不仅仅是指实际答案形式的内容，还包括问题、答案、评论和用户之间一些有趣的关系。在本文中，我们展示的只是冰山一角，我们已经看到了这个数据集上的一些基本查询，在未来的一些帖子中，我们将回到它并应用一些更高级的分析工具，如使用Spark的机器学习和图形处理来揭示数据的更多属性。</p></div></div>    
</body>
</html>