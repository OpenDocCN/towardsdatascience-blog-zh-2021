<html>
<head>
<title>What Does “Safety” in “AI Safety” Actually Mean?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">“AI安全”中的“安全”实际上是什么意思？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-does-safety-in-ai-safety-actually-mean-5aeae833469d?source=collection_archive---------36-----------------------#2021-05-19">https://towardsdatascience.com/what-does-safety-in-ai-safety-actually-mean-5aeae833469d?source=collection_archive---------36-----------------------#2021-05-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="74e1" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">三个复杂的问题</h2></div><p id="aed4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在TDS播客的最近一期节目中，嘉宾Rosie Campbell是AI Partnership on AI的安全关键AI负责人，他与T2的Jeremie Harris聊起了发表前沿人工智能研究的潜在风险。我承认，在听他们的对话之前，这是我从未想过的问题；首先，我绝对是<em class="le">而不是</em>人工智能研究者，所以我从来不需要这么做。</p><p id="4457" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，一个更深层次的原因是我多年来构建的关于危险的人工智能可能会是什么样子的心理图像。流行文化已经让我(我猜想，你们中的许多人也是如此)习惯于想象一个像HAL 9000那样邪恶的没有身体的意识，或者一个看起来傻傻的、可能致命的星球大战机器人。相比之下，现实世界、当今的人工智能风险更加抽象，也更加广泛。</p><p id="f957" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你和我一样，想了解更多关于这些风险的各种形式，这里有一些我认为特别有用的TDS资源，虽然不多，但很有效。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/43317d3fb6cd2a522fc91f8f16422c9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0YcZs505sAoGe66r"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">照片由<a class="ae lb" href="https://unsplash.com/@trinhhuyhung?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Huy-Hung Trinh </a>在<a class="ae lb" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h2 id="02f5" class="lv lw iq bd lx ly lz dn ma mb mc dp md ko me mf mg ks mh mi mj kw mk ml mm mn bi translated"><a class="ae lb" rel="noopener" target="_blank" href="/should-all-ai-research-be-published-5226ad5145b4">所有的AI研究都应该发表吗？</a></h2><p id="4a36" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">我上面提到的TDS播客插曲探索了人工智能研究社区必须导航的内在张力:一方面，使知识可访问和自由流动的愿望；另一方面，在坏人试图利用新技术进步达到邪恶目的的情况下，降低风险的竞争需要。Rosie Campbell和Jeremie Harris以OpenAI的GPT-2及其棘手的初始版本为例开始了对话，并继续就人工智能学术出版中的道德实践进行了更广泛的讨论。</p><div class="mt mu gp gr mv mw"><a rel="noopener follow" target="_blank" href="/should-all-ai-research-be-published-5226ad5145b4"><div class="mx ab fo"><div class="my ab mz cl cj na"><h2 class="bd ir gy z fp nb fr fs nc fu fw ip bi translated">所有的AI研究都应该发表吗？</h2><div class="nd l"><h3 class="bd b gy z fp nb fr fs nc fu fw dk translated">罗西·坎贝尔论人工智能中负责任的研究和出版规范</h3></div><div class="ne l"><p class="bd b dl z fp nb fr fs nc fu fw dk translated">towardsdatascience.com</p></div></div><div class="nf l"><div class="ng l nh ni nj nf nk lp mw"/></div></div></a></div><h2 id="48ba" class="lv lw iq bd lx ly lz dn ma mb mc dp md ko me mf mg ks mh mi mj kw mk ml mm mn bi translated"><a class="ae lb" rel="noopener" target="_blank" href="/democratization-of-ai-de155f0616b5">AI的民主化</a></h2><p id="904f" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">作为普华永道(PricewaterhouseCoopers)人工智能的全球负责人，<a class="lc ld ep" href="https://medium.com/u/e1679be51455?source=post_page-----5aeae833469d--------------------------------" rel="noopener" target="_blank"> AnandSRao </a>非常清楚大规模人工智能的潜在好处和风险。随着人工智能在组织中变得越来越容易获得，Anand注意到滥用的媒介也在增长:</p><blockquote class="nl nm nn"><p id="a7b6" class="kf kg le kh b ki kj jr kk kl km ju kn no kp kq kr np kt ku kv nq kx ky kz la ij bi translated">即使是由高素质工程师设计的最复杂的人工智能系统，<a class="ae lb" href="https://www.strategy-business.com/article/What-is-fair-when-it-comes-to-AI-bias" rel="noopener ugc nofollow" target="_blank">也可能成为偏见</a>、可解释性问题和其他<a class="ae lb" href="https://www.pwc.com/gx/en/issues/data-and-analytics/artificial-intelligence/what-is-responsible-ai.html" rel="noopener ugc nofollow" target="_blank">缺陷</a>的牺牲品。由没有经过适当培训的人构建的人工智能系统，或者在没有适当控制的情况下运行的人工智能系统，可能会产生非常危险的东西——引入歧视或严重错误。更糟糕的是，问题可能直到系统实施后才变得明显，让公司争先恐后地安抚利益相关者，消除损害，修复技术。</p></blockquote><p id="fc1d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">他提出的解决方案是一个微妙的民主化框架——一个考虑到技术、商业和不同利益相关者之间复杂相互作用的框架。在这里不可能做到公正，所以只要跳到Anand的帖子里去更详细地了解它。</p><div class="mt mu gp gr mv mw"><a rel="noopener follow" target="_blank" href="/democratization-of-ai-de155f0616b5"><div class="mx ab fo"><div class="my ab mz cl cj na"><h2 class="bd ir gy z fp nb fr fs nc fu fw ip bi translated">人工智能民主化</h2><div class="nd l"><h3 class="bd b gy z fp nb fr fs nc fu fw dk translated">双刃剑</h3></div><div class="ne l"><p class="bd b dl z fp nb fr fs nc fu fw dk translated">towardsdatascience.com</p></div></div><div class="nf l"><div class="nr l nh ni nj nf nk lp mw"/></div></div></a></div><h2 id="f7ea" class="lv lw iq bd lx ly lz dn ma mb mc dp md ko me mf mg ks mh mi mj kw mk ml mm mn bi translated"><a class="ae lb" rel="noopener" target="_blank" href="/the-global-artificial-intelligence-race-and-strategic-balance-which-race-are-we-running-e0176b2349e5">全球人工智能竞赛和战略平衡</a></h2><p id="69fa" class="pw-post-body-paragraph kf kg iq kh b ki mo jr kk kl mp ju kn ko mq kq kr ks mr ku kv kw ms ky kz la ij bi translated">非专家读者(是的，我说的是我自己)可能需要进行一些思维跳跃，才能理解学术研究的发表会产生不可预见的负面结果。人工智能的军事用途并非如此，当前的危险与我们科幻小说中的想象惊人地重叠。夏洛特·利维最近的帖子深入探讨了全球利用人工智能获得军事力量的争夺战——以及其他人限制其鲁莽、毁灭性行为潜力的尝试。</p><div class="mt mu gp gr mv mw"><a rel="noopener follow" target="_blank" href="/the-global-artificial-intelligence-race-and-strategic-balance-which-race-are-we-running-e0176b2349e5"><div class="mx ab fo"><div class="my ab mz cl cj na"><h2 class="bd ir gy z fp nb fr fs nc fu fw ip bi translated">全球人工智能竞赛和战略平衡:我们正在进行哪场竞赛？</h2><div class="nd l"><h3 class="bd b gy z fp nb fr fs nc fu fw dk translated">人工智能增强技术可能会带来前所未有的新安全风险。…</h3></div><div class="ne l"><p class="bd b dl z fp nb fr fs nc fu fw dk translated">towardsdatascience.com</p></div></div><div class="nf l"><div class="ns l nh ni nj nf nk lp mw"/></div></div></a></div></div><div class="ab cl nt nu hu nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="ij ik il im in"><p id="36de" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你从事人工智能安全方面的工作，我们很乐意听到你的意见——请在评论中留下你的工作的链接，或者更好的是— <a class="ae lb" rel="noopener" target="_blank" href="/questions-96667b06af5">为TDS </a>写点东西！关于人工智能安全和风险的其他深思熟虑的讨论，<a class="ae lb" href="https://towardsdatascience.com/tagged/tds-podcast" rel="noopener" target="_blank">前往TDS播客</a>。</p></div></div>    
</body>
</html>