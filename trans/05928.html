<html>
<head>
<title>Interpretable or Accurate? Why Not Both?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可解释的还是准确的？为什么不两者都要？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/interpretable-or-accurate-why-not-both-4d9c73512192?source=collection_archive---------18-----------------------#2021-05-27">https://towardsdatascience.com/interpretable-or-accurate-why-not-both-4d9c73512192?source=collection_archive---------18-----------------------#2021-05-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3f44" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用IntepretML构建可解释的Boosting模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a26a4bd80688aab555ee3f2ceaed947a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kWy3VIPUPhLGl4G2-ccTPw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由<a class="ae ky" href="https://pixabay.com/users/kingrise-4297632/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3183317" rel="noopener ugc nofollow" target="_blank"> Kingrise </a>来自<a class="ae ky" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=3183317" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><p id="777b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如<a class="ae ky" href="https://arxiv.org/abs/1706.07269" rel="noopener ugc nofollow" target="_blank">米勒</a>所总结的，可解释性指的是人类能够理解决策原因的程度。机器学习社区中的一个常见概念是准确性和可解释性之间存在权衡。这意味着越精确的学习方法提供的可解释性越少，反之亦然。然而，最近，有很多的重点放在创造内在可解释的模型，并远离他们的黑盒对应。事实上，辛西娅·鲁丁认为<a class="ae ky" href="https://www.nature.com/articles/s42256-019-0048-x" rel="noopener ugc nofollow" target="_blank">可解释的黑盒应该完全避免用于对人类生活产生深远影响的高风险预测应用</a>。所以，问题是一个模型是否可以在不牺牲可解释性的前提下具有更高的准确性？</p><p id="9d01" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">EBMs正是试图填补这一空白。EBM代表<a class="ae ky" href="https://www.youtube.com/watch?v=MREiHgHgl0k" rel="noopener ugc nofollow" target="_blank">可解释的Boosting Machine </a>，是一种模型，其精确度可与随机森林和Boosted Trees等最先进的机器学习方法相媲美，同时具有高度的可理解性和可解释性。</p><p id="1843" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文将着眼于EBM背后的思想，并通过<a class="ae ky" href="https://arxiv.org/pdf/1909.09223.pdf" rel="noopener ugc nofollow" target="_blank"> InterpretML </a>(机器学习可解释性的统一框架)将它们实现为一个人力资源案例研究。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="da90" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">机器学习的可解释性—初级读本</h1><p id="1dff" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">机器学习是一种强大的工具，正在越来越多地以多方面的方式应用于多个行业。人工智能模型越来越多地用于做出影响人们生活的决策。因此，预测必须是公平的，没有偏见或歧视。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/e11e36848520d62e9f8239db8cb48083.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/format:webp/1*0r29CPjHgDAVwyMki8LSzg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在pipeline | Image中拥有机器学习可解释性的优势</p></figure><p id="367d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这种情况下，机器学习的可解释性起着至关重要的作用。可解释性不仅能让你发现模型的错误预测，还能分析和修复潜在的原因。可解释性可以帮助您调试您的模型，检测过度拟合和数据泄漏，最重要的是，通过给出解释来激发模型和人类之间的信任。</p><h2 id="1528" class="na md it bd me nb nc dn mi nd ne dp mm li nf ng mo lm nh ni mq lq nj nk ms nl bi translated">可解释性方法</h2><p id="deae" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">根据机器学习模型的类型，用来解释模型预测的方法可以分为两大类。</p><h2 id="ef4b" class="na md it bd me nb nc dn mi nd ne dp mm li nf ng mo lm nh ni mq lq nj nk ms nl bi translated">1.玻璃盒模型与黑盒解释</h2><p id="8e67" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">被设计成可解释的算法被称为玻璃盒子模型。这些算法包括简单决策树、规则列表、线性模型等。玻璃箱方法通常提供精确或无损的可解释性。这意味着可以追踪和推理任何预测是如何做出的。GlassBox模型的解释是<strong class="lb iu">特定于模型的</strong>,因为每种方法都基于一些特定的模型内部。例如，线性模型中的权重解释计入<a class="ae ky" href="https://christophm.github.io/interpretable-ml-book/taxonomy-of-interpretability-methods.html" rel="noopener ugc nofollow" target="_blank">特定模型的解释</a>。</p><p id="474a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相反，黑盒解释者是<strong class="lb iu">模型不可知论者</strong>。它们可以应用于任何模型，并且本质上是事后的，因为它们是在模型被训练之后应用的。黑盒解释器通过将模型视为黑盒来工作，并假设他们只能访问模型的输入和输出。它们对于复杂的算法特别有用，比如提升树和深度神经网络。黑盒解释器通过反复扰动输入和分析模型输出的结果变化来工作。例子包括<a class="ae ky" href="https://arxiv.org/abs/1705.07874" rel="noopener ugc nofollow" target="_blank"> SHAP </a>、<a class="ae ky" href="https://arxiv.org/abs/1602.04938v3" rel="noopener ugc nofollow" target="_blank">石灰</a>、<a class="ae ky" href="https://christophm.github.io/interpretable-ml-book/pdp.html" rel="noopener ugc nofollow" target="_blank">部分依赖图</a> s等。，不一而足。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/7ef272ec1d80778be67e7f38f1a4f81d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/format:webp/1*8ov3dWV39WHkx8SG6pMXWA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">玻璃盒子与黑盒可解释方法|作者图片</p></figure><h2 id="55fa" class="na md it bd me nb nc dn mi nd ne dp mm li nf ng mo lm nh ni mq lq nj nk ms nl bi translated">2.本地与全球解释</h2><p id="6d7c" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">另一类可能取决于解释的范围。局部解释旨在解释单个预测，而全局解释则解释整个模型行为。</p><p id="5cab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">既然我们对机器学习模型所采用的可解释性机制有了足够的直觉，让我们换个角度，更详细地理解EBM。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="53e7" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">可解释增压机</h1><p id="5a3a" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated"><a class="ae ky" href="https://dl.acm.org/doi/10.1145/2339530.2339556" rel="noopener ugc nofollow" target="_blank">EBM</a>是玻璃箱模型，其设计精度可与最先进的机器学习方法相媲美，而不会影响精度和可解释性<em class="nn">。</em></p><p id="14c1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">EBM是一种<a class="ae ky" href="https://projecteuclid.org/journals/statistical-science/volume-1/issue-3/Generalized-Additive-Models/10.1214/ss/1177013604.full" rel="noopener ugc nofollow" target="_blank">广义加性模式</a> l或简称GAM。线性模型假设反应和预测之间存在线性关系。因此，他们无法捕捉数据中的非线性。</p><p id="424a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe no np nq nr b">Linear Model: y = β0 + β1x1 + β2x2 + … + βn xn</code></p><p id="5393" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了克服这个缺点，在80年代后期，统计学家<a class="ae ky" href="https://projecteuclid.org/journals/statistical-science/volume-1/issue-3/Generalized-Additive-Models/10.1214/ss/1177013604.full" rel="noopener ugc nofollow" target="_blank"> Hastie &amp; Tibshirani开发了广义加法模型</a> (GAMs)，它保持了加法结构，因此保持了线性模型的可解释性。因此，响应和预测变量之间的线性关系<a class="ae ky" href="https://datascienceplus.com/generalized-additive-models/" rel="noopener ugc nofollow" target="_blank">被几个非线性平滑函数</a> (f1，f2等)所代替。)来模拟和捕捉数据中的非线性。gam比简单的线性模型更精确，因为它们不包含特征之间的任何交互，用户也可以很容易地解释它们。</p><p id="6b9f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe no np nq nr b">Additive Model: y = <strong class="lb iu">f1</strong>(x1) + <strong class="lb iu">f2</strong>(x2) + … + <strong class="lb iu">fn</strong> (xn)</code></p><p id="f94e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">EBM是利用梯度推进和装袋等技术对gam的改进。EBM包括成对的相互作用项，这进一步提高了它们的准确性。</p><p id="9ac5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe no np nq nr b">EBMs: y = Ʃi <strong class="lb iu">fi</strong> (xi) + Ʃij <strong class="lb iu">fij(xi , xj) </strong>+ Ʃijk <strong class="lb iu">fijk (xi , xj , xk )</strong></code></p><p id="c70b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">EBM的创始人理查德·卡鲁阿纳(Richard Caruana)的以下演讲深入探讨了算法背后的直觉。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ns nt l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">解释背后的科学:可解释的助推机器</p></figure><p id="1b90" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里要注意的重要一点是，即使在所有这些改进之后，EBM仍然保留了线性模型的可解释性，但通常与强大的黑盒模型的准确性相当，如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nu"><img src="../Images/7c06e480c48adba16c4c66ab6dc1c979.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-gnKXfPsi5FHYcPiaLK50Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">跨数据集(行、列)的模型分类性能|官方论文:<a class="ae ky" href="https://arxiv.org/pdf/1909.09223.pdf" rel="noopener ugc nofollow" target="_blank"> InterpretML:机器学习可解释性的统一框架</a></p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="cd4a" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">案例研究:使用机器学习预测员工流失</h1><blockquote class="nv nw nx"><p id="ab35" class="kz la nn lb b lc ld ju le lf lg jx lh ny lj lk ll nz ln lo lp oa lr ls lt lu im bi translated">这是代码笔记本的<a class="ae ky" href="https://nbviewer.jupyter.org/github/parulnith/Data-Science-Articles/blob/main/Interpretable%20or%20Accurate%3F%20Why%20not%C2%A0both/Interpretable%20or%20Accurate%3F%20Why%20not%C2%A0both.ipynb" rel="noopener ugc nofollow" target="_blank"> nbviewer链接</a>,以防你想跟进。</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/5df320e03191239f1cdfc2b0380a99e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*289fHah3E3BX9CkKrIJegw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://pixabay.com/photos/get-me-out-escape-danger-security-1605906/" rel="noopener ugc nofollow" target="_blank">来自皮沙贝的安德鲁·马丁</a></p></figure><p id="f3bc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">是时候把手弄脏了。在本节中，我们将训练一个EBM模型来预测员工流失。我们还将比较EBMs与其他算法的性能。最后，我们将尝试解释我们的模型在一个叫做InterpretML的工具的帮助下做出的预测。什么是interpretML？让我们找出答案。</p><h2 id="3b60" class="na md it bd me nb nc dn mi nd ne dp mm li nf ng mo lm nh ni mq lq nj nk ms nl bi translated">IntepretML:机器学习可解释性的统一框架</h2><p id="e6b4" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">EBM被打包在一个名为<a class="ae ky" href="http://Unified Framework for Machine Learning Interpretability" rel="noopener ugc nofollow" target="_blank"> InterpretML </a>的机器学习可解释性工具包中。t是一个用于训练可解释模型以及解释黑盒系统的开源包。在InterpretML中，可解释性算法被组织成两个主要部分，即<strong class="lb iu">玻璃盒模型</strong>和<strong class="lb iu">黑盒解释</strong>。这意味着该工具不仅可以解释固有的可解释模型的决策，还可以为黑盒模型提供可能的推理。以下来自官方论文<a class="ae ky" href="https://arxiv.org/pdf/1909.09223.pdf" rel="noopener ugc nofollow" target="_blank">的代码架构很好地总结了这一点。</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/2332e280e5e29dacaace3ea6658fdc8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MxM1QHK31w16F9U0d5t7CQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来自官方论文的代码架构|来源:<a class="ae ky" href="https://arxiv.org/pdf/1909.09223.pdf" rel="noopener ugc nofollow" target="_blank"> InterpretML:机器学习可解释性的统一框架</a></p></figure><p id="6d38" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">按照作者的说法，InterpretML遵循四个基本设计原则:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/20d00a7f1e32d1bf4d000d7203b27f34.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3KXqAPM9YmONgN9wZf0x5g.png"/></div></div></figure><p id="7851" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">InterpretML还提供了一个交互式可视化仪表板。仪表板提供了关于数据集性质、模型性能和模型解释的有价值的见解。</p><h2 id="f173" class="na md it bd me nb nc dn mi nd ne dp mm li nf ng mo lm nh ni mq lq nj nk ms nl bi translated">资料组</h2><p id="7210" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">我们将使用公开发布的<a class="ae ky" href="https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset" rel="noopener ugc nofollow" target="_blank"> IBM HR Analytics员工流失&amp;绩效</a>数据集。该数据集包含关于雇员的年龄、部门、性别、教育水平等数据。，以及关于雇员是否离开公司的信息，由可变损耗表示。“否”表示没有离开公司的员工，“是”表示离开公司的员工。我们将使用该数据集构建一个分类模型来预测员工的流失概率。</p><p id="d235" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是数据集要素的快照。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/4594f50c67997ba5166d0a48d73fa1fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*lVoiOjtNnnD8QgJBiEDCgQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据集的特征|按作者分类的图像</p></figure><p id="2203" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如上所述，InterpretML支持训练可解释模型(<strong class="lb iu">玻璃盒子</strong>)，以及解释现有的ML管道(<strong class="lb iu">黑盒</strong>)，并且跨Windows、Mac和Linux得到支持。目前，该软件包支持以下算法:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/ff853171a756f14d40fb096cad641ec1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*n4r1n6T5p0f6c3AJUtWEEg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者的解释图像支持的算法</p></figure><p id="5fde" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">探索数据集</strong></p><p id="99fb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一项任务总是探索数据集并理解各列的分布。InterpretML为分类问题提供了直方图可视化。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/1ecb03b86b02d7a161e86d072ee42441.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VU6WNodpuUDh0K6zPe3OeA.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/c063d3109842744f8ad06f2a87f0b659.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*G67PKzjun3wszNTHnWQTHg.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">直方图可视化|作者提供的图像</p></figure><p id="4684" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">训练模型</strong></p><p id="9412" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用InterpretML训练EBM相对容易。在预处理我们的数据集并将其分成训练集和测试集之后，下面几行代码就完成了这项工作。InterpretML符合大家熟悉的scikit learn API。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/fc48f745d44ff165c5dfb56aabcbc272.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wzfPy8pYgK0HL4ypdy4-5w.png"/></div></div></figure><p id="f4a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦模型被训练，我们就可以从全局和局部的角度可视化和理解模型的行为。</p><p id="083f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">全局解释</strong></p><p id="00fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">全局解释有助于更好地理解模型的整体行为和总体中的一般模型行为。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/70766e56b5baa1ec936e95da3c985f57.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9MeZEKvWkmXYkFJ1my29TQ.png"/></div></div></figure><p id="b6bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们看到的第一张图表是汇总图，它表明<code class="fe no np nq nr b">Overtime</code>变量是决定某人是否会离开公司的最关键因素。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/9348a40108a3bbf8f107bddf46510807.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*j_mIItfKqWYn-wZtsccKOQ.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">查看全球解释|按作者分类的图片</p></figure><p id="7ce2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们还可以通过向下钻取更深入地查看每个特征图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/72624a2b6e97a1cac4624f3b84656524.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-FhB-jPrPGb5Y4v31voP7w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">年龄对磨损的影响|作者图片</p></figure><p id="c432" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里的分数指的是逻辑，因为这个问题是一个分类问题。你在y轴上的位置越高，你离开公司的几率就越高。然而，在35岁左右，这种行为改变了，你有更多的机会留在后面。</p><p id="42eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">本地解释</strong></p><p id="06dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">局部解释有助于我们理解个别预测背后的原因，以及为什么会做出特定的预测。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/1035cbbb193d88e6d0ea7e80937bad27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VWPy7AeBbpcJH8hqOjgG9w.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/42384492443c4c6104b58bdd78e8542d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*giADRNtdAvltV_E_L59BWg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">查看本地解释|作者图片</p></figure><p id="0b16" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">与其他型号的性能比较</strong></p><p id="e4d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">比较不同算法的性能并以仪表板格式显示结果也很容易。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/e5f9ef6ca8a1d5c03af9bdacc34fe032.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rge1s00o89Id6o6J1pTntA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">比较仪表板|按作者分类的图像</p></figure><p id="3de3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">训练黑盒模型</strong></p><p id="2a2e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果需要，InterpretML还可以训练黑盒模型，并为预测提供解释。以下是在同一数据集上训练的随机森林分类器模型的示例，以及LIME随后提供的解释。从下图可以看出，EBM的性能比随机森林好得多。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/ea057558ce1fc5093c31b3fb118c5887.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jhruLFrrbUv020peVaLZHA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">分析黑盒模型|作者图片</p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="e254" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">结论</h1><p id="6c9a" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">本文展示了EBM如何成为创建可解释的和精确的模型的最佳选择。就个人而言，当机器学习模型用于高风险决策时，可解释性应该比损失几个点的准确性得到更高的优先考虑。不仅重要的是看一个模型是否有效，我们作为机器学习从业者也应该关心它是如何工作的，以及它是否没有任何故意的偏见。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="4289" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">参考</h1><p id="9bb5" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">本文引用了大量的资源和论文，并在文章中进行了链接。然而，第一手资料来源是InterpretML 的<a class="ae ky" href="https://arxiv.org/pdf/1909.09223.pdf" rel="noopener ugc nofollow" target="_blank">官方文件及其</a><a class="ae ky" href="https://interpret.ml/docs/ebm.html" rel="noopener ugc nofollow" target="_blank">文档</a>。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="443c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nn">👉有兴趣看我写的其他文章。这个</em> <a class="ae ky" href="https://github.com/parulnith/Data-Science-Articles/blob/main/README.md" rel="noopener ugc nofollow" target="_blank"> <em class="nn">回购</em> </a> <em class="nn">包含了我分类写的所有文章。</em></p></div></div>    
</body>
</html>