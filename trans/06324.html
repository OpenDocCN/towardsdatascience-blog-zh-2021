<html>
<head>
<title>TabNet: The End of Gradient Boosting?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">TabNet:梯度推进的终结？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tabnet-e1b979907694?source=collection_archive---------5-----------------------#2021-06-07">https://towardsdatascience.com/tabnet-e1b979907694?source=collection_archive---------5-----------------------#2021-06-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="98c0" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">TabNet平衡了表格数据的可解释性和模型性能，但是它能取代boosted树模型吗？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c98f1e6fb56fa64b7b9f482cd5fa5489.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_DKm0iz5fATChDHjlWqNVQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">TabNet模型架构。图片作者。灵感来自<a class="ae ky" href="https://arxiv.org/pdf/1908.07442.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1908.07442.pdf</a>。</p></figure><h1 id="e332" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="bf71" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">XGBoost、LightGBM和Catboost等梯度推进模型一直被认为是<a class="ae ky" href="https://www.kaggle.com/shivamb/data-science-trends-on-kaggle" rel="noopener ugc nofollow" target="_blank">表格数据领域的最佳选择</a>。即使在NLP和计算机视觉方面取得了快速进展，神经网络仍然经常被基于表格数据的树型模型超越。</p><p id="2186" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">2019年进入谷歌的<a class="ae ky" href="https://arxiv.org/abs/1908.07442" rel="noopener ugc nofollow" target="_blank"> TabNet </a>。根据这篇论文，这个神经网络能够<strong class="lt iu">在各种基准测试中胜过领先的基于树的模型</strong>。不仅如此，它比增强的树模型更容易解释，因为它有内置的解释能力。也可以不经过任何特征预处理使用<strong class="lt iu">。如果是这样的话…为什么它没有流行起来？</strong></p><blockquote class="ms"><p id="b3a8" class="mt mu it bd mv mw mx my mz na nb mm dk translated">TabNet平衡了可解释性和最先进的性能。它易于实现，并且需要有限的超参数调整。那么为什么XGBoost仍然是Kaggle特级大师的首选武器呢？</p></blockquote><p id="867a" class="pw-post-body-paragraph lr ls it lt b lu nc ju lw lx nd jx lz ma ne mc md me nf mg mh mi ng mk ml mm im bi translated">本文研究了TabNet的理论，并展示了如何实现该模型的一些例子。</p><h2 id="3c55" class="nh la it bd lb ni nj dn lf nk nl dp lj ma nm nn ll me no np ln mi nq nr lp ns bi translated">先决条件</h2><p id="a263" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这篇文章是给你的，如果…</p><ul class=""><li id="4a86" class="nt nu it lt b lu mn lx mo ma nv me nw mi nx mm ny nz oa ob bi translated">你知道什么是神经网络，它是如何工作的。</li><li id="7e83" class="nt nu it lt b lu oc lx od ma oe me of mi og mm ny nz oa ob bi translated">你了解诸如<em class="oh">批量标准化</em>、<em class="oh"> ReLU </em>和G <em class="oh">梯度下降</em>等术语。</li><li id="3a77" class="nt nu it lt b lu oc lx od ma oe me of mi og mm ny nz oa ob bi translated">你在神经网络中遇到过<em class="oh">注意力</em>的概念。</li></ul><h1 id="766d" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">目录</h1><ul class=""><li id="2a2e" class="nt nu it lt b lu lv lx ly ma oi me oj mi ok mm ny nz oa ob bi translated"><a class="ae ky" href="#07d0" rel="noopener ugc nofollow">什么是TabNet </a> <br/> - <a class="ae ky" href="#79c3" rel="noopener ugc nofollow">概述</a> <br/> - <a class="ae ky" href="#2b7f" rel="noopener ugc nofollow">它是如何工作的？</a></li><li id="c409" class="nt nu it lt b lu oc lx od ma oe me of mi og mm ny nz oa ob bi translated"><a class="ae ky" href="#66f0" rel="noopener ugc nofollow">实现</a> <br/> - <a class="ae ky" href="#fa51" rel="noopener ugc nofollow">代码</a>-<br/>-<a class="ae ky" href="#173f" rel="noopener ugc nofollow">为什么是可交代的？</a> <br/> - <a class="ae ky" href="#ee15" rel="noopener ugc nofollow">自我监督学习</a></li><li id="6307" class="nt nu it lt b lu oc lx od ma oe me of mi og mm ny nz oa ob bi translated"><a class="ae ky" href="#7ffa" rel="noopener ugc nofollow">结论</a></li></ul><h1 id="07d0" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">什么是TabNet？</h1><h2 id="79c3" class="nh la it bd lb ni nj dn lf nk nl dp lj ma nm nn ll me no np ln mi nq nr lp ns bi translated">概观</h2><ol class=""><li id="a91e" class="nt nu it lt b lu lv lx ly ma oi me oj mi ok mm ol nz oa ob bi translated">TabNet输入未经任何预处理的原始表格数据<strong class="lt iu"/>，并使用基于<strong class="lt iu">梯度下降</strong>的优化进行训练。</li><li id="f9ad" class="nt nu it lt b lu oc lx od ma oe me of mi og mm ol nz oa ob bi translated">TabNet使用<strong class="lt iu">顺序注意力</strong>在每个决策步骤中选择特征，启用<strong class="lt iu">可解释性</strong>和更好的学习，因为<strong class="lt iu">学习能力用于最有用的特征。</strong></li><li id="9ba1" class="nt nu it lt b lu oc lx od ma oe me of mi og mm ol nz oa ob bi translated"><strong class="lt iu">特征选择是基于实例的</strong>，例如，对于训练数据集的每一行，它可以是不同的。</li><li id="0246" class="nt nu it lt b lu oc lx od ma oe me of mi og mm ol nz oa ob bi translated">TabNet采用<strong class="lt iu">单一深度学习架构</strong>进行特征选择和推理，这被称为软特征选择。</li><li id="9200" class="nt nu it lt b lu oc lx od ma oe me of mi og mm ol nz oa ob bi translated">上述设计选择允许TabNet启用两种可解释性:<strong class="lt iu">局部</strong> <strong class="lt iu">可解释性</strong>，它可视化了特征的重要性以及如何将它们组合成一行；以及<strong class="lt iu">全局</strong> <strong class="lt iu">可解释性</strong>，它量化了每个特征对整个数据集的训练模型的贡献。</li></ol><h2 id="2b7f" class="nh la it bd lb ni nj dn lf nk nl dp lj ma nm nn ll me no np ln mi nq nr lp ns bi translated">要点</h2><p id="180b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">尽管它提供了可解释性，但这是一个复杂的模型。我将尝试总结主要概念，但我强烈推荐阅读<a class="ae ky" href="https://arxiv.org/pdf/1908.07442.pdf" rel="noopener ugc nofollow" target="_blank">原始TabNet论文</a>以获得技术细节。</p><p id="e116" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">下面的架构图概述了TabNet中的不同组件。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c98f1e6fb56fa64b7b9f482cd5fa5489.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_DKm0iz5fATChDHjlWqNVQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">TabNet模型架构。图片作者。受到https://arxiv.org/pdf/1908.07442.pdf的启发。红线是为了防止重叠箭头造成的混淆。</p></figure><p id="0d0f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">步骤</strong></p><p id="e852" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">每个<strong class="lt iu">步骤</strong>都是一组组件。训练模型时，<strong class="lt iu">步数</strong>是一个超参数选项。增加这将增加模型的<em class="oh">学习能力</em>，但也会增加训练时间、内存使用和过度拟合的机会。</p><p id="aed2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">每个<strong class="lt iu">步骤</strong>在最终分类中获得自己的投票，这些投票的权重相等。这模仿了一个<em class="oh">集合分类。</em></p><p id="b010" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">特征转换器</strong></p><p id="b370" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">特征转换器</strong>是一个拥有自己架构的网络。</p><p id="6e8a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">它有多个层次，其中一些是每个<strong class="lt iu">步骤</strong>共有的，而另一些是每个<strong class="lt iu">步骤</strong>独有的。每层包含一个<em class="oh">全连接层</em>、<em class="oh">批量规格化</em>和一个<em class="oh">门控线性单元</em>激活。如果你不熟悉这些术语，<a class="ae ky" href="https://developers.google.com/machine-learning/glossary" rel="noopener ugc nofollow" target="_blank">谷歌的ML词汇表</a>是一个很好的起点。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/4fc2a473b89e53a827061a9416a31c92.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*cW0hXsaoCzRSi0_xjp1fKw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">TabNet特征转换器模型架构。图片作者。灵感来自<a class="ae ky" href="https://arxiv.org/pdf/1908.07442.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1908.07442.pdf</a>。</p></figure><p id="ac0a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">TabNet论文的作者指出，在决策<strong class="lt iu">步骤</strong>之间共享一些层会导致<em class="oh">“具有高容量的参数高效且鲁棒的学习”</em>，并且使用根0.5的归一化<em class="oh">“通过确保整个过程中的方差不会发生显著变化，有助于稳定学习”。</em>特征转换器的输出使用一个<em class="oh"> ReLU </em>激活功能。</p><p id="0dbc" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">功能选择</strong></p><p id="cd71" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">一旦特征被转换，它们被传送到<strong class="lt iu">关注转换器</strong>和<strong class="lt iu">屏蔽</strong>用于特征选择。</p><p id="a699" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">关注变压器</strong>由<em class="oh">全连接层、批量归一化</em>和<em class="oh">稀疏最大化</em>归一化组成。它还包括<strong class="lt iu">先前比例</strong>，这意味着<strong class="lt iu">知道每个特征在前面的步骤</strong>中使用了多少。这用于使用来自先前<strong class="lt iu">特征转换器</strong>的已处理特征来导出<strong class="lt iu">掩模</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/89f33c2d5a3d3cf3ca4183510e5918f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*oDNBFqB3mKBL38MTARLBsQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">注意变压器模型结构。图片作者。灵感来自<a class="ae ky" href="https://arxiv.org/pdf/1908.07442.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1908.07442.pdf</a>。</p></figure><p id="9028" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">掩码</strong>确保模型聚焦于最重要的特征，也用于推导可解释性。它本质上掩盖了特征，这意味着模型只能使用那些被<strong class="lt iu">关注变压器</strong>认为重要的特征。</p><p id="c706" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们还可以通过查看某个特征在所有决策和单个预测中被掩盖了多少来了解特征的重要性。</p><blockquote class="ms"><p id="c739" class="mt mu it bd mv mw mx my mz na nb mm dk translated">TabNet在端到端学习中采用稀疏度可控的软特征选择</p></blockquote><p id="d3d4" class="pw-post-body-paragraph lr ls it lt b lu nc ju lw lx nd jx lz ma ne mc md me nf mg mh mi ng mk ml mm im bi translated">这意味着一个模型联合执行特征选择和输出映射，这导致更好的性能。</p><blockquote class="ms"><p id="e5ad" class="mt mu it bd mv mw mx my mz na nb mm dk translated">TabNet使用基于实例的特征选择，这意味着为每个输入选择特征，并且每个预测可以使用不同的特征。</p></blockquote><p id="2099" class="pw-post-body-paragraph lr ls it lt b lu nc ju lw lx nd jx lz ma ne mc md me nf mg mh mi ng mk ml mm im bi translated">这种特征选择是必不可少的，因为它允许将决策边界概化为特征的线性组合，其中系数决定每个特征的比例，最终导致模型的可解释性</p><h1 id="66f0" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">PyTorch中的实现</h1><p id="8ef1" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">使用<strong class="lt iu"> TabNet </strong>的最佳方式是使用<a class="ae ky" href="https://github.com/dreamquark-ai/tabnet" rel="noopener ugc nofollow" target="_blank"> Dreamquark的PyTorch实现</a>。它使用scikit-learn风格的包装器，并且与GPU兼容。回购有大量的模型在使用中的例子，所以我强烈建议检查一下。</p><p id="fc8e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">训练模型实际上非常简单，只需几行代码就可以完成，TabNet也没有太多的超参数。</p><h2 id="fa51" class="nh la it bd lb ni nj dn lf nk nl dp lj ma nm nn ll me no np ln mi nq nr lp ns bi translated">密码</h2><p id="cde1" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">Dreamquark还提供了一些非常棒的笔记本，它们完美地展示了如何实现TabNet，同时也验证了原作者关于模型在某些基准上的准确性的声明。</p><p id="5916" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">分类</strong></p><div class="oo op gp gr oq or"><a href="https://github.com/dreamquark-ai/tabnet/blob/develop/census_example.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="os ab fo"><div class="ot ab ou cl cj ov"><h2 class="bd iu gy z fp ow fr fs ox fu fw is bi translated">dreamquark-ai/tabnet</h2><div class="oy l"><h3 class="bd b gy z fp ow fr fs ox fu fw dk translated">PyTorch实现TabNet论文:https://arxiv.org/pdf/1908.07442.pdf-dream quark-ai/TabNet</h3></div><div class="oz l"><p class="bd b dl z fp ow fr fs ox fu fw dk translated">github.com</p></div></div><div class="pa l"><div class="pb l pc pd pe pa pf ks or"/></div></div></a></div><p id="f1ae" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">回归</strong></p><div class="oo op gp gr oq or"><a href="https://github.com/dreamquark-ai/tabnet/blob/develop/regression_example.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="os ab fo"><div class="ot ab ou cl cj ov"><h2 class="bd iu gy z fp ow fr fs ox fu fw is bi translated">dreamquark-ai/tabnet</h2><div class="oy l"><h3 class="bd b gy z fp ow fr fs ox fu fw dk translated">PyTorch实现TabNet论文:https://arxiv.org/pdf/1908.07442.pdf-dream quark-ai/TabNet</h3></div><div class="oz l"><p class="bd b dl z fp ow fr fs ox fu fw dk translated">github.com</p></div></div><div class="pa l"><div class="pg l pc pd pe pa pf ks or"/></div></div></a></div><p id="dba2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这两个例子都是可重复的，并且包括一个XGBoost模型来与TabNet的性能进行比较。</p><h2 id="173f" class="nh la it bd lb ni nj dn lf nk nl dp lj ma nm nn ll me no np ln mi nq nr lp ns bi translated">可解释性</h2><p id="4fe2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">TabNet相对于Boosted树的一个关键优势是它更容易解释。如果不使用类似<strong class="lt iu"> SHAP </strong>或<strong class="lt iu">石灰</strong>的东西，我们就无法剖析梯度推进中的预测。因为有了掩码，我们可以了解我们的TabNet模型全局使用<strong class="lt iu">(跨整个数据集)和局部使用<strong class="lt iu">(用于单个预测)的特性。</strong></strong></p><p id="3daa" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">为了探究这一点，我将使用上面的分类示例，该示例使用了人口普查收入数据集。</strong></p><p id="7a30" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">特征重要性</strong></p><p id="1977" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们可以查看我们个人特征的重要性，它们加起来正好等于1。当我们从基于树的模型中获取这些数据时，它可能偏向一个变量，或者具有大量唯一值的分类变量。在某些情况下，这可能会歪曲模型实际正在做的事情。</p><p id="a8c8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在这个例子中，当使用TabNet时，我们看到了更大的重要性分布，这意味着它更平等地使用特性。这不一定更好，TabNet过程中可能存在缺陷。然而，原始论文的作者确实将特征重要性与合成数据示例进行了比较，并发现TabNet使用了他们预期的特征。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ph"><img src="../Images/358c5c938918af6f0f09da06bf516f3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cK9MXmWr4soFHOKyoXL6fQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">根据人口普查数据集训练的TabNet和XGBoost模型的要素重要性。图片作者。</p></figure><p id="3242" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">注:</strong>以数字作为特征名称的特征(如2174)似乎是匿名特征。</p><p id="2dd0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">口罩</strong></p><p id="9685" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">通过使用掩码，我们可以了解在预测级别使用了哪些特征，我们可以查看所有<strong class="lt iu">掩码</strong>或单个<strong class="lt iu">掩码</strong>的集合。</p><p id="b971" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">因此，对于第0行，即我们测试数据的第一行，似乎<strong class="lt iu">掩码</strong> 1优先考虑数据集中的第4个特征。，而其他的<strong class="lt iu">面具</strong>使用不同的功能。</p><p id="ad66" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这可以让我们了解模型使用了哪些特征来进行预测，这给了我们更多的信心，因为我们可以找出模型预测背后的“为什么”,并可以帮助我们了解它如何处理看不见的数据。</p><p id="fd42" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然而，尚不清楚这与实际特征值的关系，我们不知道模型使用该特征是因为它是高还是低。更重要的是，我们不能轻易理解交互术语。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pi"><img src="../Images/9f10067295eb4fbdc5b636825fa6b4e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TTRhWVcsH325Z2hmBwo_Ww.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Census TabNet模型中口罩的热图。较浅的颜色表示正在使用该功能。图片作者。</p></figure><h2 id="ee15" class="nh la it bd lb ni nj dn lf nk nl dp lj ma nm nn ll me no np ln mi nq nr lp ns bi translated">通过自我监督学习改善结果</h2><p id="997a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">TabNet论文还提出了自我监督学习作为预训练模型权重和减少训练数据量的方法。</p><p id="668a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为此，数据集中的要素被屏蔽，模型尝试预测它们。然后使用解码器输出结果。</p><p id="cd6f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这也可以在Dreamquark的包中完成</p><div class="oo op gp gr oq or"><a href="https://github.com/dreamquark-ai/tabnet/blob/develop/pretraining_example.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="os ab fo"><div class="ot ab ou cl cj ov"><h2 class="bd iu gy z fp ow fr fs ox fu fw is bi translated">dreamquark-ai/tabnet</h2><div class="oy l"><h3 class="bd b gy z fp ow fr fs ox fu fw dk translated">PyTorch实现TabNet论文:https://arxiv.org/pdf/1908.07442.pdf-dream quark-ai/TabNet</h3></div><div class="oz l"><p class="bd b dl z fp ow fr fs ox fu fw dk translated">github.com</p></div></div><div class="pa l"><div class="pj l pc pd pe pa pf ks or"/></div></div></a></div><p id="ead8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">使用自我监督学习应该用更少的训练数据产生更好的结果。</p><h1 id="7ffa" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><blockquote class="ms"><p id="5cce" class="mt mu it bd mv mw mx my mz na nb mm dk translated">TabNet是用于表格学习的深度学习模型。它使用顺序注意来选择有意义特征的子集，以在每个决策步骤中进行处理。基于实例的特征选择允许模型的学习能力集中在最重要的特征上，并且模型掩码的可视化提供了可解释性。</p></blockquote><p id="b2ea" class="pw-post-body-paragraph lr ls it lt b lu nc ju lw lx nd jx lz ma ne mc md me nf mg mh mi ng mk ml mm im bi translated">希望你能看到，TabNet让我们在保持可解释性的同时实现了最先进的结果。随着人工智能监管变得更加严格，理解我们的模型如何工作在未来只会变得更加重要。我强烈建议在你的下一个项目或Kaggle竞赛中尝试TabNet！</p><h2 id="a2fe" class="nh la it bd lb ni nj dn lf nk nl dp lj ma nm nn ll me no np ln mi nq nr lp ns bi translated">了解更多信息</h2><div class="oo op gp gr oq or"><a rel="noopener follow" target="_blank" href="/generalised-additive-models-6dfbedf1350a"><div class="os ab fo"><div class="ot ab ou cl cj ov"><h2 class="bd iu gy z fp ow fr fs ox fu fw is bi translated">什么是广义加性模型？</h2><div class="oz l"><p class="bd b dl z fp ow fr fs ox fu fw dk translated">towardsdatascience.com</p></div></div><div class="pa l"><div class="pk l pc pd pe pa pf ks or"/></div></div></a></div><h2 id="f48c" class="nh la it bd lb ni nj dn lf nk nl dp lj ma nm nn ll me no np ln mi nq nr lp ns bi translated">将我的内容直接发送到您的收件箱！</h2><div class="oo op gp gr oq or"><a href="https://adamsh.substack.com/p/coming-soon" rel="noopener  ugc nofollow" target="_blank"><div class="os ab fo"><div class="ot ab ou cl cj ov"><h2 class="bd iu gy z fp ow fr fs ox fu fw is bi translated">Data Siens:机器学习技巧、诀窍和教程的资源。</h2><div class="oy l"><h3 class="bd b gy z fp ow fr fs ox fu fw dk translated">欢迎来到亚当的数据西恩斯。我是一名数据科学家，有5年的分析经验。我喜欢谈论任何事情…</h3></div><div class="oz l"><p class="bd b dl z fp ow fr fs ox fu fw dk translated">adamsh.substack.com</p></div></div><div class="pa l"><div class="pl l pc pd pe pa pf ks or"/></div></div></a></div></div></div>    
</body>
</html>