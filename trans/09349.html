<html>
<head>
<title>How To Model Experience Replay, Batch Learning and Target Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何对经验重放、批量学习和目标网络进行建模</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-model-experience-replay-batch-learning-and-target-networks-c1350db93172?source=collection_archive---------23-----------------------#2021-08-30">https://towardsdatascience.com/how-to-model-experience-replay-batch-learning-and-target-networks-c1350db93172?source=collection_archive---------23-----------------------#2021-08-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6130" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用TensorFlow 2.0，快速学习稳定和成功的深度Q学习的三个基本技巧</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/157933de2e9be956709f86a816f29db8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*_C-nKJg8geoPA5_w"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@chadwiq?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">查德·沃顿</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="2feb" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">如果你认为深度Q学习只是用一个神经网络代替一个查找表，你可能会有一个粗略的觉醒。虽然深度Q学习允许处理非常大的状态空间和复杂的非线性环境，但这些好处是以巨大的成本为代价的。</p><p id="be52" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">对于这篇文章，我假设你已经对<a class="ae ky" rel="noopener" target="_blank" href="/walking-off-the-cliff-with-off-policy-reinforcement-learning-7fdbcdfe31ff"> Q-learning </a>和<a class="ae ky" rel="noopener" target="_blank" href="/a-minimal-working-example-for-deep-q-learning-in-tensorflow-2-0-e0ca8a944d5e">深度Q-learning有所了解。</a>下面的更新函数和损失函数将足以设置场景。在余下的部分，我将具体放大稳定性问题和三种常用技术来缓解这些问题:<strong class="li iu">经验重放</strong>、<strong class="li iu">批量学习</strong>和<strong class="li iu">目标网络</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mc"><img src="../Images/fa590d353c63e4edbb2de1dbf861cc25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Ef7pkJT7FWC8cQ9MBT6vg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(普通)Q-learning的口语更新功能。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi md"><img src="../Images/dd24987983661055bb596c68b1a75691.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P3QaRca-aOmLDS3mE3Cl4Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">深度Q学习的均方误差损失函数。</p></figure><h1 id="63b9" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">(深度)Q学习的稳定性</h1><p id="bddc" class="pw-post-body-paragraph lg lh it li b lj mw ju ll lm mx jx lo lp my lr ls lt mz lv lw lx na lz ma mb im bi translated">在某种程度上，稳定性是每个学习任务中的一个问题。然而，普通Q-learning相当稳定。当观察到对应于某个状态-动作对<code class="fe nb nc nd ne b">(s,a)</code>、<em class="nf">的奖励时，只有</em>对应的Q值<code class="fe nb nc nd ne b">Q(s,a)</code>被更新。查找表中的所有其他Q值保持不变。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/644763175313db7461962e530dec238f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kwp6MLw7s4Bbw0m2GX-q5g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Q表示例。对属于各个状态-动作对的Q值进行更新，从而得到稳定的表示。[图片由作者提供]</p></figure><p id="2852" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">相比之下，Q网络可以被视为一个参数化的函数<code class="fe nb nc nd ne b">f_θ:s →[Q(s,a)]_a∈A</code>，将一个状态映射到一个Q值向量。这里的关键区别在于，对于每个状态-动作对，单次更新会改变所有Q值。这种影响是相当深远的，在某种程度上由于神经网络的非线性表示(对异常值的敏感性等)而加剧。)即使对于看似简单的问题，深度Q学习也经常受到稳定性问题的困扰。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/9745858325280b2c5406fa9b08170fc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rZgjSJX2I0jdJhhPxz1f-g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Q网络的例子。单次更新影响所有状态-动作对的Q值，因为所有输入使用相同的网络。[图片由作者提供]</p></figure><h1 id="53b3" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">体验回放</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/14c4f9c049d32724ff03492693fea4f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*38y-4ATqXPbHepnuu567Ww.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">体验重放从重放缓冲区随机抽取一个观察值(一个s，a，r，s '元组)。期望和目标是通过将s和s’插入Q网络来确定的。[图片由作者提供]</p></figure><p id="6a90" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">强化学习需要做出连续的决策。这通常意味着后续状态密切相关(例如，迷宫中的一步，股票价格的一天更新)，因此非常相似。因此，顺序观察往往高度相关，这可能导致网络过度拟合(例如，在迷宫的次优区域中)。</p><p id="5c17" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">然而，即使<em class="nf">经验</em>是顺序获得的，也没有理由<em class="nf">学习</em>应该遵循相同的顺序。经验重放通过用过去的观察创建一个<strong class="li iu">重放缓冲器</strong>来分离两个过程。具体来说，重放缓冲区存储我们遇到的每个<code class="fe nb nc nd ne b">s,a,r,s’</code>元组。注意，相应的Q值是<em class="nf">而不是</em>存储的；我们在为更新目的对观察值进行采样时确定它们。具体来说，学习过程如下:</p><ul class=""><li id="212a" class="nj nk it li b lj lk lm ln lp nl lt nm lx nn mb no np nq nr bi translated">从重放缓冲区中随机抽取<code class="fe nb nc nd ne b">s,a,r,s’</code>元组。</li><li id="c033" class="nj nk it li b lj ns lm nt lp nu lt nv lx nw mb no np nq nr bi translated">使用存储的动作<code class="fe nb nc nd ne b">a</code>，将<code class="fe nb nc nd ne b">s</code>输入Q网络以获得<code class="fe nb nc nd ne b">Q_t(s,a)</code>。</li><li id="288a" class="nj nk it li b lj ns lm nt lp nu lt nv lx nw mb no np nq nr bi translated">将<code class="fe nb nc nd ne b">s’</code>输入Q网络以获得<code class="fe nb nc nd ne b">Q_t+1(s’,a*)</code>，其中<code class="fe nb nc nd ne b">a*∈A</code>是状态<code class="fe nb nc nd ne b">s’</code>中的最优动作(根据主要的Q值)。回想一下Q学习是不符合策略的，所以我们不使用实际轨迹。</li><li id="22b2" class="nj nk it li b lj ns lm nt lp nu lt nv lx nw mb no np nq nr bi translated">使用<code class="fe nb nc nd ne b">Q_t(s,a)</code>和<code class="fe nb nc nd ne b">r_t+Q_t+1(s’,a*)</code>之间的差值计算更新网络所需的损耗。</li></ul><p id="e802" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">除了打破相关性和克服过度拟合之外，理论上的好处是数据现在更接近i.i.d .数据，这通常是在监督学习收敛证明中假设的。</p><p id="6876" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">Python实现如下所示。请注意，我们所做的只是在<strong class="li iu">经验收集阶段</strong>将<code class="fe nb nc nd ne b">s,a,r,s’</code>存储在缓冲器中，并在<strong class="li iu">学习阶段</strong>对它们进行随机采样。对于后者，我们使用方便的<code class="fe nb nc nd ne b">random.choices</code>功能。</p><pre class="kj kk kl km gt nx ne ny nz aw oa bi"><span id="ca25" class="ob mf it ne b gy oc od l oe of"><strong class="ne iu">"""Experience replay implementation"""</strong></span><span id="6b70" class="ob mf it ne b gy og od l oe of"><strong class="ne iu"># Initialize replay buffer</strong><br/>replay_buffer = []</span><span id="5fa8" class="ob mf it ne b gy og od l oe of">...</span><span id="7e47" class="ob mf it ne b gy og od l oe of"><strong class="ne iu"># Experience collection phase</strong></span><span id="6635" class="ob mf it ne b gy og od l oe of"># Set state<br/>state = next_state</span><span id="4d68" class="ob mf it ne b gy og od l oe of"># Determine action (epsilon_greedy)<br/>if epsilon&lt;= 0.05:<br/>    # Select random action<br/>    action = np.random.choice(action_dim)<br/>else:<br/>    # Select action with highest q-value<br/>    action = np.argmax(q_values[0])</span><span id="fbee" class="ob mf it ne b gy og od l oe of"># Compute and store reward<br/>reward = get_reward(state, action)</span><span id="84a1" class="ob mf it ne b gy og od l oe of"># Determine next state<br/>next_state = get_state(state, action)<br/><br/># Store observation in replay buffer<br/>observation = (state, action, reward, next_state)<br/><br/>replay_buffer.append(observation)</span><span id="704a" class="ob mf it ne b gy og od l oe of">...</span><span id="67b4" class="ob mf it ne b gy og od l oe of"><strong class="ne iu"># Learning phase</strong></span><span id="749b" class="ob mf it ne b gy og od l oe of"># Select random sample from replay buffer<br/>if len(replay_buffer) &gt;= min_buffer_size:<br/>    observations = random.choices(replay_buffer, k=1)</span></pre><p id="1527" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">存储所有过去的观察数据并完全随机取样可能并不理想。事实上，这一程序可以改进。有了<em class="nf">优先重播</em>，我们更经常地尝试我们期望从中学习更多的经验。另一种常见的技术是更新重放缓冲区，删除旧的观察结果。毕竟，你不希望停留在过去的观察上，这些观察来自于状态空间中我们从一开始就不应该访问的区域。自然，像这样的改进引入了额外的建模挑战。</p><h1 id="edf9" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">批量学习</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/28a7afec5f9254c16ca783e220d16f75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vJXPbdkwFXiiw4eJYn02Ew.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">批量学习对单个网络更新的多个观察值进行采样，导致更具代表性的损失和更稳定的更新[图片由作者提供]</p></figure><p id="2da2" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">一次更新一个观测值的Q网络可能不是一个好的解决方案。在许多情况下，这样的观察可能不包含太多有用的信息——想想迷宫中的一步。更糟糕的是，观察结果可能是异常值，不能代表整个问题，然而更新可能会对未来的决策产生灾难性的影响。理想情况下，我们希望每次更新都能代表整个问题。</p><p id="60ff" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">另一方面，我们可能会执行所有的训练迭代，并使用完整的一批观察值来拟合带有单个更新的Q网络。虽然这样的一批确实具有代表性，但是所有的观察都是在我们最初的(可能非常差的)策略下进行的，因此我们永远也不会知道与一个<em class="nf">好的</em>策略相对应的Q值。</p><p id="fa61" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">因此，大批量也不是很有用。我们希望将观察和更新结合起来，逐步改进我们的政策。这并不意味着我们必须更新每一次观察的<em class="nf">。显而易见的妥协是<strong class="li iu">小批量</strong>，这意味着我们频繁地用相对较少的观测值更新我们的网络。与经验重放相结合，这是一种强大的技术，可以基于大量以前的观察获得稳定的更新。</em></p><p id="392c" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">作为基本经验回放的延伸，我们可能仍然很难获得有代表性的样本。主要的实施问题是<em class="nf">更新频率</em>(更新自然比单次观测花费更长时间)和小批量的<em class="nf">大小</em>。</p><pre class="kj kk kl km gt nx ne ny nz aw oa bi"><span id="3a19" class="ob mf it ne b gy oc od l oe of"><strong class="ne iu">"""Batch learning implementation"""</strong></span><span id="c98f" class="ob mf it ne b gy og od l oe of">no_observations = 100<br/>mini_batch_size = 10<br/>loss_value = 0</span><span id="1dee" class="ob mf it ne b gy og od l oe of">if len(replay_buffer) &gt;= no_observations and <br/>   i % update_frequency == 0:<br/>    <strong class="ne iu"># Randomly sample k observations from buffer</strong><br/>    observations = random.choices(replay_buffer, k=mini_batch_size)</span><span id="bdf1" class="ob mf it ne b gy og od l oe of">    <strong class="ne iu"># Loop over sampled observations</strong><br/>    for observation in observations:<br/>       # Determine Q-value at time t<br/>       q_values = q_network(state)<br/>       expected_value = q_values[0, action]</span><span id="4e45" class="ob mf it ne b gy og od l oe of">       # Determine Q-value at time t+1<br/>       next_q_values = tf.stop_gradient(q_network(next_state))<br/>       next_action = np.argmax(next_q_values[0])<br/>       next_q_value = next_q_values[0, next_action]</span><span id="55f1" class="ob mf it ne b gy og od l oe of">       # Add direct reward to obtain target value<br/>       target_value = reward + (gamma * next_q_value)<br/><br/>       # Compute loss value<br/>       loss_value += mse_loss(expected_value, target_value)<br/><br/><strong class="ne iu"># Compute mean loss value</strong><br/>loss_value /= batch_size</span></pre><h1 id="9fd9" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">目标网络</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/e20fca1a36854606abb9faa80836a35b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q8zl7qtA80CnxGtI2GpXQA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">目标网络(右)是原始Q网络(左)的周期性副本。目标网络用于确定Q_t+1，降低期望与目标的相关性。[图片由作者提供]</p></figure><p id="d17b" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">通过从过去的观察中随机取样(经验回放)，我们试图打破观察之间的相关性。然而，注意，观察元组包含两个密切相关的状态——<code class="fe nb nc nd ne b">s</code>和<code class="fe nb nc nd ne b">s’ </code>——它们被馈送到相同的Q网络以获得Q值。换句话说，期望和目标也是相互关联的。每次网络更新也会修改目标，即我们正在追逐一个移动的目标。</p><p id="eb19" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">为了降低期望<code class="fe nb nc nd ne b">Q(s,a)</code>和目标<code class="fe nb nc nd ne b">r+Q(s’,a’)</code>之间的相关性，我们可以使用不同的网络来确定<code class="fe nb nc nd ne b">Q(s’,a’)</code>。我们称之为<strong class="li iu">目标网络</strong>——我们的目标基于<code class="fe nb nc nd ne b">Q^T(s’,a’)</code>而不是<code class="fe nb nc nd ne b">Q(s’,a’)</code>。我们可以用TensorFlow的<code class="fe nb nc nd ne b">clone_model</code>命令复制原Q网的网络架构。请注意，该克隆程序不会<em class="nf">而不是</em>复制权重，为此我们使用了<code class="fe nb nc nd ne b">set_weights</code>命令。使用<code class="fe nb nc nd ne b">get_weights</code>，我们定期从Q-网络获得最新的权重。</p><pre class="kj kk kl km gt nx ne ny nz aw oa bi"><span id="e21e" class="ob mf it ne b gy oc od l oe of"><strong class="ne iu">"""Target network implementation"""</strong></span><span id="74c2" class="ob mf it ne b gy og od l oe of"><strong class="ne iu"># Copy network architecture</strong><br/>target_network = tf.keras.models.clone_model(q_network) </span><span id="a375" class="ob mf it ne b gy og od l oe of"><strong class="ne iu"># Copy network weights</strong><br/>target_network.set_weights(q_network.get_weights()) </span><span id="3bc1" class="ob mf it ne b gy og od l oe of">...</span><span id="db39" class="ob mf it ne b gy og od l oe of"><strong class="ne iu"># Periodically update target network</strong><br/>if episode % update_frequency_target_network == 0:<br/>    target_network.set_weights(q_network.get_weights())</span></pre><p id="2d2f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">关键的挑战是找到正确的更新频率。如果更新间隔很久，目标可能对应于过去表现不佳的政策。过于频繁，目标和期望之间的相关性仍然很高。与其他两种技术一样，该解决方案解决了一个问题，但也引入了额外的复杂性。</p><h1 id="42f0" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">外卖食品</h1><ul class=""><li id="ac5c" class="nj nk it li b lj mw lm mx lp oj lt ok lx ol mb no np nq nr bi translated"><strong class="li iu">经验重放</strong>将所有观察值— <code class="fe nb nc nd ne b">s,a,r,s’</code>元组存储在一个缓冲区中，可以从中选择随机样本。这打破了连续观测中经常存在的相关性。</li><li id="f1e1" class="nj nk it li b lj ns lm nt lp nu lt nv lx nw mb no np nq nr bi translated"><strong class="li iu">批量学习</strong>基于多次观察执行网络更新。这种方法往往比单一观测产生更稳定的更新，因为损失更好地代表了整体问题。</li><li id="7bd5" class="nj nk it li b lj ns lm nt lp nu lt nv lx nw mb no np nq nr bi translated"><strong class="li iu">目标网络</strong>降低期望<code class="fe nb nc nd ne b">Q(s,a)</code>和目标<code class="fe nb nc nd ne b">r+Q^T(s’,a’)</code>之间的相关性。目标网络只不过是Q网络的周期性副本。</li><li id="d146" class="nj nk it li b lj ns lm nt lp nu lt nv lx nw mb no np nq nr bi translated">每种技术都引入了新的建模挑战和要调整的参数。在某种程度上，这就是深度学习如此困难的原因；每个解决方案都会产生新的障碍，增加模型的复杂性。</li></ul></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="5262" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><em class="nf">对深度Q-learning感兴趣？你可能也会对下面的文章感兴趣:</em></p><div class="om on gp gr oo op"><a rel="noopener follow" target="_blank" href="/a-minimal-working-example-for-deep-q-learning-in-tensorflow-2-0-e0ca8a944d5e"><div class="oq ab fo"><div class="or ab os cl cj ot"><h2 class="bd iu gy z fp ou fr fs ov fu fw is bi translated">TensorFlow 2.0中深度Q学习的最小工作示例</h2><div class="ow l"><h3 class="bd b gy z fp ou fr fs ov fu fw dk translated">一个多臂土匪的例子来训练一个Q网络。使用TensorFlow，更新过程只需要几行代码</h3></div><div class="ox l"><p class="bd b dl z fp ou fr fs ov fu fw dk translated">towardsdatascience.com</p></div></div><div class="oy l"><div class="oz l pa pb pc oy pd ks op"/></div></div></a></div><p id="f7a6" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">想知道更多关于Q-learning(和SARSA)的基础知识吗？看看下面这篇文章:</p><div class="om on gp gr oo op"><a rel="noopener follow" target="_blank" href="/walking-off-the-cliff-with-off-policy-reinforcement-learning-7fdbcdfe31ff"><div class="oq ab fo"><div class="or ab os cl cj ot"><h2 class="bd iu gy z fp ou fr fs ov fu fw is bi translated">用非策略强化学习走下悬崖</h2><div class="ow l"><h3 class="bd b gy z fp ou fr fs ov fu fw dk translated">政策外强化学习和政策内强化学习的深入比较</h3></div><div class="ox l"><p class="bd b dl z fp ou fr fs ov fu fw dk translated">towardsdatascience.com</p></div></div><div class="oy l"><div class="pe l pa pb pc oy pd ks op"/></div></div></a></div><h1 id="717d" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">参考</h1><p id="a27f" class="pw-post-body-paragraph lg lh it li b lj mw ju ll lm mx jx lo lp my lr ls lt mz lv lw lx na lz ma mb im bi translated">坦贝特·马蒂森(2015年)。揭秘深度强化学习。<em class="nf">计算神经科学实验室。</em>从neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/取回</p><p id="432b" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">Mnih，v .，Kavukcuoglu，k .，Silver，d .，鲁苏，A. A .，Veness，j .，Bellemare，M. G .，… &amp; Hassabis，D. (2015)。通过深度强化学习实现人类水平的控制。<em class="nf">自然，518(7540)，529–533。</em></p></div></div>    
</body>
</html>