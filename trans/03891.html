<html>
<head>
<title>Foundations of NLP Explained Visually: Beam Search, How It Works</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理的基础直观解释:波束搜索，它是如何工作的</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24?source=collection_archive---------1-----------------------#2021-04-01">https://towardsdatascience.com/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24?source=collection_archive---------1-----------------------#2021-04-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="60c5" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a>，直观的NLP系列</h2><div class=""/><div class=""><h2 id="d8a2" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">波束搜索如何增强预测的简明指南</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/f6ecfef382690dae2b842806595d3ff9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GqR_ZQubZEsYBRIx"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">由<a class="ae le" href="https://unsplash.com/@mischievous_penguins?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">凯西·霍纳</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="f553" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">许多NLP应用程序，如机器翻译、聊天机器人、文本摘要和语言模型都会生成一些文本作为输出。此外，像图像字幕或自动语音识别(即语音到文本)输出文本，即使它们可能不被认为是纯NLP应用程序。</p><p id="57a4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">所有这些应用都使用一些常用的算法，作为产生最终输出的最后一步。</p><ul class=""><li id="cc26" class="mb mc iq lh b li lj ll lm lo md ls me lw mf ma mg mh mi mj bi translated">贪婪搜索就是这样一种算法。它经常被使用，因为它简单快捷。</li><li id="a9b8" class="mb mc iq lh b li mk ll ml lo mm ls mn lw mo ma mg mh mi mj bi translated">另一种方法是使用波束搜索。它非常受欢迎，因为虽然它需要更多的计算，但通常会产生更好的结果。</li></ul><p id="10fb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在本文中，我将探索波束搜索，并解释为什么使用它以及它是如何工作的。我们将简单地讨论一下贪婪搜索作为比较，这样我们就能理解波束搜索是如何改进它的。</p><p id="41cc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">另外，如果您对NLP感兴趣，我还有几篇文章，您可能会觉得有用。他们还探索了这一领域的其他有趣主题，如变形金刚、语音转文本和Bleu评分标准。</p><ol class=""><li id="ea9d" class="mb mc iq lh b li lj ll lm lo md ls me lw mf ma mp mh mi mj bi translated"><a class="ae le" rel="noopener" target="_blank" href="/transformers-explained-visually-part-1-overview-of-functionality-95a6dd460452">变形金刚直观讲解:功能概述</a> <em class="mq">(变形金刚怎么用，为什么比RNNs好。架构的组件，以及训练和推理期间的行为)</em></li><li id="03cf" class="mb mc iq lh b li mk ll ml lo mm ls mn lw mo ma mp mh mi mj bi translated"><a class="ae le" rel="noopener" target="_blank" href="/transformers-explained-visually-part-2-how-it-works-step-by-step-b49fa4a64f34">变压器如何工作，循序渐进</a> <em class="mq">(内部操作端到端。数据如何流动以及执行何种计算，包括矩阵表示)</em></li><li id="9a8c" class="mb mc iq lh b li mk ll ml lo mm ls mn lw mo ma mp mh mi mj bi translated"><a class="ae le" rel="noopener" target="_blank" href="/audio-deep-learning-made-simple-automatic-speech-recognition-asr-how-it-works-716cfce4c706">自动语音识别</a> <em class="mq">(语音转文本算法和架构，使用CTC丢失和解码进行序列对齐。)</em></li><li id="16c7" class="mb mc iq lh b li mk ll ml lo mm ls mn lw mo ma mp mh mi mj bi translated"><a class="ae le" rel="noopener" target="_blank" href="/foundations-of-nlp-explained-bleu-score-and-wer-metrics-1a5ba06d812b"> Bleu评分</a> ( <em class="mq"> Bleu评分和单词错误率是NLP模型的两个基本指标</em></li></ol><p id="605f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们将从获得一些关于NLP模型如何产生它们的输出的上下文开始，以便我们能够理解波束搜索(和贪婪搜索)适合在哪里。</p><p id="0f85" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">注意:根据他们正在解决的问题，NLP模型可以生成字符或单词形式的输出。与波束搜索相关的所有概念都同样适用于这两个术语，因此我将在本文中交替使用这两个术语。</p><h1 id="3f85" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">NLP模型如何生成输出</h1><p id="6b17" class="pw-post-body-paragraph lf lg iq lh b li nj ka lk ll nk kd ln lo nl lq lr ls nm lu lv lw nn ly lz ma ij bi translated">让我们以序列到序列模型为例。这些模型经常用于机器翻译等应用。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi no"><img src="../Images/8e8c5e5367225e46832ddfb9576da0db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GkG_5wg57IpkU8F84nJubQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">机器翻译的序列对序列模型(图片由作者提供)</p></figure><p id="0e83" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">例如，如果这个模型被用来从英语翻译成西班牙语，它将把源语言中的句子(例如英语中的“You are welcome”)作为输入，并输出目标语言中的对等句子(例如西班牙语中的“De nada”)。</p><p id="d1a6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">文本是单词(或字符)的序列，NLP模型构建了由源语言和目标语言中的整个单词集组成的词汇表。</p><p id="0cf1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">该模型将源句子作为其输入，并将其通过一个嵌入层，随后是一个编码器。然后，编码器输出压缩捕获输入的基本特征的编码表示。</p><p id="fb30" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">然后，这种表示与一个“<start>”标记一起被馈送到解码器，作为其输出的种子。解码器使用这些来生成自己的输出，这是目标语言句子的编码表示。</start></p><p id="e761" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">然后通过输出层传递，输出层可能由一些线性层和一个Softmax组成。线性层在输出序列中的每个位置输出词汇表中每个单词出现的可能性的得分。然后Softmax将这些分数转换成概率。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi np"><img src="../Images/916438a8c074bb43374ea39a16192681.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*nId-RZloZiVUdFQFK31xcg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">词汇表中每个字符在输出序列中每个位置的概率(图片由作者提供)</p></figure><p id="efc8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">当然，我们的最终目标不是这些概率，而是最终的目标句子。要做到这一点，模型必须决定它应该为目标序列中的每个位置预测哪个单词。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi np"><img src="../Images/bb34310a492f712bd46b9916880779cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:680/format:webp/1*MZGM7BnSm-L2n025P__HIg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">该模型根据概率预测输出句子(图片由作者提供)</p></figure><p id="6ff3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">它是怎么做到的？</p><h1 id="dcd9" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">贪婪搜索</h1><p id="879b" class="pw-post-body-paragraph lf lg iq lh b li nj ka lk ll nk kd ln lo nl lq lr ls nm lu lv lw nn ly lz ma ij bi translated">一个相当显而易见的方法是简单地选择在每个位置上概率最高的单词并预测它。它计算速度快，容易理解，而且经常能得出正确的结果。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/c978d84b764a5c9801a2dec95331ba2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*IWNtDrXdepfzJUIshnQOAg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">贪婪搜索(图片由作者提供)</p></figure><p id="3c71" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">事实上，贪婪搜索是如此容易理解，我们不需要花更多的时间来解释它😃。但是我们能做得更好吗？</p><p id="2576" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">啊哈，终于说到我们真正的话题了！</p><h1 id="e85a" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">波束搜索</h1><p id="ceba" class="pw-post-body-paragraph lf lg iq lh b li nj ka lk ll nk kd ln lo nl lq lr ls nm lu lv lw nn ly lz ma ij bi translated">波束搜索对贪婪搜索进行了两项改进。</p><ul class=""><li id="e25f" class="mb mc iq lh b li lj ll lm lo md ls me lw mf ma mg mh mi mj bi translated">通过贪婪搜索，我们在每个位置只取一个最好的单词。相比之下，波束搜索扩展了这一点，并选取了最佳的“N”个单词。</li><li id="578c" class="mb mc iq lh b li mk ll ml lo mm ls mn lw mo ma mg mh mi mj bi translated">使用贪婪搜索，我们孤立地考虑每个位置。一旦我们确定了最适合那个职位的词，我们就不会检查它之前是什么。在前一个位置)，或者在它之后。相比之下，波束搜索选择目前为止的“N”个最佳<em class="mq">序列</em>，并考虑所有前面的单词与当前位置的单词组合的概率。</li></ul><p id="521f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">换句话说，它把“搜索的光束”投射到比贪婪搜索更广的地方，这就是它的名字。超参数“N”称为波束宽度。</p><p id="9277" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">直觉上，这给了我们比贪婪搜索更好的结果。因为，我们真正感兴趣的是最好的完整句子，如果我们在每个位置只选择最好的单个单词，我们可能会错过它。</p><h1 id="4fda" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">波束搜索—它的作用</h1><p id="1991" class="pw-post-body-paragraph lf lg iq lh b li nj ka lk ll nk kd ln lo nl lq lr ls nm lu lv lw nn ly lz ma ij bi translated">让我们举一个简单的例子，波束宽度为2，并使用字符来保持简单。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nr"><img src="../Images/e0c0424ef754b6d6f41d02ccdb3cb71f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tEjhWqUgjX37VnT7gJN-4g.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">波束搜索示例，宽度= 2(图片由作者提供)</p></figure><h2 id="5cb4" class="ns ms iq bd mt nt nu dn mx nv nw dp nb lo nx ny nd ls nz oa nf lw ob oc nh iw bi translated"><strong class="ak">第一位置</strong></h2><ul class=""><li id="de1c" class="mb mc iq lh b li nj ll nk lo od ls oe lw of ma mg mh mi mj bi translated">考虑模型在第一个位置的输出。它从“<start>”标记开始，获取每个单词的概率。它现在选择了<em class="mq">那个位置的两个</em>最好的字符。例如“A”和“C”。</start></li></ul><h2 id="b2d3" class="ns ms iq bd mt nt nu dn mx nv nw dp nb lo nx ny nd ls nz oa nf lw ob oc nh iw bi translated">第二胎位</h2><ul class=""><li id="feb8" class="mb mc iq lh b li nj ll nk lo od ls oe lw of ma mg mh mi mj bi translated">当到达第二个位置时，它重新运行模型<em class="mq">两次</em>，通过将可能的字符固定在第一个位置来生成概率。换句话说，它将第一个位置的字符约束为“A”或“C ”,并生成两个具有两组概率的分支。具有第一组概率的分支对应于在位置1具有“A ”,具有第二组概率的分支对应于在位置1具有“C”。</li><li id="5463" class="mb mc iq lh b li mk ll ml lo mm ls mn lw mo ma mg mh mi mj bi translated">它现在基于前两个字符的组合概率，从两组概率中挑选出<em class="mq">总共两个最佳字符对</em>。所以它不会只从第一组中选择一个最佳字符对，从第二组中选择一个最佳字符对。例如“AB”和“AE”</li></ul><h2 id="036e" class="ns ms iq bd mt nt nu dn mx nv nw dp nb lo nx ny nd ls nz oa nf lw ob oc nh iw bi translated">第三位置</h2><ul class=""><li id="6113" class="mb mc iq lh b li nj ll nk lo od ls oe lw of ma mg mh mi mj bi translated">当它到达第三个位置时，它重复这个过程。它通过将前两个位置约束为“AB”或“AE”来重新运行模型两次，并再次生成两组概率。</li><li id="04c3" class="mb mc iq lh b li mk ll ml lo mm ls mn lw mo ma mg mh mi mj bi translated">再一次，它根据两组概率中前三个字符的组合概率来挑选两个最佳字符三元组。因此，对于前三个位置，我们现在有两个最佳的字符组合。例如“美国广播公司”和“AED”。</li></ul><h2 id="a1bc" class="ns ms iq bd mt nt nu dn mx nv nw dp nb lo nx ny nd ls nz oa nf lw ob oc nh iw bi translated">重复直到结束标记</h2><ul class=""><li id="0947" class="mb mc iq lh b li nj ll nk lo od ls oe lw of ma mg mh mi mj bi translated">它继续这样做，直到它挑选一个“<end>”标记作为某个位置的最佳字符，然后结束序列的那个分支。</end></li></ul><p id="c74a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">它最终以两个最佳序列结束，并预测具有更高总体概率的序列。</p><h1 id="52db" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">波束搜索——工作原理</h1><p id="55a2" class="pw-post-body-paragraph lf lg iq lh b li nj ka lk ll nk kd ln lo nl lq lr ls nm lu lv lw nn ly lz ma ij bi translated">我们现在从概念上理解了波束搜索。让我们更深入一层，了解这是如何工作的细节。我们将继续同一个例子，使用波束宽度2。</p><p id="0870" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">继续我们的序列到序列模型，编码器和解码器可能是一个由一些LSTM层组成的递归网络。或者，它也可以使用变压器而不是循环网络来构建。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi og"><img src="../Images/23ec88038dbddf750cf966dc02e06ffa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dLHChU897ypcetDRRqnMIw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">基于LSTM的序列对序列模型(图片由作者提供)</p></figure><p id="4c4b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们关注解码器组件和输出层。</p><h2 id="7cb1" class="ns ms iq bd mt nt nu dn mx nv nw dp nb lo nx ny nd ls nz oa nf lw ob oc nh iw bi translated">第一位置</h2><p id="2d53" class="pw-post-body-paragraph lf lg iq lh b li nj ka lk ll nk kd ln lo nl lq lr ls nm lu lv lw nn ly lz ma ij bi translated">在第一个时间步中，它使用编码器的输出和一个“<start>”标记的输入来生成第一个位置的字符概率。</start></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/84241b96938784eb632ac60266a0f575.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*unlaSs5XI_68iq3chlqfVQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">第一个位置的字符概率(图片由作者提供)</p></figure><p id="e7b5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">现在，它选择两个概率最高的字符，例如“A”和“C”。</p><h2 id="db8c" class="ns ms iq bd mt nt nu dn mx nv nw dp nb lo nx ny nd ls nz oa nf lw ob oc nh iw bi translated">第二胎位</h2><p id="4e8f" class="pw-post-body-paragraph lf lg iq lh b li nj ka lk ll nk kd ln lo nl lq lr ls nm lu lv lw nn ly lz ma ij bi translated">对于第二个时间步长，它像以前一样使用编码器的输出运行解码器两次。与第一个位置的“<start>”标记一起，它强制第二个位置的输入在第一次解码器运行时为“A”。在第二次解码器运行时，它强制第二个位置的输入为“C”。</start></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi no"><img src="../Images/50bae686e10ce0a6b7e3881f6de67f6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rh5LUbc1xh8sWeMMBN0jjA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">第二个位置的字符概率(图片由作者提供)</p></figure><p id="c39e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">它为第二个位置生成字符概率。但这些都是个别人物的概率。它需要计算前两个位置的字符对的组合概率。假设“A”已经固定在第一个位置，则“AB”对的概率是“A”出现在第一个位置的概率乘以“B”出现在第二个位置的概率。下面的示例显示了计算过程。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/571f136fa2ec28c67c26cba8ff14f9d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*Zz8a8X4MZsjWJEA_9Io7fg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">计算字符对在前两个位置的概率(图片由作者提供)</p></figure><p id="dda3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">它对两次解码器运行都这样做，并在两次运行中挑选具有最高组合概率的字符对。因此，它选择“AB”和“AE”。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi no"><img src="../Images/66ad36818c8a2d1a390a19e8cdf7b782.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4rlMOAAHW9Q9KaT2i4bUyw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">该模型根据组合概率挑选两个最佳字符对(图片由作者提供)</p></figure><h2 id="4f2c" class="ns ms iq bd mt nt nu dn mx nv nw dp nb lo nx ny nd ls nz oa nf lw ob oc nh iw bi translated">第三位置</h2><p id="e38e" class="pw-post-body-paragraph lf lg iq lh b li nj ka lk ll nk kd ln lo nl lq lr ls nm lu lv lw nn ly lz ma ij bi translated">对于第三时间步，它再次像以前一样运行解码器两次。与第一个位置的“<start>”标记一起，它强制第二个位置和第三个位置的输入在第一次解码器运行中分别为“A”和“B”。在第二次解码器运行中，它强制第二位置和第三位置的输入分别为“A”和“E”。</start></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oj"><img src="../Images/8d04e06ee0e68ed1d3dd617fad132dbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Nnka3yYpNo6m3JeermN-1g.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">第三个位置的字符概率(图片由作者提供)</p></figure><p id="b775" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">它计算前三个位置的字符三元组的组合概率。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/9459a912b6930daaf11ada10706779cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*TJbVv78IecgcfBoCcDfQHw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">计算前三个位置的字符三元组的概率(图片由作者提供)</p></figure><p id="643c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">它在两次运行中选择两个最好的，因此选择“ABC”和“AED”。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oj"><img src="../Images/af391968cb5a9b5af644ee4dfb00997f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2edhf25eROJAj1RgTnlChQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">该模型基于组合概率挑选两个最佳字符三元组(图片由作者提供)</p></figure><h2 id="727c" class="ns ms iq bd mt nt nu dn mx nv nw dp nb lo nx ny nd ls nz oa nf lw ob oc nh iw bi translated">重复直到结束标记</h2><p id="5dba" class="pw-post-body-paragraph lf lg iq lh b li nj ka lk ll nk kd ln lo nl lq lr ls nm lu lv lw nn ly lz ma ij bi translated">它重复这个过程，直到生成两个以“<end>”标记结束的最佳序列。</end></p><p id="73ef" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">然后，它选择具有最高组合概率的序列进行最终预测。</p><h1 id="a626" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">结论</h1><p id="83f8" class="pw-post-body-paragraph lf lg iq lh b li nj ka lk ll nk kd ln lo nl lq lr ls nm lu lv lw nn ly lz ma ij bi translated">这让我们了解了波束搜索是做什么的，它是如何工作的，以及为什么它能给我们更好的结果。这是以增加计算量和执行时间为代价的。因此，我们应该评估这种权衡对于我们的应用程序用例是否有意义。</p><p id="6a2d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">最后，如果你喜欢这篇文章，你可能也会喜欢我关于音频深度学习、地理位置机器学习和图像字幕架构的其他系列。</p><div class="ol om gp gr on oo"><a rel="noopener follow" target="_blank" href="/audio-deep-learning-made-simple-part-1-state-of-the-art-techniques-da1d3dff2504"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd ja gy z fp ot fr fs ou fu fw iz bi translated">音频深度学习变得简单(第一部分):最新技术</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">颠覆性深度学习音频应用和架构世界的温和指南。以及为什么我们都需要…</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="oy l oz pa pb ox pc ky oo"/></div></div></a></div><div class="ol om gp gr on oo"><a rel="noopener follow" target="_blank" href="/leveraging-geolocation-data-for-machine-learning-essential-techniques-192ce3a969bc"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd ja gy z fp ot fr fs ou fu fw iz bi translated">利用地理位置数据进行机器学习:基本技术</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">简明的地理空间数据特征工程和可视化指南</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="pd l oz pa pb ox pc ky oo"/></div></div></a></div><div class="ol om gp gr on oo"><a rel="noopener follow" target="_blank" href="/image-captions-with-deep-learning-state-of-the-art-architectures-3290573712db"><div class="op ab fo"><div class="oq ab or cl cj os"><h2 class="bd ja gy z fp ot fr fs ou fu fw iz bi translated">具有深度学习的图像标题:最先进的架构</h2><div class="ov l"><h3 class="bd b gy z fp ot fr fs ou fu fw dk translated">图像特征编码器、序列解码器、注意和多模态架构的简明指南，用简单的英语编写</h3></div><div class="ow l"><p class="bd b dl z fp ot fr fs ou fu fw dk translated">towardsdatascience.com</p></div></div><div class="ox l"><div class="pe l oz pa pb ox pc ky oo"/></div></div></a></div><p id="be43" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们继续学习吧！</p></div></div>    
</body>
</html>