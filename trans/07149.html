<html>
<head>
<title>Weekly review of Reinforcement Learning papers #12</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习试卷#12的每周回顾</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/weekly-review-of-reinforcement-learning-papers-12-9ec3a81720?source=collection_archive---------41-----------------------#2021-06-28">https://towardsdatascience.com/weekly-review-of-reinforcement-learning-papers-12-9ec3a81720?source=collection_archive---------41-----------------------#2021-06-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5b82" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">我在我的研究领域发表了4篇论文。大家来讨论一下吧！</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3cf1100a35d4217350e6c86563f0fdd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*qFeedzXTgJC3qdbc.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="8310" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[ <a class="ae lu" rel="noopener" target="_blank" href="/weekly-review-of-reinforcement-learning-papers-11-7e1780ddf176?sk=18185b32a63640b45cc486270c267584"> ←上一次回顾</a> ][ <a class="ae lu" rel="noopener" target="_blank" href="/review-of-reinforcement-learning-papers-13-24ed69a8fdc2?sk=7b943268ec3a5256ff4cc0f7b48aa81f">下一次回顾→ </a></p><h1 id="17b5" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">论文1:通过新颖性搜索和发射体的稀疏回报探索</h1><p id="6951" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">Paolo，g .，Coninx，a .，Doncieux，s .，和Laflaquière，A. (2021年)。<a class="ae lu" href="https://arxiv.org/abs/2102.03140" rel="noopener ugc nofollow" target="_blank">通过查新和发射体进行稀疏回报探索</a>。arXiv预印本arXiv:2102.03140 。</p><p id="2078" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi mt translated">强化学习的主要权衡是探索与利用的权衡。探索对于发现新的回报是必要的，开发对于利用已经吸收的知识也是必要的。有些环境，比如机器人环境，有非常稀有的奖励的特殊性(称为稀疏)。这项工作的目的是提出一种算法，允许在稀疏奖励的框架内进行有效的探索。他们称他们的方法为SERENE(通过新奇事物和发射器进行稀疏回报探索)。这是什么？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/6884e151730af6272bb53366dd106505.png" data-original-src="https://miro.medium.com/v2/resize:fit:1018/format:webp/1*-63LMk4AEmKFAti467YegQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图来自<a class="ae lu" href="https://arxiv.org/abs/2102.03140" rel="noopener ugc nofollow" target="_blank">文章</a>:宁静的建筑</p></figure><p id="8279" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这种算法的主要区别在于，它将清楚地区分探索过程和开发过程。换句话说，代理从探索过程开始，然后切换到利用过程，依此类推。代理如何决定是探索还是剥削？这是元调度器的角色，它必须为每个模式分配正确的时间。</p><p id="7a6f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">探索阶段实际上是奖励不可知论者，并且只寻求最大化所遇到的状态的新颖性。这允许有效地发现新的领域，而仅仅通过寻找奖励是永远不会被发现的。然后在开发过程中，代理将创建算法的本地实例，他们称之为发射器。这些发射器是基于奖励的进化算法的实例。它们适合于在空间的小区域上优化奖励。</p><p id="7c25" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所获得的结果并不具有启发性，但是在我看来，区分探索阶段和开发阶段的想法使得算法具有更好的可读性，从而允许以更好的方式管理折衷。</p><h1 id="16bc" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">论文2:针对深度强化学习策略的实时攻击</h1><p id="adcc" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">泰克古尔，B. G .，王，s .，&amp;阿索坎，N. (2021)。<a class="ae lu" href="https://arxiv.org/abs/2106.08746" rel="noopener ugc nofollow" target="_blank">针对深度强化学习策略的实时攻击</a>。<em class="ms"> arXiv预印本arXiv:2106.08746 </em>。</p><p id="a6ec" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi mt translated"><span class="l mu mv mw bm mx my mz na nb di"> D </span> eep强化学习代理和我们一样，容易受到攻击。什么是攻击？想象一个交易代理，被训练买卖股票。通过训练一个<em class="ms">攻击</em>智能体，可以愚弄第一个智能体，并通过仔细扰乱观察使他做出错误的决定。因此，代理人可能买入大量下跌的股票，或者相反，在错误的时机卖出非常有利可图的股票。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/7fc63755b892bd688675ec9de90ea185.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pITkaK35tsaMoYvYvXPKsA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图来自<a class="ae lu" href="https://arxiv.org/abs/2106.08746" rel="noopener ugc nofollow" target="_blank">篇</a>。</p></figure><p id="1e21" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">到目前为止，主要的限制是这些策略速度缓慢，无法实时部署。在这篇论文中，作者提出了一种方法，他们称之为普遍对抗性扰动(UAP)。他们在ATARI2600游戏中表明，通过正确训练扰动者，通过应用低振幅(0.5%)的扰动，有可能使主代理的性能灾难性地下降。此外，他们表明，与以前的方法不同，他们的算法的轻便性允许其实时使用。</p><p id="eacf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是好消息吗？不确定。</p><h1 id="92d8" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">论文3: DouZero:通过自我游戏深度强化学习掌握斗地主</h1><p id="a910" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">查大丁，谢军，马，张，连，胡，刘军(2021)。<a class="ae lu" href="https://arxiv.org/abs/2106.06135" rel="noopener ugc nofollow" target="_blank"> DouZero:用自玩深度强化学习掌握斗地主</a>。<em class="ms"> arXiv预印本arXiv:2106.06135 </em>。</p><p id="0419" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">斗朱迪在中国是一种非常受欢迎的纸牌游戏。它是由三个玩家玩的。每个玩家从一手牌开始，目标是在其他玩家之前除掉他们。规则很简单，这使得它非常容易学习。然而，它有着极难掌握的特殊性。为了正确地玩游戏，你需要从数学、概率和战略的角度去思考。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/2b1f29093d3b13ddc94423069e973caf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FHqz3EwsKW--l7Z7"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae lu" href="https://unsplash.com/@marlulla?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">Ulrich&amp;Mareli Aspeling</a>在<a class="ae lu" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="68f7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你可以想象一些人想知道这个游戏是否能抵抗强化学习。这篇文章的作者介绍了DouZero，一个通过掌握这种游戏来训练自我游戏的DRL特工。有趣的是实现。在这种游戏中，每一轮都可以放一张或多张牌。但是要小心，不是随便什么卡。合法的诉讼数量有限。这使得实现有点复杂。在这项工作中，作者提出了两个主要成分:<strong class="la iu">动作编码</strong>和<strong class="la iu">并行演员</strong>。训练有素的代理实现的性能超过了任何现有的机器人。似乎没有一款游戏能抵抗这种诱惑</p><h1 id="aa6f" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">论文4:通过潜在空间配置的基于模型的强化学习</h1><p id="f79f" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">朱，c .，纳加班迪，a .，达尼利迪斯，k .，莫尔达奇，I .，，莱文，S. (2020)。<a class="ae lu" href="https://arxiv.org/abs/2106.13229" rel="noopener ugc nofollow" target="_blank">基于模型的潜在空间配置强化学习</a>。</p><p id="12ac" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi mt translated">基于视觉感知的强化学习的问题在于观察空间非常大(每个像素的所有可能值的乘积)。当然，空间中的大部分观察点是不可到达的，对于所考虑的环境，只有少数会被访问。然而，这足以使强化学习算法，尤其是那些基于规划的算法变得困难。一个非常有效的解决方案是创建一个空间的潜在表示，一个更精简的观察版本，但它在理论上包含了输入中存在的所有数据。但让我们面对现实吧，长期规划仍远未明朗。事实上，使用潜在观察空间的算法通常可以预测一些未来的观察，但仍然无法进行长期规划。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/12a84097aa6bbd36767d02055d39adbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y5kMVMdNi2M-I-9Mhs05KQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图来自<a class="ae lu" href="https://arxiv.org/abs/2106.13229" rel="noopener ugc nofollow" target="_blank">论文</a> : LatCo架构</p></figure><p id="bf46" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">他们的假设如下:与其计划一系列的行动(通常在文献中是这样做的)，不如通过计划一系列的状态来更容易地解决一个任务。这背后的直觉是，每个动作对轨迹的其余部分都有重要的影响。因此，最小的误差很快就会产生相当大的预测误差。<br/>相反，作者使用了配置法，这种方法优化了一个状态序列，使报酬最大化。这种方法还保证了轨迹的可行性。</p><p id="8362" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有趣的是，他们称之为LatCo(潜在搭配)的这种方法，在某些环境中可以极大地改善结果，但在其他环境中却不行。这将是有趣的调查，以了解这种结果差异的原因。</p></div><div class="ab cl ng nh hx ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="im in io ip iq"><p id="630a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">谢谢你把我的文章看完。我很乐意阅读你的评论。</p></div></div>    
</body>
</html>