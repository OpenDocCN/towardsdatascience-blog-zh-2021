<html>
<head>
<title>Customer Churn Accuracy: Increased 4.6% With TextBlob &amp; SBERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">客户流失准确度:通过TextBlob &amp; SBERT提高了4.6%</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/customer-churn-accuracy-a-4-6-increase-with-feature-engineering-29bcb1b1ee8f?source=collection_archive---------30-----------------------#2021-05-10">https://towardsdatascience.com/customer-churn-accuracy-a-4-6-increase-with-feature-engineering-29bcb1b1ee8f?source=collection_archive---------30-----------------------#2021-05-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="96ba" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">关于我如何通过客户服务说明将小型客户流失数据集的准确率提高了4.6%的演练</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/590e32b928774157cc4fd9276a281f36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3uIVWmVYsXUhAbZnZPEcCQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><h1 id="f9a2" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">目录</h1><ul class=""><li id="e128" class="ln lo iq lp b lq lr ls lt lu lv lw lx ly lz ma mb mc md me bi translated"><strong class="lp ir">动机</strong></li><li id="2f20" class="ln lo iq lp b lq mf ls mg lu mh lw mi ly mj ma mb mc md me bi translated"><strong class="lp ir">商业问题&amp;数据</strong></li><li id="b0e9" class="ln lo iq lp b lq mf ls mg lu mh lw mi ly mj ma mb mc md me bi translated"><strong class="lp ir">特征工程&amp;建模</strong></li><li id="6536" class="ln lo iq lp b lq mf ls mg lu mh lw mi ly mj ma mb mc md me bi translated"><strong class="lp ir">评估&amp;特征分析</strong></li><li id="bf4d" class="ln lo iq lp b lq mf ls mg lu mh lw mi ly mj ma mb mc md me bi translated"><strong class="lp ir">总结</strong></li></ul><h1 id="b2f0" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">动机</h1><p id="a8c2" class="pw-post-body-paragraph mk ml iq lp b lq lr jr mm ls lt ju mn lu mo mp mq lw mr ms mt ly mu mv mw ma ij bi translated">在我上一篇博客“<a class="ae mx" rel="noopener" target="_blank" href="/what-is-embedding-and-what-can-you-do-with-it-61ba7c05efd8">什么是嵌入，你能用它做什么</a>”中，我谈到了嵌入可以将高维、非结构化的数据变成低维的数值表示，可以用在各种机器学习模型中。在另一篇博客“<a class="ae mx" rel="noopener" target="_blank" href="/what-is-mlops-and-why-we-should-care-9b2d79a29e75">什么是MLOps，为什么我们应该关注</a>”中，我还提到高质量的数据将有助于将机器学习引入小企业的运营。因此，在今天的博客中，我将向您介绍我是如何利用额外的客户服务注释将小型客户流失数据集的准确率提高4%的。</p><p id="868a" class="pw-post-body-paragraph mk ml iq lp b lq my jr mm ls mz ju mn lu na mp mq lw nb ms mt ly nc mv mw ma ij bi translated">我在我以前的一个项目中建立了这个博客。然而，我通过应用情感分析和SBERT句子嵌入扩展了旧项目。然后我用XGBoost(流行的竞争算法)和Random Forests(流行的研究算法)拟合数据。</p><h1 id="851a" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">业务问题和数据</h1><p id="972f" class="pw-post-body-paragraph mk ml iq lp b lq lr jr mm ls lt ju mn lu mo mp mq lw mr ms mt ly mu mv mw ma ij bi translated">一家电话公司从其2070名客户那里收集了原始数据集，标记为服务状态(当前/已取消)。该电话公司希望使用这些数据来了解客户流失问题，以便采取战略性措施来留住那些将来可能会取消服务的客户。这种分析也被认为有助于公司识别使顾客取消服务的因素。</p><p id="2a64" class="pw-post-body-paragraph mk ml iq lp b lq my jr mm ls mz ju mn lu na mp mq lw nb ms mt ly nc mv mw ma ij bi translated">数据集包含17个要素，包括客户id、一般人口统计信息和服务使用信息。该公司还提供了其客服留下的评论，指出了客户的问题以及他们如何帮助客户。标签以3:2的比例分发。还有115个独特的客户服务说明。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/454d091056f4fcfec8a32d845a5dd791.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*Zc4mBkyacxB_h116VLPPsQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><h1 id="e947" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated"><strong class="ak">特征工程&amp;建模</strong></h1><p id="ab3d" class="pw-post-body-paragraph mk ml iq lp b lq lr jr mm ls lt ju mn lu mo mp mq lw mr ms mt ly mu mv mw ma ij bi translated">我将在我们的数据上演示四种不同的特征工程方法:首先，我对所有的分类变量使用one-hot-encoding。其次，我对客户服务记录应用了情感分析、句子嵌入和TF-IDF。</p><h2 id="277b" class="ne kw iq bd kx nf ng dn lb nh ni dp lf lu nj nk lh lw nl nm lj ly nn no ll np bi translated">一次热编码</h2><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="3f0f" class="ne kw iq nr b gy nv nw l nx ny"># load package<br/>from sklearn.preprocessing import LabelEncoder<br/>import pandas as pd</span><span id="e4bd" class="ne kw iq nr b gy nz nw l nx ny"># read data<br/>customer = pd.read_csv("Customers.csv")<br/>comment = pd.read_csv("Comments.csv")</span><span id="8119" class="ne kw iq nr b gy nz nw l nx ny"># extract the categorical variables<br/>le = LabelEncoder()<br/>col = customer.select_dtypes(include=object)<br/>col_names = col.columns</span><span id="8690" class="ne kw iq nr b gy nz nw l nx ny"># transform the categorical variables<br/>for x in col_names:<br/>customer[x]=le.fit(customer[x]).transform(customer[x])</span></pre><p id="a07b" class="pw-post-body-paragraph mk ml iq lp b lq my jr mm ls mz ju mn lu na mp mq lw nb ms mt ly nc mv mw ma ij bi translated">LabelEncoder将帮助我们将所有分类变量转化为数字。比如它会把性别变量(M/F)变成[1或者2]。由于这个项目的主要重点是演示如何将文本特征纳入我们的分析，我没有对数据做任何额外的特征工程。</p><h2 id="9224" class="ne kw iq bd kx nf ng dn lb nh ni dp lf lu nj nk lh lw nl nm lj ly nn no ll np bi translated">情感分析</h2><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="d15a" class="ne kw iq nr b gy nv nw l nx ny"># load textblob<br/>from textblob import TextBlob</span><span id="078a" class="ne kw iq nr b gy nz nw l nx ny"># define functions to extract polarity and subjectivity<br/>def find_pol(review):<br/>    return TextBlob(review).sentiment.polarity<br/>def find_sub(review):<br/>    return TextBlob(review).sentiment.subjectivity</span><span id="aec3" class="ne kw iq nr b gy nz nw l nx ny"># perform the sentiment analysis<br/>customer['sentiment'] = comment['Comments'].apply(find_pol)<br/>customer['subjectivity'] = comment['Comments'].apply(find_sub)</span><span id="4640" class="ne kw iq nr b gy nz nw l nx ny"># assign the average score to the same message<br/>for i in range(0,115):<br/>    value = customer[customer.Comments==i].sentiment<br/>    mean = customer[customer.Comments==i].sentiment.mean()<br/>    std = customer[customer.Comments==i].sentiment.std()<br/>    outliers = (value-mean).abs() &gt; std<br/>    customer["sentiment"][outliers.index]= mean</span></pre><p id="ec86" class="pw-post-body-paragraph mk ml iq lp b lq my jr mm ls mz ju mn lu na mp mq lw nb ms mt ly nc mv mw ma ij bi translated">在这个项目中，我使用了TextBlob，一种流行的情感分析技术，来提取极性和主观性。然而，不知何故，它给了相同的信息不同的分数。所以我做了一个转换来平均出相同信息的分数方差。你可以在这里找到更多关于textblob <a class="ae mx" href="https://textblob.readthedocs.io/en/dev/quickstart.html#sentiment-analysis" rel="noopener ugc nofollow" target="_blank">的信息。</a></p><h2 id="1970" class="ne kw iq bd kx nf ng dn lb nh ni dp lf lu nj nk lh lw nl nm lj ly nn no ll np bi translated">用SBERT嵌入句子</h2><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="ebf1" class="ne kw iq nr b gy nv nw l nx ny"># load SBERT from sentence_transformer<br/>from sentence_transformers import SentenceTransformer<br/>sbert_model = SentenceTransformer('stsb-mpnet-base-v2',<br/>                                   device='cuda')</span><span id="36c9" class="ne kw iq nr b gy nz nw l nx ny">sentence_embeddings = sbert_model.encode(comment.Comments) <br/># the shape of the embedding is (2070,768), convert it to 1<br/>sent_emb = pd.DataFrame(sentence_embeddings).mean(axis=1)<br/>customer["sent_emb"] = sent_emb</span></pre><p id="2daa" class="pw-post-body-paragraph mk ml iq lp b lq my jr mm ls mz ju mn lu na mp mq lw nb ms mt ly nc mv mw ma ij bi translated">句子BERT (SBERT)是自然语言处理中最先进的模型之一。它基于BERT的想法，由德国达姆施塔特技术大学的UKP实验室开发。他们有许多针对各种用途的预训练模型。你可以在这里查看完整的<a class="ae mx" href="https://docs.google.com/spreadsheets/d/14QplCdTCDwEmTqrn1LH4yrbKvdogK4oQvYO1K1aPR5M/edit#gid=0" rel="noopener ugc nofollow" target="_blank">名单</a>。</p><p id="8abf" class="pw-post-body-paragraph mk ml iq lp b lq my jr mm ls mz ju mn lu na mp mq lw nb ms mt ly nc mv mw ma ij bi translated">句子嵌入有768个特征，这对我们的模型来说太多了。我用Tensorflow测试了autoencoder，但它没有显示出明显的改进。因此，我对这些值进行平均，将它们转换为每个客户的一个值。</p><h2 id="4010" class="ne kw iq bd kx nf ng dn lb nh ni dp lf lu nj nk lh lw nl nm lj ly nn no ll np bi translated">基于TF-IDF的词汇抽取</h2><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="10a2" class="ne kw iq nr b gy nv nw l nx ny"># load packages<br/>from sklearn.feature_extraction.text import TfidfVectorizer<br/>import math</span><span id="a917" class="ne kw iq nr b gy nz nw l nx ny"># calculate the # of unique words in the corpus<br/>feature = []<br/>for i in df["patterns"]:<br/>    feature.append(i.split())<br/>    flat_list = [item for items in feature for item in items]<br/>    feature=len(set(flat_list))</span><span id="08d9" class="ne kw iq nr b gy nz nw l nx ny"># only take the top 10% of the most frequent words<br/># set the max document frequency to 0.2 to maintain the uniqueness</span><span id="e413" class="ne kw iq nr b gy nz nw l nx ny">max_feature = math.ceil(feature*0.1)<br/>tfidfvectoriser=TfidfVectorizer(strip_accents="ascii",<br/>                                max_features=max_feature,<br/>                                max_df = 0.2)<br/>tfidfvectoriser.fit(df["patterns"])<br/>tfidf_vectors=tfidfvectoriser.transform(df["patterns"])<br/>tfidf_vectors=pd.DataFrame(tfidf_vectors.toarray())<br/>tfidf_vectors.columns = tfidfvectoriser.get_feature_names()</span><span id="3638" class="ne kw iq nr b gy nz nw l nx ny"># merge the words into the original table<br/>customer = pd.merge(customer,<br/>                    tfidf_vectors,<br/>                    left_index=True,<br/>                    right_index=True)</span></pre><p id="75ae" class="pw-post-body-paragraph mk ml iq lp b lq my jr mm ls mz ju mn lu na mp mq lw nb ms mt ly nc mv mw ma ij bi translated">到目前为止，我们已经对文本数据进行了情感分析、句子嵌入和TF-IDF。这三种不同的技术帮助我们从文档级、句子级和词汇级提取信息。现在，让我们看看这些功能工程会对模型产生什么影响，并探索影响客户决定保留或离开服务的功能。</p><h1 id="70c6" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated"><strong class="ak">评测&amp;特征分析</strong></h1><p id="36d0" class="pw-post-body-paragraph mk ml iq lp b lq lr jr mm ls lt ju mn lu mo mp mq lw mr ms mt ly mu mv mw ma ij bi translated">由于我只有一个相当小的数据集(2070个观察值)，过拟合很有可能发生。因此，我使用了交叉验证技术，而不是简单地将其分成训练和测试数据集。我将折叠数设置为10，并将平均准确度和平均roc_auc_score作为最终输出。</p><h2 id="4c24" class="ne kw iq bd kx nf ng dn lb nh ni dp lf lu nj nk lh lw nl nm lj ly nn no ll np bi translated">XGBoost</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/dc5ebc711fccb201198f33c8701ddd9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-OGF5E2ORhf4bsLB9SxH7w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="e93c" class="pw-post-body-paragraph mk ml iq lp b lq my jr mm ls mz ju mn lu na mp mq lw nb ms mt ly nc mv mw ma ij bi translated">我从基本的xgboostclassifier模型开始，逐步将情感特征、句子嵌入和TF-IDF添加到模型中。如您所见，通过添加这三个新功能，准确性得分比基础模型提高了2.7%，roc-auc得分比基础模型提高了1.3%。用<br/> <a class="ae mx" href="https://github.com/fmfn/BayesianOptimization" rel="noopener ugc nofollow" target="_blank">贝叶斯优化</a>进行超参数调优后，<strong class="lp ir"> <em class="ob">最终准确率评分比基础模型提高了4.6% </em> </strong>，roc-auc评分也上升了1.9%。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/d004e024c8b2ff44af46fa8493bafab4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1112/format:webp/1*fmsx2HIrEquZjfxMHOOnzg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="8138" class="pw-post-body-paragraph mk ml iq lp b lq my jr mm ls mz ju mn lu na mp mq lw nb ms mt ly nc mv mw ma ij bi translated">上面的混淆矩阵显示，假阳性和假阴性的数量非常接近，所以没有发生过度拟合。如果企业想找出模型无法识别这些客户的原因，我认为他们应该手动查看数据，研究他们内部的相似性以及这些客户与其他人之间的不同之处。</p><h2 id="d9cb" class="ne kw iq bd kx nf ng dn lb nh ni dp lf lu nj nk lh lw nl nm lj ly nn no ll np bi translated">随机森林</h2><p id="b775" class="pw-post-body-paragraph mk ml iq lp b lq lr jr mm ls lt ju mn lu mo mp mq lw mr ms mt ly mu mv mw ma ij bi translated">你可能已经注意到，在许多学术研究论文中，作者通常更喜欢随机森林而不是梯度推进机器。我发现在<a class="ae mx" href="https://www.quora.com/When-would-one-use-Random-Forests-over-Gradient-Boosted-Machines-GBMs" rel="noopener ugc nofollow" target="_blank"> Quora </a>上解释了两个原因:第一，RF比GBM容易调得多；其次，RF比GBM更难过拟合。因此，我还检查了这些特性如何适用于随机森林。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi od"><img src="../Images/5346f307211089def8fa66b4b3216827.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PQbrLXS50GMcyAnVWN-3gw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="4948" class="pw-post-body-paragraph mk ml iq lp b lq my jr mm ls mz ju mn lu na mp mq lw nb ms mt ly nc mv mw ma ij bi translated">如图所示，最终的准确度分数和roc_auc分数分别上升了3.8%和1%。对于超参数调谐RF，我发现<a class="ae mx" rel="noopener" target="_blank" href="/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74">这篇文章</a>可能有用。</p><h2 id="823e" class="ne kw iq bd kx nf ng dn lb nh ni dp lf lu nj nk lh lw nl nm lj ly nn no ll np bi translated">功能的重要性</h2><p id="b56f" class="pw-post-body-paragraph mk ml iq lp b lq lr jr mm ls lt ju mn lu mo mp mq lw mr ms mt ly mu mv mw ma ij bi translated">为了研究模型中最重要的特性，我使用了xgboost内置函数:“plot_importance”</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="b03a" class="ne kw iq nr b gy nv nw l nx ny">from xgboost import plot_importance</span><span id="bbef" class="ne kw iq nr b gy nz nw l nx ny">plot_importance(model_final)<br/>plt.subplots_adjust(left=0.5, bottom=1.6, right=2.4, top=6)<br/>plt.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oe"><img src="../Images/e93caaf6014add4920f3d5a9c8d1078d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tjqaO9is9U_KDqSzd5m47g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="feaf" class="pw-post-body-paragraph mk ml iq lp b lq my jr mm ls mz ju mn lu na mp mq lw nb ms mt ly nc mv mw ma ij bi translated">它表明，最重要的特征是:年龄，费率计划，长途，估计收入，客户服务信息，主观性，情绪极性等。</p><p id="06ad" class="pw-post-body-paragraph mk ml iq lp b lq my jr mm ls mz ju mn lu na mp mq lw nb ms mt ly nc mv mw ma ij bi translated">通过研究这些特征并执行一些基本的统计分析，我发现，与取消服务的客户相比，<em class="ob">当前客户平均年龄大5个月，费率计划更低，长途电话更少，每年多产生大约7，000美元，并且得到了更积极、更少主观的服务。</em></p><h1 id="2201" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated"><strong class="ak">总结</strong></h1><p id="3960" class="pw-post-body-paragraph mk ml iq lp b lq lr jr mm ls lt ju mn lu mo mp mq lw mr ms mt ly mu mv mw ma ij bi translated">在这篇博客中，我演示了如何通过从文档级、句子级和词汇级提取信息来将文本数据合并到分类问题中。这个项目展示了精细标记的小型数据集如何为小型企业实现理想的性能。它还说明了如何使用自然语言处理技术来促进监督机器学习问题，如分类。分析表明，我创建的功能是模型中最重要的功能之一，它们有助于建立对不同客户群的描述。</p><p id="1db8" class="pw-post-body-paragraph mk ml iq lp b lq my jr mm ls mz ju mn lu na mp mq lw nb ms mt ly nc mv mw ma ij bi translated"><strong class="lp ir"> <em class="ob">联系我上</em></strong><a class="ae mx" href="https://www.linkedin.com/in/jinhangjiang/" rel="noopener ugc nofollow" target="_blank"><strong class="lp ir"><em class="ob">LinkedIn</em></strong></a><strong class="lp ir"><em class="ob">如果你想讨论任何关于数据科学的事情！</em>T13】</strong></p><h1 id="b062" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated"><strong class="ak">相关阅读</strong>:</h1><p id="ee5f" class="pw-post-body-paragraph mk ml iq lp b lq lr jr mm ls lt ju mn lu mo mp mq lw mr ms mt ly mu mv mw ma ij bi translated"><a class="ae mx" rel="noopener" target="_blank" href="/integrate-text-content-into-classification-project-eddd8e18a7e9"> <strong class="lp ir">将文本特征纳入分类项目</strong> </a></p></div></div>    
</body>
</html>