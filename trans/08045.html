<html>
<head>
<title>Support Vector Machines — multinomial example</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机——多项式示例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/support-vector-machines-multinomial-example-8d9a86dde1d3?source=collection_archive---------23-----------------------#2021-07-23">https://towardsdatascience.com/support-vector-machines-multinomial-example-8d9a86dde1d3?source=collection_archive---------23-----------------------#2021-07-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f154" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">SVM非常适合对数据进行分类，你可以看到很多二项式的例子。这是一个很好的Matlab多项式例子…</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/1d15a8e4f1625b9052d5cc34e66f3477.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ABSiT821jtSch1fxxl6XOQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Sani2C 2017</p></figure><p id="b013" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你刚到这里，对SVM还不是很了解，那么先看看这篇文章。</p><h1 id="9d2e" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">让我们从一个好的数据集开始。</h1><p id="aa77" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">Iris flower数据集是由英国统计学家和生物学家罗纳德·费雪在其1936年的论文《分类问题中多重测量的使用》中引入的多元数据集。它有时被称为安德森虹膜数据集，因为埃德加·安德森收集的数据量化了三个相关物种的虹膜花的形态变化。该数据集由来自三种鸢尾(刚毛鸢尾、海滨鸢尾和杂色鸢尾)的每一种的50个样本组成。测量每个样品的四个特征:萼片和花瓣的长度和宽度，以厘米为单位。你可以从<a class="ae lr" href="https://www.kaggle.com/arshid/iris-flower-dataset" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>下载。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/16792d41e134dff59447f1e2de88ef8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JGd_6raoBWOKxu6OUeY1Bg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1:源Kaggle</p></figure><h1 id="0e60" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">加载数据集</h1><p id="3d38" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">我们的第一步是加载数据集，并让归一化我们的特征以获得更好的性能。在创建<strong class="kx ir"> y </strong>时，让我们使用grp2idx将物种转换成数字。</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="71c0" class="mv lt iq mr b gy mw mx l my mz">%% Loading our dataset<br/>clear;<br/>tbl = readtable(‘IRIS.csv’);<br/>[m,n] = size(tbl);</span><span id="7e75" class="mv lt iq mr b gy na mx l my mz">X = tbl{:,1:n-1};<br/>[y,labels] = grp2idx(tbl{:,n});</span><span id="9fd1" class="mv lt iq mr b gy na mx l my mz">nl = length(labels);<br/>[X_norm, mu, sigma] = featureNormalize(X);</span></pre><h1 id="e6df" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">分割训练和测试数据集</h1><p id="9435" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">让我们把数据集分成一个训练集和一个测试集，确保随机进行。分成比例是80/20。</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="60f6" class="mv lt iq mr b gy mw mx l my mz">%% split up train, cross validation and test set<br/>rand_num = randperm(size(X,1));<br/>X_train = X(rand_num(1:round(0.8*length(rand_num))),:);<br/>y_train = y(rand_num(1:round(0.8*length(rand_num))),:);<br/>X_test = X(rand_num(round(0.8*length(rand_num))+1:end),:);<br/>y_test = y(rand_num(round(0.8*length(rand_num))+1:end),:);</span><span id="a173" class="mv lt iq mr b gy na mx l my mz">cv = cvpartition(y_train,’k’,5);</span></pre><h1 id="f565" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">特征选择</h1><p id="c90b" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">我们需要决定哪些是我们最好的特性，所以让sequentialfs为我们做这项工作。然而，sequentialfs需要一个代价函数，它被定义为一个名为'<strong class="kx ir"> costfun </strong>'的内嵌泛函。该函数使用<a class="ae lr" href="https://www.mathworks.com/help/stats/classreg.learning.classif.compactclassificationsvm.loss.html#d123e538439" rel="noopener ugc nofollow" target="_blank"> matlab的<strong class="kx ir">损失</strong>函数</a>来评估每个模型的分类误差。最后，删除不需要的列</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="1d76" class="mv lt iq mr b gy mw mx l my mz">%% feature selection</span><span id="3518" class="mv lt iq mr b gy na mx l my mz">opts = statset(‘display’,’iter’);<br/>costfun = @(XT,yT,Xt,yt)loss(fitcecoc(XT,yT),Xt,yt);</span><span id="7255" class="mv lt iq mr b gy na mx l my mz">[fs, history] = sequentialfs(costfun, X_train, y_train, ‘cv’, cv, ‘options’, opts); </span><span id="d71f" class="mv lt iq mr b gy na mx l my mz">% Remove unwanted columns<br/>X_train = X_train(:,fs);<br/>X_test = X_test(:,fs);</span></pre><p id="638a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">运行特性选择后，您应该会看到它所选择的列[1 3 4]。这是一个小数据集，你可能会得到<strong class="kx ir">不同的</strong>结果，但我相信你已经明白了；-).如果你运行一个corrplot([X y])，看看最下面一行，就能理解为什么1，3 &amp; 4在y轴上有最好的结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nb"><img src="../Images/2d1ab7e08dab961a623502341f2f05e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*96MvguOcVDnezoJQaBE9HA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2</p></figure><h1 id="d87a" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">火车SVM</h1><p id="b038" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">Matlab有一个很棒的函数叫做<a class="ae lr" href="https://www.mathworks.com/help/stats/fitcecoc.html#d123e295398" rel="noopener ugc nofollow" target="_blank"><strong class="kx ir">fitcecoc</strong></a><strong class="kx ir"/>代表我们适合SVM的多类模型。</p><blockquote class="nc nd ne"><p id="13de" class="kv kw nf kx b ky kz jr la lb lc ju ld ng lf lg lh nh lj lk ll ni ln lo lp lq ij bi translated">太好了。我们不需要做数学计算…</p></blockquote><p id="8f88" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们选择使用高斯核来评估我们的模型。如果你需要介绍的话，我会在这里解释高斯<a class="ae lr" href="https://shaun-enslin.medium.com/support-vector-machines-the-basics-5a2acc0db450" rel="noopener"/>。然后让我们用损失函数来计算我们的准确度。</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="2c00" class="mv lt iq mr b gy mw mx l my mz">%% check loss against test dataset<br/>t = templateSVM(‘KernelFunction’,’gaussian’);<br/>mdl = fitcecoc(X_train, y_train,’Learners’,t,’Coding’,’onevsone’);<br/>L = loss(mdl,X_test,y_test) * 100</span></pre><p id="2ba0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果选择了[1 3 4]列，那么您应该会看到0%的损失。</p><h1 id="4516" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">预测的</h1><p id="c857" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">好，现在我们知道我们有100%的准确率，因为损失是0%。Matlab还有一个方便的<strong class="kx ir">预测</strong>功能来帮助我们做预测。在下面，我们需要使用featureNormalize中的<strong class="kx ir"> mu </strong>和<strong class="kx ir"> sigma </strong>来确保我们的比例是正确的。当特征为[5.2 3.5 1.2 0.3]时，我们预测结果。</p><pre class="kg kh ki kj gt mq mr ms mt aw mu bi"><span id="9e38" class="mv lt iq mr b gy mw mx l my mz">%% Predict a result</span><span id="6574" class="mv lt iq mr b gy na mx l my mz">px = bsxfun(@minus, [5.2 3.5 1.2 0.3], mu);<br/>px = bsxfun(@rdivide, px, sigma);</span><span id="138d" class="mv lt iq mr b gy na mx l my mz">predict(mdl,px([1 3 4]))</span></pre><p id="74f2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一切顺利，你的结果是“1”，也就是“Iris-setosa”。</p><h1 id="ca24" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结论</h1><p id="99ce" class="pw-post-body-paragraph kv kw iq kx b ky mk jr la lb ml ju ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">尽管我们在这里进行了更详细的讨论，多项式SVM在Matlab中使用起来并不复杂。一如既往，一切都与数据有关，因此拥有一个好的数据集是关键。</p></div></div>    
</body>
</html>