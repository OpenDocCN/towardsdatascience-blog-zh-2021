<html>
<head>
<title>How to improve your linear regression with basis functions and regularization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用基函数和正则化改进线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-improve-your-linear-regression-with-basis-functions-and-regularization-8a6fcebdc11c?source=collection_archive---------9-----------------------#2021-03-25">https://towardsdatascience.com/how-to-improve-your-linear-regression-with-basis-functions-and-regularization-8a6fcebdc11c?source=collection_archive---------9-----------------------#2021-03-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d0f9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">基础函数和正则化介绍，理论和Python实现</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/d1a5c50b8410d741e2c9e89806fe28b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*CQGp1ado4L3QTRt6L6K1sg.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">对过度拟合的模型增加正则化的说明。图片作者。</p></figure><h1 id="165b" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">内容</h1><p id="4ac2" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">这篇文章是我将要发表的一系列文章的一部分。你可以通过点击<a class="ae mi" href="https://cookieblues.github.io/guides/2021/03/22/bsmalea-notes-2/" rel="noopener ugc nofollow" target="_blank">这里</a>在我的个人博客上阅读这篇文章的更详细版本。下面你可以看到该系列的概述。</p><h2 id="5c7d" class="mj kv it bd kw mk ml dn la mm mn dp le lv mo mp lg lz mq mr li md ms mt lk mu bi translated">1.机器学习导论</h2><ul class=""><li id="2a8e" class="mv mw it lo b lp lq ls lt lv mx lz my md mz mh na nb nc nd bi translated"><a class="ae mi" rel="noopener" target="_blank" href="/what-is-machine-learning-91040db474f9">(一)什么是机器学习？</a></li><li id="d008" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated"><a class="ae mi" rel="noopener" target="_blank" href="/model-selection-in-machine-learning-813fe2e63ec6"> (b)机器学习中的模型选择</a></li><li id="f7b1" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated"><a class="ae mi" rel="noopener" target="_blank" href="/the-curse-of-dimensionality-5673118fe6d2"> (c)维度的诅咒</a></li><li id="84d7" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated"><a class="ae mi" rel="noopener" target="_blank" href="/what-is-bayesian-inference-4eda9f9e20a6"> (d)什么是贝叶斯推理？</a></li></ul><h2 id="8937" class="mj kv it bd kw mk ml dn la mm mn dp le lv mo mp lg lz mq mr li md ms mt lk mu bi translated"><strong class="ak"> 2。回归</strong></h2><ul class=""><li id="765d" class="mv mw it lo b lp lq ls lt lv mx lz my md mz mh na nb nc nd bi translated"><a class="ae mi" rel="noopener" target="_blank" href="/how-linear-regression-actually-works-theory-and-implementation-8d8dcae3222c"> (a)线性回归的实际工作原理</a></li><li id="a9d1" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated"><strong class="lo iu"> (b)如何使用基函数和正则化改进您的线性回归</strong></li></ul><h2 id="012c" class="mj kv it bd kw mk ml dn la mm mn dp le lv mo mp lg lz mq mr li md ms mt lk mu bi translated">3.分类</h2><ul class=""><li id="d387" class="mv mw it lo b lp lq ls lt lv mx lz my md mz mh na nb nc nd bi translated"><a class="ae mi" rel="noopener" target="_blank" href="/overview-of-classifiers-d0a0d3eecfd1"> (a)分类器概述</a></li><li id="9543" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated"><a class="ae mi" rel="noopener" target="_blank" href="/quadratic-discriminant-analysis-ae55d8a8148a#204a-71584f33e137"> (b)二次判别分析(QDA) </a></li><li id="2808" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated"><a class="ae mi" rel="noopener" target="_blank" href="/linear-discriminant-analysis-1894bbf04359"> (c)线性判别分析</a></li><li id="43e1" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated"><a class="ae mi" rel="noopener" target="_blank" href="/gaussian-naive-bayes-4d2895d139a"> (d)(高斯)朴素贝叶斯</a></li></ul><h1 id="3bbd" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">基本函数</h1><p id="45fa" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">在<a class="ae mi" rel="noopener" target="_blank" href="/how-linear-regression-actually-works-theory-and-implementation-8d8dcae3222c">之前的帖子</a>中我们讨论了线性回归模型</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/0e83bbc358926a8ec3f7c73c367c7f02.png" data-original-src="https://miro.medium.com/v2/resize:fit:388/1*oRtKOI9oCtjF7c7968wLhA.gif"/></div></figure><p id="912e" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">我们说<strong class="lo iu">一个模型是线性的，如果它在参数中是线性的，而不是在输入变量</strong>中。然而，<strong class="lo iu"> (1)在参数和输入变量</strong>中都是线性的，这限制了它适应非线性关系。我们可以通过用输入变量的非线性基函数代替输入变量来扩充模型</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi np"><img src="../Images/f357ea871c179d17b71044ad52b3089b.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/1*h9tanXLzUrF4ArW_CKfJpA.gif"/></div></div></figure><p id="b211" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">在哪里</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/fe305f3b33510eb73167bc2269eccaac.png" data-original-src="https://miro.medium.com/v2/resize:fit:362/1*P3trsPvLaIJocqw8E8lBew.gif"/></div></figure><p id="bd6c" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">通过使用非线性基函数，<em class="nv"> h </em>有可能适应<strong class="lo iu"> x </strong>的非线性关系，我们将很快看到——<strong class="lo iu">我们称这些模型为线性基函数模型</strong>。</p><p id="657f" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">我们已经在系列文章的<a class="ae mi" rel="noopener" target="_blank" href="/what-is-machine-learning-91040db474f9">第一篇文章中看到了基函数的一个例子，其中我们用<em class="nv"> x </em>的幂的基函数扩充了简单的线性回归模型，即，</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/9b5177db736fcaddeff07597389065b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:180/1*N3IKUpq_GJH1SaifOymNWw.gif"/></div></figure><p id="56a7" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">另一种常见的基函数是高斯函数</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/2477234d09b357f9f6752ab1e79faf03.png" data-original-src="https://miro.medium.com/v2/resize:fit:488/1*2WX5LyvvWf__WEtWuKa8Og.gif"/></div></figure><p id="57b3" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">遵循与<a class="ae mi" rel="noopener" target="_blank" href="/how-linear-regression-actually-works-theory-and-implementation-8d8dcae3222c">上一篇文章</a>中相同的推导，我们发现<strong class="lo iu"> w </strong>和<em class="nv"> α </em>的最大似然解为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/77cff4127bbc29df1099e3e84d9514b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1086/1*mnSTZuKOzu1GN0LqA4yHrA.gif"/></div></figure><p id="b85b" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">在哪里</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/a0415443e3b5d4426767e4322c33b91c.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/1*C2fm7h_e68HU-sy6avHHjg.gif"/></div></figure><p id="278f" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">下图显示了具有<em class="nv"> M </em> -1个高斯基函数的线性基函数模型。我们可以看到<strong class="lo iu">增加基函数的数量会产生更好的模型，直到我们开始过度拟合</strong>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/708181062b6de36adeedf0d95e39b144.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*BUtL9fSKuW3dv9tU8mydJQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">使用m1高斯基函数(加上截距)的效果图。图片作者。</p></figure><h2 id="5b0d" class="mj kv it bd kw mk ml dn la mm mn dp le lv mo mp lg lz mq mr li md ms mt lk mu bi translated">履行</h2><p id="fcd9" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">使用与前几篇文章相同的数据集，我们得到了下面的实现。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><h1 id="9cdd" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">正规化</h1><p id="eeca" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">在关于贝叶斯推理的<a class="ae mi" rel="noopener" target="_blank" href="/what-is-bayesian-inference-4eda9f9e20a6">文章中，我们简要地讨论了正则化的概念，我们将其描述为一种防止过度拟合的技术。如果我们回头看看我们在</a><a class="ae mi" rel="noopener" target="_blank" href="/how-linear-regression-actually-works-theory-and-implementation-8d8dcae3222c">上一篇文章</a>中定义的目标函数(用基函数扩充)，我们可以引入一个正则项</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/fa787081baa312cb9d9a21ddc05a2ef9.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/1*TkzG8FeExV-Bp2jijWE2_Q.gif"/></div></figure><p id="5c25" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">其中<em class="nv"> q </em> &gt; 0表示正则化的类型，<em class="nv"> λ </em>控制正则化的程度。</p><p id="153b" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated"><em class="nv"> q </em>最常见的值是1和2，分别称为L1正则化和L2正则化。当我们使用L1正则化时，我们称之为<strong class="lo iu">套索回归，当我们使用L2正则化时，我们称之为岭回归</strong>。</p><p id="e7d7" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">岭回归的目标函数</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="nq nr di ns bf nt"><div class="gh gi od"><img src="../Images/cc7da1a49b6acc057adba0e2b2e6f693.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/1*Mosnm99_fMBcUT3TzvDGrA.gif"/></div></div></figure><p id="f579" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">特别方便，因为它是w的二次函数，因此具有唯一的全局最小值。其解决方案是</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/6c3dfd36d7b2bcaf35b93586b7d5ccbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/1*joyaeyfIBUArt7etziPhIQ.gif"/></div></figure><p id="dbc4" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">其中<em class="nv"> α </em>保持不变，因为正则项对其没有影响。</p><p id="8f7f" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">当我们引入正则化时，<strong class="lo iu">模型选择的过程从找到合适数量的基函数到找到正则化参数<em class="nv">λ</em>T33】的合适值。</strong></p><p id="2e3f" class="pw-post-body-paragraph lm ln it lo b lp nk ju lr ls nl jx lu lv nm lx ly lz nn mb mc md no mf mg mh im bi translated">下图显示了一个线性基函数模型，其中<em class="nv"> λ </em>的值不同，而基函数的数量保持不变<em class="nv"> M </em> =8。我们可以看到<strong class="lo iu">即使我们开始时过度拟合，我们也可以调整正则化参数<em class="nv"> λ </em>来防止它</strong>——事实上，当正则化过度时，我们开始欠拟合。另一件有趣的事情是，我们的不确定性随着正则化而增加。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/d1a5c50b8410d741e2c9e89806fe28b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*CQGp1ado4L3QTRt6L6K1sg.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">对过度拟合的模型增加正则化的说明。图片作者。</p></figure><h2 id="4ba2" class="mj kv it bd kw mk ml dn la mm mn dp le lv mo mp lg lz mq mr li md ms mt lk mu bi translated">履行</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oa ob l"/></div></figure><h1 id="d844" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">摘要</h1><ul class=""><li id="1a2a" class="mv mw it lo b lp lq ls lt lv mx lz my md mz mh na nb nc nd bi translated">使模型<strong class="lo iu">线性</strong>的是它在参数而不是输入<strong class="lo iu">中是线性的。</strong></li><li id="4a4f" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated">我们可以用<strong class="lo iu">基函数</strong>增加线性回归，产生<strong class="lo iu">线性基函数模型</strong>。</li><li id="760e" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated"><strong class="lo iu">多项式回归</strong>是线性基函数模型。</li><li id="3b66" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated"><strong class="lo iu">规则化</strong>是一种<strong class="lo iu">防止过拟合</strong>的技术。</li><li id="3de1" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated">在线性回归中有不同种类的正则化，如L1和L2正则化。</li></ul></div></div>    
</body>
</html>