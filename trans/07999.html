<html>
<head>
<title>Will The GPU Star in A New Golden Age of Computer Architecture?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">GPU会开创计算机架构的新黄金时代吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/will-the-gpu-star-in-a-new-golden-age-of-computer-architecture-3fa3e044e313?source=collection_archive---------22-----------------------#2021-07-22">https://towardsdatascience.com/will-the-gpu-star-in-a-new-golden-age-of-computer-architecture-3fa3e044e313?source=collection_archive---------22-----------------------#2021-07-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/bcb567ce1321a64dbfeaeda3fc3ee3a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jcxkRBHygFwGz10t7BaSlw.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated"><a class="ae jd" href="https://unsplash.com/@m_hampi?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">米凯拉</a>在<a class="ae jd" href="https://unsplash.com/collections/90388524/landscape-/f8af6073bc11a14f44ab5228968c2f04?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><div class=""/><div class=""><h2 id="6b62" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">促进人工智能软件进一步发展的架构选择</h2></div><p id="3db9" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">2018年6月4日，作为2017年图灵奖的获奖者，约翰·汉尼斯和大卫·帕特森在他们的图灵讲座<a class="ae jd" href="https://cacm.acm.org/magazines/2019/2/234352-a-new-golden-age-for-computer-architecture/fulltext" rel="noopener ugc nofollow" target="_blank">上讲述了计算机架构的新黄金时代</a>。讲座的三个关键观点是:</p><ol class=""><li id="5a58" class="lr ls jg kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated">软件进步可以激发架构创新。</li><li id="7431" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">提升硬件/软件接口为架构创新创造了机会。</li><li id="34e5" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">市场最终解决了架构争论。</li></ol><p id="8b1b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我想用第四个观点来修正这三个关键观点，从而完成这个循环:</p><blockquote class="mf mg mh"><p id="59bb" class="kv kw mi kx b ky kz kh la lb lc kk ld mj lf lg lh mk lj lk ll ml ln lo lp lq ij bi translated">获胜的架构促进了随后的软件进步。</p></blockquote><p id="906c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">自轩尼诗/帕特森讲座以来，市场可以说已经实现了人工智能的洞察#3，并决定将图形处理单元(GPU)作为促进人工智能革命的获胜架构。在这篇文章中，我探索了人工智能革命如何激发架构创新和重新发明GPU。我希望回答我自己的一个重要问题:</p><blockquote class="mf mg mh"><p id="1ebf" class="kv kw mi kx b ky kz kh la lb lc kk ld mj lf lg lh mk lj lk ll ml ln lo lp lq ij bi translated">GPU会开创计算机架构的新黄金时代吗？</p></blockquote><h1 id="5fce" class="mm mn jg bd mo mp mq mr ms mt mu mv mw km mx kn my kp mz kq na ks nb kt nc nd bi translated">特定领域架构</h1><p id="0479" class="pw-post-body-paragraph kv kw jg kx b ky ne kh la lb nf kk ld le ng lg lh li nh lk ll lm ni lo lp lq ij bi translated">Henessy和Patterson提出了特定领域体系结构(DSA ),以创新计算机体系结构，并努力迈向新的黄金时代。顾名思义，GPU是3D图形的DSA。它旨在渲染3D虚拟世界的照片级逼真图像；然而，几乎所有的人工智能研究人员都使用GPU来探索3D图形以外的想法，在人工智能“软件”，也就是神经网络架构方面取得突破。虽然在3D中仍然不可或缺，但GPU已经成为人工智能的“CPU”，因为它促进了人工智能的软件创新。除了3D用途之外，GPU架构师一直在为非3D用途提供GPU的计算资源。我们称这种设计理念为通用GPU (GPGPU)。</p><p id="5cbc" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如今，我们看到AI DSAs取代GPGPU的激增，试图用更好的性能取代GPU。就连GPU本身也在双重人格之间挣扎:AI DSA和3D DSA。原因是AI DSA需要加速张量运算，这在AI中很丰富，在3D中却没有。同时，3D固定功能的硬件听起来对AI来说是不必要的。</p><p id="3246" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，主要架构的争论似乎在问</p><ol class=""><li id="9008" class="lr ls jg kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated">GPU会保住AI“CPU”的宝座吗？</li><li id="a7f7" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">GPU会不会分叉成两个DSA，一个用于AI，一个用于3D？</li></ol><p id="3d96" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我的预测如下:</p><ol class=""><li id="4981" class="lr ls jg kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated">GPU硬件/软件接口将使GPU成为人工智能的“CPU”。</li><li id="65a3" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">基于人工智能的渲染将使张量加速成为GPU的中流砥柱。</li><li id="f479" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">虚拟世界和现实世界相互映射的数字孪生将主宰市场，最终解决架构之争。</li></ol><h1 id="ed4c" class="mm mn jg bd mo mp mq mr ms mt mu mv mw km mx kn my kp mz kq na ks nb kt nc nd bi translated">GPU硬件/软件接口</h1><p id="4e26" class="pw-post-body-paragraph kv kw jg kx b ky ne kh la lb nf kk ld le ng lg lh li nh lk ll lm ni lo lp lq ij bi translated">我们可以将GPU在3D领域的主导地位和在AI领域的巨大成功归因于其硬件/软件接口，这是GPU和3D图形软件架构师努力拥抱的。这个接口是解决以下矛盾的关键。虽然GPU社区继续使GPU更加通用，但行业的其他人已经转向更专业的硬件来解决摩尔定律的消亡。</p><figure class="nk nl nm nn gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nj"><img src="../Images/cba7190cf86233627b4f0ece4b2fc38b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zl8UgZ5jYa5lCD-KT2l54w.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">GPU管道(图片由作者提供)</p></figure><h2 id="5645" class="no mn jg bd mo np nq dn ms nr ns dp mw le nt nu my li nv nw na lm nx ny nc nz bi translated">双层可编程性</h2><p id="639a" class="pw-post-body-paragraph kv kw jg kx b ky ne kh la lb nf kk ld le ng lg lh li nh lk ll lm ni lo lp lq ij bi translated">从概念上讲，GPU是处理阶段的长线性流水线。不同类型的工作项在管道中流动时被处理。在早期，每个处理阶段都是一个固定功能块。程序员对GPU的唯一控制是调整每个块的参数。如今，GPU硬件/软件接口让程序员可以随心所欲地处理每个工作项目，无论是顶点还是像素。不需要在每个顶点或像素循环中处理循环头，因为GPU架构师在固定的函数中实现它。这种架构选择让程序员有责任关注循环体或“着色器”，它通常以工作项的类型命名，例如用于处理顶点的“顶点着色器”和用于处理像素的“像素着色器”。</p><p id="4ca3" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现代游戏如何用这样的线性流水线制作出惊艳的画面？除了在一次通过管道时控制不同类型的着色器之外，程序员还可以多次渐进地通过管道来产生中间图像，最终产生在屏幕上看到的最终图像。程序员有效地创建计算图，描述中间图像之间的关系。图中的每个节点代表一次通过GPU管道。</p><h2 id="e801" class="no mn jg bd mo np nq dn ms nr ns dp mw le nt nu my li nv nw na lm nx ny nc nz bi translated">通用计算资源的集中池</h2><p id="2cb5" class="pw-post-body-paragraph kv kw jg kx b ky ne kh la lb nf kk ld le ng lg lh li nh lk ll lm ni lo lp lq ij bi translated">通用计算资源的集中池在处理阶段之间共享，并承担繁重的工作。这种方案的最初动机是负载平衡；在不同的使用场景中，处理阶段的工作负载可能会有很大的不同。被称为着色器核心的计算资源已经变得更加通用，以实现灵活性和产品差异化。</p><p id="4c92" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">GPU架构师抓住机会将集中式着色器池作为GPGPU提供给非3D应用程序。这种设计方案使GPU能够在运行人工智能任务方面实现突破，即使是作为兼职。</p><h2 id="2a6f" class="no mn jg bd mo np nq dn ms nr ns dp mw le nt nu my li nv nw na lm nx ny nc nz bi translated">平衡专业化</h2><p id="caea" class="pw-post-body-paragraph kv kw jg kx b ky ne kh la lb nf kk ld le ng lg lh li nh lk ll lm ni lo lp lq ij bi translated">GPU架构师通过在不改变硬件/软件接口的情况下添加协处理单元来定期“加速”或“指定域”着色器池。纹理单元就是这样一个协处理单元，纹理贴图中的纹理元素通过它被提取并过滤到着色器池。特殊功能单元(SFU)是另一个协同处理单元，执行超越数学功能，如对数、平方根倒数等。虽然拥有多个功能听起来类似于CPU中的超标量设计，但有一个显著的区别:GPU架构师根据“平均”着色器程序使用它的频率来分配协处理单元的吞吐量。例如，我们可以给纹理单元着色器池吞吐量的八分之一，假设纹理操作平均八分之一的时间出现在基准测试或游戏中。当一个协处理单元繁忙时，GPU会切换任务以保持自己忙碌。</p><h1 id="adac" class="mm mn jg bd mo mp mq mr ms mt mu mv mw km mx kn my kp mz kq na ks nb kt nc nd bi translated">三维张量加速</h1><p id="420e" class="pw-post-body-paragraph kv kw jg kx b ky ne kh la lb nf kk ld le ng lg lh li nh lk ll lm ni lo lp lq ij bi translated">在我的介绍中，我指出GPU难以在3D中采用张量加速。让我们看看，如果我们改变GPU渲染典型游戏帧的方式，这种趋势会如何逆转。GPU首先为每个像素生成并在<strong class="kx jh"> G-buffer </strong>中存储着色像素所需的所有信息。从G缓冲区，我们计算如何照亮一个像素，然后是几个处理步骤，包括</p><ol class=""><li id="0357" class="lr ls jg kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated">移除锯齿状边缘(抗锯齿(AA))</li><li id="994d" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">将低分辨率图像放大到更高的分辨率(超分辨率(SR))</li><li id="da10" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">为整个帧添加特定的视觉效果，如环境遮挡、运动模糊、高光滤镜或景深。</li></ol><p id="f145" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们称这种渲染方案为<a class="ae jd" href="https://en.wikipedia.org/wiki/Deferred_shading" rel="noopener ugc nofollow" target="_blank">延迟着色</a>，因为对一个像素的着色是“延迟”的，直到每个像素都得到它需要的信息。我们将光照后的处理步骤称为后处理。今天，后期处理消耗了大约90%的渲染时间，这意味着GPU的屏幕时间主要花在2D而不是3D上！</p><p id="508c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">英伟达已经展示了基于人工智能的DLSS 2.0，据称它可以产生比没有DLSS 2.0时更好看的图像。此外，NVIDIA还为光线跟踪提供了基于人工智能的蒙特卡罗去噪，通过它，我们可以使用少量光线来实现只有使用更多光线才能实现的质量。此外，人工智能激发了许多其他类型的后处理的新解决方案，例如用于环境遮挡的<a class="ae jd" href="http://daniel-holden.com/media/uploads/other_stuff/nnao.pdf" rel="noopener ugc nofollow" target="_blank"> NNAO </a>和用于景深的<a class="ae jd" href="http://export.arxiv.org/pdf/1810.08100" rel="noopener ugc nofollow" target="_blank"> DeepLens </a>。</p><blockquote class="mf mg mh"><p id="bbb5" class="kv kw mi kx b ky kz kh la lb lc kk ld mj lf lg lh mk lj lk ll ml ln lo lp lq ij bi translated">如果基于人工智能的后处理成为主流，张量加速将成为GPU个性的3D方面的支柱。GPU分化成3D DSA和AI DSA的可能性会变小。</p></blockquote><h1 id="bc15" class="mm mn jg bd mo mp mq mr ms mt mu mv mw km mx kn my kp mz kq na ks nb kt nc nd bi translated">3D/AI融合</h1><p id="d9b9" class="pw-post-body-paragraph kv kw jg kx b ky ne kh la lb nf kk ld le ng lg lh li nh lk ll lm ni lo lp lq ij bi translated">为了解决架构的争论，我们想要解决最后一个难题:我们是否应该最终移除3D渲染中的固定功能硬件，尤其是对AI而言？请注意，通过GPGPU，GPU可以作为纯“软件”进行3D渲染，而无需使用任何固定功能的硬件。</p><p id="001b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在严格意义上，给定场景参数，3D渲染模拟光子如何从光源传输通过空间，以与3D虚拟世界中的对象进行交互。由GPU进行的传统3D渲染是这一过程的非常粗略的近似。因此，微软在<a class="ae jd" href="https://devblogs.microsoft.com/directx/announcing-microsoft-directx-raytracing/" rel="noopener ugc nofollow" target="_blank">公告</a>中称“[传统的基于光栅化的] 3D图形是一个谎言”，以促进光线跟踪成为“未来的完整3D效果”然而，一个3D渲染纯粹主义者可能仍然会拒绝光线追踪，在光线追踪中，我们通过追踪光线从像素向后进入3D虚拟世界来实现3D渲染，这也是不真实的。</p><p id="0af2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这两种方法都近似于基于模拟的3D渲染。无论哪种情况，我们都将3D虚拟世界的建模或内容创建从渲染中分离出来。在第一种情况下，建模3D虚拟世界需要工程师和艺术家进行大量艰苦和创造性的工作来描述每个对象及其与光交互的物理属性。在第二种情况下，关于渲染，完全真实是不可能的，因为我们需要大幅简化3D渲染，以满足资源预算内的不同性能目标。</p><p id="9491" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">与用最知名的科学知识和数学理论为给定问题找到解决方案相反，人工智能方法是关于从数据中“学习”计算模型或神经网络。我们通过试错法反复调整网络参数。我们通过先前的参数估计向前运行网络，并测量不匹配或“损失”然后，我们根据其梯度调整参数以减少损失，有效地在梯度的相反方向上导航损失景观。这种机制被称为反向传播，它要求沿正向路径的所有计算都是可微分的，以便参与计算梯度。</p><p id="b6d9" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae jd" href="https://www.neuralrender.com/assets/downloads/TewariFriedThiesSitzmannEtAl_EG2020STAR.pdf" rel="noopener ugc nofollow" target="_blank">神经渲染</a>是一个新兴的人工智能研究领域，使用上述方法研究3D渲染。下面是我跟踪神经渲染进展的思维导图:</p><figure class="nk nl nm nn gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oa"><img src="../Images/b1796925c2d58b2fed26d2595c77af7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9coTSkDXDnx4mZE7"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">作者图片</p></figure><p id="bf38" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这个3D虚拟世界的模型被隐含地表示为神经网络参数(参见<a class="ae jd" href="https://www.matthewtancik.com/nerf" rel="noopener ugc nofollow" target="_blank"> NeRF </a>、<a class="ae jd" href="https://autonomousvision.github.io/graf/" rel="noopener ugc nofollow" target="_blank">格拉夫</a>、<a class="ae jd" href="https://m-niemeyer.github.io/project-pages/giraffe/index.html" rel="noopener ugc nofollow" target="_blank">长颈鹿</a>)，这些参数是我们通过比较真实世界的图像和我们从虚拟世界渲染的图像而推断出来的。然后我们反向传播比较的梯度来调整神经网络参数。可选地，我们可以从数据中学习显式3D网格(参见<a class="ae jd" href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Liao_Deep_Marching_Cubes_CVPR_2018_paper.pdf" rel="noopener ugc nofollow" target="_blank">深度行进立方体</a>、<a class="ae jd" href="https://xingangpan.github.io/projects/GAN2Shape.html" rel="noopener ugc nofollow" target="_blank"> GAN2Shape </a>)。实际上，建模3D虚拟世界与学习神经网络参数是一回事。这一过程要求我们在前进路径中包括3D渲染管道，并在紧密循环中集成3D虚拟世界的建模和渲染。通过反复渲染和测试真实世界的图像，我们获得了所需的模型和场景参数，我们可以用它们来渲染虚拟世界的新视图。</p><p id="645e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这个框架内，我们可以选择不调整每个参数的整体，例如，保持一个物体的形状不变，但估计它的位置(见<a class="ae jd" href="http://yenchenlin.me/inerf/" rel="noopener ugc nofollow" target="_blank"> iNeRF </a>)。通过这种方式，我们可以有效地识别和定位有问题的对象，而不是对其进行建模。建模和识别任务之间不再有区别。相反，这是一个我们想要“学习”或“估计”哪些场景参数的问题</p><h1 id="44ee" class="mm mn jg bd mo mp mq mr ms mt mu mv mw km mx kn my kp mz kq na ks nb kt nc nd bi translated">结论</h1><p id="def2" class="pw-post-body-paragraph kv kw jg kx b ky ne kh la lb nf kk ld le ng lg lh li nh lk ll lm ni lo lp lq ij bi translated">因此，在人工智能问题解决范式下，3D渲染不仅是关于产生3D虚拟世界的照片级逼真图像，而且是为了从真实世界构建虚拟世界。此外，新框架在以下方面重新定义了3D和AI:</p><ol class=""><li id="0276" class="lr ls jg kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated">3D渲染成为人工智能训练循环中的一个基本操作</li><li id="1d52" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">训练，或“梯度下降”，过去只发生在云中训练神经网络，现在是推理的一部分。</li><li id="485c" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">照片写实主义和保持真实世界和虚拟世界的一致性一样重要。</li></ol><p id="4a7f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">数字双胞胎将要求把巨大的和不断变化的真实世界带给它的未开发的双胞胎，并不断保持双胞胎之间的一致性。通过神经渲染获得的虚拟对象需要与经典构建的对象共存。因此，我相信神经渲染和传统渲染将在GPU上融合，利用其成熟和高性能的3D管道。数字双胞胎的需求将落在未来GPU的肩上。为了参与AI训练循环的梯度计算，需要在GPU端完成工作以变得“可微分”。</p><p id="6d3f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">假设GPU变得天生可微分和张量加速，以响应3D中人工智能的进步，我预见GPU的双重人格将成为一个。</p><blockquote class="mf mg mh"><p id="9435" class="kv kw mi kx b ky kz kh la lb lc kk ld mj lf lg lh mk lj lk ll ml ln lo lp lq ij bi translated">然后，GPU保持了其作为首选架构的地位，以促进人工智能中的进一步软件进步，并最终开启了计算机架构的新黄金时代。</p></blockquote></div></div>    
</body>
</html>