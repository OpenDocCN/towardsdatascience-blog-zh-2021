<html>
<head>
<title>Shared External Hive Metastore with Azure Databricks and Synapse Spark Pools</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">具有 Azure 数据块和 Synapse 火花池的共享外部配置单元 Metastore</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/shared-external-hive-metastore-with-azure-databricks-and-synapse-spark-pools-8d611c677688?source=collection_archive---------2-----------------------#2021-11-09">https://towardsdatascience.com/shared-external-hive-metastore-with-azure-databricks-and-synapse-spark-pools-8d611c677688?source=collection_archive---------2-----------------------#2021-11-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4be7" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><em class="kf">了解如何设置共享的外部配置单元 metastore，以便在多个数据块工作区和 Synapse Spark 池中使用(预览)</em></h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/2e2388d60c6c6c3583cf52a274311154.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*W9B0cX9DBKoxFL4I.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">图片由<a class="ae kw" href="https://pixabay.com/users/tumisu-148124/" rel="noopener ugc nofollow" target="_blank"> Tumis </a> u 在<a class="ae kw" href="https://pixabay.com/" rel="noopener ugc nofollow" target="_blank"> Pixelbay </a>上</p></figure><h1 id="68c1" class="kx ky iq bd kz la lb lc ld le lf lg lh jw li jx lj jz lk ka ll kc lm kd ln lo bi translated">1 背景</h1><p id="0ba0" class="pw-post-body-paragraph lp lq iq lr b ls lt jr lu lv lw ju lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">为了帮助构建数据湖中的数据，您可以在 Hive metastore 中以表格的形式注册和共享数据。Hive metastore 是一个数据库，它保存关于我们数据的元数据，例如数据湖中数据的路径和数据的格式(parquet、delta、CSV 等)。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ml"><img src="../Images/2ffaa5e32e20b43ed552dbcb73762711.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GyrMlJLFXgUmQzrtuLgRCg.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">图片作者。</p></figure><p id="9e49" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">Azure Databricks 中的每个工作区都带有一个托管的内置 metastore。过一会儿，您会有新的团队或项目创建多个 Databricks 工作区，并开始注册它们的表。其他 spark 用户可能会使用 Synapse Spark Pools。然后，您意识到您想要共享来自不同工作区的表，但是每个元存储都是独立的，并且只能从每个工作区内访问。</p><p id="7d14" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">您可能还想使用 metastore 在业务中共享表，将<a class="ae kw" href="https://docs.microsoft.com/en-us/azure/databricks/sql/admin/data-access-configuration#supported-properties" rel="noopener ugc nofollow" target="_blank"> Databricks SQL </a>与 metastore 结合起来为 BI 用户服务。</p><p id="0f15" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">一种解决方案是创建一个共享的 metastore，以便不同的工作空间可以将其数据注册到一个共享的 metastore 中。</p><h1 id="4d57" class="kx ky iq bd kz la lb lc ld le lf lg lh jw li jx lj jz lk ka ll kc lm kd ln lo bi translated">2 个共享外部 metastore</h1><p id="da74" class="pw-post-body-paragraph lp lq iq lr b ls lt jr lu lv lw ju lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">我们希望创建一个共享的外部 metastore。一个好的开始通常是查看文档:<a class="ae kw" href="https://docs.microsoft.com/en-us/azure/databricks/data/metastores/external-hive-metastore" rel="noopener ugc nofollow" target="_blank">外部 Apache Hive metastore—Azure data bricks | Microsoft Docs</a></p><p id="977b" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">文档中有很多信息。你必须选择:</p><ul class=""><li id="ea50" class="mr ms iq lr b ls mm lv mn ly mt mc mu mg mv mk mw mx my mz bi translated">Hive metastore 版本</li><li id="6948" class="mr ms iq lr b ls na lv nb ly nc mc nd mg ne mk mw mx my mz bi translated">metastore 的数据库</li></ul><p id="4f9e" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">我们需要 Databricks 运行时、为外部 metastore 保存元数据的数据库以及使其工作的 Hive 版本的正确组合。</p><h2 id="84b1" class="nf ky iq bd kz ng nh dn ld ni nj dp lh ly nk nl lj mc nm nn ll mg no np ln nq bi translated">2.1 数据块 spark 配置设置和外部 metastore</h2><p id="1320" class="pw-post-body-paragraph lp lq iq lr b ls lt jr lu lv lw ju lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">在我们进入匹配配置单元版本和后端数据库的细节之前，我们先看看如何告诉 Databricks 集群使用哪个 metastore。我们在集群的 spark 配置中提供我们的设置:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/5fe5d2b4d6ce1c7650408d5617f9f98e.png" data-original-src="https://miro.medium.com/v2/resize:fit:928/format:webp/1*hLh4rqEZf5LOtrbzkmJYQQ.png"/></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">图片作者。</p></figure><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/ea9107e101c3c3bfd215f63abbe71a43.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*PYC8mfhp_ytEfeYdt_NtyA.png"/></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">图片作者。</p></figure><p id="77f2" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">我们需要提供:</p><ul class=""><li id="a3b4" class="mr ms iq lr b ls mm lv mn ly mt mc mu mg mv mk mw mx my mz bi translated">要使用的配置单元架构的版本</li></ul><pre class="kh ki kj kk gt nt nu nv nw aw nx bi"><span id="aaf3" class="nf ky iq nu b gy ny nz l oa ob">spark.sql.hive.metastore.version X.Y.Z</span></pre><ul class=""><li id="2547" class="mr ms iq lr b ls mm lv mn ly mt mc mu mg mv mk mw mx my mz bi translated">作为 metastore 后端的数据库的驱动程序和 URI。下面的例子使用 MySql 的“org.mariadb.jdbc.Driver”。</li></ul><pre class="kh ki kj kk gt nt nu nv nw aw nx bi"><span id="aa50" class="nf ky iq nu b gy ny nz l oa ob">javax.jdo.option.ConnectionDriverName org.mariadb.jdbc.Driver<br/>javax.jdo.option.ConnectionURL jdbc:mysql://mysqlserveruri.mysql.database.azure.com:3306/myextmetadb013?useSSL=true&amp;requireSSL=false</span></pre><ul class=""><li id="c5e0" class="mr ms iq lr b ls mm lv mn ly mt mc mu mg mv mk mw mx my mz bi translated">连接到数据库的凭据</li></ul><pre class="kh ki kj kk gt nt nu nv nw aw nx bi"><span id="89ea" class="nf ky iq nu b gy ny nz l oa ob">javax.jdo.option.ConnectionPassword {{secrets/databricks/databricksmetastorepass}}<br/>javax.jdo.option.ConnectionUserName {{secrets/databricks/databricksmetastoreuser}}</span></pre><p id="f0ef" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">请注意，我们使用占位符来表示存储在 Azure KeyVault 中的凭据。你可以在 spark 配置中直接引用 Azure KeyVault 中的秘密。语法是:</p><pre class="kh ki kj kk gt nt nu nv nw aw nx bi"><span id="5702" class="nf ky iq nu b gy ny nz l oa ob">{{secrets/scope/secretname}}</span></pre><p id="e654" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">其中 scope 是您注册的作用域名称，secretname 是 Azure KeyVault 中的机密名称。如果你忘记了或者还没有向密钥库注册一个作用域，你可以在这里查看:<a class="ae kw" href="https://docs.microsoft.com/en-us/azure/databricks/security/secrets/secret-scopes" rel="noopener ugc nofollow" target="_blank">https://docs . Microsoft . com/en-us/azure/databricks/security/secrets/secret-scopes</a>了解如何注册。</p><p id="7dbc" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">如果您忘记了作用域名称和秘密，您总是可以使用 dbutils 来查看:</p><pre class="kh ki kj kk gt nt nu nv nw aw nx bi"><span id="fa40" class="nf ky iq nu b gy ny nz l oa ob">dbutils.secrets.listScopes()<br/>dbutils.secrets.list(scope="myscopename")</span></pre><p id="d30f" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">确保为低于 2 的配置单元版本创建配置单元架构(第一次):</p><pre class="kh ki kj kk gt nt nu nv nw aw nx bi"><span id="ca2c" class="nf ky iq nu b gy ny nz l oa ob">datanucleus.autoCreateSchema true<br/>datanucleus.schema.autoCreateTables true</span></pre><p id="219c" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">注意:模式的自动创建不适用于配置单元版本 2 和更高版本</p><ul class=""><li id="ad92" class="mr ms iq lr b ls mm lv mn ly mt mc mu mg mv mk mw mx my mz bi translated">我们需要的任何罐子(0.13 不需要)</li><li id="9898" class="mr ms iq lr b ls na lv nb ly nc mc nd mg ne mk mw mx my mz bi translated">其他一些可选设置不验证模式(第一次)，因为我们创建了模式，但无法验证它。</li></ul><pre class="kh ki kj kk gt nt nu nv nw aw nx bi"><span id="f4aa" class="nf ky iq nu b gy ny nz l oa ob">datanucleus.fixedDatastore false<br/>hive.metastore.schema.verification.record.version false hive.metastore.schema.verification false</span></pre><p id="c639" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">当您为 metastore 创建了模式后，您可以设置回该属性:</p><pre class="kh ki kj kk gt nt nu nv nw aw nx bi"><span id="0715" class="nf ky iq nu b gy ny nz l oa ob">datanucleus.fixedDatastore true</span></pre><p id="58fd" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">这可以防止对 metastore 数据库进行任何意外的结构更改。</p><h2 id="1ea7" class="nf ky iq bd kz ng nh dn ld ni nj dp lh ly nk nl lj mc nm nn ll mg no np ln nq bi translated">2.2 选择 Hive 版本和学习</h2><p id="9089" class="pw-post-body-paragraph lp lq iq lr b ls lt jr lu lv lw ju lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">根据您选择的配置单元版本，配置单元的行为会有所不同。一些区别是:</p><ul class=""><li id="932f" class="mr ms iq lr b ls mm lv mn ly mt mc mu mg mv mk mw mx my mz bi translated">并非所有数据库都支持作为后端</li><li id="f3bf" class="mr ms iq lr b ls na lv nb ly nc mc nd mg ne mk mw mx my mz bi translated">配置单元可以/不能自己创建架构。</li><li id="1566" class="mr ms iq lr b ls na lv nb ly nc mc nd mg ne mk mw mx my mz bi translated">一些错误修复和功能。在这里查看所有版本和变更日志:【apache.org T2】下载</li></ul><p id="3414" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated"><strong class="lr ir">用蜂巢学习 0.13 </strong></p><p id="f1bc" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">我第一次尝试用的是 Hive 0.13(工作区默认内置版本)。使用 0.13 有一些好处。0.13 版可以通过设置属性“data nucleus . autocreateschema true”来为 metastore 生成模式。此外，所需的 jar 由 databricks 提供。</p><p id="32a3" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">不要将 Hive 0.13 用于 Azure SQL DB。在我的测试中，由于在删除 metastore 中注册的表时 Datanucleus 中的一个<a class="ae kw" href="https://github.com/datanucleus/datanucleus-rdbms/issues/110" rel="noopener ugc nofollow" target="_blank">错误</a>,这种组合不起作用。虽然文档说它应该可以工作，但是它没有。</p><p id="f23e" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">Azure MySql DB 5.7 与 Hive 0.13 配合良好，但请确保您设置了服务器参数<em class="oc">将 lower_case_table_names </em>更改为 2。在为 metastore 创建数据库和表之前，请执行此操作。</p><p id="9c62" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">这可以在 Azure 门户中完成:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi od"><img src="../Images/58a1526b877ea937a36bb03dc6edef46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CBsEAdd9bpK6Ey_qLSxk2w.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">图片作者。</p></figure><p id="445f" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">然后在 MySql 服务器中创建一个数据库。在用 JDBC 连接字符串将数据块连接到数据库之前，我们需要创建数据库。</p><p id="01e6" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">使用您喜欢的工具登录 MySQL 服务器，并使用您选择的名称为 metastore 创建一个数据库。示例:</p><pre class="kh ki kj kk gt nt nu nv nw aw nx bi"><span id="e27c" class="nf ky iq nu b gy ny nz l oa ob">CREATE DATABASE extmetadb013;</span></pre><p id="eaa7" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">将以下内容添加到要使用的 Databricks 集群的 Spark 配置中，替换:</p><pre class="kh ki kj kk gt nt nu nv nw aw nx bi"><span id="07c3" class="nf ky iq nu b gy ny nz l oa ob">xxxscope, xxxsecretname, xxxserverurl, xxxdatabasename, xxxuser</span></pre><p id="8a27" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">使用您的数据库 URL 和凭据:</p><pre class="kh ki kj kk gt nt nu nv nw aw nx bi"><span id="c657" class="nf ky iq nu b gy ny nz l oa ob">spark.sql.hive.metastore.version 0.13</span><span id="ea71" class="nf ky iq nu b gy oe nz l oa ob">javax.jdo.option.ConnectionDriverName org.mariadb.jdbc.Driver<br/>javax.jdo.option.ConnectionURL jdbc:mysql://xxxserverurl.mysql.database.azure.com:3306/xxxdatabasename?useSSL=true&amp;requireSSL=false<br/>javax.jdo.option.ConnectionUserName xxxuser<br/>javax.jdo.option.ConnectionPassword {{secrets/xxxscope/xxxsecretname}}<br/>datanucleus.fixedDatastore false<br/>hive.metastore.schema.verification false<br/>datanucleus.schema.autoCreateTables truehive.metastore.schema.verification.record.version false<br/>datanucleus.autoCreateSchema true</span></pre><p id="a894" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">在数据块中重新启动集群，以在 metastore 存储数据库中创建表。</p><p id="f810" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">在此之后，您应该能够看到新的数据库和创建的模式。</p><h2 id="4660" class="nf ky iq bd kz ng nh dn ld ni nj dp lh ly nk nl lj mc nm nn ll mg no np ln nq bi translated"><strong class="ak">学习 Hive 2.3.7 </strong></h2><p id="460d" class="pw-post-body-paragraph lp lq iq lr b ls lt jr lu lv lw ju lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">Hive 2.3.7 使用 Azure SQL DB 作为后端。</p><p id="b791" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated"><strong class="lr ir">突触</strong></p><p id="6e83" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">如果您想要在数据块和 Synapse Spark 池之间共享相同的外部 metastore，您可以使用数据块和 Synapse Spark 都支持的 Hive 2 . 3 . 7 版。在 manage 选项卡下链接 metastore 数据库，然后设置一个 spark 属性:</p><pre class="kh ki kj kk gt nt nu nv nw aw nx bi"><span id="8369" class="nf ky iq nu b gy ny nz l oa ob">spark.hadoop.hive.synapse.externalmetastore.linkedservice.name HIVEMetaStoreLinkedName</span></pre><p id="5eaa" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">请注意，这仍处于预览阶段(在我的测试中有点不稳定)，因此还不适合生产工作负载。请参阅:<a class="ae kw" href="https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-external-metastore" rel="noopener ugc nofollow" target="_blank">将外部配置单元 Metastore 用于 Synapse Spark Pool(预览)</a>以获得逐步指南。</p><p id="7871" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated"><strong class="lr ir">数据砖块</strong></p><p id="d57d" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">Hive 版本 2.3.7 要求您在 Databricks 的 spark.config 中设置一个属性，告诉 spark 使用什么 jar:</p><pre class="kh ki kj kk gt nt nu nv nw aw nx bi"><span id="abb6" class="nf ky iq nu b gy ny nz l oa ob">spark.sql.hive.metastore.jars builtin</span></pre><p id="a0ee" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">jar 是提供的/内置的，所以您不必自己下载 jar。注意:对于 Hive 1.x 和 3.x，您必须提供一个包含 jar 的文件夹，或者您可以在集群启动时使用 Maven 下载 jar，但是 jar 的下载需要相当长的时间，因此如果您以前下载过一次 jar，将会缩短启动时间。</p><p id="51be" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated"><strong class="lr ir">手动创建 metastore 表</strong></p><p id="7be9" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">Hive 版本 2.3.7(版本 2.x 和更高版本)不会为您创建 metastore 表，并且<a class="ae kw" href="https://docs.microsoft.com/en-us/azure/databricks/kb/metastore/hive-metastore-troubleshooting#solution" rel="noopener ugc nofollow" target="_blank">文档</a>没有清楚地告诉您如何创建表。我尝试了两种有效的方法:</p><ul class=""><li id="1ddd" class="mr ms iq lr b ls mm lv mn ly mt mc mu mg mv mk mw mx my mz bi translated">使用配置单元<a class="ae kw" href="https://cwiki.apache.org/confluence/display/Hive/Hive+Schema+Tool" rel="noopener ugc nofollow" target="_blank">模式工具</a></li><li id="b4d8" class="mr ms iq lr b ls na lv nb ly nc mc nd mg ne mk mw mx my mz bi translated">使用配置单元 SQL 脚本</li></ul><p id="d8cf" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated"><strong class="lr ir">使用 Hive schematool 创建 metastore 表</strong></p><p id="e05f" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">要使用 Hive schematool 二进制文件(/Apache-Hive-2 . 3 . 9-bin/bin/schema tool)，您需要下载 Hive 和，下载 Hadoop Core，并在 hive-site.xml 中设置连接属性(您可以使用 proto-hive-site.xml 作为模板)。然后运行 schematool，它将连接到您的数据库并创建表。即使它确实有效，我也不会在这里详细讨论。我建议使用下面描述的 SQL 脚本。</p><p id="7aa7" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated"><strong class="lr ir">使用 Hive SQL 脚本创建 metastore 表</strong></p><p id="c443" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">有一种更简单的方法来为特定的 Hive 版本创建表/模式。您可以下载并运行 SQL 脚本。</p><p id="ce32" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">从这里下载并解压最新的 2.3.x HIVE 版本:<a class="ae kw" href="https://hive.apache.org/downloads.html" rel="noopener ugc nofollow" target="_blank">https://hive.apache.org/downloads.html</a></p><p id="294c" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">您将看到一个文件夹，其中包含不同配置单元版本和不同数据库的脚本:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi of"><img src="../Images/e68e6c011b0edc0bc15165a6e1b14197.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*WKVAuYo7iKakbwF5kHCevA.png"/></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">图片作者。</p></figure><p id="9b6e" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">例如，如果我们选择 mssql 文件夹(如果我们使用该数据库，则选择您的数据库)，我们将找到不同模式版本的脚本:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi og"><img src="../Images/1d3fb96484236eb7217f9f1665a256a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*YGbCtP9PUU53WpsipFsn-w.png"/></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">图片作者。</p></figure><p id="f781" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">在我们的例子中，我们将运行 Hive 2.3.7，并希望创建模式版本 2.3.0。在您喜欢的 SQL 编辑器中运行或粘贴脚本:</p><pre class="kh ki kj kk gt nt nu nv nw aw nx bi"><span id="2276" class="nf ky iq nu b gy ny nz l oa ob">hive-schema-2.3.0.mssql.sql</span></pre><p id="52ac" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">来创建表格。</p><p id="0cd7" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">我们需要为集群设置 spark 配置，以使用 Hive 2.3.7 和我们的 Azure SqlDB:</p><p id="6ada" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">在 Spark 配置中，用您的值替换:XXXDNSXXX、XXXDATABASEXXX、XXXUSERXXX 和 XXXPASSORDXXX，最好使用如上所述的 Azure KeyVault，以避免明文机密。</p><pre class="kh ki kj kk gt nt nu nv nw aw nx bi"><span id="86d8" class="nf ky iq nu b gy ny nz l oa ob">spark.sql.hive.metastore.jars builtin<br/>javax.jdo.option.ConnectionURL jdbc:sqlserver://XXXDNSXXX.database.windows.net:1433;database=XXXDATABASEXXX;encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;<br/>javax.jdo.option.ConnectionUserName XXXUSERXXX<br/>javax.jdo.option.ConnectionPassword XXXPASSORDXXX<br/>javax.jdo.option.ConnectionDriverName com.microsoft.sqlserver.jdbc.SQLServerDriver<br/>hive.metastore.schema.verification.record.version true<br/>hive.metastore.schema.verification true<br/>spark.sql.hive.metastore.version 2.3.7</span></pre><p id="ffb6" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">如果您愿意，可以将 metastore 的 spark 配置设置移动到一个共享的 init 脚本中，这样它就可以被许多集群使用。</p><h1 id="b96e" class="kx ky iq bd kz la lb lc ld le lf lg lh jw li jx lj jz lk ka ll kc lm kd ln lo bi translated">3 通过共享 metastore 访问数据</h1><p id="d830" class="pw-post-body-paragraph lp lq iq lr b ls lt jr lu lv lw ju lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">现在我们有了一个共享的 metastore，可以用来从表名读取数据湖中的数据。</p><p id="be03" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">使用 metastore 查询数据块中的表:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi oh"><img src="../Images/26d0df2f76d182a9f721db619fbe3c76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cBT_UpPFEs-nZQzk8OQKdg.png"/></div></div></figure><p id="a808" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">使用相同的 metastore 查询 Synapse Spark 池中的相同表:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi oi"><img src="../Images/e4d7b53f2f13ecbc9e9394bdeea82c42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*75Awk5GeZdc5rg7LTKtxbw.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">图片作者。</p></figure><p id="1bfb" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">这要求不同的工作区和集群拥有对数据的读取权限。当使用共享 metastore 时，我们应该使用对数据湖的直接访问，即不使用装载来访问数据，因为装载可能因工作空间而异。如果我们查询关于在 metastore 中注册的表的信息，我们可以查看表后面的数据位置(直接访问 URL):</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi oj"><img src="../Images/01107df58bac7ffdc753acee3074638c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HJXIygauA_cu9BV_-6HrsA.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">图片作者。</p></figure><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ok"><img src="../Images/f7cc47dbdd20ce2d0b6e65a2f4f503f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6Rv7hINTqeRzeoqNsdHFbg.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">作者图片</p></figure><p id="6752" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">我们可以看到，Databricks 和 Synapse 都使用在 metastore 中注册的直接访问 URL。</p><p id="7d28" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">位置必须被所有读者理解。如果位置是直接 URL，它将跨用于读取数据的所有技术工作(只要读取器有访问权)。在数据湖中挂载文件夹是行不通的，因为这会要求读者在工作空间中拥有完全相同的挂载点。此外，其他工具，如 Synapse Spark Pools，可能希望使用 metastore，但可能无法挂载数据。</p><h1 id="803d" class="kx ky iq bd kz la lb lc ld le lf lg lh jw li jx lj jz lk ka ll kc lm kd ln lo bi translated">4 始终创建外部表</h1><p id="00f3" class="pw-post-body-paragraph lp lq iq lr b ls lt jr lu lv lw ju lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">在数据块中创建表格时，使用:</p><pre class="kh ki kj kk gt nt nu nv nw aw nx bi"><span id="469d" class="nf ky iq nu b gy ny nz l oa ob">df.write.saveAsTable("MyManagedTableName")</span></pre><p id="fbbc" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">默认位置是将数据存储在本地 DBFS 文件夹下:</p><pre class="kh ki kj kk gt nt nu nv nw aw nx bi"><span id="45e9" class="nf ky iq nu b gy ny nz l oa ob">/dbfs/user/hive/warehouse/MyManagedTableName </span></pre><p id="5774" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">在 Databricks 工作区。</p><p id="470c" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">DBFS 属于工作空间，而不是您的数据湖。此外，一般建议是不要向 DBFS 写入任何敏感数据。我们应该通过在创建表时给出数据的位置来确保只创建外部的表。我们应该使用直接的 URL 到数据表，而不是挂载路径。下面是一个简单的例子，给出了在数据湖中的现有数据(增量格式)之上创建表时数据的位置:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ol"><img src="../Images/d622221cdd17e32640a9da28033628d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gAFtexZBogVdLm9u215vbg.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">图片作者。</p></figure><p id="d10c" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">我们必须说出数据的格式(上例中的 delta ),并指向数据所在的文件夹，以便在现有数据的基础上创建我们的表。</p><p id="a5d6" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">如果您想了解更多关于托管表和非托管表的信息，这里还有另一篇文章:<a class="ae kw" rel="noopener" target="_blank" href="/3-ways-to-create-tables-with-apache-spark-32aed0f355ab"> 3 种使用 Apache Spark 创建表的方法|作者 AnBento |介绍了不同的选项。另外，官方文档在这里:</a><a class="ae kw" href="https://docs.microsoft.com/en-us/azure/databricks/data/tables" rel="noopener ugc nofollow" target="_blank">数据库和表格——Azure Databricks |微软文档</a>。</p><h1 id="d9e7" class="kx ky iq bd kz la lb lc ld le lf lg lh jw li jx lj jz lk ka ll kc lm kd ln lo bi translated">5 允许集群直接访问数据</h1><p id="ae16" class="pw-post-body-paragraph lp lq iq lr b ls lt jr lu lv lw ju lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">您可能已经注意到，我们还没有讨论如何直接访问数据湖中我们需要的数据。这可以在 spark 配置中使用 Azure KeyVault 来存储秘密。如何使用密钥或服务主体访问数据湖是另一个故事，但这里有文档的链接:<br/> <a class="ae kw" href="https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/adls-gen2/azure-datalake-gen2-sp-access" rel="noopener ugc nofollow" target="_blank">使用 OAuth 2.0 和 Azure 服务主体</a> <br/>访问 Azure 数据湖存储 Gen2，具体来说，您不想挂载，而是使用直接访问:<br/> <a class="ae kw" href="https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/adls-gen2/azure-datalake-gen2-sp-access#access-adls-gen2-directly" rel="noopener ugc nofollow" target="_blank">直接访问 ADLS gen 2</a></p><p id="7515" class="pw-post-body-paragraph lp lq iq lr b ls mm jr lu lv mn ju lx ly mo ma mb mc mp me mf mg mq mi mj mk ij bi translated">在 synapse workspace 中，您可以在“管理”选项卡下链接存储帐户，方法是使用密钥、服务主体、托管身份或用户分配的托管身份(预览)将存储帐户添加为链接的服务。</p><h1 id="e31c" class="kx ky iq bd kz la lb lc ld le lf lg lh jw li jx lj jz lk ka ll kc lm kd ln lo bi translated">6 下一步是什么</h1><p id="171d" class="pw-post-body-paragraph lp lq iq lr b ls lt jr lu lv lw ju lx ly lz ma mb mc md me mf mg mh mi mj mk ij bi translated">一如既往，在这篇文章中，我想分享一些关于设置共享外部元商店的学习，例如在 Azure Databricks 和 Synapse Spark Pools 中使用。我希望您在开发下一个外部 metastore 的过程中获得了一些新的见解。</p></div></div>    
</body>
</html>