<html>
<head>
<title>Binarized Neural Networks: An Overview</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">二值化神经网络综述</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/binarized-neural-networks-an-overview-d065dc3c94ca?source=collection_archive---------17-----------------------#2021-06-15">https://towardsdatascience.com/binarized-neural-networks-an-overview-d065dc3c94ca?source=collection_archive---------17-----------------------#2021-06-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c719" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">二值化神经网络是一种有趣的神经网络变体，可以节省内存、时间和能量。了解它们是如何工作的。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/fd2f31710cf7b86af5401e002d288715.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*nmdlOGKn-mdfiu5D"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">亚历山大·辛恩在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="e61d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用神经网络的一个障碍是运行网络所需的功率、内存和时间。这对于没有强大CPU或大内存的移动和物联网设备来说是个问题。二值化神经网络是解决这一问题的方法。通过使用二进制值而不是浮点值，可以更快地计算网络，并且使用更少的存储器和功率。</p><h1 id="88a1" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">力学:</strong></h1><p id="dfbd" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">从概念上讲，二进制神经网络(BNN)类似于常规前馈神经网络(NN)。一个区别是BNN中的权重和激活被限制为只有两个值:1和-1，因此得名“二进制化”。因此，只使用了一个激活函数:符号函数。我们不能使用像sigmoid或relu这样的常规激活函数，因为它们有连续的输出，正如我们刚才所说的，激活只能是1和-1。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/40adc740f72213a0ead0736a48413cee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hasURD0KcZQcWFPwWI9aEw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:<a class="ae ky" href="https://www.researchgate.net/publication/342258884_BoMaNet_Boolean_Masking_of_an_Entire_Neural_Network" rel="noopener ugc nofollow" target="_blank">整个神经网络的布尔掩蔽</a></p></figure><p id="3914" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们使用梯度下降来训练网络时，这些限制会导致一些问题。首先，符号函数的梯度为0。这是不好的，因为它使所有权重和激活的梯度也为0，这意味着实际上不会发生任何训练。我们可以通过在进行反向传播时忽略符号激活函数来解决这个问题。然而，我们有另一个问题，即梯度更新将导致权重不再是1或-1。我们通过保留一组实值权重并对这些权重进行梯度更新来解决这个问题。然后，网络权重是这些实值权重的二进制化。</p><p id="12ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们的最终算法是:</p><ul class=""><li id="f91a" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated">前向传递:我们有实值权重W_r和输入向量x，首先，我们对所有权重应用sign函数，得到W_b = sign(W_r)。然后我们像往常一样用W_b和符号激活函数计算神经网络的输出。</li><li id="fe84" class="mt mu it lb b lc nc lf nd li ne lm nf lq ng lu my mz na nb bi translated">反向传递:我们像往常一样进行反向传播，并计算权重W_b的梯度，只是我们忽略了符号激活函数。我们通过减去这些梯度来更新权重W_r(记住，W_b来自W_r，所以不直接更新W_b！).</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/fde3388ddaa6040c4cc767ea4b437940.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*FbeFjcYHbd2_F5-xFq4iHA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:[ <a class="ae ky" href="https://www.mdpi.com/2079-9292/8/6/661/pdf" rel="noopener ugc nofollow" target="_blank"> 2 </a> ]</p></figure><h1 id="849c" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">性能:</strong></h1><p id="3aa1" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们已经看到了二进制神经网络是如何工作的。现在让我们比较一下普通神经网络和BNN之间的理论性能差异。普通的神经网络使用32位浮点数。如果我们有两个32位寄存器，我们可以用一条计算机指令在两个32位数之间执行一次乘法。二进制网络使用1位数(我们将+1编码为1，将-1编码为0)。要将两个1位数字与我们的表示相乘，我们可以使用XNOR指令。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/1f7e07396722fcd171450f0ff768e99b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VHBkNNNW11YMJsLM8o50yQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:[ <a class="ae ky" href="https://www.mdpi.com/2079-9292/8/6/661/pdf" rel="noopener ugc nofollow" target="_blank"> 2 </a></p></figure><p id="4d1c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们能做的是将32个1位数字放入我们的两个32位寄存器中的每一个，然后运行一个XNOR指令来同时相乘所有的数字。其结果是，我们可以用一条指令做32次1位乘法，而用一条指令只能做1次32位乘法。因此，与普通神经网络相比，BNN理论上有32倍的加速比。</p><p id="d4d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">实际上，32x这个数字很难达到。CPU/GPU的编程方式会影响指令调度，指令本身并不总是需要相同数量的时钟周期来执行。此外，现代的CPU/GPU并没有针对运行按位代码进行优化，因此必须注意代码的编写方式。最后，虽然乘法是神经网络中总计算的很大一部分，但也有我们没有考虑的累加/求和。尽管如此，[ <a class="ae ky" href="https://arxiv.org/pdf/1602.02830.pdf" rel="noopener ugc nofollow" target="_blank"> 1 </a> ]报告称，在比较优化的BNN和未优化的正常神经网络时，加速比提高了23倍，这表明明显的加速是完全可以实现的。</p><p id="0002" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为我们使用的是1位数字而不是32位数字，我们还希望使用大约32倍的内存。当我们使用的内存减少32倍时，我们的内存访问也减少了32倍。这也将减少大约32倍的功耗。我找不到测试这个假设的实验数据，但这个结论对我来说似乎是合理的。</p><p id="edd2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，需要注意的是，所有这些BNN优势仅适用于运行时，不适用于训练时。这是因为，正如我们前面提到的，我们保留了一组实值权重来进行训练。这意味着梯度是一个32位浮点数，并不受制于我们所描述的1位优势。因此，BNN模型不应该在内存/功率受限的设备上训练，但是它们可以在这些设备上运行。</p><h1 id="88c9" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated"><strong class="ak">精度:</strong></h1><p id="7e66" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">当然，如果网络由于测试集上的不良准确性而不可用，那么无论速度/内存/功率提高多少都没有关系。二值化激活和权重比常规神经网络的表达能力差，所以我们预计准确性会更差。问题是会恶化到什么程度。我找不到任何回答这个问题的理论论文，所以现在我认为像下面这样的经验数据是我们最好的。我们在这里描述的BNN实现是第一部分，常规NN变体是最后一部分。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/30d9a07ade702a242b8710b5a32837a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SXsYBTHG415xIAvXOOEhkw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:[ <a class="ae ky" href="https://arxiv.org/pdf/1602.02830.pdf" rel="noopener ugc nofollow" target="_blank"> 1 </a></p></figure><p id="6a74" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们看到，误差肯定会增加，特别是在CIFAR-10数据集上(10.15%对7.62%)。然而，考虑到BNNs如前所述的所有性能优势，这大约3%的差异并不是世界末日。对于每个精度点都至关重要的任务，例如医疗x射线筛查，我们不想使用BNNs。但是在精度不那么重要或者性能更重要的情况下，bnn是一个可行的选择。</p><p id="e734" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我们已经简要了解了BNN的机制。我们还看到了BNNs如何显著提高速度、内存使用和功耗，代价是精度稍低。要更深入地了解BNNs，请看下面的参考资料。</p><p id="6a7b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[1] M. Courbariaux和I. Hubara，<a class="ae ky" href="https://arxiv.org/pdf/1602.02830.pdf" rel="noopener ugc nofollow" target="_blank">二值化神经网络:训练深度神经网络，权重和激活限制为+1或-1 </a> (2016)</p><p id="1b70" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2] T. Simons和D. Lee，<a class="ae ky" href="https://www.mdpi.com/2079-9292/8/6/661/pdf" rel="noopener ugc nofollow" target="_blank">二值化神经网络综述</a> (2019)</p></div></div>    
</body>
</html>