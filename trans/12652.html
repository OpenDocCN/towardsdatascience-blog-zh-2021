<html>
<head>
<title>Paper explained: Masked Autoencoders Are Scalable Vision Learners</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">论文解释:屏蔽自动编码器是可伸缩视觉学习器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/paper-explained-masked-autoencoders-are-scalable-vision-learners-9dea5c5c91f0?source=collection_archive---------8-----------------------#2021-12-29">https://towardsdatascience.com/paper-explained-masked-autoencoders-are-scalable-vision-learners-9dea5c5c91f0?source=collection_archive---------8-----------------------#2021-12-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a827" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">重建图像中被遮蔽的部分是如何有益的</h2></div><p id="3dbe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">自动编码器在自然语言处理任务方面有着成功的历史。伯特模型开始在句子的不同部分屏蔽单词，并试图通过预测要填入空格的单词来重建完整的句子。最近的工作旨在将这一想法转移到计算机视觉领域。</p><p id="a7b2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这个故事中，我们将看看最近发表的论文<a class="ae le" href="https://arxiv.org/pdf/2111.06377.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="lf">“掩蔽的自动编码器是可扩展的视觉学习者”</em> </a>何等人从 2021 年开始。明凯·何是计算机视觉领域最具影响力的研究人员之一，他与 Meta AI Research 的其他研究人员一起取得了 ResNet、fast R-CNN 和 Mask R-CNN 等突破性进展。在他们的最新论文中，他们提出了一种新的方法，使用自动编码器对计算机视觉模型，特别是视觉变压器进行自我监督的预训练。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi lg"><img src="../Images/4def0cad763e0a5b2de7f2e4e5f94262.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l8zPV1sSDmEPbwHTDh5Rzw.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">屏蔽自动编码器训练管道的高级可视化。来源:<a class="ae le" href="https://arxiv.org/pdf/2111.06377.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a></p></figure><p id="5cf1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们深入研究他们提出的方法之前，重要的是我们要快速回顾一下自我监督的预训练，以正确设置环境。如果你已经熟悉自我监督的预备培训，可以跳过这一部分。我尽量让文章简单，这样即使没有什么先验知识的读者也能理解。事不宜迟，我们开始吧！</p><h1 id="e784" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">先决条件:计算机视觉的自我监督预培训</h1><p id="6306" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">在我们深入讨论之前，有必要快速回顾一下自我监督预培训是怎么回事。如果你熟悉自我监督的预备培训，可以跳过这一部分。</p><p id="f439" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">传统上，计算机视觉模型总是使用<strong class="kk iu">监督学习</strong>来训练。这意味着人类看着这些图像，并为它们创建了各种标签，这样模型就可以学习这些标签的模式。例如，人类注释者可以为图像分配一个类标签，或者在图像中的对象周围绘制边界框。但是，任何接触过标注任务的人都知道，创建足够的训练数据集的工作量很大。</p><p id="42c5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">相比之下，<strong class="kk iu">自我监督学习不需要任何人为创造的标签</strong>。顾名思义，<strong class="kk iu">模型学会自我监督</strong>。在计算机视觉中，对这种自我监督进行建模的最常见方式是获取图像的不同裁剪或对其应用不同的增强，并通过模型传递修改后的输入。尽管图像包含相同的视觉信息，但看起来并不相同，<strong class="kk iu">我们让模型知道这些图像仍然包含相同的视觉信息</strong>，即相同的对象。<strong class="kk iu">这导致模型学习相同对象的相似潜在表示(输出向量)。</strong></p><p id="4e79" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以稍后在这个预训练的模型上应用迁移学习。通常，这些模型然后在 10%的带有标签的数据上进行训练，以执行下游任务，如对象检测和语义分割。</p><h1 id="8b5a" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">使用掩蔽使自动编码器理解视觉世界</h1><p id="045e" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">这篇论文的一个关键创新已经包含在标题中:图像的屏蔽。在将图像输入编码器转换器之前，会对其应用一组特定的遮罩。这里的想法是从图像中移除像素，从而为模型提供一个不完整的图片。这个模型的任务是学习完整的原始图像是什么样子的。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/e89257e0f573a5f65cb4828f0e5797de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1128/format:webp/1*0Vd0bkvc5ta0yQFDOxz19w.png"/></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">在左边，可以看到被屏蔽的图像。在右边，显示原始图像。中间一栏显示了自动编码器重建的图像。来源:<a class="ae le" href="https://arxiv.org/pdf/2111.06377.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a></p></figure><p id="3d30" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">作者发现非常高的掩蔽比是最有效的。在这些例子中，他们用遮罩覆盖了 75%的图像。这带来了两个好处:</p><ol class=""><li id="b773" class="mu mv it kk b kl km ko kp kr mw kv mx kz my ld mz na nb nc bi translated">训练模型的速度提高了 3 倍，因为它需要处理的图像补片要少得多</li><li id="268d" class="mu mv it kk b kl nd ko ne kr nf kv ng kz nh ld mz na nb nc bi translated">由于模型必须彻底地从图像中学习视觉世界，因此精确度增加了</li></ol><p id="554d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">遮罩总是随机应用的，因此同一图像的多个版本可以用作输入。</p><p id="b85d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">既然已经对图像进行了预处理，让我们来看看模型架构。在他们的论文中，何等人决定采用非对称编码器设计。这意味着他们的编码器可以更深，而他们选择一个相当轻量级的解码器。</p><p id="e51d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">编码器将图像划分为分配了位置编码的面片(即上面图像中的正方形)，并且只处理图像中未被遮罩的部分。编码器的输出是输入图像块的潜在矢量表示。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi ni"><img src="../Images/a143594774c12e99fa13aa13f6c8278d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GczhOMZJz0DUHjDvwkpQNw.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">编码器接收未掩蔽的图像补片并输出潜在向量表示的可视化。来源:<a class="ae le" href="https://arxiv.org/pdf/2111.06377.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a></p></figure><p id="1bc9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在此之后，引入掩模标记，因为下一步是解码器重建初始图像。每个掩码标记都是一个共享的学习向量，用于指示缺失补丁的存在。再次应用位置编码来与解码器进行通信，其中各个补片位于原始图像中。</p><p id="f373" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">解码器接收潜在表示以及掩模标记作为输入，并输出包括掩模在内的每个补片的像素值。根据这些信息，可以将原始图像拼凑在一起，以从用作输入的掩蔽图像形成完整图像的预测版本。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi nj"><img src="../Images/c8be5c475591ab5eb6d7c2af9ded7cf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z6RqFr6FlGrt06upLgAM8g.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">从掩蔽的输入图像的潜在表示到重建的目标图像的解码过程。来源:<a class="ae le" href="https://arxiv.org/pdf/2111.06377.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a></p></figure><p id="2daa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在蓝色潜在向量计算之后添加掩码标记是一个重要的设计决策。它减少了编码器到达矢量输出的计算成本，因为它必须处理更少的面片。这使得模型在训练期间更快。</p><p id="c9a3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一旦目标图像被重建，它与原始输入图像的差异被测量并被用作损失。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi nk"><img src="../Images/fd48c2d6b6cdb4672d4c11850114aba8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Asf08MKwx0suN9luT7cFKA.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">重建图像与原始图像的比较。来源:<a class="ae le" href="https://arxiv.org/pdf/2111.06377.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a></p></figure><p id="18e3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在模型被训练之后，解码器被丢弃，只有编码器，即视觉变换器，被保留以供进一步使用。它现在能够计算图像的潜在表示，以便进一步处理。</p><p id="ad87" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们已经讨论了论文介绍的方法，让我们看看一些结果。</p><h1 id="1484" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">结果</h1><p id="d057" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">由于屏蔽自动编码器使用变压器，因此作者将其性能与其他基于变压器的自监督方法进行比较是有意义的。他们的改进表现在第一次比较中:</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi nl"><img src="../Images/2125d19817a32110415ce207740999c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AnmBCsfsP4VL82zEELKgpg.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">使用 ImageNet-1K 上的微调结果进行预训练。来源:<a class="ae le" href="https://arxiv.org/pdf/2111.06377.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a></p></figure><p id="e27f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在与其他方法的比较中，当在 ImageNet-1K 上预训练模型，然后端到端地微调它时，MAE (masked autoencoder)显示出比其他方法(如<a class="ae le" rel="noopener" target="_blank" href="/paper-explained-dino-emerging-properties-in-self-supervised-vision-transformers-f9386df266f1"> DINO </a>、MoCov3 或 BEiT)更好的性能。即使随着模型尺寸的增加，改进也保持稳定，使用 ViT-H(巨大的视觉转换器)时性能最好。MAE 达到了令人难以置信的 87.8 的准确度。</p><p id="89b8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这种表现也适用于下游任务的迁移学习:</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="lm ln di lo bf lp"><div class="gh gi nm"><img src="../Images/06123f1c805baab380f1b44295fa0c5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*41zLgru2DgtXdxgHqpPQ3w.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">迁移学习在不同变压器预训练方法中的应用。这些结果用于 COCO 检测和分割数据集。来源:<a class="ae le" href="https://arxiv.org/pdf/2111.06377.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a></p></figure><p id="1506" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当使用预训练的变换器作为在 MS COCO 检测和分割数据集上训练的掩模 R-CNN 的主干时，MAE 再次优于所有其他基于变换器的方法。它实现了令人难以置信的 53.3 AP(平均精度)的盒子。掩模 R-CNN 还输出对象的分割掩模。在这次评估中，MAE 再次以面膜高达 47.2 AP 的成绩高居所有其他方法之首。该方法甚至优于 Mask R-CNN 的完全监督训练，再次显示了自我监督预训练的益处。</p><h1 id="2ef7" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">包装它</h1><p id="a7c2" class="pw-post-body-paragraph ki kj it kk b kl mo ju kn ko mp jx kq kr mq kt ku kv mr kx ky kz ms lb lc ld im bi translated">在本文中，您已经了解了 masked autoencoders (MAE ),这是一篇利用变压器和自动编码器进行自我监督预训练的论文，并为自我监督预训练工具箱添加了另一个简单但有效的概念。在某些任务上，它甚至优于完全监督的方法。虽然我希望这个故事能让你对这篇论文有一个很好的初步了解，但是还有很多东西需要发现。因此，我会鼓励你自己阅读这篇论文，即使你是这个领域的新手。你必须从某个地方开始；)</p><p id="99db" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你对论文中介绍的方法有更多的细节感兴趣，请随时在 Twitter 上给我留言，我的账户链接在我的媒体简介上。</p><p id="c42e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我希望你喜欢这篇论文的解释。如果你对这篇文章有任何意见，或者如果你看到任何错误，请随时留下评论。</p><p id="5683" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">最后但同样重要的是，如果你想在高级计算机视觉领域更深入地探索，考虑成为我的追随者</strong>。我试着每周发一篇文章，让你和其他人了解计算机视觉研究的最新进展。</p></div><div class="ab cl nn no hx np" role="separator"><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns nt"/><span class="nq bw bk nr ns"/></div><div class="im in io ip iq"><p id="818d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">参考资料:</p><p id="8c38" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[1]何，，等，“蒙版自动编码器是可伸缩的视觉学习者。”<em class="lf"> arXiv 预印本 arXiv:2111.06377 </em> (2021)。<a class="ae le" href="https://arxiv.org/pdf/2111.06377.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2111.06377.pdf</a></p></div></div>    
</body>
</html>