<html>
<head>
<title>Recognition and Counting of Microorganisms on Petri Dishes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">培养皿上微生物的识别和计数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/recognition-and-counting-of-microorganisms-on-petri-dishes-53a499283dc8?source=collection_archive---------17-----------------------#2021-07-27">https://towardsdatascience.com/recognition-and-counting-of-microorganisms-on-petri-dishes-53a499283dc8?source=collection_archive---------17-----------------------#2021-07-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9a59" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><strong class="ak">引入深度学习方法</strong>用于分析微生物图像</h2></div><p id="87c1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb"/><a class="ae lc" href="https://medium.com/@jaroslaw.pawlowski" rel="noopener"><em class="lb">雅罗斯瓦夫</em> </a> <em class="lb">和</em> <a class="ae lc" href="https://medium.com/@sylwia.majchrowska" rel="noopener"> <em class="lb">西尔维娅</em> </a> <em class="lb">。</em></p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ld"><img src="../Images/da7de8c1a5de6a6a9451cdd748b4bfed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7nUf-RNHJZj7LEBX.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">作者图片。</p></figure><p id="86e9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">制药、化妆品或食品行业的生产过程受到严格的政策和法规的约束，制造商有义务进行持续的微生物监控。这意味着成千上万的样本，通常以标准培养皿(琼脂培养基上生长的微生物培养物)的形式，必须由经验丰富的微生物学家人工分析和计数。这是一个耗时且容易出错的过程，需要训练有素的专业人员。为了避免这些问题，应用于该任务的自动化方法将是非常受欢迎的。</p><p id="d267" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我们将介绍由NeuroSYS研究团队开发的分析微生物图像的<strong class="kh ir">深度学习方法</strong>。训练机器学习模型的关键是获得大的、构造良好的数据集。因此，我们将利用在我们之前的<a class="ae lc" href="https://neurosys.com/annotated-germs-automated-recognition/" rel="noopener ugc nofollow" target="_blank">帖子</a>中介绍的<a class="ae lc" href="https://agar.neurosys.com/" rel="noopener ugc nofollow" target="_blank">琼脂数据集</a>来训练一个模型，该模型根据细菌菌落的RGB图像对培养皿上生长的细菌菌落进行计数和分类。</p><blockquote class="lt lu lv"><p id="cccf" class="kf kg lb kh b ki kj jr kk kl km ju kn lw kp kq kr lx kt ku kv ly kx ky kz la ij bi translated">你知道这条线索吗？—拥有大量数据的愚蠢算法胜过拥有少量数据的聪明算法。</p></blockquote><h1 id="e284" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">微生物菌落的检测</h1><p id="6c0c" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">好的，那么让我们从检测微生物开始。想象一下，我们有一个培养皿的图像(这种圆形玻璃通常用于保存生长培养基，以便在实验室繁殖微生物细胞)。在不同琼脂图像设置中的这种培养皿的示例性照片显示在图1的左栏中。其他5列显示了包含5种不同微生物类型的照片片段(我们称之为<em class="lb">小块</em>)。现在很容易理解<strong class="kh ir">微生物检测</strong>是什么意思了。我们只需确定每个微生物的确切位置和大小，用蓝色矩形标记它(我们称之为<em class="lb">边界框</em>)，如图1所示，并在图2中放大。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ld"><img src="../Images/3ed0facbb80748f6a57b0abcdd565196.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*MgaGtIGmjk34_iOC.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">图一。不同类型的微生物(第2-6列)在不同照明设置(每行)的培养皿(第1列)中生长。每个蓝色边界框都是由一个训练有素的探测器预测的。作者图片。</p></figure><p id="99f2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于训练有素的专业人员来说，这似乎很容易，但请注意，微生物菌落边缘可能模糊不清，菌落本身非常小(甚至只有几个像素大)，或者相机设置(例如聚焦或照明)非常不充分(例如，参见图1中第三行的照明条件)。此外，一些群体可能重叠，这使得一个群体结束而另一个群体开始的决定非常具有挑战性。这就是为什么要建立一个微生物菌落<strong class="kh ir">定位</strong>和<strong class="kh ir">分类</strong>的自动化系统真的很难。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi mw"><img src="../Images/cbd35bdf3eefe5f2cbd316723c383e56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*7hpxoaScMbHafH5e.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">图二。由我们的探测器进行预测的放大补丁。请注意，菌落可能有非常不同的大小:一些菌落非常小，一些菌落很大，边缘模糊，一些菌落可能重叠——这使得检测非常具有挑战性。作者图片。</p></figure><p id="b796" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为此，我们开发了一种用于微生物检测的深度学习模型。深度学习是一系列人工智能模型，主要利用<strong class="kh ir">人工神经网络</strong>。这种现代方法在许多领域都非常成功，例如在计算机视觉或机器翻译领域。深度学习<strong class="kh ir">物体检测器</strong>(在我们的例子中是检测微生物菌落)是非常复杂的多级模型，有数百层，每层由数百个神经元组成。</p><h2 id="e99c" class="mx ma iq bd mb my mz dn mf na nb dp mj ko nc nd ml ks ne nf mn kw ng nh mp ni bi translated">人工智能试图解决对人类来说相对容易但极难编程的任务。</h2><p id="2737" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">这里，我们采用基于区域的卷积神经网络(R-CNN)系列的两级检测器[2，3]，已知这种检测器速度慢但非常精确(与单级检测器相比，例如著名的YOLO [4])。关于其工作原理的简短解释，请参见图3。关于各种物体检测算法的更详细的解释，请参见我们之前关于这个问题的博文。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ld"><img src="../Images/8f0621bbbacb6fed5ffaa900faa2474b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*gqY0aN-Wusjvcp9H.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">图3。R-CNN检测器的两级结构。第一阶段(a)生成区域建议，这些区域建议只是原始图像的较小部分，我们认为它们可能包含我们正在搜索的对象。在第二阶段:(b)我们采用每个区域提议，并使用深度CNN创建表示该区域的特征向量，并对每个提议进行分类:它是否相关，如果是，它包含哪类对象。图改编自[2]。</p></figure><h1 id="0c68" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">训练探测器</h1><p id="a2d2" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">展示完微生物检测结果后，让我们看看检测机是如何工作的。这包含神经网络<strong class="kh ir">监督训练</strong>过程，但不仅如此:数据预处理和后处理也出现在图4的训练方案中。为了以监督的方式训练深度学习模型，我们需要一个带标签的数据集。如前所述，我们在此使用琼脂数据集，该数据集由带有标记微生物菌落的培养皿的图像组成。</p><p id="172c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">神经网络的典型特征是模型的结构严格对应于输入大小。当训练(和评估)网络时，我们受到可用存储器的限制，因此我们不能一次处理整个高分辨率图像，所以我们必须将它分成许多小块。这个过程并不简单，因为在切割成小块的过程中，我们必须确保给定的菌落完整地出现在至少一个小块上。</p><p id="e2f4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">之后，我们准备训练检测器(图4中的上一行)。我们从R-CNN家族中挑选了8款不同的机型进行综合对比。训练完探测器后，我们<strong class="kh ir">在照片上(实际上是在补丁上)测试了</strong>探测器(图4中的下部管道)<strong class="kh ir">在训练期间看不到</strong>，以确保测试公平进行。请注意，为测试准备的补丁只是被均匀地切掉了——在这个阶段，我们不能包含关于边界框所在位置的信息。</p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ld"><img src="../Images/5ba4d1e648960f4289ee1443647cc12b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9ZEr6kZ-GjkdiozM.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">图4。监督训练和评估(测试)我们的微生物菌落检测器神经网络模型的流程图。作者图片。</p></figure><h1 id="bbbb" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">检测和计数结果</h1><p id="f852" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">我们在图1和图2中看到，我们的模型<strong class="kh ir">很好地检测到了菌落</strong>。但是如何定量描述检测的性能呢？我们可以计算一些标准数字(度量)来描述模型在测试集上的性能。其中最受欢迎的一种称为平均精度(AP)或多类检测情况下的平均精度(mAP)(详细定义见本文<a class="ae lc" href="https://neurosys.com/article/object-detection-algorithms-starter-pack/" rel="noopener ugc nofollow" target="_blank">帖子</a>)。在琼脂数据集的两个子集(较高和较低分辨率)上评估的两个选定的R-CNN模型(更快的ResNet-50和Cascade HRNet)的AP和mAP结果如图5所示(左侧表格)。</p><p id="9f32" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一般来说，AP值越高，检测越精确——预测的和真实的<strong class="kh ir">边界框</strong>彼此更好地匹配。然而，请注意，这里的情况有点复杂，因为我们有不同的微生物类型，这意味着除了找到菌落，detector还需要对它们进行分类。</p><p id="3ba8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">用不同的fidelites检测不同种类的微生物，这影响了图5所示的mAP。例如，<em class="lb">金黄色葡萄球菌</em>细菌边缘锋利的小菌落比<em class="lb">假单胞菌</em> <em class="lb">铜绿</em>细菌边缘模糊的大菌落(AP约50%)更容易被检测和标记(AP约65%)。同样值得一提的是，与在著名的COCO数据集上使用相同架构完成的报告相比，我们的结果似乎非常出色:级联R-CNN为45%，更快的R-CNN为37%[5]。</p><p id="05c1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与检测培养皿上的每个菌落严格相关的最后一项任务是<strong class="kh ir">计数</strong>。在检测到所有的微生物菌落后，我们将它们相加，并将该数值与给定培养皿中菌落的真实数值进行比较。在琼脂试验子集上用相同的两个模型计数的结果显示在图5中(右边的图)。</p><p id="f6a3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<em class="lb">x</em>-轴上，我们有由训练有素的专业人员估计的不同培养皿的地面真实菌落数，而在<em class="lb"> y </em>轴上，我们有由我们的模型预测的值——每对(<em class="lb">真实值</em>，<em class="lb">预测值</em>)由这些图上的单个黑点表示。很明显，在理想预测的情况下，所有点都应该位于由黑线表示的<em class="lb"> y = x </em>曲线上。幸运的是，绝大多数点都位于这条曲线附近——模型计算得相当好。两条额外的蓝色曲线标记+/- 10%的计数误差，我们可以看到只有少数点(特别是具有超过50个菌落的更高密度的培养皿)位于该区域之外。</p><p id="d1f2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">平均计数误差通过平均绝对误差(MAE)来测量，例如在本博客的<a class="ae lc" href="https://neurosys.com/objects-counting-by-estimating-a-density-map/" rel="noopener ugc nofollow" target="_blank">中定义的，以及所谓的对称平均绝对百分比误差(sMAPE)，其基于百分比误差来测量准确度【6】。一般来说，sMAPE不超过5%，这是一个相当合理的结果。</a></p><figure class="le lf lg lh gt li gh gi paragraph-image"><div role="button" tabindex="0" class="lj lk di ll bf lm"><div class="gh gi ld"><img src="../Images/43989c1ec0500d98a32562dc31471e2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fhUkLrktRJMMTDO6.png"/></div></div><p class="lp lq gj gh gi lr ls bd b be z dk translated">图5。微生物菌落检测的质量:左边是描述检测本身保真度的平均精度度量的结果，而右边是菌落计数的比较——预测与真实菌落数(理想预测位于y=x黑色曲线上)。作者图片。</p></figure><h1 id="3534" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">结论</h1><p id="0661" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">总之，在这篇文章中，我们介绍了关于在培养皿上识别 <strong class="kh ir">微生物的<strong class="kh ir">深度学习研究。所选的R-CNN模型在检测微生物菌落方面表现非常好。菌落具有相似的形状并且所有种类的微生物在训练数据中都有很好的表现，这一事实有助于检测，证明了琼脂数据集的实用性。此外，用基本的更快的R-CNN和更复杂的级联R-CNN获得的结果差别不大。</strong></strong></p><p id="12ea" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如上所述，对于菌落数少于50的样品，检测器更加准确。然而，对于具有数百甚至数千个菌落的培养皿，它们仍然给出了非常好的估计，如图6所示，在高度密集的样品中正确地识别出单个菌落。<strong class="kh ir">在极端情况下，一个平板上检测到的最大菌落数等于2782个</strong>。值得注意的是，深度学习系统需要几秒钟，而在人工计数的情况下，可能需要一个小时。此外，在某些情况下，探测器能够识别人类难以看到和错过的群体。这些案例证实了建立自动微生物检测系统的好处，并且这可以通过使用现代深度学习技术成功实现。</p><h1 id="76b9" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">参考</h1><p id="6b45" class="pw-post-body-paragraph kf kg iq kh b ki mr jr kk kl ms ju kn ko mt kq kr ks mu ku kv kw mv ky kz la ij bi translated">[1] P. Domingos，关于机器学习要知道的一些有用的事情，Commun。美国计算机学会，第55卷，第78-87页，2012年。</p><p id="eb31" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2] R. Girshick等，精确对象检测和语义分割的丰富特征层次，IEEE计算机视觉和模式识别会议论文集，2014。</p><p id="5e51" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[3] <a class="ae lc" href="https://www.telesens.co/2018/03/11/object-detection-and-classification-using-r-cnns/" rel="noopener ugc nofollow" target="_blank"> A. Mohan，使用R-CNNs的物体检测和分类</a>，关于RCNN模型的非常详细的博客，2018。</p><p id="a993" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[4] J. Redmon等，你只看一次:统一的、实时的物体检测，IEEE计算机视觉与模式识别会议论文集，2016。</p><p id="f995" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[5] J. Wang等，面向视觉识别的深度高分辨率表征学习，IEEE模式分析与机器智能汇刊，2020。</p><p id="3952" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[6] S. Majchrowska，J. Pawł owski，G. Guł，T. Bonus，A. Hanas，A. Loch，A. Pawlak，J. Roszkowiak，T. Golan，和Z. Drulis-Kawa，AGAR a Microbial Colony Dataset for Deep Learning Detection，2021年7月7日，预印本可在arXiv<a class="ae lc" href="https://arxiv.org/abs/2108.01234" rel="noopener ugc nofollow" target="_blank">arXiv:2108.01234</a>查阅。</p><p id="d232" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">由欧洲区域发展基金下的欧盟基金共同资助的项目，作为精明增长运营计划的一部分。作为国家研究与发展中心的一部分实施的项目:快速通道。</em></p><figure class="le lf lg lh gt li gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/a97d00a942feeb31fd0bf7ac98e9656a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/0*_fKYz99X3ghM-K2F.png"/></div></figure></div><div class="ab cl nk nl hu nm" role="separator"><span class="nn bw bk no np nq"/><span class="nn bw bk no np nq"/><span class="nn bw bk no np"/></div><div class="ij ik il im in"><p id="858f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">原载于2021年</em>10月13日<em class="lb">https://neurosys.com</em><em class="lb">。</em></p><p id="c6bc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lc" href="https://github.com/majsylw/microbial-counting-review" rel="noopener ugc nofollow" target="_blank">https://github.com/majsylw/microbial-counting-review</a></p><p id="03c5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lc" href="https://arxiv.org/abs/2108.01234" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2108.01234</a></p></div></div>    
</body>
</html>