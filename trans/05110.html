<html>
<head>
<title>What Is Embedding and What Can You Do with It</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是嵌入，你能用它做什么</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-is-embedding-and-what-can-you-do-with-it-61ba7c05efd8?source=collection_archive---------9-----------------------#2021-05-05">https://towardsdatascience.com/what-is-embedding-and-what-can-you-do-with-it-61ba7c05efd8?source=collection_archive---------9-----------------------#2021-05-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/db0f9807f7593ea87a7f423e208b5063.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bJj5Gi8bbhiUyBil0qubxQ.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="7fc9" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi la translated"><span class="l lb lc ld bm le lf lg lh li di"> W </span> ord2vec(由托马斯·米科洛夫领导的谷歌研究团队发表)作为自然语言处理领域的“突破性技术”，至今已有八年历史。他们开创了单词嵌入的概念，作为技术的基础。</p><p id="b687" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">到目前为止，嵌入的使用已经被应用到广泛的分析中，并且产生了显著的影响。机器学习或深度学习中的许多概念都是建立在彼此之上的，嵌入的概念也不例外。对嵌入有一个坚实的理解将使学习许多更高级的ML技术变得容易得多。</p><p id="cbde" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">因此，在今天的博客中，我将引导您浏览以下主题，以帮助您彻底理解嵌入的定义和应用:</p><ol class=""><li id="456e" class="lj lk iq ke b kf kg kj kk kn ll kr lm kv ln kz lo lp lq lr bi translated"><strong class="ke ir">外行人解释:一个特定任务的字典</strong></li><li id="1e54" class="lj lk iq ke b kf ls kj lt kn lu kr lv kv lw kz lo lp lq lr bi translated"><strong class="ke ir">直观解释:正在行走的点</strong></li><li id="ce4d" class="lj lk iq ke b kf ls kj lt kn lu kr lv kv lw kz lo lp lq lr bi translated"><strong class="ke ir"> <em class="lx">为什么我们想要嵌入</em> </strong></li><li id="abf8" class="lj lk iq ke b kf ls kj lt kn lu kr lv kv lw kz lo lp lq lr bi translated"><strong class="ke ir"> <em class="lx">嵌入的各种应用</em> </strong></li><li id="c0cd" class="lj lk iq ke b kf ls kj lt kn lu kr lv kv lw kz lo lp lq lr bi translated"><strong class="ke ir"> <em class="lx">汇总</em> </strong></li></ol><h1 id="3d4b" class="ly lz iq bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">外行人的解释:特定任务的字典</h1><p id="9ece" class="pw-post-body-paragraph kc kd iq ke b kf mw kh ki kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz ij bi translated">从Google的<a class="ae nb" href="https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture" rel="noopener ugc nofollow" target="_blank">机器学习速成班</a>中，我找到了关于嵌入的描述:一个<strong class="ke ir">嵌入</strong>是一个相对低维的空间，你可以将高维向量平移到其中。嵌入使得在大量输入上进行机器学习变得更加容易，比如表示单词的稀疏向量。理想情况下，嵌入通过在嵌入空间中将语义相似的输入放在一起来捕获输入的一些语义。嵌入可以跨模型学习和重用。</p><p id="a960" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">太棒了！这可能是我在网上能找到的关于嵌入的最精确和简洁的描述。尽管如此，它仍然有点令人费解和模糊。那么，在假装对机器学习只有初步了解的情况下，又该如何通俗易懂地解释呢？</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/be36992814f8eb8e0b94682a96d6d4d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*t1UdSJa7ofLWttGi3PNXhA.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="d569" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这里有一个例子:当应用于文本挖掘项目时，嵌入可以帮助我们通过研究一个词经常出现在什么其他词旁边来学习这个词的语义。然后我们可以产生一个嵌入列表，可以把它当作<strong class="ke ir"> <em class="lx">一个任务专用字典</em> </strong>。如果你想了解语料库中某个特定单词的更多信息，去“字典”里查一下。然而，它并没有为你提供人类语言的定义，而是返回<strong class="ke ir"> <em class="lx">一个数值向量来反映它的语义</em> </strong>。此外，这些向量之间的距离度量了项目中术语之间的相似性和关系。</p><p id="04ee" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">从word2vec的名字可以看出——“单词到向量”，它将单词转换成数字的向量。换句话说，<strong class="ke ir"> <em class="lx">嵌入是一串数字，作为唯一标识符</em> </strong>。我们可以使用嵌入技术为一个单词、一个人、一个声音、一幅图像等分配一个唯一的数字ID。在你的研究中。科学家利用这种想法，创造了许多有趣的2vec风格的模型，以促进机器学习过程。</p><h1 id="d39a" class="ly lz iq bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">直观解释:行走的点</h1><p id="9fc1" class="pw-post-body-paragraph kc kd iq ke b kf mw kh ki kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz ij bi translated">通常，嵌入被保存为向量。矢量的定义是“一个有方向和大小的量，特别是确定空间中一点相对于另一点的位置。”由于<strong class="ke ir">嵌入被假定为在空间</strong>中具有方向和大小，直观地，我们可以将每个矢量视为步行点的路线图。我们可以想象，所有的嵌入大概都是从同一点出发，它们会开始沿着自己的方向和权重在空间中行走。<strong class="ke ir">在行走之后，每个嵌入将到达不同的端点，并且相邻点彼此更加相似，因此应该被分类到相同的组中。</strong>因此，我们倾向于借助余弦相似度使用嵌入进行社区检测或群组聚类。它通常也用于分类任务。</p><p id="9774" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">为了说明上面提到的想法，我们可以做的是使用维度演绎技术(主成分分析，UMAP等)。)将高维向量的大小缩小到2D/3D，并在图上画出点。这里有一个博客专门向你展示如何在3D空间中实现这一点:</p><div class="nh ni gp gr nj nk"><a rel="noopener follow" target="_blank" href="/visualize-high-dimensional-network-data-with-3d-360-degree-animated-scatter-plot-d583932d3693"><div class="nl ab fo"><div class="nm ab nn cl cj no"><h2 class="bd ir gy z fp np fr fs nq fu fw ip bi translated">使用3D 360度动画散点图可视化高维网络数据</h2><div class="nr l"><h3 class="bd b gy z fp np fr fs nq fu fw dk translated">使用node2vec，networkx，pca，seaborn等。可视化高维网络数据</h3></div><div class="ns l"><p class="bd b dl z fp np fr fs nq fu fw dk translated">towardsdatascience.com</p></div></div><div class="nt l"><div class="nu l nv nw nx nt ny jw nk"/></div></div></a></div><h1 id="c5fc" class="ly lz iq bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated"><strong class="ak"> <em class="nz">为什么我们要嵌入</em> </strong></h1><p id="9c57" class="pw-post-body-paragraph kc kd iq ke b kf mw kh ki kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz ij bi translated">以下是我们希望在项目中包含嵌入的几个原因:</p><p id="81fb" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">首先，当前的<strong class="ke ir">机器学习模型继续倾向于将数值作为输入</strong>。他们类似于数学书呆子，当输入数字时，他们可以快速捕捉重要信息，但对离散的分类变量反应迟钝。然而，当研究计算机视觉或语音识别等时，我们不太可能能够收集或仅收集我们的目标/因变量的数字数据。<strong class="ke ir">将这些离散的分类变量转换成数字有助于模型拟合</strong>。</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oa"><img src="../Images/ac85600f2437f3a208c7e542afbc4e3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IYCCKTnwf5TkF6DDjVakrA.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="8fb4" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">第二，它有助于<strong class="ke ir">减小尺寸</strong>。有人可能会说，一次性编码技术是我们处理分类变量的方式。然而，在今天的数据科学世界中，它被证明是非常低效的。</p><p id="6861" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">当处理一个有四种不同类型的变量时，我们通常会创建四个新的虚拟变量来处理它。此外，它以前运行良好。</p><p id="4d56" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">然而，考虑以下场景:我们正在研究三种产品的消费者反馈。对于每个观察，我们只有一个变量——评论内容。我们可以建立一个术语-文档矩阵，然后将其放入分类器或其他算法中。然而，让我们假设每个产品有5万条评论，语料库中的唯一单词总数是一百万。那么我们最终会得到一个形状为(150K x 1M)的矩阵。对于任何模型来说，这都是一个大得离谱的输入。这就是我们需要引入嵌入概念的时候。</p><p id="da61" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">假设我们将维度减少到15(每个产品有一个15位数的ID)，取每个产品嵌入的平均值，然后根据数值给它们着色；这是我们得到的:</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/4ab07498c91fd7f856a331047f19713d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1036/format:webp/1*sHhVghE_9HC017qJwvwTyw.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="38e1" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">即使没有人类语言呈现，我们仍然可以感知到客户对产品A和B的感知彼此更相似，而对产品C的感知不同。而这个矩阵的形状只有(3×15)。</p><p id="9f0a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这是另一个在谷歌机器学习速成班上讲过的例子，讲的是如何为一个电影推荐系统使用嵌入:<a class="ae nb" href="https://developers.google.com/machine-learning/crash-course/embeddings/categorical-input-data" rel="noopener ugc nofollow" target="_blank">嵌入:分类输入数据</a>。</p><p id="3e5f" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">第三个原因是为了<strong class="ke ir">降低复杂度</strong>。这有点像第二个原因的延伸。嵌入还有助于将非常复杂的信息转化为数字向量。下面是一个社交网络分析的例子:</p><figure class="nd ne nf ng gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oc"><img src="../Images/4e4a4f4369f8d97663b62872016b8ec7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cxTUU0IBtCx857Wre4PjHw.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="62d3" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">最初，我们从社交媒体收集数据，并将其转换为社交网络分析图。在图中，我们可以使用节点之间的距离和纽带的颜色来解释节点之间的相似性。但是，它很复杂，很难读懂。现在，我们在图中只有14个节点，而且已经很乱了。你能想象如果我们调查100个节点会发生什么吗？这被称为复杂(高维)数据。然而，通过使用某些技术来帮助降维，我们可以将图转换成嵌入列表。结果，我们现在有了一个新的、干净的节点“字典”,而不是杂乱的图。我们可以使用“字典”来制作一个人类可读的可视化。</p><h1 id="0935" class="ly lz iq bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">T <strong class="ak"> <em class="nz"> he嵌入的各种应用</em>T3】</strong></h1><p id="373a" class="pw-post-body-paragraph kc kd iq ke b kf mw kh ki kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz ij bi translated">在发现了嵌入是什么以及我们为什么需要它之后，您一定很兴奋地看到了它在实践中的一些应用。因此，我选择了一系列有趣的应用程序，这些应用程序使用了嵌入的思想以及一些相关的文献或用法演示。</p><h2 id="5ede" class="od lz iq bd ma oe of dn me og oh dp mi kn oi oj mm kr ok ol mq kv om on mu oo bi translated">自然语言处理:</h2><p id="63a9" class="pw-post-body-paragraph kc kd iq ke b kf mw kh ki kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz ij bi translated">word2vec:</p><ul class=""><li id="ed88" class="lj lk iq ke b kf kg kj kk kn ll kr lm kv ln kz op lp lq lr bi translated">论文:<a class="ae nb" href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" rel="noopener ugc nofollow" target="_blank">https://papers . nips . cc/paper/5021-distributed-presentations-of-words-and-phrases-and-they-composition ity . pdf</a></li><li id="09c6" class="lj lk iq ke b kf ls kj lt kn lu kr lv kv lw kz op lp lq lr bi translated">解说:<a class="ae nb" href="https://jalammar.github.io/illustrated-word2vec/" rel="noopener ugc nofollow" target="_blank">https://jalammar.github.io/illustrated-word2vec/</a></li></ul><p id="a19a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">sent2vec:</p><ul class=""><li id="774b" class="lj lk iq ke b kf kg kj kk kn ll kr lm kv ln kz op lp lq lr bi translated">论文:<a class="ae nb" href="https://arxiv.org/abs/1703.02507" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1703.02507</a></li><li id="d2f0" class="lj lk iq ke b kf ls kj lt kn lu kr lv kv lw kz op lp lq lr bi translated">解说:<a class="ae nb" href="https://bit.ly/3uxFJ7S" rel="noopener ugc nofollow" target="_blank">https://bit.ly/3uxFJ7S</a></li></ul><p id="42fd" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">doc2vec:</p><ul class=""><li id="c550" class="lj lk iq ke b kf kg kj kk kn ll kr lm kv ln kz op lp lq lr bi translated">论文:<a class="ae nb" href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf" rel="noopener ugc nofollow" target="_blank">https://cs.stanford.edu/~quocle/paragraph_vector.pdf</a></li><li id="06d7" class="lj lk iq ke b kf ls kj lt kn lu kr lv kv lw kz op lp lq lr bi translated">演示:<a class="ae nb" rel="noopener" target="_blank" href="/detecting-document-similarity-with-doc2vec-f8289a9a7db7">https://towards data science . com/detecting-document-similarity-with-doc 2 vec-f 8289 a 9 a 7 db 7</a></li></ul><h2 id="52d7" class="od lz iq bd ma oe of dn me og oh dp mi kn oi oj mm kr ok ol mq kv om on mu oo bi translated">图像分析:</h2><p id="33d2" class="pw-post-body-paragraph kc kd iq ke b kf mw kh ki kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz ij bi translated">Img2vec:</p><ul class=""><li id="7fd5" class="lj lk iq ke b kf kg kj kk kn ll kr lm kv ln kz op lp lq lr bi translated">解释和示范:<a class="ae nb" href="https://becominghuman.ai/extract-a-feature-vector-for-any-image-with-pytorch-9717561d1d4c" rel="noopener ugc nofollow" target="_blank">https://becoming human . ai/extract-a-feature-vector-for-any-image-with-py torch-9717561 d1d 4c</a></li></ul><h2 id="4874" class="od lz iq bd ma oe of dn me og oh dp mi kn oi oj mm kr ok ol mq kv om on mu oo bi translated">社交网络分析:</h2><p id="5c52" class="pw-post-body-paragraph kc kd iq ke b kf mw kh ki kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz ij bi translated">node2vec:</p><ul class=""><li id="fd31" class="lj lk iq ke b kf kg kj kk kn ll kr lm kv ln kz op lp lq lr bi translated">论文:<a class="ae nb" href="https://arxiv.org/abs/1607.00653" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1607.00653</a></li><li id="e4ef" class="lj lk iq ke b kf ls kj lt kn lu kr lv kv lw kz op lp lq lr bi translated">案例分析&amp;代码:<a class="ae nb" href="https://medium.com/analytics-vidhya/analyzing-disease-co-occurrence-using-networkx-gephi-and-node2vec-53941da35a0f" rel="noopener">https://medium . com/analytics-vid hya/analyzing-disease-co-occurrence-using-networkx-ge phi-and-node 2 vec-53941 da 35 a 0 f</a></li></ul><h1 id="10fe" class="ly lz iq bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">总结:</h1><p id="1a53" class="pw-post-body-paragraph kc kd iq ke b kf mw kh ki kj mx kl km kn my kp kq kr mz kt ku kv na kx ky kz ij bi translated">在机器学习的背景下，嵌入的功能相当于特定任务的字典。它用一系列数字对我们的目标进行编码，作为一个唯一的ID。我们喜欢使用嵌入，因为它可以帮助将离散的分类变量转换成模型可读的数据，还可以帮助降低数据的维度和复杂性。我还列举了几款精选的2vec风格的车型。</p><p id="300d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">本月晚些时候，我可能会写另一篇文章来演示嵌入的一种用法。上面的算法只是2vec风格模型的一小部分。如果你感兴趣，我想<a class="ae nb" href="https://github.com/chihming/awesome-network-embedding" rel="noopener ugc nofollow" target="_blank">这个网站</a>可能会帮助你。</p><p id="683c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> <em class="lx">请随时与我联系</em></strong><a class="ae nb" href="https://www.linkedin.com/in/jinhangjiang/" rel="noopener ugc nofollow" target="_blank"><strong class="ke ir"><em class="lx">LinkedIn</em></strong></a><strong class="ke ir"><em class="lx">。</em> </strong></p></div></div>    
</body>
</html>