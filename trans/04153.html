<html>
<head>
<title>Introduction to Classification Using K Nearest Neighbours</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用K个最近邻的分类介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/getting-acquainted-with-k-nearest-neighbors-ba0a9ecf354f?source=collection_archive---------5-----------------------#2021-04-08">https://towardsdatascience.com/getting-acquainted-with-k-nearest-neighbors-ba0a9ecf354f?source=collection_archive---------5-----------------------#2021-04-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/de63bcf711df8a5da4f2bd25d6e742ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZwWhjcOf_AbLeVGMNPgyDA.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片由<a class="ae jd" href="https://pixabay.com/users/chenspec-7784448/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=5474989" rel="noopener ugc nofollow" target="_blank"> chenspec </a>来自<a class="ae jd" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=5474989" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><div class=""/><div class=""><h2 id="7087" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">一种逐步建立KNN分类器的方法</h2></div><p id="456c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">作为机器学习实践者，我们会遇到各种各样的机器学习算法，我们可以利用这些算法来建立特定的预测模型。在本文中，我将重点介绍最复杂的学习算法之一，即K近邻算法。该算法可用于处理最大似然法中的回归和分类问题。</p><p id="521c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这篇文章中，我将首先尝试给你一个什么是算法的直觉，以及它如何做出预测。然后，我将尝试分解与该算法密切相关的某些重要术语，最后，在本文结束时，我们将使用KNN算法设计自己的分类器。</p><p id="ee04" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在开始这篇文章之前，我建议你看一下我的<a class="ae jd" href="http://rajashwin.medium.com/" rel="noopener">以前的文章</a>，我在那里讨论了各种学习算法。此外，如果你喜欢这篇文章，请考虑为这篇文章鼓掌，并确保关注我的更多机器学习食谱。这篇文章的PDF副本可以从<a class="ae jd" href="https://drive.google.com/file/d/139nsR8YPyRPmzkuMv7C6KA0U4-s_nPID/view?usp=sharing" rel="noopener ugc nofollow" target="_blank">这里</a>下载，继续你的离线学习。</p><h1 id="7f00" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated"><strong class="ak">算法背后的想法</strong></h1><p id="a176" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">假设你从一个装满金属球的袋子里随机选择一个球，这些金属球要么是金的，要么是碳的。为了区分球是由金还是碳制成，你可以检查它的颜色、导电性、沸点和熔点。这些被称为特征。利用这些特征，我们可以确定球属于哪一类。这类似于KNN分类算法的工作原理。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mo"><img src="../Images/4c4fb7b108ca6c7f0b793ee3bb0876f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*R9P-psALmaTA8r0s9dNECQ.gif"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片由<a class="ae jd" href="https://medium.com/r?url=https%3A%2F%2Fdennisbakhuis.medium.com%2F%3Fsource%3Dpost_page-----3d16dc5c45ef--------------------------------" rel="noopener">丹尼斯·巴克豪斯</a>拍摄</p></figure><p id="4d8a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">KNN算法是一种简单、一致但通用的监督学习算法，可用于解决分类和回归问题。它本质上可以被视为一种算法，该算法基于训练数据集中与其接近的其他数据点的性质来进行预测。</p><p id="155b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">也就是说，简单地说，分类器算法通过计算输入样本和最接近输入样本的k个训练实例之间的相似性，输出对象最有可能被分配到的类别。它假设相近的值存在。KNN经常被认为是一个懒惰的学习者，因为它不学习判别函数。</p><p id="93fb" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当很少或没有关于数据分布的信息时，通常使用这种算法。此外，该算法本质上是非参数化的，这意味着它不对数据或其总体分布模式做出任何潜在假设。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/da0fef64262e5dc08e7afa8c3133c36d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*0BB6XtINbZb5DTQq2MkkGQ.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片来自<a class="ae jd" href="http://covartech.github.io" rel="noopener ugc nofollow" target="_blank"> covartech.github.io </a></p></figure><p id="0e0b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，KNN算法不需要训练时间。然而，无论何时将该算法投入使用，它都需要搜索整个数据集以找到K个最近邻。该算法主要用于设计推荐系统和执行分析评估。</p><h1 id="eb4e" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated"><strong class="ak">KNN选粉机的引擎盖下</strong></h1><p id="88a1" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">通常，解决分类问题的复杂部分是找到要使用的正确算法。不同的算法适用于不同类型的数据。KNN算法一般用于小于100K标记非文本数据样本的数据集，一般在LinearSVC无法投入使用时使用。</p><p id="4cca" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如前所述，KNN是一种监督学习算法，可用于处理回归和分类问题。在这篇文章中，我们将集中在KNN分类器。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/657e816631d51d2b8555fefa0476bc70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1160/format:webp/1*l_BBK2i96I9-eXsCjc5s7Q.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片来自<a class="ae jd" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fwww.journaldev.com%2F36835%2Fscatterplots-in-r&amp;psig=AOvVaw3BVriOgcRU6PYMK-mOxsA6&amp;ust=1617957935977000&amp;source=images&amp;cd=vfe&amp;ved=0CAMQjB1qFwoTCOD_iZeh7u8CFQAAAAAdAAAAABA7" rel="noopener ugc nofollow" target="_blank"> JournalDev </a></p></figure><p id="773f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">分类器将训练数据作为输入，并输出对象预期所属的类。该对象被分配给其大多数相邻实例所属的类。</p><h2 id="d688" class="mv ls jg bd lt mw mx dn lx my mz dp mb le na nb md li nc nd mf lm ne nf mh ng bi translated"><strong class="ak">KNN中的K分类器</strong></h2><p id="a000" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">KNN中的k是一个参数，指的是包含在决策过程中的特定数据点的最近邻居的数量。这是核心决定因素，因为分类器输出取决于这些相邻点的大多数所属的类别。</p><p id="3bc5" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">考虑如果K的值是5，那么该算法将考虑五个最近的相邻数据点来确定物体的类别。选择正确的K值被称为参数调整。随着K值的增加，预测曲线变得更加平滑。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/0b7a254d4b89562574f8fd4555d2146b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*hygeR-j2S2nM8WF4kMCqmA.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片来自<a class="ae jd" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fwww.cs.cornell.edu%2Fcourses%2Fcs4780%2F2018fa%2Flectures%2Flecturenote02_kNN.html&amp;psig=AOvVaw1ovjOwPhN4irRwxZdhWa6h&amp;ust=1617949864321000&amp;source=images&amp;cd=vfe&amp;ved=0CAMQjB1qFwoTCMC4-IuD7u8CFQAAAAAdAAAAABAD" rel="noopener ugc nofollow" target="_blank">康奈尔CS </a></p></figure><p id="8398" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">默认情况下，K的值为5。没有找到K值的结构化方法，但是K的最佳值是数据集中存在的样本总数的平方根。K的值通常取为奇数值，以避免决策过程中出现平局。误差图或精度图通常用于寻找最合适的k值。</p><h1 id="6b1a" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated"><strong class="ak">KNN的距离度量</strong></h1><p id="1103" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">KNN使用各种不同类型的距离度量来计算距离。为了使算法有效工作，我们需要选择最合适的距离度量。其中一些指标如下所述:</p><ul class=""><li id="e23f" class="ni nj jg kx b ky kz lb lc le nk li nl lm nm lq nn no np nq bi translated"><strong class="kx jh">欧几里德距离:</strong>这是sklearn KNN分类器使用的默认度量。它通常被称为L2规范。它计算欧几里得空间中两点之间的普通直线距离。</li></ul><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nr"><img src="../Images/c9aaf4832ed666aae715726af88f7d33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vAtQZbROuTdp36aQQ8cqBA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片来自<a class="ae jd" href="https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781785882104/6/ch06lvl1sec40/measuring-distance-or-similarity" rel="noopener ugc nofollow" target="_blank"> Packt订阅</a></p></figure><ul class=""><li id="75c8" class="ni nj jg kx b ky kz lb lc le nk li nl lm nm lq nn no np nq bi translated"><strong class="kx jh">曼哈顿距离:</strong>通常被称为城市街区度量，在这种情况下，两点之间的距离是它们的笛卡尔坐标的绝对差，并且在高维度的情况下使用。</li><li id="f823" class="ni nj jg kx b ky ns lb nt le nu li nv lm nw lq nn no np nq bi translated"><strong class="kx jh">汉明距离:</strong>该指标用于比较两个二进制数据字符串，通常与OneHotEncoding一起使用。它查看整个数据集，并发现数据点何时相似或不相似。</li></ul><p id="e607" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">既然我们已经讨论了KNN分类算法的理论概念，我们将应用我们的学习来使用K最近邻算法构建一个分类器。用于构建这个模型的代码和其他资源可以在我的<a class="ae jd" href="https://github.com/ashwinraj-in/MachineLearningRecipes/blob/master/KNearestNeighbour.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub句柄</a>中找到。</p><h2 id="c8d5" class="mv ls jg bd lt mw mx dn lx my mz dp mb le na nb md li nc nd mf lm ne nf mh ng bi translated">步骤1:导入所需的库和数据集</h2><p id="4f1f" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">构建分类器的第一步是导入将用于构建分类模型的所有必需的包。首先，我们将导入Pandas、Numpy、Seaborn和Matplotlib包。</p><pre class="mp mq mr ms gt nx ny nz oa aw ob bi"><span id="11f1" class="mv ls jg ny b gy oc od l oe of"><strong class="ny jh"><em class="og">#Importing the Required Libraries and Loading the Dataset<br/></em></strong>import pandas as pd<br/>import numpy as np<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt</span><span id="2349" class="mv ls jg ny b gy oh od l oe of">%matplotlib inline<br/>dataset = pd.read_csv("Iris.csv")</span></pre><p id="8c93" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这个例子中，我们将尝试使用各种特征，如花瓣长度、花瓣宽度、萼片长度和萼片宽度，对鸢尾花进行分类。我们在选择模型参数时需要非常小心，因为KNN对要使用的参数非常敏感。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/5f26200bb6f570d639ae8188a5ebb351.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*70N-3n93NIfu5vKSUAwiqg.gif"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片来自<a class="ae jd" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fwww.lancaster.ac.uk%2Fstor-i-student-sites%2Fhamish-thorburn%2F2020%2F02%2F23%2Fmodel-based-clustering%2F&amp;psig=AOvVaw3qT81cRGnMGtLAdv5fCfNB&amp;ust=1617955901640000&amp;source=images&amp;cd=vfe&amp;ved=0CAMQjB1qFwoTCNC4icqZ7u8CFQAAAAAdAAAAABAD" rel="noopener ugc nofollow" target="_blank">兰卡斯特大学</a></p></figure><p id="6e27" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一旦导入了所有这些库，下一步就是获取数据集。用于建立这个模型的数据可以从<a class="ae jd" href="https://www.kaggle.com/uciml/iris" rel="noopener ugc nofollow" target="_blank">这里</a>下载。read_csv()函数用于将数据集加载到笔记本中。用于该模型的数据集可以从这里下载。</p><h2 id="9ee9" class="mv ls jg bd lt mw mx dn lx my mz dp mb le na nb md li nc nd mf lm ne nf mh ng bi translated"><strong class="ak">第二步:数据分割和标签编码</strong></h2><p id="ea40" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">加载数据集后，下一步是将数据分割为要素和标注。在机器学习中，特征是作为输入提供给系统的独立变量，而标签是我们试图预测的属性。</p><pre class="mp mq mr ms gt nx ny nz oa aw ob bi"><span id="f4e3" class="mv ls jg ny b gy oc od l oe of"><strong class="ny jh"><em class="og">#Dividing Data Into Features and Labels<br/></em></strong>feature_columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']<br/>X = dataset[feature_columns].values<br/>y = dataset['Species'].values</span><span id="5a7d" class="mv ls jg ny b gy oh od l oe of"><strong class="ny jh"><em class="og">#Label Encoding<br/></em></strong>from sklearn.preprocessing import LabelEncoder<br/>le = LabelEncoder()<br/>y = le.fit_transform(y)</span></pre><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oj"><img src="../Images/088667ee24afaf301ae7a65203921e8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0322LCCBUOQpj3fdYSXsRw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片来自<a class="ae jd" href="https://www.researchgate.net/figure/Illustration-of-the-k-nearest-neighbours-kNN-classifier-a-kNN-assigns-a-class-to_fig2_321554029" rel="noopener ugc nofollow" target="_blank"> ResearchGate </a>的<a class="ae jd" href="https://www.researchgate.net/profile/Danilo-Bzdok" rel="noopener ugc nofollow" target="_blank"> Danilo Bzdok </a></p></figure><p id="d550" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">KNN分类器不接受字符串标签，因此有必要在对数据建模之前对这些标签进行编码。标签编码器用于将这些标签转换成数值。</p><h2 id="a723" class="mv ls jg bd lt mw mx dn lx my mz dp mb le na nb md li nc nd mf lm ne nf mh ng bi translated"><strong class="ak">步骤3:可视化数据集</strong></h2><p id="2e3f" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">在构建分类模型时，可视化数据集是一个重要的步骤。这使得理解数据和寻找数据中隐藏的模式变得更加容易。Python有一系列可以用来可视化数据的包。</p><pre class="mp mq mr ms gt nx ny nz oa aw ob bi"><span id="8322" class="mv ls jg ny b gy oc od l oe of"><strong class="ny jh"><em class="og">#Visualizing Dataset using Pairplot<br/></em></strong>plt.figure()<br/>sns.pairplot(dataset.drop("Id", axis=1), hue = "Species", size=3, markers=["o", "s", "D"])<br/>plt.show()</span><span id="b530" class="mv ls jg ny b gy oh od l oe of"><strong class="ny jh"><em class="og">#Visualizing Dataset using Boxplot<br/></em></strong>plt.figure()<br/>dataset.drop("Id", axis=1).boxplot(by="Species", figsize=(15, 10))<br/>plt.show()</span></pre><p id="ffaa" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Matplotlib是Python中静态绘图的基础。Seaborn是最流行的绘制视觉上吸引人的图形的库之一，它建立在Matplotlib之上。Matplotlib和Seaborn就是这样的两个软件包，它们用于在各种不同类型的图表上绘制数据。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ok"><img src="../Images/ab26f15fca41da81039e93b2b09d96db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*FY1gA13d3AEI3oYPsiicHA.gif"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片来自<a class="ae jd" href="https://blog.magrathealabs.com/choosing-one-of-many-python-visualization-tools-7eb36fa5855f" rel="noopener ugc nofollow" target="_blank">Magrathea</a>Gabriela Moreira Mafra</p></figure><p id="f064" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">Matplotlib的<a class="ae jd" href="https://matplotlib.org/tutorials/introductory/sample_plots.html" rel="noopener ugc nofollow" target="_blank">网站</a>包含了多个文档、教程和大量的例子，这使得它的使用更加容易。箱线图、Pairplot和Andrews曲线是几种最广泛使用的数据可视化图形，可用于绘制图形以可视化分类问题。</p><h2 id="5ddb" class="mv ls jg bd lt mw mx dn lx my mz dp mb le na nb md li nc nd mf lm ne nf mh ng bi translated"><strong class="ak">第四步:分割数据集并拟合模型</strong></h2><p id="5fa0" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">选择所需的参数并可视化数据集后，下一步是从sklearn库中导入train_test_split，该库用于将数据集拆分为训练和测试数据。我们将把数据集分成80%的训练数据和20%的测试数据。</p><pre class="mp mq mr ms gt nx ny nz oa aw ob bi"><span id="0f17" class="mv ls jg ny b gy oc od l oe of"><strong class="ny jh"><em class="og">#Splitting the Data into Training and Testing Dataset<br/></em></strong>from sklearn.model_selection import train_test_split<br/>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)</span><span id="9cfa" class="mv ls jg ny b gy oh od l oe of"><strong class="ny jh"><em class="og">#Fitting the Model and Making Predictions <br/></em></strong>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.metrics import confusion_matrix, accuracy_score<br/>from sklearn.model_selection import cross_val_score<br/><br/>classifier = KNeighborsClassifier(n_neighbors=3)<br/>classifier.fit(X_train, y_train)<br/>y_pred = classifier.predict(X_test)</span></pre><p id="917f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这之后，KNeighborsClassifier从sklearn.neighbors包中导入，并且使用设置为3的k值实例化分类器。然后将分类器拟合到数据集上，并使用y _ pred = classifier . predict(X _ test)对测试集进行预测。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/aac30fd9e9900b22704650bc54f20626.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*VoC6c5TDIO-qFq3-m1oc8g.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片来自<a class="ae jd" href="http://sumunosato.koukodou.or.jp/" rel="noopener ugc nofollow" target="_blank">sumunosato . koukodou . or . jp</a></p></figure><p id="8441" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">由于在本例中，要素处于相同的数量级，因此不对数据集执行任何要素缩放。但是，建议在拟合分类器之前对数据集进行归一化和缩放。</p><h2 id="4e5d" class="mv ls jg bd lt mw mx dn lx my mz dp mb le na nb md li nc nd mf lm ne nf mh ng bi translated"><strong class="ak">第五步:评估预测并交叉验证</strong></h2><p id="84da" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">评估分类算法预测的最简单和最常用的技术是通过构建混淆矩阵。另一种方法是计算模型的准确度分数。</p><pre class="mp mq mr ms gt nx ny nz oa aw ob bi"><span id="dc0a" class="mv ls jg ny b gy oc od l oe of"><strong class="ny jh"><em class="og">#Confusion Matrix<br/></em></strong>confusion_matrix = confusion_matrix(y_test, y_pred)<br/>confusion_matrix</span><span id="1d78" class="mv ls jg ny b gy oh od l oe of"><strong class="ny jh"><em class="og">#Calculating Model Accuracy<br/></em></strong>accuracy = accuracy_score(y_test, y_pred)*100<br/>print('Accuracy of the model:' + str(round(accuracy, 2)) + ' %.')</span></pre><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi om"><img src="../Images/85a8c07b7dcb2a37caabab97702447d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*AXpsP-0aidKr_tKfTahKbA.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片由<a class="ae jd" href="https://medium.com/@pharosprod?source=post_page-----32c68673d8fc--------------------------------" rel="noopener"> Dmytro Nasyrov </a>从<a class="ae jd" href="https://medium.com/pharos-production/learning-machine-learning-petals-32c68673d8fc" rel="noopener">媒体</a>获取</p></figure><pre class="mp mq mr ms gt nx ny nz oa aw ob bi"><span id="b664" class="mv ls jg ny b gy oc od l oe of"><strong class="ny jh"><em class="og">#Performing 10 fold Cross Validation<br/></em></strong>k_list = list(range(1,50,2))<br/>cv_scores = []<br/>for k <strong class="ny jh">in</strong> k_list:<br/>    knn = KNeighborsClassifier(n_neighbors=k)<br/>    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')<br/>    cv_scores.append(scores.mean())</span><span id="ed36" class="mv ls jg ny b gy oh od l oe of"><strong class="ny jh"><em class="og">#Finding Best K<br/></em></strong>best_k = k_list[MSE.index(min(MSE))]<br/>print("The optimal number of neighbors is <strong class="ny jh">%d</strong>." % best_k)</span></pre><p id="2f41" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">评估输出预测后，下一步是为参数调整执行多重交叉验证。在这里，我们正在进行十倍交叉验证，以找到k的最佳值。</p><h1 id="4c02" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated"><strong class="ak">KNN分类器的优势</strong></h1><p id="4af9" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">KNN分类器是最复杂和应用最广泛的分类算法之一。使该算法如此流行的一些特征如下所述:</p><ol class=""><li id="fcdf" class="ni nj jg kx b ky kz lb lc le nk li nl lm nm lq on no np nq bi translated">KNN没有对这些数据做出任何潜在的假设。</li><li id="8993" class="ni nj jg kx b ky ns lb nt le nu li nv lm nw lq on no np nq bi translated">与许多分类算法相比，它具有相对较高的精度。</li><li id="d2d7" class="ni nj jg kx b ky ns lb nt le nu li nv lm nw lq on no np nq bi translated">随着更多数据点的添加，分类器不断进化，能够快速适应输入数据集中的变化。</li><li id="21bc" class="ni nj jg kx b ky ns lb nt le nu li nv lm nw lq on no np nq bi translated">它为用户提供了选择距离度量标准的灵活性</li></ol><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/3b08726098973801a35e78204c4e9d3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/1*5guRJwB0a7MmMOCNFznGJQ.gif"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片来自<a class="ae jd" href="https://milliams.com/courses/applied_data_analysis/Nearest%20Neighbours.html" rel="noopener ugc nofollow" target="_blank"> Milliams </a></p></figure><h1 id="6d1e" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated"><strong class="ak">KNN分类器的局限性</strong></h1><p id="a0a9" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">虽然分类器有多种优点，但它也有一些严重的局限性。其中一些限制如下所述:</p><ol class=""><li id="d7b3" class="ni nj jg kx b ky kz lb lc le nk li nl lm nm lq on no np nq bi translated">KNN对异常值非常敏感。</li><li id="dc55" class="ni nj jg kx b ky ns lb nt le nu li nv lm nw lq on no np nq bi translated">随着数据集的增长，分类速度会变慢</li><li id="1961" class="ni nj jg kx b ky ns lb nt le nu li nv lm nw lq on no np nq bi translated">KNN没有能力处理缺失的价值观。</li><li id="9429" class="ni nj jg kx b ky ns lb nt le nu li nv lm nw lq on no np nq bi translated">由于高存储要求，它在计算上是昂贵的。</li></ol><h1 id="3e5e" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated"><strong class="ak">总结你所学的知识</strong></h1><p id="0845" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">为了总结我们在本文中学到的东西，首先我们看一下算法背后的思想及其理论概念。然后，我们讨论了参数“K ”,并了解了如何找到K的最佳值。之后，我们回顾了各种距离度量指标。</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div class="gh gi op"><img src="../Images/1ba932a68247c8d862a3e947e5679a3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/1*Gvwlpk2lGsxOHfxP-EZYhg.gif"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片来自<a class="ae jd" href="https://piyushpathak03.medium.com/?source=post_page-----cacefe94ffc2--------------------------------" rel="noopener"> Piyush Pathak </a></p></figure><p id="68de" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然后，我们应用我们的学习来建立KNN分类器模型，以对各种植物物种进行分类，然后我们通过浏览分类算法的优点和局限性来支持我们的学习。</p><p id="efc9" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">至此，我们已经到了这篇文章的结尾。我希望这篇文章能帮助你对KNN算法的工作原理有所了解。如果你有任何问题，或者如果你认为我犯了任何错误，请随时与我联系。通过<a class="ae jd" href="http://rajashwin733@gmail.com/" rel="noopener ugc nofollow" target="_blank">电子邮件</a>或<a class="ae jd" href="https://www.linkedin.com/in/rajashwin/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>与我联系。快乐学习！</p></div></div>    
</body>
</html>