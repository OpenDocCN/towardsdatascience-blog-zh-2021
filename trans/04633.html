<html>
<head>
<title>Why Parallelized Training Might Not be Working for You</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么并行培训可能不适合您</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-parallelized-training-might-not-be-working-for-you-4c01f606ef2c?source=collection_archive---------15-----------------------#2021-04-21">https://towardsdatascience.com/why-parallelized-training-might-not-be-working-for-you-4c01f606ef2c?source=collection_archive---------15-----------------------#2021-04-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="4ebc" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/making-sense-of-big-data" rel="noopener" target="_blank">理解大数据</a></h2><div class=""/><div class=""><h2 id="dd64" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">神经网络并行训练的动机、益处和注意事项指南</h2></div></div><div class="ab cl ko kp hu kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="ij ik il im in"><h1 id="5507" class="kv kw iq bd kx ky kz la lb lc ld le lf kf lg kg lh ki li kj lj kl lk km ll lm bi translated">介绍</h1><p id="2752" class="pw-post-body-paragraph ln lo iq lp b lq lr ka ls lt lu kd lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">有几种并行训练神经网络的方法。</p><ol class=""><li id="4c11" class="mj mk iq lp b lq ml lt mm lw mn ma mo me mp mi mq mr ms mt bi translated">模型间并行性，也称为并行化超参数搜索</li><li id="671b" class="mj mk iq lp b lq mu lt mv lw mw ma mx me my mi mq mr ms mt bi translated">数据并行性</li><li id="44ed" class="mj mk iq lp b lq mu lt mv lw mw ma mx me my mi mq mr ms mt bi translated">模型内并行性，又称模型并行性</li><li id="1d90" class="mj mk iq lp b lq mu lt mv lw mw ma mx me my mi mq mr ms mt bi translated">流水线并行</li></ol><p id="50b6" class="pw-post-body-paragraph ln lo iq lp b lq ml ka ls lt mm kd lv lw mz ly lz ma na mc md me nb mg mh mi ij bi translated">在这篇文章中，我们将探讨两种最常见的并行方法，模型间并行和数据并行。我们将在CIFAR10数据集上使用一个简单的CNN模型来演示这些技术。我们将在运行AWS深度学习AMI和Pytorch (1.7.1)的4 GPU机器上训练这个模型</p></div><div class="ab cl ko kp hu kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="ij ik il im in"><h1 id="762b" class="kv kw iq bd kx ky kz la lb lc ld le lf kf lg kg lh ki li kj lj kl lk km ll lm bi translated">并行培训的优势</h1><p id="1454" class="pw-post-body-paragraph ln lo iq lp b lq lr ka ls lt lu kd lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">并行训练最明显的优势是速度。在超参数搜索的情况下，同时评估多个配置允许我们快速缩小最有希望的选项。</p><p id="74f2" class="pw-post-body-paragraph ln lo iq lp b lq ml ka ls lt mm kd lv lw mz ly lz ma na mc md me nb mg mh mi ij bi translated">通过分布式数据并行(DDP)训练，模型参数的副本被放置在每个可用的GPU上，并且每个副本被提供给整个数据集的不同子集。</p><p id="88ca" class="pw-post-body-paragraph ln lo iq lp b lq ml ka ls lt mm kd lv lw mz ly lz ma na mc md me nb mg mh mi ij bi translated">每次批量评估后，副本的梯度将被同步和平均。基于这些同步的梯度来更新模型副本的权重。</p><p id="9a9c" class="pw-post-body-paragraph ln lo iq lp b lq ml ka ls lt mm kd lv lw mz ly lz ma na mc md me nb mg mh mi ij bi translated">这增加了模型使用的有效批处理大小，允许它在比单个GPU的内存容量更大的批处理上进行训练。随着训练数据集大小的增加，DDP成为一种保持训练时间合理的方法。</p></div><div class="ab cl ko kp hu kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="ij ik il im in"><h1 id="54e6" class="kv kw iq bd kx ky kz la lb lc ld le lf kf lg kg lh ki li kj lj kl lk km ll lm bi translated">批量的重要性</h1><p id="246e" class="pw-post-body-paragraph ln lo iq lp b lq lr ka ls lt lu kd lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">使用较大的批量会导致更快的训练，因为网络能够以更少的步骤迭代整个数据集。然而，经验证据表明，更大的批量倾向于收敛到损失面的尖锐最小值，这导致泛化能力差[ <a class="ae nc" href="https://arxiv.org/pdf/1609.04836.pdf" rel="noopener ugc nofollow" target="_blank"> 1 </a> ]。相比之下，批量越小，最小值越宽、越平，具有良好的泛化能力。</p><p id="387c" class="pw-post-body-paragraph ln lo iq lp b lq ml ka ls lt mm kd lv lw mz ly lz ma na mc md me nb mg mh mi ij bi translated">让我们通过对一组候选批量运行超参数搜索来测试这一点。我们将使用恒定的学习速率<strong class="lp ja"> 0.001 </strong>在CIFAR10上训练我们的CNN<strong class="lp ja">10个时期</strong>。</p><p id="23c6" class="pw-post-body-paragraph ln lo iq lp b lq ml ka ls lt mm kd lv lw mz ly lz ma na mc md me nb mg mh mi ij bi translated">我们将并行运行<a class="ae nc" href="https://www.comet.ml/docs/python-sdk/Optimizer/" rel="noopener ugc nofollow" target="_blank"> Comet Optimizer </a>，并作为命令行参数输入一个Optimizer配置文件。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="1ffd" class="nm kw iq ni b gy nn no l np nq">comet optimize -j 4 comet-pytorch-parallel-hpo.py optim.config</span></pre><blockquote class="nr ns nt"><p id="c878" class="ln lo nu lp b lq ml ka ls lt mm kd lv nv mz ly lz nw na mc md nx nb mg mh mi ij bi translated"><a class="ae nc" href="https://www.comet.ml/team-comet-ml/parallelism/e35cc986496e4c9c835008a658fc00ab?experiment-tab=code&amp;utm_source=medium" rel="noopener ugc nofollow" target="_blank">并行化超参数优化的源代码</a></p></blockquote><p id="243f" class="pw-post-body-paragraph ln lo iq lp b lq ml ka ls lt mm kd lv lw mz ly lz ma na mc md me nb mg mh mi ij bi translated">这里的<code class="fe ny nz oa ni b">j</code>是我们想要启动的并行进程的数量。你可以在这里找到更多关于彗星优化器<a class="ae nc" href="https://www.comet.ml/docs/command-line/#comet-optimize" rel="noopener ugc nofollow" target="_blank">的细节</a></p><p id="6a82" class="pw-post-body-paragraph ln lo iq lp b lq ml ka ls lt mm kd lv lw mz ly lz ma na mc md me nb mg mh mi ij bi translated"><code class="fe ny nz oa ni b">optim.config</code>文件只是一个JSON对象，包含我们的参数搜索网格。</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="3c02" class="nm kw iq ni b gy nn no l np nq">{<br/>   # We pick the Bayes algorithm:<br/>   "algorithm": "random",<br/>   # Declare your hyperparameters in the Vizier-inspired format:<br/>   "parameters": {<br/>      "batch_size": {<br/>         "type": "discrete", <br/>         "values": [8, 32, 128, 512]<br/>      },<br/>   },<br/>   # Declare what we will be optimizing, and how:<br/>   "spec": {<br/>     "metric": "val_loss",<br/>     "objective": "minimize",<br/>   },<br/>}</span></pre><div class="nd ne nf ng gt ab cb"><figure class="ob oc od oe of og oh paragraph-image"><div role="button" tabindex="0" class="oi oj di ok bf ol"><img src="../Images/4f84db55c4c5a9b75d856100119b3c58.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*5muyKTFkHwL3KYGdnr5QHA.png"/></div></figure><figure class="ob oc oo oe of og oh paragraph-image"><div role="button" tabindex="0" class="oi oj di ok bf ol"><img src="../Images/6a2689ea16757d048fe511fde58af0e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1006/format:webp/1*73CETcxV2nyMu-siZnPDRg.png"/></div><p class="op oq gj gh gi or os bd b be z dk ot di ou ov translated">左:每个批量配置的运行时间(秒)，右:每个批量配置的相应测试准确度。来源:<a class="ae nc" href="https://www.comet.ml/site?utm_source=medium" rel="noopener ugc nofollow" target="_blank">T5】comet . mlT7】</a></p></figure></div><p id="1f4d" class="pw-post-body-paragraph ln lo iq lp b lq ml ka ls lt mm kd lv lw mz ly lz ma na mc md me nb mg mh mi ij bi translated">当查看折线图时，我们看到批量越大，训练运行时间越短。然而，较大的批量也会导致更差的训练误差和测试精度。事实上，批量越小，测试准确度越好。这是一个问题，因为最小批量的完成需要将近<strong class="lp ja">8倍的时间</strong>。</p><blockquote class="ox"><p id="88c2" class="oy oz iq bd pa pb pc pd pe pf pg mi dk translated">完成最小批量需要将近<strong class="ak">8倍于</strong>的时间。</p></blockquote></div><div class="ab cl ko kp hu kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="ij ik il im in"><h1 id="423b" class="kv kw iq bd kx ky kz la lb lc ld le lf kf lg kg lh ki li kj lj kl lk km ll lm bi translated">提高学习速度</h1><p id="6369" class="pw-post-body-paragraph ln lo iq lp b lq lr ka ls lt lu kd lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">使用大批量的一个关键方面是调整学习速度。一般的经验法则是遵循<a class="ae nc" href="https://arxiv.org/pdf/1706.02677.pdf" rel="noopener ugc nofollow" target="_blank">线性缩放规则</a>【2】。这意味着当批量增加<strong class="lp ja"> K </strong>倍时，学习率也必须增加<strong class="lp ja"> K </strong>倍。</p><p id="649c" class="pw-post-body-paragraph ln lo iq lp b lq ml ka ls lt mm kd lv lw mz ly lz ma na mc md me nb mg mh mi ij bi translated">让我们在超参数搜索中对此进行研究。让我们根据批量大小来线性调整我们的学习速度。我们将使用批次大小<strong class="lp ja"> 8 </strong>作为我们的缩放常数。</p><blockquote class="nr ns nt"><p id="2efa" class="ln lo nu lp b lq ml ka ls lt mm kd lv nv mz ly lz nw na mc md nx nb mg mh mi ij bi translated"><a class="ae nc" href="https://www.comet.ml/team-comet-ml/parallelism/dbd4eb2a1df549f4834eda8a80142fec?experiment-tab=code&amp;utm_source=medium" rel="noopener ugc nofollow" target="_blank">这些比例学习率实验的源代码</a></p></blockquote><div class="nd ne nf ng gt ab cb"><figure class="ob oc ph oe of og oh paragraph-image"><div role="button" tabindex="0" class="oi oj di ok bf ol"><img src="../Images/9ba85f68e7987e52e5a05949dcbc9902.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*FG1Lvh7i1WNfpgnuhkZieQ.png"/></div></figure><figure class="ob oc pi oe of og oh paragraph-image"><div role="button" tabindex="0" class="oi oj di ok bf ol"><img src="../Images/e64318281facaf7d567bfe64e622bd12.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*U5dPcapeRUZmfHcVq7oYEg.png"/></div><p class="op oq gj gh gi or os bd b be z dk pj di pk ov translated">左图:具有成比例学习率的不同批量的训练损失曲线，右图:具有成比例学习率的配置的测试精度。<em class="ow">来源:</em><a class="ae nc" href="https://www.comet.ml/site?utm_source=medium" rel="noopener ugc nofollow" target="_blank"><em class="ow">comet . ml</em></a></p></figure></div><p id="caff" class="pw-post-body-paragraph ln lo iq lp b lq ml ka ls lt mm kd lv lw mz ly lz ma na mc md me nb mg mh mi ij bi translated">这似乎起了作用。根据批量调整学习速度可以缩小泛化差距！让我们将这个想法与我们的分布式数据并行方法结合起来。</p></div><div class="ab cl ko kp hu kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="ij ik il im in"><h1 id="898a" class="kv kw iq bd kx ky kz la lb lc ld le lf kf lg kg lh ki li kj lj kl lk km ll lm bi translated">使用更大的有效批量</h1><p id="638d" class="pw-post-body-paragraph ln lo iq lp b lq lr ka ls lt lu kd lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">通过DDP训练，数据集在多个可用的GPU之间进行划分。</p><p id="ddc5" class="pw-post-body-paragraph ln lo iq lp b lq ml ka ls lt mm kd lv lw mz ly lz ma na mc md me nb mg mh mi ij bi translated">让我们使用<a class="ae nc" href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html" rel="noopener ugc nofollow" target="_blank"> Pytorch分布式数据并行模块</a>运行一组实验。该模块处理将模型复制到每个GPU，以及跨GPU进程同步梯度和更新权重。</p><p id="4b95" class="pw-post-body-paragraph ln lo iq lp b lq ml ka ls lt mm kd lv lw mz ly lz ma na mc md me nb mg mh mi ij bi translated">使用ddp模块非常简单。将您的现有模型包装在DDP模块中，并将其分配给GPU</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="0aef" class="nm kw iq ni b gy nn no l np nq">model = Net()<br/>model.cuda(gpu_id)<br/>ddp_model = DDP(model, device_ids=[gpu_id])</span></pre><p id="a0ce" class="pw-post-body-paragraph ln lo iq lp b lq ml ka ls lt mm kd lv lw mz ly lz ma na mc md me nb mg mh mi ij bi translated">我们将使用DistributedSampler对象来确保数据正确地分布在每个GPU进程中</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="1345" class="nm kw iq ni b gy nn no l np nq"># Load training data<br/>    trainset, testset = load_data()<br/>    test_abs = int(len(trainset) * 0.8)<br/>    train_subset, val_subset = random_split(<br/>        trainset, [test_abs, len(trainset) - test_abs]<br/>    )<br/><br/>    train_sampler = torch.utils.data.distributed.DistributedSampler(<br/>        train_subset, num_replicas=world_size, rank=global_process_rank<br/>    )<br/><br/>    trainloader = torch.utils.data.DataLoader(<br/>        train_subset,<br/>        batch_size=PER_REPLICA_BATCH_SIZE,<br/>        sampler=train_sampler,<br/>        num_workers=8,<br/>    )<br/>    valloader = torch.utils.data.DataLoader(<br/>        val_subset, batch_size=PER_REPLICA_BATCH_SIZE, shuffle=True, num_workers=8<br/>    )</span></pre><p id="6533" class="pw-post-body-paragraph ln lo iq lp b lq ml ka ls lt mm kd lv lw mz ly lz ma na mc md me nb mg mh mi ij bi translated">最后，使用DDP包装的模型运行训练循环</p><pre class="nd ne nf ng gt nh ni nj nk aw nl bi"><span id="baaf" class="nm kw iq ni b gy nn no l np nq">for epoch in range(args.epochs):<br/>   train_loss = train(ddp_model, optimizer, criterion, trainloader,       epoch, gpu_id)<br/>   val_loss, val_acc = evaluate(ddp_model, criterion, valloader, epoch, gpu_id)</span></pre><blockquote class="nr ns nt"><p id="9c8c" class="ln lo nu lp b lq ml ka ls lt mm kd lv nv mz ly lz nw na mc md nx nb mg mh mi ij bi translated"><a class="ae nc" href="https://www.comet.ml/team-comet-ml/parallelism/c63d294dfb9b416b9d95d954bcdd3aaf?experiment-tab=code&amp;utm_source=medium" rel="noopener ugc nofollow" target="_blank">DDP示例源代码</a></p></blockquote><p id="9580" class="pw-post-body-paragraph ln lo iq lp b lq ml ka ls lt mm kd lv lw mz ly lz ma na mc md me nb mg mh mi ij bi translated">在我们的例子中，每个GPU接收四分之一的数据集。当我们以这种方式运行训练时，我们的有效批量大小是GPU数量和每个GPU的批量大小的乘积。因此，当我们将每个GPU的批处理大小设置为8时，我们的有效批处理大小实际上是32。我们可以通过将DDP训练运行与批处理大小为32的单个GPU训练运行进行比较来验证这一点。注意曲线看起来是多么的相似，并且跑相似的训练步数。这些曲线表明，即使我们<strong class="lp ja">在每个GPU进程</strong>中使用较小的批量，我们的模型性能仍然依赖于<strong class="lp ja">的有效批量</strong>。</p></div><div class="ab cl ko kp hu kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="ij ik il im in"><h1 id="dd98" class="kv kw iq bd kx ky kz la lb lc ld le lf kf lg kg lh ki li kj lj kl lk km ll lm bi translated">把所有的放在一起</h1><p id="667c" class="pw-post-body-paragraph ln lo iq lp b lq lr ka ls lt lu kd lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">让我们结合我们所学的学习率调整，重新运行DDP培训。</p><div class="nd ne nf ng gt ab cb"><figure class="ob oc pl oe of og oh paragraph-image"><div role="button" tabindex="0" class="oi oj di ok bf ol"><img src="../Images/4000b0f71e064003815c1469dc4fdb0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*OJ-Lwh5QUq_1nGURiX9vyA.png"/></div></figure><figure class="ob oc pm oe of og oh paragraph-image"><div role="button" tabindex="0" class="oi oj di ok bf ol"><img src="../Images/b35ef6a44f9fa3381b56ddbddd22e49b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*vmWfflFuMPYpcL0GYS-Q5Q.png"/></div><p class="op oq gj gh gi or os bd b be z dk pn di po ov translated">左图:使用比例学习率和DDP训练的训练损失，右图:使用比例学习率和DDP训练的测试Acc。<em class="ow">来源:</em> <a class="ae nc" href="https://www.comet.ml/site?utm_source=medium" rel="noopener ugc nofollow" target="_blank"> <em class="ow"> Comet.ml </em> </a></p></figure></div><p id="c296" class="pw-post-body-paragraph ln lo iq lp b lq ml ka ls lt mm kd lv lw mz ly lz ma na mc md me nb mg mh mi ij bi translated">看起来，即使是学习率缩放也有其局限性。当我们将批量增加到某个值时，训练得到了改善，在此之后，我们看到我们的测试精度开始下降。</p></div><div class="ab cl ko kp hu kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="ij ik il im in"><h1 id="e093" class="kv kw iq bd kx ky kz la lb lc ld le lf kf lg kg lh ki li kj lj kl lk km ll lm bi translated">警告</h1><p id="db21" class="pw-post-body-paragraph ln lo iq lp b lq lr ka ls lt lu kd lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">理论上，将训练分布在多个GPU上应该会加快训练模型所需的总时间。实际上，情况并非总是如此。跨多个GPU进程同步梯度会导致通信开销，这对于最小化是不可忽视的。</p><p id="5fb0" class="pw-post-body-paragraph ln lo iq lp b lq ml ka ls lt mm kd lv lw mz ly lz ma na mc md me nb mg mh mi ij bi translated">在下面的面板中，我们将批量为32的单个流程培训运行的运行时间与具有相同有效批量的DDP培训运行的运行时间进行了比较。单个流程运行需要73秒才能完成，而DDP培训运行几乎比慢八倍<strong class="lp ja">，需要443秒才能完成。</strong></p><p id="227d" class="pw-post-body-paragraph ln lo iq lp b lq ml ka ls lt mm kd lv lw mz ly lz ma na mc md me nb mg mh mi ij bi translated">这可能是因为每次我们在训练代码中调用<code class="fe ny nz oa ni b">loss.backward()</code>时，梯度都是同步的。进程之间的持续通信导致总体运行时间增加。这是设置DDP培训时需要注意的一点。如果您在多机器设置中运行您的实验，请确保网络带宽足以处理发送和接收整个模型的检查点数据的每个进程。</p><p id="c25c" class="pw-post-body-paragraph ln lo iq lp b lq ml ka ls lt mm kd lv lw mz ly lz ma na mc md me nb mg mh mi ij bi translated">加速这个过程的另一个方法是改变梯度的同步时间表，但是这超出了本报告的范围。</p><figure class="nd ne nf ng gt oc gh gi paragraph-image"><div role="button" tabindex="0" class="oi oj di ok bf ol"><div class="gh gi pp"><img src="../Images/862b5a49414fe6d83e554a59d2d7a3b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JzKVJgjSTg-bieq8B_E7sA.png"/></div></div><p class="op oq gj gh gi or os bd b be z dk translated">单一流程培训与分布式培训的培训损失曲线。<em class="ow">来源:</em> <a class="ae nc" href="https://www.comet.ml/site?utm_source=medium" rel="noopener ugc nofollow" target="_blank"> <em class="ow"> Comet.ml </em> </a></p></figure></div><div class="ab cl ko kp hu kq" role="separator"><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt ku"/><span class="kr bw bk ks kt"/></div><div class="ij ik il im in"><h1 id="db64" class="kv kw iq bd kx ky kz la lb lc ld le lf kf lg kg lh ki li kj lj kl lk km ll lm bi translated">结论</h1><p id="d845" class="pw-post-body-paragraph ln lo iq lp b lq lr ka ls lt lu kd lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">我们希望这份并行培训入门指南对您有所帮助。在随后的文章中，我们将讨论如何优化您的分布式培训代码，以充分利用您的计算基础设施。</p><blockquote class="nr ns nt"><p id="e702" class="ln lo nu lp b lq ml ka ls lt mm kd lv nv mz ly lz nw na mc md nx nb mg mh mi ij bi translated"><a class="ae nc" href="https://www.comet.ml/team-comet-ml/parallelism/reports/advanced-ml-parallelism?utm_source=medium" rel="noopener ugc nofollow" target="_blank">点击此处阅读完整的交互式可视化报告</a></p><p id="f60a" class="ln lo nu lp b lq ml ka ls lt mm kd lv nv mz ly lz nw na mc md nx nb mg mh mi ij bi translated"><a class="ae nc" href="https://www.comet.ml/site?utm_source=medium" rel="noopener ugc nofollow" target="_blank"> Comet.ml </a> —轻松跟踪、比较和调试您的模型！</p><p id="7925" class="ln lo nu lp b lq ml ka ls lt mm kd lv nv mz ly lz nw na mc md nx nb mg mh mi ij bi translated">如果你还没有试用过Comet，它是一个很棒的工具，可以让你跟踪、比较和调试你的ML模型！</p><p id="7221" class="ln lo nu lp b lq ml ka ls lt mm kd lv nv mz ly lz nw na mc md nx nb mg mh mi ij bi translated">它可以与Colab、Jupyter笔记本和脚本一起使用，最重要的是它是100%免费的！</p><p id="29b2" class="ln lo nu lp b lq ml ka ls lt mm kd lv nv mz ly lz nw na mc md nx nb mg mh mi ij bi translated"><em class="iq">入门</em> <a class="ae nc" href="https://www.comet.ml/site/data-scientists?utm_source=medium" rel="noopener ugc nofollow" target="_blank"> <em class="iq">实验跟踪</em> </a> <em class="iq">今天！</em></p><p id="40e8" class="ln lo nu lp b lq ml ka ls lt mm kd lv nv mz ly lz nw na mc md nx nb mg mh mi ij bi translated"><a class="ae nc" href="https://www.comet.ml/signup?utm_source=medium" rel="noopener ugc nofollow" target="_blank"> <em class="iq">今天免费试用彗星！</em>T24】</a></p></blockquote><h1 id="ee77" class="kv kw iq bd kx ky pq la lb lc pr le lf kf ps kg lh ki pt kj lj kl pu km ll lm bi translated">参考</h1><p id="0dec" class="pw-post-body-paragraph ln lo iq lp b lq lr ka ls lt lu kd lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">[1] Keskar，Nitish Shirish等，“关于深度学习的大批量训练:泛化差距和尖锐极小值。”<em class="nu"> arXiv预印本arXiv:1609.04836 </em> (2016)。</p><p id="a105" class="pw-post-body-paragraph ln lo iq lp b lq ml ka ls lt mm kd lv lw mz ly lz ma na mc md me nb mg mh mi ij bi translated">[2] Goyal，Priya等，“精确的大型迷你批次sgd:在1小时内训练ImageNet。”<em class="nu"> arXiv预印本arXiv:1706.02677 </em> (2017)。</p></div></div>    
</body>
</html>