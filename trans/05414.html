<html>
<head>
<title>A Guide to Building Nonlinear Least Squares (NLS) Regression Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">构建非线性最小二乘(NLS)回归模型指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-guide-to-building-nonlinear-least-squares-nls-regression-models-310b97a7baeb?source=collection_archive---------13-----------------------#2021-05-13">https://towardsdatascience.com/a-guide-to-building-nonlinear-least-squares-nls-regression-models-310b97a7baeb?source=collection_archive---------13-----------------------#2021-05-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/f1ba5d662e1cad76bdd566ee2f733f97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Eavhmdq9jtAJo8XRor_aBw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图片由<a class="ae jd" href="https://pixabay.com/users/alexandra_koch-621802/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1817646" rel="noopener ugc nofollow" target="_blank"> Alexandra_Koch </a>来自<a class="ae jd" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1817646" rel="noopener ugc nofollow" target="_blank"> Pixabay </a> ( <a class="ae jd" href="https://pixabay.com/service/license/" rel="noopener ugc nofollow" target="_blank"> Pixabay许可</a>)</p></figure><div class=""/><div class=""><h2 id="1c70" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">以及用Python和SciPy编写的NLS回归教程</h2></div><p id="871e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">非线性最小二乘法(NLS) </strong>是一种优化技术，可用于为包含非线性特征的数据集构建回归模型。这种数据集的模型在系数上是非线性的。</p><p id="bacf" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">本文的结构:</p><p id="dd87" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">第1部分:NLS回归模型的概念和理论。这部分有一些数学知识。如果你喜欢数学和/或对非线性最小二乘回归如何工作感到好奇，你会喜欢它。</p><p id="bfd7" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">第2部分:关于如何使用Python和SciPy <strong class="kx jh">构建和训练NLS回归模型的教程</strong>。你不需要阅读第一部分来理解第二部分。</strong></p><h1 id="d194" class="ls lt jg bd lu lv lw lx ly lz ma mb mc km md kn me kp mf kq mg ks mh kt mi mj bi translated">第一部分:NLS回归背后的理论</h1><p id="659a" class="pw-post-body-paragraph kv kw jg kx b ky mk kh la lb ml kk ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">我们将遵循这些代表性惯例:</p><blockquote class="mp mq mr"><p id="8dfa" class="kv kw lr kx b ky kz kh la lb lc kk ld ms lf lg lh mt lj lk ll mu ln lo lp lq ij bi translated">“帽子”符号(^)将用于在数据上拟合回归模型的过程中生成的值。例如，<strong class="kx jh"> β_(hat) </strong>是<strong class="kx jh">拟合的</strong>系数的向量。</p><p id="a3e0" class="kv kw lr kx b ky kz kh la lb lc kk ld ms lf lg lh mt lj lk ll mu ln lo lp lq ij bi translated"><strong class="kx jh"><em class="jg">y _ OBS</em></strong><em class="jg"/>是因变量<strong class="kx jh"> y </strong>的观测值的向量。</p><p id="f72a" class="kv kw lr kx b ky kz kh la lb lc kk ld ms lf lg lh mt lj lk ll mu ln lo lp lq ij bi translated">普通样式的变量是定标器，粗体样式的变量代表它们的向量或矩阵等价物。例如，y_obs_i是包含大小为(m×1)的<strong class="kx jh"> y_obs </strong>向量的第I个观察值的定标器。</p><p id="14cd" class="kv kw lr kx b ky kz kh la lb lc kk ld ms lf lg lh mt lj lk ll mu ln lo lp lq ij bi translated">我们将假设回归矩阵<strong class="kx jh"> X </strong>的大小为(m x n ),即它有m个数据行，每行包含n个回归变量。y矩阵的大小为(m×1)，系数矩阵的大小为(m×1)(或者转置形式为1×m)</p></blockquote><p id="e672" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在让我们来看三个可以用NLS训练的非线性模型的例子。</p><p id="80a1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在下面的模型中，回归系数<em class="lr"> β_1 </em>和<em class="lr"> β_2 </em>是2和3的幂，因此不是线性的。</p><figure class="mw mx my mz gt is gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/4509497e494297f4048c0465a3f7adec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1212/format:webp/1*CcHgqTxh-K8pzwNdXaVWMw.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">系数非线性的模型(图片由<a class="ae jd" href="https://sachin-date.medium.com" rel="noopener">作者</a>提供)</p></figure><p id="7d4f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="lr"> e </em>为模型的残差，即观测值<em class="lr"> y </em>与预测值(即R.H.S. <em class="lr">上的<em class="lr"> β_0，β_1 </em>和<em class="lr"> β_2 </em>和<strong class="kx jh">T9】x</strong></em>的组合)</p><p id="d95e" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">以下模型是一个自回归时间序列模型，包含系数<em class="lr"> s β_1 </em>和<em class="lr"> β_2 </em>的乘法关系，因此本质上是非线性的。</p><figure class="mw mx my mz gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi na"><img src="../Images/8c4ef3da8d3549702e5fa831270e79fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jw-MfDjQKpin1N0HWiRjoQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">系数非线性的自回归时间序列模型(图片由<a class="ae jd" href="https://sachin-date.medium.com" rel="noopener">作者</a>提供)</p></figure><p id="a594" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在下面的模型中，预测值是回归变量<strong class="kx jh"> <em class="lr"> X </em> </strong>的线性组合的指数函数。</p><figure class="mw mx my mz gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nb"><img src="../Images/a983c90a70781ecd9cfd4ac82ca5a993.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TJPn6dFuOmUFbqqIhCIwJQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">指数均值模型(图片由<a class="ae jd" href="https://sachin-date.medium.com" rel="noopener">作者</a>提供)</p></figure><p id="ca2c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后一个公式通常用于<a class="ae jd" rel="noopener" target="_blank" href="/an-illustrated-guide-to-the-poisson-regression-model-50cccba15958"> <strong class="kx jh">泊松回归模型</strong> </a> <strong class="kx jh"> </strong>或其衍生模型，如<a class="ae jd" rel="noopener" target="_blank" href="/generalized-poisson-regression-for-real-world-datasets-d1ff32607d79">广义泊松模型</a>或<a class="ae jd" rel="noopener" target="_blank" href="/negative-binomial-regression-f99031bb25b4">负二项式回归模型</a>。具体而言，拟合均值<em class="lr"> _cap </em>被表示为泊松概率分布的条件均值，如下所示:</p><figure class="mw mx my mz gt is gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/845c2a86d4897ab5f02a74e169a5a7ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*cR6LIT522PbTW9dSbyIYqw.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">给定单位时间内看到y个事件的泊松概率<em class="nd">单位时间内_(cap)个事件的平均预测率，其中_(cap)是回归参数的函数</em><strong class="bd ne"><em class="nd"/></strong>为了简洁起见，我们去掉了_i下标(图片由<a class="ae jd" href="https://sachin-date.medium.com" rel="noopener">作者</a>提供)</p></figure><p id="f436" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这种泊松回归模型用于拟合基于计数的数据集，例如每天在自行车共享计划中租赁其中一辆自行车的人数。</p><h1 id="c3e2" class="ls lt jg bd lu lv lw lx ly lz ma mb mc km md kn me kp mf kq mg ks mh kt mi mj bi translated">NLS优化是如何工作的？</h1><p id="0a10" class="pw-post-body-paragraph kv kw jg kx b ky mk kh la lb ml kk ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">在NLS，我们的目标是寻找模型参数向量<strong class="kx jh"><em class="lr"/></strong>，它将<strong class="kx jh">最小化残差</strong>的平方和。换句话说，我们必须尽量减少以下情况:</p><figure class="mw mx my mz gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nf"><img src="../Images/ce9aa02db952f2b6974522b229562af5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kArsgECkqKeBIbNGl0lpcg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">拟合回归模型的残差平方和(图片由<a class="ae jd" href="https://sachin-date.medium.com" rel="noopener">作者</a>提供)</p></figure><p id="623b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="lr"> _cap_i </em>(模型对数据集中第<em class="lr">行</em>行的预测)是模型参数向量<strong class="kx jh"> <em class="lr"> β_cap </em> </strong>和回归变量<strong class="kx jh"> <em class="lr"> x_i </em> </strong>的函数，即:</p><figure class="mw mx my mz gt is gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/06b6df6085b4e9bc87cad4182243933d.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*dr4YbYLigF0p4BTsjvoJwA.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">由模型预测的给定<strong class="bd ne"><em class="nd"/></strong>x _ I的条件均值是拟合的<strong class="bd ne"> <em class="nd"> β和x_i </em> </strong>的函数(图片由<a class="ae jd" href="https://sachin-date.medium.com" rel="noopener">作者</a></p></figure><p id="40b1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">将<em class="lr"> _i </em>替换为<em class="lr">f(</em><strong class="kx jh"><em class="lr">β_ cap</em></strong><em class="lr">，</em><strong class="kx jh"><em class="lr">x _ I</em></strong><em class="lr">)</em>在前面的RSS等式中，我们有:</p><figure class="mw mx my mz gt is gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/d13a16d5bf70db7168b4f1717c3ef40a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*IWgbC6LevLsy_pUu6dIyHA.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">拟合回归模型的残差平方和(图片由<a class="ae jd" href="https://sachin-date.medium.com" rel="noopener">作者</a>提供)</p></figure><p id="b376" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">再次，<strong class="kx jh"> <em class="lr"> β_cap </em> </strong>是拟合 系数的<strong class="kx jh"> <em class="lr">向量，而<strong class="kx jh"> <em class="lr"> x_i </em> </strong>是回归变量矩阵<strong class="kx jh"> <em class="lr"> X </em> </strong>的第<em class="lr">与</em>行。</em></strong></p><p id="9c0b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最小化RSS的一种方法是相对于<strong class="kx jh"> <em class="lr"> β_cap </em> </strong>对RSS进行微分，然后将微分设置为零并求解<strong class="kx jh"> <em class="lr"> β_cap、</em> </strong>，即:</p><figure class="mw mx my mz gt is gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/72ce46864f9c8ddba52ec81d2701837d.png" data-original-src="https://miro.medium.com/v2/resize:fit:936/format:webp/1*be82ezSJSLykIpkhOAH1Lw.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">RSS w . r . t .<strong class="bd ne"><em class="nd">β_ cap</em></strong>的偏导数，并设置为零(图片由<a class="ae jd" href="https://sachin-date.medium.com" rel="noopener">作者</a>)</p></figure><p id="cd13" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">由于<strong class="kx jh"> <em class="lr"> β_cap </em> </strong>是对应于<em class="lr"> n </em>回归变量<em class="lr"> x1，x2，…xn </em>的长度为<em class="lr"> n </em>的向量，RSS需要对w.r.t .进行部分微分，这些<em class="lr"> β_cap_j </em>系数的每一个和每一个方程都设置为零。例如，RSS w.r.t. <em class="lr"> β_cap_1 </em>的偏导数如下:</p><figure class="mw mx my mz gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nj"><img src="../Images/46b209b41eeb848bfa6abd71f86dbcc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ONlxoQXb4JSVD9u_WFKtng.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">残差平方和w.r.t .系数的偏导数<em class="nd"> β_1 </em>(图片由<a class="ae jd" href="https://sachin-date.medium.com" rel="noopener">作者</a>提供)</p></figure><p id="7f92" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">由于有<em class="lr"> n </em>个系数<em class="lr"> β_cap_1到β_cap_n </em>，我们得到上面在<em class="lr"> n </em>个变量中所示的<em class="lr"> n </em>个方程。然而，相对于<a class="ae jd" rel="noopener" target="_blank" href="/assumptions-of-linear-regression-5d87c347140"><strong class="kx jh">【OLS】</strong></a>估计，这个<em class="lr"> n </em>方程组没有封闭形式的解。因此，我们必须使用迭代优化技术，在每次迭代<em class="lr"> k </em>时，我们对如下所示的<em class="lr"> β_cap_1至β_cap_n </em>的值进行小的调整，并重新评估RSS:</p><figure class="mw mx my mz gt is gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/3c5ff5dfd5d067c26d5e4a5b864e70e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*YQtBb1HN6euxPOzPdQaUGA.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">在第<em class="nd">k次</em>迭代时，将<em class="nd"> β_j增加</em> δ <em class="nd"> β_j </em></p></figure><p id="a6c3" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">已经设计了几种算法来有效地更新<strong class="kx jh"> <em class="lr"> β_cap </em> </strong>向量，直到达到将最小化RSS的一组最佳值。其中最主要的是基于<a class="ae jd" href="https://en.wikipedia.org/wiki/Trust_region" rel="noopener ugc nofollow" target="_blank">信任区域</a>的<a class="ae jd" href="https://optimization.mccormick.northwestern.edu/index.php/Trust-region_methods" rel="noopener ugc nofollow" target="_blank">方法</a>，例如<a class="ae jd" href="https://epubs.siam.org/doi/10.1137/S1064827595289108" rel="noopener ugc nofollow" target="_blank">信任区域反射</a>算法、<a class="ae jd" href="https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm" rel="noopener ugc nofollow" target="_blank">leven Berg-Marquardt算法</a>和被想象命名为<a class="ae jd" href="https://nmayorov.wordpress.com/2015/06/19/dogbox-algorithm/" rel="noopener ugc nofollow" target="_blank"> Dogbox算法</a>。SciPy 支持所有三种算法。</p><p id="05a3" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们回到前面介绍的指数均值模型。在这个模型中，我们有:</p><figure class="mw mx my mz gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nl"><img src="../Images/e94d59f28a39895c435e9a8a54ed5dd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5phbzHZVwH7OjW3KHG1oJg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">指数均值模型(图片由<a class="ae jd" href="https://sachin-date.medium.com" rel="noopener">作者</a>提供)</p></figure><p id="6ccb" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在RSS中替换:</p><figure class="mw mx my mz gt is gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/67a5d2002a247b7bcd656f75c6e8e3f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:952/format:webp/1*FxzPbXw7XB2_-3-8sD0QIg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">拟合泊松模型的剩余平方和(图片由<a class="ae jd" href="https://sachin-date.medium.com" rel="noopener">作者</a>提供)</p></figure><p id="423a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">注意，指数中的<strong class="kx jh"> <em class="lr"> x_i*β_cap </em> </strong>是两个维度为<em class="lr">【1 x n】</em>和<em class="lr">【n x 1】</em>的矩阵的矩阵乘法，因此结果是一个<em class="lr">【1x1】</em>矩阵，即有效的缩放器。</p><p id="f9ab" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对上面的等式w.r.t. <strong class="kx jh"> <em class="lr"> β_cap </em> </strong>进行微分，并将微分设置为零，我们得到下面的一组等式(以<strong class="kx jh">向量</strong>格式表示)，其需要使用上面提到的迭代优化算法之一来求解:</p><figure class="mw mx my mz gt is gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/7f6582217fd05dee3bde169f48a7fc73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*CrKqrk38cZvLlAgKLMneOw.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">该方程组的解产生泊松回归模型的拟合估计量<strong class="bd ne"> <em class="nd"> β_cap </em> </strong>(图片由<a class="ae jd" href="https://sachin-date.medium.com" rel="noopener">作者</a>提供)</p></figure></div><div class="ab cl no np hu nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="ij ik il im in"><h1 id="75ef" class="ls lt jg bd lu lv nv lx ly lz nw mb mc km nx kn me kp ny kq mg ks nz kt mi mj bi translated">第2部分:使用Python和SciPy的NLS回归教程</h1><p id="66d6" class="pw-post-body-paragraph kv kw jg kx b ky mk kh la lb ml kk ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">让我们使用<strong class="kx jh">非线性最小二乘技术</strong>将泊松回归模型拟合到一个为期两年的租赁自行车日常使用数据集。</p><p id="0898" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">数据集的前10行如下:</p><figure class="mw mx my mz gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oa"><img src="../Images/8831f8a73c72b5735770a07bd8d8afa7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9Db762JGYVvrA_pd.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">租赁自行车使用计数(来源:<a class="ae jd" href="https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset" rel="noopener ugc nofollow" target="_blank"> UCI机器学习库</a>)(图片由<a class="ae jd" href="https://sachin-date.medium.com/" rel="noopener">作者</a>)</p></figure><p id="0d54" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">你可以从这里下载数据集<a class="ae jd" href="https://gist.github.com/sachinsdate/413910079ab4ef4332e7a97cae55d13a" rel="noopener ugc nofollow" target="_blank"/>。</p><h2 id="1748" class="ob lt jg bd lu oc od dn ly oe of dp mc le og oh me li oi oj mg lm ok ol mi om bi translated">回归模型</h2><p id="af8f" class="pw-post-body-paragraph kv kw jg kx b ky mk kh la lb ml kk ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">我们将建立一个回归模型，其中<strong class="kx jh">因变量</strong> ( <strong class="kx jh"> <em class="lr"> y </em> </strong>)为:</p><p id="6fe1" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh"> total_user_count </strong>:自行车租赁者总数</p><p id="ac61" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">回归变量</strong>矩阵<strong class="kx jh"> <em class="lr"> X </em> </strong>将包含以下解释变量:</p><p id="a4f6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx jh">季节</strong>:当前天气季节<br/> <strong class="kx jh">年份</strong>:当前年份:0=2011年，1=2012年<br/> <strong class="kx jh">月</strong>:当前月份:1至12日<br/> <strong class="kx jh">假日</strong>:测量是否在假日进行(是=1，否=0) <br/> <strong class="kx jh">工作日</strong>:星期几(0至6) <br/> <strong class="kx jh">工作日<br/>2 =薄雾+多云，薄雾+碎云，薄雾+少云，薄雾。<br/>3 =小雪、小雨+雷雨+散云、小雨+散云。<br/>4 =暴雨+冰粒+雷雨+薄雾，雪+雾。<br/> <strong class="kx jh"> temp </strong>:温度，归一化到39C <br/> <strong class="kx jh"> atemp </strong>:真实感受，归一化到50C <br/> <strong class="kx jh">嗡嗡声</strong>:湿度，归一化到100 <br/> <strong class="kx jh">风速</strong>:风速，归一化到67</strong></p><p id="2612" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们导入所有需要的包</p><pre class="mw mx my mz gt on oo op oq aw or bi"><span id="ecf8" class="ob lt jg oo b gy os ot l ou ov"><strong class="oo jh">from </strong>scipy.optimize <strong class="oo jh">import </strong>least_squares<br/><strong class="oo jh">import </strong>pandas <strong class="oo jh">as </strong>pd<br/><strong class="oo jh">from </strong>patsy <strong class="oo jh">import </strong>dmatrices<br/><strong class="oo jh">import </strong>numpy <strong class="oo jh">as </strong>np<br/><strong class="oo jh">import </strong>statsmodels.api <strong class="oo jh">as </strong>sm<br/><strong class="oo jh">import </strong>statsmodels.formula.api <strong class="oo jh">as </strong>smf<br/><strong class="oo jh">import </strong>statsmodels.stats.stattools <strong class="oo jh">as </strong>st<br/><strong class="oo jh">import </strong>matplotlib.pyplot <strong class="oo jh">as </strong>plt</span></pre><p id="1897" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">将数据集读入熊猫数据帧:</p><pre class="mw mx my mz gt on oo op oq aw or bi"><span id="8236" class="ob lt jg oo b gy os ot l ou ov">df = pd.<strong class="oo jh">read_csv</strong>('bike_sharing_dataset_daywise.csv', <strong class="oo jh">header</strong>=0, <strong class="oo jh">parse_dates</strong>=[<strong class="oo jh">'</strong>dteday<strong class="oo jh">'</strong>], <strong class="oo jh">infer_datetime_format</strong>=True)</span></pre><p id="e229" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">创建训练和测试数据集。由于我们将该数据视为<a class="ae jd" href="https://en.wikipedia.org/wiki/Cross-sectional_data" rel="noopener ugc nofollow" target="_blank">横截面数据</a>，我们将随机选择90%的数据行作为我们的训练数据，剩余的10%作为我们的测试数据:</p><pre class="mw mx my mz gt on oo op oq aw or bi"><span id="b79f" class="ob lt jg oo b gy os ot l ou ov">mask = np.<strong class="oo jh">random</strong>.<strong class="oo jh">rand</strong>(<strong class="oo jh">len</strong>(df)) &lt; 0.9<br/>df_train = df[mask]<br/>df_test = df[~mask]<br/><strong class="oo jh">print</strong>('Training data set length=<strong class="oo jh">'</strong>+<strong class="oo jh">str</strong>(<strong class="oo jh">len</strong>(df_train)))<br/><strong class="oo jh">print</strong>(<strong class="oo jh">'</strong>Testing data set length=<strong class="oo jh">'</strong>+<strong class="oo jh">str</strong>(<strong class="oo jh">len</strong>(df_test)))</span></pre><p id="39d9" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">用<a class="ae jd" href="https://patsy.readthedocs.io/en/latest/quickstart.html" rel="noopener ugc nofollow" target="_blank"> Patsy </a>语法创建回归表达式。我们说<strong class="kx jh"> total_user_count </strong>是因变量，它取决于波浪号(~)右侧提到的所有变量:</p><p id="add6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">使用Patsy雕刻出<strong class="kx jh"> <em class="lr"> y </em> </strong>和<strong class="kx jh"> <em class="lr"> X </em> </strong>矩阵:</p><pre class="mw mx my mz gt on oo op oq aw or bi"><span id="0aed" class="ob lt jg oo b gy os ot l ou ov">y_train, X_train = <strong class="oo jh">dmatrices</strong>(expr, df_train, <strong class="oo jh">return_type</strong>=<strong class="oo jh">'</strong>dataframe<strong class="oo jh">'</strong>)<br/>y_test, X_test = <strong class="oo jh">dmatrices</strong>(expr, df_test, <strong class="oo jh">return_type</strong>=<strong class="oo jh">'</strong>dataframe<strong class="oo jh">'</strong>)</span></pre><p id="18bb" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们定义几个函数。我们将定义的第一个函数是计算指数平均值的函数:<strong class="kx jh"><em class="lr">mu _ cap</em></strong><em class="lr">= exp(</em><strong class="kx jh"><em class="lr">X *β_ cap):</em></strong></p><pre class="mw mx my mz gt on oo op oq aw or bi"><span id="2a9b" class="ob lt jg oo b gy os ot l ou ov"><strong class="oo jh">def </strong>calc_exponentiated_mean(beta, x):<br/>    lin_combi = np.<strong class="oo jh">matmul</strong>(np.<strong class="oo jh">array</strong>(x), np.<strong class="oo jh">array</strong>(beta))<br/>    mean = np.<strong class="oo jh">exp</strong>(lin_combi)<br/>    <strong class="oo jh">return </strong>mean</span></pre><p id="e353" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们将定义的第二个函数是在给定输入矩阵<strong class="kx jh"> <em class="lr"> β </em> </strong>、<strong class="kx jh"> <em class="lr"> X </em> </strong>和<strong class="kx jh"> <em class="lr"> y_obs </em> </strong>的情况下计算简单残差<em class="lr">r _ I =(y _ predicted—y _ OBS)</em>:</p><pre class="mw mx my mz gt on oo op oq aw or bi"><span id="e763" class="ob lt jg oo b gy os ot l ou ov"><strong class="oo jh">def </strong>calc_residual(beta, x, y_obs):<br/>    y_pred = <strong class="oo jh">calc_exponentiated_mean</strong>(beta, x)<br/>    r = np.<strong class="oo jh">subtract</strong>(y_pred, np.<strong class="oo jh">array</strong>(y_obs).<strong class="oo jh">flatten</strong>())<br/>    <strong class="oo jh">return </strong>r</span></pre><p id="7209" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">将系数<strong class="kx jh"> <em class="lr"> β </em> </strong>向量初始化为所有1.0值。X中有回归变量(数一数验证！)和回归截距，即总共两个变量。所以<strong class="kx jh"> <em class="lr"> β </em> </strong>大小为(<em class="lr"> 1 x 12)。</em>我们将构建的numpy向量将具有转置形状(12，)，这适合我们，因为我们必须将X_train与该向量相乘，并且X_train的形状为(661，12):</p><pre class="mw mx my mz gt on oo op oq aw or bi"><span id="2d4c" class="ob lt jg oo b gy os ot l ou ov">num_params = <strong class="oo jh">len</strong>(X_train.<strong class="oo jh">columns</strong>)<br/>beta_initial = np.<strong class="oo jh">ones</strong>(num_params)</span></pre><p id="6840" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">最后，是时候使用SciPy中的<em class="lr">最小平方()</em>方法在(y_train，X_train)上训练NLS回归模型，如下所示:</p><pre class="mw mx my mz gt on oo op oq aw or bi"><span id="a7da" class="ob lt jg oo b gy os ot l ou ov">result_nls_lm = <strong class="oo jh">least_squares</strong>(<strong class="oo jh">fun</strong>=calc_residual, <strong class="oo jh">x0</strong>=beta_initial, <strong class="oo jh">args</strong>=(X_train, y_train), <strong class="oo jh">method</strong>='lm', verbose=1)</span></pre><p id="5cd2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">注意，我们正在使用<a class="ae jd" href="https://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm" rel="noopener ugc nofollow" target="_blank">leven Berg–Marquardt算法</a> ( <strong class="kx jh"> <em class="lr">方法</em></strong><em class="lr">=‘lm’</em>)来执行<strong class="kx jh"><em class="lr"/></strong>向量的迭代优化。</p><p id="2668" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><code class="fe ow ox oy oo b">result_nls_lm.<strong class="kx jh">x</strong></code>变量包含拟合的<strong class="kx jh"> <em class="lr"> β </em> </strong>向量，换句话说，<strong class="kx jh"> <em class="lr"> β_cap </em> </strong>。</p><p id="7e8a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们将<strong class="kx jh"> <em class="lr"> β_cap </em> </strong>组织成一个熊猫数据帧并打印出这些值:</p><pre class="mw mx my mz gt on oo op oq aw or bi"><span id="183f" class="ob lt jg oo b gy os ot l ou ov">df_beta_cap = pd.<strong class="oo jh">DataFrame</strong>(<strong class="oo jh">data</strong>=res_lsq_lm.<strong class="oo jh">x</strong>.<strong class="oo jh">reshape</strong>(1,num_params), <strong class="oo jh">columns</strong>=X_train.<strong class="oo jh">columns</strong>)</span><span id="c30c" class="ob lt jg oo b gy oz ot l ou ov"><strong class="oo jh">print</strong>(df_beta_cap)</span></pre><p id="c95d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们看到以下输出显示了每个回归变量的拟合系数值和拟合回归截距:</p><figure class="mw mx my mz gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pa"><img src="../Images/a97ae6e9a19226f5b62b53d3920f217e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vXTZoEnD-4oEuCHq34D9kQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">每个回归变量的拟合系数值和拟合回归截距</p></figure><h1 id="453a" class="ls lt jg bd lu lv lw lx ly lz ma mb mc km md kn me kp mf kq mg ks mh kt mi mj bi translated">预言；预测；预告</h1><p id="8140" class="pw-post-body-paragraph kv kw jg kx b ky mk kh la lb ml kk ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">让我们看看我们的模型在我们之前雕刻出来的测试数据集<strong class="kx jh"> <em class="lr"> X_test </em> </strong>上表现如何。我们将使用之前定义的calc_exponentiated_mean()函数，但这一次，我们将传入拟合的<strong class="kx jh"> <em class="lr"> β_cap </em> </strong>向量和<strong class="kx jh"> <em class="lr"> X_test </em> </strong>:</p><pre class="mw mx my mz gt on oo op oq aw or bi"><span id="77ed" class="ob lt jg oo b gy os ot l ou ov">predicted_counts=<strong class="oo jh">calc_exponentiated_mean</strong>(<strong class="oo jh">beta</strong>=result_nls_lm.x, <strong class="oo jh">x</strong>=X_test)</span></pre><p id="1aad" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们绘制预测计数与实际计数的对比图:</p><pre class="mw mx my mz gt on oo op oq aw or bi"><span id="d95f" class="ob lt jg oo b gy os ot l ou ov">actual_counts = y_test[<strong class="oo jh">'registered_user_count'</strong>]</span><span id="dcf6" class="ob lt jg oo b gy oz ot l ou ov">fig = plt.<strong class="oo jh">figure</strong>()<br/>fig.<strong class="oo jh">suptitle</strong>(<strong class="oo jh">'Predicted versus actual user counts'</strong>)</span><span id="ab1d" class="ob lt jg oo b gy oz ot l ou ov">predicted, = plt.<strong class="oo jh">plot</strong>(X_test.<strong class="oo jh">index</strong>, predicted_counts, 'go-', <strong class="oo jh">label</strong>='Predicted counts')</span><span id="b63c" class="ob lt jg oo b gy oz ot l ou ov">actual, = plt.<strong class="oo jh">plot</strong>(X_test.<strong class="oo jh">index</strong>, actual_counts, <strong class="oo jh">'ro-',</strong> <strong class="oo jh">label</strong>='Actual counts')</span><span id="daa3" class="ob lt jg oo b gy oz ot l ou ov">plt.<strong class="oo jh">legend</strong>(<strong class="oo jh">handles</strong>=[predicted, actual])<br/>plt.<strong class="oo jh">show</strong>()</span></pre><figure class="mw mx my mz gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pb"><img src="../Images/3297d68df4076bd27d05809ed20b739d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lmr1ec2QqXNWaJylDBMP8Q.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">测试数据集上的预测自行车用户数与实际自行车用户数的对比图(图片由<a class="ae jd" href="https://sachin-date.medium.com" rel="noopener">作者</a>提供)</p></figure></div><div class="ab cl no np hu nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="ij ik il im in"><h1 id="3d44" class="ls lt jg bd lu lv nv lx ly lz nw mb mc km nx kn me kp ny kq mg ks nz kt mi mj bi translated">纸张、书籍和数据链接</h1><p id="be15" class="pw-post-body-paragraph kv kw jg kx b ky mk kh la lb ml kk ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">Cameron A. Colin，Trivedi Pravin K .，<a class="ae jd" href="http://cameron.econ.ucdavis.edu/racd/count.html" rel="noopener ugc nofollow" target="_blank"> <em class="lr">计数数据的回归分析</em> </a>，计量经济学学会专论№30，剑桥大学出版社，1998年。国际标准书号:0521635675</p><h2 id="318f" class="ob lt jg bd lu oc od dn ly oe of dp mc le og oh me li oi oj mg lm ok ol mi om bi translated">数据集</h2><p id="a531" class="pw-post-body-paragraph kv kw jg kx b ky mk kh la lb ml kk ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated"><em class="lr"> Fanaee-T，Hadi，和Gama，Joao，“结合集合检测器和背景知识的事件标记”，《人工智能进展》(2013):第1–15页，Springer Berlin Heidelberg，doi:10.1007/s 13748–013–0040–3。</em></p></div><div class="ab cl no np hu nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="ij ik il im in"><h1 id="bd48" class="ls lt jg bd lu lv nv lx ly lz nw mb mc km nx kn me kp ny kq mg ks nz kt mi mj bi translated">形象</h1><p id="cc52" class="pw-post-body-paragraph kv kw jg kx b ky mk kh la lb ml kk ld le mm lg lh li mn lk ll lm mo lo lp lq ij bi translated">本文中的所有图片的版权归<a class="ae jd" href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener ugc nofollow" target="_blank"> CC-BY-NC-SA </a>所有，除非图片下方提到了不同的来源和版权。</p></div><div class="ab cl no np hu nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="ij ik il im in"><h1 id="41fa" class="ls lt jg bd lu lv nv lx ly lz nw mb mc km nx kn me kp ny kq mg ks nz kt mi mj bi translated">相关文章</h1><div class="ip iq gp gr ir pc"><a rel="noopener follow" target="_blank" href="/an-illustrated-guide-to-the-poisson-regression-model-50cccba15958"><div class="pd ab fo"><div class="pe ab pf cl cj pg"><h2 class="bd jh gy z fp ph fr fs pi fu fw jf bi translated">泊松回归模型图解指南</h2><div class="pj l"><h3 class="bd b gy z fp ph fr fs pi fu fw dk translated">和使用Python的泊松回归教程</h3></div><div class="pk l"><p class="bd b dl z fp ph fr fs pi fu fw dk translated">towardsdatascience.com</p></div></div><div class="pl l"><div class="pm l pn po pp pl pq ix pc"/></div></div></a></div></div><div class="ab cl no np hu nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="ij ik il im in"><p id="948d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="lr">感谢阅读！如果您喜欢这篇文章，请</em> <a class="ae jd" href="https://timeseriesreasoning.medium.com" rel="noopener"> <strong class="kx jh"> <em class="lr">关注我</em> </strong> </a> <em class="lr">获取关于回归和时间序列分析的提示、操作方法和编程建议。</em></p></div></div>    
</body>
</html>