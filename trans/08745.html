<html>
<head>
<title>From Prediction to Action — How to Learn Optimal Policies From Data (4/4)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从预测到行动——如何从数据中学习最佳策略(4/4)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/from-prediction-to-action-how-to-learn-optimal-policies-from-data-4-4-14c63cc0c938?source=collection_archive---------24-----------------------#2021-08-12">https://towardsdatascience.com/from-prediction-to-action-how-to-learn-optimal-policies-from-data-4-4-14c63cc0c938?source=collection_archive---------24-----------------------#2021-08-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="850a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">在这个系列的最后一篇文章中，我们将学习如何找到一个最优策略。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/2653478a8604e630f770d009b5ab944e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rTt6pdn_VIqjYCsT"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae kv" href="https://unsplash.com/@madebyjens?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">延斯·勒列</a>拍摄</p></figure><p id="b828" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">概述:</p><ul class=""><li id="4df8" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">在<a class="ae kv" rel="noopener" target="_blank" href="/from-prediction-to-action-how-to-learn-optimal-policies-from-data-part-1-1edbfdcb725d">第一部分</a>中，我们讨论了从数据中学习<em class="mb">最优</em>策略的必要性。政策优化涵盖了广泛的实际情况，我们简要地看了医疗保健、防止流失、目标营销和市政府的例子。</li><li id="def1" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr lx ly lz ma bi translated">在<a class="ae kv" rel="noopener" target="_blank" href="/from-making-predictions-to-optimizing-actions-an-introduction-to-policy-learning-2-4-9fc46ba8f3d0">第2部分</a>中，我们描述了如何创建一个数据集，使其适合策略优化。</li><li id="21f6" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr lx ly lz ma bi translated">在第3部分的<a class="ae kv" rel="noopener" target="_blank" href="/from-prediction-to-action-how-to-learn-optimal-policies-from-data-3-4-aa56c974a505">中，我们提出了一种简单的方法来使用这样一个数据集来估计<strong class="ky ir"> <em class="mb">任何</em> </strong>策略的结果。</a></li></ul></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><p id="cc26" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，值得注意的是，许多领域的研究人员——因果推理、计量经济学和上下文强盗——已经研究这个问题多年，并设计了许多算法来解决它——双重鲁棒方法、直接方法、转换结果方法、双重机器学习、X-Learner、R-Learner等等。看看最近的<a class="ae kv" href="http://arxiv.org/abs/2007.12769" rel="noopener ugc nofollow" target="_blank">调查</a>的第3部分，了解我的意思。</p><p id="5a69" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">面对如此丰富的财富，我面临着在这篇文章中关注什么的难题。但幸运的是，对这一领域做出重大贡献的研究团队之一在2018年组织了“<a class="ae kv" href="http://arxiv.org/abs/1802.04064" rel="noopener ugc nofollow" target="_blank">一场上下文相关的强盗烘烤大赛</a>”，其中许多关键算法相互竞争。</p><p id="9ca3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一种叫做<strong class="ky ir"> <em class="mb">重要性加权回归</em></strong><em class="mb">【IWR】</em>的相对简单的方法在竞争中表现出色，我在下面描述它。IWR直接利用我们关于如何构建回归模型的知识，易于理解，并且可以使用您选择的任何监督学习回归方法轻松实现(如果您对底层理论感兴趣，请参见<a class="ae kv" href="http://arxiv.org/abs/1803.01088" rel="noopener ugc nofollow" target="_blank">使用回归神谕的实用上下文强盗</a>)。</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><p id="766e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在<a class="ae kv" rel="noopener" target="_blank" href="/from-making-predictions-to-optimizing-actions-an-introduction-to-policy-learning-2-4-9fc46ba8f3d0">第2部分</a>中，我们组装了这样一个数据集:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mo"><img src="../Images/5bd742d402456faa8ba9e7663ee05cdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RSeDlUQhfoY-I8jFfbiMBQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="f289" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">或者，更具体地说，让我们以网飞流失为例，采取三个行动:提供20%的折扣，在另外两个设备上提供免费流媒体，什么也不做。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/a8f152bee3af2636a7f8be7146f43537.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s-WccbE66OaokAK3QqGuXw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="26b9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">我们的目标是从这个数据集中学习一个函数，为每个客户分配最佳行动，也就是说，我们希望学习一个最佳策略。</strong></p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><p id="4242" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用重要性加权回归(IWR)学习最优策略是一个三步过程。</p><p id="db41" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">步骤1 </strong>:根据分配的<em class="mb">实际</em>动作拆分上面的数据集。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/d5e2094b46ae475de4f37e061d62123c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7Tdzk7h9l1pXWeCf0MHc7Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">每个动作的数据集(图片由作者提供)</p></figure><p id="9a88" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，您将拥有与操作数量一样多的数据集。</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><p id="cedb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">步骤2 </strong>:使用<em class="mb">每个</em>数据集，构建一个回归模型，该模型根据客户特征向量<em class="mb"> x. </em>预测结果</p><p id="7121" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="mb">至关重要的是，你需要使用加权平方损失作为损失函数，其中权重是概率列中数字的倒数。</em> </strong></p><p id="f87e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">回归模型通常默认最小化平方损失，所以这很标准。你要做的<em class="mb">额外的</em>事情是为训练集中的每个数据点发送一个权重。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/ba3cf44e2675340ccbc39fbaf6b18ac4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2efx7kLFPXKOY8giyNJ5uQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="8263" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们用代数的方法写下来，这样就不会有歧义了。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/74bab4bc095050bdd4ce772e92faabe1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ba7SH3sUIW4pEqIqpK2qqw.png"/></div></div></figure><p id="6990" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就是这样。</p><p id="1e87" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">(顺便说一句，加权回归通常是现成的。在<a class="ae kv" href="https://scikit-learn.org/stable/index.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>中，例如<a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html" rel="noopener ugc nofollow" target="_blank">线性回归</a>、<a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html" rel="noopener ugc nofollow" target="_blank">梯度推进回归</a>和<a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html" rel="noopener ugc nofollow" target="_blank">随机森林回归</a>都以简单统一的方式支持权重，使用<strong class="ky ir"> fit </strong>函数的<em class="mb"> sample_weight </em>参数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/982361b65e213359ad2db11b45114d83.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*5OePYO90TJ6fwYRrWVi3Zg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来自<a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . linear _ model。LinearRegression.html</a></p></figure><p id="57b7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">调用<strong class="ky ir">拟合</strong>函数时，只需将<em class="mb">样本权重</em>设置为概率栏中数字的倒数)</p><p id="7598" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这一步结束时，您将已经构建了与操作一样多的回归模型。</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><p id="c045" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">第三步</strong>:对于任何一个客户特征向量<em class="mb"> x </em>，运行<em class="mb"> x </em>到<em class="mb">T7】每个模型，得到其行动的预测结果。</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mu"><img src="../Images/1e87571fa2387ce162541878ac3790cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TH0dYaMPmlFaPmlQHtuzEA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="184f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">现在，选择预测结果最高的行动。</strong></p><p id="39a4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">你学会了一个最优策略！</strong></p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><p id="9c51" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如你在上面看到的，IWR唯一的新东西(与传统回归相比)是使用概率的倒数作为权重。</p><p id="f0ec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用概率背后的直觉是什么？</p><p id="c347" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了简单起见，假设您的客户特征向量<em class="mb"> x </em>只有一个元素:它是一个二元变量，如果有问题的客户曾经看过关于网飞的纪录片，则为1，如果没有，则为0。</p><p id="bea3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在与业务利益相关者协商后，假设您提出了以下<em class="mb">数据收集政策</em>(在<a class="ae kv" rel="noopener" target="_blank" href="/from-making-predictions-to-optimizing-actions-an-introduction-to-policy-learning-2-4-9fc46ba8f3d0">第2部分</a>中定义):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mv"><img src="../Images/6b4d99698da576098e12a258f1a3d997.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mgHuLBFLceYnwb0sjURMBQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">数据收集政策(图片由作者提供)</p></figure><p id="91b2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们来关注一下“八折”这一栏。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mw"><img src="../Images/dcca0c89c67c36f6e93f8a0654f2efe4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*66YZ3yfn_bp8ylDcb1AYsw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="bb86" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设我们选择随机抽样的<em class="mb"> N </em>位客户进行数据收集，而跟单观察人代表了样本的一小部分<em class="mb"> p </em>，那么我们样本中大约<em class="mb"> Np </em>位客户将是跟单观察人，而<em class="mb"> N(1-p) </em>位客户则不是。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/b730cea83d1229b00167df1252bd5b77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1390/format:webp/1*XyGENqT0Kq8-LSMDe8EgkA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="9c05" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有鉴于此,“八折”数据集中的客户数量大约为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi my"><img src="../Images/0f5354d020cd2f2c90e44d24b6e77691.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tpbppMbQKTn3GO6WABkLhw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="81ab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">(即，在我们样本中观看纪录片的<em class="mb"> Np </em>客户中，根据我们的数据收集政策，其中50%将被分配“八折”，因此我们得到<em class="mb"> N </em> x <em class="mb"> p </em> x 0.5)</p><p id="dc60" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是我们现在有一个问题。“八折”列中x =1和x=0客户的组合不同于样本中x =1和x=0客户的总体组合。如果我们用这个倾斜的数据集建立一个回归模型，它对“20%折扣”行动结果的预测可能会有偏差。</p><p id="93b4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">更一般地说，在您计划在整个客户群中使用的模型中，您用来构建模型的数据点应该有<em class="mb"> x </em>以与在您的客户群中相同的方式分布。</p><p id="477e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">回到我们的例子，我们如何解决这个问题？</p><p id="1d5f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一种方法是“<em class="mb">制作点数的副本</em>，使得“20%折扣”栏中x=1和x=0点数的组合与样本中x =1和x=0客户的总体组合相匹配。</p><p id="7cb3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设我们对x=0点使用乘数<em class="mb"> u </em>，对x=1点使用乘数<em class="mb"> v </em>。乘法(即复制)完成后，新的点数显示在下面最右边的列中。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mz"><img src="../Images/57ed204e2feebe9623164965b5a4f410.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dWrOmapBuATI_lm8MAfJug.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="d4ef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们应该如何选择<em class="mb"> u </em>和<em class="mb"> v </em>使得“八折”栏中调整后的客户数量与总体样本中的数量相匹配？</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi na"><img src="../Images/d30e1b0f905870b6cbcdf37f5787b216.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WNhIO7LvlgWOSVjQS7Ygdw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="e618" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">简单。只需将乘数设置为概率的<strong class="ky ir">倒数！</strong></p><p id="2e2a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">设置<em class="mb"> u </em> = 1/0.5和<em class="mb"> v </em> = 1/0.2。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nb"><img src="../Images/74b88dd2e49c372b5b3ce7b6692f0dc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_pCWcOsa5heOQij2wLJoJA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="d6cc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">乘数和概率完美抵消，调整后的数字与总体数字相匹配！</strong></p><p id="dacd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">(实际上，我们不会复制数据点。相反，我们简单地将概率的倒数作为权重插入损失函数，并且它具有相同的效果)</p><p id="ef23" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">总之，在回归中使用概率的倒数作为权重，消除了由我们的数据收集策略引起的数据点的偏斜。</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><p id="e909" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果这个概率倒数的生意让你想起了第三部分中的霍维茨-汤普森估值器是如何工作的，你的直觉是正确的！事实上，我们可以用类似的论证来证明IWR是可行的。如果你很好奇，一个代数证明在帖子的底部。</p><p id="55ef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">(顺便说一句，这种使用概率倒数进行调整的想法是一种非常普遍和强大的技术的简单例子，这种技术被称为<a class="ae kv" href="https://en.wikipedia.org/wiki/Importance_sampling" rel="noopener ugc nofollow" target="_blank">重要性抽样</a>，被广泛用于许多领域)</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><p id="8b5a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你已经学习了使用IWR的最优策略，并准备将你的最优策略付诸实施。</p><p id="5025" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我建议在发布前做最后一点调整。</p><p id="415d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">世界在不断变化，在您的最佳策略中捕获和体现的客户行为模式也会随着时间而变化。<em class="mb">今天是最优的政策在下个季度可能不是最优的</em>。</p><p id="173f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当然，您可以使用数据收集策略定期收集新数据，并利用这些数据学习新的最佳策略。但是有更好的方法。</p><p id="61cb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">给你的最优策略添加一点随机化，然后启动</strong>(参见<a class="ae kv" rel="noopener" target="_blank" href="/from-making-predictions-to-optimizing-actions-an-introduction-to-policy-learning-2-4-9fc46ba8f3d0">第二部分</a>中关于<em class="mb">ε-贪婪</em>的讨论，提醒你如何做)。</p><p id="270a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过这样做，您的最佳策略可以兼作数据收集策略。偶尔，你可以从中获取数据，使用IWR重新学习一个最优策略，然后切换到那个策略(但是，同样，当你启动它时，你会添加一点随机化)。</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><p id="e826" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章中，我们将策略优化描述为一个<strong class="ky ir">批处理</strong>周期:运行一段时间的数据收集策略，对收集的数据使用IWR来学习最优策略，部署添加了一点随机化的最优策略，并且<strong class="ky ir">经常重复</strong>。</p><p id="6ae3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但这可以在<strong class="ky ir">实时</strong>中完成。在极端的情况下，我们可以在<em class="mb">每次交互之后(即，在观察到每个x - &gt;动作- &gt;结果之后)</em>通过递增地更新对应于被分配的动作的IWR模型来这样做。更多信息见<a class="ae kv" href="https://vowpalwabbit.org/docs/vowpal_wabbit/python/latest/tutorials/python_Contextual_bandits_and_Vowpal_Wabbit.html" rel="noopener ugc nofollow" target="_blank">此处</a>。</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><p id="006e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们结束之前，让我们考虑一个非常重要的特例:<em class="mb">当我们只有</em> <strong class="ky ir"> <em class="mb">两个</em> </strong> <em class="mb">动作可以从</em>中选择的时候。通常，其中一个是<em class="mb">主动</em>的事情(例如，给病人X药，给顾客打折)，另一个是<em class="mb">被动</em>的事情(例如，给安慰剂，什么都不做)。</p><p id="4285" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">特别是在因果推断/计量经济学中，两个行动(或两个治疗)的场景非常常见，因为你经常想要估计一个<em class="mb">干预</em>(又名行动或治疗)<em class="mb">与不干预</em>相比对现有系统的影响。</p><p id="85b2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这种特殊情况下，你实际上可以通过一个直接预测两个行动之间结果差异的模型来学习最优策略。在某些情况下，这可能比学习两个不同的模型更有效。要了解更多信息，请参见最近的<a class="ae kv" href="http://arxiv.org/abs/2007.12769" rel="noopener ugc nofollow" target="_blank">调查</a>的第3部分，并遵循其中的参考资料。</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><p id="fa85" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这个由4部分组成的系列文章中，我试图从大量快速增长的研究中提取政策优化的最实用的元素。有大量非常有趣的材料——如果你想了解更多，这里有一些建议。</p><ul class=""><li id="aa1c" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">IWR在2018年才首次被描述(<a class="ae kv" href="http://arxiv.org/abs/1803.01088" rel="noopener ugc nofollow" target="_blank">参考文献</a>)，如果我试图在2018年之前写这篇文章，我可能会选择<a class="ae kv" href="https://projecteuclid.org/journals/statistical-science/volume-29/issue-4/Doubly-Robust-Policy-Evaluation-and-Optimization/10.1214/14-STS500.full" rel="noopener ugc nofollow" target="_blank">双稳健方法</a>。更一般地说，除了IWR还有很多其他选择，如果你感兴趣，就从最近的这个<a class="ae kv" href="http://arxiv.org/abs/2007.12769" rel="noopener ugc nofollow" target="_blank">调查</a>开始，去你的好奇心想去的地方吧。</li><li id="469f" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr lx ly lz ma bi translated">如果您对用于策略优化的库/模块感兴趣，这里有一个示例:<a class="ae kv" href="https://vowpalwabbit.org/index.html" rel="noopener ugc nofollow" target="_blank"> Vowpal Wabbit </a>、<a class="ae kv" href="https://causalml.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank"> CausalML </a>、<a class="ae kv" href="https://www.microsoft.com/en-us/research/project/econml/" rel="noopener ugc nofollow" target="_blank"> EconML </a>、<a class="ae kv" href="https://www.aboutwayfair.com/data-science/2018/10/pylift-a-fast-python-package-for-uplift-modeling/" rel="noopener ugc nofollow" target="_blank"> PyLift </a>和<a class="ae kv" href="https://cran.r-project.org/web/packages/uplift/uplift.pdf" rel="noopener ugc nofollow" target="_blank"> Uplift </a>。特别是，本文中描述的<a class="ae kv" href="https://vowpalwabbit.org/docs/vowpal_wabbit/python/latest/tutorials/python_Contextual_bandits_and_Vowpal_Wabbit.html" rel="noopener ugc nofollow" target="_blank"> IWR方法是在Vowpal Wabbit库</a>中实现的。</li><li id="37fa" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr lx ly lz ma bi translated">如果您对商业可用的政策优化产品/服务感兴趣，这里有一个<a class="ae kv" href="https://azure.microsoft.com/en-us/services/cognitive-services/personalizer/" rel="noopener ugc nofollow" target="_blank">示例</a>。</li></ul><p id="92c6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">快乐学习！</p></div><div class="ab cl mh mi hu mj" role="separator"><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm mn"/><span class="mk bw bk ml mm"/></div><div class="ij ik il im in"><p id="91ad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">可选</strong></p><p id="b260" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">出于好奇，这里有一个IWR如何工作的代数证明。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/dfe410943e986375ae1bedd2bf34393d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8ab2cr5QzGQx5_3Ilc258w.jpeg"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nb"><img src="../Images/da723352bdeed2f4b823f66090881950.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WbafCB2PW0x6chNDP6x_0g.jpeg"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/707c203cba29edbd55cfda306eace64f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aYtEAddzg2IkiXmLmD6NZA.jpeg"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/91668bfca5f60304b84a473e6ce199cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bDEkcsh2CGVeqzkQq-nsvQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure></div></div>    
</body>
</html>