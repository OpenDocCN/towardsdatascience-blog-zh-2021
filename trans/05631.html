<html>
<head>
<title>Neural Machine Translation using a Seq2Seq Architecture and Attention (ENG to POR)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Seq2Seq架构和注意力的神经机器翻译</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-machine-translation-using-a-seq2seq-architecture-and-attention-eng-to-por-fe3cc4191175?source=collection_archive---------23-----------------------#2021-05-19">https://towardsdatascience.com/neural-machine-translation-using-a-seq2seq-architecture-and-attention-eng-to-por-fe3cc4191175?source=collection_archive---------23-----------------------#2021-05-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="d784" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="37d5" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">基于Tensorflow和Keras的深度学习应用</h2></div><h1 id="9789" class="kr ks it bd kt ku kv kw kx ky kz la lb ki lc kj ld kl le km lf ko lg kp lh li bi translated">1.介绍</h1><p id="c06f" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">神经机器翻译(NMT)是一种用于自动翻译的端到端学习方法<a class="ae mf" href="https://arxiv.org/abs/1609.08144" rel="noopener ugc nofollow" target="_blank">【1】</a>。它的优势在于它直接学习从输入文本到相关输出文本的映射。它已被证明比传统的基于短语的机器翻译更有效，传统的基于短语的机器翻译需要更多的努力来设计模型。另一方面，NMT模型的训练成本很高，尤其是在大规模翻译数据集上。由于使用了大量的参数，它们在推理时也要慢得多。其他限制是它在翻译罕见单词和无法翻译输入句子的所有部分时的鲁棒性。为了克服这些问题，已经有一些解决方案，比如利用注意机制复制生僻字<a class="ae mf" href="https://arxiv.org/abs/1412.2007" rel="noopener ugc nofollow" target="_blank">【2】</a>。</p><p id="2c76" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">通常，NMT模型遵循通用的序列到序列学习架构。它由一个编码器和一个解码器递归神经网络(RNN)组成(关于如何设置RNN的简单示例，请参见<a class="ae mf" rel="noopener" target="_blank" href="/generating-text-with-recurrent-neural-networks-based-on-the-work-of-f-pessoa-1e804d88692d">【3】</a>)。编码器将输入句子转换成向量列表，每个输入一个向量。给定这个列表，解码器一次产生一个输出，直到产生特殊的句子结束标记。</p><p id="6765" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">我们的任务是使用中等规模的示例对语料库，为英语输入句子生成葡萄牙语翻译。我们使用序列到序列架构来构建我们的NMT模型。对于编码器RNN，我们使用一种预训练的嵌入，一种在英文Google News 200B语料库<a class="ae mf" href="https://tfhub.dev/google/tf2-preview/nnlm-en-dim128" rel="noopener ugc nofollow" target="_blank">【4】</a>上训练的基于令牌的文本嵌入。另一方面，我们在解码器RNN中训练自己的嵌入，词汇量设置为语料库中唯一葡萄牙语单词的数量。由于模型的复杂架构，我们实现了一个定制的训练循环来训练我们的模型。</p><p id="6ca0" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">本文使用了一个由170，305对英语和葡萄牙语句子组成的数据集<a class="ae mf" href="https://www.kaggle.com/luisroque/engpor-sentence-pairs" rel="noopener ugc nofollow" target="_blank">【5】</a>。这些数据来自Tatoeba.org，一个由志愿者翻译成多种语言的大型例句数据库。</p><p id="e650" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">和往常一样，代码和数据也可以在我的<a class="ae mf" href="https://github.com/luisroque/deep-learning-articles" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上获得。</p><p id="1b27" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">本文属于使用TensorFlow进行深度学习的系列文章:</p><ul class=""><li id="bb48" class="ml mm it ll b lm mg lp mh ls mn lw mo ma mp me mq mr ms mt bi translated"><a class="ae mf" rel="noopener" target="_blank" href="/transfer-learning-and-data-augmentation-applied-to-the-simpsons-image-dataset-e292716fbd43">迁移学习和数据增强应用于辛普森图像数据集</a></li><li id="171e" class="ml mm it ll b lm mu lp mv ls mw lw mx ma my me mq mr ms mt bi translated"><a class="ae mf" rel="noopener" target="_blank" href="/generating-text-with-recurrent-neural-networks-based-on-the-work-of-f-pessoa-1e804d88692d">基于F. Pessoa的工作用递归神经网络生成文本</a></li><li id="3a1a" class="ml mm it ll b lm mu lp mv ls mw lw mx ma my me mq mr ms mt bi translated"><a class="ae mf" rel="noopener" target="_blank" href="/neural-machine-translation-using-a-seq2seq-architecture-and-attention-eng-to-por-fe3cc4191175">使用Seq2Seq架构和注意力的神经机器翻译(ENG to POR) </a></li><li id="be73" class="ml mm it ll b lm mu lp mv ls mw lw mx ma my me mq mr ms mt bi translated"><a class="ae mf" rel="noopener" target="_blank" href="/residual-networks-in-computer-vision-ee118d3be68f">残差网络从无到有应用于计算机视觉</a></li></ul><h1 id="61ff" class="kr ks it bd kt ku kv kw kx ky kz la lb ki lc kj ld kl le km lf ko lg kp lh li bi translated">2.预处理</h1><p id="68fb" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">我们首先在葡萄牙语的每个句子中添加两个特殊的记号，一个<code class="fe mz na nb nc b">&lt;start&gt;</code>和<code class="fe mz na nb nc b">&lt;end&gt;</code>记号。它们被用来向解码RNN发送一个句子的开始和结束的信号。接下来，我们标记葡萄牙语句子，并用零填充句子的结尾。</p><pre class="nd ne nf ng gt nh nc ni nj aw nk bi"><span id="ec0c" class="nl ks it nc b gy nm nn l no np">import pandas as pd<br/>import numpy as np<br/>import tensorflow as tf<br/>import tensorflow_hub as hub<br/>from sklearn.model_selection import train_test_split<br/>from tensorflow.keras.layers import Layer<br/>import matplotlib.pyplot as plt</span><span id="4530" class="nl ks it nc b gy nq nn l no np">NUM_EXAMPLES = <!-- -->100000<br/>data_examples = []<br/>with open('por.txt', 'r', encoding='utf8') as f:<br/>    for line in f.readlines():<br/>        if len(data_examples) &lt; NUM_EXAMPLES:<br/>            data_examples.append(line)<br/>        else:<br/>            break<br/><br/>df = pd.DataFrame(data_examples, columns=['all'])<br/>df = df['all'].str.split('\t', expand=True)<br/><br/>df.columns = columns=['english', 'portuguese', 'rest']<br/><br/>df = df[['english', 'portuguese']]<br/><br/>df['portuguese'] = df.apply(lambda row: "&lt;start&gt; " + row['portuguese'] + " &lt;end&gt;", axis=1)<br/><br/>tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')<br/><br/>tokenizer.fit_on_texts(df['portuguese'].to_list())<br/><br/>df['portuguese_tokens'] = df.apply(lambda row: <br/>                        tokenizer.texts_to_sequences([row['portuguese']])[0], axis=1)<br/><br/># Print 5 examples from each language<br/><br/>idx = np.random.choice(df.shape[0],5)<br/><br/>for i in idx:<br/>    print('English')<br/>    print(df['english'][i])<br/>    <br/>    print('\nportuguese')<br/>    print(df['portuguese'][i])<br/>    print(df['portuguese_tokens'][i])<br/>    print('\n----\n')<br/><br/>portuguese_tokens = tf.keras.preprocessing.sequence.pad_sequences(df['portuguese_tokens'].to_list(),<br/>                                                 padding='post',<br/>                                                 value=0)</span><span id="a637" class="nl ks it nc b gy nq nn l no np">English<br/>Have you ever taken singing lessons?<br/><br/>portuguese<br/>&lt;start&gt; Você já fez aulas de canto? &lt;end&gt;<br/>[1, 10, 60, 100, 1237, 8, 19950, 2]<br/><br/>----<br/><br/>English<br/>Tom rode his bicycle to the beach last weekend.<br/><br/>portuguese<br/>&lt;start&gt; O Tom foi à praia de bicicleta no último fim de semana. &lt;end&gt;<br/>[1, 5, 3, 34, 61, 1243, 8, 696, 28, 869, 551, 8, 364, 2]<br/><br/>----<br/><br/>English<br/>I have to go to sleep.<br/><br/>portuguese<br/>&lt;start&gt; Tenho que ir dormir. &lt;end&gt;<br/>[1, 45, 4, 70, 616, 2]<br/><br/>----<br/><br/>English<br/>I have already told Tom that Mary isn't here.<br/><br/>portuguese<br/>&lt;start&gt; Eu já disse a Tom que Maria não está aqui. &lt;end&gt;<br/>[1, 7, 60, 50, 9, 3, 4, 106, 6, 13, 88, 2]<br/><br/>----<br/><br/>English<br/>If Tom was hurt, I'd know it.<br/><br/>portuguese<br/>&lt;start&gt; Se Tom estivesse ferido, eu o saberia. &lt;end&gt;<br/>[1, 21, 3, 602, 10016, 7, 5, 16438, 2]<br/><br/>----</span></pre><h2 id="72c4" class="nl ks it bd kt nr ns dn kx nt nu dp lb ls nv nw ld lw nx ny lf ma nz oa lh iz bi translated">2.1预训练嵌入层</h2><p id="3945" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">对于编码器和解码器rnn，我们需要定义嵌入层来将我们的单词索引转换成固定大小的密集向量。对于解码器RNN，我们训练自己的嵌入。对于编码器RNN，我们使用来自Tensorflow Hub的预训练英语单词embedding。这是一个基于标记的文本嵌入，在英文谷歌新闻200B语料库上训练。它允许我们利用在非常大的语料库上训练的单词表示，遵循迁移学习的原则(参见<a class="ae mf" rel="noopener" target="_blank" href="/transfer-learning-and-data-augmentation-applied-to-the-simpsons-image-dataset-e292716fbd43">【6】</a>的扩展定义和对计算机视觉的迁移学习应用)。在把英语句子输入RNN之前，我们对它们进行了填充。</p><pre class="nd ne nf ng gt nh nc ni nj aw nk bi"><span id="0a97" class="nl ks it nc b gy nm nn l no np"># Load embedding module from Tensorflow Hub<br/><br/>embedding_layer = hub.KerasLayer("https://tfhub.dev/google/tf2-preview/nnlm-en-dim128/1", <br/>                                 output_shape=[128], input_shape=[], dtype=tf.string)</span><span id="5596" class="nl ks it nc b gy nq nn l no np">maxlen = 13</span><span id="08cc" class="nl ks it nc b gy nq nn l no np">def split_english(dataset):<br/>    dataset = dataset.map(lambda x, y: (tf.strings.split(x, sep=' '), y))<br/>    return dataset<br/><br/>def filter_func(x,y):<br/>    return (tf.math.less_equal(len(x),maxlen)) <br/><br/>def map_maxlen(x, y):<br/>    paddings = [[maxlen - tf.shape(x)[0], 0], [0, 0]]<br/>    x = tf.pad(x, paddings, 'CONSTANT', constant_values=0)<br/>    return (x, y)<br/><br/>def embed_english(dataset):<br/>    dataset = dataset.map(lambda x, y: (embedding_layer(x), y))<br/>    return dataset</span><span id="4939" class="nl ks it nc b gy nq nn l no np">english_strings = df['english'].to_numpy()<br/>english_string_train, english_string_valid, portuguese_token_train, portuguese_token_valid = train_test_split(english_strings, portuguese_tokens, train_size=0.8)<br/><br/>dataset_train = (embed_english(<br/>                        split_english(<br/>                            tf.data.Dataset.from_tensor_slices((english_string_train, portuguese_token_train)))<br/>                                                                .filter(filter_func))<br/>                                                                .map(map_maxlen)<br/>                                                                .batch(16))<br/>dataset_valid = (embed_english(<br/>                        split_english(<br/>                            tf.data.Dataset.from_tensor_slices((english_string_valid, portuguese_token_valid)))<br/>                                                                .filter(filter_func))<br/>                                                                .map(map_maxlen)<br/>                                                                .batch(16))<br/><br/>dataset_train.element_spec</span><span id="d327" class="nl ks it nc b gy nq nn l no np">(TensorSpec(shape=(None, None, 128), dtype=tf.float32, name=None),<br/> TensorSpec(shape=(None, 35), dtype=tf.int32, name=None))</span><span id="cd7d" class="nl ks it nc b gy nq nn l no np"># shape of the English data example from the training Dataset<br/>list(dataset_train.take(1).as_numpy_iterator())[0][0].shape</span><span id="a150" class="nl ks it nc b gy nq nn l no np">(16, 13, 128)</span><span id="3a84" class="nl ks it nc b gy nq nn l no np"># shape of the portuguese data example Tensor from the validation Dataset<br/>list(dataset_valid.take(1).as_numpy_iterator())[0][1].shape</span><span id="621c" class="nl ks it nc b gy nq nn l no np">(16, 35)</span></pre><h1 id="7ecd" class="kr ks it bd kt ku kv kw kx ky kz la lb ki lc kj ld kl le km lf ko lg kp lh li bi translated">3.编码器递归神经网络</h1><p id="839e" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">编码器网络是一个RNN，其工作是将一系列向量<strong class="ll jd"> x </strong> =(x_1，…，x_T)读入向量<em class="ob"> c </em>，这样:</p><figure class="nd ne nf ng gt od gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/310a5baf0537ce27ab9997768500ff93.png" data-original-src="https://miro.medium.com/v2/resize:fit:370/format:webp/1*IpQ-4760XVfceU7iFkjayQ.png"/></div></figure><p id="5d4c" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">其中<em class="ob"> h_t </em>为时间<em class="ob"> t </em>的隐藏状态，<em class="ob"> c </em>为隐藏状态序列生成的向量，<em class="ob"> f </em>和<em class="ob"> q </em>为非线性函数。</p><p id="72a3" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">在定义我们的编码器网络之前，我们引入了一个学习英语语料库的结束标记的128维表示(嵌入空间的大小)的层。因此，RNN的输入维数增加了1。RNN由1024个单位的长短期记忆(LSTM)层组成。填充值在RNN中被屏蔽，因此它们被忽略。编码器是一个多输出模型:它输出LSTM层的隐藏状态和单元状态。LSTM层的输出不用于序列间架构。</p><pre class="nd ne nf ng gt nh nc ni nj aw nk bi"><span id="5952" class="nl ks it nc b gy nm nn l no np">class EndTokenLayer(Layer):<br/><br/>    def __init__(self, embedding_dim=128, **kwargs):<br/>        super(EndTokenLayer, self).__init__(**kwargs)<br/>        self.end_token_embedding = tf.Variable(initial_value=tf.random.uniform(shape=(embedding_dim,)), trainable=True)<br/><br/>    def call(self, inputs):<br/>        end_token = tf.tile(tf.reshape(self.end_token_embedding, shape=(1, 1, self.end_token_embedding.shape[0])), [tf.shape(inputs)[0],1,1])<br/>        return tf.keras.layers.concatenate([inputs, end_token], axis=1)</span><span id="1ee0" class="nl ks it nc b gy nq nn l no np">end_token_layer = EndTokenLayer()</span><span id="8fa3" class="nl ks it nc b gy nq nn l no np">inputs = tf.keras.Input(shape=(maxlen, 128))<br/>x = end_token_layer(inputs)<br/>x = tf.keras.layers.Masking(mask_value=0)(x)<br/>whole_seq_output, final_memory_state, final_carry_state = tf.keras.layers.LSTM(512, return_sequences=True, return_state=True)(x)<br/>outputs = (final_memory_state, final_carry_state)</span><span id="2930" class="nl ks it nc b gy nq nn l no np">encoder_model = tf.keras.Model(inputs=inputs, outputs=outputs, name="encoder_model")<br/>encoder_model.summary()</span><span id="07d7" class="nl ks it nc b gy nq nn l no np">Model: "encoder_model"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>input_1 (InputLayer)         [(None, 13, 128)]         0         <br/>_________________________________________________________________<br/>end_token_layer (EndTokenLay (None, 14, 128)           128       <br/>_________________________________________________________________<br/>masking (Masking)            (None, 14, 128)           0         <br/>_________________________________________________________________<br/>lstm (LSTM)                  [(None, 14, 512), (None,  1312768   <br/>=================================================================<br/>Total params: 1,312,896<br/>Trainable params: 1,312,896<br/>Non-trainable params: 0<br/>_________________________________________________________________</span><span id="24ac" class="nl ks it nc b gy nq nn l no np">inputs_eng = list(dataset_train.take(1).as_numpy_iterator())[0][0]</span><span id="421e" class="nl ks it nc b gy nq nn l no np">memory_state, carry_state = encoder_model(inputs_eng)</span></pre><h1 id="c44a" class="kr ks it bd kt ku kv kw kx ky kz la lb ki lc kj ld kl le km lf ko lg kp lh li bi translated">4.注意力</h1><p id="1906" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">我们将要使用的注意力机制是由<a class="ae mf" href="https://arxiv.org/pdf/1409.0473.pdf" rel="noopener ugc nofollow" target="_blank">【7】</a>提出的。使用注意机制的主要区别在于，我们增加了模型的表达能力，尤其是编码器组件。它不再需要将源句子中的所有信息编码成一个固定长度的向量。上下文向量<em class="ob"> c_i </em>然后被计算为:</p><figure class="nd ne nf ng gt od gh gi paragraph-image"><div class="gh gi og"><img src="../Images/34c0e5b51f16bbcadc4022bc72781344.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/format:webp/1*khsiPEcXYFqF6-U85AkRcQ.png"/></div></figure><p id="0d9f" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">权重<em class="ob"> α_ij </em>计算如下</p><figure class="nd ne nf ng gt od gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/7a8113d27e0613beeffbcf21ec48120a.png" data-original-src="https://miro.medium.com/v2/resize:fit:432/format:webp/1*0P6o74sSPVqpXjrd4v_sMA.png"/></div></figure><p id="28e7" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">其中<em class="ob"> e_ij=a(s_i-1，h_j) </em>是位置<em class="ob"> j </em>周围的输入与位置<em class="ob"> i </em>处的输出匹配程度的分数。</p><figure class="nd ne nf ng gt od gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/0556a27481979819b87419416d8f830e.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/1*O-nb4s_YWPbljkhcG98JvQ.png"/></div><p class="oj ok gj gh gi ol om bd b be z dk translated">图1:局部注意力模型——该模型首先预测当前目标单词的单个对齐位置p_t <em class="on"> </em>。然后，以源位置p_t为中心的窗口被用于计算上下文向量c_t，即窗口中源隐藏状态的加权平均值。从当前目标状态h_t和窗口中的那些源状态h_s推断出权重a_t。改编自<a class="ae mf" href="https://arxiv.org/pdf/1508.04025v5.pdf" rel="noopener ugc nofollow" target="_blank">【8】</a>。</p></figure><pre class="nd ne nf ng gt nh nc ni nj aw nk bi"><span id="4ec5" class="nl ks it nc b gy nm nn l no np"><strong class="nc jd">class</strong> <strong class="nc jd">Attention</strong>(tf.keras.layers.Layer):<br/>    <strong class="nc jd">def</strong> __init__(self, units):<br/>        super(Attention, self).__init__()<br/>        self.W1 = tf.keras.layers.Dense(units)<br/>        self.W2 = tf.keras.layers.Dense(units)<br/>        self.V = tf.keras.layers.Dense(1)<br/><br/>    <strong class="nc jd">def</strong> call(self, hidden_states, cell_states):<br/>        hidden_states_with_time = tf.expand_dims(hidden_states, 1)<br/><br/>        score = self.V(tf.nn.tanh(<br/>            self.W1(hidden_states_with_time) + self.W2(cell_states)))<br/><br/>        attention_weights = tf.nn.softmax(score, axis=1)<br/><br/>        context_vector = attention_weights * cell_states<br/>        context_vector = tf.reduce_sum(context_vector, axis=1)<br/><br/>        <strong class="nc jd">return</strong> context_vector, attention_weights</span></pre><h1 id="c6d2" class="kr ks it bd kt ku kv kw kx ky kz la lb ki lc kj ld kl le km lf ko lg kp lh li bi translated">5.解码器递归神经网络</h1><p id="e052" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">给定上下文向量<em class="ob"> c </em>和所有先前预测的字<em class="ob"> y_1，…，y_t-1 </em>，解码器被训练来预测下一个字<em class="ob"> y_t </em>，使得:</p><figure class="nd ne nf ng gt od gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/169f79ceeddaf3cca2d122b540d0fc32.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*-2iw4c8P0Zln0-q_yFqQRA.png"/></div></figure><p id="195d" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">其中<strong class="ll jd"> y </strong> =(y_1，…，y_T)。我们使用RNN，这意味着每个条件概率被建模为</p><figure class="nd ne nf ng gt od gh gi paragraph-image"><div class="gh gi op"><img src="../Images/05b823ea3dd0b76f64bab992d0c3a4fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*9q4pagkg5v0LBtpalvy6vA.png"/></div></figure><p id="9984" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">其中<em class="ob"> g </em>是非线性函数<em class="ob"> s_t </em>是RNN的隐藏状态。</p><p id="84cf" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">对于解码器RNN，我们定义了一个嵌入层，其词汇大小设置为唯一葡萄牙语标记的数量。在该嵌入层之后是具有1024个单元的LSTM层和具有与独特葡萄牙语令牌的数量相等的单元数量且没有激活功能的密集层。请注意，尽管网络相当浅，因为我们只使用了一个递归层，我们最终有近24M的参数进行训练。</p><pre class="nd ne nf ng gt nh nc ni nj aw nk bi"><span id="4db7" class="nl ks it nc b gy nm nn l no np">import json<br/>word_index = json.loads(tokenizer.get_config()['word_index'])<br/><br/>max_index_value = max(word_index.values())</span><span id="de58" class="nl ks it nc b gy nq nn l no np">class decoder_RNN(tf.keras.Model):<br/>    def __init__(self, **kwargs):<br/>        super(decoder_RNN, self).__init__()<br/>        self.embed = tf.keras.layers.Embedding(input_dim=max_index_value+1, output_dim=128, mask_zero=True)<br/>        self.lstm_1 = tf.keras.layers.LSTM(1024, return_sequences=True, return_state=True)<br/>        self.dense_1 = tf.keras.layers.Dense(max_index_value+1)<br/>        self.attention = Attention(1024)</span><span id="2452" class="nl ks it nc b gy nq nn l no np">def call(self, inputs, training=False, hidden_state=None, cell_state=None):<br/>        context_vector, attention_weights = self.attention(hidden_state, cell_state)<br/>        x = self.embed(inputs)<br/>        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)</span><span id="06cf" class="nl ks it nc b gy nq nn l no np">x, hidden_state, cell_state = self.lstm_1(x)<br/>        x = self.dense_1(x)<br/>        return x</span><span id="8ba6" class="nl ks it nc b gy nq nn l no np">decoder_model = decoder_RNN()</span><span id="b7a8" class="nl ks it nc b gy nq nn l no np">decoder_model(inputs = tf.random.uniform((16, 1)), hidden_state = memory_state, cell_state = carry_state)<br/>decoder_model.summary()</span><span id="5f95" class="nl ks it nc b gy nq nn l no np">Model: "decoder_rnn" _________________________________________________________________ Layer (type)                 Output Shape              Param #    ================================================================= embedding_1 (Embedding)      multiple                  1789568    _________________________________________________________________ lstm_3 (LSTM)                multiple                  6819840    _________________________________________________________________ dense_4 (Dense)              multiple                  14330525   _________________________________________________________________ attention_1 (Attention)      multiple                  1051649    ================================================================= Total params: 23,991,582 <br/>Trainable params: 23,991,582 <br/>Non-trainable params: 0 <br/>_________________________________________________________________</span></pre><h1 id="4995" class="kr ks it bd kt ku kv kw kx ky kz la lb ki lc kj ld kl le km lf ko lg kp lh li bi translated">6.培养</h1><p id="b472" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">为了训练一个具有序列到序列架构的模型，我们需要定义一个定制的训练循环。首先，我们定义了一个函数，将葡萄牙语语料库分成输入和输出张量，然后输入到解码器模型。其次，我们创建了完整模型的向前和向后传递。我们将英文输入传递给编码器，以获得编码器LSTM的隐藏和单元格状态。这些隐藏和单元状态随后与葡萄牙语输入一起被传递到解码器。我们定义了损失函数，该函数是在解码器输出和先前分离的葡萄牙语输出之间计算的，并且计算了关于编码器和解码器可训练变量的梯度。最后，我们将训练循环运行了规定数量的时期。它遍历训练数据集，从葡萄牙语序列创建解码器输入和输出。然后，它计算梯度并相应地更新模型的参数。</p><p id="2914" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">模型训练起来相当慢，即使使用GPU也是如此。回想一下，我们甚至没有在任何rnn中堆叠层，这将减少我们的损失，但同时，使我们的模型更难训练。我们可以从下面的图中看到，训练和验证都随着时间的推移而稳步减少。</p><pre class="nd ne nf ng gt nh nc ni nj aw nk bi"><span id="6181" class="nl ks it nc b gy nm nn l no np">def portuguese_input_output(data):<br/>    return (tf.cast(data[:,0:-1], tf.float32), tf.cast(data[:,1:], tf.float32))</span><span id="38ce" class="nl ks it nc b gy nq nn l no np">optimizer_obj = tf.keras.optimizers.Adam(learning_rate=0.001)<br/>loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)</span><span id="e8a1" class="nl ks it nc b gy nq nn l no np"><a class="ae mf" href="http://twitter.com/tf" rel="noopener ugc nofollow" target="_blank">@tf</a>.function<br/>def grad(encoder_model, decoder_model, english_input, portuguese_input, portuguese_output, loss):<br/>    with tf.GradientTape() as tape:<br/>        loss_value = 0<br/>        hidden_state, cell_state = encoder_model(english_input)<br/>        dec_input = tf.expand_dims([word_index['&lt;start&gt;']] * 16, 1)</span><span id="95e3" class="nl ks it nc b gy nq nn l no np"># Teacher forcing - feeding the target as the next input<br/>        for t in range(1, portuguese_output.shape[1]):<br/>            predictions = decoder_model(dec_input, hidden_state = hidden_state, cell_state = cell_state)</span><span id="8d46" class="nl ks it nc b gy nq nn l no np">loss_value += loss(portuguese_output[:, t], predictions)</span><span id="6579" class="nl ks it nc b gy nq nn l no np"># using teacher forcing<br/>            dec_input = tf.expand_dims(portuguese_output[:, t], 1)</span><span id="d8d0" class="nl ks it nc b gy nq nn l no np">grads = tape.gradient(loss_value, encoder_model.trainable_variables + decoder_model.trainable_variables)<br/>    return loss_value, grads</span><span id="e4df" class="nl ks it nc b gy nq nn l no np">def train_model(encoder_model, decoder_model, num_epochs, train_dataset, valid_dataset, optimizer, loss, grad_fn):<br/>    inputs = (14,)<br/>    train_loss_results = []<br/>    train_loss_results_valid = []<br/>    for epoch in range(num_epochs):<br/>        start = time.time()</span><span id="2656" class="nl ks it nc b gy nq nn l no np">epoch_loss_avg = tf.keras.metrics.Mean()<br/>        <br/>        for x, y in train_dataset:<br/>            dec_inp, dec_out = portuguese_input_output(y)<br/>            loss_value, grads = grad_fn(encoder_model, decoder_model, x, dec_inp, dec_out, loss)<br/>            optimizer.apply_gradients(zip(grads, encoder_model.trainable_variables + decoder_model.trainable_variables))<br/>            epoch_loss_avg(loss_value)</span><span id="992d" class="nl ks it nc b gy nq nn l no np">train_loss_results.append(epoch_loss_avg.result())<br/>        <br/>        print("Epoch {:03d}: Loss: {:.3f}".format(epoch,<br/>                                                  epoch_loss_avg.result()))<br/>        print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\n')<br/>    return train_loss_results</span><span id="6558" class="nl ks it nc b gy nq nn l no np">num_epochs=15<br/>train_loss_results = train_model(encoder_model, decoder_model, num_epochs, dataset_train, dataset_valid, optimizer_obj, loss_obj, grad)</span><span id="a001" class="nl ks it nc b gy nq nn l no np">plt.plot(np.arange(num_epochs), train_loss_results)<br/>plt.ylabel('loss')<br/>plt.xlabel('epoch');</span></pre><figure class="nd ne nf ng gt od gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/c2ed98b04a04c02c75de08d03afc4f8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*Fth1TVb70OGyfjImVIIQCg.png"/></div><p class="oj ok gj gh gi ol om bd b be z dk translated">图2:seq 2 seq模型几个时期的损失演变。</p></figure><h1 id="9180" class="kr ks it bd kt ku kv kw kx ky kz la lb ki lc kj ld kl le km lf ko lg kp lh li bi translated">7.结果</h1><p id="5bda" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">为了测试我们的模型，我们定义了一组英语句子。为了翻译句子，我们首先预处理和嵌入句子，就像我们处理训练集和验证集一样。接下来，我们通过编码器RNN传递嵌入的句子，以获得隐藏和单元格状态。从特殊的<code class="fe mz na nb nc b">"&lt;start&gt;"</code>令牌开始，我们使用该令牌和编码器网络的最终隐藏和单元状态从解码器获得一步预测和更新的隐藏和单元状态。之后，我们创建了一个循环来获得下一步预测，并使用最近的隐藏和单元状态从解码器更新隐藏和单元状态。当发出<code class="fe mz na nb nc b">"&lt;end&gt;"</code>标记或句子达到定义的最大长度时，循环终止。最后，我们将输出的令牌序列解码成葡萄牙语文本。</p><p id="4cff" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">获得的翻译相当不错。一个有趣且更复杂的例子是输入<em class="ob">‘你还在家吗？’。</em>一个问题有非常清晰的语法规则，其中一些是特定于语言的。返回的翻译<em class="ob">‘你在家吗？’</em>表明该模型能够捕捉这些特异性。</p><pre class="nd ne nf ng gt nh nc ni nj aw nk bi"><span id="aef7" class="nl ks it nc b gy nm nn l no np">english_test = <!-- -->['that is not safe .',  <br/>                'this is my life .',  <br/>                'are you still at home ?',  <br/>                'it is very cold here .']</span><span id="f549" class="nl ks it nc b gy nq nn l no np">english_test_emb = embed_english(split_english(tf.data.Dataset.from_tensor_slices((english_test, portuguese_token_train[:len(english_test),:])))).map(map_maxlen).batch(1)</span><span id="3ff8" class="nl ks it nc b gy nq nn l no np">total_output=[]<br/><br/>for x, y in english_test_emb:<br/>    hidden_state, cell_state = encoder_model(x)<br/>    output_decoder = decoder_model(inputs = np.tile(word_index['&lt;start&gt;'], (1, 1)), hidden_state = hidden_state, cell_state = cell_state)<br/>    output=[]<br/>    output.append(output_decoder)<br/>    for i in range(13):<br/>        if tf.math.argmax(output_decoder, axis=2).numpy()[0][0] == 2: # &lt;end&gt; character<br/>            break<br/>        output_decoder = decoder_model(inputs = tf.math.argmax(output_decoder, axis=2), hidden_state = hidden_state, cell_state = cell_state)<br/>        output.append(output_decoder)<br/>    total_output.append(output)</span><span id="d55e" class="nl ks it nc b gy nq nn l no np">total_output_trans = []<br/>for j in range(test_size):<br/>    output_trans = []<br/>    for i in range(len(total_output[j])):<br/>        output_trans.append(tf.math.argmax(total_output[j][i], axis=2).numpy()[0][0])<br/>    total_output_trans.append(output_trans)</span><span id="1b00" class="nl ks it nc b gy nq nn l no np">output_trans = np.array([np.array(xi) for xi in total_output_trans], dtype=object)</span><span id="940a" class="nl ks it nc b gy nq nn l no np">portuguese_trans_batch=[]<br/>inv_word_index = {v: k for k, v in word_index.items()}<br/>for i in range(output_trans.shape[0]):<br/>    portuguese_trans_batch.append(' '.join(list(np.vectorize(inv_word_index.get)(output_trans[i]))))</span><span id="53ee" class="nl ks it nc b gy nq nn l no np">list(english_test)</span><span id="979a" class="nl ks it nc b gy nq nn l no np">['that is not safe .',  <br/> 'this is my life .',  <br/> 'are you still at home ?',  <br/> 'it is very cold here .']</span><span id="f7ee" class="nl ks it nc b gy nq nn l no np">portuguese_trans_batch</span><span id="0145" class="nl ks it nc b gy nq nn l no np">['nao e seguro .',  <br/> 'e a minha vida .',  <br/> 'ainda esta em casa ?',  <br/> 'muito frio aqui .']</span></pre><h1 id="b3dd" class="kr ks it bd kt ku kv kw kx ky kz la lb ki lc kj ld kl le km lf ko lg kp lh li bi translated">8.结论</h1><p id="def7" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated">NMT模型的架构非常具有挑战性，需要大量的定制工作，例如在其培训过程中。当在非常大的语料库中使用预先训练的嵌入来嵌入英语序列时，我们使用迁移学习的原理。另一方面，我们为用作解码器网络输入的葡萄牙语开发了自己的嵌入功能。编码器和解码器rnn保持尽可能简单，因为训练该模型在计算上是昂贵的。</p><p id="58ca" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">我们生成了从英语文本到葡萄牙语的翻译，除了提供英语和葡萄牙语的句子对来训练我们的模型之外，没有提供任何东西。该模型理解肯定和否定，重要的语法区别，如在建立一个疑问类型的从句时，并能够解释语法规则，如英语中常用的主语-助词倒装，不能直接翻译成葡萄牙语。</p><p id="4ddb" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">这种方法可以通过增加模型的深度和每层中单元的数量来扩展。可以调整批量大小等超参数来提高准确性。还可以测试更广泛的注意力策略。</p><p id="3951" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">保持联系:<a class="ae mf" href="https://www.linkedin.com/in/luisbrasroque/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a></p><h1 id="21ea" class="kr ks it bd kt ku kv kw kx ky kz la lb ki lc kj ld kl le km lf ko lg kp lh li bi translated">9.参考</h1><p id="621f" class="pw-post-body-paragraph lj lk it ll b lm ln kd lo lp lq kg lr ls lt lu lv lw lx ly lz ma mb mc md me im bi translated"><a class="ae mf" href="https://arxiv.org/abs/1609.08144" rel="noopener ugc nofollow" target="_blank">【1】</a>【吴等人，2016】吴、y、舒斯特、m、陈、z、乐、qv、、m、马切里、w、克里昆、m、曹、y、高、q、马切里、k、克林纳、j、沙阿、a、约翰逊、m、刘、x、祖卡斯凯泽、Gouws、s、加藤、y、工藤、t、和泽、h、史蒂文斯、k、库连、g、帕蒂尔、n、王谷歌的神经机器翻译系统:弥合人类和机器翻译之间的差距。</p><p id="832c" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated"><a class="ae mf" href="https://arxiv.org/abs/1412.2007" rel="noopener ugc nofollow" target="_blank">【2】</a>【让等人，2015】让，s .，乔，k .，梅米塞维奇，r .，本吉奥，Y. (2015)。使用超大目标词汇进行神经机器翻译。</p><p id="6e7d" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated"><a class="ae mf" rel="noopener" target="_blank" href="/generating-text-with-recurrent-neural-networks-based-on-the-work-of-f-pessoa-1e804d88692d">【3】</a><a class="ae mf" rel="noopener" target="_blank" href="/generating-text-with-recurrent-neural-networks-based-on-the-work-of-f-pessoa-1e804d88692d">https://towardsdatascience . com/generating-text-with-recurrent-neural-networks-on-the-work-of-f-pes SOA-1e 804d 88692d</a></p><p id="df22" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated"><a class="ae mf" href="https://tfhub.dev/google/tf2-preview/nnlm-en-dim128" rel="noopener ugc nofollow" target="_blank">【4】</a><a class="ae mf" href="https://tfhub.dev/google/tf2-preview/nnlm-en-dim128" rel="noopener ugc nofollow" target="_blank">https://tfhub.dev/google/tf2-preview/nnlm-en-dim128</a></p><p id="83ae" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated"><a class="ae mf" href="https://www.kaggle.com/luisroque/engpor-sentence-pairs" rel="noopener ugc nofollow" target="_blank">【5】</a>【https://www.kaggle.com/luisroque/engpor-sentence-pairs】T2</p><p id="6d1c" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated"><a class="ae mf" rel="noopener" target="_blank" href="/transfer-learning-and-data-augmentation-applied-to-the-simpsons-image-dataset-e292716fbd43">【6】</a><a class="ae mf" rel="noopener" target="_blank" href="/transfer-learning-and-data-augmentation-applied-to-the-simpsons-image-dataset-e292716fbd43">https://towards data science . com/transfer-learning-and-data-augmentation-applied-to-the-Simpsons-image-dataset-e 292716 FBD 43</a></p><p id="0aac" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">[7] [Bahdanau等人，2016年]bahda nau d . Cho k .和beng io y .(2016年)。通过联合学习对齐和翻译的神经机器翻译。</p><p id="f574" class="pw-post-body-paragraph lj lk it ll b lm mg kd lo lp mh kg lr ls mi lu lv lw mj ly lz ma mk mc md me im bi translated">[8] [Luong等人，2015年] Luong，m-t .，Pham，h .和Manning，C. D. (2015年)。基于注意力的神经机器翻译的有效方法。</p></div></div>    
</body>
</html>