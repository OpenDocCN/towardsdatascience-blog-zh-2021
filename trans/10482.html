<html>
<head>
<title>Deep Q-network with Pytorch and Gym to solve the Acrobot game</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度Q网配合Pytorch和Gym解决Acrobot游戏</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-q-network-with-pytorch-and-gym-to-solve-acrobot-game-d677836bda9b?source=collection_archive---------5-----------------------#2021-10-06">https://towardsdatascience.com/deep-q-network-with-pytorch-and-gym-to-solve-acrobot-game-d677836bda9b?source=collection_archive---------5-----------------------#2021-10-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="00d8" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一种深度强化学习算法的介绍与实现</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/9780bd829e2e3a2f53ccd38650e32689.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q-uBVUwpUSXmyeoaL-H5VA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者插图</p></figure><p id="5874" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">强化学习</strong>是机器学习的一个分支，受人类和动物行为主义心理学的启发。在<strong class="la iu">监督学习</strong> (SL)中，学习受到限制，因为它总是需要外部教学信号来帮助解决任务，如分类和回归。</p><p id="3eee" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">与第二语言不同，强化学习基于不同的原则。主要目标之一是生产一个自主代理，它与环境交互，学习和选择<strong class="la iu">最优行动</strong>，有助于实现其目标，如最大化回报。代理观察环境中的状态，并在给定状态的情况下执行动作。每次代理通过采取行动进行交互，他可能会收到一个<strong class="la iu">正</strong>或<strong class="la iu">负奖励</strong>。</p><p id="ca59" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通过<strong class="la iu">反复试验</strong>，最佳行为会随着时间的推移而改进。这种学习方式让我想起了我们从经验中学习的方式。我们讨厌犯错，但当我们从中吸取教训并在未来做得更好时，错误会变得很有价值。同样，强化学习探索不同的动作和状态，以找到最优的动作。此外，当在学习过程的早期出现错误时，对代理的训练更加健壮。</p><p id="1bb5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于这些特点，强化学习经常被应用于玩游戏、机器人和许多其他决策问题。强化学习与深度学习技术的结合，使得在高维状态空间的问题上取得巨大进步成为可能。例如，当代理需要从屏幕像素中学习时，它就很有用。这些方法被称为<strong class="la iu">深度强化学习</strong>。</p><p id="04f0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本教程中，我将训练一个执行Acrobot任务的代理。Pytorch将用于实现Deep Q网络，而Google Colab笔记本将被使用，因为它提供免费的GPU来加速训练。</p><h2 id="435c" class="lu lv it bd lw lx ly dn lz ma mb dp mc lh md me mf ll mg mh mi lp mj mk ml mm bi translated">目录:</h2><ol class=""><li id="78f0" class="mn mo it la b lb mp le mq lh mr ll ms lp mt lt mu mv mw mx bi translated"><a class="ae my" href="#ef3f" rel="noopener ugc nofollow"> OpenAI健身房</a></li><li id="e876" class="mn mo it la b lb mz le na lh nb ll nc lp nd lt mu mv mw mx bi translated"><a class="ae my" href="#ec9f" rel="noopener ugc nofollow">安装并导入库</a></li><li id="024f" class="mn mo it la b lb mz le na lh nb ll nc lp nd lt mu mv mw mx bi translated"><a class="ae my" href="#babf" rel="noopener ugc nofollow">行动和观察空间</a></li><li id="73ea" class="mn mo it la b lb mz le na lh nb ll nc lp nd lt mu mv mw mx bi translated"><a class="ae my" href="#06b7" rel="noopener ugc nofollow">随机代理</a></li><li id="378a" class="mn mo it la b lb mz le na lh nb ll nc lp nd lt mu mv mw mx bi translated"><a class="ae my" href="#80c1" rel="noopener ugc nofollow">实现深度Q网</a></li><li id="f70e" class="mn mo it la b lb mz le na lh nb ll nc lp nd lt mu mv mw mx bi translated"><a class="ae my" href="#4949" rel="noopener ugc nofollow">经验回放</a></li><li id="eddf" class="mn mo it la b lb mz le na lh nb ll nc lp nd lt mu mv mw mx bi translated"><a class="ae my" href="#214e" rel="noopener ugc nofollow">ε-贪婪政策</a></li><li id="30c4" class="mn mo it la b lb mz le na lh nb ll nc lp nd lt mu mv mw mx bi translated"><a class="ae my" href="#9bf2" rel="noopener ugc nofollow"> Softmax策略</a></li><li id="b477" class="mn mo it la b lb mz le na lh nb ll nc lp nd lt mu mv mw mx bi translated"><a class="ae my" href="#22c2" rel="noopener ugc nofollow">勘探剖面图</a></li><li id="5cd1" class="mn mo it la b lb mz le na lh nb ll nc lp nd lt mu mv mw mx bi translated"><a class="ae my" href="#2c00" rel="noopener ugc nofollow">训练</a></li></ol></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><h1 id="ef3f" class="nl lv it bd lw nm nn no lz np nq nr mc jz ns ka mf kc nt kd mi kf nu kg ml nv bi translated">1.奥鹏健身馆</h1><p id="3c57" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh nw lj lk ll nx ln lo lp ny lr ls lt im bi translated"><strong class="la iu"> Gym </strong>是一个开源库，提供强化学习算法的实现【1】。有许多教学代理可供培训，如Cart-Pole和Pong。在本教程中，我将重点介绍Acrobot环境。在<a class="ae my" href="https://www.analyticsvidhya.com/blog/2021/06/acrobot-with-deep-q-learning/" rel="noopener ugc nofollow" target="_blank">之前的教程</a>中，我很好地解释了这个游戏如何如果你想更深入地理解它。</p><h1 id="ec9f" class="nl lv it bd lw nm nz no lz np oa nr mc jz ob ka mf kc oc kd mi kf od kg ml nv bi translated">2.安装和导入库</h1><p id="f8dd" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh nw lj lk ll nx ln lo lp ny lr ls lt im bi translated">我们需要安装和导入python包。最重要的套餐是健身房，它提供强化学习环境。还有其他有用的库，如:</p><ul class=""><li id="f2f8" class="mn mo it la b lb lc le lf lh oe ll of lp og lt oh mv mw mx bi translated"><code class="fe oi oj ok ol b">torch.nn</code>实现深度Q网络</li><li id="0c87" class="mn mo it la b lb mz le na lh nb ll nc lp nd lt oh mv mw mx bi translated">需要<code class="fe oi oj ok ol b">IPython.display</code>和<code class="fe oi oj ok ol b">pyvirtualdisplay.display</code>来创建一个虚拟显示器，在上面绘制游戏图像</li><li id="50c1" class="mn mo it la b lb mz le na lh nb ll nc lp nd lt oh mv mw mx bi translated"><code class="fe oi oj ok ol b">collections</code>是Python基本对象的替代品，如列表、元组、字典和集合。它用来存储代理的经验。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om on l"/></div></figure><h1 id="babf" class="nl lv it bd lw nm nz no lz np oa nr mc jz ob ka mf kc oc kd mi kf od kg ml nv bi translated">3.行动和观察空间</h1><p id="6140" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh nw lj lk ll nx ln lo lp ny lr ls lt im bi translated">我们可以使用make函数实例化体育馆环境。我们也可以设置一个随机的环境种子，每次都产生相同的结果。你大概可以想象，每个环境都有不同数量的动作和观察。通过打印，我们可以很容易地看到操作和观察空间:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om on l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/fdd8d26a51ee1ccc4753f93013cc5d98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lpJ1J5LKrBsylKU8imGGpw.png"/></div></div></figure><p id="4c93" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">动作空间是离散的，因此动作可以是以下非负数之一:[0，1，2]。与动作空间不同的是，观察空间是盒子，代表一个n维的盒子。然后，每个观察值将是一个范围在-28和28之间的6个数字的数组。</p><h1 id="06b7" class="nl lv it bd lw nm nz no lz np oa nr mc jz ob ka mf kc oc kd mi kf od kg ml nv bi translated">4.随机代理</h1><p id="e8ec" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh nw lj lk ll nx ln lo lp ny lr ls lt im bi translated">这是一个运行代理的例子，它只是随机选择一个动作。Acrobot环境将运行10集，显示游戏每一步的视频。在运行代理之前，我们定义了可视化环境视频的函数。在Google Colab中启用健身房环境渲染需要这些函数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om on l"/></div></figure><p id="dcd1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，我们运行随机代理10个时间步:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om on l"/></div></figure><p id="4693" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这段代码总结了有限马尔可夫决策过程(MDP)的形式问题，这是一个强化学习问题的一般设置。我们知道有五个组件协同工作:</p><ul class=""><li id="2da3" class="mn mo it la b lb lc le lf lh oe ll of lp og lt oh mv mw mx bi translated"><strong class="la iu">代理</strong>是由两个关节和两个连杆组成的机械臂，两个连杆之间的关节被驱动</li><li id="73ae" class="mn mo it la b lb mz le na lh nb ll nc lp nd lt oh mv mw mx bi translated"><strong class="la iu">环境</strong>对应于Acrobot环境</li><li id="bece" class="mn mo it la b lb mz le na lh nb ll nc lp nd lt oh mv mw mx bi translated"><strong class="la iu">状态</strong>由一组6个数字组成</li><li id="3a39" class="mn mo it la b lb mz le na lh nb ll nc lp nd lt oh mv mw mx bi translated"><strong class="la iu">动作</strong>:有3种可能的动作</li><li id="e0d4" class="mn mo it la b lb mz le na lh nb ll nc lp nd lt oh mv mw mx bi translated">每一步奖励 : -1</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/6e7c74bc95d54923578da98346b1a1ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X9q30ZvkDFDZ96euoyIJOQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者插图</p></figure><p id="40f5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">根据萨顿和巴尔托在<em class="oq">的《强化学习:导论</em>一书中对MDP的主体-环境交互的定义，我们知道:</p><blockquote class="or os ot"><p id="29a1" class="ky kz oq la b lb lc ju ld le lf jx lg ou li lj lk ov lm ln lo ow lq lr ls lt im bi translated">主体和环境在一系列离散的时间步长t=0，1，2，3，…中的每一步都相互作用。在每个时间步骤t，代理接收环境的<strong class="la iu">状态</strong>的一些表示，并在此基础上选择<strong class="la iu">动作</strong>。一个时间步骤之后，部分地作为其动作的结果，代理收到一个数字<strong class="la iu">奖励</strong>，并发现自己处于一个<strong class="la iu">新状态</strong>【3】。</p></blockquote><p id="0519" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，实现了代理-环境循环:在每一集里，我们得到:</p><ul class=""><li id="813b" class="mn mo it la b lb lc le lf lh oe ll of lp og lt oh mv mw mx bi translated"><strong class="la iu">初始状态</strong>调用<code class="fe oi oj ok ol b">env.reset</code></li><li id="deba" class="mn mo it la b lb mz le na lh nb ll nc lp nd lt oh mv mw mx bi translated">代理选择一个<strong class="la iu">随机动作</strong>，它取值[0，1，2]中的一个</li><li id="cc8a" class="mn mo it la b lb mz le na lh nb ll nc lp nd lt oh mv mw mx bi translated">通过step函数，应用随机动作并返回四个值:新状态<strong class="la iu">、上一步获得的奖励<strong class="la iu">、游戏结束时为真的标志<strong class="la iu">完成</strong>以及用于调试的诊断信息<strong class="la iu"/>。</strong></strong></li><li id="d052" class="mn mo it la b lb mz le na lh nb ll nc lp nd lt oh mv mw mx bi translated">为下一次迭代设置当前状态</li></ul><p id="6312" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们调用函数show_videos来显示视频，每集一个:</p><pre class="kj kk kl km gt ox ol oy oz aw pa bi"><span id="0544" class="lu lv it ol b gy pb pc l pd pe">show_videos()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pf"><img src="../Images/f88a3dcb57412dfd02f19c45fdbdc8d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W4jBroKtXbsP86RiC_sfVg.png"/></div></div></figure><h1 id="80c1" class="nl lv it bd lw nm nz no lz np oa nr mc jz ob ka mf kc oc kd mi kf od kg ml nv bi translated">5.实施深度Q网络</h1><p id="32c1" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh nw lj lk ll nx ln lo lp ny lr ls lt im bi translated">在这一节中，我将展示如何在Acrobot游戏上用Pytorch实现深度Q网络。该模型是一个神经网络，它将状态空间的维度作为输入，并返回对应于每个可能动作的最佳q值。因为有三种可能的动作来移动机械臂，所以返回的输出数是3。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om on l"/></div></figure><h1 id="4949" class="nl lv it bd lw nm nz no lz np oa nr mc jz ob ka mf kc oc kd mi kf od kg ml nv bi translated">6.体验回放</h1><p id="799b" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh nw lj lk ll nx ln lo lp ny lr ls lt im bi translated">一旦我们定义了网络，我们就可以创建一个名为ReplayMemory的类。当观察序列中存在相关性时，神经网络可能不稳定或发散。出于这个原因，我们还需要体验回放，它使代理能够记住过去的体验并从中学习。此外，它使数据随机化，从而提供不相关的数据。</p><blockquote class="or os ot"><p id="5042" class="ky kz oq la b lb lc ju ld le lf jx lg ou li lj lk ov lm ln lo ow lq lr ls lt im bi translated">为了执行体验重放，我们将代理在每个时间步长t的体验e_t=(状态，动作，下一个状态，奖励)存储在数据集D_t={e_1，…，e_t}中。在学习过程中，我们应用Q学习更新，对经验(s，a，r，s')~U(D)的样本(或小批量)进行更新，这些样本是从存储的样本池中均匀随机抽取的[4]。</p></blockquote><p id="8dda" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在实践中，我们需要一个具有预定义容量的队列。当我们达到最大容量时，队列中最旧的元素将被新的元素替换。使用python集合库中的deque对象可以实现这种行为。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om on l"/></div></figure><p id="d6fe" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最大容量是<code class="fe oi oj ok ol b">ReplayMemory</code>对象请求的唯一输入。这样，我们用maxlen capacity定义了一个等同于deque对象的内存属性。</p><p id="72d0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们还定义了push函数，将新的体验添加到重放存储器中。</p><p id="7c04" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe oi oj ok ol b">sample</code>方法被定义为从记忆中采样经验。体验次数等于batch_size。如果请求的批量大于内存中当前的样本数，我们将获取所有样本。</p><h1 id="214e" class="nl lv it bd lw nm nz no lz np oa nr mc jz ob ka mf kc oc kd mi kf od kg ml nv bi translated">7.ε贪婪策略</h1><p id="bc2c" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh nw lj lk ll nx ln lo lp ny lr ls lt im bi translated">在执行体验重放之后，下一步是根据ε-greedy策略选择并执行一个动作。该策略选择一个概率为ε的<strong class="la iu">随机行动</strong>，否则，选择对应于最高Q值的<strong class="la iu">最佳行动</strong>。主要思想是代理在开始时探索环境而不是利用它。代理从环境中学习得越多，它就越会选择基于开发的最优行动。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om on l"/></div></figure><h1 id="9bf2" class="nl lv it bd lw nm nz no lz np oa nr mc jz ob ka mf kc oc kd mi kf od kg ml nv bi translated">8.Softmax策略</h1><p id="fc15" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh nw lj lk ll nx ln lo lp ny lr ls lt im bi translated">在给定温度参数的情况下，Softmax策略用于基于通过将softmax应用于估计的Q值而获得的分布来选择最佳动作。有两种可能的情况:</p><ul class=""><li id="6d10" class="mn mo it la b lb lc le lf lh oe ll of lp og lt oh mv mw mx bi translated">温度越高，分布就越趋于随机均匀分布。</li><li id="9b06" class="mn mo it la b lb mz le na lh nb ll nc lp nd lt oh mv mw mx bi translated">在零度时，策略将总是选择具有最高Q值的动作。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om on l"/></div></figure><h1 id="22c2" class="nl lv it bd lw nm nz no lz np oa nr mc jz ob ka mf kc oc kd mi kf od kg ml nv bi translated">9.勘探剖面</h1><p id="f43b" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh nw lj lk ll nx ln lo lp ny lr ls lt im bi translated">我们使用softmax策略定义了一个指数递减的勘探配置文件:</p><p id="227e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="oq">soft max _ temperature = initial _ temperature * exponential_decay^(i)</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om on l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pg"><img src="../Images/844563421f984a3117808c6ae035a1c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*a4i6rZa7MpP3SC7P-QbD6A.png"/></div></div></figure><h1 id="2c00" class="nl lv it bd lw nm nz no lz np oa nr mc jz ob ka mf kc oc kd mi kf od kg ml nv bi translated">10.培养</h1><p id="c856" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh nw lj lk ll nx ln lo lp ny lr ls lt im bi translated">我们可以初始化超参数，如SGD优化器和Huber损失函数、ReplayMemory对象、策略网络和目标网络。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om on l"/></div></figure><p id="4b98" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">函数update_step被定义为执行单个优化步骤。首先，它从重放存储器中抽取一批样本。之后，它为批中的每个元素创建张量。它还计算非最终状态的掩码，并连接批处理元素。</p><p id="3619" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">随后，它计算给定实际状态的策略网络的所有Q值。与策略网络不同，价值函数是基于使用目标网络的下一个状态来计算的。</p><p id="4098" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，我们可以基于策略网络的Q值和使用目标网络计算的Q值的最大值来获得预期的Q值。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om on l"/></div></figure><p id="bc2a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，是时候训练深度Q学习代理了，运行800集。在for循环中，我们只需要调用前面定义的所有函数。我们还会每100集放一次视频。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om on l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ph"><img src="../Images/4157e590ce26aed843c86ec2bc05c165.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eLF038iCh-EVdZ2AxZ2jsg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">最后几集的结果</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/c19498f44ebf6101891aef0ff46cc430.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*s6BkfKxCH3qxgsCJ3mcsvw.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">第100集视频</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/617ddfd76b1933a8bf50cb6bd87dca6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*pfwzartdKZ1QHVFgUCPRlg.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">最后一集的视频</p></figure><p id="ab60" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从上一集的视频中，我们可以注意到代理学会了解决任务。很明显，如果你也看了100集之后的视频，代理人还在探索最佳行动。训练结束后，我们还可以显示每集获得的累积分值:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om on l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/a1fd84a9e21c7e5e4e81114d3247a11d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*qXmkqBoRV1_iGn-c5ymF3A.png"/></div></figure><p id="fbce" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">开始时，累积奖励保持在-500。在200集之后，分数呈指数增长，保持在-100左右。最后一步是测试代理是否真的学会了解决它的任务。类似于之前实现的随机代理。我们选择温度参数设置为0的最佳操作，而不是采取随机操作。这样，softmax策略将总是选择具有最高Q值的动作。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="om on l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/ecef05ba055291efc98856b793d27a73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*LVx8RqOUIu_R58FQY_TcaQ.png"/></div></figure><p id="5f27" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以观察到，在最终的测试结果中，累积奖励分数仍然很小。</p><h1 id="7fef" class="nl lv it bd lw nm nz no lz np oa nr mc jz ob ka mf kc oc kd mi kf od kg ml nv bi translated">最终想法:</h1><p id="5f1f" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh nw lj lk ll nx ln lo lp ny lr ls lt im bi translated">在本教程中，我提供了使用Pytorch构建深度Q网络的概述。为了更好地理解它，更好的方法是将代码分成构建块，每次只关注一个块。起初，如果你以前只尝试过监督和非监督技术，这似乎很难。但经过一些努力，你将能够理解它，甚至将深度q网络应用到其他环境中。这里的GitHub代码是<a class="ae my" href="https://github.com/eugeniaring/Medium-Articles/blob/main/Pytorch/acrobot.ipynb" rel="noopener ugc nofollow" target="_blank"/>。</p><h1 id="89f8" class="nl lv it bd lw nm nz no lz np oa nr mc jz ob ka mf kc oc kd mi kf od kg ml nv bi translated">参考资料:</h1><p id="26d1" class="pw-post-body-paragraph ky kz it la b lb mp ju ld le mq jx lg lh nw lj lk ll nx ln lo lp ny lr ls lt im bi translated">[1]https://gym.openai.com/docs/<a class="ae my" href="https://gym.openai.com/docs/" rel="noopener ugc nofollow" target="_blank"/></p><p id="4fc0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[2]<a class="ae my" href="https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html" rel="noopener ugc nofollow" target="_blank">py torch的强化学习(DQN)教程</a></p><p id="4d69" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">[3] <a class="ae my" href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf" rel="noopener ugc nofollow" target="_blank">萨顿&amp;巴尔托著，《强化学习:导论》(2018)。</a></p><p id="85a0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae my" href="https://www.nature.com/articles/nature14236" rel="noopener ugc nofollow" target="_blank">https://www.nature.com/articles/nature14236</a></p></div><div class="ab cl ne nf hx ng" role="separator"><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj nk"/><span class="nh bw bk ni nj"/></div><div class="im in io ip iq"><p id="795c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你喜欢我的文章吗？<a class="ae my" href="https://eugenia-anello.medium.com/membership" rel="noopener"> <em class="oq">成为会员</em> </a> <em class="oq">每天无限获取数据科学新帖！这是一种间接的支持我的方式，不会给你带来任何额外的费用。如果您已经是会员，</em> <a class="ae my" href="https://eugenia-anello.medium.com/subscribe" rel="noopener"> <em class="oq">订阅</em> </a> <em class="oq">每当我发布新的数据科学和python指南时，您都会收到电子邮件！</em></p></div></div>    
</body>
</html>