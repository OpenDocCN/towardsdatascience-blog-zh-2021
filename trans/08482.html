<html>
<head>
<title>Knowledge Graphs in Natural Language Processing @ ACL 2021</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理中的知识图@ ACL 2021</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/knowledge-graphs-in-natural-language-processing-acl-2021-6cac04f39761?source=collection_archive---------2-----------------------#2021-08-05">https://towardsdatascience.com/knowledge-graphs-in-natural-language-processing-acl-2021-6cac04f39761?source=collection_archive---------2-----------------------#2021-08-05</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="3d86" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">最先进的，2011年夏天</h2><div class=""/><div class=""><h2 id="944c" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">你的KG相关的NLP研究指南，ACL版</h2></div><p id="fe23" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">欢迎来到我们关于知识图的NLP论文的第三次常规综述，这次发表在<a class="ae lk" href="https://2021.aclweb.org/" rel="noopener ugc nofollow" target="_blank"> ACL 2021 </a>！今年你不想错过的流行趋势是什么？👀</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/f77a01b4f135dd89defb2c592ccb9716.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kfXiTIlHBbAqkVlyK3SvAA.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">米尔科维在Unsplash上拍摄的背景照片，由作者改编</p></figure><p id="fb7d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">ACL'21仍然是最大的NLP场地之一:今年<a class="ae lk" href="https://2021.aclweb.org/blog/acceptance-decision/" rel="noopener ugc nofollow" target="_blank">接受了700+完整论文和500+ACL论文的研究结果</a>📈。除此之外，不要忘记通常有影响力的短论文和广泛选择的<a class="ae lk" href="https://2021.aclweb.org/program/workshops/" rel="noopener ugc nofollow" target="_blank">研讨会</a>和<a class="ae lk" href="https://2021.aclweb.org/program/tutorials/" rel="noopener ugc nofollow" target="_blank">教程</a>。我试图从所有这些曲目中提炼出一些KG的论文，放到一个帖子中。以下是今天的话题:</p><ul class=""><li id="e60c" class="mb mc iq kq b kr ks ku kv kx md lb me lf mf lj mg mh mi mj bi translated"><a class="ae lk" href="#3683" rel="noopener ugc nofollow">神经数据库&amp;检索</a></li><li id="6c63" class="mb mc iq kq b kr mk ku ml kx mm lb mn lf mo lj mg mh mi mj bi translated"><a class="ae lk" href="#ea5b" rel="noopener ugc nofollow">KG-增强语言模型</a></li><li id="c3fa" class="mb mc iq kq b kr mk ku ml kx mm lb mn lf mo lj mg mh mi mj bi translated"><a class="ae lk" href="#0c08" rel="noopener ugc nofollow"> KG嵌入&amp;链接预测</a></li><li id="d48f" class="mb mc iq kq b kr mk ku ml kx mm lb mn lf mo lj mg mh mi mj bi translated"><a class="ae lk" href="#044d" rel="noopener ugc nofollow">实体对齐</a></li><li id="52d5" class="mb mc iq kq b kr mk ku ml kx mm lb mn lf mo lj mg mh mi mj bi translated"><a class="ae lk" href="#aa43" rel="noopener ugc nofollow"> KG构建、实体链接、关系提取</a></li><li id="9fa6" class="mb mc iq kq b kr mk ku ml kx mm lb mn lf mo lj mg mh mi mj bi translated"><a class="ae lk" href="#77f2" rel="noopener ugc nofollow"> KGQA:时间、会话和AMR </a></li><li id="a6a4" class="mb mc iq kq b kr mk ku ml kx mm lb mn lf mo lj mg mh mi mj bi translated"><a class="ae lk" href="#af08" rel="noopener ugc nofollow">TL；博士</a></li></ul><p id="ca74" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">对于数据集爱好者，我用💾，因此您可以更轻松地进行搜索和导航。话虽如此，你可能想要在🧭.这个高质量内容的海洋中导航</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/e008f45d56bbaf63e19927257cf49110.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*1vsBqIp4Oo8p7mX2fQFQvg.jpeg"/></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">我尽力了，我保证。作者迷因</p></figure></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h1 id="3683" class="mx my iq bd mz na nb nc nd ne nf ng nh kf ni kg nj ki nk kj nl kl nm km nn no bi translated">神经数据库和检索</h1><p id="4142" class="pw-post-body-paragraph ko kp iq kq b kr np ka kt ku nq kd kw kx nr kz la lb ns ld le lf nt lh li lj ij bi translated">神经修复仍然是发展最快和最热门的领域之一🔥NLP中的主题:它现在可以处理100+ GB规模的数十亿个向量和索引。如果NLP栈足够成熟，我们能否从神经方面接近数据库研究的圣杯，嗯，<em class="nu">数据库</em>？🤨</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi nv"><img src="../Images/ff191e842136deb40f53c3f5cc159131.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*gR6Dp70aD3T-fY3Acv9wxQ.jpeg"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">作者迷因</p></figure><p id="45be" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">是啊！<a class="ae lk" href="https://arxiv.org/pdf/2106.01074.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja"> Thorne等人</strong> </a>引入了<em class="nu">自然语言数据库</em>(表示为<a class="ae lk" href="https://github.com/facebookresearch/NeuralDB" rel="noopener ugc nofollow" target="_blank"> NeuralDB </a> ) <em class="nu"> : </em>没有预定义的严格模式，相反，你可以在写作时将事实存储为文本话语。</p><p id="7dc2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">注意:如果你更喜欢数据库，并把“合适的数据库场所”排在更高的位置，那么基本原则也在同一组作者最近的<a class="ae lk" href="https://dl.acm.org/doi/10.14778/3447689.3447706" rel="noopener ugc nofollow" target="_blank">VLDB 21论文</a>中有所阐述。</p><p id="19bb" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">它是如何工作的？什么是查询引擎？有什么连接吗？(没有联接—不是数据库！)</p><p id="8edb" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">一个引入的NL数据库由K个文本<strong class="kq ja">事实</strong>(本研究中为25-1000个)组成。本质上，对文本事实的查询回答任务被框架化为检索🔎+提取质量保证📑+聚合🧹(to支持<em class="nu">最小/最大/计数</em>查询)。给定一个自然语言问题，我们首先要<em class="nu">检索</em>几个相关事实(<strong class="kq ja">支持事实</strong>)。然后，有了一个查询和<em class="nu"> m </em>个支持集，我们执行一个连接(<strong class="kq ja"> select-project join </strong>，SPJ操作符，好了，现在它有资格成为一个数据库了😀)对每一对(查询、支持)寻找答案或确认其不存在(抽取式QA)。最后，通过简单的后处理来聚合连接结果。</p><p id="39ea" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">🧱Can:我们只是把所有的k个事实连接起来，放在一个大的转换器里？从技术上来说，是的，但是作者表明，当数据库大小超过25个事实时，效率相当低。相比之下，多阶段方法允许并行处理和更好的缩放。目前看来，NL数据库的症结在于检索机制——我们不想创建一个包含所有可能组合的强大集，而是只提取相关的组合。到目前为止，这是通过类似DPR的密集检索(<strong class="kq ja">支持集生成器</strong>)来完成的，它是针对每个查询的带注释的支持集来训练的。</p><p id="7a93" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">说到注释和训练，作者用一组新的数据集支持NLDBs💾Wikidata中的KG三元组在句子中被动词化了(并且你可以生成你自己的DB来改变许多事实)。</p><p id="f1c2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">🧪实验表明，T5和Longformer(具有更大的上下文窗口)在给定黄金检索结果时只能在最小的图上与神经SPJ算子竞争。否则，在25个以上事实的较大数据库上，它们的性能会迅速恶化，而SPJ + SSG则稳定得多。这篇论文对NLP的普通读者来说非常容易理解，绝对是我今年的最爱之一👍！</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi nw"><img src="../Images/3ae7d01d5a75c628d578d056bd6b7b18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AHVUnNYgk2pqpJ_tLutcgg.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">神经数据库架构。来源:<a class="ae lk" href="https://arxiv.org/pdf/2106.01074.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="bd nx">索恩等人</strong> </a></p></figure><p id="3044" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">随着检索变得越来越重要(即使不是在神经数据库的环境中)，ACL'21围绕开创性的<a class="ae lk" href="https://github.com/facebookresearch/DPR" rel="noopener ugc nofollow" target="_blank">密集段落检索</a>及其相关检索器家族拥有丰富的新方法集合。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ny"><img src="../Images/662fea841fda070d07b0237e14e59afb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sJMC6p0eHrnY8NFtenrEQQ.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">来源:</p></figure><p id="a0f2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><a class="ae lk" href="https://arxiv.org/pdf/2106.06830.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja">陈等</strong> </a>解决了<em class="nu">实体消歧</em>的一个常见且重要的信息检索问题，即你有许多实体，它们共享相同的名称(表面形式)但具有不同的属性(👈亚伯·林肯——政治家和亚伯·林肯——音乐家)。</p><p id="432b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了更系统地评估寻回犬，作者设计了一个新的数据集💾<a class="ae lk" href="https://github.com/anthonywchen/AmbER-Sets" rel="noopener ugc nofollow" target="_blank">琥珀色</a>(歧义实体检索)收集自维基百科-维基数据页面对齐。具体来说，数据集强调🌟“受欢迎程度差距”🌟:在大多数情况下，检索器会退回到其索引中最突出的实体(例如，被查看次数最多、内容更多的页面)，我们希望量化这种转变。Wikidata实体和谓词被用作参考KG集合，以生成新的复杂消歧任务(称为<em class="nu">琥珀集</em>)。</p><p id="2142" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">AmbER由两部分组成:<strong class="kq ja"> AmbER-H </strong>(消除人类歧义)和<strong class="kq ja"> AmbER-N </strong>(非人类，如电影、乐队、公司)，并在3个任务中测量性能:QA、槽填充和事实检查。</p><p id="456c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">🧪在实验中，作者表明目前的索塔猎犬确实遭受了低效的歧义消除——在涉及稀有实体的任务中的表现下降了15-20分📉。也就是说，要提高检索器的精度还有很多工作要做。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi nz"><img src="../Images/866a7ed3b70142dc48ecdc8d4d902a26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oZkaUR0FU-C6K_56z2ZCkA.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">来源:<a class="ae lk" href="https://arxiv.org/abs/2106.00882" rel="noopener ugc nofollow" target="_blank"> <strong class="bd nx">山田等人</strong> </a></p></figure><p id="5b64" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现代寻回犬的一个常见计算问题是它们的索引大小:拥有2100万个条目的DPR占用了大约65GB的内存。<a class="ae lk" href="https://arxiv.org/abs/2106.00882" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja"> Yamada等人</strong> </a>提出了一个优雅的解决方案:使用<em class="nu">学习散列</em>的思想让我们训练一个近似<strong class="kq ja">符号函数</strong>的散列层，使得连续向量变成+1/-1的二进制向量。然后，我们可以使用高效的CPU实现<a class="ae lk" href="https://en.wikipedia.org/wiki/Hamming_distance" rel="noopener ugc nofollow" target="_blank">汉明距离</a>来计算粗略的top-K候选(文中为1000)，而不是昂贵的点积(索引上的MIPS)。然后，我们可以很容易地计算一个问题和1000个候选项之间的点积。</p><p id="ad5e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><a class="ae lk" href="https://github.com/studio-ousia/bpr" rel="noopener ugc nofollow" target="_blank"> BPR </a>方法(二进制段落检索器)有几个优点:1️⃣索引大小减少到大约2 GB(从66GB减少到66gb！)而没有大的性能下降(预计只有前1名的准确度受到影响)；2️⃣业务流程再造在<a class="ae lk" href="https://efficientqa.github.io/" rel="noopener ugc nofollow" target="_blank">效率质量挑战</a>中名列前茅👏。总的来说，这是一篇有影响力的短文的很好的例子！</p><p id="f0c8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">📬最后，我将从会议中概述一些更多的以检索为中心的作品:<a class="ae lk" href="https://arxiv.org/pdf/2101.00408.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja"> Sachan等人</strong> </a>检验了逆向完形填空任务和掩蔽显著跨度的预训练如何提高DPR在开放领域问答任务中的表现。<a class="ae lk" href="https://aclanthology.org/2021.acl-long.89.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja">美拉德、卡普欣等人</strong> </a> <strong class="kq ja"> </strong>设计一🌐<em class="nu">通用寻回犬</em>🌐，一个经过多任务训练的检索器，适用于许多NLP任务，并在结合QA、实体链接、槽填充和对话任务的<a class="ae lk" href="https://ai.facebook.com/tools/kilt/" rel="noopener ugc nofollow" target="_blank"> KILT基准</a>上进行评估。<a class="ae lk" href="https://aclanthology.org/2021.acl-demo.25.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja"> Ravfogel等人</strong> </a>展示了一个<a class="ae lk" href="https://spike.apps.allenai.org/datasets/cord19/search?neural#welcome" rel="noopener ugc nofollow" target="_blank">很酷的神经提取搜索系统的演示</a>(通过新冠肺炎数据)供大家玩。</p></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h1 id="ea5b" class="mx my iq bd mz na nb nc nd ne nf ng nh kf ni kg nj ki nk kj nl kl nm km nn no bi translated">kg-增强语言模型:🪴🚿</h1><p id="16e5" class="pw-post-body-paragraph ko kp iq kq b kr np ka kt ku nq kd kw kx nr kz la lb ns ld le lf nt lh li lj ij bi translated">BERTology的一个主要趋势是探索大型LMs的事实知识，例如，输入查询<em class="nu">“史蒂夫·乔布斯出生在[面具]”</em>以预测<em class="nu">“加利福尼亚”</em>。然后，我们可以使用各种基准来量化这些探针，如<a class="ae lk" href="https://github.com/facebookresearch/LAMA" rel="noopener ugc nofollow" target="_blank"> LAMA </a>。换句话说，我们是否可以将<em class="nu">语言模型视为知识库？到目前为止，我们有证据表明LMs可以正确预测一些简单的事实。</em></p><p id="32c7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">但是真的，他们能吗？🤔</p><blockquote class="oa ob oc"><p id="8655" class="ko kp nu kq b kr ks ka kt ku kv kd kw od ky kz la oe lc ld le of lg lh li lj ij bi translated">我们的发现强烈质疑先前文献的结论，并证明当使用基于提示的检索范式时，当前的MLMs不能作为可靠的知识库。——<a class="ae lk" href="https://arxiv.org/pdf/2106.09231.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="kq ja">曹等人</strong> </a></p></blockquote><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi og"><img src="../Images/b65ab736313c334f8cb2d771d3fbc3a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*US5QJfwaseaGCW8UuTL0hg.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">来源:<a class="ae lk" href="https://arxiv.org/pdf/2106.09231.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="bd nx">曹等人</strong> </a></p></figure><p id="5cf6" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><a class="ae lk" href="https://arxiv.org/pdf/2106.09231.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja">曹等人</strong> </a>的工作几乎给整个领域泼了一盆冷水——他们发现，大部分报告的表现都可以归因于<strong class="kq ja"/>🥴的虚假相关性，而不是实际的“知识”。作者研究了3种类型的探测(图示👈):<em class="nu">提示</em>、<em class="nu">案例</em>(又名少拍学习)和<em class="nu">上下文</em>。在所有场景中，LMs表现出许多缺陷，例如，<em class="nu">案例</em>只能帮助识别答案类型(人、城市等)，但不能指向该类中的特定实体。这篇论文非常容易阅读和理解，并且有很多说明性的例子，🖌，所以我建议即使对于那些不积极从事这方面工作的人也要好好读一读。</p><p id="5eff" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">有趣的是，在ACL'21的<a class="ae lk" href="https://arxiv.org/pdf/2106.01561.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja"> Wang等人</strong> </a>也报道了开放域问答中的类似结果。他们分析了巴特和GPT-2的性能，得出了几乎相同的结论。是时候反思我们如何在LMs中打包显性知识了？🤔</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/ccf920493d1741244152f6ea27e9b359.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*JiQt2E3HGOVia5obLEh17w.gif"/></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">当你意识到LMs一直在作弊。来源:<a class="ae lk" href="https://gfycat.com/ru/capitalcleanfossa" rel="noopener ugc nofollow" target="_blank"> gfycat </a></p></figure><p id="2433" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">从前面的帖子中，我们知道存在相当多的Transformer语言模型，这些模型中包含了来自知识图的事实。让我们欢迎两位新家庭成员！👨‍👩‍👦‍👦</p><p id="867c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><a class="ae lk" href="https://openreview.net/pdf?id=CLnj31GZ4cI" rel="noopener ugc nofollow" target="_blank">、王等人、</a>提出<em class="nu"> K-Adapters </em>，一种基于预训练LMs之上的知识注入机制。有了K-Adapters，你就不需要从头开始训练一个大的变压器栈了。相反，作者建议在已经预先训练好的冷冻模型(他们用BERT和RoBERTa做实验)的层间放置几个<em class="nu">适配器层</em>，例如，在第0层、第12层和第23层之后。冻结的LM功能与可学习的<em class="nu">适配器功能</em>连接在一起，并在一组新任务上进行训练——在这里，它是基于对齐的维基百科-维基数据文本三元组的<a class="ae lk" href="https://aclanthology.org/L18-1544.pdf" rel="noopener ugc nofollow" target="_blank"> T-REx数据集</a>的1️⃣关系预测；2️⃣依赖树关系预测。实验表明，这种方法提高了实体分类、常识问答和关系分类任务的性能。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oi"><img src="../Images/171a61fc3fb642d2b1bb39f1e05316bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EYEdg80UIIBUJ5w9En1lOg.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">来源:<a class="ae lk" href="https://openreview.net/pdf?id=CLnj31GZ4cI" rel="noopener ugc nofollow" target="_blank"> <strong class="bd nx">王等人</strong> </a></p></figure><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oj"><img src="../Images/8abaa62e0a1e2d6aaecea77c3c9a3950.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PHuqoGaTcBvplermmAb3eA.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">艾丽卡预训练任务。来源:<a class="ae lk" href="https://arxiv.org/pdf/2012.15022.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="bd nx">秦等人</strong> </a></p></figure><p id="01c1" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">秦等</strong> 设计<em class="nu"> ERICA </em>，一种用实体和关系信息丰富LMs的对比机制。具体来说，他们增加了两个更多的损失标准传销:实体歧视(🖼👈)和关系歧视。在实体辨别的例子中，预训练文档具有成对的注释🍏 🍏实体跨度。要求该模型产生真实对的更高余弦相似度🍏 🍏而不是消极的🍏 🍅通过对比损失项。ERICA在关系预测和多跳QA任务中的低资源微调场景(1–10%的训练数据)中表现特别好。</p></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h1 id="0c08" class="mx my iq bd mz na nb nc nd ne nf ng nh kf ni kg nj ki nk kj nl kl nm km nn no bi translated">KG嵌入和链接预测</h1><p id="0cf8" class="pw-post-body-paragraph ko kp iq kq b kr np ka kt ku nq kd kw kx nr kz la lb ns ld le lf nt lh li lj ij bi translated">多关系KG嵌入模型的长处可以是自身容易受到对抗性攻击的弱点吗？一群算法经常被用来比较它们捕捉某些关系模式的能力，比如对称、反转、组合等等。一个简短的回答是<em class="nu">是</em> :/</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ok"><img src="../Images/142e9ffdefb012f7f8d779b4b2202f15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xDZhoAeaOzExoc7KinrdCw.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">来源:<a class="ae lk" href="https://github.com/pykeen/pykeen#models-31" rel="noopener ugc nofollow" target="_blank"> <strong class="bd nx">巴德瓦杰等人</strong> </a></p></figure><p id="8c3d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">Bhardwaj等人 的一部颇有见地的著作研究了各种类型和方向的中毒🔫通过添加<em class="nu">对抗性三元组</em>嵌入模型(查看示例说明👈).它确实假设我们都可以访问预训练的权重，并且可以执行前转呼叫(<em class="nu">白盒</em>设置)。在<a class="ae lk" href="https://github.com/PeruBhardwaj/InferenceAttack" rel="noopener ugc nofollow" target="_blank">提出搜索<strong class="kq ja">敌对关系</strong>和潜在<strong class="kq ja">诱饵实体</strong>的几种方式</a>后，实验表明最有效的攻击利用了<strong class="kq ja">对称性</strong>🦋模式(至少在标准FB15k-237和WN18RR图上)。有趣的是，没有几何或平移先验的卷积模型ConvE看起来对设计攻击最有弹性🛡，即普通的TransE或DistMult中毒更严重。</p><p id="96f4" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">⚖️本人还概述了<a class="ae lk" href="https://arxiv.org/pdf/2106.07250.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja"> Kamigaito和Hayashi </strong> </a> <strong class="kq ja"> </strong>对用于训练KG嵌入模型的两个流行损失函数家族的理论相似性的长期预期研究:<em class="nu"> softmax交叉熵</em>和<em class="nu">负采样</em>，特别是<em class="nu">自我对抗负采样</em>。在众多的研究中(例如<a class="ae lk" href="https://www.researchgate.net/profile/Asja-Fischer/publication/342436015_Bringing_Light_Into_the_Dark_A_Large-scale_Evaluation_of_Knowledge_Graph_Embedding_Models_Under_a_Unified_Framework/links/5f00cd9f45851550508b2aaa/Bringing-Light-Into-the-Dark-A-Large-scale-Evaluation-of-Knowledge-Graph-Embedding-Models-Under-a-Unified-Framework.pdf" rel="noopener ugc nofollow" target="_blank">无耻之徒</a>或<a class="ae lk" href="https://openreview.net/pdf?id=BkxSmlBFvr" rel="noopener ugc nofollow" target="_blank">来自ICLR的Ruffinelli等人</a>20)，我们已经看到受过这样或那样损失训练的模型表现出相似的性能。最后，在这项工作中，作者通过<a class="ae lk" href="https://en.wikipedia.org/wiki/Bregman_divergence" rel="noopener ugc nofollow" target="_blank"> Bregman散度</a>的透镜来研究它们的理论性质。看完这篇文章你想带回家的两个重要信息:1️⃣ <em class="nu">自我对立的负采样</em>与<em class="nu">带标签平滑的交叉熵</em>非常相似。2️⃣ <em class="nu">交叉熵</em>模型可能比<em class="nu">负抽样</em>模型更适合。如果你忘记用更多的损失函数进行实验，你现在可以引用这篇论文😉</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ol"><img src="../Images/ed42539613457ee6264f9e39128822ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ixSn98Ic1j8skdXwl9672Q.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">来源:<a class="ae lk" href="https://aclanthology.org/2021.acl-long.534.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="bd nx">曹等人</strong> </a></p></figure><p id="5f7f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">🚨<strong class="kq ja">新LP数据集警报</strong>🚨Freebase和Wordnet图表作为基准已经存在太久了，作为一个社区，我们最终应该采用偏差更少、规模更大的新数据集作为2021-2022测试套件。<a class="ae lk" href="https://aclanthology.org/2021.acl-long.534.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja">曹等</strong> </a>探究FB15k-237和WN18RR的测试集并发现(如图👈)通常，测试三元组要么对人类来说是不可预测的，要么没有太大的实际意义。受此激励，他们创建了一组新的数据集💾<a class="ae lk" href="https://github.com/TaoMiner/inferwiki" rel="noopener ugc nofollow" target="_blank"><strong class="kq ja">infer wiki 16K&amp;infer wiki 64K</strong></a>(基于维基数据😍)其中测试案例在列车组中有接地。他们还为三元组分类任务创建了一组<em class="nu">未知的</em>三元组(除了真/假)。🧪The的主要假设在实验中得到证实——当测试三元组在训练中确实有基础时，嵌入模型确实在<em class="nu">非随机分裂</em>上运行得更好。</p><p id="b48d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">让我们欢迎👋链路预测的几种新方法。1️⃣<a class="ae lk" href="https://github.com/justinlovelace/robust-kg-completion" rel="noopener ugc nofollow" target="_blank">BERT-ResNet</a>by<a class="ae lk" href="https://arxiv.org/pdf/2106.06555.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="kq ja">lovelace等人</strong> </a>通过Bert对实体名称和描述进行编码，并通过resnet风格的深度CNN传递三元组，随后进行重新排序和提炼(相当多的内容都放在那里！).该模型产生了很大的改进📈在常识风格的图形上，如SNOMED CT Core和ConceptNet，大量知识编码到文本描述中。接下来，<a class="ae lk" href="https://arxiv.org/pdf/2011.03798.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja"> Chao等人</strong> </a>提出了<a class="ae lk" href="https://github.com/alipay/KnowledgeGraphEmbeddingsViaPairedRelationVectors_PairRE" rel="noopener ugc nofollow" target="_blank"> PairRE </a>，这是RotatE的一个扩展，其中关系嵌入被分成特定于头部和特定于尾部的部分。PairRE在<a class="ae lk" href="https://ogb.stanford.edu/docs/leader_linkprop/#ogbl-wikikg2" rel="noopener ugc nofollow" target="_blank"> OGB </a>数据集上展示了极具竞争力的结果。顺便说一下，模型已经在<a class="ae lk" href="https://github.com/pykeen/pykeen" rel="noopener ugc nofollow" target="_blank"> PyKEEN </a>库中可用，用于训练和评估KG嵌入模型。😉<a class="ae lk" href="https://aclanthology.org/2021.acl-long.365.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja">李等</strong> </a>设计聚类，用于时态KG链接预测的模型。CluSTeR在第一个<em class="nu">线索搜索</em>阶段使用RL，在第二个阶段在它们上面运行<em class="nu"> R-GCN </em>。4️⃣:最后，我很高兴看到更多关于超关系知识的研究！🎇(在这里找到我的<a class="ae lk" rel="noopener" target="_blank" href="/representation-learning-on-rdf-and-lpg-knowledge-graphs-6a92f2660241">评论文章</a>)。<a class="ae lk" href="https://arxiv.org/pdf/2105.08476.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja">王等人</strong> </a>在Transformer的基础上构建了他们的<a class="ae lk" href="https://github.com/PaddlePaddle/Research/tree/master/KG/ACL2021_GRAN" rel="noopener ugc nofollow" target="_blank"> GRAN </a>模型，采用了包括限定词交互在内的改进的注意机制。我很想看看它在我们新的WD50K超关系基准测试中的表现！</p></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h1 id="044d" class="mx my iq bd mz na nb nc nd ne nf ng nh kf ni kg nj ki nk kj nl kl nm km nn no bi translated">实体对齐:2个新数据集💾</h1><p id="3c1c" class="pw-post-body-paragraph ko kp iq kq b kr np ka kt ku nq kd kw kx nr kz la lb ns ld le lf nt lh li lj ij bi translated">在<em class="nu">实体对齐(EA) </em>的任务中，你有两个图(可能共享相同的关系集)，这两个图具有两个不相交的实体集，比如来自英文和中文DBpedia的实体，你必须确定一个图中的哪些实体可以映射到另一个图中。</p><p id="a874" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">多年以来⏳⌛️，实体对齐数据集意味着两个图之间存在完美的1-1映射，但这对于现实世界的任务来说是一个相当人工的假设。最后，<a class="ae lk" href="https://arxiv.org/pdf/2106.02248.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja">孙等人</strong> </a>通过<em class="nu">悬空</em>实体(那些没有各自映射的实体)的概念更正式地研究这种设置。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi om"><img src="../Images/1fd89e53234d21ba670de0d7290c40f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ds1cGmAbt1YWdiRz-3uZ9w.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">来源:<a class="ae lk" href="https://arxiv.org/pdf/2106.02248.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="bd nx">孙等人</strong> </a></p></figure><p id="7dd1" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">作者建立了一个新的数据集💾，<strong class="kq ja"> DBP 2.0 </strong>，其中只有30–50%的实体是“可映射的”，其余的是<em class="nu">悬空</em>。因此，这意味着您的对齐模型必须学习一种方法来决定一个节点是否可以被映射——作者探索了3种可能的方法来做到这一点。</p><p id="03e5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">由于大多数EA基准已经在非常高的值附近饱和，所以看到添加“嘈杂的”实体急剧下降是很有趣的📉整体表现。朝着更实用的设置又迈进了一步！</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi on"><img src="../Images/068b2540947b63f15c60c38864ed52d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oifYGIVROvPI8wIXa1NpEQ.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">来源:<a class="ae lk" href="https://arxiv.org/pdf/2106.01586.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="bd nx">帕胡贾等人</strong> </a></p></figure><p id="b55c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">通常，如果一个图的一些边可以隐式地包含在一些文本中，那么我们就说<em class="nu"> KG-Text </em>对齐。特别是，我们感兴趣的是是否有任何方法可以用文本嵌入来丰富图形嵌入，反之亦然。<a class="ae lk" href="https://arxiv.org/pdf/2106.01586.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja"> Pahuja等人</strong> </a>通过设计一个新颖的数据集，提供了对这个问题的<a class="ae lk" href="https://github.com/dki-lab/joint-kb-text-embedding" rel="noopener ugc nofollow" target="_blank">大规模研究</a>💾源自全英文维基百科和维基数据:1500万个实体和2.61亿个事实🏋。作者分析了4种对齐方法(例如，通过将KG嵌入投影到文本嵌入空间)并联合训练KG /文本嵌入。</p><p id="4e10" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">🧪任务方面，作者测量了少量链接预测(超过KG三元组)和类比推理(超过文本部分)的性能。事实上，与单一模态相比，所有4种对齐方法确实提高了两项任务的质量，例如，在类比推理中，融合KG信息的最佳方法带来了超过<a class="ae lk" href="https://github.com/wikipedia2vec/wikipedia2vec" rel="noopener ugc nofollow" target="_blank"> Wikipedia2Vec </a>基线的16%的绝对提高💪。在链路预测任务中，融合可以产生高达10%的命中@ 1的绝对改进。</p><p id="64ed" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">值得注意的是，该方法假设两个独立的模型进行联合训练。绕过校准问题，在这个新任务中探索KG增强LMs(一个在KG上预训练的模型)肯定是有趣的。</p></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h1 id="aa43" class="mx my iq bd mz na nb nc nd ne nf ng nh kf ni kg nj ki nk kj nl kl nm km nn no bi translated">KG构建，实体链接，关系抽取</h1><p id="34d0" class="pw-post-body-paragraph ko kp iq kq b kr np ka kt ku nq kd kw kx nr kz la lb ns ld le lf nt lh li lj ij bi translated">从文本中自动构建🧩kg是一项非常重要且受欢迎的任务，适用于许多工业应用。</p><p id="3b21" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><a class="ae lk" href="https://arxiv.org/pdf/2106.01167.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja">蒙达尔等人</strong> </a>提出了从<a class="ae lk" href="https://aclanthology.org/" rel="noopener ugc nofollow" target="_blank"> ACL选集</a>(例如，本文所评论的所有论文都属于该文集)中KG构造NLP论文的工作流程。得到的图形称为<a class="ae lk" href="https://github.com/Ishani-Mondal/SciKG" rel="noopener ugc nofollow" target="_blank"> SciNLP-KG </a>。它并不完全是标题中所说的端到端的(作者在第5节中通过错误传播来证明)，而是由3个阶段组成(🖼👇)围绕关系抽取。SciNLP-KG建立在<a class="ae lk" href="https://aclanthology.org/2021.eacl-main.59/" rel="noopener ugc nofollow" target="_blank">之前的研究</a> (NAACL'21)的基础上，提取<strong class="kq ja">任务</strong>、<strong class="kq ja">数据集</strong>和<strong class="kq ja">指标</strong> (TDM) <strong class="kq ja">的提及。</strong>KG模式有4个不同的谓词:<em class="nu"> evaluatedOn </em>、<em class="nu"> evaluatedBy </em>、<em class="nu">co reference</em>和<em class="nu"> related </em>来捕获TDM实体之间的链接。作者构建了两个版本的SciKG:一个小型MVP和一个具有5K个节点和15K条边的成熟版本。这种方法的一个优点是，自动构建的大SciKG与代码为的<a class="ae lk" href="https://paperswithcode.com/" rel="noopener ugc nofollow" target="_blank">论文有很大的重叠(大约50%的实体)！</a></p><p id="b315" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">是的，这只是一个受限领域中的4个关系，但这是一个良好的开端——毫无疑问，更多可扩展的端到端方法将随之而来。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oo"><img src="../Images/2c99d27483fa09a79d6cf28fe12cb891.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9GFuN07JTiTwE-xvVcS7Ig.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">来源:<a class="ae lk" href="https://arxiv.org/pdf/2106.01167.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="bd nx">蒙达尔等人</strong> </a></p></figure><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi op"><img src="../Images/7b970a1dc43e763a676fd191d606c1e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/1*UEGXTfNlMKrhkHmTDqFKVQ.gif"/></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">对已知任务的非传统方法。来源:<a class="ae lk" href="https://www.reddit.com/r/interestingasfuck/comments/if592f/next_time_you_see_a_croc_floating_towards_you/" rel="noopener ugc nofollow" target="_blank"> Reddit </a></p></figure><p id="dfcd" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在像<a class="ae lk" href="https://github.com/facebookresearch/BLINK" rel="noopener ugc nofollow" target="_blank"> BLINK </a>和<a class="ae lk" href="https://github.com/facebookresearch/BLINK/tree/master/elq" rel="noopener ugc nofollow" target="_blank"/>这样的神经实体链接器的时代，<strong class="kq ja">江、Gurajada等人</strong> 的一部作品对这个问题采取了非正统的观点:让我们在一个加权的基于规则的框架(<a class="ae lk" href="https://arxiv.org/pdf/2006.13155.pdf" rel="noopener ugc nofollow" target="_blank">逻辑神经网络</a>)中将文本启发与神经特征结合起来。事实上，<em class="nu">LNN-埃尔</em>是更大的神经符号<em class="nu"> NSQA </em>系统的一个组成部分，但在下面的KGQA部分会有更多的介绍。</p><p id="7163" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">📝该方法<em class="nu">LNN-埃尔</em>，要求实体提及与目标KG的前K名候选人已经在那里。一些文本特征可以是，例如，提及和候选之间的Jaro-Winkler距离或基础KG中的节点中心性。眨眼和神经方法也可以作为特性插入。然后，专家创建一组具有权重的规则，例如，将<em class="nu"> w1 </em>分配给Jaro-Winkler，将<em class="nu"> w2 </em>分配给BLINK，并且通过边际损失和负采样来学习权重。</p><p id="6098" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">🧪 LNN-EL的表现与BLINK相当，并返回一个可解释的加权规则树。此外，它可以推广到使用相同的底层KG的其他数据集👏</p><p id="5c68" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">➖There也存在一些缺点:看起来眨眼实际上是整体表现中的关键因素，占规则总权重的70-80%。因此，自然的问题是——制定复杂的专家规则实际上值得吗？🤔其次，作者使用<a class="ae lk" href="https://lookup.dbpedia.org/" rel="noopener ugc nofollow" target="_blank"> DBpedia lookup </a>来检索top-K候选项，并“假设类似的服务存在或者可以在其他kg上实现”。不幸的是，情况往往并非如此——事实上，这种候选检索系统只存在于DBpedia和(部分)Wikidata中，而对于其他大型kg来说，创建这样的机制非常重要。然而，LNN-EL为KGQA的神经符号实体链接奠定了坚实的基础。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/facf743f88558d1dd7f3be2e591e9b45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*k6BAQR7mxx4eCIDOmJsGyg.gif"/></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">当盒子嵌入尺寸过小时。资料来源:gifsboom.net</p></figure><p id="aa0a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">实体链接通常是紧密相连的🤝使用实体类型。<a class="ae lk" href="https://arxiv.org/pdf/2101.00345.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja"> Onoe等人</strong> </a>用<a class="ae lk" href="https://github.com/yasumasaonoe/Box4Types" rel="noopener ugc nofollow" target="_blank">框嵌入(Box4Types) </a>解决了<em class="nu">细粒度</em>实体类型化(当你有成百上千个类型时)的问题。通常，细粒度的实体被建模为向量，具有编码的提及+上下文向量和所有类型向量的矩阵之间的点积。相反，作者建议从矢量转向📦方框(d维超矩形)。此外，不是“只是盒子”，而是<a class="ae lk" href="https://arxiv.org/pdf/2010.04831.pdf" rel="noopener ugc nofollow" target="_blank"> Gumbel(软)盒子(neur IPS’20)</a>，当“只是盒子”不相交时，允许在角落情况下进行背面投影。🖼👇给出了一个很好的直觉:本质上，我们将所有的相互作用建模为几何运算符📦将它们的体积标为1会给概率解释带来额外的好处。</p><p id="3eac" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">🧪实验表明，盒子至少和更重的基于向量的模型一样好，在某些情况下甚至超过它们5-7个F1分👏。除此之外，还有许多具有洞察力的定性实验。总的来说，我很喜欢阅读这篇论文——强烈推荐它作为一篇优秀论文的范例。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi oq"><img src="../Images/e423c5a4f1fbe29d3a4a4c170cfe7a19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gc0SF7Fm-lvRNIV_qPMWMg.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">来源:<a class="ae lk" href="https://arxiv.org/pdf/2101.00345.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="bd nx"> Onoe等人</strong> </a></p></figure><p id="f712" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">让我们在<strong class="kq ja">关系提取</strong>论文上补充几句，这些论文在几个基准测试中略微改进了SOTA。<a class="ae lk" href="https://aclanthology.org/2021.acl-long.359.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja">胡等</strong> </a>研究预训练的KG实体嵌入如何帮助进行包级关系抽取(实际上只是一点点)，并创建新的<a class="ae lk" href="https://github.com/zig-kwin-hu/how-KG-ATT-help" rel="noopener ugc nofollow" target="_blank">数据集BagRel-Wiki73K </a>💾基于维基数据的实体和关系！<a class="ae lk" href="https://aclanthology.org/2021.acl-long.375.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja">田等</strong> </a>提出了一个<em class="nu"/><em class="nu"/>透视图、<em class="nu"> SteREoRel、</em>关于re的任务，即段落中的实体、关系、词语都可以建模为一个3D立方体。一段的BERT编码被发送到几个解码器，以重建正确的关系三元组。最后，<a class="ae lk" href="https://arxiv.org/pdf/2106.00459.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja">纳德格里等人</strong> </a>提出了<em class="nu"> KGPool </em>，其中来自句子的已知实体诱导出具有后续GCN层和池的局部邻域。</p></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h1 id="77f2" class="mx my iq bd mz na nb nc nd ne nf ng nh kf ni kg nj ki nk kj nl kl nm km nn no bi translated">基于KGs的问题回答:时态、会话、AMR</h1><p id="ef5e" class="pw-post-body-paragraph ko kp iq kq b kr np ka kt ku nq kd kw kx nr kz la lb ns ld le lf nt lh li lj ij bi translated">当代KGQA主要关注经典静态图，即当你有一组固定的实体和边，而问题没有任何时间维度时。</p><p id="0e26" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">⏳但是时候到了！<a class="ae lk" href="https://arxiv.org/pdf/2106.01515.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja"> Saxena等人</strong> </a>介绍了一个对时态kg进行<em class="nu"> QA的大规模任务，那些在边上有时间戳的kg像<code class="fe or os ot ou b">(Barack Obama, position held, POTUS, 2008, 2016)</code>一样表示其有效性。它开启了一系列全新的简单和复杂的问题，围绕着<em class="nu">时间</em>维度:“奥巴马之前/之后谁是总统？”、“奥巴马当总统的时候谁演钢铁侠？”诸如此类。作者创建了一个新的数据集💾CronQuestions (基于维基数据😍)具有超过KG的410K个问题，具有123K个实体、大约200个关系和300K个三元组，并带有时间戳。</em></p><p id="6f4e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">🧐毫不奇怪，BERT和T5不能以任何像样的准确性处理这样的问题，所以作者将<strong class="kq ja"> <em class="nu"> EmbedKGQA </em> </strong>(我们在之前的ACL评论中强调的来自ACL'20的方法<a class="ae lk" rel="noopener" target="_blank" href="/knowledge-graphs-in-natural-language-processing-acl-2020-ebb1f0a6e0b1#1e21">)与预先训练的时态KG嵌入<strong class="kq ja"> <em class="nu"> TNT-ComplEx </em> </strong>(来自ICLR'20评论</a>的<a class="ae lk" href="https://mgalkin.medium.com/knowledge-graphs-iclr-2020-f555c8ef10e3#762b" rel="noopener">)结合起来，看，通过这个系列，您可以与大多数最新的好东西保持同步😉)在一款新的</a><a class="ae lk" href="https://github.com/apoorvumang/CronKGQA" rel="noopener ugc nofollow" target="_blank"> CronKGQA </a>中。本质上，我们将一个句子的BERT嵌入作为一个关系嵌入，并将其传递给静态的&amp;时态评分函数，如下所示。<br/> 🧪Experimentally，CronKGQA对简单问题的命中率约为99%,但对更复杂的问题仍有改进的空间。对于KGQA社区的其他人:看，有一个新的非饱和基准👀！</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ov"><img src="../Images/b60135ae1c28539dcce6c180e86c9159.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EpnQ7XpQdaJ7aigHseh3Eg.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">来源:<a class="ae lk" href="https://arxiv.org/pdf/2106.01515.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="bd nx">萨克森纳等人</strong> </a></p></figure><p id="65ab" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">🗣对话式KGQA处理连续的<em class="nu">问答</em>步骤，其中在生成对底层KG的查询和形成预测时，上下文和对话历史具有更高的重要性。在对话式KGQA中，<strong class="kq ja">后续问题</strong>往往是最难处理的。传统上，对话历史被编码为一个向量，并且没有对最近实体的特殊处理。此外，后续问题中的显式实体命名经常被省略(因为人类通常擅长共指消解)，所以自然的问题是:我们如何才能跟踪当前对话中最相关的实体？</p><p id="e8b2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">🎯蓝和江提出了焦点实体的概念，焦点实体是一个在对话中被讨论的实体，我们很可能会提出后续的问题。<a class="ae lk" href="https://github.com/lanyunshi/ConversationalKBQA" rel="noopener ugc nofollow" target="_blank">方法</a>假设我们可以访问SPARQL端点来动态查询KG(显然，这不是端到端的神经网络，而是我们可以对整个Wikidata的规模更大的图进行操作)。</p><p id="f6ca" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">主要的想法是，我们可以通过计算一个<strong class="kq ja">实体转移图中实体的分布，动态地改变正在进行的对话的焦点。1️⃣ </strong>首先，我们通过围绕对话的起始节点扩展图形(1-2跳)来构建这样一个ETG。2️⃣然后，ETG通过GCN编码器获得更新的实体状态。3️⃣更新的实体状态与<em class="nu">焦点实体预测器</em>(见下图)中的对话历史聚集在一起，该预测器建立关于作为焦点实体的实体分布。4️⃣最后，更新的分布被发送到现成的答案预测器，该预测器返回对当前话语的答案。</p><p id="52d4" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">改变焦点实体的🧪The想法在<a class="ae lk" href="https://convex.mpi-inf.mpg.de/" rel="noopener ugc nofollow" target="_blank">对话问题</a>和<a class="ae lk" href="https://amritasaha1812.github.io/CSQA/" rel="noopener ugc nofollow" target="_blank"> CSQA </a>的对话版本上产生了显著的收益(平均超过强基线10分)💾！最大的误差来源源于不正确的关系预测，因此肯定有改进的空间。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ow"><img src="../Images/16a05a561f7381804012d165c0646e1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_9FdtmZqS_oSK16KnUBLPA.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">来源:<a class="ae lk" href="https://aclanthology.org/2021.acl-long.255.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="bd nx">兰和姜</strong> </a></p></figure><p id="7f3c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">最后，让我们用一个问题来讨论一下普通的KGQA用一个图表给出一个答案。Kapanipathi和来自IBM Research的另外29个人提出了一个巨大的神经符号KGQA系统，<em class="nu"> NSQA </em>，围绕<strong class="kq ja"> AMR解析</strong>构建。NSQA是一个流水线系统，有专门定制的组件🧩.也就是说，输入的问题首先被解析成AMR树(预训练组件1️⃣)，然后树中的实体被链接到背景KG (2️⃣就是上面描述的<em class="nu">lnn-El</em><strong class="kq ja"><em class="nu"/></strong>！).通过AMR树的基于规则的BFS遍历来构建查询图。而关系链接是一个单独的组件<em class="nu"> SemRel </em> (3️⃣在另一篇ACL'21论文中由<a class="ae lk" href="https://aclanthology.org/2021.acl-short.34.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kq ja"> Naseem等人</strong> </a>提出)。</p><p id="e0e2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">NSQA严重依赖AMR帧及其互连来更好地将树解析为SPARQL查询，例如，<em class="nu">“AMR-unknown”</em>节点将是查询中的变量。当然，为了处理AMR输出，在精心创建的规则中投入了大量工作👏另一方面，所有其他组件都以某种方式使用变压器。长得相当漂亮的<em class="nu">的确像极了</em>！🧪实验表明，AMR解析在<a class="ae lk" href="https://github.com/AskNowQA/LC-QuAD" rel="noopener ugc nofollow" target="_blank"> LC-QuAD 1.0 </a>基准上的准确率约为84%(与人类专家创建的结果相比)，而整体NSQA将F1度量提高了约11个点。亲爱的IBM，一些公开可用的源代码会非常方便😉</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi nw"><img src="../Images/44c95546a70a4cd733775716a11271fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xDRhnmhbB9Li9_geGHyacA.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">来源:<a class="ae lk" href="https://arxiv.org/pdf/2012.01707.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="bd nx">卡帕尼帕蒂等人</strong> </a></p></figure></div><div class="ab cl mq mr hu ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="ij ik il im in"><h1 id="af08" class="mx my iq bd mz na nb nc nd ne nf ng nh kf ni kg nj ki nk kj nl kl nm km nn no bi translated">TL；博士；医生</h1><p id="a5e2" class="pw-post-body-paragraph ko kp iq kq b kr np ka kt ku nq kd kw kx nr kz la lb ns ld le lf nt lh li lj ij bi translated">你进入了最后一部分！无论是从目录还是在阅读了一些相关章节后，感谢您的时间和对这一领域的兴趣😍。请在评论中让我知道你对这整个努力和总体形式的看法！</p><p id="5389" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">从神经数据库到问题回答，知识库正被应用于比以往更多的任务中。总的来说，我认为这是一个做KG研究的好时机:你总是可以找到一个合适的位置，解决理论和实践的挑战，这可能会被社区中成千上万的人使用。</p><p id="523d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">期待我们在下一次会议上看到的！</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/929949fb0ce7d8e39e475fde03a2987c.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/1*IKTDE9yMqmBllEr6iov3KA.gif"/></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">下一堆文件！资料来源:tenor.com</p></figure></div></div>    
</body>
</html>