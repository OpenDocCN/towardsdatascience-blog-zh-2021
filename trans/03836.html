<html>
<head>
<title>Overview of Classifiers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分类器概述</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/overview-of-classifiers-d0a0d3eecfd1?source=collection_archive---------22-----------------------#2021-03-30">https://towardsdatascience.com/overview-of-classifiers-d0a0d3eecfd1?source=collection_archive---------22-----------------------#2021-03-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="176e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">对分类、模型概述和其中重要概念的简要介绍</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/8182bf5ded79ac45a1303d42672103b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7QK5ebvML2w7f0iXjVE4jg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">多元高斯分布中不同协方差矩阵的图示。图片作者。</p></figure><h1 id="00eb" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">内容</h1><p id="852c" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">这篇文章是我将要发表的一系列文章的一部分。你可以通过点击<a class="ae mm" href="https://cookieblues.github.io/guides/2021/03/30/bsmalea-notes-3a/" rel="noopener ugc nofollow" target="_blank">这里</a>在我的个人博客上阅读这篇文章的更详细版本。下面你可以看到该系列的概述。</p><h2 id="8fea" class="mn kz it bd la mo mp dn le mq mr dp li lz ms mt lk md mu mv lm mh mw mx lo my bi translated">1.机器学习导论</h2><ul class=""><li id="5e1e" class="mz na it ls b lt lu lw lx lz nb md nc mh nd ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/what-is-machine-learning-91040db474f9">(一)什么是机器学习？</a></li><li id="8a97" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/model-selection-in-machine-learning-813fe2e63ec6"> (b)机器学习中的模型选择</a></li><li id="cd4b" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/the-curse-of-dimensionality-5673118fe6d2"> (c)维度的诅咒</a></li><li id="be3e" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/what-is-bayesian-inference-4eda9f9e20a6"> (d)什么是贝叶斯推理？</a></li></ul><h2 id="d839" class="mn kz it bd la mo mp dn le mq mr dp li lz ms mt lk md mu mv lm mh mw mx lo my bi translated">2.回归</h2><ul class=""><li id="e972" class="mz na it ls b lt lu lw lx lz nb md nc mh nd ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/how-linear-regression-actually-works-theory-and-implementation-8d8dcae3222c"> (a)线性回归的实际工作原理</a></li><li id="0f62" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/how-to-improve-your-linear-regression-with-basis-functions-and-regularization-8a6fcebdc11c"> (b)如何使用基函数和正则化改进您的线性回归</a></li></ul><h2 id="104b" class="mn kz it bd la mo mp dn le mq mr dp li lz ms mt lk md mu mv lm mh mw mx lo my bi translated">3.分类</h2><ul class=""><li id="ab91" class="mz na it ls b lt lu lw lx lz nb md nc mh nd ml ne nf ng nh bi translated"><strong class="ls iu"> (a)分类器概述</strong></li><li id="6b31" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/quadratic-discriminant-analysis-ae55d8a8148a#204a-71584f33e137"> (b)二次判别分析(QDA) </a></li><li id="a29a" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/linear-discriminant-analysis-1894bbf04359"> (c)线性判别分析</a></li><li id="7cf8" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/gaussian-naive-bayes-4d2895d139a"> (d)(高斯)朴素贝叶斯</a></li></ul><h1 id="f007" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">分类器的类型</h1><p id="8797" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">正如在系列文章的第一篇<a class="ae mm" rel="noopener" target="_blank" href="/what-is-machine-learning-91040db474f9">中提到的，在分类中，目标变量的可能值是离散的，我们称这些可能值为“类”。在</a><a class="ae mm" rel="noopener" target="_blank" href="/how-linear-regression-actually-works-theory-and-implementation-8d8dcae3222c"> 2(a) </a>和<a class="ae mm" rel="noopener" target="_blank" href="/how-to-improve-your-linear-regression-with-basis-functions-and-regularization-8a6fcebdc11c"> 2(b) </a>中，我们经历了回归，简而言之，这是指从数据集<strong class="ls iu"> X </strong>中构造一个函数<em class="nn"> h </em> ( <strong class="ls iu"> x </strong>)，该函数为新的值<strong class="ls iu"> x </strong>产生预测值<em class="nn"> t </em>。分类的目的是相同的，除了<em class="nn"> t </em>的值是离散的。</p><p id="dd08" class="pw-post-body-paragraph lq lr it ls b lt no ju lv lw np jx ly lz nq mb mc md nr mf mg mh ns mj mk ml im bi translated">我们将讨论3种不同的方法或分类器类型:</p><ul class=""><li id="9265" class="mz na it ls b lt no lw np lz nt md nu mh nv ml ne nf ng nh bi translated"><strong class="ls iu">生成分类器</strong>，对输入和目标变量Pr的联合概率分布进行建模(<strong class="ls iu"> x </strong>，<em class="nn"> t </em>)。</li><li id="be7a" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated"><strong class="ls iu">鉴别分类器</strong>，其对给定输入变量Pr( <em class="nn"> t </em> | <strong class="ls iu"> x </strong>)的目标的条件概率分布进行建模。</li><li id="763b" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated"><strong class="ls iu">不使用概率模型但直接将输入分配给目标变量的无分布分类器</strong>。</li></ul><p id="c293" class="pw-post-body-paragraph lq lr it ls b lt no ju lv lw np jx ly lz nq mb mc md nr mf mg mh ns mj mk ml im bi translated">这个主题的一个快速声明:<strong class="ls iu">术语会很混乱</strong>，但是当我们跨越这些桥梁时，我们会处理它。</p><h1 id="6d47" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">生成性与鉴别性</h1><p id="c543" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">下面是我们将要讨论的分类器列表:对于<strong class="ls iu">生成分类器</strong>来说，它是<strong class="ls iu">二次判别分析(QDA) </strong>、<strong class="ls iu">线性判别分析(LDA) </strong>和(高斯)<strong class="ls iu">朴素贝叶斯</strong>，它们都是同一模型的特例；对于<strong class="ls iu">歧视分类器</strong>来说是<strong class="ls iu">逻辑回归</strong>；对于<strong class="ls iu">无分布分类器</strong>，我们将看看<strong class="ls iu">感知器</strong>以及<strong class="ls iu">支持向量机(SVM) </strong>。</p><p id="4895" class="pw-post-body-paragraph lq lr it ls b lt no ju lv lw np jx ly lz nq mb mc md nr mf mg mh ns mj mk ml im bi translated">所以，他们都做同样的事情(分类)。哪个最好？你应该用哪一个？好吧，让我们回忆一下“没有免费的午餐”定理，它概括地说，没有一个模型总是比另一个更好。它总是取决于你的数据。也就是说，关于生成性和区别性量词，我们可以说一些事情。Ng和Jordan (2002)发现，重复将朴素贝叶斯和逻辑回归应用于二元分类任务的实验，<strong class="ls iu">朴素贝叶斯(生成型)在数据较少的情况下表现更好，但逻辑回归通常表现更好</strong> [1]。然而，Ulusoy和Bishop (2006)指出，<strong class="ls iu">只有当数据遵循生成模型的假设时才会出现这种情况</strong>【2】，这意味着逻辑回归(判别)通常优于朴素贝叶斯(生成)。</p><p id="081c" class="pw-post-body-paragraph lq lr it ls b lt no ju lv lw np jx ly lz nq mb mc md nr mf mg mh ns mj mk ml im bi translated"><strong class="ls iu">普遍的共识是，在大多数情况下，判别模型优于生成模型</strong>。其原因是，生成模型在某种程度上有更困难的工作，因为它们试图模拟联合分布，而不仅仅是后验分布。他们还经常对数据做出不切实际的假设。然而，怎么强调都不为过，尽管情况并非总是如此，而且<strong class="ls iu">你不应该忽视生成模型</strong>。例如，生成对抗网络(GANs)是一种生成模型，已经证明在各种任务中非常有用。还有一些其他的原因可以解释为什么你不应该忽视生成模型，例如，它们更容易适应。无论如何，我们在这里不是要找出使用哪种模型，而是要了解这两种模型。</p><h1 id="8623" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">重要工具</h1><h2 id="01d7" class="mn kz it bd la mo mp dn le mq mr dp li lz ms mt lk md mu mv lm mh mw mx lo my bi translated">多元高斯分布</h2><p id="d02b" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在接下来的文章中，我们将非常依赖多元高斯(正态)分布，掌握它非常重要。多元高斯分布表示为<em class="nn"> N </em> ( <strong class="ls iu"> <em class="nn"> μ </em> </strong>，<strong class="ls iu">σ</strong>)，其中<strong class="ls iu"> <em class="nn"> μ </em> </strong>为均值向量，<strong class="ls iu">σ</strong>为协方差矩阵。<em class="nn"> D </em>维的概率密度函数定义为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/8b9e79b3de5ac433c0528142635b7a3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1024/1*5Ttn0lKTw651dNri26dcjg.gif"/></div></figure><p id="ad23" class="pw-post-body-paragraph lq lr it ls b lt no ju lv lw np jx ly lz nq mb mc md nr mf mg mh ns mj mk ml im bi translated">协方差矩阵决定了高斯分布的形状，对于我们将要研究的分类器来说是一个重要的概念。下图展示了不同类型的协方差矩阵。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/f3806373230948283197650db8d3f589.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-zSeWxaP2QGMtzanvMhkPw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">多元高斯分布中不同协方差矩阵的图示。图片作者。</p></figure><h2 id="c678" class="mn kz it bd la mo mp dn le mq mr dp li lz ms mt lk md mu mv lm mh mw mx lo my bi translated">贝叶斯定理</h2><p id="298a" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们要使用的另一个重要工具是贝叶斯定理。如果你没有读过关于频率主义和贝叶斯主义的文章，那么这里有一个关于贝叶斯定理的快速回顾。给定两个事件<em class="nn"> A </em>和<em class="nn"> B </em>，我们可以用条件概率扩展它们的联合概率</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/05caddae9bf542bc4ec273b206ed9471.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/1*CZfqZxmH24zqRFvJkHh6tw.gif"/></div></figure><p id="129e" class="pw-post-body-paragraph lq lr it ls b lt no ju lv lw np jx ly lz nq mb mc md nr mf mg mh ns mj mk ml im bi translated">利用右边的等式，我们可以改写它，得到贝叶斯定理</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/cfc9127a39daf3c6b7b532838fa5c622.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/1*xG7-FX0qJGyCVjFoav7qZA.gif"/></div></figure><p id="04db" class="pw-post-body-paragraph lq lr it ls b lt no ju lv lw np jx ly lz nq mb mc md nr mf mg mh ns mj mk ml im bi translated">就假设和数据而言，我们经常使用后验、似然、先验和证据来指代贝叶斯定理的部分</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/e3cdd8f0a949729266e0ce85049c8979.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/1*U9fxykxHlmrfNe2YjfIh-w.gif"/></div></figure><p id="5a8d" class="pw-post-body-paragraph lq lr it ls b lt no ju lv lw np jx ly lz nq mb mc md nr mf mg mh ns mj mk ml im bi translated">我们经常这样写</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/a2e72c20ec35971ca48e97df43dc41bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/1*xLdf0_WrH9a4J3OY13aaUQ.gif"/></div></figure><p id="e27d" class="pw-post-body-paragraph lq lr it ls b lt no ju lv lw np jx ly lz nq mb mc md nr mf mg mh ns mj mk ml im bi translated">其中∝表示“成比例”。</p><h1 id="efc0" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">参考</h1><p id="6f45" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">[1] Andrew Y. Ng和Michael I. Jordan，“判别型和生成型分类器:逻辑回归和朴素贝叶斯的比较”，2001年。</p><p id="86de" class="pw-post-body-paragraph lq lr it ls b lt no ju lv lw np jx ly lz nq mb mc md nr mf mg mh ns mj mk ml im bi translated">[2] Ilkay Ulusoy和Christopher Bishop，“用于对象检测和分类的生成和鉴别技术的比较”，2006年。</p></div></div>    
</body>
</html>