<html>
<head>
<title>Creating Custom Activation Functions with Lambda Layers in TensorFlow 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在TensorFlow 2中使用Lambda层创建自定义激活函数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/creating-custom-activation-functions-with-lambda-layers-in-tensorflow-691398b8a52d?source=collection_archive---------17-----------------------#2021-06-18">https://towardsdatascience.com/creating-custom-activation-functions-with-lambda-layers-in-tensorflow-691398b8a52d?source=collection_archive---------17-----------------------#2021-06-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="648a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">学习在TensorFlow 2中使用lambda层创建一个简单的自定义ReLU激活函数</h2></div><p id="be68" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">之前我们已经看到了如何创建自定义损失函数— <a class="ae le" rel="noopener" target="_blank" href="/creating-custom-loss-functions-using-tensorflow-2-96c123d5ce6c">使用TensorFlow 2 </a>创建自定义损失函数</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi lf"><img src="../Images/53bd4a2ee27d4d4dc83ee680db0ea1fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*ZcMeKogXSwYU7gQXc7LlAA.png"/></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">自定义ReLU函数(来源:图片由作者创建)</p></figure><p id="7c48" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">简介</strong></p><p id="848e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本文中，我们看看如何创建自定义激活函数。虽然TensorFlow已经包含了一系列内置的激活函数，但仍有方法可以创建您自己的自定义激活函数或编辑现有的激活函数。</p><p id="5acc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">ReLU(整流线性单元)仍然是任何神经网络结构的隐藏层中最常用的激活函数。ReLU也可以表示为函数f(x ),其中，</p><p id="de67" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">f(x) = 0，当x&lt;0,</p><p id="33ef" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">and, f(x) = x, when x ≥ 0.</p><p id="97cd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Thus the function takes into consideration only the positive part, and is written as,</p><p id="263e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">T5】f(x)= max(0，x)T7】</strong></p><p id="0a2c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">或者在代码表示中，</p><pre class="lg lh li lj gt ls lt lu lv aw lw bi"><span id="8137" class="lx ly it lt b gy lz ma l mb mc"><strong class="lt iu">if input &gt; 0:<br/>   return input<br/>else:<br/>   return 0</strong></span></pre><p id="e81c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是这个ReLU函数是预定义的。如果我们想自定义这个函数或者创建自己的ReLU激活怎么办？在TensorFlow中有一个非常简单的方法来做到这一点——我们只需使用<em class="lr"> Lambda层</em>。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="me mf di mg bf mh"><div class="gh gi md"><img src="../Images/815c20fcd302b783382825b091e53a24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X8ELRHDrnL7StobXa-cnpQ.png"/></div></div><p class="ln lo gj gh gi lp lq bd b be z dk translated">ReLU和GeLU激活功能(资料来源:由Ringdongdang —自己的作品，CC BY-SA 4.0，<a class="ae le" href="https://commons.wikimedia.org/w/index.php?curid=95947821" rel="noopener ugc nofollow" target="_blank">https://commons.wikimedia.org/w/index.php?curid=95947821</a>)</p></figure><p id="78c4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">如何使用lambda图层？</strong></p><pre class="lg lh li lj gt ls lt lu lv aw lw bi"><span id="8457" class="lx ly it lt b gy lz ma l mb mc"><strong class="lt iu">tf.keras.layers.Lambda(lambda x: tf.abs(x))</strong></span></pre><p id="7c92" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Lambda只是另一个可以在TensorFlow中直接调用的层。在lambda层中，首先指定参数。在上面的代码片段中，这个值是‘x’(λx)。在这种情况下，我们希望找到x的绝对值，所以我们使用tf.abs(x)。所以如果x的值为-1，这个lambda层会将x的值更改为1。</p><p id="eae5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">如何使用lambda图层创建自定义ReLU？</strong></p><pre class="lg lh li lj gt ls lt lu lv aw lw bi"><span id="a128" class="lx ly it lt b gy lz ma l mb mc"><strong class="lt iu">def custom_relu(x):<br/>    return K.maximum(0.0,x)</strong></span><span id="4c75" class="lx ly it lt b gy mi ma l mb mc"><strong class="lt iu">model = tf.keras.models.Sequential([<br/>     tf.keras.layers.Flatten(input_shape=(128,128)),<br/>     tf.keras.layers.Dense(512),<br/>     tf.keras.layers.Lambda(custom_relu),<br/>     tf.keras.layers.Dense(5, activation = 'softmax')<br/>])</strong></span></pre><p id="1a6a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面的代码片段展示了如何在TensorFlow模型中实现自定义ReLU。我们创建一个函数custom_relu，并返回最大值0或x(与relu函数一样)。</p><p id="3cc8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在下面的顺序模型中，在密集层之后，我们创建一个Lambda层，并在自定义激活函数中传递它。但是这段代码仍然没有做任何与ReLU激活函数不同的事情。</p><p id="9c10" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当我们开始摆弄自定义函数的返回值时，乐趣就开始了。假设我们取0.5和x的最大值，而不是0和x的最大值。然后可以根据需要更改这些值。</p><p id="7321" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="lr">def custom _ relu(x):<br/>return k . maximum(0.5，x) </em></p><pre class="lg lh li lj gt ls lt lu lv aw lw bi"><span id="daa4" class="lx ly it lt b gy lz ma l mb mc"><strong class="lt iu">def custom_relu(x):<br/>    return K.maximum(0.5,x)</strong></span><span id="78c5" class="lx ly it lt b gy mi ma l mb mc"><strong class="lt iu">model = tf.keras.models.Sequential([<br/>     tf.keras.layers.Flatten(input_shape=(128,128)),<br/>     tf.keras.layers.Dense(512),<br/>     tf.keras.layers.Lambda(custom_relu),<br/>     tf.keras.layers.Dense(5, activation = 'softmax')<br/>])</strong></span></pre><p id="6ed3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">在mnist数据集上使用lambda激活的示例</strong></p><pre class="lg lh li lj gt ls lt lu lv aw lw bi"><span id="e0ce" class="lx ly it lt b gy lz ma l mb mc"><strong class="lt iu"><em class="lr">#using absolute value (Lambda layer example 1)</em></strong></span><span id="5fae" class="lx ly it lt b gy mi ma l mb mc"><strong class="lt iu">import tensorflow as tf<br/>from tensorflow.keras import backend as K</strong></span><span id="c621" class="lx ly it lt b gy mi ma l mb mc"><strong class="lt iu">mnist = tf.keras.datasets.mnist</strong></span><span id="318f" class="lx ly it lt b gy mi ma l mb mc"><strong class="lt iu">(x_train, y_train),(x_test, y_test) = mnist.load_data()<br/>x_train, x_test = x_train / 255.0, x_test / 255.0</strong></span><span id="2261" class="lx ly it lt b gy mi ma l mb mc"><strong class="lt iu">model = tf.keras.models.Sequential([<br/>  tf.keras.layers.Flatten(input_shape=(28, 28)),<br/>  tf.keras.layers.Dense(128),<br/>  tf.keras.layers.Lambda(lambda x: tf.abs(x)), <br/>  tf.keras.layers.Dense(10, activation='softmax')<br/>])</strong></span><span id="0618" class="lx ly it lt b gy mi ma l mb mc"><strong class="lt iu">model.compile(optimizer='adam',<br/>              loss='sparse_categorical_crossentropy',<br/>              metrics=['accuracy'])</strong></span><span id="5399" class="lx ly it lt b gy mi ma l mb mc"><strong class="lt iu">model.fit(x_train, y_train, epochs=5)<br/>model.evaluate(x_test, y_test)</strong></span></pre><p id="6f01" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">用mnist数据集上的绝对值替换ReLU激活给出了97.384%的测试准确度。</p><pre class="lg lh li lj gt ls lt lu lv aw lw bi"><span id="a339" class="lx ly it lt b gy lz ma l mb mc"><strong class="lt iu"><em class="lr">#using custom ReLU activation (Lambda layer example 2)</em></strong></span><span id="72d4" class="lx ly it lt b gy mi ma l mb mc"><strong class="lt iu">import tensorflow as tf<br/>from tensorflow.keras import backend as K</strong></span><span id="924c" class="lx ly it lt b gy mi ma l mb mc"><strong class="lt iu">mnist = tf.keras.datasets.mnist</strong></span><span id="03c1" class="lx ly it lt b gy mi ma l mb mc"><strong class="lt iu">(x_train, y_train),(x_test, y_test) = mnist.load_data()<br/>x_train, x_test = x_train / 255.0, x_test / 255.0</strong></span><span id="126c" class="lx ly it lt b gy mi ma l mb mc"><strong class="lt iu">def my_relu(x):<br/>    return K.maximum(-0.1, x)</strong></span><span id="fca7" class="lx ly it lt b gy mi ma l mb mc"><strong class="lt iu">model = tf.keras.models.Sequential([<br/>    tf.keras.layers.Flatten(input_shape=(28, 28)),<br/>    tf.keras.layers.Dense(128),<br/>    tf.keras.layers.Lambda(my_relu), <br/>    tf.keras.layers.Dense(10, activation='softmax')<br/>])</strong></span><span id="f4c0" class="lx ly it lt b gy mi ma l mb mc"><strong class="lt iu">model.compile(optimizer='adam',<br/>              loss='sparse_categorical_crossentropy',<br/>              metrics=['accuracy'])</strong></span><span id="27a4" class="lx ly it lt b gy mi ma l mb mc"><strong class="lt iu">model.fit(x_train, y_train, epochs=5)<br/>model.evaluate(x_test, y_test)</strong></span></pre><p id="454d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在mnist数据集上，用定制的ReLU激活替换ReLU激活，取最大值-0.1或x，给出97.778%的测试准确度。</p><p id="7b1e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">结论</strong></p><p id="e3c0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">尽管lambda层使用起来非常简单，但它们有许多限制。在下一篇文章中，我将讲述如何在TensorFlow中创建完全自定义的层，这些层也是可训练的。</p></div></div>    
</body>
</html>