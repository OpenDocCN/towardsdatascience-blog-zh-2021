<html>
<head>
<title>Graph Neural Networks: A learning journey since 2008— Part 1</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">图形神经网络:2008年以来的学习之旅——第一部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/graph-neural-networks-a-learning-journey-since-2008-part-1-7df897834df9?source=collection_archive---------17-----------------------#2021-09-22">https://towardsdatascience.com/graph-neural-networks-a-learning-journey-since-2008-part-1-7df897834df9?source=collection_archive---------17-----------------------#2021-09-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="105c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">图形神经网络获得了越来越多的成功，但它们到底是什么？它们是如何工作的？让我们一起来看看这些故事中的图表吧！今日:斯卡塞利关于图形神经网络的开创性论文</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/32a18097fd538ab4fee9349da609aff6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BNRoH9AptCz-Hwav2hclWw.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图片由<a class="ae le" href="https://unsplash.com/@wirhabenzeit" rel="noopener ugc nofollow" target="_blank">张秀坤·施罗德</a>在<a class="ae le" href="https://unsplash.com/photos/FIKD9t5_5zQ" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><div class="lf lg gp gr lh li"><a href="https://medium.com/@stefanobosisio1/membership" rel="noopener follow" target="_blank"><div class="lj ab fo"><div class="lk ab ll cl cj lm"><h2 class="bd iu gy z fp ln fr fs lo fu fw is bi translated">通过我的推荐链接加入Medium-Stefano Bosisio</h2><div class="lp l"><h3 class="bd b gy z fp ln fr fs lo fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="lq l"><p class="bd b dl z fp ln fr fs lo fu fw dk translated">medium.com</p></div></div><div class="lr l"><div class="ls l lt lu lv lr lw ky li"/></div></div></a></div><p id="e459" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">数据通常很容易被形象化并解释为图表。除了揭示数据点之间的潜在关系，图表还有助于在复杂的模式中找到隐藏的答案。因此，图形应用无处不在也就不足为奇了，从社交媒体分析[1–5]到神经科学[6，7]，页面排名[8–10]，最短路径理论[11–14]和化学[15–19]。</p><p id="fb1c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">自2006年以来，随着图形神经网络应用的新概念，图论与机器学习有了密切的联系。Scarselli和Gori [20]在2006年发表了第一个建议，随后在2008年[21]通过论文“<em class="lx">图神经网络模型</em>”对其进行了概括。在这里，作者为现代图形神经网络奠定了数学基础。从那以后，文献中出现了graph ML作品[22–28]的高峰，这使得图形世界越来越进化，更精确地定义了这些数学结构的关键元素，以及如何将它们与更精确的机器学习算法联系起来。</p><p id="6dad" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我认为每一篇关于图论和ML的论文都是一次冒险，作者们正在建立他们的术语和数学发展。因此，我想开始这一系列关于图形的教程，用严密的数学和Python中的计算例子给出一个简单的解释。今天，我们将从斯卡塞利的主要论文“图形神经网络模型”开始这一旅程。让我们开始吧！</p><h1 id="dc7e" class="ly lz it bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">图形神经网络模型</h1><h2 id="175a" class="mw lz it bd ma mx my dn me mz na dp mi kb nb nc mm kf nd ne mq kj nf ng mu nh bi translated">定义</h2><p id="b27d" class="pw-post-body-paragraph jq jr it js b jt ni jv jw jx nj jz ka kb nk kd ke kf nl kh ki kj nm kl km kn im bi translated">图形神经网络(GNN)源于两种ML技术:<em class="lx">递归神经网络</em>(RNN)【30–32】和<em class="lx">马尔可夫链</em>【33–35】，其中的基本思想是使用图形编码数据，利用节点之间的关系。特别地，RNN方法是<em class="lx">图集中的</em>，其中用户的目标是在用带标签的图训练之后给给定的图加标签。马尔可夫链方法是<em class="lx">以节点为中心的</em>，其中每个节点都被标记，用户正在寻找节点级的预测。GNN模型封装了从这些以前的模型中获得的知识，适用于以图形和节点为中心的应用程序。</p><p id="ae9a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">此外，GNN推广了基于图的计算，以处理<em class="lx">对称</em>和<em class="lx">非对称</em> <em class="lx">图，</em>图1。如果在修改节点位置后，节点之间的属性不变，则该图被定义为对称的，即您总是获得相同的图。相反，在非对称图中，顺序非常重要，因为通过改变节点的位置可以获得不同的图。在不对称的情况下，一对节点定义了一条称为<em class="lx">弧</em>的边，并且该图被称为<em class="lx">有向</em>——即，它具有特定的方向。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nn"><img src="../Images/76992edd9b03eee6de48ac2a251767f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b_zew9smCBQrkoyr-8VeOA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图1:A)对称图形和B)不对称图形的例子。在前一种情况下，如果改变节点的顺序，图形不会改变，而在不对称情况下，图形在节点中具有特定的顺序。作者图片</p></figure><h2 id="9271" class="mw lz it bd ma mx my dn me mz na dp mi kb nb nc mm kf nd ne mq kj nf ng mu nh bi translated">计算洞察力</h2><p id="1d0f" class="pw-post-body-paragraph jq jr it js b jt ni jv jw jx nj jz ka kb nk kd ke kf nl kh ki kj nm kl km kn im bi translated">至于计算图，每个节点都被赋予了一些属性，这些属性被收集到一个标签<em class="lx">中</em>。标签是具有预定义维度的向量，其编码描述每个节点的特征(例如，图像中像素的平均颜色、对象的形状等)和节点-边缘之间的关系(例如，图像识别中感兴趣的点之间的距离等)。).给定基本的特征条件，图结构与简单的神经网络显著不同。事实上，每个节点都可以知道在其邻居中正在发生什么，并且可以看到相邻节点的当前状态(或特征)(图2)</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi no"><img src="../Images/1c9913db05926b63376c71d4327a323d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wsR-LD2UKKe2tem2CGe2mg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图2:让我们检查一个图形网络。所有节点都有它们自己的标签(l1，l2，l3，l4)以及状态向量x (x1，x2，x3，x4)和边信息(l12，l13，l14，l15)。由于转移函数fw，每个节点的状态，例如节点1 x1，是所有相邻节点的状态的函数。作者图片</p></figure><p id="5b57" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">因此，可以将一个图定义为一个数学对象。每个节点都有一个定义的状态<em class="lx"> x </em> ₙ，其中包含节点的特征以及邻居的特征。此外，每个节点具有相关联的<em class="lx">局部转移函数f𝓌 </em>，其对特征<em class="lx"> x </em> ₙ相对于节点<em class="lx"> n </em>的上下文的依赖性进行建模，以及描述每个节点的输出如何产生的局部输出函数<em class="lx"> g𝓌 </em>。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi np"><img src="../Images/c21dd987d5dd954955865a69c57b8cc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:836/format:webp/1*e72ha_nGehKiKcL004h44A.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">情商。1:图形网络的位置形式。节点n的状态x_n取决于节点标签ln、边标签l_co[n]、邻居状态x_ne[n]和邻居节点标签l_ne[n]。上的输出取决于本地输出函数gw，它是通过节点n的状态x_n和标签l_n计算的。</p></figure><p id="861b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">到目前为止一切顺利，但在处理与f𝓌.的关系时有问题根据邻域大小或邻居集合未排序的情况，可能会有数量可变的自变量。因此<em class="lx"> f𝓌.</em>应该是<em class="lx">不变量</em>对邻域中节点的排列。为了满足这种数学和技术上的约束，可以将<em class="lx"> f𝓌 </em>推广为<em class="lx"> h𝓌.的函数</em>这个最后的转移函数对于图中的节点位置和节点数量是不变的，并且它取决于节点<em class="lx"> n </em>和<em class="lx"> u </em>之间的弧线的给定标签</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/969df8181727befbf99a9064cdebe4a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:834/format:webp/1*yn_Ia5D6kCnc7OoBMpJq8A.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">情商。2:方程1的非正形式。在这种情况下，我们可以通过一般的转移函数h_w来定义节点n的状态x_n，该转移函数依赖于节点标号l_n、节点n和节点u之间的弧标号l_(n，u)、节点u的状态x_u和节点u的标号l_u。</p></figure><h2 id="114a" class="mw lz it bd ma mx my dn me mz na dp mi kb nb nc mm kf nd ne mq kj nf ng mu nh bi translated">一个要求:Banach不动点定理</h2><p id="f4bb" class="pw-post-body-paragraph jq jr it js b jt ni jv jw jx nj jz ka kb nk kd ke kf nl kh ki kj nm kl km kn im bi translated">一旦定义了一个图的主要函数和结构，就需要一个状态<em class="lx"> x </em> ₙ的计算——我们需要定义a .斯卡塞利处理GNN计算的核心点是<a class="ae le" href="https://en.wikipedia.org/wiki/Banach_fixed-point_theorem" rel="noopener ugc nofollow" target="_blank"> <em class="lx">巴拿赫不动点定理</em> </a> <em class="lx">。</em> Banach不动点定理也叫c <em class="lx">压缩映射定理。</em>该定理建立了在定义的度量空间中压缩映射函数的不动点的存在唯一性。这是什么意思？</p><ul class=""><li id="ed02" class="nr ns it js b jt ju jx jy kb nt kf nu kj nv kn nw nx ny nz bi translated"><a class="ae le" href="https://en.wikipedia.org/wiki/Fixed_point_(mathematics)" rel="noopener ugc nofollow" target="_blank">数学中的不动点是给定函数F的解，即当函数是恒等式时</a></li><li id="267b" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn nw nx ny nz bi translated">然后，考虑距离的概念，即两点<em class="lx"> a </em>和<em class="lx"> b </em>相距多远。给定一组数字<em class="lx"> X，</em>，可以计算这些点之间的距离。距离度量定义了一个新的空间，称为度量空间，由集合<em class="lx"> X </em>和距离函数<em class="lx"> (X，d) </em>的结果表示</li><li id="e582" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn nw nx ny nz bi translated">现在，在这个度量空间中，我们可能有一个数学函数<em class="lx"> F </em>，它可以应用到点<em class="lx"> a </em>和<em class="lx"> b </em>来检索另一个度量。如果结果点<em class="lx"> F(a) </em>和<em class="lx"> F(b) </em>之间的距离小于<em class="lx"> a </em>和<em class="lx"> b </em>之间的真实距离，则<em class="lx"> F </em>为<em class="lx">收缩图</em>。</li><li id="c6f2" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn nw nx ny nz bi translated">如果<em class="lx"> F </em>是一个压缩映射，那么Banach证明了对于这样一个函数存在一个不动点解</li></ul><p id="9033" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">这样一个定理的结果是，有可能用迭代方案计算当前节点的状态。我们可以说，基本上每次都更新节点状态，以便找到正确的标签。</p><p id="1518" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">此时，我们几乎准备好进入生产阶段。但是，对于<em class="lx"> f𝓌/h𝓌 </em>和<em class="lx"> g𝓌 </em>要有什么函数才能满足巴拿赫不动点定理呢？惊喜，惊喜，这里是GNN的第二个重要部分:这些功能可以是简单的多层感知器(MLP)。事实上，MLP可以利用通用近似理论，所以他们满足巴拿赫的要求。最初，每个单元都启动了随机状态。<em class="lx"> f𝓌/h𝓌 </em>函数将更新这些状态，直到实现收敛，即找到定点解。然后，可以使用<em class="lx"> g𝓌 </em>进行最终的MLP计算，这将依次返回每个节点或图形本身的输出预测。</p><p id="1c9a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">既然我们已经从理论上了解了GNN是如何工作的，请继续关注下一篇文章，我们将通过计算实现GNN！</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi of"><img src="../Images/ef4f4b9e6456390f93d284dfca709974.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l3Bg9vWYf2mAqJNaBVz-xw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图3:图形神经网络(GNN)的最终视图。原始图可以被看作是从时间T到时间T+步骤的时间上的步骤的组合，其中每个功能接收输入的组合。fina展开图每一层对应一个时刻，并有一个先前步骤的所有单位的副本。</p></figure></div><div class="ab cl og oh hx oi" role="separator"><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol"/></div><div class="im in io ip iq"><p id="c6a0" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">如有任何问题或意见，请随时发送电子邮件至:stefanobosisio1@gmail.com或直接发送至Medium。</p><h1 id="1927" class="ly lz it bd ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv bi translated">文献学</h1><ol class=""><li id="0cf9" class="nr ns it js b jt ni jx nj kb on kf oo kj op kn oq nx ny nz bi translated">艾索普斯、福蒂斯、乔治·帕帕达基斯和提奥多拉·瓦里古。“使用n元图对社交媒体内容进行情感分析。”<em class="lx">第三届ACM SIGMM社交媒体国际研讨会论文集</em>。2011</li><li id="d0b0" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn oq nx ny nz bi translated">威廉·坎贝尔、查理·k·达格里和克利福德·j·温斯坦。"带有内容和图表的社会网络分析."<em class="lx">林肯实验室期刊</em>20.1(2013):61–81。</li><li id="9722" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn oq nx ny nz bi translated">《推特对话图的重建和分析》<em class="lx">首届ACM跨学科社会网络研究热点国际研讨会论文集</em>。2012.</li><li id="dd5e" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn oq nx ny nz bi translated">推特时间演变分析:比较事件和话题驱动的转发图。<em class="lx"> IADIS国际计算机科学杂志&amp;信息系统</em> 11.2 (2016)。</li><li id="2fbd" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn oq nx ny nz bi translated">伊曼纽·罗西等人，《动态图上深度学习的时态图网络》arXiv预印本arXiv:2006.10637  (2020)。</li><li id="f864" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn oq nx ny nz bi translated">丹尼尔·s·巴塞特、佩里·朱恩和约书亚·I·戈尔德。"网络神经科学中模型的本质和使用."<em class="lx">自然评论神经科学</em>19.9(2018):566–578。</li><li id="306b" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn oq nx ny nz bi translated">斯波恩斯，奥拉夫。"图论方法:在大脑网络中的应用."<em class="lx">临床神经科学对话</em> 20.2 (2018): 111。</li><li id="b9b2" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn oq nx ny nz bi translated">阿贝丁、巴巴克和巴巴克·索拉比。"图论在网站链接结构改进中的应用及网页排序."<em class="lx">行为&amp;信息技术</em>28.1(2009):63–72。</li><li id="ff0d" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn oq nx ny nz bi translated">乔治·梅加布。"谷歌的网页排名适用于不同的拓扑网络图结构."<em class="lx">美国信息科学与技术学会杂志</em>52.9(2001):736–747。</li><li id="3930" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn oq nx ny nz bi translated">《网页排序的图形神经网络》。<em class="lx">2005年IEEE/WIC/ACM网络智能国际会议(WI'05) </em>。IEEE，2005年。</li><li id="ffb0" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn oq nx ny nz bi translated">戈德堡，安德鲁v，和克里斯哈里森。"计算最短路径:搜索符合图论."<em class="lx">汽水</em>。第五卷。2005.</li><li id="4570" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn oq nx ny nz bi translated">盖洛、乔治和斯特凡诺·帕洛蒂诺。"最短路径算法。"<em class="lx">运筹学年鉴</em>13.1(1988):1–79。</li><li id="5588" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn oq nx ny nz bi translated">博格瓦德，卡斯滕m，和汉斯-彼得克里格尔。"图上的最短路径核."<em class="lx">第五届IEEE数据挖掘国际会议(ICDM 05)</em>。IEEE，2005年。</li><li id="57ca" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn oq nx ny nz bi translated">网格图的最短路径算法。网络7.4(1977):323–334。</li><li id="7010" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn oq nx ny nz bi translated">图论在化学中的应用。化学信息和计算机科学杂志25.3(1985):334–343。</li><li id="020e" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn oq nx ny nz bi translated">拓扑量子化学的图表理论数据。<em class="lx">体检E </em> 96.2 (2017): 023310。</li><li id="d2d2" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn oq nx ny nz bi translated">拓扑量子化学的能带连接性:作为图论问题的能带结构。<em class="lx">体检B </em> 97.3 (2018): 035138。</li><li id="76d5" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn oq nx ny nz bi translated">预测化学反应性的图形卷积神经网络模型。<em class="lx">化学科学</em>10.2(2019):370–377。</li><li id="6354" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn oq nx ny nz bi translated">基于自我注意的信息传递神经网络预测分子亲脂性和水溶性。化学信息学杂志12.1(2020):1–9。</li><li id="d520" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn oq nx ny nz bi translated">《网页排序的图形神经网络》。2005年IEEE/WIC/ACM网络智能国际会议。IEEE，2005年。</li><li id="8719" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn oq nx ny nz bi translated">《图形神经网络模型》IEEE神经网络汇刊20.1(2008):61–80。</li><li id="d864" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn oq nx ny nz bi translated">佩罗齐、布莱恩、拉米·艾尔弗和史蒂文·斯基纳。"深度行走:社交表征的在线学习."<em class="lx">第20届ACM SIGKDD知识发现和数据挖掘国际会议论文集</em>。2014.</li><li id="3491" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn oq nx ny nz bi translated">李，，等，“门控图序列神经网络”<em class="lx"> arXiv预印本arXiv:1511.05493 </em> (2015)。</li><li id="ddf9" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn oq nx ny nz bi translated">量子化学的神经讯息传递。<em class="lx">机器学习国际会议</em>。PMLR，2017。</li><li id="e55b" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn oq nx ny nz bi translated">基普夫，托马斯n，和马克斯韦林。"图卷积网络的半监督分类."<em class="lx"> arXiv预印本arXiv:1609.02907 </em> (2016)。</li><li id="4d8c" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn oq nx ny nz bi translated">图卷积网络的简化。<em class="lx">机器学习国际会议</em>。PMLR，2019。</li><li id="5397" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn oq nx ny nz bi translated">汉密尔顿、林子幸、Rex Ying和Jure Leskovec。"大型图上的归纳表示学习."<em class="lx">第31届国际神经信息处理系统会议录</em>。2017.</li><li id="9401" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn oq nx ny nz bi translated">彼得·w·巴塔格利亚等人，《关系归纳偏差、深度学习和图形网络》<em class="lx"> arXiv预印本arXiv:1806.01261 </em> (2018)。</li><li id="f7bf" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn oq nx ny nz bi translated">小团体中冲突和分裂的信息流模型。人类学研究杂志33.4(1977):452–473。</li><li id="e6ca" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn oq nx ny nz bi translated">弗拉斯科尼、保罗、马尔科·戈里和亚历桑德罗·斯佩尔杜蒂。"数据结构自适应处理的通用框架."IEEE神经网络汇刊9.5(1998):768–786。</li><li id="2222" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn oq nx ny nz bi translated">斯珀杜蒂、亚历山德罗和安东尼娜·斯塔丽塔。"用于结构分类的监督神经网络."神经网络IEEE汇刊8.3(1997):714–735。</li><li id="6a0f" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn oq nx ny nz bi translated">哈根布奇纳，马库斯，亚历桑德罗·斯佩尔杜蒂和阿忠蔡。“自组织映射，用于结构化数据的自适应处理。”IEEE神经网络汇刊14.3(2003):491–505。</li><li id="6eda" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn oq nx ny nz bi translated">布林、谢尔盖和劳伦斯·佩奇。“大规模超文本网络搜索引擎的剖析。”<em class="lx">计算机网络和ISDN系统</em>30.1–7(1998):107–117。</li><li id="cbd3" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn oq nx ny nz bi translated">《超链接环境中的权威来源》<em class="lx">汽水</em>。第98卷。1998.</li><li id="81ad" class="nr ns it js b jt oa jx ob kb oc kf od kj oe kn oq nx ny nz bi translated">蔡，阿忠，等，〈网页的适应性排序〉。<em class="lx">第12届万维网国际会议论文集</em>。2003.</li></ol></div></div>    
</body>
</html>