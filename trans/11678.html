<html>
<head>
<title>Modeling uncertainty in neural networks with TensorFlow Probability</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用张量流概率模拟神经网络中的不确定性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-d519a4426e9c?source=collection_archive---------5-----------------------#2021-11-19">https://towardsdatascience.com/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-d519a4426e9c?source=collection_archive---------5-----------------------#2021-11-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="5004" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">第三部分:认知的不确定性</h2></div><p id="d245" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="le">本系列是使用TensorFlow概率库对不确定性建模的简要介绍。我是作为我的</em><a class="ae lf" href="https://pydata.org/global2021/schedule/presentation/13/modeling-aleatoric-and-epistemic-uncertainty-using-tensorflow-and-tensorflow-probability/" rel="noopener ugc nofollow" target="_blank"><em class="le">PyData Global 2021 talk</em></a><em class="le">关于神经网络中不确定性估计的补充材料写的。</em></p><p id="1846" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">系列文章:</strong></p><ul class=""><li id="6680" class="lg lh it kk b kl km ko kp kr li kv lj kz lk ld ll lm ln lo bi translated"><a class="ae lf" rel="noopener" target="_blank" href="/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-part-1-an-introduction-2bb564c67d6"> <strong class="kk iu">第一部分:</strong>一个简介</a></li><li id="e27d" class="lg lh it kk b kl lp ko lq kr lr kv ls kz lt ld ll lm ln lo bi translated"><a class="ae lf" rel="noopener" target="_blank" href="/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-a706c2274d12"> <strong class="kk iu">第二部分</strong>:任意不确定性</a></li><li id="a923" class="lg lh it kk b kl lp ko lq kr lr kv ls kz lt ld ll lm ln lo bi translated"><strong class="kk iu">第三部分</strong>:认知不确定性</li><li id="d5b0" class="lg lh it kk b kl lp ko lq kr lr kv ls kz lt ld ll lm ln lo bi translated"><a class="ae lf" rel="noopener" target="_blank" href="/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-391b29538a7a"> <strong class="kk iu">第四部分:</strong>完全概率性</a></li></ul><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/533e143531780d6f6e2ffc1e30839d16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*SislMbrbBQ5wne8QtWu9SQ.jpeg"/></div><p class="mc md gj gh gi me mf bd b be z dk translated">图片由<a class="ae lf" href="https://www.pexels.com/@fotios-photos" rel="noopener ugc nofollow" target="_blank">丽莎</a>在<a class="ae lf" href="https://www.pexels.com/photo/turned-on-led-bulb-1393363/" rel="noopener ugc nofollow" target="_blank">https://www.pexels.com/photo/turned-on-led-bulb-1393363/</a>拍摄</p></figure><h2 id="bdae" class="mg mh it bd mi mj mk dn ml mm mn dp mo kr mp mq mr kv ms mt mu kz mv mw mx my bi translated">介绍</h2><p id="eb43" class="pw-post-body-paragraph ki kj it kk b kl mz ju kn ko na jx kq kr nb kt ku kv nc kx ky kz nd lb lc ld im bi translated">在本系列的<a class="ae lf" href="https://medium.com/@aleksander.molak?p=a706c2274d12" rel="noopener">前一部分</a>中，我们谈到了<strong class="kk iu"> <em class="le">任意不确定性</em>。</strong>我们已经展示了如何在实践中使用<a class="ae lf" href="https://www.tensorflow.org/probability" rel="noopener ugc nofollow" target="_blank"> TensorFlow Probability </a>的概率层和自定义损失函数对其建模，该函数允许我们使用概率最终层来训练模型。</p><p id="7e5f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">本周，我们将关注<strong class="kk iu"> <em class="le">认知不确定性</em> </strong>。我们将探索更高级的概率层，并了解在神经网络中估计权重不确定性的技术。先说一个定义。</p><h2 id="80a4" class="mg mh it bd mi mj mk dn ml mm mn dp mo kr mp mq mr kv ms mt mu kz mv mw mx my bi translated">什么是认知不确定性？</h2><p id="8a80" class="pw-post-body-paragraph ki kj it kk b kl mz ju kn ko na jx kq kr nb kt ku kv nc kx ky kz nd lb lc ld im bi translated">在<a class="ae lf" rel="noopener" target="_blank" href="/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-a706c2274d12">第2部分</a>中，我们说过<em class="le">随机不确定性</em>与数据生成过程有内在联系，不能通过添加更多数据来减少。</p><p id="9cb7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"><em class="le"/></strong>—另一方面，认知不确定性与我们对数据生成过程的了解(或无知)有关。这个名字来自古希腊语<a class="ae lf" href="https://en.wiktionary.org/wiki/%E1%BC%90%CF%80%CE%B9%CF%83%CF%84%CE%AE%CE%BC%CE%B7" rel="noopener ugc nofollow" target="_blank"><strong class="kk iu"/></a><strong class="kk iu"/><em class="le">/e . pisˇti . mi/</em>——这个术语被亚里士多德或柏拉图等哲学家广泛使用，大致翻译为“知识”。<em class="le">认知不确定性</em>的另一个名称是<em class="le">模型不确定性</em>。</p><p id="d083" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">关于<em class="le">认知不确定性的一个重要事实是</em>它<strong class="kk iu">可以通过增加<strong class="kk iu">更多的数据</strong>来</strong>减少<strong class="kk iu"/>。</p><h2 id="cdef" class="mg mh it bd mi mj mk dn ml mm mn dp mo kr mp mq mr kv ms mt mu kz mv mw mx my bi translated">用张量流概率建模认知不确定性</h2><p id="1283" class="pw-post-body-paragraph ki kj it kk b kl mz ju kn ko na jx kq kr nb kt ku kv nc kx ky kz nd lb lc ld im bi translated">先说一些数据。我们将使用与在<a class="ae lf" href="https://medium.com/@aleksander.molak?p=a706c2274d12" rel="noopener">第2部分</a>中相同的数据生成过程，但这次我们将创建两个数据集，而不是一个。一个数据集较小(<em class="le"> n=100 </em>)，另一个较大(<em class="le"> n=1000 </em>)。这些数据集将帮助我们理解样本大小如何影响不确定性估计。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="ne nf l"/></div><p class="mc md gj gh gi me mf bd b be z dk translated">我们导入库，使用相同的数据生成过程生成两个数据集，但是样本大小不同(<em class="ng"> n=100 </em>和<em class="ng"> n=1000 </em>)。最后，我们绘制数据。</p></figure><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nh"><img src="../Images/36b9bdc2ca2cf547feea14b7b4a869b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uxiV0ktvGLLi4icjNIc1cA.png"/></div></div><p class="mc md gj gh gi me mf bd b be z dk translated">我们两个数据集的散点图。真实的你的形象。</p></figure><p id="18f2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的数据准备好了。在我们开始建模之前，让我们了解需要做出什么架构决策来在我们的模型中启用<em class="le">认知不确定性</em>估计。</p><p id="90b6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从概念上讲，我们需要做的是用分布代替逐点权重估计。让我们打开它。在传统的神经网络中，每个权重由一个数字表示。在训练期间，我们最小化一些关于模型权重的损失函数。当模型表现足够好时，我们停止训练并存储权重。我们有多确定我们所学的权重是我们问题的好估计？</p><p id="be43" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们实际上不知道。</p><p id="4871" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了解决这个问题，我们需要一种方法来量化这种(不)确定性。我们将用一个分布替换每个权重，而不是学习每个权重的单个值，我们将学习每个分布的一组参数。</p><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nm"><img src="../Images/69b80102fdbd95cf01cd0495926b7eed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6bAir5BABxZviBnvGud7KA.png"/></div></div><p class="mc md gj gh gi me mf bd b be z dk translated">在一个常规的神经网络(左)中，所有的权重都是点估计值(或者也可以是一个数字)。当对认知不确定性建模时(右)，每个权重被建模为分布而不是点估计。右图中的橙色线条代表发行版的pdf。真实的你的形象。</p></figure><p id="f54d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要了解分布，我们需要从某处开始—我们需要一个初步分布，然后我们将更新它以反映最佳数据拟合。这种初步分配被称为<em class="le">优先</em>。最终分布称为<em class="le">后验</em>。</p><p id="97ad" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">给定<em class="le">先验</em>分布和我们的数据，我们将利用<a class="ae lf" href="https://en.wikipedia.org/wiki/Bayes%27_theorem" rel="noopener ugc nofollow" target="_blank">贝叶斯定理</a>的力量来学习<em class="le">后验</em>分布。但是有一个问题，在我们的例子中，计算精确的后验概率非常困难。</p><blockquote class="nn no np"><p id="7623" class="ki kj le kk b kl km ju kn ko kp jx kq nq ks kt ku nr kw kx ky ns la lb lc ld im bi translated"><strong class="kk iu"> <em class="it">变分图层</em> </strong></p></blockquote><p id="cd38" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">幸运的是，TensorFlow Probability提供了通过反向投影<em class="le"/>【1】实现<a class="ae lf" href="https://arxiv.org/pdf/1505.05424.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="le">贝叶斯的<code class="fe nt nu nv nw b">tfpl.DenseVariational</code>层——这是一种可用于神经网络中高效权重不确定性估计的方法。这是一种近似的方法——但绝对足够好，可以引导我们获得巨大的实际结果。</em></a></p><p id="68ba" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此外，<code class="fe nt nu nv nw b">tfpl.DenseVariational</code>层将为我们完成大部分繁重的工作，包括证据下限(<a class="ae lf" href="https://en.wikipedia.org/wiki/Evidence_lower_bound" rel="noopener ugc nofollow" target="_blank"> ELBO </a>)的计算——我们将使用这个量在内部找到<em class="le">后验</em>。</p><p id="4b89" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们使用<code class="fe nt nu nv nw b">tfpl.DenseVariational</code>的方式类似于常规的<code class="fe nt nu nv nw b">tf.keras.Dense</code>，但是我们需要指定几个额外的参数。最重要的两个是<em class="le">优先</em>生成函数和<em class="le">p</em>o优先<em class="le"> g </em>生成函数。这两个函数都需要带三个参数:<code class="fe nt nu nv nw b">kernel_size</code>、<code class="fe nt nu nv nw b">bias_size</code>和<code class="fe nt nu nv nw b">dtype</code>，并且都应该返回一个分布。</p><p id="0c65" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们来定义一下！</p><p id="7609" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将从一个<em class="le">先验</em>分布开始。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="ne nf l"/></div><p class="mc md gj gh gi me mf bd b be z dk translated">使用多元正态分布和“tfpl.DistributionLambda”包装器定义不可训练的先验。</p></figure><p id="4f7c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们打开它。</p><p id="3509" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们使用平均值为零(<code class="fe nt nu nv nw b">loc=tf.zeros(n)</code>)和标准差为1 ( <code class="fe nt nu nv nw b">scale_diag=tf.ones(n)</code>)的<code class="fe nt nu nv nw b">tfd.MultivariateNormalDiag</code>。这个发行版被打包到<code class="fe nt nu nv nw b">tfpl.DistributionLambda</code>中，它只是将一个发行版对象转换成一个Keras兼容的层。这一层然后被传递给一个标准的<code class="fe nt nu nv nw b">tf.keras.Sequential</code>模型。这有点拗口，还有一个稍微简单一点的方法来定义这个<em class="le">先于</em>。不过，我们将坚持上面介绍的方法，因为它更通用，更符合我们定义<em class="le">后验</em>的方式。</p><p id="3352" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那就让我们看看<em class="le">后路</em>吧！</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="ne nf l"/></div><p class="mc md gj gh gi me mf bd b be z dk translated">定义一个可训练的后验分布。` tflp.VariableLayer '生成一个可训练变量来参数化` tfpl.MultivariateNormalTriL'。</p></figure><p id="4338" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们采用完全相同的一组参数——内核和偏差大小以及<code class="fe nt nu nv nw b">dtype</code>。类似于我们之前所做的，我们使用<code class="fe nt nu nv nw b">tf.keras.Sequential</code>。与之前的<em class="le">分布不同，我们希望我们的<em class="le">后</em>是可训练的。我们使用生成可训练变量的<code class="fe nt nu nv nw b">tfpl.VariableLayer</code>来参数化<code class="fe nt nu nv nw b">tfpl.MultivariateNormalTriL</code>。正如在<a class="ae lf" rel="noopener" target="_blank" href="/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-a706c2274d12">第2部分</a>中，我们使用了一种静态便利方法<code class="fe nt nu nv nw b">.params_size()</code>来获得参数化<code class="fe nt nu nv nw b">tfpl.MultivariateNormalTriL</code>所需的精确参数数。</em></p><blockquote class="nn no np"><p id="7291" class="ki kj le kk b kl km ju kn ko kp jx kq nq ks kt ku nr kw kx ky ns la lb lc ld im bi translated"><strong class="kk iu"> <em class="it">建模认知不确定性</em> </strong></p></blockquote><p id="c85e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是很多，但现在我们终于准备好定义模型了！</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="ne nf l"/></div><p class="mc md gj gh gi me mf bd b be z dk translated">定义一个可以估计认知不确定性的概率模型。我们使用一个函数来定义模型，以便轻松地为每个数据集构建单独的模型。</p></figure><p id="fd1a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们简单地研究一下变分层参数。第一个参数是单位数。这里我们只有一个单位——就像常规线性回归中一样。第二个参数— <code class="fe nt nu nv nw b">input_shape</code> —看起来应该也很熟悉——它与规则密集层的情况具有完全相同的含义。接下来，我们传递先验和后验函数。注意，我们应该将这些函数作为对象<strong class="kk iu">传递，而不调用</strong>它们。</p><p id="d4e5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">最后，我们有两个与<a class="ae lf" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener ugc nofollow" target="_blank"><em class="le">kull back–lei bler散度</em> </a> ( <em class="le"> KL散度</em>)相关的参数。<em class="le"> Kullback-Leibler散度</em>是我们ELBO目标中的一个术语。因为这一项与ELBO中的第二项是在不同的数据子集上计算的，我们需要重新调整它以确保它是无偏的。这就是我们将<code class="fe nt nu nv nw b">x_train_shape</code>作为参数传递给模型生成函数的原因。最后一个参数——<code class="fe nt nu nv nw b">kl_use_exact</code>——控制我们是想通过分析计算<em class="le"> KL散度</em>还是使用经验近似法。在我们的例子中，我们可以解析地计算它，因为当我们有一个作为先验和后验的正态分布时，这很容易做到。然而，这并不总是可能的。我们将<code class="fe nt nu nv nw b">kl_use_exact</code>明确设置为<code class="fe nt nu nv nw b">False</code>,以证明该参数exists⁴.</p><p id="e801" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在第2部分的<a class="ae lf" rel="noopener" target="_blank" href="/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-a706c2274d12">中，我们需要两层来解决一个简单的回归问题——一层用来表示输出分布，另一层用来参数化这个分布。量化<strong class="kk iu"> <em class="le">认知的不确定性</em> </strong>虽然一个变分层就够了。该层返回输出的点估计值，但其权重(和偏差)是概率性的。这两个事实的后果如下:</a></p><ul class=""><li id="8a0b" class="lg lh it kk b kl km ko kp kr li kv lj kz lk ld ll lm ln lo bi translated">我们可以使用<em class="le">均方误差</em>作为损失函数(尽管在内部，我们仍将使用ELBO来逼近<em class="le">后验</em>分布)，</li><li id="edef" class="lg lh it kk b kl lp ko lq kr lr kv ls kz lt ld ll lm ln lo bi translated">当我们要求模型进行预测时，它几乎每次都会返回不同的输出；这是因为现在每次查询模型时，我们的权重都是从学习的分布中采样的。</li></ul><p id="76b4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们准备好安装模型了。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="ne nf l"/></div><p class="mc md gj gh gi me mf bd b be z dk translated">拟合两个模型:一个在较小的数据集上，一个在较大的数据集上。</p></figure><p id="2250" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们对每个模型训练500个时期，以确保两个模型都收敛。请注意，就参数而言，这些模型是迄今为止最大的——我们需要估计偏差项和权重项的均值和标准差以及它们之间的协方差，这样我们总共有5个参数。</p><p id="a3f3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们画一条学习曲线。</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="ne nf l"/></div></figure><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/dc690c2be7daaced5eddeb6404f3c9da.png" data-original-src="https://miro.medium.com/v2/resize:fit:866/format:webp/1*nwbsOHWB59XX1s07Nwd-nA.png"/></div><p class="mc md gj gh gi me mf bd b be z dk translated">作为时代数的函数的损失。蓝线代表在较小数据集上训练的模型(<em class="ng"> n=100 </em>)。红线代表在更大的数据集上训练的模型(<em class="ng"> n=1000 </em>)。真实的你的形象。</p></figure><p id="9371" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">正如你所看到的，在较小的数据集上训练的模型的损失(<em class="le"> n=100 </em>)要波动得多。这是有意义的，因为模型很难在较小的数据集上找到稳定的参数估计值。</p><p id="0c25" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">太好了！我们的模型经过训练，我们最终可以生成和绘制预测！</p><figure class="lv lw lx ly gt lz"><div class="bz fp l di"><div class="ne nf l"/></div><p class="mc md gj gh gi me mf bd b be z dk translated">为更小和更大的数据集生成和绘制预测。我们为每个模型生成15个预测。</p></figure><figure class="lv lw lx ly gt lz gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nh"><img src="../Images/5e3788abad6e0a025313fe2f2c5cec31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6zfICvyQmzLxP1r-98dIgA.png"/></div></div><p class="mc md gj gh gi me mf bd b be z dk translated">我们两个数据集的散点图和每个数据集的15条拟合线。真实的你的形象。</p></figure><p id="e4c0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们为每个模型生成了15个预测。每次预测都是不同的，因为对于每次迭代，我们从学习到的<em class="le">后验</em>分布中随机抽取权重。请注意，在较大数据集上训练的模型(右)在<em class="le">斜率</em>和<em class="le">截距</em>估计值上的方差比在较小数据集上训练的模型(左)小得多。这是一个很好的例子，说明如何通过添加更多的数据来减少认知不确定性。</p><h2 id="e4ff" class="mg mh it bd mi mj mk dn ml mm mn dp mo kr mp mq mr kv ms mt mu kz mv mw mx my bi translated">摘要</h2><p id="bc8c" class="pw-post-body-paragraph ki kj it kk b kl mz ju kn ko na jx kq kr nb kt ku kv nc kx ky kz nd lb lc ld im bi translated">在本集<em class="le">用张量流概率</em> <strong class="kk iu"> <em class="le"> </em> </strong>对神经网络中的不确定性建模中，我们已经看到了如何对<em class="le">认知不确定性</em>进行建模。</p><p id="074d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们使用了更高级的概率层，比如<code class="fe nt nu nv nw b">tfpl.VariationalDense</code>。我们定义了我们的<em class="le">先验</em>和<em class="le">后验</em>分布。最后，我们已经看到样本大小如何影响<em class="le">认知不确定性</em>估计。</p><p id="1ae5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在下一部分，我们将介绍一个<strong class="kk iu">完全概率</strong>模型，我们将在一个更复杂的非线性数据集上训练它，看看如何使概率模型更深入。</p><p id="6c83" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><a class="ae lf" rel="noopener" target="_blank" href="/modeling-uncertainty-in-neural-networks-with-tensorflow-probability-391b29538a7a">第四部</a>见！</p></div><div class="ab cl ny nz hx oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="im in io ip iq"><h2 id="2564" class="mg mh it bd mi mj mk dn ml mm mn dp mo kr mp mq mr kv ms mt mu kz mv mw mx my bi translated">脚注</h2><p id="802a" class="pw-post-body-paragraph ki kj it kk b kl mz ju kn ko na jx kq kr nb kt ku kv nc kx ky kz nd lb lc ld im bi translated">ELBO有两个分量:似然分量和Kullback-Leibler散度分量。更多细节请参考[1]。</p><p id="6245" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">变量<code class="fe nt nu nv nw b">n</code>表示层中参数的总数。</p><p id="15e7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><code class="fe nt nu nv nw b"><a class="ae lf" href="https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/MultivariateNormalTriL" rel="noopener ugc nofollow" target="_blank">tfpl.MultivariateNormalTriL</a></code>是<code class="fe nt nu nv nw b">tfp.distributions</code>模块中多元正态分布的参数之一。</p><p id="ad69" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">⁴这个参数的默认值被设置为<code class="fe nt nu nv nw b">False</code>，如果你不知道的话，很容易忽略它。</p><h2 id="9a2b" class="mg mh it bd mi mj mk dn ml mm mn dp mo kr mp mq mr kv ms mt mu kz mv mw mx my bi translated">参考</h2><p id="c59d" class="pw-post-body-paragraph ki kj it kk b kl mz ju kn ko na jx kq kr nb kt ku kv nc kx ky kz nd lb lc ld im bi translated">[1] C. Blundell，J. Cornebise，K. Kavukcuoglu，D. Wierstra，<a class="ae lf" href="https://arxiv.org/pdf/1505.05424.pdf" rel="noopener ugc nofollow" target="_blank">神经网络中的权重不确定性</a> (2015)。第32届机器学习国际会议论文集(ICML 2015)。</p><p id="2889" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi">________________</p><p id="bfe0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">❤️ <em class="le">对获取更多这样的内容感兴趣吗？使用此链接加入:</em></p><div class="of og gp gr oh oi"><a href="https://aleksander-molak.medium.com/membership" rel="noopener follow" target="_blank"><div class="oj ab fo"><div class="ok ab ol cl cj om"><h2 class="bd iu gy z fp on fr fs oo fu fw is bi translated">通过我的推荐链接加入媒体-亚历山大·莫拉克</h2><div class="op l"><h3 class="bd b gy z fp on fr fs oo fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="oq l"><p class="bd b dl z fp on fr fs oo fu fw dk translated">aleksander-molak.medium.com</p></div></div><div class="or l"><div class="os l ot ou ov or ow ma oi"/></div></div></a></div><p id="431b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">谢谢大家！</p><p id="a3c9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi">_______________</p></div></div>    
</body>
</html>