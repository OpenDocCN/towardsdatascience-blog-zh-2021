<html>
<head>
<title>RetinaNet: The beauty of Focal Loss</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">RetinaNet:失焦之美</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/retinanet-the-beauty-of-focal-loss-e9ab132f2981?source=collection_archive---------22-----------------------#2021-05-12">https://towardsdatascience.com/retinanet-the-beauty-of-focal-loss-e9ab132f2981?source=collection_archive---------22-----------------------#2021-05-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="6c47" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">改变游戏的一阶段物体检测模型！</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/a6ffef858728b66b638cfc5f73f18201.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7pL8PgEUwAeKnTp9UdDYdQ.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/s/photos/retina?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae kv" href="https://unsplash.com/@arteum?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Arteum.ro </a>拍摄的照片</p></figure><p id="ddfa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对象检测已经成为众多领域应用的福音。因此，可以肯定地说，它比其他一些领域更值得研究。多年来，研究人员不懈地致力于改进对象检测算法，并成功地做到了这一点，以至于我无法将5年前的对象检测算法的性能指标与今天的进行比较。所以为了更全面的观点，我想建立一个我们如何到达这里的前提。</p><h1 id="7ccc" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">目标检测</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mk"><img src="../Images/9437ebcda40b13d38a29f8b6203a53f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QFI-BiMz2kZpOWl0RJ6RDA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1 —目标检测示例(照片由<a class="ae kv" href="https://www.pexels.com/@mccutcheon?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">莎伦·麦卡琴</a>从<a class="ae kv" href="https://www.pexels.com/photo/person-holding-siamese-cat-and-chihuahua-1909802/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">像素</a>拍摄)</p></figure><p id="6dbb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对象检测开始时是一个两阶段的实现，在第一阶段检测图像中的对象(定位)，在第二阶段对其进行分类(分类)。该过程的早期先驱是RCNN及其后续改进(快速RCNN，更快RCNN)。它们的实现是基于区域提议机制的，正是这种机制在后来的版本中得到了主要的改进。还有另一种物体检测方法，其中定位和分类在单个步骤中执行。尽管一些网络在早期阶段执行单级对象检测，例如SSD(单次检测器)，但YOLO在2016年彻底改变了该领域。YOLO能够用边界框定位物体，并立刻计算出它们的等级分数。由于其令人难以置信的速度，YOLO是大多数实时应用程序的首选模型。沿着同样的思路，宗-林逸等人发表了一篇论文，“密集物体探测的<a class="ae kv" href="https://arxiv.org/abs/1708.02002" rel="noopener ugc nofollow" target="_blank">焦点损失</a>”，其中介绍了一种称为RetinaNet的探测器。它胜过当时市场上的所有其他型号。在深入RetinaNet的本质之前，我将讨论焦点丢失的概念。</p><h1 id="6fca" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">焦点损失</h1><p id="494c" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">焦点损失被设计为在密集检测器训练期间通过交叉熵损失观察到的类别不平衡的补救措施。我说的类别不平衡是指(或作者指的是)前景和背景类别的差异，通常是1:1000的比例。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/868babbd2db08d8efcc9fdf81dc014a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*Rm-vU6yZjB9lnCIhWhza4Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2-交叉熵和焦损失之间的比较</p></figure><p id="37b8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">暂且忽略公式吧。考虑γ = 0的情况，对应交叉熵。如果你观察曲线，你会发现即使对于分类良好的例子，损失也不是微不足道的。现在，如果我加入阶级不平衡的问题，那里有大量的容易否定的东西，它倾向于压倒前景阶级。</p><p id="aadb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了解决这个问题，作者增加了一个可调聚焦参数的调制因子。因此，焦点损失的公式变为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/3a60f576a79178d0bb0f022a61632130.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*0C0xSoMs-seGAptf98g-Cg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3 —焦点丢失(作者提供的图片)</p></figure><p id="7b1d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你注意到，负项和对数项构成了交叉熵损失，γ代表可调参数。如果我考虑一个具有低概率p_t的错误分类样本，调制因子实际上是不变的，而如果概率p_t很高(容易分类)，那么损失函数将趋向于0。从而降低容易分类的样本的权重。这就是聚焦损失如何区分易分类和难分类样本的。</p><p id="8aec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，损失函数还有一个附加项，即平衡因子α。α对应于一个权重因子，该权重因子或者用反向类别频率计算，或者作为交叉验证优化变量。焦点损失公式现在变为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/e0ada093945135c062fabe20a36f8c42.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*HcosX1suo4J_OCjl8PDT8Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图4 —修改后的焦损失(作者提供的图片)</p></figure><p id="9048" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作者(通过实验)注意到焦点损失形式不需要精确。相反，有几种形式可以整合样本之间的差异。</p><h1 id="4bfc" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">RetinaNet</h1><p id="e6e7" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">RetinaNet实际上是由一个主干网和两个特定任务子网组成的网络组合。主干网络是一个纯卷积网络，负责计算整个图像的卷积特征图。第一个子网对主干网的输出执行卷积分类，第二个子网对主干网的输出执行卷积包围盒回归。总体架构看起来非常简单，但作者调整了每个组件以改善结果。</p><h2 id="139b" class="mt lt iq bd lu mu mv dn ly mw mx dp mc lf my mz me lj na nb mg ln nc nd mi ne bi translated">特征金字塔网络主干</h2><p id="ce76" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">作者实现了由T. Y. Lin等人提出的特征金字塔网络(FPN)作为主干网络。FPN通过横向连接实施自上而下的方法，提供了丰富的多尺度要素金字塔。对于RetinaNet来说，FPN是建立在ResNet架构之上的。金字塔有5个等级，从P₃到P₇，其中分辨率可以计算为2ˡ，其中l对应于金字塔等级，在本例中为3到7。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/03c3e21e39c84344350cb680db54fa70.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TVDleshqg1cA-jkBDGeY-A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图5 —包含单个组件的RetinaNet架构</p></figure><h2 id="e63d" class="mt lt iq bd lu mu mv dn ly mw mx dp mc lf my mz me lj na nb mg ln nc nd mi ne bi translated">锚</h2><p id="0025" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">RetinaNet使用平移不变的锚盒，分别在P₃到P₇级别上具有从32到512的区域。为了加强更密集的覆盖范围，锚增加了{2⁰,2^(1/3),2^(2/3)}.的大小所以，每个金字塔等级有9个锚。每个锚被分配一个分类目标K的独热向量和一个盒回归目标的4向量。</p><h2 id="d8e7" class="mt lt iq bd lu mu mv dn ly mw mx dp mc lf my mz me lj na nb mg ln nc nd mi ne bi translated">分类子网</h2><p id="0a47" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">分类子网是附加到每个金字塔等级的FCN，其参数在所有等级之间共享。考虑通道的数量C为256，锚A为9，分类目标的数量K，特征图通过具有C滤波器的四个3×3 conv层馈送。这是继ReLU激活和另一个3×3 conv层，但与K×A过滤器的应用。最后，sigmoid激活被附加到每个空间位置的K×A二元预测的输出。所以，最终输出变成(W，H，K×A)，其中W和H分别代表特征图的宽度和高度。为了获得更好的结果，作者在分类子网的参数不与回归子网共享的地方做了一点修改。</p><h2 id="71ad" class="mt lt iq bd lu mu mv dn ly mw mx dp mc lf my mz me lj na nb mg ln nc nd mi ne bi translated">箱式回归子网</h2><p id="50a8" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">如果参考图5，可以看到分类子网和回归子网同时接收到特征映射。因此，它们并行运行。对于回归，将另一个小FCN附加到每个金字塔等级，以回归从每个锚定框到附近地面实况对象(如果存在)的偏移。整个结构类似于分类子网，但区别在于输出不是K×A，而是4×A。具体数字“4”代表用于确定偏移量的参数，即中心坐标以及宽度和高度。对象分类子网和盒回归子网虽然共享一个公共结构，但使用不同的参数。</p><h2 id="69de" class="mt lt iq bd lu mu mv dn ly mw mx dp mc lf my mz me lj na nb mg ln nc nd mi ne bi translated">模型的整体流程</h2><p id="0bfd" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">让我们取一个样本图像，并将其馈送给网络。第一站，FPN。在这里，图像将在不同的尺度(4个级别)上进行处理，在每个级别上，它将输出一个特征图。每个级别的特征图将被提供给下一个组件包，即分类子网和回归子网。FPN输出的每个特征图然后由分类子网处理，并且它输出具有形状(W，H，K×A)的张量。同样的，回归子网会处理特征图，会输出a (W，H，4×A)。这两个输出被同时处理，并发送给损失函数。RetinaNet中的多任务损失函数由用于分类的修正聚焦损失和基于回归子网产生的4×A通道向量计算的平滑L1损失组成。然后损失被反向传播。这就是模型的整体流程。接下来，让我们看看该模型与其他对象检测模型相比表现如何。</p><h1 id="60e4" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">情况如何？</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/c455abb01c3bd90fc5455610d00d015d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1304/format:webp/1*v_rpWLRBzOi6A4g0myO6bA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图6 —结果对比</p></figure><p id="9232" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从上表可以明显看出，采用ResNeXt-101主干的RetinaNet优于之前提出的所有其他两级和一级模型。唯一有0.9错误的类别是大型对象的AP。</p><h1 id="8ddd" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">我想实现RetinaNet，我应该注意什么？</h1><p id="4579" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">当然，每个问题陈述都是不同的，作者建议的一些参数可能不适合你。但是作者进行了严格的研究来获得这些最佳结果。所以，我觉得它们可以很好地解决一般问题，或者至少，它们可以作为一个基本参数集，用户可以进一步调整它们。</p><ol class=""><li id="44e8" class="nh ni iq ky b kz la lc ld lf nj lj nk ln nl lr nm nn no np bi translated">γ = 2在实践中工作良好，RetinaNet对γ ∈ [0.5，5]相对鲁棒。分配给稀有类的权重α也有一个稳定的范围，但它与γ相互作用，因此必须同时选择两者。一般来说，当γ增加时，α应该稍微减少。对作者来说，最合适的构型是γ = 2，α = 0.25。</li><li id="b734" class="nh ni iq ky b kz nq lc nr lf ns lj nt ln nu lr nm nn no np bi translated">使用0.5的交集(IoU)阈值将锚点分配给地面实况对象框；并且如果它们的IoU在[0，0.4]中，则返回到背景。由于每个锚点被分配给至多一个对象框，所以其长度K标签向量中的相应条目被设置为1，而所有其他条目被设置为0。如果锚点未被赋值(这可能发生在[0.4，0.5]中的重叠)，则它在训练期间被忽略。</li><li id="90b6" class="nh ni iq ky b kz nq lc nr lf ns lj nt ln nu lr nm nn no np bi translated">为了提高速度，在阈值检测器置信度为0.05之后，每个FPN级别仅解码来自至多1k最高得分预测的盒预测。来自所有级别的顶部预测被合并，并且应用阈值为0.5的非最大值抑制来产生最终检测。</li><li id="6c50" class="nh ni iq ky b kz nq lc nr lf ns lj nt ln nu lr nm nn no np bi translated">除了RetinaNet子网中的最后一个图层之外，所有新的conv图层都使用偏差b = 0和σ = 0.01的高斯权重填充进行初始化。对于分类子网的最终conv层，偏置初始化被设置为b = log((1π)/π)，其中π指定训练每个锚的开始应该被标记为置信度为τπ的前景。π的值在所有实验中都是0.01。</li></ol></div><div class="ab cl nv nw hu nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="ij ik il im in"><h1 id="dc1e" class="ls lt iq bd lu lv oc lx ly lz od mb mc jw oe jx me jz of ka mg kc og kd mi mj bi translated">摘要</h1><p id="90ae" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">总之，RetinaNet在推出时对物体探测领域进行了重大改进。一级检测器优于两级检测器的想法是非常不现实的，但RetinaNet使其成为现实。从那时起，有许多新的算法被设计来进一步改进这些结果，随着我不断发现它们，我肯定会将它们记录到新的文章中。所以，敬请期待下一期！</p><p id="e2eb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望你喜欢我的文章，如果你想联系我:</p><ol class=""><li id="62e3" class="nh ni iq ky b kz la lc ld lf nj lj nk ln nl lr nm nn no np bi translated">我的网站:<a class="ae kv" href="https://preeyonujboruah.tech/" rel="noopener ugc nofollow" target="_blank">https://preeyonujboruah.tech/</a></li><li id="16c2" class="nh ni iq ky b kz nq lc nr lf ns lj nt ln nu lr nm nn no np bi translated">Github简介:【https://github.com/preeyonuj T2】</li><li id="1cfe" class="nh ni iq ky b kz nq lc nr lf ns lj nt ln nu lr nm nn no np bi translated">上一篇媒体文章:<a class="ae kv" rel="noopener" target="_blank" href="/the-implications-of-information-theory-in-machine-learning-707132a750e7">https://towards data science . com/the-implications-of-information-theory-in-machine-learning-707132 a 750 e 7</a></li><li id="7d9a" class="nh ni iq ky b kz nq lc nr lf ns lj nt ln nu lr nm nn no np bi translated">领英简介:【www.linkedin.com/in/pb1807 T2】</li></ol></div><div class="ab cl nv nw hu nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="ij ik il im in"><h1 id="d8e3" class="ls lt iq bd lu lv oc lx ly lz od mb mc jw oe jx me jz of ka mg kc og kd mi mj bi translated">参考</h1><p id="362a" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">[1]密集目标检测的焦损失—<a class="ae kv" href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Lin_Focal_Loss_for_ICCV_2017_paper.pdf" rel="noopener ugc nofollow" target="_blank">https://open access . the CVF . com/content _ ICCV _ 2017/papers/Lin _ Focal _ Loss _ for _ ICCV _ 2017 _ paper . pdf</a></p></div></div>    
</body>
</html>