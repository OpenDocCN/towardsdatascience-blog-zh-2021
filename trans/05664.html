<html>
<head>
<title>Forward and Reverse KL Divergence</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">正向和反向KL散度</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/forward-and-reverse-kl-divergence-906625f1df06?source=collection_archive---------14-----------------------#2021-05-20">https://towardsdatascience.com/forward-and-reverse-kl-divergence-906625f1df06?source=collection_archive---------14-----------------------#2021-05-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/f3dd729f8c3b8b5f561263e5f7569c74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*lYpHvbRXboG6y-Ch"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">爱丽丝·山村在<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="1455" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文中，我们将调查正向和反向KL偏离之间的差异，以及这可能对您的项目产生的影响。为了做到这一点，我们将着眼于理论，然后用一些实验来证明实际结果😊</p><h1 id="517b" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">什么是KL发散？</h1><p id="76fe" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">很高兴你问了！本质上，<strong class="kf ir">描述了一个概率分布与另一个</strong>的偏差。它实质上是从“真实”分布和我们想要与“真实”分布进行比较的“预测”分布之间的交叉熵中减去“真实”分布的熵。这里的想法是，如果两个分布相同，那么交叉熵应该等于熵，所以结果将是0。否则，交叉熵将大于熵项，因此结果将大于0。换句话说，该值越接近0，则“真实”和“预测”分布越相似。</p><p id="76d4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面给出了该方程，以及从初始定义导出的一些进一步的形式:</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi me"><img src="../Images/e82d68eed53acb46708d5cf1a6535d5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1188/format:webp/1*mnsuXRtcWGqAf5eKBOioVg.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">图片作者:KL散度方程</p></figure><p id="453e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这为什么有用？好吧，假设你想用一个我们可以控制参数的模型给出的“预测”分布来拟合一些“真实”的概率分布。有一个度量标准告诉我们模型分布与实际目标分布有多接近是很有用的。交叉熵可以服务于这个角色，但它并不持有直观的价值；也就是说，KL散度直观地告诉你，你离单个数字的解有多近。</p><h1 id="e53c" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">不对称的证明</h1><p id="ad30" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">首先，一个定义。无论参数的顺序如何，对称函数都具有相同的值。</p><p id="f977" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">看起来KL散度也应该是一个对称函数。如果它给出了两个分布之间的不相似性的度量，那么函数的分布输入的顺序应该无关紧要，因为这不会改变任何关于不相似性的东西，对吗？</p><p id="167a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">好吧，我们来做个实验！为了简单起见，我们来看两个任意的伯努利分布，一个定义为<strong class="kf ir"> p(x) </strong>，一个定义为<strong class="kf ir"> q(x) </strong>。</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/7739c0958671ca823c42c133ceab5a09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1182/format:webp/1*M4l5pBS_PeJiLQb_mvYE1g.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">图片作者:<strong class="bd mk"> p(x) </strong>有<strong class="bd mk"> p = 0.1 </strong>和<strong class="bd mk"> q(x) </strong>有<strong class="bd mk"> p = 0.6 </strong></p></figure><p id="95ff" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当我们计算具有不同参数顺序的相同分布的KL散度时，我们得到的值会发生变化:</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/24fbcdec4f5b5902ee50b2168d71bdfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:378/format:webp/1*wv59-bNpUaviVaw80Bq2jw.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片:参数排序影响结果！</p></figure><p id="893d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，<strong class="kf ir"> KL发散不是对称的</strong>。我们定义如下:</p><ul class=""><li id="9b11" class="mm mn iq kf b kg kh kk kl ko mo ks mp kw mq la mr ms mt mu bi translated">KL(p||q)是<strong class="kf ir">前向KL散度</strong></li><li id="f6e7" class="mm mn iq kf b kg mv kk mw ko mx ks my kw mz la mr ms mt mu bi translated">KL(q||p)是<strong class="kf ir">反向KL发散</strong></li></ul><h1 id="cc78" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">理论</h1><p id="65a4" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">那么有什么区别呢？再次设置场景，我们经常想要最小化KL散度，以使可控的“预测”分布符合一些固定的“真实”分布，但是我们应该最小化哪个KL散度度量？在下面的部分中，<strong class="kf ir"> p(x) </strong>是固定的“真实”分布，<strong class="kf ir"> q(x) </strong>是可控的“预测”分布。为了说明这一点，我将下标<strong class="kf ir"> q </strong>和<strong class="kf ir"> θ </strong>，它们是我们可以改变的<strong class="kf ir"> q </strong>分布参数。</p><p id="9b61" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们首先来看看前向KL散度的最小化:</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi na"><img src="../Images/fb85a07bd4da22fbe8284a6f23a5a086.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*tvIf11j0JWhsCVIiU7_X2w.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="d9e5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">换句话说，凡是<strong class="kf ir"> p(x) </strong>有大概率的地方，<strong class="kf ir"> q(x) </strong>一定也有大概率。这是<strong class="kf ir">均值搜索</strong>行为，因为<strong class="kf ir"> q(x) </strong>必须覆盖<strong class="kf ir"> p(x) </strong>中所有高概率的模态和区域，但是<strong class="kf ir"> q(x) </strong>不会因为具有高概率质量而受到惩罚，而<strong class="kf ir"> p(x) </strong>不会。</p><p id="0e9b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，让我们将这与反向KL发散的最小化进行对比:</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/e5771a279e9f2f7a00c872cda61e085d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1130/format:webp/1*eIQtRKFh-jlqcE64Bk7Zmw.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="ba96" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们逐项分析一下。第一项类似于KL向前发散的目标。也就是说，它陈述了在<strong class="kf ir"> q(x) </strong>具有高概率的地方，<strong class="kf ir"> p(x) </strong>也必须具有高概率。这是<strong class="kf ir">模式搜索</strong>行为，因为来自<strong class="kf ir"> q(x) </strong>的任何样本必须位于<strong class="kf ir"> p(x) </strong>的模式内。注意，<strong class="kf ir"> q(x) </strong>不会因为没有在<strong class="kf ir"> p(x) </strong>的所有模式上放置概率质量而受到惩罚。第二项其实只是<strong class="kf ir"> q(x) </strong>的熵！这阻止了分布坍缩到非常窄的模式。</p><h1 id="8a59" class="lb lc iq bd ld le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">实验</h1><p id="f577" class="pw-post-body-paragraph kd ke iq kf b kg lz ki kj kk ma km kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">为了证明这一点，让我们尝试预测一个GMM(2)分布，<strong class="kf ir"> p(x) </strong>，一个正态分布，<strong class="kf ir"> q(x) </strong>。“真实”分布<strong class="kf ir"> p(x) </strong>如下所示:</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/a5f504188e0e07453f3004db50971993.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*5KW4rtdnqhadimF5iSD_gw.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片:均值[-1，2]，方差[0.25，0.5]和权重[0.5，0.5]的GMM分布</p></figure><p id="844e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我觉得很懒。我不想计算最优化方程的导数的解析形式，这太复杂了。相反，让我们使用进化策略(ES)算法和蒙特卡罗采样来优化函数！这很好，因为我们不再需要任何梯度信息；更多信息，请参见我写的关于ES <a class="ae kc" rel="noopener" target="_blank" href="/evolutionary-strategy-a-theoretical-implementation-guide-9176217e7ed8"> <strong class="kf ir">这里</strong> </a>的文章。对于正向和反向情况，我们将针对<strong class="kf ir"> q(x) </strong>的均值和标准差进行优化。</p><p id="cda3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">先说<strong class="kf ir">前进KL发散</strong>。学习曲线如下所示:</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/12c0b4ac8f52debde1699d7b37076e17.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*DLV56st61K4Y3E6UXOy3OA.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="42b6" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最终，该算法的平均值为0.0401，标准偏差为1.65，我们可以看到最终的<strong class="kf ir"> q </strong>分布相对于下面的<strong class="kf ir"> p </strong>分布的位置:</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/26bb89fc28fea323b1f70f9dd02c9cf8.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/format:webp/1*g8brz8djZQosSZ4C5y76fQ.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片:最小化KL向前发散</p></figure><p id="106a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">显然,“预测”分布并没有确定“真实”分布的任何一种模式，而是证实了从理论中得出的均值寻找行为。</p><p id="abb0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，让我们对<strong class="kf ir">反向KL发散</strong>做同样的事情。同样，学习曲线如下:</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/f78e6aca0426047ee37427a045faa5a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*exVvP7obW-ZqcnbkCxMbEg.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="4308" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这一次，算法确定平均值为-0.745，标准偏差为0.585。我们可以看到合成的<strong class="kf ir"> q </strong>分布相对于下面的<strong class="kf ir"> p </strong>分布的位置:</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/469787adb72c98aaa832f4b0683d06c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*Ra4lr3rc_l-Tkn7xxAlDPw.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片:最小化反向KL分歧</p></figure><p id="2fe1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">显然，优化反向KL发散导致从“预测”分布中寻找模式的行为，几乎忽略了“真实”分布的其他模式。</p></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><p id="a9f4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">总的来说，当优化模型目标函数时，前向和反向KL发散之间的细微差异会导致不同的行为；重要的是要仔细考虑哪一种更适合您的特定场景！作为机器学习设置中的一般规则，我们在监督学习中使用正向KL散度，在强化学习中使用反向KL散度。</p><p id="d4ae" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您觉得这篇文章有用，请考虑:</p><ul class=""><li id="e86a" class="mm mn iq kf b kg kh kk kl ko mo ks mp kw mq la mr ms mt mu bi translated">跟踪我🙌</li><li id="0082" class="mm mn iq kf b kg mv kk mw ko mx ks my kw mz la mr ms mt mu bi translated"><a class="ae kc" href="https://medium.com/subscribe/@rohan.tangri" rel="noopener"> <strong class="kf ir">订阅我的邮件通知</strong> </a>永不错过上传📧</li><li id="75a9" class="mm mn iq kf b kg mv kk mw ko mx ks my kw mz la mr ms mt mu bi translated">使用我的媒介<a class="ae kc" href="https://medium.com/@rohan.tangri/membership" rel="noopener"> <strong class="kf ir">推荐链接</strong> </a> <strong class="kf ir"> </strong>直接支持我并获得无限量的优质文章🤗</li></ul><p id="4de7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">推广的方式，我希望这篇文章是一个有趣的阅读，并让我知道你的想法！！</p></div></div>    
</body>
</html>