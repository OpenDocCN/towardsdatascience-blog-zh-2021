<html>
<head>
<title>From pandas to PySpark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从熊猫到PySpark</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/from-pandas-to-pyspark-fd3a908e55a0?source=collection_archive---------7-----------------------#2021-09-02">https://towardsdatascience.com/from-pandas-to-pyspark-fd3a908e55a0?source=collection_archive---------7-----------------------#2021-09-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/ad02c21c88a2631129533aeaf46cf905.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*7svl6b4_PwXadlAw"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">照片由<a class="ae jg" href="https://unsplash.com/@heyerlein?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">h·海尔林</a>在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><div class=""/><div class=""><h2 id="2a91" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated">利用您的熊猫数据操作技能来学习PySpark</h2></div><p id="eab3" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于数据分析师、数据科学家和任何与数据打交道的人来说，能够熟练、高效地操作大数据是一项有用的技能。如果您已经熟悉Python和pandas，并且想要学习讨论大数据，一个好的开始方式是熟悉PySpark，这是一个用于Apache Spark的Python API，<a class="ae jg" href="https://www.ibm.com/cloud/blog/hadoop-vs-spark" rel="noopener ugc nofollow" target="_blank">一个流行的大数据开源数据处理引擎</a>。在这篇文章中，我们将并排比较用于基本数据操作任务的pandas代码片段和它们在PySpark中的对应部分。</p><p id="1c2d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">这篇文章假设读者能够熟练地使用Python中的pandas操作数据。</em></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="e1bd" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">0.资料组📦</h1><p id="d748" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">让我们从导入必要的库开始。在PySpark中，我们需要创建一个Spark会话。一旦创建了Spark会话，就可以从:<a class="ae jg" href="http://localhost:4040/" rel="noopener ugc nofollow" target="_blank"> http://localhost:4040/ </a>访问Spark web用户界面(Web UI)。下面定义的应用程序名称“教程”将作为应用程序名称显示在Web UI的右上角。在这篇文章中，我们不会使用Web UI，但是，如果你有兴趣了解更多，请查看官方文档。</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="c665" class="ni md jj ne b gy nj nk l nl nm">import pandas as pd<br/>from pyspark.sql import SparkSession<br/>spark = SparkSession.builder.appName('tutorial').getOrCreate()</span></pre><p id="f0ee" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这篇文章中，我们将使用<a class="ae jg" href="https://github.com/mwaskom/seaborn-data/blob/master/penguins.csv" rel="noopener ugc nofollow" target="_blank">企鹅数据集</a>。使用下面的脚本，我们将在工作目录中保存数据的修改版本<code class="fe nn no np ne b">penguins.csv</code>。</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="6696" class="ni md jj ne b gy nj nk l nl nm">from seaborn import load_dataset<br/>(load_dataset('penguins')<br/>    .drop(columns=['culmen_length_mm', 'culmen_depth_mm'])<br/>    .rename(columns={'flipper_length_mm': 'flipper',<br/>                     'body_mass_g': 'mass'})<br/>    .to_csv('penguins.csv', index=False))</span></pre></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="bc2e" class="mc md jj bd me mf mg mh mi mj mk ml mm kp mn kq mo ks mp kt mq kv mr kw ms mt bi translated">1.比较🔎</h1><p id="1703" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">现在，让我们看看两个库之间的语法比较。在本节中，只显示PySpark输出，以减少帖子的混乱。</p><h2 id="1bdd" class="ni md jj bd me nq nr dn mi ns nt dp mm lh nu nv mo ll nw nx mq lp ny nz ms oa bi translated">📍 1.1.基础</h2><p id="9ed5" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">两个库的数据对象都被称为data frame:pandas data frame vs PySpark data frame。让我们导入数据并检查其形状:</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="8172" class="ni md jj ne b gy nj nk l nl nm"># 🐼 pandas <br/>df = pd.read_csv('penguins.csv')<br/>df.shape</span><span id="e4c1" class="ni md jj ne b gy ob nk l nl nm"># 🎇 PySpark<br/>df = spark.read.csv('penguins.csv', header=True, inferSchema=True)<br/>df.count(), len(df.columns)</span></pre><figure class="mz na nb nc gt iv gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/d33fcdac3987628e1aa46e0d5358dfb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:160/format:webp/1*DaW9-ug4i5CffnKkdmuwrg.png"/></div></figure><p id="7c26" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当使用PySpark导入数据时，第一行被用作标题，因为我们指定了<code class="fe nn no np ne b">header=True</code>，并且数据类型被推断为更合适的类型，因为我们设置了<code class="fe nn no np ne b">inferSchema=True</code>。如果您感到好奇，可以尝试不使用这些选项进行导入，并检查数据帧及其数据类型(与pandas类似，您可以使用PySpark数据帧的<code class="fe nn no np ne b">df.dtypes</code>来检查数据类型)。</p><p id="ed7e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">与熊猫DataFrame不同，PySpark DataFrame没有<code class="fe nn no np ne b">.shape</code>这样的属性。所以为了得到数据形状，我们分别找到行数和列数。</p><p id="ad64" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，让我们检查有关数据的高级信息:</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="8ef5" class="ni md jj ne b gy nj nk l nl nm"># 🐼 pandas <br/>df.info()</span><span id="be74" class="ni md jj ne b gy ob nk l nl nm"># 🎇 PySpark<br/>df.printSchema()</span></pre><figure class="mz na nb nc gt iv gh gi paragraph-image"><div class="gh gi od"><img src="../Images/b9d6a14a135572575e970d631eef40bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*1lvegpAIOiKlvFzzGId9dg.png"/></div></figure><p id="1fc0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">虽然这个方法不会给<code class="fe nn no np ne b">df.info()</code>相同的输出，但它是最接近的内置方法之一。是时候看看数据的开头了:</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="c35e" class="ni md jj ne b gy nj nk l nl nm"># 🐼 pandas <br/>df.head()</span><span id="9882" class="ni md jj ne b gy ob nk l nl nm"># 🎇 PySpark<br/>df.show(5)</span></pre><figure class="mz na nb nc gt iv gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/2cc7481f3c8bdf7dfa1716e953822fa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*sJdd7bU1DY4GmB8Wd6ObBg.png"/></div></figure><p id="5158" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">默认情况下，如果超过20行，<code class="fe nn no np ne b">df.show()</code>将显示20行。PySpark DataFrame其实有一个叫<code class="fe nn no np ne b">.head()</code>的方法。运行<code class="fe nn no np ne b">df.head(5)</code>提供如下输出:</p><figure class="mz na nb nc gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi of"><img src="../Images/4dd285b411edce7b203710ac882db451.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ybSNhTJ_SjT-kqoi2-iNEA.png"/></div></div></figure><p id="599b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">来自<code class="fe nn no np ne b">.show()</code>方法的输出更加简洁，所以我们将在这篇文章的剩余部分使用<code class="fe nn no np ne b">.show()</code>来查看数据集的顶部行。现在让我们看看如何选择列:</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="1eb9" class="ni md jj ne b gy nj nk l nl nm"># 🐼 pandas <br/>df[['island', 'mass']].head(3)</span><span id="64de" class="ni md jj ne b gy ob nk l nl nm"># 🎇 PySpark<br/>df[['island', 'mass']].show(3)</span></pre><figure class="mz na nb nc gt iv gh gi paragraph-image"><div class="gh gi og"><img src="../Images/c9022576cde276240b4b10a0b9e81361.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*u_GvlYYiFpkAVXyrDztUBQ.png"/></div></figure><p id="2bbb" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">虽然我们在这里可以使用几乎类似熊猫的语法，但以下版本的代码片段可能更常用于在PySpark中选择列:</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="fc3c" class="ni md jj ne b gy nj nk l nl nm">df.select('island', 'mass').show(3)<br/>df.select(['island', 'mass']).show(3)</span></pre><h2 id="66b5" class="ni md jj bd me nq nr dn mi ns nt dp mm lh nu nv mo ll nw nx mq lp ny nz ms oa bi translated">📍 1.2.过滤</h2><p id="fd22" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">让我们看看如何根据条件过滤数据:</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="6b94" class="ni md jj ne b gy nj nk l nl nm"># 🐼 pandas <br/>df[df['species']=='Gentoo'].head()</span><span id="5d84" class="ni md jj ne b gy ob nk l nl nm"># 🎇 PySpark<br/>df[df['species']=='Gentoo'].show(5)</span></pre><figure class="mz na nb nc gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi od"><img src="../Images/1ec6d7e67cd18f1c9c1497a08b8b190b.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*nyAWj9FU-K_OLu2cyhuZ1A.png"/></div></div></figure><p id="c489" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这两个库的语法几乎相同。为了获得相同的输出，我们也可以使用:</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="b42f" class="ni md jj ne b gy nj nk l nl nm">df.filter(df['species']=='Gentoo').show(5) <!-- -->df.filter("species=='Gentoo'").show(5)<!-- --> </span></pre><p id="2203" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面显示了一些常见的过滤器比较:</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="dd6e" class="ni md jj ne b gy nj nk l nl nm"># 🐼 pandas <br/>2a df[df['species'].isin(['Chinstrap', 'Gentoo'])].head()<br/>3a df[df['species'].str.match('G.')].head()<br/>4a df[df['flipper'].between(225,229)].head()<br/>5a df[df['mass'].isnull()].head()</span><span id="abd9" class="ni md jj ne b gy ob nk l nl nm">1b df.loc[df['species']!='Gentoo'].head()<br/>2b df[~df['species'].isin(['Chinstrap', 'Gentoo'])].head()<br/>3b df[-df['species'].str.match('G.')].head()<br/>4b df[~df['flipper'].between(225,229)].head()<br/>5b df[df['mass'].notnull()].head()</span><span id="1722" class="ni md jj ne b gy ob nk l nl nm">6 df[(df['mass']&lt;3400) &amp; (df['sex']=='Male')].head()<br/>7 df[(df['mass']&lt;3400) | (df['sex']=='Male')].head()</span><span id="ddf7" class="ni md jj ne b gy ob nk l nl nm"># 🎇 PySpark<br/>2a df[df['species'].isin(['Chinstrap', 'Gentoo'])].show(5)<br/>3a df[df['species'].rlike('G.')].show(5)<br/>4a df[df['flipper'].between(225,229)].show(5)<br/>5a df[df['mass'].isNull()].show(5)</span><span id="c698" class="ni md jj ne b gy ob nk l nl nm">1b df[df['species']!='Gentoo'].show(5)<br/>2b df[~df['species'].isin(['Chinstrap', 'Gentoo'])].show(5)<br/>3b df[~df['species'].rlike('G.')].show(5)<br/>4b df[~df['flipper'].between(225,229)].show(5)<br/>5b df[df['mass'].isNotNull()].show(5)</span><span id="a077" class="ni md jj ne b gy ob nk l nl nm">6 df[(df['mass']&lt;3400) &amp; (df['sex']=='Male')].show(5)<br/>7 df[(df['mass']&lt;3400) |(df['sex']=='Male')].show(5)</span></pre><p id="3c14" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">虽然在pandas中<code class="fe nn no np ne b">~</code>和<code class="fe nn no np ne b">-</code>都用作否定，但是在PySpark中只有<code class="fe nn no np ne b">~</code>用作有效否定。</p><h2 id="e332" class="ni md jj bd me nq nr dn mi ns nt dp mm lh nu nv mo ll nw nx mq lp ny nz ms oa bi translated">📍 1.3.整理</h2><p id="3f44" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">让我们对数据进行分类，检查质量<em class="lu">最小的5行</em>:</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="070c" class="ni md jj ne b gy nj nk l nl nm"># 🐼 pandas <br/>df.nsmallest(5, 'mass')</span><span id="da8d" class="ni md jj ne b gy ob nk l nl nm"># 🎇 PySpark<br/>df[df['mass'].isNotNull()].orderBy('mass').show(5)</span></pre><figure class="mz na nb nc gt iv gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/96b1a3eaa658c13e4ce30b349b177548.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*V-3vN_DJznXqphUr8fZDpw.png"/></div></figure><p id="f85e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Pandas的<code class="fe nn no np ne b">.nsmallest()</code>和<code class="fe nn no np ne b">.nlargest()</code>方法明智地排除了缺失值。然而，PySpark没有等效的方法。为了获得相同的输出，我们首先过滤掉缺少<em class="lu">质量</em>的行，然后我们对数据进行排序并检查前5行。如果没有丢失数据，语法可以缩短为:<code class="fe nn no np ne b">df.orderBy(‘mass’).show(5)</code>。</p><p id="8e9c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们看看使用<code class="fe nn no np ne b">.sort()</code>代替<code class="fe nn no np ne b">.orderBy()</code>的另一种排序方式:</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="5d55" class="ni md jj ne b gy nj nk l nl nm"># 🐼 pandas <br/>df.nlargest(5, 'mass')</span><span id="8775" class="ni md jj ne b gy ob nk l nl nm"># 🎇 PySpark<br/>df.sort('mass', ascending=False).show(5)</span></pre><figure class="mz na nb nc gt iv gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/6c62cdec7e95ebee316f1bef6018c7e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*Zos6rN9-_1hvSF-mp_fxZA.png"/></div></figure><p id="75a7" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">语法的这些变化也是有效的:</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="abf5" class="ni md jj ne b gy nj nk l nl nm">df.sort(df['mass'].desc()).show(5)<br/>df.orderBy('mass', ascending=False).show(5)<br/>df.orderBy(df['mass'].desc()).show(5)</span></pre><p id="ec78" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以按多列排序，如下所示:</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="ec55" class="ni md jj ne b gy nj nk l nl nm"># 🐼 pandas <br/>df.sort_values(['mass', 'flipper'], ascending=False).head()</span><span id="6ac0" class="ni md jj ne b gy ob nk l nl nm"># 🎇 PySpark<br/>df.orderBy(['mass', 'flipper'], ascending=False).show(5)</span></pre><figure class="mz na nb nc gt iv gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/705c5810d51707b15edeef7fa767be7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/1*owo3yCuBwjIf-RFYbwGXWA.png"/></div></figure><p id="8fbd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在PySpark里，你可以这样不用单子就脱身:<code class="fe nn no np ne b">df.orderBy(‘mass’, ‘flipper’, ascending=False).show(5)</code>。要按不同方向的多列排序:</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="b1a4" class="ni md jj ne b gy nj nk l nl nm"># 🐼 pandas <br/>df.sort_values(['mass', 'flipper'], ascending=[True, False]).head()</span><span id="ad34" class="ni md jj ne b gy ob nk l nl nm"># 🎇 PySpark<br/>df[df['mass'].isNotNull()]\<br/>  .sort('mass', 'flipper', ascending=[True, False]).show(5)</span></pre><figure class="mz na nb nc gt iv gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/67f4aa3e59947a86ecb10ddec049a51d.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*EnccT-bvoWO3FS1FmDBdgw.png"/></div></figure><p id="a5e8" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里有一个替代方案:</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="d290" class="ni md jj ne b gy nj nk l nl nm">df[df['mass'].isNotNull()]\<br/>  .orderBy(df['mass'].asc(), df['flipper'].desc()).show(5)</span></pre><h2 id="27be" class="ni md jj bd me nq nr dn mi ns nt dp mm lh nu nv mo ll nw nx mq lp ny nz ms oa bi translated">📍 1.4.聚合</h2><p id="5c8d" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">现在，让我们看几个汇总数据的例子。简单的聚合非常类似，如下所示:</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="b02a" class="ni md jj ne b gy nj nk l nl nm"># 🐼 pandas <br/>df.agg({‘flipper’: ‘mean’})</span><span id="15c1" class="ni md jj ne b gy ob nk l nl nm"># 🎇 PySpark<br/>df.agg({'flipper': 'mean'}).show()</span></pre><figure class="mz na nb nc gt iv gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/353564ab1fde29969920ff1e0d2824ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/1*emk2wDaooxmKCPEY1U6Zow.png"/></div></figure><p id="ec1c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">查看多个聚合时，我们需要采用不同的方法:</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="997b" class="ni md jj ne b gy nj nk l nl nm"># 🐼 pandas <br/>df.agg({'flipper': ['min', 'max']})</span><span id="d79f" class="ni md jj ne b gy ob nk l nl nm"># 🎇 PySpark<br/>from pyspark.sql import functions as F<br/>df.agg(F.min('flipper'), F.max('flipper')).show()</span></pre><figure class="mz na nb nc gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi om"><img src="../Images/d3b631dd00b0acb9e53f6a18ac2ac3c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:530/format:webp/1*ZvLO7cbvRefR4EZb_4UVKA.png"/></div></div></figure><p id="736c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">要获取列中的不同值:</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="0484" class="ni md jj ne b gy nj nk l nl nm"># 🐼 pandas <br/>df['species'].unique()</span><span id="0b1b" class="ni md jj ne b gy ob nk l nl nm"># 🎇 PySpark<br/>df.select('species').distinct().show()</span></pre><figure class="mz na nb nc gt iv gh gi paragraph-image"><div class="gh gi on"><img src="../Images/4942239148d09f87a2395df2f476d794.png" data-original-src="https://miro.medium.com/v2/resize:fit:226/format:webp/1*T_Fmt3kMckljvPR5Jk2LSg.png"/></div></figure><p id="93bc" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">要在一列中获取多个不同的值，请执行以下操作:</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="b69e" class="ni md jj ne b gy nj nk l nl nm"># 🐼 pandas <br/>df['species'].nunique()</span><span id="1aaa" class="ni md jj ne b gy ob nk l nl nm"># 🎇 PySpark<br/>df.select('species').distinct().count()</span></pre><figure class="mz na nb nc gt iv gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/462418ec775db2b831fd450303803906.png" data-original-src="https://miro.medium.com/v2/resize:fit:42/format:webp/1*ptlQaWOlHhGOta37sw-KhQ.png"/></div></figure><h2 id="c7e8" class="ni md jj bd me nq nr dn mi ns nt dp mm lh nu nv mo ll nw nx mq lp ny nz ms oa bi translated">📍 1.5.按组汇总</h2><p id="245e" class="pw-post-body-paragraph ky kz jj la b lb mu kk ld le mv kn lg lh mw lj lk ll mx ln lo lp my lr ls lt im bi translated">到目前为止，您可能已经注意到PySpark在方法和函数中使用了<em class="lu">驼峰</em>。对于<code class="fe nn no np ne b">.groupBy()</code>也是如此。下面是一个简单的按聚合分组的示例:</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="55fb" class="ni md jj ne b gy nj nk l nl nm"># 🐼 pandas <br/>df.groupby('species')['mass'].mean()</span><span id="6b4b" class="ni md jj ne b gy ob nk l nl nm"># 🎇 PySpark<br/>df.groupBy('species').agg({'mass': 'mean'}).show()</span></pre><figure class="mz na nb nc gt iv gh gi paragraph-image"><div class="gh gi op"><img src="../Images/5b2a580fbe64c1a02b880c0da7bb2d26.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*t8ABhadZD9Lbjhv5101pRA.png"/></div></figure><p id="9036" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以下是聚合多个选定列的示例:</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="f6a4" class="ni md jj ne b gy nj nk l nl nm"># 🐼 pandas <br/>df.groupby(‘species’).agg({‘flipper’: ‘sum’, ‘mass’: ‘mean’})</span><span id="b868" class="ni md jj ne b gy ob nk l nl nm"># 🎇 PySpark<br/>df.groupBy('species').agg({'flipper': 'sum', 'mass': 'mean'}).show()</span></pre><figure class="mz na nb nc gt iv gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/7976ae3dd82bec2517d5581e00e9cc6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:838/format:webp/1*AHXy654HvK-fq8EESKRLWw.png"/></div></figure><p id="8ac0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们不指定列，它将显示所有数字列的统计信息:</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="05e1" class="ni md jj ne b gy nj nk l nl nm"># 🐼 pandas <br/>df.groupby('species').mean()</span><span id="a74a" class="ni md jj ne b gy ob nk l nl nm"># 🎇 PySpark<br/>df.groupBy('species').mean().show()</span></pre><figure class="mz na nb nc gt iv gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/500deda6e86e56987849d72ba20e910b.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*XPqMfcN4Ukc3TbnXEgDGjQ.png"/></div></figure><p id="f724" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们也可以用<code class="fe nn no np ne b">.avg()</code>代替<code class="fe nn no np ne b">.mean()</code>。换句话说，我们可以使用<code class="fe nn no np ne b">df.groupBy(‘species’).avg().show()</code>。</p><p id="5527" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这就是这篇文章的全部内容！希望这些比较对您有用，并对PySpark语法有所了解。您可能已经注意到，在基本任务方面，这两个库有很多相似之处。这使得对熊猫有工作知识的人更容易开始使用PySpark。</p><figure class="mz na nb nc gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi or"><img src="../Images/b2b8c93b70af741d58ac7583d76ae966.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*D-EhHwdnI5ANdim5"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">照片由<a class="ae jg" href="https://unsplash.com/@hishahadat?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">沙哈达特·拉赫曼</a>在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="d191" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">您想访问更多这样的内容吗？媒体会员可以无限制地访问媒体上的任何文章。如果您使用</em> <a class="ae jg" href="https://zluvsand.medium.com/membership" rel="noopener"> <em class="lu">我的推荐链接</em> </a>，<em class="lu">成为会员，您的一部分会费将直接用于支持我。</em></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="d24c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢您阅读我的文章。如果你感兴趣，这里有我的一些其他帖子的链接:<br/> ◼️️ <a class="ae jg" rel="noopener" target="_blank" href="/5-tips-for-pandas-users-e73681d16d17">给pandas用户的5个提示</a> <br/> ◼️️ <a class="ae jg" rel="noopener" target="_blank" href="/writing-5-common-sql-queries-in-pandas-90b52f17ad76">在pandas中聚合数据的5个提示</a> <br/> ◼️️ <a class="ae jg" rel="noopener" target="_blank" href="/writing-5-common-sql-queries-in-pandas-90b52f17ad76">在pandas中编写5个常见的SQL查询</a> <br/> ◼️️ <a class="ae jg" rel="noopener" target="_blank" href="/writing-advanced-sql-queries-in-pandas-1dc494a17afe">在pandas中编写高级SQL查询</a> <br/> ◼️️ <a class="ae jg" rel="noopener" target="_blank" href="/transforming-variables-in-a-pandas-dataframe-bce2c6ef91a1">如何在pandas DataFrame中转换变量</a></p><p id="911b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">再见🏃 💨</p></div></div>    
</body>
</html>