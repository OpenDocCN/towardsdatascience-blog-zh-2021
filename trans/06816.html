<html>
<head>
<title>Superior Person Name Recognition with Pre-Built Google BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用预建的Google BERT进行高级人名识别</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/superior-person-name-recognition-with-pre-built-google-bert-e6215186eae0?source=collection_archive---------7-----------------------#2021-06-20">https://towardsdatascience.com/superior-person-name-recognition-with-pre-built-google-bert-e6215186eae0?source=collection_archive---------7-----------------------#2021-06-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ef96" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">当基地需要电梯时</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/2f0fb0464a6d1d254df61a3c35d29463.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NI6NHguNmGRWO8gp"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@brett_jordan?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">布雷特·乔丹</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="0126" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用spaCy进行命名实体识别效果很好，但并不是在所有情况下都是如此，尤其是涉及人名时。然而，由于<a class="ae ky" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank">拥抱脸</a>，你可以作为一名ML工程师(而不是作为一名数据科学家)使用谷歌伯特模型，轻松提高个人NER准确性。</p><p id="742f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">免责声明</strong> : spaCy可以结合类似于我将要描述的技术，所以不要把spaCy排除在外，这篇文章只是结合该技术的一种替代方法。</p><h2 id="583f" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">伯特和拥抱脸上的几个字</h2><p id="de70" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">拥抱脸将自己描述为一个社区，在这里我们可以“构建、训练和部署由自然语言处理中的参考开源提供支持的最先进的模型。”</p><p id="ecad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个构建模型或使用其他人构建的模型的地方——这最后一点尤其重要。</p><p id="7b36" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Google BERT(来自变压器的双向编码器表示)是Google为NLP创造的技术，没有被它的工作方式所干扰。<a class="ae ky" href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html" rel="noopener ugc nofollow" target="_blank">看看这件由伯特建筑师事务所创作的作品，它深入探究了伯特是如何工作的</a>。简而言之，使用Google BERT构建的模型运行良好。是的，我知道，这是一个荒谬的过度简化，但这篇文章是关于使用BERT，而不是创建基于BERT的模型。</p><h2 id="f3ea" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">具有空间结果的样本</h2><p id="1675" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">下面是一个为循环抛出(基)空间的名称列表:</p><p id="a3c1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe mt mu mv mw b">names_string = “Hi there Jon Jones Jon Jones Jr. Jon Paul Jones John D. Jones”</code></p><p id="54c5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是一些Python代码，使用spaCy的基本NER和那个名字列表。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="d623" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果是:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="e150" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这并不可怕，但如果你需要它变得伟大，它将功亏一篑。</p><p id="f2be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">另一个选择是使用transformers Python包，它(多亏了拥抱脸)可以让你访问包括Google BERT在内的模型。</p><h2 id="1b05" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated"><code class="fe mt mu mv mw b">transformers</code> Python包</h2><p id="0ade" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">变形金刚包装是使用拥抱脸模型的关键部分。安装可能有点复杂，这取决于你是否已经安装了PyTorch和/或TensorFlow。在我的一台计算机上，我已经安装了TensorFlow，我所需要做的(运行我的示例，它可能不完全适合您的情况)就是运行:</p><p id="3e8b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe mt mu mv mw b">pip install transformers[torch]</code></p><p id="5b89" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我通过在命令行输入以下命令打开interactive Python来验证我的安装:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="f121" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">发生了几件重要的事情:</p><ol class=""><li id="4fc1" class="na nb it lb b lc ld lf lg li nc lm nd lq ne lu nf ng nh ni bi translated">情感分析模型是从拥抱脸自动下载的(这很重要)。</li><li id="495a" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nf ng nh ni bi translated">我从第二行代码中得到以下结果:</li></ol><p id="45b2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe mt mu mv mw b">[{‘label’: ‘POSITIVE’, ‘score’: 0.9998704791069031}]</code></p><p id="1e45" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">模型结果是这样返回的(至少对于我尝试过的所有NLP模型是这样):一个字典列表。好的，很好。是时候说出人名了。</p><h2 id="f1ed" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">NER谷歌伯特模型</h2><p id="c260" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">有几个关于拥抱脸的模型用于命名实体识别(NER)。我只关注其中一个——它被称为<code class="fe mt mu mv mw b">dslim/bert-large-NER.</code>。使用它，你所需要的(除了安装之外)就是如下代码。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="3215" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">导入引入了一些助手对象，用于随管道引入模型。<code class="fe mt mu mv mw b">pipeline</code>允许我指定从拥抱脸中引入什么模型。</p><p id="c9f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">执行前三行代码时，会自动从Hugging Face下载几个文件(如果您有防火墙限制，可以手动下载这些文件——如果需要，可以留下评论)。<strong class="lb iu">重要提示</strong>:BERT型号为1.3GB，下载需要一点时间。虽然您只下载一次，但是即使在下载之后，也总是需要4-6秒来实例化模型(您可能想要找出一种方法来将模型保存在内存中)。</p><p id="8ec5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe mt mu mv mw b">pipeline</code>设置模型以供使用，并返回一个对象，即您的“nlp”引擎。请注意，我给了对象一个字符串，我想为它获取命名实体。就像情感分析的简单例子一样，<code class="fe mt mu mv mw b">nlp</code>对象返回一个字典列表，只是这个更大。</p><p id="e949" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe mt mu mv mw b">[{‘entity’: ‘B-PER’, ‘score’: 0.99802744, ‘index’: 3, ‘word’: ‘Jon’, ‘start’: 9, ‘end’: 12}, {‘entity’: ‘I-PER’, ‘score’: 0.9969795, ‘index’: 4, ‘word’: ‘Jones’, ‘start’: 13, ‘end’: 18}, {‘entity’: ‘B-PER’, ‘score’: 0.99703735, ‘index’: 5, ‘word’: ‘Jon’, ‘start’: 19, ‘end’: 22}, {‘entity’: ‘I-PER’, ‘score’: 0.99666214, ‘index’: 6, ‘word’: ‘Jones’, ‘start’: 23, ‘end’: 28}, {‘entity’: ‘I-PER’, ‘score’: 0.8733999, ‘index’: 7, ‘word’: ‘Jr’, ‘start’: 29, ‘end’: 31}, {‘entity’: ‘B-PER’, ‘score’: 0.9935467, ‘index’: 9, ‘word’: ‘Jon’, ‘start’: 33, ‘end’: 36}, {‘entity’: ‘I-PER’, ‘score’: 0.9749524, ‘index’: 10, ‘word’: ‘Paul’, ‘start’: 37, ‘end’: 41}, {‘entity’: ‘I-PER’, ‘score’: 0.9960336, ‘index’: 11, ‘word’: ‘Jones’, ‘start’: 42, ‘end’: 47}, {‘entity’: ‘B-PER’, ‘score’: 0.9971614, ‘index’: 12, ‘word’: ‘John’, ‘start’: 48, ‘end’: 52}, {‘entity’: ‘I-PER’, ‘score’: 0.9858689, ‘index’: 13, ‘word’: ‘D’, ‘start’: 53, ‘end’: 54}, {‘entity’: ‘I-PER’, ‘score’: 0.4625939, ‘index’: 14, ‘word’: ‘.’, ‘start’: 54, ‘end’: 55}, {‘entity’: ‘I-PER’, ‘score’: 0.9968941, ‘index’: 15, ‘word’: ‘Jones’, ‘start’: 56, ‘end’: 61}]</code></p><h2 id="5298" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">这是件大事</h2><p id="8124" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">对我来说，这个BERT模型的特别之处在于<code class="fe mt mu mv mw b">B-PER</code> <code class="fe mt mu mv mw b">entity</code>。根据文档，BERT模型可以识别几种不同类型的实体:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/cf0303aea701503bb995668458d22de0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*DQdmzBo9SNsbIQAC5V4k-A.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片作者来自<a class="ae ky" href="https://huggingface.co/dslim/bert-base-NER" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/dslim/bert-base-NER</a></p></figure><p id="71d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对B-PER的描述有一点误导，因为B-PER不仅是另一个人名字之后的一个人名字的开头，它还是第一个人名字中的第一个名字。这里非常重要的一点是，B-PER是名称列表中每个名称的分隔符。(同样的概念也适用于其他实体类型，这同样很有价值。)</p><p id="c940" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请允许我剖析一下伯特的名单和NER实体。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/ca39ec0d75ecd74490efce8faf09a1c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*8rxZNbp_PafOoIG8JOjxpA.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="aa8e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">太棒了。每个实际的名字都是一个<code class="fe mt mu mv mw b">B-PER</code>实体。然后我需要做的就是通过遍历字典将这些名字组合在一起。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="fc90" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">输出并不完全是我想要的(很快就会变得更有意义)，但是很接近:</p><p id="7db8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe mt mu mv mw b">[[[‘Jon’, ‘Jones’]], [[‘Jon’, ‘Jones’, ‘Jr’]], [[‘Jon’, ‘Paul’, ‘Jones’]], [[‘John’, ‘D’, ‘.’, ‘Jones’]]]</code></p><p id="9408" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我可以加入这些，我就完成了，但这并不那么简单(但几乎如此)。</p><h2 id="37a0" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">不寻常名字的奇怪结果</h2><p id="a0d1" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我见过的“奇怪”的事情之一(即，我不知道它为什么这样做，但可能有一个完全合乎逻辑的原因)是从BERT返回的一个字典条目列表，它似乎分解了不常见的名称。就拿我的姓来说吧——纽格鲍尔。</p><p id="4051" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe mt mu mv mw b">[{‘entity’: ‘I-PER’, ‘score’: 0.9993844, ‘index’: 2, ‘word’: ’N’, ‘start’: 6, ‘end’: 7}, {‘entity’: ‘I-PER’, ‘score’: 0.99678946, ‘index’: 3, ‘word’: ‘##eu’, ‘start’: 7, ‘end’: 9}, {‘entity’: ‘I-PER’, ‘score’: 0.9744214, ‘index’: 4, ‘word’: ‘##ge’, ‘start’: 9, ‘end’: 11}, {‘entity’: ‘I-PER’, ‘score’: 0.92124516, ‘index’: 5, ‘word’: ‘##ba’, ‘start’: 11, ‘end’: 13}, {‘entity’: ‘I-PER’, ‘score’: 0.82978016, ‘index’: 6, ‘word’: ‘##uer’, ‘start’: 13, ‘end’: 16}]</code></p><p id="6283" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">伯特似乎把我的名字分解成了碎片，我猜测(没有证据)这是因为变形金刚在伯特体内是如何工作的。不管什么原因，这种情况必须处理，这就是为什么我没有从上一节开始完成我的代码。</p><p id="a03d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是最后一段代码，它建立在最后一段代码的基础上，没有修改最后一段代码。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="bdd4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这不仅消除了不常见名称的标签，还将文本连接在一起。(句点的替换类似于“John J. Jones”中的标签和交易的替换。)</p><p id="146b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">运行这段代码给了我想要的东西:</p><p id="20f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe mt mu mv mw b">[[‘Jon Jones’], [‘Jon Jones Jr’], [‘Jon Paul Jones’], [‘John D. Jones’]]</code></p><p id="9cc1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我必须大声承认，当我第一次看到这个run时，我震惊地发现所有的名字都是正确的。玩得好谷歌，拥抱脸，和谁建立了伯特的名字模型。打得好。</p><h2 id="5330" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">结论和改进</h2><p id="0580" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我发现伯特NER引擎非常准确，但注意它并不总是<em class="mz">准确，它只是<em class="mz">更准确</em>。但更直接的说，它标志着好和伟大的区别。</em></p><p id="fa44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以通过使用分数并保留从BERT(即transformers)返回的开始和结束字符串位置来改进我的代码。但是，您可能需要做一些修改:</p><ul class=""><li id="485a" class="na nb it lb b lc ld lf lg li nc lm nd lq ne lu nq ng nh ni bi translated">分数应该是由<code class="fe mt mu mv mw b">B-PER</code>类型分隔的名称序列中每个标记的平均分数</li><li id="3244" class="na nb it lb b lc nj lf nk li nl lm nm lq nn lu nq ng nh ni bi translated">开始是一个人序列中<code class="fe mt mu mv mw b">B-PER</code>的开始，结束是<em class="mz">最后</em> <code class="fe mt mu mv mw b">I-PER</code>的结束</li></ul><p id="0399" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望你喜欢这个，并有机会与<code class="fe mt mu mv mw b">transformers</code>一起工作。真的很了不起。我还想尽可能地尊重spaCy的好人们，并再次声明，我刚才展示的只是另一种做事方式(这可能会被纳入spaCy)。</p><h2 id="aa04" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">参考</h2><p id="2b05" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">[1]HuggingFace.org。(未注明)拥抱脸—<em class="mz">AI社区构建未来</em>。https://<a class="ae ky" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank">huggingface.co/</a></p><p id="e1f8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2]德夫林，雅各布和陈，刘明伟.(2018年11月2日。)<em class="mz">开源BERT:最先进的自然语言处理预培训</em>。<a class="ae ky" href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html" rel="noopener ugc nofollow" target="_blank">https://ai . Google blog . com/2018/11/open-sourcing-Bert-state-of-art-pre . html</a></p><p id="b965" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3]HuggingFace.org。(未标明)<em class="mz"> dslim/bert-base-NER </em>。<a class="ae ky" href="https://huggingface.co/dslim/bert-base-NER" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/dslim/bert-base-NER</a></p></div></div>    
</body>
</html>