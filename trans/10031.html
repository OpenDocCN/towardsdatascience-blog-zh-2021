<html>
<head>
<title>A Query Worth a Thousand Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一个价值一千个模型的查询</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-query-worth-a-thousand-models-48be1f414bf5?source=collection_archive---------37-----------------------#2021-09-21">https://towardsdatascience.com/a-query-worth-a-thousand-models-48be1f414bf5?source=collection_archive---------37-----------------------#2021-09-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="cfe2" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="4b07" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">尝试从结构化查询搜索中构建分类器</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/34368f123f912739591c0c5979eecfc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TxujEOtp6S1gDwIzXjgpJw.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片由<a class="ae lh" href="https://pixabay.com/users/terimakasih0-624267/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2341784" rel="noopener ugc nofollow" target="_blank">迪安·莫里亚蒂</a>从<a class="ae lh" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=2341784" rel="noopener ugc nofollow" target="_blank">皮克斯拜</a>拍摄</p></figure><p id="9d71" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">数据科学家面临的最大问题是什么？</p><p id="b2e7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了回答这个问题，我们可能会争论一整天。虽然有很多候选，但对我来说最重要的是访问新用例的标记数据。</p><p id="1825" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">举个例子，一个经理打电话来说:</p><blockquote class="me mf mg"><p id="824b" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">“我们需要能够自动识别收到的邮件是应该转到应收帐款部门还是客户服务部，这样我们就可以降低管理成本。”</p><p id="ad13" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">数据科学家:</p><p id="4661" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">"我能拿到他们放在每个桶里的邮件吗？"</p><p id="f037" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">经理:</p><p id="5852" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">“没有”</p><p id="9241" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">数据科学家:</p><p id="6176" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">*smh</p></blockquote><p id="2476" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">几乎所有的数据科学家都以某种形式面临这个问题。事实上，这个问题如此之大，以至于整个行业都在致力于帮助数据科学家解决这个问题。最近的一份<a class="ae lh" href="https://www.grandviewresearch.com/industry-analysis/data-collection-labeling-market" rel="noopener ugc nofollow" target="_blank">报告</a>估计该行业的收入在“13.077亿美元”的范围内。是的，再读一遍那个数字。不是大，是巨。</p><p id="ebbe" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这个行业如此之大，部分是因为人类很昂贵。因此，人工标记数据也很昂贵。当然，公司已经尝试使用免费劳动力来解决这个“昂贵的人力”问题的某些方面。不，我不是在说奴役人民。我在这里指的是验证码。如果你没有意识到，验证码不仅仅是用于安全，它们也是公司让人们免费标记他们数据的一种方式。</p><p id="8f9d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">但是，如果有一种方法，我们可以提供一个更自动化的方法来标记数据呢？如果我们可以使用现有的数据和为我们提供这些数据的智能作为代理来创建标签，然后用于训练模型，会怎么样？</p><p id="93e6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在本文中，我尝试了一种自动标记数据的方法，来训练一个可以用来对新数据进行评分的模型。剧透警报，使用Google I背后的智能开发结构化查询来获取训练数据和训练标签。一个查询抵得上一千个模型吗？让我们来了解一下！</p><h1 id="47af" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">问题空间</h1><p id="3620" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">在这次演示中，我们将尝试对文本进行分类。更具体地说，我们希望能够对网站文本进行分类，以识别公司网站是否涵盖感兴趣的特定主题。</p><p id="05ed" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">假设你有一个客户对更好地识别企业社会责任项目感兴趣。目标是确定公司可能在其网站上描述CSR计划的地方。问题是，尽管大多数财富500强公司确实使用某种类型的网址来提供有关其CSR计划的信息，但细节却很难确定。</p><p id="1bb7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">以<a class="ae lh" href="https://www.microsoft.com/en-us/corporate-responsibility" rel="noopener ugc nofollow" target="_blank">微软</a>为例。他们的企业社会责任网站通常被称为“……企业责任”但进一步审查后，我们看到他们的登录页面链接到几个不同的，更详细的计划。现在想象一下为财富500强的所有500家公司做这件事。可能需要一分钟。</p><p id="2498" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了节省一些时间并测试出解决这个问题的数据科学解决方案，我利用谷歌来捕获训练数据，并将它们分配给标签，这些标签可用于训练一个模型来对新文本进行评分，以进一步对该网站文本的内容进行分类。以下是该解决方案的简要概述:</p><blockquote class="me mf mg"><p id="9ee1" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">1.从结构化查询中获取前N名的Google页面结果</p><p id="2eae" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">2.从每个登录页面抓取文本数据</p><p id="ef82" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">3.从查询中为每个登录页面创建分类标签</p><p id="5ea6" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">4.训练多类分类模型</p><p id="08cc" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">5.保存模型并在财富500强公司的页面上评分，以确定具体的CSR计划</p><p id="e749" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">6.可视化结果</p></blockquote><h1 id="c941" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">从结构化查询构建标记数据</h1><p id="40fe" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">为了测试我们的解决方案，我们首先需要构建训练数据，并将它们与标签相关联。在这种情况下，我们对作为标签的CSR类别感兴趣。快速的网络搜索和<a class="ae lh" href="https://www.researchgate.net/publication/258126583_Which_CSR-Related_Headings_Do_Fortune_500_Companies_Use_on_Their_Websites" rel="noopener ugc nofollow" target="_blank">这篇</a>研究论文给了我一些寻找不同潜在类别的想法。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ni"><img src="../Images/a155af5435a327bd1189df096ef4af78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7-l4XxoDjiigpybcmDL_0w.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="6252" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里，我们重点关注以下CSR类型，作为我们数据的标签:</p><blockquote class="me mf mg"><p id="2bc1" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">人权</p><p id="96df" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">使用条件</p><p id="5e32" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">环境</p><p id="0f61" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">腐败</p><p id="9052" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">披露信息</p><p id="fded" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">公司治理</p><p id="3434" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">消费者利益</p><p id="673f" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">性别平等</p><p id="c073" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">职业融合</p><p id="40a5" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">赋税</p></blockquote><h1 id="17c2" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">计算环境</h1><blockquote class="me mf mg"><p id="0463" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">Windows 10操作系统</p><p id="3aa5" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">Python 3.7</p></blockquote><h2 id="163c" class="nj mm it bd mn nk nl dn mr nm nn dp mv lr no np mx lv nq nr mz lz ns nt nb iz bi translated">图书馆</h2><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nu nv l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">用Carbon.now创建的代码图像</p></figure><h2 id="07b8" class="nj mm it bd mn nk nl dn mr nm nn dp mv lr no np mx lv nq nr mz lz ns nt nb iz bi translated">密码</h2><p id="7534" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">第一组代码使用了python中可用的<a class="ae lh" href="https://pypi.org/project/googlesearch-python/" rel="noopener ugc nofollow" target="_blank"> googlesearch-python </a>库。我们从googlesearch-python库中导入“search ”,并构建一个传递查询字符串并返回前50个结果的函数。请注意，这50个结果可以根据您的需要进行调整，但该函数的默认设置是返回前50个Google搜索链接。</p><pre class="ks kt ku kv gt nw nx ny nz aw oa bi"><span id="024b" class="nj mm it nx b gy ob oc l od oe">import pandas as pd<br/>import numpy as np<br/>from googlesearch import search</span><span id="080e" class="nj mm it nx b gy of oc l od oe">def Gsearch(query, num_res = 50):<br/>    results = []<br/>    for i in search(query=query,tld=’co.in’,lang=’en’,num=num_res,stop=num_res,pause=5):<br/>        results.append(i)<br/>    return results</span></pre><p id="8ef9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在定义了我们的搜索函数之后，我们开发代码来迭代每个CSR类型以构造一个查询，我们将这个查询传递给googlesearch API，返回一个包含CSR标签和前50个结果列表的元组。</p><pre class="ks kt ku kv gt nw nx ny nz aw oa bi"><span id="032c" class="nj mm it nx b gy ob oc l od oe">#develop queries</span><span id="f764" class="nj mm it nx b gy of oc l od oe">csr = [‘human rights’,’working conditions’,’the environment’,’corruption’,<br/> ‘disclosing information’,’corporate governance’,’consumer interests’,<br/> ‘gender equality’,’occupational integration’,’taxes’]</span><span id="c538" class="nj mm it nx b gy of oc l od oe">results = [(c, Gsearch(‘corporate responsibility and ‘+c)) for c in csr]</span></pre><h1 id="c324" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">内脏检查</h1><p id="e12a" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">这是一般的思考过程，因为Google擅长返回与查询相关的信息，通过传递一个特定的和结构良好的查询，我们应该能够返回与我们的查询所代表的标签更具体相关的数据。通过提供一些不同的查询，我们应该能够训练一个模型，该模型能够识别语言数据何时更好地表示一个类而不是另一个类。</p><h1 id="d865" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">返回代码</h1><p id="a307" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">一旦我们的搜索代码完成了，下一步就是把所有的东西放入熊猫的数据框架中，以便可视化我们到目前为止的结果。有用的“explode”方法允许我们将每个链接结果放到数据帧中它自己的行中。</p><p id="8ce7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">但是链接本身并不足以训练一个模型。下一步是访问每个链接，从登录页面中抓取文本。为了完成下一步，我们使用Python中的“requests”和“bs4”(beautiful soup)库(注意:这一部分可能需要一段时间)。</p><pre class="ks kt ku kv gt nw nx ny nz aw oa bi"><span id="5869" class="nj mm it nx b gy ob oc l od oe">df = pd.DataFrame(results, columns=[‘label’,’links’]).explode(‘links’)</span><span id="b2af" class="nj mm it nx b gy of oc l od oe">#get website text<br/>import requests<br/>from bs4 import BeautifulSoup<br/>import re</span><span id="ecbc" class="nj mm it nx b gy of oc l od oe">def website_text(URL):<br/> #USER_AGENT = “Mozilla/5.0 (Macintosh; Intel Mac OS X 10.14; rv:65.0) Gecko/20100101 Firefox/65.0”<br/> #headers = {“user-agent” : USER_AGENT}<br/>    headers = {‘User-Agent’: ‘Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36’,’Accept-Encoding’: ‘gzip, deflate, br’,’Accept-Language’: ‘en-US,en;q=0.9,hi;q=0.8’}</span><span id="f5ef" class="nj mm it nx b gy of oc l od oe">    try:<br/>       resp = requests.get(URL, headers=headers)<br/>       if resp.status_code == 200:<br/>           soup = BeautifulSoup(resp.content, “html.parser”)<br/>           for script in soup([“script”, “style”]):<br/>             script.extract() <br/> <br/>           text = soup.get_text()<br/>           lines = (line for line in text.splitlines())<br/>           chunks = (phrase for line in lines for phrase in line.split(“ “))<br/>           text = ‘ \n’.join(chunk for chunk in chunks if chunk) <br/>           clean_output = re.sub(r’&lt;.+?&gt;’, ‘ ‘, text) <br/>        else:<br/>           clean_output = ‘failed to retreive text’<br/>     except:<br/>        clean_output = ‘failed to retreive text’<br/> <br/>     return clean_output</span><span id="fef5" class="nj mm it nx b gy of oc l od oe">df[‘website_text’] = df[‘links’].apply(website_text)<br/>df = df.loc[df[‘website_text’] != ‘failed to retreive text’]</span></pre><p id="1e07" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在我们有了一个包含标签、链接和这些链接的文本的数据框架。下一步是获得网站文本的“肉”,以提高我们捕捉有意义的语言数据的机会。此外，我们还希望快速清理文本数据，以增加至少一个基本的标准化级别。以下代码使用“nltk”库对句子进行标记化，删除任何少于7个单词的句子，并执行一些轻量级清理(例如，小写、删除停用词、删除字符/数字)。</p><pre class="ks kt ku kv gt nw nx ny nz aw oa bi"><span id="d6f9" class="nj mm it nx b gy ob oc l od oe">#clean the website text<br/>import nltk<br/>from nltk.corpus import stopwords<br/>stop_words = set(stopwords.words(‘english’))</span><span id="d3a1" class="nj mm it nx b gy of oc l od oe">def clean_sentences(text):<br/>    sent_text = nltk.sent_tokenize(text)<br/>    sentences = [s for s in sent_text if len(s.split()) &gt; 6]<br/>    if sentences:<br/>        sentences = ‘ ‘.join(sentences)<br/>        low = sentences.lower()<br/>        no_num = re.sub(r’\d+’, ‘’, low)<br/>        no_char = re.sub(r’[\W_]+’, ‘ ‘, no_num)<br/>        tokens = nltk.word_tokenize(no_char)<br/>        stop_words = set(stopwords.words(‘english’))<br/>        no_stop = [i for i in tokens if not i in stop_words]<br/>        return no_stop</span><span id="ccf7" class="nj mm it nx b gy of oc l od oe">df[‘clean_text’] = df[‘website_text’].apply(clean_sentences)<br/>df = df.dropna(subset=[‘clean_text’])</span></pre><p id="7747" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">快到了。</p><p id="cd13" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在我们需要训练一个模型。在这种情况下，我们将利用流行的“scikit-learn”库来训练一个简单的文本分类模型。由于我们有一个多类输出问题，我们将利用随机森林模型。</p><p id="ddd5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这里，我们遵循使用语言数据进行模型训练的标准步骤；使用term-frequency-inverse-document-frequency(<a class="ae lh" href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank">tfi df</a>)过程构建将单词映射到整数的vocab，将数据分成训练和测试数据集，将模型拟合到训练数据，并检查测试数据的准确性。</p><pre class="ks kt ku kv gt nw nx ny nz aw oa bi"><span id="e38e" class="nj mm it nx b gy ob oc l od oe">#set up data for ML<br/>from sklearn.feature_extraction.text import TfidfVectorizer <br/>from sklearn.model_selection import train_test_split <br/>from sklearn.ensemble import RandomForestClassifier</span><span id="f886" class="nj mm it nx b gy of oc l od oe">def dummy_fun(doc):<br/> return doc</span><span id="8135" class="nj mm it nx b gy of oc l od oe">tfidf = TfidfVectorizer(max_features=600, tokenizer = dummy_fun, <br/> preprocessor=dummy_fun, token_pattern=None, <br/> ngram_range=(1,2))</span><span id="b9f4" class="nj mm it nx b gy of oc l od oe">X = tfidf.fit_transform(df[‘clean_text’])<br/>y = df[‘label’]</span><span id="6ff7" class="nj mm it nx b gy of oc l od oe">X_train, X_test, y_train, y_test = train_test_split(<br/> X, y, test_size=0.33, random_state=42)</span><span id="1f90" class="nj mm it nx b gy of oc l od oe">clf = RandomForestClassifier(max_depth=3, random_state=0)<br/>clf.fit(X_train, y_train)</span><span id="5dfe" class="nj mm it nx b gy of oc l od oe">y_pred_test = clf.predict(x_test)<br/>print('Accuracy: '+str(accuracy_score(y_pred_test, y_test))<br/>&gt;&gt;&gt; 'Accuracy: 0.60'</span></pre><p id="eb45" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们表现如何？</p><p id="d360" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">准确性不好，但这只是冰山一角。我们可以采取许多额外的步骤来进一步优化我们的方法并提高模型性能。这里只是将这个解决方案提升到下一个级别的一些附加想法；</p><blockquote class="me mf mg"><p id="2d70" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">使用更精确查询语言，</p><p id="c6d0" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">包括单词嵌入作为特征，</p><p id="1a44" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">对词类应用更详细的清理过程，</p><p id="0194" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">尝试不同的模型类型，</p><p id="b486" class="li lj mh lk b ll lm kd ln lo lp kg lq mi ls lt lu mj lw lx ly mk ma mb mc md im bi translated">打开一瓶威士忌，站在我们的头上，让60%的准确率看起来像90%的准确率。</p></blockquote><p id="83d6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">要获得更详细的信息，我们可以利用scikit-learn提供的分类报告，该报告有助于我们更好地评估我们的模型在哪些方面表现良好，在哪些方面表现不佳。下面是在对整个数据集(训练+测试)的模型进行评分后，我们如何将报告放入一个易于阅读或保存的Pandas数据框架中:</p><pre class="ks kt ku kv gt nw nx ny nz aw oa bi"><span id="1f13" class="nj mm it nx b gy ob oc l od oe">y_pred = clf.predict(X)<br/>report = classification_report(y, y_pred, output_dict=True)<br/>r = pd.DataFrame(report).transpose()</span></pre><p id="56a2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们当前解决方案的细节显示，我们在识别人权方面有相当好的表现，但在工作条件方面不太好。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi og"><img src="../Images/62cb8674d5d8690ef87ba5f47c100c9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*Vw2U0q2XGzq2sLkh26zWXA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="086f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">考虑到60%的总体准确率并不可怕，我们采取下一步措施，在新的网站实例上对模型进行评分，以对财富500强公司的CSR计划进行分类。为了做到这一点，我们<a class="ae lh" href="https://dofo.com/blog/fortune-500-domain-names/" rel="noopener ugc nofollow" target="_blank">拉出</a>财富500强的登陆页面，使用与上面类似的代码，我们抓取每个链接，访问这些链接，并拉出相关的文本。这有助于为我们的模型创建数据集，数据集的设置方式与我们的训练数据的设置方式相同。</p><p id="3348" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">一旦dataframe设置了公司名称列、链接列和网站文本列，我就可以用这个模型给每个网站打分了。为了实现评分，我将数据预处理和模型评分打包到一个简单方便的函数中，该函数输出预测的标签以及与该标签相关联的概率。</p><pre class="ks kt ku kv gt nw nx ny nz aw oa bi"><span id="71ba" class="nj mm it nx b gy ob oc l od oe">def score_csr_model(text, tfidf_model, model):<br/>    clean = clean_sentences(text)<br/>    tfidf = tfidf_model.transform([clean])<br/>    pred = model.predict(tfidf)<br/>    pred = pred[0]<br/>    pred_proba = np.amax(model.predict_proba(tfidf))<br/>    return pred, pred_proba</span><span id="dbe3" class="nj mm it nx b gy of oc l od oe">df[‘predicted’] = df[‘website_text’].apply(score_csr_model, <br/> args = [tfidf,clf])</span></pre><p id="55ab" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在最后一步，我根据每类公司的平均预测概率，从公司样本中构建一个热图，以估计每家公司包含与每项CSR计划相关的信息的程度。为了生成热图，我首先需要根据公司名称和我们模型中该页面上确定的CSR计划类型对数据进行分组。作为此代码的一部分，我对模型中每个标签的预测概率进行了平均。</p><pre class="ks kt ku kv gt nw nx ny nz aw oa bi"><span id="80c0" class="nj mm it nx b gy ob oc l od oe">import seaborn<br/>import matplotlib.pyplot as plt</span><span id="d7b2" class="nj mm it nx b gy of oc l od oe">#unpack predicted column into two columns <br/>df[[‘predicted’,’proba’]] = pd.DataFrame(df[‘predicted’].tolist(),index=df.index).drop(columns = [‘predicted’])</span><span id="bd32" class="nj mm it nx b gy of oc l od oe">dfsub = df[['label','predicted','proba']].groupby(['label','predicted'])['proba'].mean().reset_index()<br/>dfsub= dfsub.pivot(index='label', columns='predicted', values='proba')</span><span id="8b96" class="nj mm it nx b gy of oc l od oe">figure, ax = plt.subplots(figsize=(10,10))<br/>ax.xaxis.tick_top() # x axis on top<br/>ax.xaxis.set_label_position('top')<br/>ax.set_xticklabels(list(dfsub.columns), size=14)<br/>ax.set_yticklabels(list(dfsub.index.values), size=14)<br/>#sns.set(font_scale=5)<br/>svm = sns.heatmap(dfsub, mask=(df_sub==0), center=0, cmap='Greens', cbar=False, <br/>                  xticklabels=True, yticklabels=True, linewidths=.1,ax=ax)</span><span id="60bb" class="nj mm it nx b gy of oc l od oe">figure = svm.get_figure()</span><span id="4115" class="nj mm it nx b gy of oc l od oe">figure.savefig('path/to/save/csr_svm_conf.png', orientation='landscape',<br/>               transparent=True,format='png', pad_inches=.1)</span></pre><p id="e375" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">调查结果中最明显的一点是，在消费者利益和工作条件方面，覆盖面似乎存在很大差距。在模型中，我们预测工作条件和消费者兴趣的准确性都很低，所以我们可能只是看到了这里表现的模型性能差。然而，有趣的是，我们的模型在识别人权倡议方面表现更好，但似乎更少的公司在其CSR沟通中关注人权倡议。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/656f79fdfb4eb6a8a1ffffb6afd88755.png" data-original-src="https://miro.medium.com/v2/resize:fit:1392/format:webp/1*bstr00ZV8-pvmFSEH_Gj3g.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="c6d5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">尽管只是一个演示，这里收集和分析的数据可能有一些有价值的用途。</p><p id="08ae" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于企业来说，也许你想与这些大公司中的一家合作，来领导与CSR类别之一相关的推广工作或产品。了解这些公司如何与这些努力保持一致，可以用来帮助关注最合适的人选。</p><p id="004f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于消费者来说，知道哪些公司符合你的价值观有助于我们知道我们想要支持哪些公司并向其购买产品。</p><p id="3ad2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于股东来说，投资于具有强大企业社会责任的盈利性公司的资金需要知道这些优先事项得到了有效的沟通。</p><p id="883b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对于监管者来说，了解企业社会责任倡议在不同行业的差异可能有助于支持那些需要更多关注企业社会责任倡议的行业的政策变化。</p><h1 id="a123" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">结论</h1><p id="4c11" class="pw-post-body-paragraph li lj it lk b ll nd kd ln lo ne kg lq lr nf lt lu lv ng lx ly lz nh mb mc md im bi translated">构建包含模型标签的训练数据是一项挑战，自该行业诞生以来一直困扰着数据科学家。在这个演示中，我使用Google搜索查询来帮助建立训练数据和相关联的标签，而不需要人工干预。</p><p id="7d14" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">无论采用哪种方法，这篇文章都代表了一种尝试为分类任务自动标记有价值的训练数据的可能方法。</p><p id="770c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">比如参与学习更多关于数据科学、职业发展或糟糕的商业决策的知识？<a class="ae lh" href="https://www.facebook.com/groups/thinkdatascience" rel="noopener ugc nofollow" target="_blank">加入我</a>。</p></div></div>    
</body>
</html>