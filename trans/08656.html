<html>
<head>
<title>Learn To Reproduce Papers: Beginner’s Guide</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">学习复制论文:初学者指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/learn-to-reproduce-papers-beginners-guide-2b4bff8fcca0?source=collection_archive---------8-----------------------#2021-08-10">https://towardsdatascience.com/learn-to-reproduce-papers-beginners-guide-2b4bff8fcca0?source=collection_archive---------8-----------------------#2021-08-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="cbf1" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="320b" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated"><em class="ko">如何理解深度学习论文并实现所描述的方法的分步指导。举个例子:今天我们要复制一篇关于图像超分辨率的基础论文。</em></h2></div><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/174bdb436cf3777284afe44978124e3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3w_rpS7hi_6AGydJJN8Y4A.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">来源:https://arxiv.org/abs/1608.00367<a class="ae lf" href="https://arxiv.org/abs/1608.00367" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="3b71" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">对于数据科学家来说，能够复制最新的科学论文是一项极具竞争力的技能。因为没多少人能做到。</p><p id="f0e7" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">如果你想成为一名思想家，了解黑匣子里正在发生的事情，激发你的创造力，或者成为第一个将最新科学研究引入商业的开发者——这篇文章就是为你准备的。</p><p id="f756" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">今天我们将讨论如何选择一篇“好”的论文，这对初学者来说相对容易；我们将概述典型的论文结构和重要信息的位置；我会给你一步一步的指导，告诉你如何实现纸质实现，并分享一些链接，当你遇到困难时，这些链接会对你有所帮助。</p><p id="1b00" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">这个帖子也有一个“实践部分”，所以今天我们要复制一篇关于图像超分辨率的基础论文。我不指望你有超分辨率的经验，但我假设你已经与卷积神经网络。这次的深度学习框架是Tensorflow 2。</p><p id="786f" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja">内容</strong> <br/> —从哪里开始？<br/> —论文结构:跳过什么，读什么<br/> —举例。理解论文<br/> —哪里可以找到帮助？<br/> —举例。我们来编码一下<br/> —祝你好运！</p><h1 id="3290" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">从哪里开始？</h1><p id="d72c" class="pw-post-body-paragraph lg lh iq li b lj mu ka ll lm mv kd lo lp mw lr ls lt mx lv lw lx my lz ma mb ij bi translated">如果你希望你的学习顺利，没有压力，你应该找到一个“好”的论文。作为一个起点，我真的推荐你选择<strong class="li ja"> <em class="mz">一篇老的高被引论文</em> </strong>来描述你所熟悉的概念。</p><ul class=""><li id="078f" class="na nb iq li b lj lk lm ln lp nc lt nd lx ne mb nf ng nh ni bi translated">老的高被引论文通常解释非常基本的概念，成为最近研究的基础。你知道基本原理——你也会更好地理解最近的论文。对于深度学习来说，2016年之前的论文被认为已经很老了。</li><li id="27fc" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nf ng nh ni bi translated">高被引论文可复制。这意味着许多其他科学家能够理解和实施这种方法。要想知道某篇论文被引用的次数，在谷歌学术谷歌一下。引用次数超过1000次的论文被认为是高被引论文。</li><li id="69a5" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nf ng nh ni bi translated">通常，较老的论文描述的是较简单的概念，这对于初学的你来说是一大加分。</li></ul><p id="9e83" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">这几周我一直在看关于超分辨率的论文，所以我选择了这个2016年的研究来和你一起重现——<a class="ae lf" href="https://arxiv.org/abs/1608.00367" rel="noopener ugc nofollow" target="_blank">加速超分辨率卷积神经网络</a>。它描述了用于超分辨率任务的第一个基于CNN的模型之一——fsr CNN(快速超分辨率CNN)。</p><p id="0627" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">超分辨率任务非常简单:取一张低分辨率(小，质量差)的图像，然后把它变成高分辨率(大，质量好)。正如我所说的，你不需要任何超分辨率的经验就能理解这篇论文，然而，CNN是一个你必须熟悉的概念。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi no"><img src="../Images/1f01c39c23917e01e6c050c3e1cd9261.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AVws2aMpfef-Azo0Lp6sWw.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><em class="ko">超分辨率任务:高级可视化。作者图片</em></p></figure><h1 id="28c8" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">论文结构:跳过什么，阅读什么</h1><p id="1ae7" class="pw-post-body-paragraph lg lh iq li b lj mu ka ll lm mv kd lo lp mw lr ls lt mx lv lw lx my lz ma mb ij bi translated">典型的深度学习论文具有以下结构:</p><p id="2b45" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">1.摘要<br/> 2。简介<br/> 3。相关工作<br/> 4。详细介绍<br/> 5。实验<br/> 6。结论<br/> 7。参考</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi np"><img src="../Images/ad88e3db196e7a373ef83a3d76d90e41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8CpCG0OfQeB0sd-EpSoRhg.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><em class="ko">典型深度学习论文的结构。作者图片</em></p></figure><p id="21b8" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja"> 1。摘要</strong>是“营销”的总结。它很短，重点是为什么这种方法比以前的方法更好，以及它的新颖之处。摘要发表在会议日程和在线档案中(如<a class="ae lf" href="https://arxiv.org/" rel="noopener ugc nofollow" target="_blank"> Arxiv </a>)，它们唯一的目标就是说服你阅读这篇特定的论文。你已经选择了一篇要阅读(和复制)的论文，所以可以随意跳过摘要。</p><p id="b623" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja"> 2。简介</strong>是一个重要的章节，也是必读的。它给出了该方法的高级概述。通常情况下，它并不太专业，而且非常“人性化”，所以先阅读介绍部分，在深入研究算法细节之前，先预热一下大脑。</p><p id="6260" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja"> 3。相关工作</strong>。所有科学论文(以及深度学习)都是相关的:每一项发现都是建立在数十名研究人员之前的工作基础上的。相关工作概述是每篇论文的必修部分。作者必须确保他们的工作是相关的，解决重要的问题，并且不重复其他研究人员以前做过的工作。这对于科学界来说是一个重要的部分——但对于我们(从业者)来说不是，所以跳过它！</p><p id="e1be" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">(好吧。有时你可能需要它——但只是在你寻找该领域其他基本论文/概念的情况下。)</p><p id="9dfd" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja"> 4。走近细节</strong>。有趣的事情开始了。这是论文中最复杂和最具挑战性的部分，也是最重要的部分(一定要读！).不要期望太高，也不要期望从一次阅读中理解所有的东西。这是你在编码时会一次又一次回来的部分。</p><p id="6882" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">不要害怕复杂的公式，大多数情况下，它们解释的是基本概念。我相信研究人员就是这样开玩笑的。过一会儿，你会习惯的。</p><p id="442e" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">在阅读论文时，捕捉你可能需要的所有信息——数据预处理技术、详细的神经网络架构、损失函数、训练技巧和后处理。尽你所能去得到它。如果你试了几次还是不明白，没关系，以后我会告诉你怎么做。</p><p id="ec76" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja"> 5。实验</strong>。这个部分充满了图表、表格和图像。通常，它包含有关数据集、训练和评估的详细信息，以及对该模型在各种超参数下的表现以及与其他论文中最先进方法的比较的综述。如果论文是关于计算机视觉的，也会有模型预测的可视化。我会说这一部分是你唯一感兴趣的部分。</p><p id="0c26" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja"> 6。结论</strong>是论文的总结，有时包含作者对未来研究的想法。你应该跳过它。</p><p id="8474" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja"> 7。参考文献</strong>。科学界(不仅仅是科学界)有一条严格的规则:如果研究人员使用了属于他人的观点，他/她需要添加对原始作品的引用(引用)。当这些参考文献中的概念已经在论文中解释过，或者不重要时，你可能会跳过大部分这样的参考文献。然而，有时作者可能会说:“我们使用了论文[2]中描述的模型架构，只修改了最终层中的激活”。在这种情况下，你需要找到并阅读文章[2]来完全理解这种方法。</p><p id="a077" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">现在——该读报了。关掉音乐，把手机调到飞行模式，喝杯茶。在接下来的30分钟里，你应该高度集中注意力，因为你正在潜入一个新的世界——令人兴奋，但相当具有挑战性。</p><p id="9648" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">在这一点上，我建议您暂停阅读论文<a class="ae lf" href="https://arxiv.org/abs/1608.00367" rel="noopener ugc nofollow" target="_blank">加速超分辨率卷积神经网络</a>，我们接下来将复制该论文。注意数据预处理、模型架构、损失函数和训练细节。</p><h1 id="e278" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">举例。理解论文</h1><p id="d086" class="pw-post-body-paragraph lg lh iq li b lj mu ka ll lm mv kd lo lp mw lr ls lt mx lv lw lx my lz ma mb ij bi translated">完成了吗？太好了。现在，让我们回顾一下我们从论文中获得的所有重要细节，以再现快速超分辨率CNN (FSRCNN)。</p><p id="8fed" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja"> FSRCNN:一般信息</strong></p><ul class=""><li id="c31d" class="na nb iq li b lj lk lm ln lp nc lt nd lx ne mb nf ng nh ni bi translated">所提出的方法是一种卷积神经网络，它采用低分辨率(LR)图像，“巧妙地”以某种因子放大它，并返回高分辨率(HR)图像。所以输出图像比输入大N倍。</li><li id="3973" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nf ng nh ni bi translated">放大因子N由用户定义，并且是像2、3、4等这样的数字。放大因子越大，恢复质量越差。</li><li id="e8f6" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nf ng nh ni bi translated">特定的放大因子需要单独的神经网络来训练。这些网络仅在最后(去卷积)层不同，并且可以共享所有其他层的权重。</li></ul><p id="4221" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja"> FSRCNN:模型架构</strong></p><ul class=""><li id="9048" class="na nb iq li b lj lk lm ln lp nc lt nd lx ne mb nf ng nh ni bi translated">模型由几个连续的卷积层和一个最终的反卷积层组成。下面是一个详细的流程:<br/> —内核大小= 5×5、滤波器个数= d <br/>的卷积层—参数化ReLU激活<br/> —内核大小= 1×1、滤波器个数= s的卷积层<br/> —参数化ReLU激活<br/> —内核大小= 3×3、滤波器个数= s <br/>的m个卷积层—参数化ReLU激活(不清楚这m个卷积层之间是否使用了激活函数。可能，他们是。)<br/> —卷积层，内核大小= 1×1，滤波器数量= d <br/> —参数ReLU激活<br/> —去卷积层，内核大小= 9×9，步幅=放大因子，滤波器数量=图像中的通道数量(由于某种原因，作者使用1作为通道数量，但对于彩色图像，它应该是3。还是我错过了什么？)<br/> —最后一层无激活功能</li><li id="9d49" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nf ng nh ni bi translated">零填充用于所有层。</li><li id="ff86" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nf ng nh ni bi translated">d、s、m是超参数。作者推荐使用d=32，s=5，m=1进行实时预测，使用d=56，s=12，m=4获得更好的恢复质量。</li></ul><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nq"><img src="../Images/be110f46c70585902a8f527548338568.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZgmHMrCAXqEyRg8ePPjNaQ.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><em class="ko"> FSRCNN架构。来源:</em> <a class="ae lf" href="https://arxiv.org/abs/1608.00367" rel="noopener ugc nofollow" target="_blank"> <em class="ko">加速超分辨率卷积神经网络</em> </a></p></figure><p id="9683" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja"> FSRCNN:数据</strong></p><ul class=""><li id="6344" class="na nb iq li b lj lk lm ln lp nc lt nd lx ne mb nf ng nh ni bi translated">模型在<a class="ae lf" href="https://www.kaggle.com/ll01dm/t91-image-dataset" rel="noopener ugc nofollow" target="_blank"> T91 </a>数据集上进行训练，并在<a class="ae lf" href="http://mmlab.ie.cuhk.edu.hk/projects/FSRCNN.html" rel="noopener ugc nofollow" target="_blank"> General-100 </a>上进行微调。</li><li id="bc0a" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nf ng nh ni bi translated">这些数据集只有高分辨率图像。低分辨率图像是通过缩小高分辨率图像来创建的。</li><li id="80ef" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nf ng nh ni bi translated">数据扩充是90度旋转和作物。每张图像都以各种可能的方式旋转了90度，并通过滑动窗口裁剪成小块。看起来这些扩充是在训练之前完成的(但不是随机进行的)，所以最终的训练集是预定义和固定的。</li></ul><p id="fdc5" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja"> FSRCNN:培训详情</strong></p><ul class=""><li id="e78b" class="na nb iq li b lj lk lm ln lp nc lt nd lx ne mb nf ng nh ni bi translated">损失是MSE(均方误差)。</li><li id="511f" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nf ng nh ni bi translated">优化器是随机梯度下降。</li><li id="2b54" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nf ng nh ni bi translated">卷积层训练期间的学习率是0.001，去卷积层是0.0001；微调学习率低两倍。</li><li id="5bfa" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nf ng nh ni bi translated">去卷积层的权重通过从具有零均值和标准偏差= 0.001的高斯分布中随机抽取来初始化。</li><li id="2fa1" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nf ng nh ni bi translated">卷积层的权重用[23]中为PReLU设计的方法初始化。这是对另一篇论文的引用，那篇论文解释了正规的初始化式。</li></ul><p id="3095" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">我们现在对FSRCNN有了相当多的了解，可以转到编码和训练。</p><h1 id="b8ef" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">去哪里寻求帮助？</h1><p id="c381" class="pw-post-body-paragraph lg lh iq li b lj mu ka ll lm mv kd lo lp mw lr ls lt mx lv lw lx my lz ma mb ij bi translated">对许多人来说，“复制论文”意味着“快速阅读论文，然后在互联网上寻找现成的实现”。这是最简单的方法，但不是有益的方法。我真的建议你从头开始，不要马上寻找现成的解决方案。至少自己做些事情——那是你学习的时候。</p><p id="824f" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">即使你完全是复印纸张的初学者，你也总有办法做到:</p><ul class=""><li id="8778" class="na nb iq li b lj lk lm ln lp nc lt nd lx ne mb nf ng nh ni bi translated">下载数据集，探索它，编写一个数据加载器。</li></ul><p id="255e" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">简单易行的任务，但一旦完成，它会给你信心并帮助你继续前进:</p><ul class=""><li id="a979" class="na nb iq li b lj lk lm ln lp nc lt nd lx ne mb nf ng nh ni bi translated">开始写模型架构，简化或者跳过不理解的部分。有一个奇怪的权重初始值设定项——现在跳过它，使用默认的。您以前从未使用过PReLU激活，请改用ReLU。你现在的目标是创建一个可训练的模型，不是纸上谈兵的模型或者性能好的模型，只是可训练的。它有输入，有输出，所以你可以运行培训。</li><li id="ed57" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nf ng nh ni bi translated">论文中有一个自定义丢失——用深度学习库中实现的类似的替换掉。</li><li id="f825" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nf ng nh ni bi translated">对优化器、数据扩充、后处理做同样的事情——简化或跳过。</li></ul><p id="0d94" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">你最终会被征召入伍。你甚至可以训练一下草稿，看看效果如何——也许结果不会那么糟糕🙂</p><p id="124d" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">然后填补空白，修复工作不佳的部分。首先，自己进行实验——测试你在写草稿时想到的想法，再读一遍论文，希望能抓住你之前漏掉的概念。如果你很快就卡住了，不要沮丧。你写了一份草稿，这是一个很大的进步，你已经学到了很多。所以下一次，下一篇论文你会写得更好。这是一个学习的过程。</p><p id="bc04" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">感觉完全卡住了？搜索的最佳时机。</p><p id="f43b" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">记住，我建议你选择一篇高被引论文。现在你会感受到好处。流行的报纸在互联网和博客上有许多描述复杂部分的实现。尽情享受吧！</p><p id="e3f4" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">第一个要检查的地方是代码为的<a class="ae lf" href="https://paperswithcode.com/" rel="noopener ugc nofollow" target="_blank">论文，这是一个很大的库，可能包含了所有流行论文的代码实现。这些实现要么是官方的，要么来自像你我这样的研究人员。例如，在PyTorch和Tensorflow中，FSRCNN在代码为</a>的论文上实现了<a class="ae lf" href="https://paperswithcode.com/paper/accelerating-the-super-resolution" rel="noopener ugc nofollow" target="_blank"> 7。(好了，现在是8，我也添加了我的实现。)</a></p><p id="bde0" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">你可以复制粘贴，但是要花时间去理解这些代码。这是最后一条建议。</p><h1 id="422f" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">举例。我们来编码吧！</h1><p id="0293" class="pw-post-body-paragraph lg lh iq li b lj mu ka ll lm mv kd lo lp mw lr ls lt mx lv lw lx my lz ma mb ij bi translated">现在我将向您展示如何用代码实现FSRCNN。深度学习库这次是Tensorflow 2。</p><p id="22ea" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">当你在实现论文中的方法时，没有必要100%准确地复制它。所以关掉完美主义模式，开心就好！使用不同的数据集，试验模型架构，添加数据扩充，改变损失函数，…</p><p id="cebb" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja">数据</strong></p><ul class=""><li id="8ab4" class="na nb iq li b lj lk lm ln lp nc lt nd lx ne mb nf ng nh ni bi translated">让我们使用<a class="ae lf" href="https://data.vision.ee.ethz.ch/cvl/DIV2K/" rel="noopener ugc nofollow" target="_blank"> DIV2K </a>来代替T91和General-100数据集。这是一个更新、更大的数据集，是专门为超分辨率任务创建的。</li><li id="0bb5" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nf ng nh ni bi translated">我们只从DIV2K获取高分辨率(HR)图像，包括训练和验证部分，总共900张图像。然后将训练和验证部分合并到一个文件夹中，并根据图像id创建自定义的训练-验证-测试分割:训练的id为1–700，验证的id为701–800，测试的id为801–900。低分辨率(LR)图像是通过双三次下采样从高分辨率图像生成的。</li><li id="dd0c" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nf ng nh ni bi translated">对于训练和验证数据扩充，让我们使用随机裁剪成648×648小块、水平翻转以及亮度、对比度、饱和度和色调的轻微变化。对于测试集，我们只对648×648的小块进行随机裁剪。在HR图像上执行数据增强，并且从增强的HR图像创建LR图像。</li><li id="1de8" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nf ng nh ni bi translated">选择这种随机裁剪大小，以便它甚至可以应用于DIV2K数据集中最小的图像。648可以被4整除，这是我们模型中的比例因子。</li><li id="9914" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nf ng nh ni bi translated">因此，这里是我们得到的<a class="ae lf" href="https://github.com/OlgaChernytska/Super-Resolution-with-FSRCNN/blob/main/utils/dataset.py" rel="noopener ugc nofollow" target="_blank"> Tensorflow数据加载器</a>。</li></ul><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nr"><img src="../Images/f1249d77ac97c8c1d9b8bd0e92822bff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E56Otk8HSVeyvKLqAiF7jw.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><em class="ko">数据准备流程。作者图片</em></p></figure><p id="7b30" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja">型号</strong></p><ul class=""><li id="57ef" class="na nb iq li b lj lk lm ln lp nc lt nd lx ne mb nf ng nh ni bi translated">我们创建一个上采样因子为4的模型，因此输入是大小为162×162的LR图像(因为648/4 = 162)，输出是大小为648×648的HR图像。</li><li id="b192" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nf ng nh ni bi translated">模型层、过滤器大小和数量、激活、填充和初始化器与论文中描述的相同。</li><li id="1253" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nf ng nh ni bi translated">PReLU激活有点棘手，因为它有一个可学习的参数— alpha。当在Tensorflow中使用默认参数初始化PReLU时，每次激活都有数千个这样的alphas。然而，根据论文，这个数字应该很小。我在<a class="ae lf" href="https://keras.io/api/layers/activation_layers/prelu/" rel="noopener ugc nofollow" target="_blank"> Tensorflow PReLU文档</a>中找到一个建议，像这样跨维度共享alpha】，我也这么做了。它大大减少了激活中的参数数量。</li><li id="a572" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nf ng nh ni bi translated">这里是完整的<a class="ae lf" href="https://github.com/OlgaChernytska/Super-Resolution-with-FSRCNN/blob/main/utils/model.py" rel="noopener ugc nofollow" target="_blank"> Tensorflow模型代码</a>。超参数d、s、m是在模型初始化期间指定的。我们使用d=56、s=12和m=4，这应该给出最佳的恢复质量。</li></ul><p id="8443" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja">训练详情</strong></p><ul class=""><li id="4f59" class="na nb iq li b lj lk lm ln lp nc lt nd lx ne mb nf ng nh ni bi translated">初始学习率为0.001。对所有层使用相同的学习速率更简单，并且该模型也训练得相当好。让我们使用<a class="ae lf" href="https://keras.io/api/callbacks/reduce_lr_on_plateau/" rel="noopener ugc nofollow" target="_blank"> ReduceLROnPlateau </a>回调来降低每次列车损失停止下降时的学习率。</li><li id="d916" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nf ng nh ni bi translated">RMSprop被用来代替SGD，因为它可以确保更快更流畅的训练。</li><li id="4e0d" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nf ng nh ni bi translated">我用batch_size=30和steps_per_epoch=20进行训练。使用batch_size=20，steps_per_epoch=4进行验证。您可以更改这些数字，但是请注意，batch_size*steps_per_epoch不能超过数据集大小。</li><li id="3e6d" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nf ng nh ni bi translated">最大历元数为500。当验证损失停止下降时，我们使用<a class="ae lf" href="https://keras.io/api/callbacks/early_stopping/" rel="noopener ugc nofollow" target="_blank">提前停止</a>回调来完成训练。当我训练模型时，它在大约300个时期内完成。</li><li id="cd0e" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nf ng nh ni bi translated">我在单个GPU上花了大约5个小时来训练这个模型。</li><li id="0f10" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nf ng nh ni bi translated">这里是用于训练的<a class="ae lf" href="https://github.com/OlgaChernytska/Super-Resolution-with-FSRCNN/blob/main/train.py" rel="noopener ugc nofollow" target="_blank">代码和带有训练配置</a>的<a class="ae lf" href="https://github.com/OlgaChernytska/Super-Resolution-with-FSRCNN/blob/main/config.yaml" rel="noopener ugc nofollow" target="_blank">文件。</a></li></ul><p id="0590" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><strong class="li ja">评估</strong></p><ul class=""><li id="749d" class="na nb iq li b lj lk lm ln lp nc lt nd lx ne mb nf ng nh ni bi translated">超分辨率模型用度量— <a class="ae lf" href="https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio" rel="noopener ugc nofollow" target="_blank">峰值信噪比</a> (PSNR)进行评估。这里是<a class="ae lf" href="https://www.tensorflow.org/api_docs/python/tf/image/psnr" rel="noopener ugc nofollow" target="_blank">它的Tensorflow实现</a>。我们的模型PSNR在DIV2K测试集上是26.625。</li><li id="f17a" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nf ng nh ni bi translated">更有意思的是看结果是什么样的。通常，模型恢复质量是根据双三次上采样进行视觉评估的。</li><li id="59ff" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nf ng nh ni bi translated">虽然用双三次插值上采样的图像看起来有点模糊，但用FSRCNN上采样的图像具有更好的纹理。</li><li id="f272" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nf ng nh ni bi translated">如果你想看更多的模型预测，这里有一个<a class="ae lf" href="https://github.com/OlgaChernytska/Super-Resolution-with-FSRCNN/blob/main/notebooks/Inference.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>供你参考。</li></ul><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nw"><img src="../Images/fe05736eb3197d38b968eb941a811792.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-YaeAWipT9Ge4KJop8949g.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><em class="ko"> FSRCNN在DIV2K测试集上的性能。左—输入图像，右FSRCNN的输出。</em></p></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nx"><img src="../Images/cfef6e137b81b4f0340e8859427ef081.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HZgQz1BBDHaR4YMwitsBfw.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">由FSRCNN恢复的图像比向上采样的图像具有更多的纹理。</p></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ny"><img src="../Images/9899bd827e84243ec5198de20b9ed959.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mRMy9eJUIrK0hWtydCl-rg.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><em class="ko"> FRCNN恢复质量仍然远非完美，即使它是在2K分辨率图像上训练的</em> s。</p></figure><h1 id="20d4" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">祝你好运！</h1><p id="eee0" class="pw-post-body-paragraph lg lh iq li b lj mu ka ll lm mv kd lo lp mw lr ls lt mx lv lw lx my lz ma mb ij bi translated">再一次:</p><ol class=""><li id="84f4" class="na nb iq li b lj lk lm ln lp nc lt nd lx ne mb nz ng nh ni bi translated">选择一篇旧的高被引论文。</li><li id="502a" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nz ng nh ni bi translated">通读一遍，尽量抓住——数据准备、模型架构、损失函数、训练细节。</li><li id="b4c7" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nz ng nh ni bi translated">什么都不懂也不要难过。</li><li id="68e3" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nz ng nh ni bi translated">通过跳过和简化您不理解的概念来创建一个实现草案。如果你的草稿与论文中的方法相差甚远，也没关系。</li><li id="4111" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nz ng nh ni bi translated">试着自己去完善草稿。</li><li id="8d8c" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nz ng nh ni bi translated">当你遇到困难的时候——在互联网上寻找文章和用纸实现的代码。复制粘贴，但要阅读理解。</li><li id="5e4d" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nz ng nh ni bi translated">把你的工作包装成Github项目(为什么不呢？).看，<a class="ae lf" href="https://github.com/OlgaChernytska/Super-Resolution-with-FSRCNN" rel="noopener ugc nofollow" target="_blank">我是怎么做到的</a>。</li><li id="77fb" class="na nb iq li b lj nj lm nk lp nl lt nm lx nn mb nz ng nh ni bi translated">用一篇新论文重复。感受一下第二次有多顺利🙂</li></ol><p id="7221" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">真正的学习发生在步骤2-5，所以你在这里花的时间越多，你学得越快。祝你好运！</p></div><div class="ab cl oa ob hu oc" role="separator"><span class="od bw bk oe of og"/><span class="od bw bk oe of og"/><span class="od bw bk oe of"/></div><div class="ij ik il im in"><p id="5c1e" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><em class="mz">原载于2021年8月10日</em><a class="ae lf" href="https://notrocketscience.blog/learn-to-reproduce-papers-beginners-guide/" rel="noopener ugc nofollow" target="_blank"><em class="mz">https://notrocketseconomy . blog</em></a><em class="mz">。<br/>如果你想阅读更多类似的教程，请订阅我的博客“不是火箭科学”——</em><a class="ae lf" href="https://t.me/notrocketscienceblog" rel="noopener ugc nofollow" target="_blank"><em class="mz">电报</em> </a> <em class="mz">和</em> <a class="ae lf" href="https://twitter.com/nRocketScience" rel="noopener ugc nofollow" target="_blank"> <em class="mz">推特</em> </a> <em class="mz">。</em></p></div></div>    
</body>
</html>