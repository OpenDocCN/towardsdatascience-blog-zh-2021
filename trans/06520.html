<html>
<head>
<title>Distributing learning for sentiment analysis with Pyspark</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Pyspark的分布式情感分析学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/distributing-learning-for-sentiment-analysis-with-pyspark-1e90065c7137?source=collection_archive---------16-----------------------#2021-06-11">https://towardsdatascience.com/distributing-learning-for-sentiment-analysis-with-pyspark-1e90065c7137?source=collection_archive---------16-----------------------#2021-06-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b00d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">Pyspark中不同方法在学习速度和准确性方面的权衡</h2></div><p id="37ad" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">在本文中，我将展示Pyspark中不同方法在学习速度和识别积极和消极twit任务的准确性方面的权衡。</em></p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/94a0e560f33b87da6026b7361d31abe2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0Qh_Tp6ou3NvRg9g"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">照片由<a class="ae ls" href="https://unsplash.com/@purzlbaum?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">🇨🇭·克劳迪奥·施瓦茨| @purzlbaum </a>在<a class="ae ls" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="0339" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">情绪分析在社交媒体监控中非常有用，因为它允许我们获得特定话题背后更广泛的公众意见的概览。这个任务已经用不同的框架和编程语言进行了大量的研究和实现。</p><p id="2f87" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文中，我将展示我们如何在Pyspark中对数据块使用不同的方法，我们可以获得哪些指标，最重要的是我们需要多少时间。众所周知，模型时间性能可能是现实生活中最重要的事情，因为新数据和案例增长非常快，我们几乎每天都需要重新训练我们的模型。第二件事是模型预测响应时间，如果模型具有99.9%的准确性，这是非常好的，但如果在输入数据流期间模型响应时间太长，我们就会错过机器和深度学习给我们带来的所有好处。</p><p id="8047" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这项研究中，我使用了来自<a class="ae ls" href="https://www.kaggle.com/" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>的<a class="ae ls" href="https://www.kaggle.com/cosmos98/twitter-and-reddit-sentimental-analysis-dataset" rel="noopener ugc nofollow" target="_blank"> Twitter和Reddit情感分析数据集</a>。</p><p id="3d02" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我的第一种做法是—<strong class="kh ir"><em class="lb">“HashingTF+IDF+Logistic回归”</em> </strong> <em class="lb">。</em></p><p id="6f69" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们开始弄清楚它是什么以及它是如何工作的。<a class="ae ls" href="http://en.wikipedia.org/wiki/Tf%E2%80%93idf" rel="noopener ugc nofollow" target="_blank">词频-逆文档频数(TF-IDF) </a>是一种广泛应用于文本挖掘的特征矢量化方法，用以反映语料库中某个词对某个文档的重要性。</p><p id="f7e3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">术语频率和文档频率的定义有几种变体。在MLlib中，将TF和IDF分开，使它们具有灵活性。</p><p id="9fca" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">TF</strong>:<code class="fe lt lu lv lw b">HashingTF</code>和<code class="fe lt lu lv lw b">CountVectorizer</code>都可以用来生成术语频率向量。</p><p id="1de2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe lt lu lv lw b">HashingTF</code>是一个<code class="fe lt lu lv lw b">Transformer</code>,它接受多组术语，并将这些集合转换成固定长度的特征向量。在文本处理中，“术语集”可能是一个单词包。<code class="fe lt lu lv lw b">HashingTF</code>利用<a class="ae ls" href="http://en.wikipedia.org/wiki/Feature_hashing" rel="noopener ugc nofollow" target="_blank">哈希技巧</a>。通过应用散列函数将原始特征映射到索引(术语)中。这里使用的哈希函数是<a class="ae ls" href="https://en.wikipedia.org/wiki/MurmurHash" rel="noopener ugc nofollow" target="_blank"> MurmurHash 3 </a>。然后基于映射的索引计算术语频率。这种方法避免了计算全局术语-索引映射的需要，这对于大型语料库来说可能是昂贵的，但是它遭受潜在的哈希冲突，其中不同的原始特征在哈希之后可能变成相同的术语。为了减少冲突的机会，我们可以增加目标特征维数，即哈希表的桶的数量。由于使用散列值的简单模来确定向量索引，建议使用2的幂作为特征维数，否则，特征将不会均匀地映射到向量索引。默认特征尺寸为<code class="fe lt lu lv lw b">218=262,144218=262,144</code>。可选的二进制切换参数控制术语频率计数。当设置为真时，所有非零频率计数被设置为1。这对于模拟二进制而非整数计数的离散概率模型尤其有用。</p><p id="cf44" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> IDF </strong> : <code class="fe lt lu lv lw b">IDF</code>是一个<code class="fe lt lu lv lw b">Estimator</code>，它适合数据集并产生一个<code class="fe lt lu lv lw b">IDFModel</code>。<code class="fe lt lu lv lw b">IDFModel</code>获取特征向量(通常由<code class="fe lt lu lv lw b">HashingTF</code>创建)并缩放每个特征。直观上，它降低了语料库中频繁出现的特征的权重。</p><p id="5b3d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是响应文本预处理的管道模式的第一部分。我的第一个模型是逻辑回归，它是机器学习中最简单和快速的模型之一，但如果有良好的数据预处理，它可以给出良好的结果。在<a class="ae ls" href="https://en.wikipedia.org/wiki/Statistics" rel="noopener ugc nofollow" target="_blank">统计</a>中，使用<strong class="kh ir">逻辑模型</strong>(或<strong class="kh ir"> logit模型</strong>)对某类或某事件存在的概率进行建模，如正类或负类。逻辑回归使用最大似然技术实现最佳预测。Sigmoid是一个数学函数，其特征是可以取-∞和+∞之间的任何实数值，并将其映射到0到1之间的实数值。因此，如果sigmoid函数的结果大于0.5，那么我们将其归类为正类，如果小于0.5，那么我们可以将其归类为负类。</p><p id="830c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">结果我们得到了:</p><ul class=""><li id="a593" class="lx ly iq kh b ki kj kl km ko lz ks ma kw mb la mc md me mf bi translated">准确度分数:0.8771</li><li id="988a" class="lx ly iq kh b ki mg kl mh ko mi ks mj kw mk la mc md me mf bi translated">ROC-AUC: 0.8529</li><li id="fef4" class="lx ly iq kh b ki mg kl mh ko mi ks mj kw mk la mc md me mf bi translated">训练时间56.79秒</li></ul><p id="aec4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第二种方法是— <strong class="kh ir"> <em class="lb">“计数向量器+ IDF +逻辑回归”</em> </strong> <em class="lb">。</em>这种方法与前一种非常相似，但是对于文本预处理，我想使用CountVectorizer方法。那么，它是什么，它是如何工作的？</p><p id="f3f7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe lt lu lv lw b">CountVectorizer</code>和<code class="fe lt lu lv lw b">CountVectorizerModel</code>旨在帮助将一组文本文档转换成令牌计数的向量。当没有先验词典时，可以使用<code class="fe lt lu lv lw b">CountVectorizer</code>作为<code class="fe lt lu lv lw b">Estimator</code>来提取词汇，并生成一个<code class="fe lt lu lv lw b">CountVectorizerModel</code>。该模型为词汇表中的文档生成稀疏表示，然后可以将其传递给LDA等其他算法。</p><p id="5f24" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在拟合过程中，<code class="fe lt lu lv lw b">CountVectorizer</code>将选择语料库中按词频排序的前<code class="fe lt lu lv lw b">vocabSize</code>个单词。可选参数<code class="fe lt lu lv lw b">minDF</code>也通过指定一个术语必须出现在词汇表中的最小文档数(如果是&lt; 1.0，则为分数)来影响拟合过程。另一个可选的二进制切换参数控制输出向量。如果设置为真，所有非零计数都被设置为1。这对于模拟二进制而非整数计数的离散概率模型尤其有用。</p><p id="993e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">结果我们得到了:</p><ul class=""><li id="35d6" class="lx ly iq kh b ki kj kl km ko lz ks ma kw mb la mc md me mf bi translated">准确度分数:0.8931</li><li id="bc81" class="lx ly iq kh b ki mg kl mh ko mi ks mj kw mk la mc md me mf bi translated">ROC-AUC: 0.8708</li><li id="f8a8" class="lx ly iq kh b ki mg kl mh ko mi ks mj kw mk la mc md me mf bi translated">训练时间1.27分钟</li></ul><p id="9326" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如你所见，准确率更高，但我们需要按训练时间付费。</p><p id="e7ea" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第三种方法是——<strong class="kh ir"><em class="lb"/>N-gram+Chi平方<em class="lb"> + Logistic回归</em> </strong> <em class="lb">。</em></p><p id="6e79" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这个方法中，我使用N-gram作为文本预处理的技术，使用卡方作为特征选择的方法。</p><p id="86c1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们从N-gram开始。n-gram是某个整数n的n个标记(通常是单词)的序列。NGram类可用于将输入要素转换为n-gram。NGram将一系列字符串作为输入(例如，分词器的输出)。参数n用于确定每个n元语法中的项数。输出将由一系列n-gram组成，其中每个n-gram由一个由n个连续单词组成的空格分隔的字符串表示。如果输入序列包含的字符串少于n个，则不会产生输出。Spark不会自动组合来自不同n-gram的特征，所以我不得不在管道中使用VectorAssembler来组合我从每个n-gram获得的特征。</p><p id="5046" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这种方法的下一步是特征选择。要素选择是自动选择数据中对您感兴趣的预测变量或输出贡献最大的那些要素的过程。在对数据建模之前执行要素选择的好处是:</p><ul class=""><li id="8a9e" class="lx ly iq kh b ki kj kl km ko lz ks ma kw mb la mc md me mf bi translated">避免过度拟合:较少的冗余数据会提高模型的性能，从而减少根据噪声做出决策的机会</li><li id="d82d" class="lx ly iq kh b ki mg kl mh ko mi ks mj kw mk la mc md me mf bi translated">减少训练时间:更少的数据意味着算法训练更快</li></ul><p id="30b9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">卡方</em>检验用于数据集中的分类特征。我们计算每个特征和目标之间的卡方，并选择具有最佳卡方分数的期望数量的特征。它确定样本的两个分类变量之间的关联是否反映了它们在总体中的真实关联。<br/>机器学习模型不变——逻辑回归。</p><p id="1a8b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">结果我们得到了:</p><ul class=""><li id="4676" class="lx ly iq kh b ki kj kl km ko lz ks ma kw mb la mc md me mf bi translated">准确度分数:0.9111</li><li id="0a75" class="lx ly iq kh b ki mg kl mh ko mi ks mj kw mk la mc md me mf bi translated">ROC-AUC: 0.8921</li><li id="ca8c" class="lx ly iq kh b ki mg kl mh ko mi ks mj kw mk la mc md me mf bi translated">训练时间15.25分钟</li></ul><p id="86fb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们做同样的，但没有特征选择。于是，我的第四种方法——<strong class="kh ir"><em class="lb"/>N-gram<em class="lb">+Logistic回归</em> </strong> <em class="lb">。这里我想利用</em> <strong class="kh ir"> N-gram </strong>产生的序列分析能力和快速的逻辑回归。</p><p id="31ab" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">结果我们得到了:</p><ul class=""><li id="d70f" class="lx ly iq kh b ki kj kl km ko lz ks ma kw mb la mc md me mf bi translated">准确度分数:0.9022</li><li id="8cc8" class="lx ly iq kh b ki mg kl mh ko mi ks mj kw mk la mc md me mf bi translated">ROC-AUC: 0.8816</li><li id="b89b" class="lx ly iq kh b ki mg kl mh ko mi ks mj kw mk la mc md me mf bi translated">训练时间1.54分钟</li></ul><p id="1587" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如我们所看到的，在训练时间不可思议地减少的同时，精确度降低了一点点。</p><p id="67d3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">好吧，逻辑回归是好的，但是如何使用更复杂的东西呢——梯度增强树。让我们保留相同的数据预处理—N-gram +卡方，并增加梯度增强分类器的功能。所以，我的下一个迭代将是—<strong class="kh ir"><em class="lb"/>N元文法+卡方<em class="lb"> + </em>梯度提升树分类器<em class="lb"/></strong><em class="lb">。</em></p><p id="ebaa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">梯度推进树(GBTs)是一种使用决策树集成的流行分类和回归方法。GBTs迭代地训练决策树以最小化损失函数。在梯度提升中，弱学习器的集合用于提高机器学习模型的性能。弱学习者通常是决策树。结合起来，它们的输出会产生更好的模型。在梯度推进中，弱学习者顺序工作。每个模型都试图改进前一个模型的误差。</p><p id="077e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">结果我们得到了:</p><ul class=""><li id="f4f1" class="lx ly iq kh b ki kj kl km ko lz ks ma kw mb la mc md me mf bi translated">准确度分数:0.8003</li><li id="3f03" class="lx ly iq kh b ki mg kl mh ko mi ks mj kw mk la mc md me mf bi translated">ROC-AUC: 0.7227</li><li id="0f72" class="lx ly iq kh b ki mg kl mh ko mi ks mj kw mk la mc md me mf bi translated">训练时间52.408分钟</li></ul><p id="c9bf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对我来说，它太长了，而且我们有很多风险让GBT生产的模型过度拟合。GBT是一个非常好的模型，但在这种情况下，我们有太多的特征，对我来说卡方检验法可能不太好。</p><p id="a1c7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们检查相同的管道，但是没有卡方检验，只是为了检查如果我们排除特征选择，我们节省了多少时间。于是，就有了下一种方法——<strong class="kh ir"><em class="lb"/>N-gram+梯度提升树分类器<em class="lb"/></strong><em class="lb">。</em></p><p id="45f8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">结果我们得到了:</p><ul class=""><li id="9f54" class="lx ly iq kh b ki kj kl km ko lz ks ma kw mb la mc md me mf bi translated">准确度分数:0.7976</li><li id="e435" class="lx ly iq kh b ki mg kl mh ko mi ks mj kw mk la mc md me mf bi translated">ROC-AUC: 0.7185</li><li id="b5ad" class="lx ly iq kh b ki mg kl mh ko mi ks mj kw mk la mc md me mf bi translated">训练时间39.564分钟</li></ul><p id="8bd2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们真的可以减少时间，但正如我之前所说，我们的算法确实需要更少的功能，所以我们可以看到我们在精度上损失了一点。</p><p id="d772" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">假设我们不知道或不想在我们的管道中使用任何其他特征选择技术，但仍然希望提高准确性。在这种情况下，我们可以使用深度学习的力量。</p><p id="da7a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在接下来的步骤中，我还想保留相同的数据预处理管道，但使用深度学习(MLP)模型— <strong class="kh ir"> N-gram +卡方<em class="lb">+</em>MLP<em class="lb"/></strong><em class="lb">。</em></p><p id="c5c2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">多层感知器分类器(MLPC)是基于<a class="ae ls" href="https://en.wikipedia.org/wiki/Feedforward_neural_network" rel="noopener ugc nofollow" target="_blank">前馈人工神经网络</a>的分类器。MLPC由多层节点组成。网络中的每一层都完全连接到下一层。输入层中的节点表示输入数据。所有其他节点通过输入与节点权重<code class="fe lt lu lv lw b">w</code>和偏差<code class="fe lt lu lv lw b">b</code>的线性组合以及应用激活函数，将输入映射到输出。这可以用矩阵形式写为MLPC与<code class="fe lt lu lv lw b">K+1</code>层如下:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/857a52f29e037b2e47b0aaeb661586f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*WbqIRHJWRkfrxqDYbphJ-A.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated"><a class="ae ls" href="https://spark.apache.org/docs/latest/ml-classification-regression.html#multilayer-perceptron-classifier" rel="noopener ugc nofollow" target="_blank">多层感知器分类器</a></p></figure><p id="54d4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">中间层中的节点使用sigmoid(逻辑)函数:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/84ba3aea08ba9cac62bb37a1dca17f8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:328/format:webp/1*Fj9MhPfjHltBK3jbdMlmhg.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated"><a class="ae ls" href="https://spark.apache.org/docs/latest/ml-classification-regression.html#multilayer-perceptron-classifier" rel="noopener ugc nofollow" target="_blank">乙状结肠(逻辑)功能</a></p></figure><p id="e079" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">输出层中的节点使用softmax函数:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/d52efb46d0a0c3b376aa59975a8565f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:308/format:webp/1*NvA04Y-qc95oWfAagMVrGQ.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated"><a class="ae ls" href="https://spark.apache.org/docs/latest/ml-classification-regression.html#multilayer-perceptron-classifier" rel="noopener ugc nofollow" target="_blank"> Softmax功能</a></p></figure><p id="0aa2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">输出层中的节点数量<code class="fe lt lu lv lw b">N</code>对应于类的数量。MLPC采用反向传播来学习模型。我们使用逻辑损失函数优化和L-BFGS作为优化程序。</p><p id="1a8c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我的例子中，我将对64和32个神经元使用2个隐藏层。</p><p id="f6ae" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">结果我们得到了:</p><ul class=""><li id="c3a5" class="lx ly iq kh b ki kj kl km ko lz ks ma kw mb la mc md me mf bi translated">准确度分数:0.9261</li><li id="977b" class="lx ly iq kh b ki mg kl mh ko mi ks mj kw mk la mc md me mf bi translated">ROC-AUC: 0.9143</li><li id="cb76" class="lx ly iq kh b ki mg kl mh ko mi ks mj kw mk la mc md me mf bi translated">训练时间70.2分钟</li></ul><p id="6e5c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">与以前的方法相比，这是最好的结果，但是训练时间太长，而且只针对少量数据。</p><p id="913e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们检查相同的管道，但是没有卡方检验，只是为了检查如果我们排除特征选择，我们节省了多少时间。于是，就有了下一种方法——<strong class="kh ir"><em class="lb"/>N-gram+MLP<em class="lb"/></strong><em class="lb">。</em></p><p id="841e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">结果我们得到了:</p><ul class=""><li id="5bea" class="lx ly iq kh b ki kj kl km ko lz ks ma kw mb la mc md me mf bi translated">准确度分数:0.9115</li><li id="9389" class="lx ly iq kh b ki mg kl mh ko mi ks mj kw mk la mc md me mf bi translated">ROC-AUC: 0.8975</li><li id="df37" class="lx ly iq kh b ki mg kl mh ko mi ks mj kw mk la mc md me mf bi translated">训练时间54.108分钟</li></ul><p id="c823" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们确实节省了时间，但如果我们将结果与之前的所有测试进行比较，我们会发现使用逻辑回归的方法会更好，这种方法可以产生几乎相同的准确性，但时间更短。</p><p id="e6c8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以下是所有结果的表格和图表:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi mo"><img src="../Images/7a3e710781ed01342df6edb0de8272ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*p9onyDIGqxRI3XTB"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">作者的实验结果</p></figure><h1 id="6c61" class="mp mq iq bd mr ms mt mu mv mw mx my mz jw na jx nb jz nc ka nd kc ne kd nf ng bi translated">结论</h1><p id="c818" class="pw-post-body-paragraph kf kg iq kh b ki nh jr kk kl ni ju kn ko nj kq kr ks nk ku kv kw nl ky kz la ij bi translated">结论很简单——建模越快，就越希望使用简单的模型和数据预处理管道。在本文中，我只使用了数据科学的全部方法和模型中的一小部分。</p><p id="b939" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于这个任务，你也可以使用架构比MLP更复杂的TensorFlow或者PyTorch框架。要使用Databricks的强大功能，您还可以使用分布式培训方法。<a class="ae ls" href="https://github.com/horovod/horovod" rel="noopener ugc nofollow" target="_blank"> Horovod </a>是一个用于TensorFlow、Keras和PyTorch的分布式培训框架。Databricks支持使用HorovodRunner和<code class="fe lt lu lv lw b">horovod.spark</code>包的分布式深度学习训练。对于使用Keras或PyTorch的Spark ML流水线应用，可以使用<code class="fe lt lu lv lw b">horovod.spark</code> <a class="ae ls" href="https://spark.apache.org/docs/latest/ml-pipeline.html#estimators" rel="noopener ugc nofollow" target="_blank">估算器API </a>。</p><p id="1db0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，您可以使用微调语言模型进行情感分析任务，例如使用BERT和所有其他衍生工具。</p><p id="d90c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你能在Git仓库中找到的所有代码— <a class="ae ls" href="https://github.com/AndriiShchur/Distributing-learning-for-sentiment-analysis-with-Pyspark" rel="noopener ugc nofollow" target="_blank">链接</a>。</p><p id="2c05" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你也可以在我的文章中阅读如何使用Keras进行情感分析——“深度学习方法的<a class="ae ls" href="https://medium.com/@andriishchur/fake-news-detector-with-deep-learning-approach-part-ii-modeling-42b9f901b12b" rel="noopener">假新闻检测器(第二部分)建模</a>”。</p></div></div>    
</body>
</html>