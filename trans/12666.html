<html>
<head>
<title>Exploring BERT variants (Part 2): SpanBERT, DistilBERT, TinyBERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">探索 BERT 变体(第 2 部分):SpanBERT、DistilBERT、TinyBERT</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/exploring-bert-variants-part-2-spanbert-distilbert-tinybert-8e9bbef4eef1?source=collection_archive---------22-----------------------#2021-12-29">https://towardsdatascience.com/exploring-bert-variants-part-2-spanbert-distilbert-tinybert-8e9bbef4eef1?source=collection_archive---------22-----------------------#2021-12-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="75cb" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">对伯特家庭的一次温和的深入探究</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f99fe3bf21f63ad79d04c2f5d791159d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*q6N1PlnaEwyiHmTC"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">纳吉布·卡利尔在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="cdb1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，这是本系列的第 2 部分。本文将深入探讨伯特模型的三个变体的细节，即<strong class="lb iu">斯潘伯特、迪夫伯特、蒂尼伯特。</strong>第 1 部分涵盖了其他三个变体— <strong class="lb iu"> ALBERT、RoBERTa 和 ELECTRA。如果你还没有读过第一篇文章，我强烈建议你也读一读。本文利用了第一篇文章中的一些基础知识，并扩展了这些概念。</strong></p><p id="3a43" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是快速参考的链接:</p><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/exploring-bert-variants-albert-roberta-electra-642dfe51bc23"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">探索 BERT 变体(第 1 部分):ALBERT，RoBERTa，ELECTRA</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">对伯特家族的一次温和的深入探究…</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm ks ly"/></div></div></a></div><p id="586a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们从三种变体中的一种开始考虑:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/c775a0b447452cf1cd3ca9ad62e3df63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/format:webp/1*H2E04i2eC-ajAAnzcgiz4Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="dc38" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，让我们开始吧:</p><p id="92f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">1.SpanBERT  : SpanBERT 是 BERT 的一个特殊变体，其目标是使用模型进行问题回答、关系提取等。为了避免任何混淆，应该注意的是，该模型不是“创建答案”,而是“找到”段落(文本块)中要回答的单词组(跨度)。直观上，这意味着 BERT 模型需要进行以下修改:</p><p id="90f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> a. </strong> <strong class="lb iu">【字长】掩蔽比【随机掩蔽】:</strong></p><p id="3eb4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们考虑一个句子的例子。</p><p id="38c4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> Tokens = </strong>【数据，科学<strong class="lb iu">，</strong>结合了，领域，专长，编程，技能，和，知识，数学，和，统计】</p><p id="c34f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">在 MLM 的情况下，执行随机屏蔽:</strong></p><p id="a8c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">令牌= </strong>【数据，科学，[面具]，领域，[面具]，编程，技能，[面具]，知识，of，数学，[面具]，统计】</p><p id="c6f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，对于跨度，掩蔽是随机进行的，但是是在连续单词的跨度上进行的:</p><p id="1714" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> Tokens = </strong>【数据、科学、组合、【面具】、【面具】、【面具】、【面具】、【面具】、知识、of、数学、统计】</p><p id="9ffd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> b .更新损失函数，以便给予“单词跨度”中的记号必要的考虑</strong></p><p id="10e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除了屏蔽逻辑的改变之外，成本函数也需要被更新以考虑以下两种情况:</p><p id="013b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">1.<strong class="lb iu"> MLM 目标</strong>:预测屏蔽词，即词汇表中所有单词成为屏蔽词的概率。</p><p id="d9f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 2。</strong> <strong class="lb iu">跨度边界目标(SBO): </strong>对于 SBO，<strong class="lb iu"> </strong>不采用屏蔽记号的表示，而是考虑相邻记号的<strong class="lb iu">表示</strong>。</p><p id="401a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们的例子中:</p><p id="b10c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> Tokens = </strong>【数据，科学，<strong class="lb iu">结合了</strong>，【面具】，【面具】，【面具】，【面具】，<strong class="lb iu">知识，</strong>的，数学，和，统计</p><p id="0faf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">单词<strong class="lb iu">“组合”和“知识”</strong>的表示将用于导出屏蔽令牌。这进一步意味着位置嵌入将用于理解记号的相对位置。</p><p id="e608" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">斯潘伯特总体损失函数= MLM 目标损失+ SBO 目标损失</strong></p><p id="fb0b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该模型被训练以最小化该损失函数。这个预先训练的模型可以用于任何问答任务或关系提取。</p><p id="e987" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> 2 和 3。在你在笛卡尔曲线中扬起眉毛之前，我有一个理由来折叠这两个变体。与调整 BERT 模型的某些方面以创建新版本的其他变体不同，DistilBERT 和 TinyBERT 都以不同的方式工作。</strong></p><p id="1829" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些变体不调整模型，而是试图通过创建一个较小的模型来减小模型的大小，该模型以某种方式复制了较大模型的输出。</p><p id="294c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这一过程被称为知识提取，即这些较小的模型提取大模型的知识，并试图以较小的规模获得相似的模型结果。较大和较小模型的通用术语是“学生”和“教师”。如果你想了解更多关于师生框架如何运作以及知识是如何提炼的，请浏览下面的文章。本文还介绍了 DistilBERT 的细节，所以我不打算在这里详细介绍。</p><p id="0868" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/why-you-need-to-stop-using-bulky-neural-network-models-d43681d9916f">https://towards data science . com/why-you-neural-network-models-d 43681d 9916 f</a></p><p id="3c73" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总的来说，在 DistilBERT 中，预先训练的网络(教师)被用来训练学生网络，以通过蒸馏从教师网络获得知识。</p><p id="6301" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您对 DistilBERT 的工作方式有所了解，TinyBERT 是它的进一步扩展，除了从输出层提取知识之外，TinyBERT 还从中间层提取信息。</p><p id="db15" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">即</p><p id="5f20" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> TinyBERT =蒸馏+从中间层提取的知识</strong></p><p id="0af8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是解释差异的图示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mo"><img src="../Images/a415aa188d21ad76cdfc8f35c5e72133.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EXGOw3vyiRHwlD0p8czT2g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自作者</p></figure><p id="ca07" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如所料，TinyBERT 的损失函数不仅包括来自输出层的损失，还包括来自隐藏注意力和嵌入层的损失。</p><p id="0674" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">差不多就是这样。希望你喜欢这篇文章，现在所有的 BERT 变体都更加直观了。</p><p id="2003" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您有任何问题或意见，请随时留言。</p></div><div class="ab cl mp mq hx mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="im in io ip iq"><p id="708e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">免责声明:本文中表达的观点是作者以个人身份发表的意见，而非其雇主的意见。</p></div></div>    
</body>
</html>