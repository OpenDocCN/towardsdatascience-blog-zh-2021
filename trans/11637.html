<html>
<head>
<title>Open Source Alternative to the AWS Deep Learning AMI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">AWS 深度学习 AMI 的开源替代方案</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/open-source-alternative-to-the-aws-deep-learning-ami-f8f77318f8a8?source=collection_archive---------22-----------------------#2021-11-17">https://towardsdatascience.com/open-source-alternative-to-the-aws-deep-learning-ami-f8f77318f8a8?source=collection_archive---------22-----------------------#2021-11-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="3f4b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">一篇描述如何使用 conda、Docker、make 和 terraform 自动设置 GPU 基础架构的文章。</h2></div><p id="1885" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">由</em> <a class="ae lc" href="https://github.com/Nixtla/" rel="noopener ugc nofollow" target="_blank"> <em class="lb">尼克斯特拉团队</em> </a> <em class="lb">。</em> <a class="ld le ep" href="https://medium.com/u/2855bd3e0293?source=post_page-----f8f77318f8a8--------------------------------" rel="noopener" target="_blank"> <em class="lb">费德·加尔萨·拉米雷斯</em> </a>，<a class="ld le ep" href="https://medium.com/u/76b639655285?source=post_page-----f8f77318f8a8--------------------------------" rel="noopener" target="_blank">马克斯·梅根塔尔</a></p><blockquote class="lf lg lh"><p id="04af" class="kf kg lb kh b ki kj jr kk kl km ju kn li kp kq kr lj kt ku kv lk kx ky kz la ij bi translated"><strong class="kh ir">TLDR；</strong>使用 GPU 运行深度学习模型非常复杂，尤其是在配置基础设施时。预制的 GPU 云基础设施往往特别昂贵。</p><p id="bf23" class="kf kg lb kh b ki kj jr kk kl km ju kn li kp kq kr lj kt ku kv lk kx ky kz la ij bi translated">为了帮助人们专注于他们的模型而不是他们的硬件和配置，我们在 Nixtla 开发了一种快速简单的方法来在 AWS 云上使用 gpu，而无需为 AMI 环境付费，并将其开源:<a class="ae lc" href="https://github.com/Nixtla/nixtla/tree/main/utils/docker-gpu" rel="noopener ugc nofollow" target="_blank">https://github.com/Nixtla/nixtla/tree/main/utils/docker-gpu</a>和<a class="ae lc" href="https://github.com/Nixtla/nixtla/tree/main/utils/terraform-gpu" rel="noopener ugc nofollow" target="_blank">https://github . com/Nixtla/Nixtla/tree/main/utils/terraform-GPU</a>。</p></blockquote><p id="506d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">简介</strong></p><p id="a180" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">深度学习已经广泛应用于许多领域:计算机视觉、自然语言处理、时间序列预测等。由于它所获得的最先进的结果，它在数据科学家和研究人员的日常实践中变得越来越受欢迎。</p><p id="2e0c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">GPU 加速了模型的训练和推理，因为它们经过优化，可以执行深度学习严重依赖的线性代数计算。然而，对这种专用硬件的需求增加了试验和将这些模型部署到生产中的货币/经济成本。</p><p id="18a7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">深度学习实践者面临的一个普遍问题是云上 GPU 基础设施的正确配置。安装硬件管理所需的驱动程序往往很麻烦。如果不能正确解决这个问题，可能会对再现性造成损害，或者不必要地增加这些新模型的成本。在本帖中，我们使用 Docker 为社区提供了一个简单的解决方案。</p><p id="20a8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">解决方案</strong></p><ol class=""><li id="b119" class="ll lm iq kh b ki kj kl km ko ln ks lo kw lp la lq lr ls lt bi translated"><strong class="kh ir">英伟达深度学习 AMI +康达环境+ Terraform </strong></li></ol><p id="c41a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> a)英伟达深度学习 AMI </strong></p><p id="a837" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要使用 GPU 加速计算运行您的代码，您需要做两件事:(1)拥有 NVIDIA GPUs，以及(2)它们必要的驱动程序。</p><p id="3cc8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你选择 EC2 实例<a class="ae lc" rel="noopener" target="_blank" href="/choosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86"> (P2、P3、P4D 或 G4) </a>，NVIDIA 提供免费的<a class="ae lc" href="https://aws.amazon.com/marketplace/pp/prodview-e7zxdqduz4cbs#pdp-reviews" rel="noopener ugc nofollow" target="_blank"> AMI </a>，预装优化的 GPU 软件，你只需要支付 EC2 计算费用。</p><p id="fb03" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您可以通过 AWS 控制台从终端轻松启动 GPU EC2 实例及其相应的驱动程序。为此，您需要:</p><ol class=""><li id="ef55" class="ll lm iq kh b ki kj kl km ko ln ks lo kw lp la lq lr ls lt bi translated"><a class="ae lc" href="https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html" rel="noopener ugc nofollow" target="_blank"> AWS CLI 安装</a>。</li><li id="94d6" class="ll lm iq kh b ki lu kl lv ko lw ks lx kw ly la lq lr ls lt bi translated">EC2 启动权限。</li><li id="c371" class="ll lm iq kh b ki lu kl lv ko lw ks lx kw ly la lq lr ls lt bi translated">EC2 连接权限:(I)<em class="lb">。pem </em>文件从实例启动&lt; YOUR_KEY_NAME &gt;(您可以按照这里<a class="ae lc" href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html" rel="noopener ugc nofollow" target="_blank">的说明创建一个</a>)。(II)实例的安全组&lt; YOUR_SECURITY_GROUP &gt;。</li></ol><p id="565e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您没有自己的<your_security_group>，您可以使用创建一个:</your_security_group></p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="1869" class="mi mj iq me b gy mk ml l mm mn">aws ec2 create-security-group \<br/>        --group-name nvidia-ami \<br/>        --description “security group for nvidia ami”</span></pre><p id="6f70" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">并使用以下内容向其添加入口规则:</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="e60b" class="mi mj iq me b gy mk ml l mm mn">aws ec2 authorize-security-group-ingress \<br/>        --group-name nvidia-ami \<br/>        --protocol tcp \<br/>        --port 22 \<br/>        --cidr 0.0.0.0/0</span></pre><p id="9660" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有了以上内容，启动 GPU ready EC2 实例就像运行以下命令一样简单:</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="7d87" class="mi mj iq me b gy mk ml l mm mn">aws ec2 run-instances \<br/>        --image-id ami-05e329519be512f1b \<br/>        --count 1 \<br/>        --instance-type g4dn.2xlarge \<br/>        --key-name &lt;YOUR_KEY_NAME&gt; \<br/>        --security-groups nvidia-ami</span></pre><p id="06dd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">图像 id ( <em class="lb"> —图像 id </em>)标识所需的 NVIDIA AMI。实例数量(<em class="lb"> —计数</em>)和实例类型(<em class="lb"> —实例类型</em>)的值是可选的。</p><p id="3914" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一旦实例被初始化，我们就可以用 ssh 来访问它。AMI 预装了 git，因此我们可以毫不费力地克隆我们项目的 repo。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="c465" class="mi mj iq me b gy mk ml l mm mn">ssh -i path/to/&lt;YOUR_KEY_NAME&gt;.pem ubuntu@&lt;PUBLIC_EC2_IP&gt;</span></pre><p id="c8c1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> b)康达环境</strong></p><p id="9c1d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们推荐使用<a class="ae lc" href="https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html" rel="noopener ugc nofollow" target="_blank"> <em class="lb"> Conda </em> </a>来方便处理深度学习依赖(<em class="lb"> PyTorch、TensorFlow 等)。</em>)，特别推荐用<em class="lb"> environment.yml 文件</em>创建环境。</p><p id="628b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下图显示了一个示例。本例中使用的深度学习框架是<em class="lb"> PyTorch </em>，也包含了<em class="lb"> NumPy </em>和<em class="lb"> pandas </em>等标准库。这个文件是一个框架，所以可以毫无困难地添加任何附加的依赖项。此外，还包括<em class="lb"> jupyterlab </em>。</p><figure class="lz ma mb mc gt mp gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/19e3af50a25f2aa0356bba96e837cc77.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*0Vdsg7CI8hS3H3wAEBywZw.png"/></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">原始文件可以在这里找到<a class="ae lc" href="https://github.com/Nixtla/nixtla/blob/main/utils/docker-gpu/environment.yml" rel="noopener ugc nofollow" target="_blank">。</a></p></figure><p id="9f0a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">可以看到，要用的 python 版本是 3.7。这个版本可以很容易地根据用户的需要进行调整，其他版本的软件包也是如此。</p><p id="b16b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要使用 conda 环境，您需要先安装 conda，因为 NVIDIA AMI 没有安装它。您可以遵循下一组说明:</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="7153" class="mi mj iq me b gy mk ml l mm mn">wget <a class="ae lc" href="https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh" rel="noopener ugc nofollow" target="_blank">https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh</a> &amp;&amp; \<br/>bash Miniconda3-latest-Linux-x86_64.sh -b -p $HOME/miniconda &amp;&amp; \<br/>rm -rf Miniconda3-latest-Linux-x86_64.sh &amp;&amp; \<br/>source $HOME/miniconda/bin/activate &amp;&amp; \<br/>conda init</span></pre><p id="3b01" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此您可以在您的环境中安装，</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="7a8b" class="mi mj iq me b gy mk ml l mm mn">conda env create -n &lt;NAME_OF_YOUR_ENVIROMENT&gt; -f environment.yml</span></pre><p id="a365" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了验证一切都正确安装了，您可以克隆我们的 repo 并运行一个测试，</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="dbf9" class="mi mj iq me b gy mk ml l mm mn">git clone <a class="ae lc" href="https://github.com/Nixtla/nixtla.git" rel="noopener ugc nofollow" target="_blank">https://github.com/Nixtla/nixtla.git</a><br/>cd nixtla/utils/docker-gpu<br/>conda env create -n gpu-env -f environment.yml<br/>conda activate gpu-env<br/>python -m test</span></pre><p id="85b3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后一条建议:用户必须小心所使用的深度学习框架的版本，验证它是否与 NVIDIA AMI 驱动程序兼容。</p><p id="fdfc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> c)地形</strong></p><p id="0b0a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了促进上述整个过程的创建，我们开发了一个<a class="ae lc" href="https://www.terraform.io/" rel="noopener ugc nofollow" target="_blank">地形</a>脚本。Terraform 是一个开源的基础设施，作为代码工具，允许你将所有的手动开发合成一个自动脚本。在这种情况下，我们编写的基础设施代码挂载 NVIDIA AMI(包括创建一个兼容的安全组)并安装 conda。下图显示了 main.tf 文件。</p><figure class="lz ma mb mc gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi mw"><img src="../Images/42aad9c623c325b6773f76048ec71d6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QfIS4XlNxdiJrJi52fYFNw.png"/></div></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">原始文件可以在这里找到<a class="ae lc" href="https://github.com/Nixtla/nixtla/blob/main/utils/terraform-gpu/main.tf" rel="noopener ugc nofollow" target="_blank">。</a></p></figure><p id="fe38" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，凭据需要 terraform.tfvars 文件。该文件的图像如下所示。</p><figure class="lz ma mb mc gt mp gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/37e15758fd2b3d65b5a2620e679c7393.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*sy4FOwwbWFUUTTGy4ysz3A.png"/></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">原始文件可以在这里找到<a class="ae lc" href="https://github.com/Nixtla/nixtla/blob/main/utils/terraform-gpu/terraform.tfvars" rel="noopener ugc nofollow" target="_blank">。</a></p></figure><p id="d1ca" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要使用 Terraform，您只需按照这些说明<a class="ae lc" href="https://www.terraform.io/downloads.html" rel="noopener ugc nofollow" target="_blank">进行安装。随后，您必须运行</a></p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="b7e4" class="mi mj iq me b gy mk ml l mm mn">terraform init<br/>terraform apply</span></pre><p id="6a8d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这将创建所需的基础设施，并在部署的 EC2 上安装 conda。当 Terraform 完成运行时，您将能够看到与实例相关联的公共 IP，因此您只需要使用 ssh 连接来访问它。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="117b" class="mi mj iq me b gy mk ml l mm mn">ssh -i path/to/&lt;YOUR_KEY_NAME&gt;.pem ubuntu@&lt;PUBLIC_EC2_IP&gt;</span></pre><p id="05dd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 2) NVIDIA 深度学习 AMI + Conda 环境+ Terraform + Docker + Make </strong></p><p id="d185" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> a)码头工人</strong></p><p id="77c5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用 Docker 来确保项目和实验的可复制性是常见的做法。此外，它允许用户将所有必需的依赖项集中在一个地方，避免在本地安装依赖项，这可能会在以后导致冲突。</p><p id="ac89" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们使用 docker 是因为它允许我们将软件从硬件中分离出来，使计算更加灵活。如果负载非常重，只需更改 EC2 实例并运行容器内的代码就足够了。另一方面，如果负载较轻，我们可以选择较小的实例。</p><p id="dbe9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下图显示了我们为图像构建的 Dockerfile，用于访问实例的 GPU。首先，必须选择一个与 EC2 上安装的驱动程序兼容的映像。迄今为止，NVIDIA AMI 使用 CUDA 版本 11.2，因此这是所选的图像。</p><figure class="lz ma mb mc gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi nc"><img src="../Images/ec277cd83c5b298894399d6292b63193.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UziFRyAJyLciGINX2iErZA.png"/></div></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">原始文件可以在这里找到<a class="ae lc" href="https://github.com/Nixtla/nixtla/blob/main/utils/docker-gpu/Dockerfile" rel="noopener ugc nofollow" target="_blank">。</a></p></figure><p id="1828" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">随后，安装项目可能需要的其他操作系统库。例如，在上面的 docker 文件中，安装了<em class="lb"> wget </em>和<em class="lb"> curl </em>，这对于下载项目所需的数据可能很有用。</p><p id="7428" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在下一条指令中，安装了<em class="lb"> miniconda </em>。正如我们之前所讨论的，Conda 将允许我们处理 python 依赖项，并使用前一节中显示的<em class="lb"> environment.yml </em>文件来安装它们。</p><p id="b46c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们强烈推荐使用<a class="ae lc" href="https://github.com/mamba-org/mamba" rel="noopener ugc nofollow" target="_blank"> <em class="lb"> mamba </em> </a>进行版本管理和安装，因为它可以显著减少等待时间。如果用户愿意，她可以很容易地切换到康达。</p><p id="d157" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，前面创建的<em class="lb"> environment.yml </em>文件被添加到 Docker 映像并安装在基础环境中。没有必要在每次需要容器时都初始化特定的环境。</p><p id="c7e0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> b) Makefile </strong></p><p id="be0d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们为 Makefile 的使用提供了便利。<a class="ae lc" href="https://www.gnu.org/software/make/" rel="noopener ugc nofollow" target="_blank"> Make </a>是控制工作流和可执行文件的强大工具。我们的工作流将允许我们从 Docker 文件快速构建 Docker 映像，并运行 python 和 bash 模块，而无需连续声明必要的参数。</p><figure class="lz ma mb mc gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi nd"><img src="../Images/16e4d62954dab0b2812ec006a288c4e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ADjK4GuZRMU7nMn3F0WSzQ.png"/></div></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">原始文件可以在这里找到<a class="ae lc" href="https://github.com/Nixtla/nixtla/blob/main/utils/docker-gpu/Makefile" rel="noopener ugc nofollow" target="_blank">。</a></p></figure><p id="bffb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本例中，Docker 映像将被称为<em class="lb"> gpucontainer </em>，您可以运行<em class="lb"> make init </em>来构建它。一旦执行了这条指令，用户就可以使用<em class="lb"> run_module </em>指令来使用 GPU 运行她的 python 或 bash 模块。</p><figure class="lz ma mb mc gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi ne"><img src="../Images/9a8acf57943c3bbdcc38bf11d0cd33b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*G3oGUNYdo1uBdCCO"/></div></div></figure><p id="2465" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">例如，为了验证一切工作正常，我们创建了<em class="lb"> test.py </em>文件，确保 CUDA 可用于<em class="lb"> PyTorch </em>并且 GPU 可用。该模块将按如下方式执行:</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="f5f1" class="mi mj iq me b gy mk ml l mm mn">make run_module module="python -m test"</span></pre><figure class="lz ma mb mc gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi nf"><img src="../Images/735d70d23059370bf5e051c80e44ffac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WzoIf0CehX-Hx_gpBqv5PQ.png"/></div></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">原始文件可以在这里找到<a class="ae lc" href="https://github.com/Nixtla/nixtla/blob/main/utils/docker-gpu/test.py" rel="noopener ugc nofollow" target="_blank">。</a></p></figure><figure class="lz ma mb mc gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi ng"><img src="../Images/828243dff80f9f07a3a46e5e0dad971f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mBIK_vEeTn6ahDsM"/></div></div></figure><p id="167f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其他有价值的指令可能是在 Docker 容器中运行<em class="lb"> nvidia-smi </em>来验证一切正常:</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="8fce" class="mi mj iq me b gy mk ml l mm mn">make run_module module="nividia-smi"</span></pre><p id="609c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">或者交互初始化容器，可以用<em class="lb"> make bash_docker </em>来完成。最后，提供了一个在 docker 中运行 jupyterlab 并进行交互式实验的指令:</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="47e8" class="mi mj iq me b gy mk ml l mm mn">make jupyter</span></pre><p id="dfff" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果端口 8888(默认)被另一个进程使用，可以使用</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="7f69" class="mi mj iq me b gy mk ml l mm mn">make jupyter -e PORT=8886</span></pre><figure class="lz ma mb mc gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi nh"><img src="../Images/61ce00018230b644094c57d77b128b41.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sNv5SJUlwHUNnDwj"/></div></div></figure><p id="b987" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">总结</strong></p><p id="c75b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我们展示了一个简单的解决方案，来解决在云上配置 GPU 进行深度学习的问题。有了这个完全开源的工作流程，我们希望该领域的从业者将花费更多的时间来实现模型，而不是像我们一样花太多时间在基础设施上。</p></div></div>    
</body>
</html>