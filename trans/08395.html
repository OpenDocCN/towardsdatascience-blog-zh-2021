<html>
<head>
<title>On Continuous Delivery in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的连续交付</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/on-continuous-delivery-in-machine-learning-71a1afebdf54?source=collection_archive---------20-----------------------#2021-08-02">https://towardsdatascience.com/on-continuous-delivery-in-machine-learning-71a1afebdf54?source=collection_archive---------20-----------------------#2021-08-02</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="6bad" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">对用于高级分析系统的机器学习模型的持续交付过程的设计和实现方法的概念性理解</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/7cc688a4c4db24f1994655e2688d1860.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s4qMNoXnCGUNs1CMUa64zw.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">图1:高级分析系统概述。作者图。</p></figure><p id="4ac0" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">现代高级分析系统广泛使用机器学习(ML)模型。这些系统日复一日地为复杂的业务决策提供支持。随着业务现实的不断变化，这些模型需要失效和重新培训，如果不是以同样的速度，而是定期进行。这个讨论的一个关键部分是如何在很少或没有专家干预的情况下有效地服务模型。在本文中，我们将介绍一些概念和一些解决方法，需要理解这些概念和方法才能实现一个高效的模型服务系统。这种系统的设计和实施对于持续实现全年高级分析能力至关重要。</p><h1 id="fc38" class="lv lw iu bd lx ly lz ma mb mc md me mf ka mg kb mh kd mi ke mj kg mk kh ml mm bi translated">包装机器学习模型</h1><p id="a05b" class="pw-post-body-paragraph kz la iu lb b lc mn jv le lf mo jy lh li mp lk ll lm mq lo lp lq mr ls lt lu in bi translated">在基于ML的分析系统中，模型被创建，然后以特定的文件格式存储。这些文件稍后在不同平台之间移动，加载到内存中，并用于预测作业。这需要一种有效的序列化和反序列化格式，允许以可移植的方式有效地处理模型文件。不幸的是，模型文件格式的前景是分散的，不同的框架使用不同的文件格式。下面的列表提供了几个例子。</p><ul class=""><li id="462a" class="ms mt iu lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated"><a class="ae nb" href="https://scikit-learn.org/stable/modules/model_persistence.html" rel="noopener ugc nofollow" target="_blank"> Scikit-Learn </a> : pickle(。pkl)</li><li id="6620" class="ms mt iu lb b lc nc lf nd li ne lm nf lq ng lu mx my mz na bi translated"><a class="ae nb" href="https://www.tensorflow.org/tutorials/distribute/save_and_load" rel="noopener ugc nofollow" target="_blank"> TensorFlow </a>:协议缓冲区(。pb)</li><li id="0176" class="ms mt iu lb b lc nc lf nd li ne lm nf lq ng lu mx my mz na bi translated"><a class="ae nb" href="https://www.tensorflow.org/tutorials/keras/save_and_load" rel="noopener ugc nofollow" target="_blank"> Keras </a> : HDF5 (.h5)</li><li id="f93f" class="ms mt iu lb b lc nc lf nd li ne lm nf lq ng lu mx my mz na bi translated"><a class="ae nb" href="https://combust.github.io/mleap-docs/spark/" rel="noopener ugc nofollow" target="_blank">火花ML </a> : MLleap(。zip)</li><li id="635f" class="ms mt iu lb b lc nc lf nd li ne lm nf lq ng lu mx my mz na bi translated"><a class="ae nb" href="https://pytorch.org/tutorials/beginner/saving_loading_models.html" rel="noopener ugc nofollow" target="_blank"> PyTorch </a> : PyTorch文件格式(。pt)</li></ul><p id="5e1a" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">标准化方面有努力。许多基于Python的ML库支持pickle格式。一些ML库也支持目前失宠的<a class="ae nb" href="https://en.wikipedia.org/wiki/Predictive_Model_Markup_Language" rel="noopener ugc nofollow" target="_blank">预测模型标记语言</a>(。pmml)格式。<a class="ae nb" href="https://onnx.ai/" rel="noopener ugc nofollow" target="_blank">开放神经网络交换</a>(。onnx)由不同的主要库支持，但是它还没有达到广泛的覆盖范围。</p><p id="55e4" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">为了处理这样一个碎片化的空间，现代模型管理平台，如<a class="ae nb" href="https://mlflow.org/docs/0.2.0/index.html" rel="noopener ugc nofollow" target="_blank"> MLflow </a>，支持广泛的<a class="ae nb" href="https://www.mlflow.org/docs/latest/models.html#h2o-h2o" rel="noopener ugc nofollow" target="_blank">序列化/反序列化方法</a>。事实上，当一个模型在MLflow中被<a class="ae nb" href="https://www.mlflow.org/docs/latest/models.html#storage-format" rel="noopener ugc nofollow" target="_blank">记录</a>时，它实际上创建了一个包含模型文件的目录，一个名为MLmodel的YAML文件，它提供了关于反序列化模型文件的各种信息。</p><h1 id="d713" class="lv lw iu bd lx ly lz ma mb mc md me mf ka mg kb mh kd mi ke mj kg mk kh ml mm bi translated"><strong class="ak">模型服务的架构</strong></h1><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj nh"><img src="../Images/96efe087633802e96b523746ec3e81a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vVlTtIhlaXT1hlpT5m58rA.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">图2:不同类型的模型服务架构。作者图。</p></figure><p id="f20e" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">应该如何服务模型在很大程度上取决于模型应用系统的性质。从架构设计角度来看，有两种类型:<em class="ni">批量</em>和<em class="ni">在线服务</em>。首先，模型可应用于<em class="ni">批次</em> <em class="ni">作业</em>，其中大量数据用于预测大量目标值。这些工作可以容忍一定程度的延迟，可能长达数天。第二，模型可以应用于<em class="ni">在线</em> <em class="ni">作业</em>，其中仅使用小数据集来预测少量目标值。这些作业只能容忍非常小的延迟，即不超过几秒钟。</p><p id="0420" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">图2展示了三种不同的高级架构，它们解决了此类应用程序的模型服务问题。前两个处理批处理作业，而最后一个处理在线应用。在批处理作业的情况下，需要在应用程序代码运行的执行环境中将模型加载到内存中。图2(a)显示了一种设计，其中当应用程序请求模型时，模型被下载到应用程序的执行环境的存储器中。一旦加载完毕，应用程序中的预测任务就可以开始了。这主要适用于模型规模很小或频繁刷新或两者兼有的情况。如果模型很大，那么每次发出请求时都加载模型可能会违反服务级别协议，即使它们是宽松的。此外，在这种情况下，在任何执行之前在应用程序的执行环境中烘焙模型将是理想的。这可以通过在应用程序的计划运行之前，将模型放入安装在执行环境中的文件系统中，作为模型部署过程的一部分来完成。图2(b)显示了这样一种设置。最后，图2(c)给出了在线服务架构的简化视图，其中模型保持在服务应用程序附近，而与应用程序的交互仅限于数据集请求和响应。</p><h1 id="72bb" class="lv lw iu bd lx ly lz ma mb mc md me mf ka mg kb mh kd mi ke mj kg mk kh ml mm bi translated">实现模型服务系统</h1><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nj"><img src="../Images/47f7ea63a4200d0b34ebc3000a22b2f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/format:webp/1*zO16x3YrNUW3CJ9Ssl4k2w.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">图3:模型服务系统的设计。作者图。</p></figure><p id="8211" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">图3展示了一个模型服务系统的高层架构。直到最近，Rest(或其他类型的)API在这些类型的架构中扮演着重要的角色。对于基于Python的设计来说，<a class="ae nb" href="https://flask.palletsprojects.com/en/2.0.x/tutorial/index.html" rel="noopener ugc nofollow" target="_blank"> Flask </a>和<a class="ae nb" href="https://fastapi.tiangolo.com/tutorial/" rel="noopener ugc nofollow" target="_blank"> FastAPI </a>是非常好的选择。API应该与管理(打包的)模型文件的模型存储系统集成在一起。有很多存储选项，包括网络文件系统(<a class="ae nb" href="https://docs.microsoft.com/en-us/azure/storage/files/" rel="noopener ugc nofollow" target="_blank"> Azure文件</a>)、数据库(<a class="ae nb" href="https://docs.microsoft.com/en-us/azure/azure-sql/" rel="noopener ugc nofollow" target="_blank"> Azure SQL数据库</a>)和Blob存储(<a class="ae nb" href="https://docs.microsoft.com/en-us/azure/storage/blobs/" rel="noopener ugc nofollow" target="_blank"> Azure Blob存储</a>)。请注意，其他云供应商在这一领域提供了大量其他解决方案和平台。在API和存储层之间，服务系统可能需要一个缓存层。Redis是一个流行的数据库系统，用于管理缓存。在API的前面，可以放置一个反向代理，以便于处理请求/响应。Nginx 是实现这种组件的一个众所周知的平台。整个解决方案可以使用Kubernetes服务来部署，比如<a class="ae nb" href="https://docs.microsoft.com/en-us/azure/aks/" rel="noopener ugc nofollow" target="_blank"> Azure Kubernetes服务</a>。在这样的设计中，API组件可以使用能够有效平衡负载的副本集来部署。</p><p id="afd6" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">使用上述方法构建一个模型服务系统适合于具有坚实工程能力的团队。这种能力可以在支持ML产品团队的中央平台团队中获得。挑战不在于开发这样一个系统，这需要有时间限制的努力。交付商业价值的团队需要优先考虑产生价值的任务，远高于系统的维护和改进工作。对于团队来说，最好采用一个平台，即使不能完全消除，至少部分消除这种努力。例如，团队可以通过集成模型服务框架，如<a class="ae nb" href="https://docs.seldon.io/projects/seldon-core/en/v1.1.0/" rel="noopener ugc nofollow" target="_blank"> Seldon-core </a>来减少上述设计的开销。这种类型的选择允许在为模型服务时进行充分的定制，例如AB测试，而不需要付出太多的努力。喜欢香草模型服务需求的团队应该考虑使用更多电池实现服务系统，包括平台即服务，如<a class="ae nb" href="https://docs.microsoft.com/en-us/azure/machine-learning/" rel="noopener ugc nofollow" target="_blank"> Azure机器学习</a>、<a class="ae nb" href="https://docs.microsoft.com/en-us/azure/databricks/applications/mlflow/" rel="noopener ugc nofollow" target="_blank">托管在Azure Databricks </a>中的MLflow等。</p><h1 id="fa45" class="lv lw iu bd lx ly lz ma mb mc md me mf ka mg kb mh kd mi ke mj kg mk kh ml mm bi translated"><strong class="ak">连续部署</strong></h1><p id="e586" class="pw-post-body-paragraph kz la iu lb b lc mn jv le lf mo jy lh li mp lk ll lm mq lo lp lq mr ls lt lu in bi translated">ML模型的持续部署基本上意味着如何将模型放入服务环境中，以自动用于操作目的。我们避免讨论如何选择操作使用的模型。这样的讨论走向ML模型的持续集成。为了简单起见，假设解决了这样一个问题。</p><p id="2b49" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">如果您使用MLflow进行其他模型管理活动，如跟踪、编码、生命周期变更，使用MLflow服务可能是最简单的入门方式。如果模型需要更好的SLA，即更便宜、更快，或者在复杂的场景中，即AB测试，请考虑替换或集成更健壮的框架，如Seldon-core。</p><p id="83cb" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">虽然将几个脚本放在一起建立一个专家驱动的方法更容易，但一个更大胆的想法是采用一个闭环系统，使用基础设施作为代码和管道，很少或没有人类专家的参与。参见图4，了解这种系统的概况。该系统的核心是一个由绿色方框表示的持续部署管道。管道从模型注册表中以序列化形式检索所选模型，构建(Docker)容器的<a class="ae nb" href="https://docs.docker.com/engine/reference/commandline/image/" rel="noopener ugc nofollow" target="_blank">映像</a>并将该映像推入容器注册表，并使用该映像更新服务微服务。这样的管道可以使用工作流管理平台开发为代码，如<a class="ae nb" href="https://airflow.apache.org/docs/apache-airflow/stable/" rel="noopener ugc nofollow" target="_blank"> Airflow </a>、<a class="ae nb" href="https://www.kubeflow.org/docs/about/kubeflow/" rel="noopener ugc nofollow" target="_blank"> Kubeflow </a>，或DevOps平台，如<a class="ae nb" href="https://docs.microsoft.com/en-us/azure/devops/pipelines/?view=azure-devops" rel="noopener ugc nofollow" target="_blank"> Azure Pipelines </a>。可以通过监听标记模型被训练和选择使用的事件来触发流水线。当模型的性能下降或模型过期时，可以触发模型的训练和后续选择。在前一种情况下，监控系统可以生成这样的警报，而在后一种情况下，人类专家可以按照时间表启动该过程。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj nk"><img src="../Images/a4887581c9282912f6d1be9774e0bb5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XpzQ-MTs9YsHH0hGpXsxEQ.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">图4:模型服务的持续部署。作者图。</p></figure><h1 id="adf3" class="lv lw iu bd lx ly lz ma mb mc md me mf ka mg kb mh kd mi ke mj kg mk kh ml mm bi translated">强大的持续部署</h1><p id="ec53" class="pw-post-body-paragraph kz la iu lb b lc mn jv le lf mo jy lh li mp lk ll lm mq lo lp lq mr ls lt lu in bi translated">对于高可用性，可以考虑更健壮的部署策略。受web服务部署的启发，可以采用以下策略:</p><ul class=""><li id="eb5f" class="ms mt iu lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated">蓝绿色部署:新部署(蓝色)与旧部署(绿色)并行部署，两者共享相同的设置。有限份额的流量被路由到蓝色部署。一旦部署显示在有限的负载下达到可接受的SLA，绿色部署就会过期，蓝色部署将成为新的绿色部署。</li><li id="0f13" class="ms mt iu lb b lc nc lf nd li ne lm nf lq ng lu mx my mz na bi translated">金丝雀部署:蓝色和绿色部署并行运行，但是蓝色部署的流量份额会根据预定义的约束随着时间的推移而逐渐增加。一旦违反此类限制，此类部署可以回滚到旧模型以获得全部流量份额。</li></ul><p id="4b50" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">Kubernetes服务和Seldon-core微服务是我们可以采用的推荐解决方案。组装这样一个平台比实现全部功能要容易得多。</p><h1 id="833b" class="lv lw iu bd lx ly lz ma mb mc md me mf ka mg kb mh kd mi ke mj kg mk kh ml mm bi translated">评论</h1><p id="c383" class="pw-post-body-paragraph kz la iu lb b lc mn jv le lf mo jy lh li mp lk ll lm mq lo lp lq mr ls lt lu in bi translated">实现闭环持续部署系统的所有构件都在那里等着我们去抓取。将所有这些整合在一起以轻松实现ML模型持续部署的平台服务还处于早期阶段。但是，基于公有云的平台，如<a class="ae nb" href="https://cloud.google.com/vertex-ai/docs" rel="noopener ugc nofollow" target="_blank"> Google </a>、<a class="ae nb" href="https://docs.aws.amazon.com/sagemaker/latest/dg/whatis.html" rel="noopener ugc nofollow" target="_blank"> Amazon </a>、<a class="ae nb" href="https://docs.microsoft.com/en-us/azure/machine-learning/" rel="noopener ugc nofollow" target="_blank"> Azure </a>，以及ML/大数据平台服务，如<a class="ae nb" href="https://docs.databricks.com/" rel="noopener ugc nofollow" target="_blank"> Databricks </a>、<a class="ae nb" href="https://docs.neptune.ai/" rel="noopener ugc nofollow" target="_blank"> Neptune AI </a>等。，正在为实现这样的愿景而大步前进。请务必定期检查这些平台的版本，看看这些平台发展到什么程度了。</p><p id="84a2" class="pw-post-body-paragraph kz la iu lb b lc ld jv le lf lg jy lh li lj lk ll lm ln lo lp lq lr ls lt lu in bi translated">有趣的是，我们可以看到，在您的高级分析之旅中，您更喜欢和考虑哪一种选择。让我们知道你对此事的经验和看法。</p></div></div>    
</body>
</html>