<html>
<head>
<title>Detecting Disaster from Tweets (Classical ML and LSTM Approach)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从推文中检测灾难(经典的ML和LSTM方法)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/detecting-disaster-from-tweets-classical-ml-and-lstm-approach-4566871af5f7?source=collection_archive---------17-----------------------#2021-04-06">https://towardsdatascience.com/detecting-disaster-from-tweets-classical-ml-and-lstm-approach-4566871af5f7?source=collection_archive---------17-----------------------#2021-04-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/e53eb91e56872cd84f224abf0984e4b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Rd1AOfn-yh7pGzNHpmPjw.jpeg"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">克里斯·j·戴维斯在<a class="ae jg" href="https://unsplash.com/s/photos/tweet?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class=""/><div class=""><h2 id="3756" class="pw-subtitle-paragraph kg ji jj bd b kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx dk translated"><strong class="ak">使用<strong class="ak"> NLP并比较两种方法的分类</strong>任务。</strong></h2></div><p id="6b27" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我将应用两种不同的方法来完成分类任务。我将首先应用使用梯度推进分类器的经典机器学习分类算法。在代码的后面，我将使用LSTM技术来训练一个RNN模型。因为我们正在处理推文，所以这是一项NLP任务，我将分享一些技术，这样你将更加熟悉大多数NLP项目中的一些常见步骤。</p><p id="1439" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我将使用来自Kaggle挑战赛的数据，该挑战赛名为“<strong class="la jk">自然语言处理灾难推文</strong>”。你可以在下面链接的“数据”部分找到“<em class="lu"> train.csv </em>”文件。</p><div class="is it gp gr iu lv"><a href="https://www.kaggle.com/c/nlp-getting-started/overview" rel="noopener  ugc nofollow" target="_blank"><div class="lw ab fo"><div class="lx ab ly cl cj lz"><h2 class="bd jk gy z fp ma fr fs mb fu fw ji bi translated">灾难微博的自然语言处理</h2><div class="mc l"><h3 class="bd b gy z fp ma fr fs mb fu fw dk translated">预测哪些推文是关于真正的灾难，哪些不是</h3></div><div class="md l"><p class="bd b dl z fp ma fr fs mb fu fw dk translated">www.kaggle.com</p></div></div><div class="me l"><div class="mf l mg mh mi me mj ja lv"/></div></div></a></div><p id="d0cd" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">数据集有5列。列“<em class="lu"> target </em>”是标签列，这意味着我将使用其他列，如“<em class="lu"> text </em>”、“<em class="lu"> location </em>”和“<em class="lu"> keyword </em>”来训练一个可以预测列“<em class="lu"> target </em>的值的模型。现在，首先让我们了解每一列的含义:</p><ul class=""><li id="d6ee" class="mk ml jj la b lb lc le lf lh mm ll mn lp mo lt mp mq mr ms bi translated"><code class="fe mt mu mv mw b">id</code> -每条推文的唯一标识符</li><li id="034b" class="mk ml jj la b lb mx le my lh mz ll na lp nb lt mp mq mr ms bi translated"><code class="fe mt mu mv mw b">text</code> -推文的文本</li><li id="465e" class="mk ml jj la b lb mx le my lh mz ll na lp nb lt mp mq mr ms bi translated"><code class="fe mt mu mv mw b">location</code> -发送推文的位置(可能为空)</li><li id="2967" class="mk ml jj la b lb mx le my lh mz ll na lp nb lt mp mq mr ms bi translated"><code class="fe mt mu mv mw b">keyword</code> -推文中的特定关键词(可能为空)</li><li id="2a6b" class="mk ml jj la b lb mx le my lh mz ll na lp nb lt mp mq mr ms bi translated"><code class="fe mt mu mv mw b">target</code> -仅在<em class="lu"> train.csv </em>中，这表示一条推文是否是关于一场真正的灾难(<code class="fe mt mu mv mw b">1</code>)或者不是(<code class="fe mt mu mv mw b">0</code>)</li></ul><p id="64e6" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于这个任务，我将使用Sklearn和Keras等库来训练分类器模型。<strong class="la jk"> Sklearn </strong>用于使用梯度增强分类器训练模型，而<strong class="la jk"> Keras </strong>用于训练LSTM模型。</p><pre class="nc nd ne nf gt ng mw nh ni aw nj bi"><span id="d90a" class="nk nl jj mw b gy nm nn l no np"><strong class="mw jk">import</strong> <strong class="mw jk">pandas</strong> <strong class="mw jk">as</strong> <strong class="mw jk">pd</strong><br/><strong class="mw jk">import</strong> <strong class="mw jk">numpy</strong> <strong class="mw jk">as</strong> <strong class="mw jk">np</strong><br/><strong class="mw jk">import</strong> <strong class="mw jk">matplotlib.pyplot</strong> <strong class="mw jk">as</strong> <strong class="mw jk">plt</strong><br/><strong class="mw jk">import</strong> <strong class="mw jk">seaborn</strong> <strong class="mw jk">as</strong> <strong class="mw jk">sns</strong><br/><strong class="mw jk">import</strong> <strong class="mw jk">re</strong><br/><br/><strong class="mw jk">import</strong> <strong class="mw jk">nltk</strong> <br/>nltk.download('stopwords')<br/><strong class="mw jk">from</strong> <strong class="mw jk">nltk.corpus</strong> <strong class="mw jk">import</strong> stopwords<br/><strong class="mw jk">from</strong> <strong class="mw jk">nltk.tokenize</strong> <strong class="mw jk">import</strong> word_tokenize <br/><strong class="mw jk">from</strong> <strong class="mw jk">nltk.stem</strong> <strong class="mw jk">import</strong> SnowballStemmer<br/><br/><strong class="mw jk">from</strong> <strong class="mw jk">sklearn</strong> <strong class="mw jk">import</strong> model_selection, metrics, preprocessing, ensemble, model_selection, metrics<br/><strong class="mw jk">from</strong> <strong class="mw jk">sklearn.feature_extraction.text</strong> <strong class="mw jk">import</strong> CountVectorizer<br/><br/><br/><strong class="mw jk">import</strong> <strong class="mw jk">tensorflow</strong> <strong class="mw jk">as</strong> <strong class="mw jk">tf</strong><br/><strong class="mw jk">from</strong> <strong class="mw jk">tensorflow.keras.models</strong> <strong class="mw jk">import</strong> Model<br/><strong class="mw jk">from</strong> <strong class="mw jk">tensorflow.keras.preprocessing.text</strong> <strong class="mw jk">import</strong> Tokenizer<br/><strong class="mw jk">from</strong> <strong class="mw jk">tensorflow.keras.preprocessing.sequence</strong> <strong class="mw jk">import</strong> pad_sequences<br/><strong class="mw jk">from</strong> <strong class="mw jk">tensorflow.keras.layers</strong> <strong class="mw jk">import</strong> Conv1D, Bidirectional, LSTM, Dense, Dropout, Input<br/><strong class="mw jk">from</strong> <strong class="mw jk">tensorflow.keras.optimizers</strong> <strong class="mw jk">import</strong> Adam</span></pre><h1 id="b6ea" class="nq nl jj bd nr ns nt nu nv nw nx ny nz kp oa kq ob ks oc kt od kv oe kw of og bi translated"><strong class="ak">了解数据:</strong></h1><p id="62b1" class="pw-post-body-paragraph ky kz jj la b lb oh kk ld le oi kn lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">对于此任务，我们仅使用'<em class="lu"> train.csv </em>'并将它分解为训练和测试数据集两部分。我将把数据加载到Pandas Dataframe中，并查看前几行。</p><pre class="nc nd ne nf gt ng mw nh ni aw nj bi"><span id="ee2c" class="nk nl jj mw b gy nm nn l no np"><em class="lu"># Rreading train dataset</em><br/>file_path = "./train.csv"<br/>raw_data = pd.read_csv(file_path)<br/>print("Data points count: ", raw_data['id'].count())<br/>raw_data.head()</span></pre><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi om"><img src="../Images/74f6817c14d55962fb0011399b85923a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aSGZ6U6ch53_RuFoR06_3A.png"/></div></div></figure><p id="ee91" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，我想更熟悉数据集，以了解特性(列)。列"<em class="lu">目标</em>"是我们的模型将要学习预测的列。由于它只有两个唯一值<code class="fe mt mu mv mw b">0</code>和<code class="fe mt mu mv mw b">1</code>，因此这是一个二元分类任务。我想知道标签为<code class="fe mt mu mv mw b">0</code>与<code class="fe mt mu mv mw b">1</code>的推文的比例，所以让我们基于列“<em class="lu">目标</em>”绘制数据。</p><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi on"><img src="../Images/01c19fe934254608773f81887727337d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ig0jvTkzAjGucMaORRb8XA.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="8213" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如你所看到的，标签为<code class="fe mt mu mv mw b">0</code>的数据点较多，表示推文并非灾难推文，标签为<code class="fe mt mu mv mw b">1</code>的数据点较少，表示推文与灾难相关。通常，对于有一些倾斜标签的数据，建议使用F分数而不是准确性来进行模型评估，我们将在本文的结尾解决这个问题。</p><p id="145b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">接下来，我想知道我们的数据集中每一列的缺失数据点的情况。下面的热图显示“<em class="lu">关键字</em>”列几乎没有缺失的数据点，我将填充缺失的数据点，并将此列用作一个特征。列<em class="lu">位置</em>数据缺失严重，数据质量很差。它具有与位置无关的值。所以我决定不使用这个专栏，不再写了。列"<em class="lu"> text </em>"是包含需要处理和清理的实际tweet的主列。它没有丢失数据。</p><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oo"><img src="../Images/5bb0a5c456f97bd4822b1ecab8ab3070.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G1oveYvPad9XroPHt99J4Q.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="ab0a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我也注意到有些推文包含不到3个单词，我认为两个单词的句子可能无法很好地传递知识。为了了解句子的字数，我想看一下每个句子的字数直方图。</p><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi op"><img src="../Images/504ec8f78fca7ab9593d09b450801693.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*liAQG8tBsoR_ujqUQbv3lQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="c968" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如我们所见，大多数推文都在11到19个单词之间，所以我决定删除少于2个单词的推文。我相信三个单词的句子足以说明这条推文。删除超过25-30个单词的推文可能是个好主意，因为它们可能会减慢训练时间。</p><h1 id="1e94" class="nq nl jj bd nr ns nt nu nv nw nx ny nz kp oa kq ob ks oc kt od kv oe kw of og bi translated">数据清理和预处理:</h1><p id="9de7" class="pw-post-body-paragraph ky kz jj la b lb oh kk ld le oi kn lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">在处理tweets的NLP任务中，常见的数据清理步骤是删除特殊字符、删除停用词、删除URL、删除数字和进行词干处理。但是，让我们首先更熟悉一些NLP数据预处理概念:</p><p id="60f5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">矢量化:</strong></p><p id="ca96" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">单词矢量化是一种将单词映射到实数的技术，或者更好地说是实数的向量。我使用过Sklearn和Keras库中的矢量化工具。</p><p id="8d47" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">标记化:</strong></p><p id="e549" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">记号化的任务是将一个短语(可以是任何东西，比如一个句子、一个段落或者仅仅是一个文本)分解成更小的部分，比如一系列单词、一系列字符或者一系列子单词，它们被称为记号。标记化的一个用途是从文本生成标记，然后将标记转换为数字(矢量化)。</p><p id="fc1d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">填充:</strong></p><p id="e0b0" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">神经网络模型要求输入具有相同的形状和大小，这意味着一个接一个输入到模型中的所有tweets必须具有完全相同的长度，这就是填充在这里有用的地方。数据集中的每条推文都有不同的字数，我们将为每条推文设置最大字数，如果推文较长，那么如果推文的字数少于最大字数，我们可以用固定值如“0”填充推文的开头或结尾。</p><p id="5c45" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">词干:</strong></p><p id="0e49" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">词干提取的任务是将单词中多余的字符减少到单词的词根或词根。例如，词干中的“工作”和“已工作”都变成了“工作”。</p><p id="d150" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我使用了雪球斯特梅尔，这是一种词干算法(也称为Porter2词干算法)。这是波特斯特梅尔的一个更好的版本，因为在这个词干分析器中修复了一些问题。</p><p id="2d1a" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">单词嵌入:</strong></p><p id="7b20" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">单词嵌入是对文本的学习表示，其中具有相同含义的单词具有相似的表示。每个单词被映射到一个向量，向量值以类似于神经网络的方式被学习。</p><p id="b2d5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在让我们来看看完整的数据清理代码:</p><pre class="nc nd ne nf gt ng mw nh ni aw nj bi"><span id="7ecf" class="nk nl jj mw b gy nm nn l no np"><strong class="mw jk">def</strong> clean_text(each_text):<br/><br/>    <em class="lu"># remove URL from text</em><br/>    each_text_no_url = re.sub(r"http\S+", "", each_text)<br/>    <br/>    <em class="lu"># remove numbers from text</em><br/>    text_no_num = re.sub(r'\d+', '', each_text_no_url)<br/><br/>    <em class="lu"># tokenize each text</em><br/>    word_tokens = word_tokenize(text_no_num)<br/>    <br/>    <em class="lu"># remove sptial character</em><br/>    clean_text = []<br/>    <strong class="mw jk">for</strong> word <strong class="mw jk">in</strong> word_tokens:<br/>        clean_text.append("".join([e <strong class="mw jk">for</strong> e <strong class="mw jk">in</strong> word <strong class="mw jk">if</strong> e.isalnum()]))<br/><br/>    <em class="lu"># remove stop words and lower</em><br/>    text_with_no_stop_word = [w.lower() <strong class="mw jk">for</strong> w <strong class="mw jk">in</strong> clean_text <strong class="mw jk">if</strong> <strong class="mw jk">not</strong> w <strong class="mw jk">in</strong> stop_words]  <br/><br/>    <em class="lu"># do stemming</em><br/>    stemmed_text = [stemmer.stem(w) <strong class="mw jk">for</strong> w <strong class="mw jk">in</strong> text_with_no_stop_word]<br/>    <br/>    <strong class="mw jk">return</strong> " ".join(" ".join(stemmed_text).split())<br/><br/><br/>raw_data['clean_text'] = raw_data['text'].apply(<strong class="mw jk">lambda</strong> x: clean_text(x) )<br/>raw_data['keyword'] = raw_data['keyword'].fillna("none")<br/>raw_data['clean_keyword'] = raw_data['keyword'].apply(<strong class="mw jk">lambda</strong> x: clean_text(x) )</span></pre><p id="ed2c" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了能够同时使用"<em class="lu"> text </em>和"<em class="lu"> keyword </em>"列，有多种方法可以应用，但我应用的一个简单方法是将这两个特性组合成一个新特性，称为"<em class="lu"> keyword_text </em>"</p><pre class="nc nd ne nf gt ng mw nh ni aw nj bi"><span id="a8fa" class="nk nl jj mw b gy nm nn l no np"><em class="lu"># Combine column 'clean_keyword' and 'clean_text' into one</em><br/>raw_data['keyword_text'] = raw_data['clean_keyword'] + " " + raw_data["clean_text"]</span></pre><p id="524b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我已经用Sklearn的“<em class="lu"> train_test_split </em>”函数做了一个带数据洗牌的训练和测试分割。</p><pre class="nc nd ne nf gt ng mw nh ni aw nj bi"><span id="3674" class="nk nl jj mw b gy nm nn l no np">feature = "keyword_text"<br/>label = "target"<br/><br/><em class="lu"># split train and test</em><br/>X_train, X_test,y_train, y_test = model_selection.train_test_split(raw_data[feature],raw_data[label],test_size=0.3,random_state=0,shuffle=<strong class="mw jk">True</strong>)</span></pre><p id="7a4f" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如我已经提到的矢量化，我们必须将文本转换为数字，因为机器学习模型只能处理数字，所以我们在这里使用“<em class="lu">反矢量化”</em>。我们对训练数据进行拟合和转换，并且只对测试数据进行转换。确保测试数据没有出现拟合现象。</p><pre class="nc nd ne nf gt ng mw nh ni aw nj bi"><span id="4890" class="nk nl jj mw b gy nm nn l no np"><em class="lu"># Vectorize text</em><br/>vectorizer = CountVectorizer()<br/>X_train_GBC = vectorizer.fit_transform(X_train_GBC)<br/>x_test_GBC = vectorizer.transform(x_test_GBC)</span></pre></div><div class="ab cl oq or hx os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="im in io ip iq"><h1 id="5a61" class="nq nl jj bd nr ns ox nu nv nw oy ny nz kp oz kq ob ks pa kt od kv pb kw of og bi translated">梯度增强分类器:</h1><p id="5d32" class="pw-post-body-paragraph ky kz jj la b lb oh kk ld le oi kn lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">梯度推进分类器是一种机器学习算法，它将许多弱学习模型(如决策树)结合在一起，以创建强预测模型。</p><pre class="nc nd ne nf gt ng mw nh ni aw nj bi"><span id="a9e4" class="nk nl jj mw b gy nm nn l no np">model = ensemble.GradientBoostingClassifier(learning_rate=0.1,                                            <br/>                                            n_estimators=2000,<br/>                                            max_depth=9,<br/>                                            min_samples_split=6,<br/>                                            min_samples_leaf=2,<br/>                                            max_features=8,<br/>                                            subsample=0.9)<br/>model.fit(X_train_GBC, y_train)</span></pre><p id="5a98" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">评估我们模型性能的一个很好的指标是F-score。在计算F分数之前，让我们先熟悉一下<strong class="la jk">精度</strong>和<strong class="la jk">召回</strong>。</p><p id="d082" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">精度:</strong>在我们正确标注阳性的数据点中，有多少我们正确标注阳性。</p><p id="ca2b" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk">回忆:</strong>在我们正确标记为阳性的数据点中，有多少实际上是阳性的。</p><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi pc"><img src="../Images/681b3911de603a5fd332cbefad6fbc0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8ARQCywxXKz7ZlA0L3zXrQ.png"/></div></div></figure><p id="0d1e" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la jk"> F-score: </strong>是查全率和查准率的调和平均值。</p><pre class="nc nd ne nf gt ng mw nh ni aw nj bi"><span id="f439" class="nk nl jj mw b gy nm nn l no np"><em class="lu"># Evaluate the model</em><br/>predicted_prob = model.predict_proba(x_test_GBC)[:,1]<br/>predicted = model.predict(x_test_GBC)<br/><br/>accuracy = metrics.accuracy_score(predicted, y_test)<br/>print("Test accuracy: ", accuracy)<br/>print(metrics.classification_report(y_test, predicted, target_names=["0", "1"]))<br/>print("Test F-scoare: ", metrics.f1_score(y_test, predicted))</span><span id="b2ab" class="nk nl jj mw b gy pd nn l no np">Test accuracy:  0.7986784140969163<br/>              precision    recall  f1-score   support<br/><br/>           0       0.79      0.88      0.83      1309<br/>           1       0.81      0.69      0.74       961<br/><br/>    accuracy                           0.80      2270<br/>   macro avg       0.80      0.78      0.79      2270<br/>weighted avg       0.80      0.80      0.80      2270<br/><br/>Test F-scoare:  0.7439775910364146</span></pre><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/0ec5776f7f2869027b660ac25848fe20.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*XG-JRtIoeTjfWKYhu61Czg.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="d2f5" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">混淆矩阵是显示分类模型与两个类别相比的性能的表格。正如我们在图中看到的，我们的模型在检测目标值“0”时比检测目标值“1”时具有更好的性能。</p></div><div class="ab cl oq or hx os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="im in io ip iq"><h1 id="1eca" class="nq nl jj bd nr ns ox nu nv nw oy ny nz kp oz kq ob ks pa kt od kv pb kw of og bi translated">LSTM:</h1><p id="2287" class="pw-post-body-paragraph ky kz jj la b lb oh kk ld le oi kn lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">LSTM代表长期短期记忆网络是一种RNN(递归神经网络),能够学习长期依赖关系，它们可以长时间记住信息，因为设计了内部记忆系统。</p><p id="e559" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我已经在上面谈到了单词嵌入，现在是时候把它用于我们的LSTM方法了。我用的是斯坦福的手套嵌入，你可以从这里的<a class="ae jg" href="http://nlp.stanford.edu/data/glove.6B.zip" rel="noopener ugc nofollow" target="_blank">下载。在我们读取手套嵌入文件后，我们使用Keras创建一个嵌入层。</a></p><pre class="nc nd ne nf gt ng mw nh ni aw nj bi"><span id="bf4f" class="nk nl jj mw b gy nm nn l no np"><em class="lu"># Read word embeddings</em><br/>embeddings_index = {}<br/><strong class="mw jk">with</strong> open(path_to_glove_file) <strong class="mw jk">as</strong> f:<br/>    <strong class="mw jk">for</strong> line <strong class="mw jk">in</strong> f:<br/>        word, coefs = line.split(maxsplit=1)<br/>        coefs = np.fromstring(coefs, "f", sep=" ")<br/>        embeddings_index[word] = coefs<br/><br/>print("Found <strong class="mw jk">%s</strong> word vectors." % len(embeddings_index))</span><span id="40d8" class="nk nl jj mw b gy pd nn l no np"><em class="lu"># Define embedding layer in Keras</em><br/>embedding_matrix = np.zeros((vocab_size, embedding_dim))<br/><strong class="mw jk">for</strong> word, i <strong class="mw jk">in</strong> word_index.items():<br/>    embedding_vector = embeddings_index.get(word)<br/>    <strong class="mw jk">if</strong> embedding_vector <strong class="mw jk">is</strong> <strong class="mw jk">not</strong> <strong class="mw jk">None</strong>:<br/>        embedding_matrix[i] = embedding_vector<br/>        <br/>embedding_layer = tf.keras.layers.Embedding(vocab_size,embedding_dim,weights[embedding_matrix],input_length=sequence_len,trainable=<strong class="mw jk">False</strong>)</span></pre><p id="5008" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于LSTM模型，我从一个嵌入层开始，为每个输入序列生成一个嵌入向量。然后，我使用了一个卷积模型来减少特征的数量，然后是一个双向LSTM层。最后一层是致密层。因为这是一个二元分类，我们使用<em class="lu">s形</em>作为激活函数。</p><pre class="nc nd ne nf gt ng mw nh ni aw nj bi"><span id="7f24" class="nk nl jj mw b gy nm nn l no np"><em class="lu"># Define model architecture</em><br/>sequence_input = Input(shape=(sequence_len, ), dtype='int32')<br/>embedding_sequences = embedding_layer(sequence_input)<br/><br/>x = Conv1D(128, 5, activation='relu')(embedding_sequences)<br/>x = Bidirectional(LSTM(128, dropout=0.5, recurrent_dropout=0.2))(x)<br/>x = Dense(512, activation='relu')(x)<br/>x = Dropout(0.5)(x)<br/>x = Dense(512, activation='relu')(x)<br/>outputs = Dense(1, activation='sigmoid')(x)<br/>model = Model(sequence_input, outputs)<br/>model.summary()</span></pre><p id="5573" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于模型优化，我使用了Adam优化，用<em class="lu"> binary_crossentropy </em>作为损失函数。</p><pre class="nc nd ne nf gt ng mw nh ni aw nj bi"><span id="69e2" class="nk nl jj mw b gy nm nn l no np"><em class="lu"># Optimize the model</em><br/>model.compile(optimizer=Adam(learning_rate=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])</span></pre><p id="89ea" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">模型训练完成后，我想看看训练准确性和损失的学习曲线。该图显示了随着每个时期模型精度的增加和损失的减少。</p><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/62c189eee10132ce8ace024f6973c2b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*m4X4jCNJ95FPhgH2io1hgQ.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="6d3d" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我已经训练了模型，所以是时候评估它的模型性能了。我将得到模型的准确性和测试数据的F值。因为预测值是介于0和1之间的浮点数，所以我用0.5作为阈值来区分“0”和“1”。</p><pre class="nc nd ne nf gt ng mw nh ni aw nj bi"><span id="9b0f" class="nk nl jj mw b gy nm nn l no np"><em class="lu">#Evaluate the model</em><br/>predicted = model.predict(X_test, verbose=1, batch_size=10000)<br/><br/>y_predicted = [1 <strong class="mw jk">if</strong> each &gt; 0.5 <strong class="mw jk">else</strong> 0 <strong class="mw jk">for</strong> each <strong class="mw jk">in</strong> predicted]<br/><br/>score, test_accuracy = model.evaluate(X_test, y_test, batch_size=10000)<br/><br/>print("Test Accuracy: ", test_accuracy)<br/>print(metrics.classification_report(list(y_test), y_predicted))</span><span id="ef71" class="nk nl jj mw b gy pd nn l no np"><br/>Test Accuracy:  0.7726872<br/>              precision    recall  f1-score   support<br/><br/>           0       0.78      0.84      0.81      1309<br/>           1       0.76      0.68      0.72       961<br/><br/>    accuracy                           0.77      2270<br/>   macro avg       0.77      0.76      0.76      2270<br/>weighted avg       0.77      0.77      0.77      2270</span></pre><p id="1540" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如我们在混淆矩阵中看到的，RNN方法的表现与梯度推进分类器方法非常相似。该模型在检测“0”方面比检测“1”做得更好。</p><figure class="nc nd ne nf gt iv gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/0935dbbd061ebba351d9262dd36b6b2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*0-jCCfpF5ww7ux-2rZ4u7g.png"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure></div><div class="ab cl oq or hx os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="im in io ip iq"><h1 id="5d2c" class="nq nl jj bd nr ns ox nu nv nw oy ny nz kp oz kq ob ks pa kt od kv pb kw of og bi translated">结论:</h1><p id="4a95" class="pw-post-body-paragraph ky kz jj la b lb oh kk ld le oi kn lg lh oj lj lk ll ok ln lo lp ol lr ls lt im bi translated">如您所见，两种方法的输出非常接近。梯度推进分类器的训练速度比LSTM模型快得多。</p><p id="5ca9" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有许多方法可以提高模型的性能，如修改输入数据、应用不同的训练方法或使用超参数搜索算法，如<strong class="la jk"> GridSearch </strong>或<strong class="la jk"> RandomizedSearch </strong>来找到超参数的最佳值。</p><p id="1526" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您可以在这里找到完整的代码:</p><figure class="nc nd ne nf gt iv"><div class="bz fp l di"><div class="pg ph l"/></div></figure></div><div class="ab cl oq or hx os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="im in io ip iq"><p id="ac03" class="pw-post-body-paragraph ky kz jj la b lb lc kk ld le lf kn lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">参考:</p><div class="is it gp gr iu lv"><a href="https://keras.io/examples/nlp/pretrained_word_embeddings/" rel="noopener  ugc nofollow" target="_blank"><div class="lw ab fo"><div class="lx ab ly cl cj lz"><h2 class="bd jk gy z fp ma fr fs mb fu fw ji bi translated">Keras文档:使用预先训练的单词嵌入</h2><div class="mc l"><h3 class="bd b gy z fp ma fr fs mb fu fw dk translated">作者:fchollet创建日期:2020/05/05最近修改时间:2020/05/05描述:新闻组上的文本分类20…</h3></div><div class="md l"><p class="bd b dl z fp ma fr fs mb fu fw dk translated">keras.io</p></div></div><div class="me l"><div class="pi l mg mh mi me mj ja lv"/></div></div></a></div></div></div>    
</body>
</html>