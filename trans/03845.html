<html>
<head>
<title>Syntax Gotchas writing PySpark when knowing Pandas</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解熊猫时写PySpark的语法陷阱</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/syntax-gotchas-writing-pyspark-when-knowing-pandas-57ace4ae769b?source=collection_archive---------31-----------------------#2021-03-30">https://towardsdatascience.com/syntax-gotchas-writing-pyspark-when-knowing-pandas-57ace4ae769b?source=collection_archive---------31-----------------------#2021-03-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/241e5bbab5fe09671b89657be10622c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cdpBFq0yNq7udecq"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">米卡·鲍梅斯特在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class=""/><div class=""><h2 id="6b4a" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">如果您对Python Pandas的数据分析有一些基础知识，并且对PySpark很好奇，不知道从哪里开始，请跟随我。</h2></div><p id="8d7c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae jd" href="https://pandas.pydata.org/" rel="noopener ugc nofollow" target="_blank"> Python熊猫</a>鼓励我们抛弃excel表格，转而从编码者的角度来看待数据。数据集变得越来越大，从数据库变成了数据文件，变成了数据湖。Apache的一些聪明人用基于Scala的框架Spark祝福我们在合理的时间内处理更大的数量。由于Python是当今数据科学的主流语言，很快就有了一个Python API，叫做<a class="ae jd" href="https://spark.apache.org/docs/latest/api/python/index.html" rel="noopener ugc nofollow" target="_blank"> PySpark </a>。</p><p id="0853" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">有一段时间，我试图用大数据世界中每个人都称赞的非pythonic语法征服这个Spark接口。我尝试了几次，现在仍在进行中。然而，在这篇文章中，我想告诉你，谁也开始学习PySpark，如何复制相同的分析，否则你会做熊猫。</p><p id="fb9d" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们要看的数据分析示例可以在Wes McKinney的书<a class="ae jd" href="https://wesmckinney.com/pages/book.html" rel="noopener ugc nofollow" target="_blank">“Python for Data Analysis”</a>中找到。在该分析中，目标是从MovieLens 1M数据集中找出排名最高的电影，该数据由明尼苏达大学的<a class="ae jd" href="https://grouplens.org/" rel="noopener ugc nofollow" target="_blank"> GroupLens </a>研究项目获取和维护。</p><p id="f956" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">作为一个编码框架，我使用了<a class="ae jd" href="https://www.kaggle.com/" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>，因为它带来了笔记本电脑的便利，安装了基本的数据科学模块，只需点击两下就可以使用。</p><p id="151b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">完整的分析和Pyspark代码你也可以在<a class="ae jd" href="https://www.kaggle.com/christine12/movielens-1m-dataset-pyspark" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jh">这本Kaggle笔记本</strong> </a> <strong class="kx jh"> </strong>和<strong class="kx jh"> </strong> <a class="ae jd" href="https://www.kaggle.com/christine12/movielens-1m-dataset-python-pandas" rel="noopener ugc nofollow" target="_blank"> <strong class="kx jh">这本</strong> </a>中找到熊猫代码。这里我们不会重复相同的分析，而是集中在处理Pandas和Pyspark数据帧时的语法差异上。我将始终首先显示熊猫代码，然后显示PySpark等价物。</p><p id="d6f5" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">此分析所需的基本功能是:</p><ul class=""><li id="3d67" class="lr ls jg kx b ky kz lb lc le lt li lu lm lv lq lw lx ly lz bi translated">从csv格式加载数据</li><li id="ac3c" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">组合不同表中的数据集</li><li id="cc9c" class="lr ls jg kx b ky ma lb mb le mc li md lm me lq lw lx ly lz bi translated">提取信息</li></ul></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><h1 id="6535" class="mm mn jg bd mo mp mq mr ms mt mu mv mw km mx kn my kp mz kq na ks nb kt nc nd bi translated"><strong class="ak">加载数据</strong></h1><p id="2cee" class="pw-post-body-paragraph kv kw jg kx b ky ne kh la lb nf kk ld le ng lg lh li nh lk ll lm ni lo lp lq ij bi translated">Pandas有不同的<a class="ae jd" href="https://pandas.pydata.org/docs/reference/io.html" rel="noopener ugc nofollow" target="_blank">读取功能</a>，这使得根据存储数据的文件类型导入数据变得容易。</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="b776" class="ns mn jg no b gy nt nu l nv nw"># Pandas<br/>pd.read_table(path_to_file, sep='::', header=None, names=column_names, engine='python')</span></pre><p id="2e42" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">使用PySpark，我们首先需要创建一个<a class="ae jd" href="https://spark.apache.org/docs/latest/sql-getting-started.html" rel="noopener ugc nofollow" target="_blank"> Spark上下文</a>作为Sparks功能的入口点。这并不是说当您将它复制粘贴到代码中时，缩进可能不匹配。</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="16f7" class="ns mn jg no b gy nt nu l nv nw"># PySpark<br/>from pyspark.sql import SparkSession<em class="nx"><br/></em>spark = SparkSession.Builder().getOrCreate()<em class="nx"> #--&gt; Spark Context</em></span><span id="cbf4" class="ns mn jg no b gy ny nu l nv nw"><em class="nx">spark.read                        #--&gt; what do you want to do</em><br/><em class="nx">    .format("csv")                #--&gt; which format is the file</em><br/><em class="nx">    .option("delimiter", "::")    #--&gt; specify delimiter that's used </em><br/><em class="nx">    .option("inferSchema", "true")#--&gt; default value type is string</em><br/><em class="nx">    .load(path_to_file)            #--&gt; file path</em></span></pre><h1 id="dfe5" class="mm mn jg bd mo mp nz mr ms mt oa mv mw km ob kn my kp oc kq na ks od kt nc nd bi translated">组合数据集</h1><p id="426d" class="pw-post-body-paragraph kv kw jg kx b ky ne kh la lb nf kk ld le ng lg lh li nh lk ll lm ni lo lp lq ij bi translated">Pandas中合并数据集的默认函数是<a class="ae jd" href="https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html" rel="noopener ugc nofollow" target="_blank"><strong class="kx jh"/></a><strong class="kx jh">()，</strong>，它合并一列或多列上的数据集。就连Pandas <strong class="kx jh"> join() </strong>函数也使用merge与一个索引列相结合。评级、用户和电影都在这个代码片段中，它们共享各自的列，所以我们可以调用合并函数。在名称不匹配的情况下，我们需要使用join()来代替。</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="8523" class="ns mn jg no b gy nt nu l nv nw"># Pandas<br/>data = pd.merge(pd.merge(ratings, users), movies)</span></pre><p id="4e5b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在PySpark中没有merge函数，这里默认的是对选择的列使用<strong class="kx jh"> join() </strong>。否则看起来相当相似。</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="137c" class="ns mn jg no b gy nt nu l nv nw"># PySpark<br/>data = ratings.join(users, ["user_id"]).join(movies, ["movie_id"])</span></pre><h1 id="e5cf" class="mm mn jg bd mo mp nz mr ms mt oa mv mw km ob kn my kp oc kq na ks od kt nc nd bi translated"><strong class="ak">提取信息</strong></h1><p id="5380" class="pw-post-body-paragraph kv kw jg kx b ky ne kh la lb nf kk ld le ng lg lh li nh lk ll lm ni lo lp lq ij bi translated">现在我们已经有了可以处理的数据形式，我们可以看看如何真正提取内容。</p><h2 id="1cbf" class="ns mn jg bd mo oe of dn ms og oh dp mw le oi oj my li ok ol na lm om on nc oo bi translated">显示行条目</h2><p id="fce2" class="pw-post-body-paragraph kv kw jg kx b ky ne kh la lb nf kk ld le ng lg lh li nh lk ll lm ni lo lp lq ij bi translated">Pandas数据帧源自numpy数组，因此可以使用方形括号来访问元素。如果我们想查看表格的前5个元素，我们可以这样做:</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="3fac" class="ns mn jg no b gy nt nu l nv nw"># Pandas<br/>users[:5]</span></pre><p id="fa24" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在Spark中，数据帧是对象，因此我们需要使用函数。在显示前5行的情况下，我们可以使用<strong class="kx jh"> show() </strong>，它来自Spark dataframes使用的底层SQL语法。函数show只是表示数据集的一种方式，如果你想从选择的条目中创建一个新的dataframe，你需要使用<strong class="kx jh"> take() </strong>。</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="c830" class="ns mn jg no b gy nt nu l nv nw"># PySpark<br/>users.show(5) # --&gt; print result </span><span id="db79" class="ns mn jg no b gy ny nu l nv nw">users.take(5) # --&gt; list of Row() objects</span></pre><h2 id="f247" class="ns mn jg bd mo oe of dn ms og oh dp mw le oi oj my li ok ol na lm om on nc oo bi translated"><strong class="ak">过滤</strong></h2><p id="0682" class="pw-post-body-paragraph kv kw jg kx b ky ne kh la lb nf kk ld le ng lg lh li nh lk ll lm ni lo lp lq ij bi translated">在熊猫身上，我们可以用不同的方式过滤。我们可以使用的一个函数是<strong class="kx jh"> loc </strong>，这是一个基于标签的过滤器，用于访问行或列。在数据分析示例中，我们希望过滤数据帧中出现在标题列表中的所有行。</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="e4aa" class="ns mn jg no b gy nt nu l nv nw"># Pandas<br/>mean_ratings = mean_ratings.loc[active_titles]</span></pre><p id="9634" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">PySpark数据帧不支持<strong class="kx jh"> loc，</strong>因此我们需要使用过滤函数。处理PySpark数据帧中的列的一种简单方法是使用<strong class="kx jh"> col() </strong>函数。用列名调用这个函数，将从dataframe返回相应的列。</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="fdf0" class="ns mn jg no b gy nt nu l nv nw"># PySpark<br/>from pyspark.sql.functions import col<br/>mean_ratings = mean_ratings.filter(col('title').isin(active_titles))</span></pre><h2 id="af2e" class="ns mn jg bd mo oe of dn ms og oh dp mw le oi oj my li ok ol na lm om on nc oo bi translated">分组</h2><p id="88bf" class="pw-post-body-paragraph kv kw jg kx b ky ne kh la lb nf kk ld le ng lg lh li nh lk ll lm ni lo lp lq ij bi translated">Pandas和PySpark中的分组看起来非常类似于<strong class="kx jh"> groupby() </strong>。Pandas数据帧有许多为分组对象实现的聚合函数<a class="ae jd" href="https://pandas.pydata.org/docs/reference/groupby.html" rel="noopener ugc nofollow" target="_blank">和</a>，如中位数、平均值、总和、方差等。对于Pyspark数据帧，有必要将其导入，例如从pyspark.sql.functions导入，如<strong class="kx jh"> mean() </strong>和standard dev <strong class="kx jh"> stddev() </strong>。</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="4a8a" class="ns mn jg no b gy nt nu l nv nw"># Pandas<br/>ratings_by_title = data.groupby('title')['rating'].std()</span><span id="a8f0" class="ns mn jg no b gy ny nu l nv nw"># PySpark<br/>from pyspark.sql.functions import mean, stddev<br/>coldata_sdf.groupBy('title').agg(stddev(col('rating')).alias('std'))</span></pre><h2 id="f6a8" class="ns mn jg bd mo oe of dn ms og oh dp mw le oi oj my li ok ol na lm om on nc oo bi translated">数据透视表</h2><p id="dec6" class="pw-post-body-paragraph kv kw jg kx b ky ne kh la lb nf kk ld le ng lg lh li nh lk ll lm ni lo lp lq ij bi translated">数据透视表常用于数据分析。熊猫数据框的排列和聚集都在一个函数<a class="ae jd" href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html" rel="noopener ugc nofollow" target="_blank"><strong class="kx jh">pivot _ table</strong></a><strong class="kx jh">中。</strong></p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="5126" class="ns mn jg no b gy nt nu l nv nw"># Pandas<br/>mean_ratings = data.pivot_table('rating',<br/>                                index='title',<br/>                                columns='gender',<br/>                                aggfunc='mean')</span></pre><p id="7af6" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了用Pyspark dataframes复制相同的结果，我们需要连接分组、透视和聚合函数。</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="16de" class="ns mn jg no b gy nt nu l nv nw"># PySpark<br/>mean_ratings_pivot = data<br/>                       .groupBy('title')<br/>                       .pivot('gender')<br/>                       .agg(mean('rating'))</span></pre><h2 id="67e4" class="ns mn jg bd mo oe of dn ms og oh dp mw le oi oj my li ok ol na lm om on nc oo bi translated">整理</h2><p id="b0d2" class="pw-post-body-paragraph kv kw jg kx b ky ne kh la lb nf kk ld le ng lg lh li nh lk ll lm ni lo lp lq ij bi translated">要对一列中的值进行升序或降序排序，我们可以为Pandas数据帧调用<strong class="kx jh"> sort_values() </strong>函数。请记住，默认的排序顺序是升序。</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="2ab4" class="ns mn jg no b gy nt nu l nv nw"># Pandas<br/>top_female_ratings = mean_ratings.sort_values(by='F',<br/>                                              ascending=False)</span></pre><p id="6b1a" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在PySpark中有一个类似的函数叫做<strong class="kx jh"> sort() </strong>，但是我们想要排序的列，我们必须作为输入给出。类似地，我们可以使用函数<strong class="kx jh"> orderBy() </strong>并得到相同的结果。</p><pre class="nj nk nl nm gt nn no np nq aw nr bi"><span id="1e8b" class="ns mn jg no b gy nt nu l nv nw"># PySpark<br/>top_female_ratings = mean_ratings.sort(mean_ratings.F.desc())</span></pre></div><div class="ab cl mf mg hu mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="ij ik il im in"><p id="cb14" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">随着时间的推移，Pandas和PySpark的语法将会改变。也许我们运气好，它们会变得更蟒蛇。还有一些模块结合了provide和PySpark的一个熊猫API，叫做<a class="ae jd" href="https://koalas.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">考拉</a>。如果它的性能可以与PySpark竞争，我们可能会在未来看到更多这种情况。</p><p id="25b9" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我希望你找到了一些有用的提示，并请让我知道你从学习Spark，语法和功能的见解！</p></div></div>    
</body>
</html>