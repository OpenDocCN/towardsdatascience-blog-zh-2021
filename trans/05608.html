<html>
<head>
<title>Masked-Language Modeling With BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用BERT进行屏蔽语言建模</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/masked-language-modelling-with-bert-7d49793e5d2c?source=collection_archive---------0-----------------------#2021-05-19">https://towardsdatascience.com/masked-language-modelling-with-bert-7d49793e5d2c?source=collection_archive---------0-----------------------#2021-05-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b5b3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在任何数据集上微调您的模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7ad93baa1a95f1ea54320be3a0bbbd0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O44j6aoz_O4Z2kI9kjMaJw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">伯特的双向二头肌-作者图片。</p></figure><p id="29be" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi lu translated"><span class="l lv lw lx bm ly lz ma mb mc di"> B </span> ERT，大家最喜欢的变形金刚花费Google ~$7K训练[1](还有谁知道R &amp; D花费多少)。从那里，我们编写几行代码来使用同一个模型——全部免费。</p><p id="fa29" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">由于两种独特的训练方法，掩蔽语言建模(MLM)和下一句预测(NSP)，BERT在NLP中获得了无与伦比的成功。</p><p id="662f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在许多情况下，我们可能能够将预先训练好的BERT模型开箱即用，并成功地应用到我们自己的语言任务中。</p><p id="c243" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但通常，我们可能需要微调模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="md me l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">MLM是如何运作的</p></figure><p id="2144" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">与MLM的进一步训练使我们能够微调伯特，以更好地理解语言在更具体领域的特殊用途。</p><p id="ff88" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现成的BERT —非常适合一般用途。使用MLM·伯特进行微调，非常适合特定领域的使用。</p><p id="f759" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我们将深入了解什么是MLM，它是如何工作的，以及我们如何用它来改进我们的模型。</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h1 id="104a" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">屏蔽语言建模</h1><p id="dbb8" class="pw-post-body-paragraph ky kz it la b lb ne ju ld le nf jx lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated">MLM包括给伯特一个句子，并优化伯特内部的权重，以在另一端输出相同的句子。</p><p id="0406" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以我们输入一个句子，要求BERT输出同样的句子。</p><p id="99e8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然而，在我们实际给BERT输入句子之前，我们屏蔽了一些标记。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4ec8df26ee2d93d78c2d8eb13bed54ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*phTLnQ8itb3ZX5_h9BWjWw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在这幅图中，在将我们的令牌传递给BERT之前，我们已经屏蔽了<strong class="bd nj"> lincoln </strong>令牌，用<strong class="bd nj">【屏蔽】</strong>替换它。</p></figure><p id="a750" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以我们实际上是在输入一个不完整的句子，然后让伯特帮我们完成它。</p><h2 id="653c" class="nk mn it bd mo nl nm dn ms nn no dp mw lh np nq my ll nr ns na lp nt nu nc nv bi translated">填补空白</h2><p id="c449" class="pw-post-body-paragraph ky kz it la b lb ne ju ld le nf jx lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated">这是什么效果？嗯，这就像我们很多人在学校里被问到的那些问题——在那里，给定一个句子，我们必须填补空白。</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="86c6" class="nk mn it nx b gy ob oc l od oe">In Autumn the ______ fall from the trees.</span></pre><p id="9b65" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你知道答案吗？很可能你知道，你知道是因为你考虑了句子的上下文。</p><p id="3913" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们看到单词<em class="of">落在</em>和<em class="of">树上</em>——我们知道丢失的单词是<em class="of">从树上</em>落下的东西。</p><p id="182b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">很多东西从树上掉下来，橡子、树枝、树叶——但是我们有另一个条件，秋天<em class="of"/>——这缩小了我们的搜索范围，秋天最有可能从树上掉下来的东西是<em class="of">树叶</em>。</p><p id="39e9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">作为人类，我们综合运用一般的世界知识和语言理解来得出结论。对伯特来说，这个猜测将来自于大量阅读<em class="of">——并且非常好地学习语言模式。</em></p><p id="c00b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">伯特可能不知道什么是秋天、树木和树叶，但他知道给定的语言模式和这些单词的上下文，答案很可能是树叶。</p><p id="bdda" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个过程的结果——对伯特来说——是提高了对所用语言风格的理解。</p><h2 id="6ea3" class="nk mn it bd mo nl nm dn ms nn no dp mw lh np nq my ll nr ns na lp nt nu nc nv bi translated">该过程</h2><p id="30f7" class="pw-post-body-paragraph ky kz it la b lb ne ju ld le nf jx lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated">所以我们明白MLM在做什么，但这实际上是如何运作的呢？我们在代码中需要遵循的逻辑步骤是什么？</p><p id="b3e1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi lu translated"><span class="l lv lw lx bm ly lz ma mb mc di"> 1。</span>我们<strong class="la iu">标记化</strong>我们的文本。就像我们通常使用变形金刚一样，我们从文本标记化开始。</p><p id="d9c2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">通过标记化，我们将得到三个不同的张量:</p><ul class=""><li id="0c24" class="og oh it la b lb lc le lf lh oi ll oj lp ok lt ol om on oo bi translated"><em class="of">输入_标识</em></li><li id="e451" class="og oh it la b lb op le oq lh or ll os lp ot lt ol om on oo bi translated"><em class="of">令牌类型标识</em></li><li id="088f" class="og oh it la b lb op le oq lh or ll os lp ot lt ol om on oo bi translated"><em class="of">注意_屏蔽</em></li></ul><p id="2bb7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于MLM，我们不需要<em class="of">token _ type _ ids</em>——在本例中<em class="of"> attention_mask </em>并不重要。</p><p id="4ad2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对我们来说，<em class="of"> input_ids </em>张量最为重要。在这里，我们将有一个我们的文本的标记化表示——这是我们将修改前进。</p><p id="58b5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi lu translated"><span class="l lv lw lx bm ly lz ma mb mc di"> 2。</span>创建一个<strong class="la iu"> <em class="of">标签</em> </strong>张量。我们在这里训练我们的模型，所以我们需要一个<em class="of">标签</em>张量来计算损失——并优化。</p><p id="0462" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="of">标签</em>张量就是<em class="of">input _ ids</em>——所以我们需要做的就是复制一份。</p><p id="f68e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi lu translated"><span class="l lv lw lx bm ly lz ma mb mc di"> 3。</span> <strong class="la iu">屏蔽<em class="of"> input_ids </em>中的</strong>令牌。既然我们已经为<em class="of">标签</em>创建了<em class="of"> input_ids </em>的副本，我们就可以继续并屏蔽随机选择的令牌了。</p><p id="a9e4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">BERT论文在模型预训练期间使用15%的概率屏蔽每个单词，并使用一些附加规则-我们将使用这种方法的简化版本，并指定每个单词被屏蔽的概率为15%。</p><p id="c3f3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi lu translated"><span class="l lv lw lx bm ly lz ma mb mc di"> 4。</span>计算<strong class="la iu">损失</strong>。我们通过我们的BERT模型处理<em class="of"> input_ids </em>和<em class="of">label</em>张量，并计算它们之间的损失。</p><p id="8db7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">利用这一损失，我们通过BERT计算所需的梯度变化，并优化我们的模型权重。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/0ef32d3ef0d1e8867856258413dc80c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0KvOrY6rY055m9oq36HRkg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">所有512个标记产生一个最终的输出嵌入—<strong class="bd nj">logits</strong>——其向量长度等于模型vocab的大小。预测的<strong class="bd nj"> token_id </strong>是使用softmax和argmax转换从该logit中提取的。</p></figure><p id="c7e6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">损失计算为每个输出“令牌”的输出概率分布与真正的独热编码标签<em class="of">之间的差异。</em></p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h1 id="c2e7" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">代码中的MLM</h1><p id="32f9" class="pw-post-body-paragraph ky kz it la b lb ne ju ld le nf jx lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated">好的，这些都很好，但是我们如何用代码演示MLM呢？</p><p id="b38e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将使用拥抱脸的变形金刚和PyTorch，以及<code class="fe ou ov ow nx b">bert-base-uncased</code>模型。因此，让我们首先导入并初始化所有内容:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox me l"/></div></figure><p id="316c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们进入每一个逻辑步骤，从以下步骤开始:</p><p id="5abb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi lu translated"><span class="l lv lw lx bm ly lz ma mb mc di"> 1。</span> <strong class="la iu">标记化</strong> —标记化很简单，我们已经初始化了一个<code class="fe ou ov ow nx b">BertTokenizer</code>，我们现在要做的就是标记化我们的输入<code class="fe ou ov ow nx b">text</code>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox me l"/></div></figure><p id="6d1f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们现在不担心填充/截断。我们应该注意的是前面描述的三个张量——<em class="of">token _ type _ ids</em>和<em class="of"> attention_mask </em>不需要我们的任何关注——但是<em class="of"> input_ids </em>需要。</p><p id="ada5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi lu translated"><span class="l lv lw lx bm ly lz ma mb mc di"> 2。</span> <strong class="la iu">创建标签</strong> —下一步很简单，我们需要做的就是将我们的<em class="of"> input_ids </em>张量克隆到一个新的<em class="of">标签</em>张量中。我们也将把它存储在<code class="fe ou ov ow nx b">inputs</code>变量中。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox me l"/></div></figure><p id="f6b9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi lu translated"><span class="l lv lw lx bm ly lz ma mb mc di"> 3。</span> <strong class="la iu">屏蔽</strong> —现在我们需要屏蔽<em class="of"> input_ids </em>张量中的随机选择的记号。</p><p id="9e32" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了创建屏蔽任何一个令牌的15%的概率，我们可以使用<code class="fe ou ov ow nx b">torch.rand</code>和每个值的条件<code class="fe ou ov ow nx b">&lt; 0.15</code>。总之，这些将产生我们的掩蔽阵列<code class="fe ou ov ow nx b">mask_arr</code>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox me l"/></div></figure><p id="eaf8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们使用<code class="fe ou ov ow nx b">mask_arr</code>来选择放置<em class="of">掩码</em>令牌的位置—但是我们不想将<em class="of">掩码</em>令牌放置在其他特殊令牌之上，例如<em class="of"> CLS </em>或<em class="of"> SEP </em>令牌(分别为<em class="of"> 101 </em>和<em class="of"> 102 </em>)。</p><p id="ea51" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以，我们需要增加一个附加条件。检查包含令牌id<em class="of">101</em>或<em class="of"> 102 </em>的位置。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox me l"/></div></figure><p id="5f17" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在这就是我们的屏蔽张量，为了应用它，我们将首先提取我们找到一个<code class="fe ou ov ow nx b">True</code>值的索引位置，然后使用这个<em class="of">选择</em>将这些位置的值设置为<em class="of"> 103 </em>(屏蔽令牌id)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox me l"/></div></figure><p id="5bae" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们可以在上面的<em class="of"> input_ids </em>张量中看到由<em class="of"> 103 </em>表示的<em class="of">掩码</em>令牌。</p><p id="ba3f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi lu translated"><span class="l lv lw lx bm ly lz ma mb mc di"> 4。</span> <strong class="la iu">计算损失</strong> —我们这里的最后一步与典型的模型训练过程没有什么不同。</p><p id="5974" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有了<em class="of"> input_ids </em>和<em class="of">标签</em>在我们的<code class="fe ou ov ow nx b">inputs</code>字典中，我们可以将它传递给我们的<code class="fe ou ov ow nx b">model</code>并返回模型损失。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox me l"/></div></figure></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h1 id="a54a" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">培养</h1><p id="6965" class="pw-post-body-paragraph ky kz it la b lb ne ju ld le nf jx lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated">很好，我们已经完成了所有的要点——但是当微调一个模型时，所有这些看起来会怎么样呢？</p><p id="fb74" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有两种方法，(1)我们使用目前为止所学的一切来实现我们自己版本的训练功能，或者(2)我们使用HuggingFace的<code class="fe ou ov ow nx b">Trainer</code>。</p><p id="28b0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe ou ov ow nx b">Trainer</code>显然是一种优化、易用的解决方案。我们将看看如何使用它——但首先，让我们试着自己实现它。</p><h2 id="f16e" class="nk mn it bd mo nl nm dn ms nn no dp mw lh np nq my ll nr ns na lp nt nu nc nv bi translated">我们的实施</h2><p id="d16c" class="pw-post-body-paragraph ky kz it la b lb ne ju ld le nf jx lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated">了解了所有这些之后，如果不尝试实现我们自己的培训功能，那将是一种浪费。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="md me l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用PyTorch为MLM训练BERT的演练</p></figure><p id="451c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，我们需要数据。因为我们只是随机屏蔽了一些标记，所以我们几乎可以使用任何文本。我们不需要有标签的或特殊的数据。</p><p id="9eee" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这里，我们将使用<em class="of">马库斯·奥勒留</em>的<em class="of">冥想</em>，来源于<a class="ae oy" href="http://classics.mit.edu/Antoninus/meditations.html" rel="noopener ugc nofollow" target="_blank">这里</a>并稍加预处理(<a class="ae oy" href="https://github.com/jamescalam/transformers/blob/main/data/text/meditations/clean.txt" rel="noopener ugc nofollow" target="_blank">干净版</a>)。</p><p id="748a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，我们将导入/初始化并加载我们的文本数据。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox me l"/></div></figure><p id="cc30" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后我们<strong class="la iu">对</strong>进行标记化——这一次我们截断并填充每个序列——因为我们有许多不同长度的序列。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox me l"/></div></figure><p id="5928" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们克隆<em class="of">输入标识</em>来创建我们的<em class="of">标签</em>张量。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox me l"/></div></figure><p id="3603" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">接下来是我们的屏蔽代码，这次有一点不同，原因有二:</p><ul class=""><li id="ab2f" class="og oh it la b lb lc le lf lh oi ll oj lp ok lt ol om on oo bi translated">我们的遮罩不应包括<em class="of">垫</em>令牌(如之前的<em class="of"> CLS </em>和<em class="of"> SEP </em>)。</li><li id="2ce7" class="og oh it la b lb op le oq lh or ll os lp ot lt ol om on oo bi translated">我们有许多序列——不只是一个。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox me l"/></div></figure><p id="1bbe" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以看到值<em class="of"> 103 </em>被分配在与在<code class="fe ou ov ow nx b">mask_arr</code>张量中找到的<em class="of">真值</em>相同的位置。</p><p id="5c92" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><code class="fe ou ov ow nx b">inputs</code>张量现在准备好了——我们可以开始设置它们，以便在训练期间输入到我们的模型中。</p><p id="123f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在训练期间，我们将使用PyTorch <code class="fe ou ov ow nx b">DataLoader</code>来加载我们的数据。要使用它，我们需要将数据格式化成PyTorch <code class="fe ou ov ow nx b">Dataset</code>对象。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox me l"/></div></figure><p id="9213" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们准备进入我们的训练循环。在开始我们的循环之前，我们需要设置三样东西:</p><ul class=""><li id="94dc" class="og oh it la b lb lc le lf lh oi ll oj lp ok lt ol om on oo bi translated">将模型移动到GPU/CPU (GPU如果可用)。</li><li id="6f53" class="og oh it la b lb op le oq lh or ll os lp ot lt ol om on oo bi translated">激活模型训练模式。</li><li id="c1cf" class="og oh it la b lb op le oq lh or ll os lp ot lt ol om on oo bi translated">用加权衰减优化器初始化Adam。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox me l"/></div></figure><p id="98d6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们终于设置好了—我们可以开始训练了！我们将其格式化为PyTorch中的典型训练循环。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox me l"/></div></figure><p id="2b4a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">至此，我们完成了—我们已经实现了自己的MLM微调脚本。</p><h2 id="d7d7" class="nk mn it bd mo nl nm dn ms nn no dp mw lh np nq my ll nr ns na lp nt nu nc nv bi translated">运动鞋</h2><p id="5d26" class="pw-post-body-paragraph ky kz it la b lb ne ju ld le nf jx lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated">切换到我们的<code class="fe ou ov ow nx b">Trainer</code>实现——我们仍然需要做我们之前所做的一切，直到我们创建了我们的<code class="fe ou ov ow nx b">dataset</code>——因为<code class="fe ou ov ow nx b">Trainer</code>将期待这作为训练的输入。</p><p id="2ad7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将首先定义我们的训练参数，初始化<code class="fe ou ov ow nx b">Trainer</code>——然后训练！</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ox me l"/></div></figure><p id="5421" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此<code class="fe ou ov ow nx b">Trainer</code>方法肯定要简单得多——它允许我们通过在初始化时简单地指定检查点和其他特性来实现它们。</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="8b26" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">好了，这就是我们开始用MLM微调模型所需要知道的一切。</p><p id="931f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">MLM有很多，但是概念和实现并不复杂——而且非常强大。</p><p id="c75c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用我们在这里学到的知识，我们可以采用NLP中最好的模型，并对它们进行微调以适应我们更特定于领域的语言用例——只需要未标记的文本——通常是很容易找到的数据源。</p><p id="116b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我希望你喜欢这篇文章！如果你有任何问题，请通过<a class="ae oy" href="https://twitter.com/jamescalam" rel="noopener ugc nofollow" target="_blank"> Twitter </a>或在下面的评论中告诉我。如果你想要更多这样的内容，我也会在<a class="ae oy" href="https://www.youtube.com/c/jamesbriggs" rel="noopener ugc nofollow" target="_blank"> YouTube </a>上发布。</p><p id="e628" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢阅读！</p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><h1 id="a159" class="mm mn it bd mo mp mq mr ms mt mu mv mw jz mx ka my kc mz kd na kf nb kg nc nd bi translated">参考</h1><p id="3318" class="pw-post-body-paragraph ky kz it la b lb ne ju ld le nf jx lg lh ng lj lk ll nh ln lo lp ni lr ls lt im bi translated">[1] <a class="ae oy" href="https://syncedreview.com/2019/06/27/the-staggering-cost-of-training-sota-ai-models/" rel="noopener ugc nofollow" target="_blank">训练SOTA人工智能模型的惊人成本</a> (2019)，同步评论</p><p id="8ad6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae oy" href="https://bit.ly/nlp-transformers" rel="noopener ugc nofollow" target="_blank">🤖带变压器的NLP课程</a></p></div><div class="ab cl mf mg hx mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="im in io ip iq"><p id="c03d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="of">*所有图片均由作者提供，除非另有说明</em></p></div></div>    
</body>
</html>