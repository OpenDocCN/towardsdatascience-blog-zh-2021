<html>
<head>
<title>Hyper-Parameter Tuning in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的超参数调整</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hyper-parameter-tuning-in-python-1923797f124f?source=collection_archive---------11-----------------------#2021-06-10">https://towardsdatascience.com/hyper-parameter-tuning-in-python-1923797f124f?source=collection_archive---------11-----------------------#2021-06-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="595d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">网格搜索与随机搜索</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/cbeb97c923d03a30f4917084dc7b342d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LZlBs8EfAkyjdYgETVt0EA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@denisseleon?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">丹尼斯·莱昂</a>在<a class="ae ky" href="https://unsplash.com/s/photos/tune?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="e750" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我跑完一个模型有一个结果的时候，我总想问问自己。这是我能得到的最好结果吗？</p><p id="21c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除了改变算法或增加训练数据量，我们还可以利用超参数调整来<strong class="lb iu">使模型对我们的数据集更加稳健</strong>。</p><blockquote class="lv lw lx"><p id="9f41" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">首先，让我们弄清楚术语。什么是参数，什么是超参数？</p></blockquote><p id="87dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">参数:</strong><strong class="lb iu">可以通过训练数据</strong>进行训练和调整的模型参数。例如y = Ax+B。A和B是可以由训练数据训练的参数。</p><p id="976e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">超参数:</strong>不受训练数据影响的<strong class="lb iu">模型参数。它为该特定算法定义了<strong class="lb iu">底层模型结构</strong>。例如，决策树算法的休假次数</strong></p><p id="c7ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文将介绍进行超参数调优的一般步骤和两个常用的自动调优包。出于演示目的，我使用make_classification包创建了一个分类数据集。</p><pre class="kj kk kl km gt mc md me mf aw mg bi"><span id="fb45" class="mh mi it md b gy mj mk l ml mm">#Create classification dataset<br/>from sklearn.datasets import make_classification<br/>X, Y = make_classification(n_samples=10000, n_informative=10,n_redundant=0, n_classes=3,<br/>                               n_features=10,<br/>                               random_state=0)</span><span id="615e" class="mh mi it md b gy mn mk l ml mm">#split into training and testing set<br/>from sklearn.model_selection import train_test_split<br/>X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)</span></pre><h2 id="8cf8" class="mh mi it bd mo mp mq dn mr ms mt dp mu li mv mw mx lm my mz na lq nb nc nd ne bi translated">基线模型(无调整)</h2><pre class="kj kk kl km gt mc md me mf aw mg bi"><span id="bffd" class="mh mi it md b gy mj mk l ml mm">import time<br/>start = time.time()<br/>import lightgbm as lgb<br/>from sklearn.metrics import accuracy_score<br/>clf = lgb.LGBMClassifier()<br/>clf.fit(X=X_train, y=Y_train)<br/>predicted=clf.predict(X_test)<br/>print('Classification of the result is:')<br/>print(accuracy_score(Y_test, predicted))<br/>end = time.time()<br/>print('Execution time is:')<br/>print(end - start)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/2aaa89f16007d52a31da72fb9bb87076.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*C7vpnWqntlFFuLoGFOQaVg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">基本模型结果</p></figure><p id="5b88" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我在这里使用了Light GBM作为算法。基线模型的准确率为88.8%，完成整个过程只需要0.3秒。</p><h1 id="e34d" class="ng mi it bd mo nh ni nj mr nk nl nm mu jz nn ka mx kc no kd na kf np kg nd nq bi translated">超参数调整的步骤</h1><h2 id="8344" class="mh mi it bd mo mp mq dn mr ms mt dp mu li mv mw mx lm my mz na lq nb nc nd ne bi translated">第一步:我们想要实现什么？</h2><p id="a1ab" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">首先，我们需要确定我们在调优过程中试图实现的主要目标。通常，有3个共同的目标。</p><ol class=""><li id="01f6" class="nw nx it lb b lc ld lf lg li ny lm nz lq oa lu ob oc od oe bi translated">提高准确性</li><li id="a63a" class="nw nx it lb b lc of lf og li oh lm oi lq oj lu ob oc od oe bi translated">减少过度拟合</li><li id="0969" class="nw nx it lb b lc of lf og li oh lm oi lq oj lu ob oc od oe bi translated">更快的运行速度</li></ol><p id="ffa0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">前两个目标可以通过自动调整超参数来实现。我们可以通过在样本外数据集上测试来评估该模型(这意味着我们没有使用它进行训练)。</p><p id="8f71" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后一个目标“运行速度”通常在处理大量数据时需要。如果数据科学模型已部署并每天运行，我们希望该模型能在合理的时间范围内给出结果。通常，我们可以在最后一步手动调整这一部分。</p><p id="a9b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于我的例子，我没有非常大的数据集，因此我只关心准确性和过度拟合问题(前两个目标)。</p><h2 id="7717" class="mh mi it bd mo mp mq dn mr ms mt dp mu li mv mw mx lm my mz na lq nb nc nd ne bi translated">步骤2:每个目标的调优参数是什么？</h2><p id="3f45" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">在我们明确了我们想要达到的目标之后，我们应该确定每个目标的调整参数。通常，根据您选择的算法，您可以找到关于所有可能的调优参数的文档。</p><p id="938a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个例子中，我使用的是Light GBM，您可以在这里找到完整的参数列表<a class="ae ky" href="https://lightgbm.readthedocs.io/en/latest/Parameters.html" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="c280" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是我为自动调谐选择的5个超参数:</p><ul class=""><li id="3ab6" class="nw nx it lb b lc ld lf lg li ny lm nz lq oa lu ok oc od oe bi translated"><strong class="lb iu"> num_leaves: </strong>一棵树的最大叶子数，树模型的主要参数</li><li id="725f" class="nw nx it lb b lc of lf og li oh lm oi lq oj lu ok oc od oe bi translated"><strong class="lb iu"> min_child_samples: </strong>一次休假的最小数据数</li><li id="6cd3" class="nw nx it lb b lc of lf og li oh lm oi lq oj lu ok oc od oe bi translated"><strong class="lb iu">最大深度:</strong>树的最大深度</li><li id="a42c" class="nw nx it lb b lc of lf og li oh lm oi lq oj lu ok oc od oe bi translated"><strong class="lb iu"> learning_rate: </strong>收缩率，决定模型可以学习的速度</li><li id="df89" class="nw nx it lb b lc of lf og li oh lm oi lq oj lu ok oc od oe bi translated"><strong class="lb iu"> reg_alpha: </strong>正则化，处理过度拟合</li></ul><p id="0212" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">“叶数”、“最小_子_样本数”、“最大_深度”和“注册_alpha”会影响精确度和过度拟合。“叶子数量”和“最大深度”的值越高，精确度越高，也会导致更多的过度拟合。min_child_samples和reg_alpha越高，过拟合越小，但精度会降低。学习速率决定了模型学习的速度，但也会影响精度。</p><h2 id="db61" class="mh mi it bd mo mp mq dn mr ms mt dp mu li mv mw mx lm my mz na lq nb nc nd ne bi translated">步骤3:确定调谐的值范围</h2><p id="5dea" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">通常，当我们第一次开始使用某些算法时，我们不知道为自动调整设置什么值范围。我所做的是<strong class="lb iu">首先找出该超参数的默认值</strong>，然后<strong class="lb iu">围绕默认值</strong>设置数值范围。</p><p id="1c3c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我通常会运行模型2回合。<strong class="lb iu">对于第一轮，我通常会设置区间，主要是为了找到一个合适的参数值范围</strong>。例如，在我的例子中，num_leaves是需要优化的最重要的参数。默认值为31。因此，首先，我将范围设置为[10，20，30，60，90，120]，范围很大。</p><p id="9195" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在第一轮结果之后，我得到num_leaves的最佳参数值为60。对于第二轮，我会将间隔再次设置为60左右，更接近于[40，50，60，70，80]。</p><blockquote class="lv lw lx"><p id="9368" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">您不需要运行模型太多次，因为最佳参数是不同参数的组合，因此<strong class="lb iu">单个参数</strong>没有最佳值。运行模型2次只是你在合理范围内缩小参数值的一种方法。</p></blockquote><h2 id="c2e1" class="mh mi it bd mo mp mq dn mr ms mt dp mu li mv mw mx lm my mz na lq nb nc nd ne bi translated">步骤4:为模型运行自动调整</h2><p id="ce8e" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">这里，我使用两种常用的方法<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" rel="noopener ugc nofollow" target="_blank"> GridSearch </a>和<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html" rel="noopener ugc nofollow" target="_blank"> RandomSearch </a>进行自动调优。这两种方法都采用了<strong class="lb iu">“适应和评分”方法</strong>。这意味着它将从定义的范围中选择一组超参数，拟合模型，产生分数，并最终输出具有最高分数的模型。这里的“分数”是由我们定义的，它<strong class="lb iu">取决于我们试图最大化</strong>什么。例如，它可以是准确度、对数损失、F1分数等。</p><blockquote class="ol"><p id="ee40" class="om on it bd oo op oq or os ot ou lu dk translated">G <!-- --> rid搜索与随机搜索</p></blockquote><figure class="ow ox oy oz pa kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/c149077be3997472dea2b5e693461fbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d8fuFpz2pYT8sMc1rjlTXA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf" rel="noopener ugc nofollow" target="_blank">麻省理工学院关于随机搜索的论文</a></p></figure><p id="8eea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">网格搜索:</strong>在预定义的参数值范围内进行穷举搜索。试验次数由调谐参数的数量和范围决定。</p><pre class="kj kk kl km gt mc md me mf aw mg bi"><span id="abb1" class="mh mi it md b gy mj mk l ml mm">start = time.time()<br/>from sklearn.model_selection import GridSearchCV<br/>import lightgbm as lgb<br/>lgb=lgb.LGBMClassifier()</span><span id="ad65" class="mh mi it md b gy mn mk l ml mm">#Define the parameters<br/>parameters = {'num_leaves':[20,40,60,80,100], 'min_child_samples':[5,10,15],'max_depth':[-1,5,10,20],<br/>             'learning_rate':[0.05,0.1,0.2],'reg_alpha':[0,0.01,0.03]}</span><span id="21f3" class="mh mi it md b gy mn mk l ml mm">#Define the scoring<br/>clf=GridSearchCV(lgb,parameters,scoring='accuracy')<br/>clf.fit(X=X_train, y=Y_train)<br/>print(clf.best_params_)<br/>predicted=clf.predict(X_test)<br/>print('Classification of the result is:')<br/>print(accuracy_score(Y_test, predicted))<br/>end = time.time()<br/>print('Execution time is:')<br/>print(end - start)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pb"><img src="../Images/312bbc5a71864cc565592bf03bd76ce0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MymDoi7xUhp3Lmiohlb7qQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">网格搜索的结果</p></figure><p id="7366" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与基线模型相比，网格搜索将精确度提高了大约1.2%。但是，运行时间是4个多小时！</p><p id="a08f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">随机搜索:</strong>从预定义的参数值范围内随机抽取样本。<strong class="lb iu">试验次数由‘n _ ITER’参数</strong>决定，因此更具灵活性。此外，如图所示，当您有一个连续的值范围时，它可以检索更多随机分布的情况，而不是从“网格线”中检索参数集。</p><pre class="kj kk kl km gt mc md me mf aw mg bi"><span id="634c" class="mh mi it md b gy mj mk l ml mm">start = time.time()<br/>from sklearn.model_selection import RandomizedSearchCV<br/>import lightgbm as lgb<br/>lgb=lgb.LGBMClassifier()<br/>parameters = {'num_leaves':[20,40,60,80,100], 'min_child_samples':[5,10,15],'max_depth':[-1,5,10,20],<br/>             'learning_rate':[0.05,0.1,0.2],'reg_alpha':[0,0.01,0.03]}<br/>clf=RandomizedSearchCV(lgb,parameters,scoring='accuracy',n_iter=100)<br/>clf.fit(X=X_train, y=Y_train)<br/>print(clf.best_params_)<br/>predicted=clf.predict(X_test)<br/>print('Classification of the result is:')<br/>print(accuracy_score(Y_test, predicted))<br/>end = time.time()<br/>print('Execution time is:')<br/>print(end - start)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/8c663895cd5605241d01b6681fa64ef1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*utt7Bo8WzujdoxLiXeMunA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">随机搜索的结果</p></figure><p id="f05b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我指定试验次数为100次。随机搜索调优模型的精度仅比网格搜索低0.3%。然而，它只需要3.5分钟，比网格搜索快得多。</p><h2 id="089b" class="mh mi it bd mo mp mq dn mr ms mt dp mu li mv mw mx lm my mz na lq nb nc nd ne bi translated">第五步:看看调好的结果，做一些手动调整</h2><p id="a531" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">根据目标和调整后的模型结果，您可以选择手动调整一些超参数。例如，如果测试精度仍然远低于训练精度，您可以选择为正则化设置更高的值，以减少过度拟合。或者，如果你想提高跑步速度，你可以设置一个较高的学习率或减少num_leaves。</p><h1 id="8268" class="ng mi it bd mo nh ni nj mr nk nl nm mu jz nn ka mx kc no kd na kf np kg nd nq bi translated">建议/结论</h1><p id="3b6d" class="pw-post-body-paragraph kz la it lb b lc nr ju le lf ns jx lh li nt lk ll lm nu lo lp lq nv ls lt lu im bi translated">本文的主要信息不仅仅是关于超参数自动调整的技术部分，还包括我们应该如何调整超参数的<strong class="lb iu">思考过程</strong>。</p><p id="7151" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在网格搜索和随机搜索之间，我推荐使用随机搜索，原因如下:</p><ol class=""><li id="e998" class="nw nx it lb b lc ld lf lg li ny lm nz lq oa lu ob oc od oe bi translated">随机搜索有更多的灵活性，可以控制你想进行多少次试验。</li><li id="f12e" class="nw nx it lb b lc of lf og li oh lm oi lq oj lu ob oc od oe bi translated">随机搜索的运行时间通常要短得多，因此它给你更多的时间来尝试更多轮次的调优。</li><li id="96d8" class="nw nx it lb b lc of lf og li oh lm oi lq oj lu ob oc od oe bi translated">对于连续超参数，随机搜索可以探索一组更独特的值</li></ol><p id="d49d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你有任何问题或建议，或者任何你想了解的新话题，请在评论中提出！</p><p id="502d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢您的阅读，希望它能帮助您更好地理解超参数调优。</p></div></div>    
</body>
</html>