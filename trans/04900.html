<html>
<head>
<title>Recent Advances in Graph Convolutional Network (GCN)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">图形卷积网络(GCN)的最新进展</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/recent-advances-in-graph-convolutional-network-gcn-9166b27969e5?source=collection_archive---------29-----------------------#2021-04-28">https://towardsdatascience.com/recent-advances-in-graph-convolutional-network-gcn-9166b27969e5?source=collection_archive---------29-----------------------#2021-04-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="aa9e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用通俗易懂的英语介绍GCN最新发展的便车指南</h2></div><p id="f1f4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">图卷积网络(GCN)因其在解决深度互联的现实世界问题中的多功能性而越来越受欢迎。如果您需要快速复习GNN/GCN，请在继续之前点击此处。在本帖中，我们将用更简单的语言来强调GCN建筑的一些进步…</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/736924b1b43a00761e5fff4a3b56000e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*RVaB-UXTPwzhbWM1"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated"><a class="ae le" href="https://unsplash.com/@alinnnaaaa?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">阿丽娜·格鲁布尼亚</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="9f9f" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">目录:</h1><ol class=""><li id="f8c9" class="mn mo it kk b kl mp ko mq kr mr kv ms kz mt ld mu mv mw mx bi translated">GCN积木</li><li id="fbce" class="mn mo it kk b kl my ko mz kr na kv nb kz nc ld mu mv mw mx bi translated">萨格科夫</li><li id="1dc6" class="mn mo it kk b kl my ko mz kr na kv nb kz nc ld mu mv mw mx bi translated">金康夫</li><li id="69c8" class="mn mo it kk b kl my ko mz kr na kv nb kz nc ld mu mv mw mx bi translated">图形注意网络</li></ol><h1 id="5f7f" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">GCN积木</h1><p id="eef9" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">GCN将更传统的卷积神经网络(CNN)的卷积原理结合到图形数据结构中。让我们更深入地探讨这个问题。</p><h2 id="9596" class="ng lw it bd lx nh ni dn mb nj nk dp mf kr nl nm mh kv nn no mj kz np nq ml nr bi translated"><strong class="ak">卷积和消息传递</strong></h2><p id="bbcb" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">简而言之，图中的卷积聚合来自相邻节点的信息，应用特定的聚合函数，并输出某些内容(例如，新特征嵌入、输出)。下图可以清楚地说明这一点。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ns"><img src="../Images/869dbcd06760faf54e403f60a8b0cfa5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IBtdI1iusQFL-yV2sfOwXA.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">GCN的卷积法</p></figure><p id="c5fc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上图中感兴趣的节点(即。绿色节点)正在从它的直接邻居(即蓝色节点)，应用平均值作为其集合函数，并输出新的嵌入。这个操作被称为<strong class="kk iu">消息传递(MP) </strong>，由所述图中的每个节点执行。</p><h2 id="6ebb" class="ng lw it bd lx nh ni dn mb nj nk dp mf kr nl nm mh kv nn no mj kz np nq ml nr bi translated">聚合函数</h2><p id="5544" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">聚合函数是许多GCN变量的关键。上述GCN的初始公式使用归一化作为其聚合函数，定义如下。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/4e87214ac301cb9a6376f79defa48f59.png" data-original-src="https://miro.medium.com/v2/resize:fit:390/format:webp/1*DfJnTChiSXSMFu0oCzXA6A.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">GCN的聚合函数</p></figure><p id="fac5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中<strong class="kk iu"> D </strong>为度矩阵(即。每个节点有多少个邻居)，<strong class="kk iu"> A </strong>是邻接矩阵(即每个节点如何与其他节点连接)，以及<strong class="kk iu"> X </strong>特征矩阵(即如何描述节点)。</p><p id="e178" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">上面的规格化公式仅仅意味着一个平均运算:如果一个节点比其他节点有更多的邻居，按比例平均X。</p><p id="6b56" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们已经有了基本的构建模块，让我们讨论一些在GCN更受欢迎的进展。</p><h1 id="6b53" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">萨格科夫</h1><p id="4d74" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">(论文:<a class="ae le" href="https://arxiv.org/abs/1706.02216" rel="noopener ugc nofollow" target="_blank"> ICLR </a>)</p><p id="620c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="nu">想象训练一个复杂的图形。如果再增加一个节点呢？你需要从头开始重新训练整个图形吗？</em></p><p id="e7d8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">SAGEConv从这个问题出发，通过归纳学习使GCN训练更加健壮。这是通过在集合函数中引入可学习的W1和W2权重矩阵，并对每个节点邻居的特征矩阵应用均值运算(类似于GCN)来实现的。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/0171801d3cee1f0a77c6f04e27d05099.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/1*10FFjZQZOnZ3rKL4ZxTrRA.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">SAGEConv聚集函数</p></figure><p id="8003" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在训练之后，如果您需要将一个额外的节点包括到现有的图中，您不必从头开始重新学习:您可以应用学习到的权重矩阵来为新的节点生成新的嵌入。</p><h1 id="86c7" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">金康夫</h1><p id="ba79" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">(论文:<a class="ae le" href="https://arxiv.org/abs/1810.00826" rel="noopener ugc nofollow" target="_blank"> ICLR </a>)</p><p id="0b78" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="nu">如果您想在聚合步骤中学习非线性表示法(不是传统的平均值、加法等),以近似模拟不同节点之间真实世界的复杂交互，该怎么办？</em></p><p id="5f74" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">接下来是GINConv，它将聚集的特征矩阵输入到可学习的人工神经网络(ANN)中。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nw"><img src="../Images/52850089ddfe88f8f64425028a87a4fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*UXbmNBMggsnHgcNn6fWcVg.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">GINConv聚合函数</p></figure><p id="a1b8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中h是前馈神经网络。该公式简单地意味着，向邻接矩阵中的每个元素添加标量值ε，与聚集的特征矩阵相乘，并将结果馈送到可学习的ANN中。</p><p id="21a2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该论文报告说，这种架构比GCN的早期变体更稳定，尤其是在区分更简单的图形时。</p><h1 id="8d50" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">手枪</h1><p id="4bd8" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">(论文:<a class="ae le" href="https://arxiv.org/abs/1710.10903" rel="noopener ugc nofollow" target="_blank"> ICLR </a>)</p><p id="f73d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你能在GCN中加入注意力机制会怎么样？通过专注于重要的特征节点而将其余的降级？</p><p id="d773" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这就是GAT通过修改下面的聚合函数所要达到的目的。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/37d5fef1065922b034a1bb1065978420.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*9e7e-piOlCtHIc1i6rpwng.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">GAT聚合函数</p></figure><p id="662d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中alpha为关注系数(在这篇<a class="ae le" href="https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/" rel="noopener ugc nofollow" target="_blank">帖子</a>中可以了解更多关于关注的内容)，X为每个节点的特征矩阵。</p><h1 id="8fb3" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="0ed4" class="pw-post-body-paragraph ki kj it kk b kl mp ju kn ko mq jx kq kr nd kt ku kv ne kx ky kz nf lb lc ld im bi translated">我们已经到了帖子的末尾。在这里，我们讨论了一些更重要的GCN变种。如果你注意到，这些变化大部分来自聚合函数中的新公式:线性(均值、求和)或非线性(ANN、注意系数)运算。所以下次你阅读任何GCN最先进的模型论文时，请留意这些变化！</p></div><div class="ab cl ny nz hx oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="im in io ip iq"><p id="2617" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="nu">做订阅我的邮件简讯:</em></strong><a class="ae le" href="https://tinyurl.com/2npw2fnz" rel="noopener ugc nofollow" target="_blank"><em class="nu">https://tinyurl.com/2npw2fnz</em></a><em class="nu"/><strong class="kk iu"><em class="nu">在那里我定期用通俗易懂的语言和漂亮的可视化方式总结AI研究论文。</em> </strong></p></div></div>    
</body>
</html>