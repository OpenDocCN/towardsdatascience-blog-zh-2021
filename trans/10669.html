<html>
<head>
<title>How To Delete Columns From PySpark DataFrames</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何从PySpark数据帧中删除列</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/delete-columns-pyspark-df-ba9272db1bb4?source=collection_archive---------10-----------------------#2021-10-13">https://towardsdatascience.com/delete-columns-pyspark-df-ba9272db1bb4?source=collection_archive---------10-----------------------#2021-10-13</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="ff7d" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">讨论在PySpark中从数据帧中删除列的不同方法</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/fe5cdb4591014ca1b8418533d0b9df8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RhWYVMhPITizjEZnmaTfmA.jpeg"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">由<a class="ae kz" href="https://unsplash.com/@melocokr?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">萨姆·帕克</a>在<a class="ae kz" href="https://unsplash.com/s/photos/delete?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><h2 id="a2ea" class="la lb iu bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">介绍</h2><p id="72b7" class="pw-post-body-paragraph lw lx iu ly b lz ma jv mb mc md jy me lj mf mg mh ln mi mj mk lr ml mm mn mo in bi translated">从数据帧中删除列是PySpark中最常执行的任务之一。在今天的简短指南中，我们将探索从PySpark数据帧中删除列的几种不同方法。具体来说，我们将讨论如何</p><ul class=""><li id="bcfb" class="mp mq iu ly b lz mr mc ms lj mt ln mu lr mv mo mw mx my mz bi translated">删除单个列</li><li id="5b3e" class="mp mq iu ly b lz na mc nb lj nc ln nd lr ne mo mw mx my mz bi translated">删除多列</li><li id="0cef" class="mp mq iu ly b lz na mc nb lj nc ln nd lr ne mo mw mx my mz bi translated">反向操作，在更方便的情况下选择所需的列。</li></ul></div><div class="ab cl nf ng hy nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="in io ip iq ir"><p id="f15d" class="pw-post-body-paragraph lw lx iu ly b lz mr jv mb mc ms jy me lj nm mg mh ln nn mj mk lr no mm mn mo in bi translated">首先，让我们创建一个示例数据框架，我们将在本指南中引用它来演示一些概念。</p><pre class="kk kl km kn gu np nq nr ns aw nt bi"><span id="5fe5" class="la lb iu nq b gz nu nv l nw nx">from pyspark.sql import SparkSession</span><span id="3a35" class="la lb iu nq b gz ny nv l nw nx"># Create an instance of spark session<br/>spark_session = SparkSession.builder \<br/>    .master('local[1]') \<br/>    .appName('Example') \<br/>    .getOrCreate()</span><span id="ce64" class="la lb iu nq b gz ny nv l nw nx"># Create an example DataFrame<br/>df = spark_session.createDataFrame(<br/>    [<br/>        (1, True, 'a', 1.0),<br/>        (2, True, 'b', 2.0),<br/>        (3, False, 'c', 3.0),<br/>        (4, False, 'd', 4.0),<br/>    ],<br/>    ['colA', 'colB', 'colC', 'colD']<br/>)</span><span id="c8ce" class="la lb iu nq b gz ny nv l nw nx">df.show()<br/><em class="nz">+----+-----+----+----+<br/>|colA| colB|colC|colD|<br/>+----+-----+----+----+<br/>|   1| true|   a| 1.0|<br/>|   2| true|   b| 2.0|<br/>|   3|false|   c| 3.0|<br/>|   4|false|   d| 4.0|<br/>+----+-----+----+----+</em></span></pre></div><div class="ab cl nf ng hy nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="in io ip iq ir"><h2 id="395e" class="la lb iu bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">删除单个列</h2><p id="557f" class="pw-post-body-paragraph lw lx iu ly b lz ma jv mb mc md jy me lj mf mg mh ln mi mj mk lr ml mm mn mo in bi translated">删除列最优雅的方式是使用<code class="fe oa ob oc nq b"><a class="ae kz" href="https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.drop.html" rel="noopener ugc nofollow" target="_blank">pyspark.sql.DataFrame.drop</a></code>函数，该函数返回一个新的数据帧，其中包含要删除的指定列:</p><pre class="kk kl km kn gu np nq nr ns aw nt bi"><span id="355e" class="la lb iu nq b gz nu nv l nw nx"><strong class="nq iv">df  = df.drop('colC')</strong></span><span id="c34a" class="la lb iu nq b gz ny nv l nw nx">df.show()<br/><em class="nz">+----+-----+----+<br/>|colA| colB|colD|<br/>+----+-----+----+<br/>|   1| true| 1.0|<br/>|   2| true| 2.0|<br/>|   3|false| 3.0|<br/>|   4|false| 4.0|<br/>+----+-----+----+</em></span></pre><p id="fa37" class="pw-post-body-paragraph lw lx iu ly b lz mr jv mb mc ms jy me lj nm mg mh ln nn mj mk lr no mm mn mo in bi translated">请注意，如果指定的列在该列中不存在，这将是一个空操作，意味着操作不会失败，也不会有任何影响。</p></div><div class="ab cl nf ng hy nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="in io ip iq ir"><h2 id="c3a6" class="la lb iu bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">删除多列</h2><p id="f522" class="pw-post-body-paragraph lw lx iu ly b lz ma jv mb mc md jy me lj mf mg mh ln mi mj mk lr ml mm mn mo in bi translated">通常，您可能需要一次删除多个列。如果是这种情况，那么您可以将希望删除的列指定为一个列表，然后使用星号对它们进行解包，如下所示。</p><pre class="kk kl km kn gu np nq nr ns aw nt bi"><span id="66bf" class="la lb iu nq b gz nu nv l nw nx"><strong class="nq iv">cols_to_drop = ['colB', 'colC']<br/>df = df.drop(*cols_to_drop)</strong></span><span id="23dd" class="la lb iu nq b gz ny nv l nw nx">df.show()<br/><em class="nz">+----+----+<br/>|colA|colD|<br/>+----+----+<br/>|   1| 1.0|<br/>|   2| 2.0|<br/>|   3| 3.0|<br/>|   4| 4.0|<br/>+----+----+</em></span></pre></div><div class="ab cl nf ng hy nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="in io ip iq ir"><h2 id="958a" class="la lb iu bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">颠倒逻辑</h2><p id="1f81" class="pw-post-body-paragraph lw lx iu ly b lz ma jv mb mc md jy me lj mf mg mh ln mi mj mk lr ml mm mn mo in bi translated">在某些情况下，更方便的做法是反转拖放操作，实际上只选择想要保留的列的子集。例如，如果要删除的列数大于要在结果数据帧中保留的列数，则执行选择是有意义的。</p><p id="d786" class="pw-post-body-paragraph lw lx iu ly b lz mr jv mb mc ms jy me lj nm mg mh ln nn mj mk lr no mm mn mo in bi translated">例如，假设我们只想保留上面数据帧中的一列。在这种情况下，选择该列比删除其他3列更有意义:</p><pre class="kk kl km kn gu np nq nr ns aw nt bi"><span id="598a" class="la lb iu nq b gz nu nv l nw nx"><strong class="nq iv">df = df.select('colA')</strong></span><span id="0a21" class="la lb iu nq b gz ny nv l nw nx">df.show()<br/><em class="nz">+----+<br/>|colA|<br/>+----+<br/>|   1|<br/>|   2|<br/>|   3|<br/>|   4|<br/>+----+</em></span></pre></div><div class="ab cl nf ng hy nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="in io ip iq ir"><h2 id="78f3" class="la lb iu bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">最后的想法</h2><p id="f311" class="pw-post-body-paragraph lw lx iu ly b lz ma jv mb mc md jy me lj mf mg mh ln mi mj mk lr ml mm mn mo in bi translated">在今天的简短指南中，我们讨论了从PySpark数据帧中删除列的几种不同方法。除了直接删除列之外，我们还看到，在某些情况下，反向操作可能更方便，实际上只选择您希望保留在结果数据帧中的所需列。</p></div><div class="ab cl nf ng hy nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="in io ip iq ir"><p id="1b77" class="pw-post-body-paragraph lw lx iu ly b lz mr jv mb mc ms jy me lj nm mg mh ln nn mj mk lr no mm mn mo in bi translated"><a class="ae kz" href="https://gmyrianthous.medium.com/membership" rel="noopener"> <strong class="ly iv">成为会员</strong> </a> <strong class="ly iv">阅读介质上的每一个故事。你的会员费直接支持我和你看的其他作家。你也可以在媒体上看到所有的故事。</strong></p></div><div class="ab cl nf ng hy nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="in io ip iq ir"><p id="676e" class="pw-post-body-paragraph lw lx iu ly b lz mr jv mb mc ms jy me lj nm mg mh ln nn mj mk lr no mm mn mo in bi translated"><strong class="ly iv">你可能也会喜欢</strong></p><div class="od oe gq gs of og"><a rel="noopener follow" target="_blank" href="/how-to-efficiently-convert-a-pyspark-dataframe-to-pandas-8bda2c3875c3"><div class="oh ab fp"><div class="oi ab oj cl cj ok"><h2 class="bd iv gz z fq ol fs ft om fv fx it bi translated">加快PySpark和Pandas数据帧之间的转换</h2><div class="on l"><h3 class="bd b gz z fq ol fs ft om fv fx dk translated">将大火花数据帧转换为熊猫时节省时间</h3></div><div class="oo l"><p class="bd b dl z fq ol fs ft om fv fx dk translated">towardsdatascience.com</p></div></div><div class="op l"><div class="oq l or os ot op ou kt og"/></div></div></a></div></div><div class="ab cl nf ng hy nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="in io ip iq ir"><div class="kk kl km kn gu og"><a rel="noopener follow" target="_blank" href="/sparksession-vs-sparkcontext-vs-sqlcontext-vs-hivecontext-741d50c9486a"><div class="oh ab fp"><div class="oi ab oj cl cj ok"><h2 class="bd iv gz z fq ol fs ft om fv fx it bi translated">spark session vs spark context vs SQLContext vs hive context</h2><div class="on l"><h3 class="bd b gz z fq ol fs ft om fv fx dk translated">SparkSession、SparkContext HiveContext和SQLContext有什么区别？</h3></div><div class="oo l"><p class="bd b dl z fq ol fs ft om fv fx dk translated">towardsdatascience.com</p></div></div><div class="op l"><div class="ov l or os ot op ou kt og"/></div></div></a></div></div><div class="ab cl nf ng hy nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="in io ip iq ir"><div class="kk kl km kn gu og"><a rel="noopener follow" target="_blank" href="/apache-spark-3-0-the-five-most-exciting-new-features-99c771a1f512"><div class="oh ab fp"><div class="oi ab oj cl cj ok"><h2 class="bd iv gz z fq ol fs ft om fv fx it bi translated">Apache Spark 3.0:5个最激动人心的新特性</h2><div class="on l"><h3 class="bd b gz z fq ol fs ft om fv fx dk translated">Apache Spark 3.0新版本中最激动人心的5个特性</h3></div><div class="oo l"><p class="bd b dl z fq ol fs ft om fv fx dk translated">towardsdatascience.com</p></div></div><div class="op l"><div class="ow l or os ot op ou kt og"/></div></div></a></div></div></div>    
</body>
</html>