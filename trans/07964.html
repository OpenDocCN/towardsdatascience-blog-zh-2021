<html>
<head>
<title>Support Vector Machines: The Basics</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机:基础知识</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/support-vector-machines-the-basics-5a2acc0db450?source=collection_archive---------36-----------------------#2021-07-21">https://towardsdatascience.com/support-vector-machines-the-basics-5a2acc0db450?source=collection_archive---------36-----------------------#2021-07-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ff2c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">对数据集进行分类时，SVM是逻辑回归的一个很好的替代方法。它被用于线性和非线性分类，在Matlab和Python中都得到了很好的处理。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/e2b23508cc363cae82c62d75b9454764.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GmqFcho5-QV_-h8F9vMp5Q.png"/></div></div></figure><h1 id="0deb" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">基础知识</h1><p id="751c" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">SVM执行的重要工作是找到一个决策边界来分类我们的数据。这个决策边界也被称为<strong class="ll ir">超平面</strong>。</p><p id="443a" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">让我们以一个例子来解释它。<strong class="ll ir">视觉上</strong>，如果你看图1，你会发现紫线是比黑线更好的超平面是有道理的。黑线也可以完成这项工作，但是稍微滑动一点接近其中一个红点，使它成为一个好的决策线。</p><p id="fb83" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">从视觉上看，这很容易发现。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/3959175a6835a2e6a3749a354f23db73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*45NuP7qq29bmOHMjlcGxHw.png"/></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">图1</p></figure><h1 id="0c03" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">线性分类</h1><p id="3da3" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">所以，问题依然存在，SVM是如何证明这一点的呢？如果我们坚持将紫色线作为最佳超平面，那么SVM将寻找最接近它的点，计算这些点和超平面之间的距离。这个距离被称为余量，如图2中绿色虚线所示。</p><p id="0522" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">在图2中，超平面和我们的数据点之间的这些间隙(亮绿色)被称为支持向量。</p><p id="359c" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">一旦它找到了聚类之间具有最大间隔的超平面，<strong class="ll ir"> BOOM </strong> - <strong class="ll ir"> BAM </strong>，我们就找到了我们的最优超平面。因此，SVM确保集群之间的差距尽可能大。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/f269082ac0a4bf395e40afb09171d82b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8vogExmJuIeDDxb2ioRLAw.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">图2</p></figure><h1 id="d7d3" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">非线性分类</h1><p id="ff88" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">非线性分类在SVM也是可行的，事实上，这是SVM的闪光点。这里，SVM使用<strong class="ll ir">K<em class="mq">ernels</em>’</strong>将非线性数据映射到更高维度，使其成为线性数据，并找到最佳超平面。</p><p id="802e" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">事实上，SVM总是使用核函数。不管它是线性的还是非线性的数据，但是当数据不是如图1所示的直线形式时，它的主要功能就开始发挥作用了。核函数为分类问题增加了维度。</p><p id="96b4" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">图3示出了非线性情况下的SVM超平面的简化图示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/daaba1b0330c2c69b7afe3cd7d1970ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z7fd7FMYNj6JaMOsnCKt4g.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">图3</p></figure></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="df81" class="kr ks iq bd kt ku mz kw kx ky na la lb jw nb jx ld jz nc ka lf kc nd kd lh li bi translated">核</h1><p id="03ad" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">有几个核心选择，我们必须协助我们与SVM。我发现你需要尝试一下，看看最适合你的数据。最流行的是<strong class="ll ir">高斯</strong>内核。<a class="ae ne" href="http://pages.stat.wisc.edu/~mchung/teaching/MIA/reading/diffusion.gaussian.kernel.pdf.pdf" rel="noopener ugc nofollow" target="_blank">这里</a>是对这个内核的详细解释。我们选择的参数决定了高斯曲线的形状。我不会去解释数学，因为我们有图书馆为我们做这些。可以说，你可以看到图3是图4的俯视图。曲线的<strong class="ll ir">形状</strong>取决于我们接下来将讨论的<strong class="ll ir">参数</strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/87052a67879daad1e956ec415e9b2c72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CWgH3Ia4hOk2uQxT1s_Jgg.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">图4</p></figure></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><h1 id="19aa" class="kr ks iq bd kt ku mz kw kx ky na la lb jw nb jx ld jz nc ka lf kc nd kd lh li bi translated">重要参数</h1><p id="18a7" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated"><strong class="ll ir">“C”</strong>控制平滑决策边界和正确分类训练点之间的权衡。此参数表示SVM错误分类的错误惩罚。它保持了更平滑的超平面和错误分类之间的折衷。</p><ul class=""><li id="198a" class="nf ng iq ll b lm mf lp mg ls nh lw ni ma nj me nk nl nm nn bi translated">一个<strong class="ll ir">大的</strong>值意味着你会得到更多正确的训练点，因为你会有一个较低的偏差和一个较高的方差。不要把它做得太小，否则你的数据会不足。</li><li id="b821" class="nf ng iq ll b lm no lp np ls nq lw nr ma ns me nk nl nm nn bi translated">小的<strong class="ll ir">值意味着较高的偏差和较低的方差。不要把它做得太小，否则你会使你的数据过多。</strong></li></ul><p id="ff18" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated"><strong class="ll ir">【Gamma】</strong><em class="mq">方差</em>决定高斯核的<em class="mq">宽度</em>。在统计学中，当我们考虑高斯概率密度函数时，它被称为<em class="mq">标准差</em>，它的平方s2是<em class="mq">方差</em>。</p><h1 id="0b9a" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">我们何时使用SVM、逻辑回归或神经网络？</h1><p id="82ca" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">如果n =特征的数量，而m=训练集的行数，则可以将此作为指导:</p><ul class=""><li id="538b" class="nf ng iq ll b lm mf lp mg ls nh lw ni ma nj me nk nl nm nn bi translated">n =大(&gt; 1000)，m =小(&lt;1000)<br/> &gt;)使用无内核的逻辑回归或SVM</li><li id="0972" class="nf ng iq ll b lm no lp np ls nq lw nr ma ns me nk nl nm nn bi translated">n =小(&lt;1000), m=small (&lt;10 000)<br/>&gt;)SVM与高斯核</li><li id="fa30" class="nf ng iq ll b lm no lp np ls nq lw nr ma ns me nk nl nm nn bi translated">n =小(&lt;1000), m=small (&gt; 50 000) <br/> &gt;添加特性<br/> &gt;物流回归或SVM无内核</li></ul><p id="3575" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">神经网络在上述所有情况下都能很好地工作，但是训练起来比较慢。</p><h1 id="58d7" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">让我们在matlab中编码</h1><p id="81cc" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">现在，我们有了基础，让我们开始编码，这样我们就可以看到它的工作。在开始之前，<a class="ae ne" href="https://www.kaggle.com/priyanka841/breast-cancer-diagnostics-prediction" rel="noopener ugc nofollow" target="_blank">从kaggle下载</a>数据集。了解数据，你会发现第一列是不需要的，而我们的分类在第二列。因此，我们的特征是第3列到第31列。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/7d4fb5ea0c721359f780ccefbc59b67c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Td14Gg8rR_8ddeVIPhD9ZA.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">图5: <a class="ae ne" href="https://www.kaggle.com/priyanka841/breast-cancer-diagnostics-prediction" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="212e" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">我们将从加载数据开始，然后归一化特征。</p><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="0d45" class="nz ks iq nv b gy oa ob l oc od">%% setup X &amp; Y<br/>clear;<br/>tbl = readtable(‘breast cancer.csv’);<br/>[m,n] = size(tbl);<br/>X = zeros(m,n-2);<br/>X(:,1:30) = tbl{:,3:32};<br/>[y,labels] = grp2idx(tbl{:,2});</span><span id="9e52" class="nz ks iq nv b gy oe ob l oc od">[X_norm, mu, sigma] = featureNormalize(X);</span></pre><p id="46e5" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">接下来，我们可以创建我们的训练和测试数据集。我们还将创建“cv ”,它将把训练集分成一个训练集和一个交叉验证集。“cv”将在SVM期间使用。</p><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="fa7c" class="nz ks iq nv b gy oa ob l oc od">%% split up train, cross validation and test set<br/>rand_num = randperm(size(X,1));<br/>X_train = X(rand_num(1:round(0.8*length(rand_num))),:);<br/>y_train = y(rand_num(1:round(0.8*length(rand_num))),:);</span><span id="e3e8" class="nz ks iq nv b gy oe ob l oc od">X_test = X(rand_num(round(0.8*length(rand_num))+1:end),:);<br/>y_test = y(rand_num(round(0.8*length(rand_num))+1:end),:);</span><span id="b849" class="nz ks iq nv b gy oe ob l oc od">cv = cvpartition(y_train,’k’,5);</span></pre><p id="8052" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">在运行SVM之前，让sequencialfs告诉我们哪些是最好的特性</p><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="3a5f" class="nz ks iq nv b gy oa ob l oc od">%% feature selection<br/>opts = statset(‘display’,’iter’);</span><span id="0389" class="nz ks iq nv b gy oe ob l oc od">% create an inline function which will perform SVM on each column<br/>classf = @(train_data, train_labels, test_data, test_labels)…<br/>  sum(predict(fitcsvm(train_data, train_labels,’KernelFunction’,’rbf’), test_data) ~= test_labels);</span><span id="657f" class="nz ks iq nv b gy oe ob l oc od">% fs will hold the most appropriate features<br/>[fs, history] = sequentialfs(classf, X_train, y_train, ‘cv’, cv, ‘options’, opts);</span></pre><p id="b060" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">现在，我们可以删除不适合假设的特征</p><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="f7f5" class="nz ks iq nv b gy oa ob l oc od">%% retain columns from feature selection<br/>X_train = X_train(:,fs);<br/>X_test = X_test(:,fs);</span></pre><p id="435c" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">最后，运行SVM并使用测试数据集来检查准确性</p><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="fe20" class="nz ks iq nv b gy oa ob l oc od">%% predict, try linear, gaussian, rbf<br/>mdl = fitcsvm(X_train,y_train,’KernelFunction’,’rbf’,...<br/>  ’OptimizeHyperparameters’,’auto’,...  <br/>  ‘HyperparameterOptimizationOptions’,...<br/>  struct(‘AcquisitionFunctionName’,...<br/>  ‘expected-improvement-plus’,’ShowPlots’,true));<br/>pred = predict(mdl,X_test);<br/>accuracy = (mean(double(pred == y_test)) * 100)</span></pre><p id="d069" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">您最终应该得到98.24%的准确度，而结果图应该如下所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/04d951ea06e9cd2d5291f136d6e9cf95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-bbsgkT0rWdYveDZurUc1g.png"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">图7:来自Matlab</p></figure><p id="16df" class="pw-post-body-paragraph lj lk iq ll b lm mf jr lo lp mg ju lr ls mh lu lv lw mi ly lz ma mj mc md me ij bi translated">最后，在我们结束之前，这里是特征归一化函数</p><pre class="kg kh ki kj gt nu nv nw nx aw ny bi"><span id="5d2a" class="nz ks iq nv b gy oa ob l oc od">function [X_norm, mu, sigma] = featureNormalize(X)<br/>  mu = mean(X);<br/>  X_norm = bsxfun(@minus, X, mu);<br/>  sigma = std(X_norm);<br/>  X_norm = bsxfun(@rdivide, X_norm, sigma);<br/>end</span></pre><h1 id="04bf" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">结论</h1><p id="f0a3" class="pw-post-body-paragraph lj lk iq ll b lm ln jr lo lp lq ju lr ls lt lu lv lw lx ly lz ma mb mc md me ij bi translated">对数据集进行分类时，SVM是一个不错的选择。了解它何时是优于逻辑回归的好选择，以及何时使用高斯核或在没有核的情况下运行它是很有用的。</p><h1 id="b674" class="kr ks iq bd kt ku kv kw kx ky kz la lb jw lc jx ld jz le ka lf kc lg kd lh li bi translated">来源</h1><ul class=""><li id="403a" class="nf ng iq ll b lm ln lp lq ls og lw oh ma oi me nk nl nm nn bi translated">图1-6是在www.desmos.com建造的</li></ul></div></div>    
</body>
</html>