<html>
<head>
<title>Deep Q-Network, with PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度Q网，带PyTorch</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-q-network-with-pytorch-146bfa939dfe?source=collection_archive---------9-----------------------#2021-07-12">https://towardsdatascience.com/deep-q-network-with-pytorch-146bfa939dfe?source=collection_archive---------9-----------------------#2021-07-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4443" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">解释无模型RL算法的基础:深度Q-网络模型(带代码！)</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7df7fcdc97db3875767baf90b4f7bd18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZfRP-47wuVJgtrr4XyHlgg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Mathias P.R. Reding 在<a class="ae ky" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="fc86" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在Q学习中，我们将Q值表示为一个表格。然而，在许多现实世界的问题中，存在巨大的状态和/或动作空间，并且表格表示是不够的。例如，电脑围棋有10个⁷⁰状态，像马里奥兄弟这样的游戏有连续的状态空间。当不可能在二维数组或Q表中存储状态和动作对值的所有可能组合时，我们需要使用深度Q网络(DQN)来代替Q学习算法。[1]</p><p id="8737" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">DQN也是一种无模型的RL算法，其中使用了现代深度学习技术。DQN算法使用Q学习来学习在给定状态下采取的最佳动作，并使用深度神经网络或卷积神经网络来估计Q值函数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/4b1e2e924dc2ae54a53782a5207b0b98.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*xhs-YKVEDtv0UDft8w3Gng.png"/></div></figure><h1 id="027f" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">DQN建筑的插图</h1><blockquote class="mo mp mq"><p id="41a0" class="kz la mr lb b lc ld ju le lf lg jx lh ms lj lk ll mt ln lo lp mu lr ls lt lu im bi translated">神经网络的输入包括一个84 x 84 x 4的图像，随后是3个卷积层和2个完全连接的层，它们为每个有效动作输出一个输出。[1]</p></blockquote><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mv"><img src="../Images/f5c3906064cb068feb4a13d06eb21899.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SR80h8psMzkhInOri6vnLw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">DQN建筑的插图[1]</p></figure><h1 id="6f54" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">DQN算法</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mw"><img src="../Images/2d2211499591f48adf1f6f22b05a0b7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xq6RioWPiUo3VppN_tb0OQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">DQN算法[1]</p></figure><h2 id="b244" class="mx lx it bd ly my mz dn mc na nb dp mg li nc nd mi lm ne nf mk lq ng nh mm ni bi translated">DQN的主要成分— 1。q值函数</h2><p id="6a3b" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">在DQN，我们用权重w来表示价值函数，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/4af6de32ceb19a22b987c007b2360c4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8eFyTntu0CPTobmBtV6tkA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">q值函数。作者图片来源于[1]。</p></figure><ul class=""><li id="947b" class="np nq it lb b lc ld lf lg li nr lm ns lq nt lu nu nv nw nx bi translated">在选择行动时，Q网络的工作方式类似于Q学习中的Q表。Q-learning中的状态是可数的和有限的，而DQN中的状态可以是有限的或无限的/连续的或离散的。</li><li id="db48" class="np nq it lb b lc ny lf nz li oa lm ob lq oc lu nu nv nw nx bi translated">Q网络中的更新是通过更新权重来完成的。</li></ul><h2 id="9e20" class="mx lx it bd ly my mz dn mc na nb dp mg li nc nd mi lm ne nf mk lq ng nh mm ni bi translated">DQN的主要成分— 2。损失函数</h2><p id="78bd" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">让我们用Q值的均方误差来定义目标函数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/bca33ab6a0357334a47a619b5a77b85e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uxDRB4jZxBxW3eYROtcA_g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">损失函数。作者图片来源于[1]。</p></figure><p id="e102" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个损失函数，用于最小化在Q网络中更新权重的误差。</p><h2 id="dd5b" class="mx lx it bd ly my mz dn mc na nb dp mg li nc nd mi lm ne nf mk lq ng nh mm ni bi translated">DQN的主要成分— 3。优化算法</h2><p id="297d" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">让我们用随机梯度来优化上面的目标函数，用<strong class="lb iu"> δL(w)/δw </strong>。Tensorflow或PyTorch中有许多可用的优化算法。比如亚当，RMSProp，阿达格拉德等。</p><h2 id="2ed6" class="mx lx it bd ly my mz dn mc na nb dp mg li nc nd mi lm ne nf mk lq ng nh mm ni bi translated">DQN的主要成分— 4。体验回放</h2><blockquote class="mo mp mq"><p id="a67a" class="kz la mr lb b lc ld ju le lf lg jx lh ms lj lk ll mt ln lo lp mu lr ls lt lu im bi translated"><strong class="lb iu">朴素Q学习用神经网络振荡或发散。</strong></p><p id="ab6a" class="kz la mr lb b lc ld ju le lf lg jx lh ms lj lk ll mt ln lo lp mu lr ls lt lu im bi translated">数据是连续的，这意味着连续的样本是相关的，而不是独立的和同分布的。</p><p id="c2b4" class="kz la mr lb b lc ld ju le lf lg jx lh ms lj lk ll mt ln lo lp mu lr ls lt lu im bi translated">政策会随着Q值的轻微变化而快速变化或振荡，因此，数据的分布会从一个极端转移到另一个极端。</p><p id="cf07" class="kz la mr lb b lc ld ju le lf lg jx lh ms lj lk ll mt ln lo lp mu lr ls lt lu im bi translated">奖励和Q值的比例是未知的。当反向传播时，朴素Q学习的梯度可能很不稳定。[2]</p></blockquote><p id="6672" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了解决上述问题，我们可以将转换存储在重放缓冲器中，并从重放缓冲器中采样一小批经验来更新Q网络。从中。通过使用经验重放，它将打破样本之间的连续相关性，并且还允许网络更好地利用经验。[1]</p></div><div class="ab cl oe of hx og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="im in io ip iq"><p id="bd99" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面的教程将使用DQN来解决来自<a class="ae ky" href="https://gym.openai.com/" rel="noopener ugc nofollow" target="_blank">健身房</a>环境的雅达利游戏。对于所有Atari游戏，它接受84x84x4图像帧作为输入。因此，首先，我们需要进行数据预处理，即跳过连续样本之间的一些帧，因为这些连续样本几乎是相同的，重新缩放图像，灰度化图像，归一化图像等。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol om l"/></div></figure><p id="b6ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">DQN解算器将使用3层卷积神经网络来构建<strong class="lb iu"> Q网络</strong>。然后，它将使用<strong class="lb iu">优化器</strong>(下面代码中的Adam)和<strong class="lb iu">经验重放</strong>来<strong class="lb iu">最小化误差</strong>以更新Q网络中的权重。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol om l"/></div></figure><p id="a10a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们可以用<code class="fe on oo op oq b">run(training_mode=True, pretrained=False)</code>进行训练，用<code class="fe on oo op oq b">run(training_mode=False, pretrained=True, num_episodes=1, exploration_max=0.05)</code>进行测试。</p><ul class=""><li id="029e" class="np nq it lb b lc ld lf lg li nr lm ns lq nt lu nu nv nw nx bi translated">第一次训练网络时，可以使用<code class="fe on oo op oq b">pretrained=False</code>，否则可以使用<code class="fe on oo op oq b">pretrained=True</code>从上次停止的地方继续训练。</li><li id="89bd" class="np nq it lb b lc ny lf nz li oa lm ob lq oc lu nu nv nw nx bi translated">然而，对于测试，你只能使用<code class="fe on oo op oq b">pretrained=True</code>，这意味着你使用训练好的Q-网络来测试代理的性能。</li><li id="fa4d" class="np nq it lb b lc ny lf nz li oa lm ob lq oc lu nu nv nw nx bi translated"><code class="fe on oo op oq b">exploration_max=1</code>在训练过程中，因为我们希望agent以ε概率随机进行探索，这样就不会陷入局部极小值。然而，在测试过程中，我们希望代理从经过训练的Q网络中采取优化的行动，因此，我们应该使用0或非常少的探索<code class="fe on oo op oq b">exploration_max=0.05.</code></li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ol om l"/></div></figure></div><div class="ab cl oe of hx og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="im in io ip iq"><h1 id="6cec" class="lw lx it bd ly lz or mb mc md os mf mg jz ot ka mi kc ou kd mk kf ov kg mm mn bi translated">推荐阅读</h1><div class="ow ox gp gr oy oz"><a rel="noopener follow" target="_blank" href="/q-learning-and-sasar-with-python-3775f86bd178"><div class="pa ab fo"><div class="pb ab pc cl cj pd"><h2 class="bd iu gy z fp pe fr fs pf fu fw is bi translated">Q-Learning和SASAR，使用Python</h2><div class="pg l"><h3 class="bd b gy z fp pe fr fs pf fu fw dk translated">解释无模型RL算法的基础:Q-Learning和SARSA(带代码！)</h3></div><div class="ph l"><p class="bd b dl z fp pe fr fs pf fu fw dk translated">towardsdatascience.com</p></div></div><div class="pi l"><div class="pj l pk pl pm pi pn ks oz"/></div></div></a></div></div><div class="ab cl oe of hx og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="im in io ip iq"><h1 id="524e" class="lw lx it bd ly lz or mb mc md os mf mg jz ot ka mi kc ou kd mk kf ov kg mm mn bi translated">参考</h1><p id="bace" class="pw-post-body-paragraph kz la it lb b lc nj ju le lf nk jx lh li nl lk ll lm nm lo lp lq nn ls lt lu im bi translated">[1] V. Mnih <em class="mr"> et al. </em>，<a class="ae ky" href="https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf" rel="noopener ugc nofollow" target="_blank">通过深度强化学习的人级控制。</a></p><p id="11a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2]华盛顿大学CSE571讲座—“<a class="ae ky" href="https://courses.cs.washington.edu/courses/cse571/16au/slides/20-rl-silver.pdf" rel="noopener ugc nofollow" target="_blank">概述深度学习强化学习深度价值功能深度政策深度模型。</a>”</p></div></div>    
</body>
</html>