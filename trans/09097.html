<html>
<head>
<title>Predicting individual survival curves with Keras</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Keras预测个体生存曲线</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/predicting-individual-survival-curves-with-keras-abb1f1f051f?source=collection_archive---------27-----------------------#2021-08-22">https://towardsdatascience.com/predicting-individual-survival-curves-with-keras-abb1f1f051f?source=collection_archive---------27-----------------------#2021-08-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="86a6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用于客户终身价值模型的Kaplan-Meier估计量的深度学习适应</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/358f4d52124463b183015a15f69e87c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*2Cxv2Pqnez2kp8sn"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">迈克尔·朗米尔在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="ac7e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">TL；灾难恢复生存分析模型广泛应用于从医学到电子商务的不同领域。人们越来越关注如何开发个体生存函数，而不是群体生存函数，主要是通过使用深度学习框架。这篇文章介绍了对人口生存分析最常见的非参数方法之一的深度学习改编，<a class="ae kv" href="https://en.wikipedia.org/wiki/Kaplan%E2%80%93Meier_estimator" rel="noopener ugc nofollow" target="_blank">卡普兰-迈耶估计器</a>。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="088c" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated"><strong class="ak">简介</strong></h2><p id="4b9a" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">在研究和工业中，对预测个体生存函数，即任何给定时间的生存概率函数的兴趣越来越大。这项任务的大多数现有方法要么是参数化的，要么是半参数化的，而很少是严格非参数化的。</p><p id="d032" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一些基于深度学习的最流行模型的<a class="ae kv" href="https://github.com/pytorch/pytorch" rel="noopener ugc nofollow" target="_blank"> PyTorch </a>实现可以在<a class="ae kv" href="https://github.com/havakv/pycox" rel="noopener ugc nofollow" target="_blank"> pycox </a>库中找到，而<a class="ae kv" href="https://github.com/sebp/scikit-survival" rel="noopener ugc nofollow" target="_blank"> scikit-survival </a>和<a class="ae kv" href="https://github.com/dmlc/xgboost" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>为Survival regression提供了其他机器学习替代方案，如随机森林和梯度增强。</p><p id="d97c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们介绍了一种最广为人知的非参数生存分析方法的改进，即<a class="ae kv" href="https://en.wikipedia.org/wiki/Kaplan%E2%80%93Meier_estimator" rel="noopener ugc nofollow" target="_blank"> Kaplan-Meier估计量</a>，用于预测个体生存函数。我们通过深度学习变异的<a class="ae kv" href="https://papers.nips.cc/paper/2011/file/1019c8091693ef5c5f55970346633f92-Paper.pdf" rel="noopener ugc nofollow" target="_blank">多任务逻辑回归</a> (MTLR)和<a class="ae kv" href="https://arxiv.org/pdf/1801.05512.pdf" rel="noopener ugc nofollow" target="_blank"> N-MTLR </a>来实现这一点。我们的模型的主要区别在于，使用样本权重处理删失数据，并且模型在每个时间段的输出是前一个时间段的输出和sigmoid层的乘积。</p><h2 id="dc56" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated"><strong class="ak">卡普兰-迈耶估计值</strong></h2><p id="673a" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">设S(t)是生存<strong class="ky ir">至少</strong> t个时间单位的概率，即生存函数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/b4b6d852463d332bd2d4df0774bc26b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*w5OLk0dK0_GrRUsf9CGwgg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://en.wikipedia.org/wiki/Kaplan%E2%80%93Meier_estimator" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Kaplan % E2 % 80% 93 Meier _ estimator</a></p></figure><p id="9f55" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据条件概率，它也是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi my"><img src="../Images/5e137e548e85eda9ff6b992efa272ffa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6s9qjdEUO5XCPlBHsCw__A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://en.wikipedia.org/wiki/Kaplan%E2%80%93Meier_estimator" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Kaplan % E2 % 80% 93 Meier _ estimator</a></p></figure><p id="ba06" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在哪里</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/6817e3700d249ed9586d75ff01c0cc8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*SRZtd3vJY5uU7nwHs0kDjQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://en.wikipedia.org/wiki/Kaplan%E2%80%93Meier_estimator" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Kaplan % E2 % 80% 93 Meier _ estimator</a></p></figure><p id="c4b4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其估计量由下式给出:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi na"><img src="../Images/94cf56dfc23b84d801ae535ed407529f.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/format:webp/1*VBHvyFQT2bSKULhkQFpa5w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://en.wikipedia.org/wiki/Kaplan%E2%80%93Meier_estimator" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Kaplan % E2 % 80% 93 Meier _ estimator</a></p></figure><p id="0dba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">换句话说，t时刻的KM估计量等于t-1时刻的KM估计量乘以t时刻未死亡的个体在已知存活到t时刻的个体中所占的比例。</p><h2 id="f02b" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">深度学习适应</h2><p id="8e79" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">我们的方法很简单:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/7d88c7d32b7cc13e983233497d024402.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*A2rhMC5yV_j9NZ6MMFM7RA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://en.wikipedia.org/wiki/Kaplan%E2%80%93Meier_estimator" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/Kaplan % E2 % 80% 93 Meier _ estimator</a></p></figure><ol class=""><li id="d897" class="nc nd iq ky b kz la lc ld lf ne lj nf ln ng lr nh ni nj nk bi translated">我们用多输出前馈神经网络来表示上述递归，其中每个输出是前一个输出乘以表示概率q(t)的sigmoid层。</li><li id="ea3d" class="nc nd iq ky b kz nl lc nm lf nn lj no ln np lr nh ni nj nk bi translated">使用样本权重处理删失数据:对于每个输出t，如果个体的开始日期至少在t个时间段之前，则样本权重为1，否则为0。</li></ol><p id="56f8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">代码看起来怎么样？</p><pre class="kg kh ki kj gt nq nr ns nt aw nu bi"><span id="92db" class="lz ma iq nr b gy nv nw l nx ny">def build_model(<br/>    self,<br/>    input_shape: int,<br/>    hidden_units: List[int],<br/>    dropout: Optional[float] = None,<br/>    activation: Optional[str] = None,<br/>    kernel_regularizer: Optional[str] = None,<br/>    kernel_constraint: bool = False,<br/>    noise: Optional[float] = None,<br/>    normalization: bool = False,<br/>):<br/>    K.clear_session()<br/>    inputs = Input(shape=(input_shape,))<br/>    x = inputs<br/>    for units in hidden_units:<br/>        x = Dense(<br/>            units,<br/>            activation=activation,<br/>            kernel_regularizer=kernel_regularizer,<br/>            kernel_constraint=UnitNorm() if kernel_constraint else None,<br/>        )(x)<br/>        x = GaussianNoise(noise)(x) if noise else x<br/>        x = BatchNormalization()(x) if normalization else x<br/>        x = Dropout(dropout)(x) if dropout else x<br/>    outputs = []<br/>    for period in range(self._periods):<br/>        if period == 0:<br/>            o = Dense(<br/>                1,<br/>                activation="sigmoid",<br/>                kernel_regularizer=kernel_regularizer,<br/>                kernel_constraint=UnitNorm() if kernel_constraint else None,<br/>            )(x)<br/>            outputs.append(o)<br/>            continue<br/>        o = Dense(<br/>            1,<br/>            activation="sigmoid",<br/>            kernel_regularizer=kernel_regularizer,<br/>            kernel_constraint=UnitNorm() if kernel_constraint else None,<br/>        )(x)<br/>        o = Multiply()([o, outputs[period - 1]])<br/>        outputs.append(o)<br/>    self.model = tf.keras.Model(inputs=inputs, outputs=outputs)</span><span id="dec6" class="lz ma iq nr b gy nz nw l nx ny">def fit(<br/>    self,<br/>    X_train: np.ndarray,<br/>    y_train: List[np.ndarray],<br/>    w_train: List[np.ndarray],<br/>    validation_data: Tuple[np.ndarray, List[np.ndarray], List[np.ndarray]],<br/>    epochs: Optional[int] = 100,<br/>    batch_size: Optional[int] = 256,<br/>    patience: Optional[int] = 10,<br/>):<br/>    self.model.compile(optimizer="Adam", loss="binary_crossentropy")<br/>    callback = tf.keras.callbacks.EarlyStopping(<br/>        monitor="val_loss", patience=patience<br/>    )<br/>    self.model.fit(<br/>        X_train,<br/>        y_train,<br/>        sample_weight=w_train,<br/>        epochs=epochs,<br/>        batch_size=batch_size,<br/>        validation_data=validation_data,<br/>        callbacks=[callback],<br/>    )</span></pre><p id="c596" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当然，架构只是一个参考。如上所述，应将X连同shape (n_samples，n_features)、y (n_samples，periods)和w (n_samples，periods)传递给此模型的拟合方法。</p><h2 id="c95b" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">摘要</h2><p id="dcf9" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">有几种方法来拟合生存回归模型，每种方法都有其优点和缺点。在这篇文章中，我提出了一个非常简单的方法，利用<a class="ae kv" href="https://github.com/tensorflow/tensorflow" rel="noopener ugc nofollow" target="_blank"> Tensorflow </a>的灵活性，使用前馈神经网络来生成个体生存曲线，而不依赖于强假设，这在概念上是对最常见的生存分析模型之一的改编:Kaplan-Meier估计量。</p></div></div>    
</body>
</html>