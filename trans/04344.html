<html>
<head>
<title>How to Use Pairwise Correlation For Robust Feature Selection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用成对相关性进行鲁棒的特征选择</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-use-pairwise-correlation-for-robust-feature-selection-20a60ef7d10?source=collection_archive---------3-----------------------#2021-04-13">https://towardsdatascience.com/how-to-use-pairwise-correlation-for-robust-feature-selection-20a60ef7d10?source=collection_archive---------3-----------------------#2021-04-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0fbf" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">将久经考验的方法添加到您的武器库中</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/849282d44cc3474fe7bc958e447de0e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M3lj-phPy0NmmO3TG6Zrvg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">照片由</strong> <a class="ae kz" href="https://www.pexels.com/@pixabay?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> <strong class="bd ky"> Pixabay </strong> </a> <strong class="bd ky">上</strong> <a class="ae kz" href="https://www.pexels.com/photo/2-grey-and-black-birds-45853/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> <strong class="bd ky">像素</strong> </a></p></figure><h2 id="b43f" class="la lb it bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">相关系数是多少？</h2><p id="bd2f" class="pw-post-body-paragraph lw lx it ly b lz ma ju mb mc md jx me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">在我上一篇关于特性选择的文章中，我们关注了一种基于特性的移除技术。在本帖中，我们将探讨一种更可靠、更稳健的方法，让我们看到特性之间的联系，并决定它们是否值得保留。正如你从标题中所看到的，这种方法使用成对相关。</p><p id="b88a" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">首先，让我们简单的接触一下皮尔逊相关系数——通常表示为<em class="mu"> r </em>。该系数可用于量化单个指标中两个分布(或特征)之间的线性关系。它的范围从-1到1，-1是完全负相关，+1是完全正相关。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/98a0cecb83d13e0d147e273f1cd96464.png" data-original-src="https://miro.medium.com/v2/resize:fit:716/format:webp/0*o45Ecs1w7QC3WeY-.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="a162" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">例如，身体测量通常有很强的正相关性。你可以预期高个子会有更长的胳膊和腿，或者宽肩膀的人会更重。还有，冰淇淋的销量和温度负相关，或者跑得越远越慢等等。</p><blockquote class="mw mx my"><p id="e77e" class="lw lx mu ly b lz mp ju mb mc mq jx me mz mr mg mh na ms mj mk nb mt mm mn mo im bi translated">关于如何使用、解释和理解相关系数的深入指导，请参考我的另一篇文章<a class="ae kz" href="https://towardsdev.com/how-to-not-misunderstand-correlation-75ce9b0289e" rel="noopener ugc nofollow" target="_blank">。</a></p></blockquote><p id="6168" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">但是相关系数和机器学习或者特征选择有什么关系呢？嗯，假设两个特征之间的相关系数是0.85。这意味着在85%的情况下，您可以仅使用特性1的值来预测特性2。换句话说，如果数据集中有要素1，则要素2不会带来太多新信息。这就是为什么保留特征2没有意义，因为它只会在训练模型时增加复杂性。</p><p id="1a26" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">使用成对相关进行要素选择就是这么回事-识别高度相关的要素组并仅保留其中一个要素，以便您的模型可以使用尽可能少的要素获得尽可能大的预测能力。</p><div class="nc nd gp gr ne nf"><a href="https://ibexorigin.medium.com/membership" rel="noopener follow" target="_blank"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd iu gy z fp nk fr fs nl fu fw is bi translated">通过我的推荐链接加入Medium-BEXGBoost</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">获得独家访问我的所有⚡premium⚡内容和所有媒体没有限制。支持我的工作，给我买一个…</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">ibexorigin.medium.com</p></div></div><div class="no l"><div class="np l nq nr ns no nt ks nf"/></div></div></a></div><p id="d48c" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">获得由强大的AI-Alpha信号选择和总结的最佳和最新的ML和AI论文:</p><div class="nc nd gp gr ne nf"><a href="https://alphasignal.ai/?referrer=Bex" rel="noopener  ugc nofollow" target="_blank"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd iu gy z fp nk fr fs nl fu fw is bi translated">阿尔法信号|机器学习的极品。艾总结的。</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">留在循环中，不用花无数时间浏览下一个突破；我们的算法识别…</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">alphasignal.ai</p></div></div><div class="no l"><div class="nu l nq nr ns no nt ks nf"/></div></div></a></div></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="3c4a" class="la lb it bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">绘制完美的相关矩阵</h2><p id="fef5" class="pw-post-body-paragraph lw lx it ly b lz ma ju mb mc md jx me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">识别高度相关特征的最快且通常是最好的方法是使用相关矩阵。该矩阵显示了数据集中每一对数值要素之间的相关性。</p><p id="18e1" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">让我们来看一个使用墨尔本住房数据集的例子，该数据集包含13个数字特征:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl oe"><img src="../Images/49a295cc4bccc99687993f78353f223b.png" data-original-src="https://miro.medium.com/v2/format:webp/1*2WKJXtzLYl5yA51RtG6q8A.png"/></div></figure><p id="fc17" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">计算矩阵很容易通过调用DataFrame上的<code class="fe of og oh oi b">.corr()</code>方法来完成。接下来，我们将它传递给Seaborn的热图函数，通常这样做是为了生成漂亮的视觉效果:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl oe"><img src="../Images/ee8bda1680e2c79705a26d4984b821fe.png" data-original-src="https://miro.medium.com/v2/format:webp/1*MD8avyXKKXrNXm8pU5MsSw.png"/></div></figure><p id="929c" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">是的，默认热图没有那么好。让我们给它补个妆:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl oe"><img src="../Images/26558b287fe5da48d6fa316377751305.png" data-original-src="https://miro.medium.com/v2/format:webp/1*r1NQAf3CW6zUc3MNOIjhrw.png"/></div></figure><p id="ffd4" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">首先，我们创建一个定制的发散调色板(蓝色-&gt;白色-&gt;红色)。然后，我们将颜色条以0为中心，使注释能够看到每个相关性，并使用2个小数点。</p><p id="b346" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">如您所见，矩阵将每对之间的相关性显示两次，因为A和B之间的相关性与B和A之间的相关性相同。此外，对角线由1组成，表示特征与其自身的相关性。所有这些额外的信息会让人分心。这就是为什么我们要去掉矩阵的上半部分:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl oe"><img src="../Images/f3fae608abf139e3e9c850b7489ed003.png" data-original-src="https://miro.medium.com/v2/format:webp/1*ISGtKFbGQdVV2tgLsnQbcw.png"/></div></figure><p id="7c37" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">在上面代码片段的第4行，我们创建了一个2D布尔掩码。首先，<code class="fe of og oh oi b">np.ones_like</code>创建一个2D numpy数组，其形状与填充了<code class="fe of og oh oi b">True</code>值的相关矩阵相同。然后，将这个数组传递给<code class="fe of og oh oi b">np.triu</code>，将它转换成适合我们需要的布尔掩码。我们只是将这个面具传递给<code class="fe of og oh oi b">heatmap</code>的<code class="fe of og oh oi b">mask</code>论证，这给了我们上面美丽的情节。</p><p id="5271" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">下面是一个绘制这些完美相关矩阵的小函数:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="f2bb" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">即使我们可以通过编程找到所有高度相关的特性，我们仍然应该直观地探索它们。因为像上面这样的热图有助于我们了解相关性是否有意义。例如，新生婴儿的数量可能与附近鹳的数量高度相关，尽管这没有任何意义。这就是为什么要进行视觉探索，以查看各组特征是否实际上相互关联，以及寻找它们之间的联系是否有意义。</p></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="9f36" class="la lb it bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">如何移除高度相关的特征</h2><p id="3c8f" class="pw-post-body-paragraph lw lx it ly b lz ma ju mb mc md jx me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">现在，让我们看看我们如何实际删除高度相关的功能。这次我们将使用不同的数据集— <a class="ae kz" href="https://www.kaggle.com/seshadrikolluri/ansur-ii" rel="noopener ugc nofollow" target="_blank"> Ansur数据集</a>。它包含了6000名美国陆军人员的所有可以想象的身体尺寸，还包含了98个数字特征。这是一个练习要素选择技能的绝佳数据集，因为该数据集中有许多相关的要素:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl oe"><img src="../Images/3fafe01083ef8cc810021c8c99e4efdf.png" data-original-src="https://miro.medium.com/v2/format:webp/1*1IPsMf3j8X6dHBcIm5YwHg.png"/></div></figure><p id="04b5" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">我们将只处理数字特征，因此，我们将隔离它们:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><blockquote class="mw mx my"><p id="622e" class="lw lx mu ly b lz mp ju mb mc mq jx me mz mr mg mh na ms mj mk nb mt mm mn mo im bi translated">顺便说一句，试图用这么多特征来绘制数据集的相关矩阵是没有用的。</p></blockquote><p id="8b43" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">我们的目标是消除强烈的相关性，无论是负面的还是正面的。因此，我们这次将通过在<code class="fe of og oh oi b">.corr()</code>上链接<code class="fe of og oh oi b">.abs()</code>方法来构建具有相关性绝对值的矩阵:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="437f" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">接下来，我们再次创建一个布尔掩码，用于矩阵的子集化:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="3912" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">使用熊猫数据帧的<code class="fe of og oh oi b">mask</code>方法(相关矩阵是一个数据帧)将NaN值放在矩阵的上半部分和对角线上:</p><pre class="kj kk kl km gt oj oi ok ol aw om bi"><span id="02a0" class="la lb it oi b gy on oo l op oq">&gt;&gt;&gt; reduced_matrix.iloc[:5, :5]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl oe"><img src="../Images/9562d19d3ce6a3f86e22eac2701ac820.png" data-original-src="https://miro.medium.com/v2/format:webp/1*5kFDbaHu7hz-_WATUSeNvQ.png"/></div></figure><p id="4ab7" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">接下来，我们需要设置一个阈值来决定是否删除某个特性。应该仔细选择这个阈值，并且只有在彻底了解数据集的情况下才能选择。对于我们的例子，我们将选择阈值0.9:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="82b0" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">上面的列表理解找到了满足阈值0.9的所有其他列。使用<code class="fe of og oh oi b">any</code>可以让我们不使用嵌套的for循环，就像这样:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="071b" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">列表理解无疑是一个更好的解决方案。注意，如果我们没有将<code class="fe of og oh oi b">NaN</code>放在矩阵的上半部分和对角线上，我们将丢失所有相关的特征，而不是保留其中的一个。</p><p id="2eb8" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">下面是一个函数，它对任意阈值执行上述操作，并返回要删除的列的列表:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="9a28" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">现在，让我们实际删除这些列:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="3bf6" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">现在，让我们看看放弃这么多特性是否是正确的选择。我们将通过训练两个<code class="fe of og oh oi b">RandomForestRegressor</code>来检查这一点:一个在完整数据集上，一个在缩减的、特征选择的数据集上。目标重量以磅为单位- <code class="fe of og oh oi b">Weightlbs</code>:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="db89" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">我们得到了一个非常好的结果，即使有这么多的功能。现在，让我们在特征选择的数据帧上重复上述内容:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="ab37" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">我们的测试分数只降低了2%,而运行时间却减少了两倍。</p></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><h2 id="a4c5" class="la lb it bd lc ld le dn lf lg lh dp li lj lk ll lm ln lo lp lq lr ls lt lu lv bi translated">结论</h2><p id="bad5" class="pw-post-body-paragraph lw lx it ly b lz ma ju mb mc md jx me lj mf mg mh ln mi mj mk lr ml mm mn mo im bi translated">总之，使用成对相关允许我们检测高度相关的特征，这些特征不会给数据集带来新的信息。因为这些特性只会增加模型的复杂性，增加过度拟合的机会，并且需要更多的计算，所以应该去掉它们。</p><p id="e8cf" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">然而，在使用这种技术之前，通过做<a class="ae kz" rel="noopener" target="_blank" href="/my-6-part-powerful-eda-template-that-speaks-of-ultimate-skill-6bdde3c91431">适当的EDA </a>来彻底理解你的数据集。总是寻找没有意义的相关性，例如，没有联系的随机相关特征。</p></div><div class="ab cl nv nw hx nx" role="separator"><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa ob"/><span class="ny bw bk nz oa"/></div><div class="im in io ip iq"><p id="243b" class="pw-post-body-paragraph lw lx it ly b lz mp ju mb mc mq jx me lj mr mg mh ln ms mj mk lr mt mm mn mo im bi translated">如果你不知道接下来要读什么，这里我为你挑选了一些:</p><div class="nc nd gp gr ne nf"><a rel="noopener follow" target="_blank" href="/how-to-use-variance-thresholding-for-robust-feature-selection-a4503f2b5c3f"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd iu gy z fp nk fr fs nl fu fw is bi translated">如何使用方差阈值进行鲁棒特征选择</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">即使删除50个功能，也能获得相同的性能</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">towardsdatascience.com</p></div></div><div class="no l"><div class="or l nq nr ns no nt ks nf"/></div></div></a></div><div class="nc nd gp gr ne nf"><a rel="noopener follow" target="_blank" href="/11-times-faster-hyperparameter-tuning-with-halvinggridsearch-232ed0160155"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd iu gy z fp nk fr fs nl fu fw is bi translated">HalvingGridSearch将超参数调谐速度提高了11倍</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">连续减半彻底粉碎了GridSearch和RandomSearch</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">towardsdatascience.com</p></div></div><div class="no l"><div class="os l nq nr ns no nt ks nf"/></div></div></a></div><div class="nc nd gp gr ne nf"><a rel="noopener follow" target="_blank" href="/beginners-guide-to-xgboost-for-classification-problems-50f75aac5390"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd iu gy z fp nk fr fs nl fu fw is bi translated">XGBoost分类问题入门指南</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">利用最热门的ML库实现一流的分类性能</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">towardsdatascience.com</p></div></div><div class="no l"><div class="ot l nq nr ns no nt ks nf"/></div></div></a></div><div class="nc nd gp gr ne nf"><a href="https://towardsdev.com/intro-to-object-oriented-programming-for-data-scientists-9308e6b726a2" rel="noopener  ugc nofollow" target="_blank"><div class="ng ab fo"><div class="nh ab ni cl cj nj"><h2 class="bd iu gy z fp nk fr fs nl fu fw is bi translated">面向数据科学家的面向对象编程介绍</h2><div class="nm l"><h3 class="bd b gy z fp nk fr fs nl fu fw dk translated">自己用OOP基础实现一个简单的线性回归</h3></div><div class="nn l"><p class="bd b dl z fp nk fr fs nl fu fw dk translated">towardsdev.com</p></div></div><div class="no l"><div class="ou l nq nr ns no nt ks nf"/></div></div></a></div></div></div>    
</body>
</html>