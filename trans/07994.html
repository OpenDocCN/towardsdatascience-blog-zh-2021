<html>
<head>
<title>Sentiment &amp; Engagement Analysis from your Slack data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">根据您的闲置数据进行情绪和参与度分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sentiment-engagement-analysis-from-your-slack-data-11f7ff995b62?source=collection_archive---------17-----------------------#2021-07-22">https://towardsdatascience.com/sentiment-engagement-analysis-from-your-slack-data-11f7ff995b62?source=collection_archive---------17-----------------------#2021-07-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="719d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一瞥你松弛空间的情绪</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c9746f2a7ec3f93d81a34c77f28300ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0AMVtYxjg8aEN1kI7duLow.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由Pankaj Patel从<a class="ae ky" href="https://unsplash.com/photos/OXkUz1Dp-4g" rel="noopener ugc nofollow" target="_blank">Unplash.com</a>拍摄</p></figure><p id="62cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有没有想过你发布的内容有多吸引人？它是清晰的还是令人困惑的？或者在全公司会议上人们误解了你的意思？</p><p id="5465" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">远程环境让教师和领导者很少有机会获得反馈并优化他们的内容以取得更好的绩效。</p><p id="5390" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为我职业生涯的相当一部分已经是远程完成的了(实际上是在covid时代之前！)，我发现这些问题在我渴望创造性解决方案的大脑中闪烁着兴奋和喜悦。我有数据，我所要做的就是起草我想回答的问题，然后去做。</p><p id="666d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这也是我训练营的最后一个e2e项目，我身边有主题专家作为利益相关者(我的首席老师和我的项目导师)指导我开发一个价值驱动的产品。</p><h1 id="bb5e" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">数据源</h1><p id="8720" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我的数据集包括从第一天到最后一天由Slack管理员提供的Ironhack训练营的公开对话。也可以通过Slack API来完成，但是这超出了我的项目范围。</p><p id="403c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了保持这篇博文的简洁，请注意，我重点强调了代码中令人兴奋的部分，而不是它给出的见解。</p><p id="f08d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您正在寻找:</p><ul class=""><li id="2a8c" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated">视觉效果(在画面中完成)看看我的<a class="ae ky" href="https://docs.google.com/presentation/d/1qMoWoY_3LL31Y4WvFe2di2z1FIUNOo3gU2yuRwjrI6o/edit?usp=sharing" rel="noopener ugc nofollow" target="_blank">演示</a></li><li id="efcd" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">详细代码，浏览我的GitHub repo <a class="ae ky" href="https://github.com/lillaszulyovszky/ironhack-final-project" rel="noopener ugc nofollow" target="_blank">这里</a>。</li></ul><h1 id="1f5f" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">数据清理和争论</h1><p id="74ff" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">为了表示我在JSON文件数量方面遇到的挑战，这里是通用频道的文件夹，其中包含按天细分的所有对话，如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/8393de57941317bb514afd37d91eac31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eSQMNak0-pSz7hjj61b1sQ.png"/></div></div></figure><p id="11ec" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以我首先将每个通道的JSON文件加载到一个数据帧中。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="c699" class="nm lw it ni b gy nn no l np nq"><em class="nr"># defining file path</em><br/>path_to_json = '../raw_data/general/' <br/><br/><em class="nr"># get all json files from there</em><br/>json_pattern = os.path.join(path_to_json,'*.json')<br/>file_list = glob.glob(json_pattern)<br/><br/><em class="nr"># an empty list to store the data frames</em><br/>dfs = []<br/><strong class="ni iu">for</strong> file <strong class="ni iu">in</strong> file_list:<br/>    <em class="nr"># read data frame from json file</em><br/>    data = pd.read_json(file)<br/>    <em class="nr"># append the data frame to the list</em><br/>    dfs.append(data)<br/><br/><em class="nr"># concatenate all the data frames in the list</em><br/>channel_gen = pd.concat(dfs, ignore_index=<strong class="ni iu">True</strong>)<br/><em class="nr"># test</em><br/>channel_gen.tail(100)</span></pre><p id="3e7e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后为了方便起见，将每个通道的独立数据帧合并成一个数据帧。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="f7a5" class="nm lw it ni b gy nn no l np nq"><em class="nr">#frames = [channel_gen, channel_books, channel_dmemes, channel_dresource, channel_dbootcamp, channel_funcommittee, channel_dvizbeauties, channel_frustrations, channel_finalproject, channel_frustrations, channel_funcommittee, channel_katas, channel_labhelp, channel_music, channel_random, channel_vanilla]</em><br/><br/>df = pd.concat([channel_gen, channel_books,<br/>                    channel_dmemes, channel_dresource, <br/>                    channel_dbootcamp, channel_funcommittee,<br/>                    channel_dvizbeauties, channel_frustrations, <br/>                    channel_finalproject, channel_frustrations, <br/>                    channel_funcommittee, channel_katas, <br/>                    channel_labhelp, channel_music, <br/>                    channel_random, channel_vanilla], ignore_index=<strong class="ni iu">True</strong>, join="outer")</span></pre><p id="bd4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">到目前为止，我的dataframe有5263行和13列，其中有一堆与我的项目无关的数据。清洁是艰难的。</p><p id="f023" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">清洗柱子&amp;抬杠:</strong></p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="6575" class="nm lw it ni b gy nn no l np nq">- subtype: filter out it's values from df, remove the original column\<br/>- ts: changing it to datetime, remove miliseconds, get days of the week, months of the year, type of the day, parts of the day\<br/>- user_profile: extract real_name in new column, remove the original\<br/>- attachments: extract title, text, link in new columns\<br/>- files: extract url_private and who shared\<br/>- attachments: extract title, text, link in new columns\<br/>- reactions: extract user, count, name of the emoji\</span></pre><p id="c56e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为几乎所有的数据都嵌套在JSON库中，所以我项目的大部分时间都花在了迭代特性工程任务上，以获得可以用来训练模型的变量。同时，提取数据是我最喜欢的。下面你可以看到几个函数的例子，这些函数被创建来从数据帧中获得洞察力。</p><p id="b391" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">谁发送的回复最多:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="0d03" class="nm lw it ni b gy nn no l np nq"><em class="nr">#</em> user_profile column: extract real_name</span><span id="f212" class="nm lw it ni b gy ns no l np nq">def getrealnamefromprofile(x):<br/>    """this function is applied to column user_profile<br/>    """<br/>    <br/>    if x != x:<br/>        return 'noname'<br/>    else:<br/>        return x['real_name']</span><span id="0f09" class="nm lw it ni b gy ns no l np nq">df_clean['real_name'] = df_clean['user_profile'].apply(getrealnamefromprofile)</span><span id="ebe2" class="nm lw it ni b gy ns no l np nq">df_clean</span></pre><p id="d7f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">群组中使用最多的表情符号是什么:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="72d4" class="nm lw it ni b gy nn no l np nq"><em class="nr"># reactions column:  extract frequency</em><br/><br/><strong class="ni iu">def</strong> getcountfromreactions(x):<br/>    <em class="nr">"""this function is applied to column reactions</em><br/><em class="nr">    """</em><br/>    <br/>    <strong class="ni iu">if</strong> x != x:<br/>        <strong class="ni iu">return</strong> 0<br/>    <strong class="ni iu">else</strong>:<br/>        <strong class="ni iu">return</strong> x[0]['count']<br/><br/>df_clean['reactions_count'] = df_clean['reactions'].apply(getcountfromreactions)<br/><br/>df_clean</span></pre><p id="041a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">人们在频道上分享了哪些链接:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="77fb" class="nm lw it ni b gy nn no l np nq"><em class="nr"># files column: extract link</em><br/><br/><strong class="ni iu">def</strong> geturlfromfile(x):<br/>    <em class="nr">"""this function is applied to column files</em><br/><em class="nr">    """</em><br/>    <br/>    <strong class="ni iu">if</strong> x != x:<br/>        <strong class="ni iu">return</strong> 'nofile'<br/>    <strong class="ni iu">else</strong>:<br/>        <strong class="ni iu">try</strong>:<br/>            <strong class="ni iu">return</strong> x[0]['url_private']<br/>        <strong class="ni iu">except</strong> <strong class="ni iu">KeyError</strong>:<br/>            <strong class="ni iu">return</strong> 'nolink_infiles'<br/><br/>df_clean['link_of_file'] = df_clean['files'].apply(geturlfromfile)<br/><br/>df_clean</span></pre><p id="f997" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了帮助我找到交流的来源，我创建了另一个函数来区分首席教师和助教与学生。</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="124c" class="nm lw it ni b gy nn no l np nq"><em class="nr"># create a new column with teaching and students</em><br/><strong class="ni iu">def</strong> applyFunc(s):<br/>    <strong class="ni iu">if</strong> s == 'siand the LT (she/her)':<br/>        <strong class="ni iu">return</strong> 'teacher'<br/>    <strong class="ni iu">if</strong> s ==  'Florian Titze':<br/>        <strong class="ni iu">return</strong> 'teacher'<br/>    <strong class="ni iu">if</strong> s ==  'Kosta':<br/>        <strong class="ni iu">return</strong> 'teacher'<br/>    <strong class="ni iu">else</strong>:<br/>        <strong class="ni iu">return</strong> 'student'<br/>    <strong class="ni iu">return</strong> ''<br/><br/>df_clean['participant'] = df_clean['real_name'].apply(applyFunc)<br/>df_clean['participant'].value_counts()</span></pre><p id="ef6c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，在准备模型之前，我花了一点时间带着感激的心情检查了一下我清理过的数据框:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/b3be165e218c1c2b2fd58f4fe2ee095f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lu7Pggo1UOfTwBPogLFLaQ.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/1a27e9b8535f2143f803dede154c72d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:950/format:webp/1*nvYH1f7-85PuPHGi92cW6w.png"/></div></figure><h1 id="21ee" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">自然语言处理</h1><p id="83b2" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在学习这一部分的时候，我意识到这是我想专攻的。文本分析，听起来很酷，对吧？想象一下，当机器可以在几毫秒内完成时，人们花在阅读繁琐的文本并试图用充满偏见的大脑分析它的时间有多长。让我颤抖。</p><p id="2ac1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我的范围最初还包括文本特征提取(因为这是你可以从书面交流中获得的最有价值的东西),这是我现在正在做的事情，但是，在那5天里我没有时间做这件事，这个主题也超出了训练营的范围。</p><p id="0f74" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">相反，我专注于获得每条评论的情感分数，并从最常用的单词中生成一个令人敬畏的worldcloud，作为给我同事的礼物。❤️</p><h2 id="7a9d" class="nm lw it bd lx nv nw dn mb nx ny dp mf li nz oa mh lm ob oc mj lq od oe ml of bi translated">密码</h2><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="057a" class="nm lw it ni b gy nn no l np nq"><strong class="ni iu">def</strong> clean_links(df):<br/><em class="nr">#replace URL of a text</em><br/>    df_sent['text'] = df_sent['text'].str.replace('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ')<br/><br/>clean_links(df_sent)<br/>df_sent['text']</span><span id="3a18" class="nm lw it ni b gy ns no l np nq"><em class="nr"># load VADER</em><br/>sid = SentimentIntensityAnalyzer()</span><span id="76d6" class="nm lw it ni b gy ns no l np nq"># add VADER metrics to dataframe</span><span id="d743" class="nm lw it ni b gy ns no l np nq">df_sent['scores'] = df_sent['text'].apply(lambda text: sid.polarity_scores(text))</span><span id="9a3e" class="nm lw it ni b gy ns no l np nq">df_sent['compound']  = df_sent['scores'].apply(lambda score_dict: score_dict['compound'])</span><span id="08f8" class="nm lw it ni b gy ns no l np nq">df_sent['comp_score'] = df_sent['compound'].apply(lambda c: 'pos' if c &gt;=0 else 'neg')</span><span id="db29" class="nm lw it ni b gy ns no l np nq">#test <br/>df_sent.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/b06e9aeef660591c8f7c92e513cdfc69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qAfLk3arcH0c1G4u60F2bA.png"/></div></div></figure><p id="dc68" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这很容易。现在，到了具有挑战性的预处理部分，创建一个没有链接、数字、标点、停用词的世界云:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="823d" class="nm lw it ni b gy nn no l np nq"><em class="nr"># set of stopwords to be removed from text</em><br/>stop = set(stopwords.words('english'))<br/><br/><em class="nr"># update stopwords to have punctuation too</em><br/>stop.update(list(string.punctuation))<br/><br/><strong class="ni iu">def</strong> clean_text(text_list):<br/>    <br/>    <em class="nr"># Remove unwanted html characters</em><br/>    re1 = re.compile(r'  +')<br/>    x1 = text_list.lower().replace('#39;', "'").replace('amp;', '&amp;').replace('#146;', "'").replace(<br/>    'nbsp;', ' ').replace('#36;', '$').replace('<strong class="ni iu">\\</strong>n', "<strong class="ni iu">\n</strong>").replace('quot;', "'").replace(<br/>    '&lt;br /&gt;', "<strong class="ni iu">\n</strong>").replace('<strong class="ni iu">\\</strong>"', '"').replace('&lt;unk&gt;', 'u_n').replace(' @.@ ', '.').replace(<br/>    ' @-@ ', '-').replace('<strong class="ni iu">\\</strong>', ' <strong class="ni iu">\\</strong> ')<br/>    text = re1.sub(' ', html.unescape(x1))<br/>    <br/>    <em class="nr"># remove non-ascii characters</em><br/>    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')<br/>    <br/>    <em class="nr"># strip html</em><br/>    soup = BeautifulSoup(text, 'html.parser')<br/>    text = soup.get_text()<br/>    <br/>    <em class="nr"># remove between square brackets</em><br/>    text = re.sub('\[[^]]*\]', '', text)<br/>    <br/>    <em class="nr"># remove URLs</em><br/>    text = re.sub(r'http\S+', '', text)<br/>    <br/>    <em class="nr"># remove twitter tags</em><br/>    text = text.replace("@", "")<br/>    <br/>    <em class="nr"># remove hashtags</em><br/>    text = text.replace("#", "")<br/>    <br/>    <em class="nr"># remove all non-alphabetic characters</em><br/>    text = re.sub(r'[^a-zA-Z ]', '', text)<br/>    <br/>    <em class="nr"># remove stopwords from text</em><br/>    final_text = []<br/>    <strong class="ni iu">for</strong> word <strong class="ni iu">in</strong> text.split():<br/>        <strong class="ni iu">if</strong> word.strip().lower() <strong class="ni iu">not</strong> <strong class="ni iu">in</strong> stop:<br/>            final_text.append(word.strip().lower())<br/>    <br/>    text = " ".join(final_text)<br/>    <br/>    <em class="nr"># lemmatize words</em><br/>    lemmatizer = WordNetLemmatizer()    <br/>    text = " ".join([lemmatizer.lemmatize(word) <strong class="ni iu">for</strong> word <strong class="ni iu">in</strong> text.split()])<br/>    text = " ".join([lemmatizer.lemmatize(word, pos = 'v') <strong class="ni iu">for</strong> word <strong class="ni iu">in</strong> text.split()])<br/>    <br/>    <em class="nr"># replace all numbers with "num"</em><br/>    text = re.sub("\d", "num", text)<br/>    <br/>    <strong class="ni iu">return</strong> text.lower()</span><span id="3175" class="nm lw it ni b gy ns no l np nq"><em class="nr"># apply cleaning function</em><br/>df_train['prep_text'] = df_train['text'].apply(clean_text)<br/>df_train['prep_text'].head(5)</span><span id="5d99" class="nm lw it ni b gy ns no l np nq"><em class="nr"># apply wordcloud function</em><br/>make_wordcloud(df_train['prep_text'])</span></pre><p id="b970" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果呢:(ta-daa)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/d65d97583d2a4777e181f22ef6f7bb3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vB6Ns63k7xLZiletzVmZbA.png"/></div></div></figure><h1 id="eed8" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">机器学习模型</h1><p id="c0b8" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">为了在这里强调一些很酷的东西，我采用了随机森林分类模型来查看您需要哪些特征来获得回复(在这种情况下，从群组中获得帮助),准确度分数为0.86:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="278f" class="nm lw it ni b gy nn no l np nq"><em class="nr"># feature importance</em><br/>feat_importances = pd.Series(importances, index=X.columns)<br/>plt.figure(figsize=(10,10))<br/>feat_importances.nlargest(15).plot(kind='barh', color='#FF9B48', width= 0.7)<br/>plt.xlabel('Level of importance', fontsize=16)<br/>plt.ylabel('Features', fontsize=16)<br/>plt.yticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14], ['length_of_text', 'neutral_tone', 'positive_tone',<br/>                                                   'amount_of_reactions', 'negative_tone',<br/>                                                   'may', 'morning','march', 'files_attached', <br/>                                                   'teacher_posted', 'evening', 'early_morning',<br/>                                                   'labhelp_channel', 'general_channel', 'got_reaction'])<br/><br/>plt.title("Top 15 Important Features", fontsize=20)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/c790a249a4a0a75f645c1fd2a9ffc4d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eVeXnAjqMp_Z2uZ1zpUTuA.png"/></div></div></figure><p id="80c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">看起来你有更好的机会得到回复，如果你:用中性或积极的语气写一封长信，收到很多回复，早上发送也有帮助，或者你有一个附件。</p><h1 id="7a42" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="0031" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">通过这个项目，我学到了一些东西:</p><ul class=""><li id="7f34" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated">在你投资的事情上工作是一个游戏改变者</li><li id="5fd3" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">利益相关者在你身边是无价的</li><li id="6192" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">迭代是关键</li><li id="acaa" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">从长远来看，函数可以节省您的时间</li></ul><p id="9cfa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我将利用这个数据集，运用我从Udemy的NLP课程中获得的知识，从评论中提取一些很酷的东西。</p></div></div>    
</body>
</html>