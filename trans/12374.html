<html>
<head>
<title>How to Speed Up XGBoost Model Training</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何加快XGBoost模型训练</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-speed-up-xgboost-model-training-fcf4dc5dbe5f?source=collection_archive---------18-----------------------#2021-12-16">https://towardsdatascience.com/how-to-speed-up-xgboost-model-training-fcf4dc5dbe5f?source=collection_archive---------18-----------------------#2021-12-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/4a8215538c6e093ce22332719df47198.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xwVUk2XODS_NUv6MFBwaQA.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">虽然增加计算资源可以加快XGBoost模型训练的速度，但为了更好地利用可用的计算资源，您也可以选择更高效的算法(图片由<a class="ae kc" href="https://twitter.com/GalarnykMichael" rel="noopener ugc nofollow" target="_blank"> Michael Galarnyk </a>提供)。</p></figure><p id="c436" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">梯度推进算法广泛应用于监督学习。虽然他们很强大，但他们可能需要很长时间来训练。Extreme gradient boosting，或<a class="ae kc" href="https://xgboost.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> XGBoost </a>，是一个开源的梯度增强实现，旨在提高速度和性能。然而，即使是XGBoost训练有时也会很慢。</p><p id="90ba" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有很多方法可以加速这一过程，例如:</p><ul class=""><li id="8c66" class="lb lc iq kf b kg kh kk kl ko ld ks le kw lf la lg lh li lj bi translated">改变树的构造方法</li><li id="9ed0" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated">利用云计算</li><li id="b008" class="lb lc iq kf b kg lk kk ll ko lm ks ln kw lo la lg lh li lj bi translated"><a class="ae kc" href="https://docs.ray.io/en/latest/xgboost-ray.html#:~:text=XGBoost%2DRay%20integrates%20with%20Ray,training%20run%20parallelized%20by%20itself." rel="noopener ugc nofollow" target="_blank">光线上分布的XGBoost</a></li></ul><p id="d41c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本文将回顾每种方法的优点和缺点，以及如何开始。</p><h1 id="6516" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">改变你的树构造算法</h1><p id="b18d" class="pw-post-body-paragraph kd ke iq kf b kg mn ki kj kk mo km kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">XGBoost的<code class="fe ms mt mu mv b">tree_method</code>参数允许您指定想要使用的树构造算法。为你的问题选择一个合适的树构造算法(<code class="fe ms mt mu mv b">exact</code>、<code class="fe ms mt mu mv b">approx</code>、<code class="fe ms mt mu mv b">hist</code>、<code class="fe ms mt mu mv b">gpu_hist</code>、<code class="fe ms mt mu mv b">auto</code>，可以帮助你更快的产生一个最优的模型。现在让我们回顾一下算法。</p><p id="d5b3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae kc" href="https://xgboost.readthedocs.io/en/latest/treemethod.html#exact-solution" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir">确切的</strong> </a></p><p id="bad9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是一个精确的算法，但是它的可扩展性不是很好，因为在每个split find过程中，它会遍历所有的输入数据条目。实际上，这意味着长时间的训练。它也不支持分布式训练。可以在原<a class="ae kc" href="https://arxiv.org/abs/1603.02754" rel="noopener ugc nofollow" target="_blank"> XGBoost论文</a>中了解更多关于这个算法的内容。</p><p id="652f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae kc" href="https://xgboost.readthedocs.io/en/stable/treemethod.html#approximated-solutions" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir">约</strong> </a></p><p id="b2f8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然确切的算法是准确的，但当数据不能完全适合内存时，它是低效的。原始<a class="ae kc" href="https://arxiv.org/abs/1603.02754" rel="noopener ugc nofollow" target="_blank"> XGBoost论文</a>中的近似树方法使用分位数草图和梯度直方图。</p><p id="8aed" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae kc" href="https://xgboost.readthedocs.io/en/stable/treemethod.html#approximated-solutions" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir">hist</strong>T6】</a></p><p id="fa71" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<a class="ae kc" href="https://lightgbm.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> LightGBM </a>中使用的近似树方法与<code class="fe ms mt mu mv b">approx</code>在实现上略有不同(使用了一些性能改进，如bin缓存)。这通常比<code class="fe ms mt mu mv b">approx</code>更快。</p><p id="0ecc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae kc" href="https://xgboost.readthedocs.io/en/stable/treemethod.html#approximated-solutions" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir"> gpu_hist </strong> </a></p><p id="1381" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于GPU对于许多机器学习应用来说至关重要，XGBoost拥有hist算法<code class="fe ms mt mu mv b">gpu_hist</code>的GPU实现，该算法支持外部存储器。<a class="ae kc" href="https://xgboost.readthedocs.io/en/latest/gpu/index.html" rel="noopener ugc nofollow" target="_blank">它比hist </a>快得多，使用的内存也少得多。注意，XGBoost在某些操作系统上没有对GPU的<strong class="kf ir">原生支持</strong>。</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mw"><img src="../Images/5f4f7db2c3c27f5a4dd40cc36a07488a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*YPkEXiodkU-h2sPL.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated"><a class="ae kc" href="https://xgboost.readthedocs.io/en/latest/install.html#python" rel="noopener ugc nofollow" target="_blank"> XGBoost文档</a></p></figure><p id="96df" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae kc" href="https://xgboost.readthedocs.io/en/stable/treemethod.html#approximated-solutions" rel="noopener ugc nofollow" target="_blank"> <strong class="kf ir">汽车</strong> </a></p><p id="1b83" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是参数的默认值。基于数据集的大小，XGBoost将选择“最快的方法”。对于小型数据集，将使用exact。对于较大的数据集，将使用近似值。注意，在这种基于启发式的方法中，不考虑hist和gpu_hist，尽管它们通常更快。</p><p id="6036" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你运行下面的<a class="ae kc" href="https://gist.github.com/mGalarnyk/16d15183f691594bc2c256505a4c42b1" rel="noopener ugc nofollow" target="_blank">代码</a>，你会看到使用gpu_hist运行模型是如何节省大量时间的。在我的计算机上的一个相对较小的数据集(100，000行，1000个要素)上，从hist更改为gpu_hist将训练时间减少了大约1/2。</p><figure class="mx my mz na gt jr"><div class="bz fp l di"><div class="nb nc l"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">如果XGBoost在您的操作系统上没有对GPU的<strong class="ak">本地支持</strong>，请将第17行修改为methods = ['exact '，' approx '，' hist '，' auto']。这将删除“gpu_hist”。</p></figure><h1 id="037d" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">利用云计算</h1><figure class="mx my mz na gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nd"><img src="../Images/6832cf5514c86b813993daba8debbd6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9aDDguTieZHQZIEE.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">云计算不仅可以让你利用比本地机器更多的内核和内存，还可以让你访问专门的资源，比如GPU。</p></figure><p id="24fc" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后一节主要是关于选择更有效的算法，以便更好地利用可用的计算资源。然而，有时可用的计算资源是不够的，你只是需要更多。比如下图所示的MacBook，只有4核，16GB内存。此外，它运行在MacOS上，而在撰写本文时，XGBoost还没有GPU支持。</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ne"><img src="../Images/e3e2429602709f43437a82356c4ff994.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*4a_NG_vVvRDhp2c_.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">出于本文的目的，你可以将上面的MacBook想象成一个4核的单节点(图片由<a class="ae kc" href="https://twitter.com/GalarnykMichael" rel="noopener ugc nofollow" target="_blank"> Michael Galarnyk </a>提供)。</p></figure><p id="56d9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">解决这个问题的方法是在云上利用更多的资源。利用云提供商不是免费的，但他们通常允许你利用比本地机器更多的内核和内存。此外，如果XGBoost不支持您的本地机器，很容易选择XGBoost支持的实例类型。</p><p id="c98e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你想尝试加速你在云上的训练，下面是来自<a class="ae kc" href="https://machinelearningmastery.com/train-xgboost-models-cloud-amazon-web-services/" rel="noopener ugc nofollow" target="_blank">杰森·布朗利的文章</a>中关于如何在AWS EC2实例上训练XGBoost模型的步骤概述:</p><p id="c364" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">1.设置AWS帐户(如果需要)</p><p id="2a1e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2.启动AWS实例</p><p id="0f73" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">3.登录并运行代码</p><p id="5782" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">4.训练XGBoost模型</p><p id="0772" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">5.关闭AWS实例(仅在使用实例时付费)</p><p id="7ba1" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您选择一个比本地更强大的实例，您可能会发现云上的训练更快。<strong class="kf ir">注意，使用XGBoost的多GPU训练实际上需要分布式训练，这意味着您需要不止一个节点/实例来完成这个</strong>。</p><h1 id="f80b" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">使用Ray进行分布式XGBoost训练</h1><p id="11d5" class="pw-post-body-paragraph kd ke iq kf b kg mn ki kj kk mo km kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">到目前为止，本教程已经介绍了通过改变树构造算法和通过云计算增加计算资源来加速训练。另一个解决方案是用利用Ray的<a class="ae kc" href="https://docs.ray.io/en/latest/xgboost-ray.html" rel="noopener ugc nofollow" target="_blank"> XGBoost-Ray </a>来分发XGBoost模型训练。</p><h2 id="b60d" class="nf lq iq bd lr ng nh dn lv ni nj dp lz ko nk nl md ks nm nn mh kw no np ml nq bi translated">雷是什么？</h2><p id="4062" class="pw-post-body-paragraph kd ke iq kf b kg mn ki kj kk mo km kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">Ray是一个快速、简单的分布式执行框架，可以轻松扩展您的应用程序并利用最先进的机器学习库。使用Ray，您可以将按顺序运行的Python代码，通过最少的代码更改，转换成分布式应用程序。如果你想了解雷和<a class="ae kc" href="https://en.wikipedia.org/wiki/Actor_model" rel="noopener ugc nofollow" target="_blank">演员模型</a>，你可以在这里了解<a class="ae kc" href="https://www.anyscale.com/blog/writing-your-first-distributed-python-application-with-ray" rel="noopener ugc nofollow" target="_blank">。</a></p><figure class="mx my mz na gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi nr"><img src="../Images/4e5c0b22c4b544b1a8ceab8318401eaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w4Daajq5TCf7TfTJr7PMRg.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">虽然本教程探索了Ray如何使XGBoost代码的并行化和分发变得容易，但需要注意的是，Ray及其生态系统也使普通Python代码以及现有库的分发变得容易，如<a class="ae kc" href="https://www.anyscale.com/blog/how-to-speed-up-scikit-learn-model-training" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>、<a class="ae kc" href="https://www.anyscale.com/blog/introducing-distributed-lightgbm-training-with-ray" rel="noopener ugc nofollow" target="_blank"> LightGBM </a>、<a class="ae kc" href="https://medium.com/pytorch/getting-started-with-distributed-machine-learning-with-pytorch-and-ray-fd83c98fdead" rel="noopener"> PyTorch </a>等等(图片由<a class="ae kc" href="https://twitter.com/GalarnykMichael" rel="noopener ugc nofollow" target="_blank"> Michael Galarnyk </a>提供)。</p></figure><h1 id="fcb1" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">如何开始使用XGBoost-Ray</h1><p id="5bdb" class="pw-post-body-paragraph kd ke iq kf b kg mn ki kj kk mo km kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">要开始使用XGBoost-Ray，<a class="ae kc" href="https://docs.ray.io/en/latest/xgboost-ray.html#installation" rel="noopener ugc nofollow" target="_blank">首先需要安装它</a>。</p><p id="348a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><code class="fe ms mt mu mv b">pip install "xgboost_ray"</code></p><p id="58c9" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因为它与核心XGBoost API完全兼容，所以您只需要修改一些代码，就可以将XGBoost培训从单台机器扩展到拥有数百个节点的集群。</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ns"><img src="../Images/4ac4c05239544a8bc0012e4e9e459cb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Xqra4y7MQufpGUhe.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">XGBoost-Ray支持多节点/多GPU训练。在机器上，GPU通过NCCL2传递梯度。在节点之间，他们用Rabit代替。你可以<a class="ae kc" href="https://www.anyscale.com/blog/distributed-xgboost-training-with-ray" rel="noopener ugc nofollow" target="_blank">在这里</a>了解更多信息。</p></figure><p id="c5ea" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如您在下面的代码中看到的，这个API非常类似于XGBoost。粗体部分是代码与普通XGBoost API不同的地方。</p><pre class="mx my mz na gt nt mv nu nv aw nw bi"><span id="c58a" class="nf lq iq mv b gy nx ny l nz oa"><strong class="mv ir">from xgboost_ray import RayXGBClassifier, RayParams</strong><br/>from sklearn.datasets import load_breast_cancer<br/>from sklearn.model_selection import train_test_split<br/><br/>seed = 42<br/><br/>X, y = load_breast_cancer(return_X_y=True)<br/>X_train, X_test, y_train, y_test = train_test_split(<br/>    X, y, train_size=0.25, random_state=42<br/>)<br/><br/><strong class="mv ir">clf = RayXGBClassifier(<br/>    n_jobs=4,  # In XGBoost-Ray, n_jobs sets the number of actors<br/>    random_state=seed)</strong><br/><br/># scikit-learn API will automatically convert the data<br/># to RayDMatrix format as needed.<br/># You can also pass X as a RayDMatrix, in which case<br/># y will be ignored.<br/><br/>clf.fit(X_train, y_train)<br/><br/>pred_ray = clf.predict(X_test)<br/>print(pred_ray)<br/><br/>pred_proba_ray = clf.predict_proba(X_test)<br/>print(pred_proba_ray)</span></pre><p id="021d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上面的代码显示了使用XGBoost-Ray只需要修改很少的代码。虽然您不需要XGboost-Ray来训练乳腺癌数据集，但<a class="ae kc" href="https://www.anyscale.com/blog/distributed-xgboost-training-with-ray" rel="noopener ugc nofollow" target="_blank">之前的一篇文章</a>在不同数量的工作人员(1到8)中对几个数据集大小(大约1.5M到大约12M行)进行了基准测试，以显示它在单个节点上对更大的数据集的性能。</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ob"><img src="../Images/bfa1c75cad270885504cf4a186b41265.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ephgD5rpBj2MZiTE.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">单节点基准测试的训练时间(越短越好)。XGBoost-Ray和XGBoost-Dask在单个AWS m 5.4x大型实例上实现了类似的性能，该实例具有16个内核和64 GB内存(<a class="ae kc" href="https://www.anyscale.com/blog/distributed-xgboost-training-with-ray" rel="noopener ugc nofollow" target="_blank">图像源</a>)，</p></figure><p id="2924" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">XGBoost-Ray在多节点(分布式)设置中也是高性能的，如下图所示。</p><figure class="mx my mz na gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi oc"><img src="../Images/a233fb377630f1f407190a02f9011f39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Mw1EyMyiUIT5LOUigVcTNw.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">多个合成数据集的多节点训练时间从大约400k到大约2B行(越低越好)。XGBoost-Ray和XGBoost-Spark实现了类似的性能(<a class="ae kc" href="https://www.anyscale.com/blog/distributed-xgboost-training-with-ray" rel="noopener ugc nofollow" target="_blank">图像来源</a>)。</p></figure><p id="6c3f" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你想了解更多关于XGBoost Ray的信息，<a class="ae kc" href="https://www.anyscale.com/blog/distributed-xgboost-training-with-ray" rel="noopener ugc nofollow" target="_blank">请查看XGBoost-Ray </a>上的这篇帖子。</p><h1 id="4f69" class="lp lq iq bd lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm bi translated">结论</h1><p id="aff7" class="pw-post-body-paragraph kd ke iq kf b kg mn ki kj kk mo km kn ko mp kq kr ks mq ku kv kw mr ky kz la ij bi translated">这篇文章介绍了几种可以用来加速XGBoost模型训练的方法，比如改变树的构造方法、利用云计算和在Ray上分布式XGBoost。请记住，有许多不同的方法可以做到这一点，所以请随时用你最喜欢的方式发表评论。如果你想了解雷的最新消息，<a class="ae kc" href="https://twitter.com/raydistributed" rel="noopener ugc nofollow" target="_blank">可以考虑在twitter上关注@ Ray distributed</a>。</p></div></div>    
</body>
</html>