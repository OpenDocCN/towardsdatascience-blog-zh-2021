<html>
<head>
<title>Support Vector Machine (SVM)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">支持向量机(SVM)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/support-vector-machine-svm-719e530a725f?source=collection_archive---------13-----------------------#2021-05-16">https://towardsdatascience.com/support-vector-machine-svm-719e530a725f?source=collection_archive---------13-----------------------#2021-05-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/b2ff3ae1bca084d211aaf3d4b1c28c62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5stif7Jjmmdclx7-qTDBgQ.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">资料来源:Unsplash.com</p></figure><div class=""/><div class=""><h2 id="c88c" class="pw-subtitle-paragraph kc je jf bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">所有关于支持向量机(SVM)从推导到面试准备</h2></div><p id="9075" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">最近，我有机会为一群数据科学领域的新人和有经验的人准备一个关于SVM的讲座。我准备了一个讲座，涉及到基础知识，甚至是对SVM(向量法)推导的直觉，还有一些问题以及讨论部分的答案。我希望这次讲座能让更多的观众受益。请在评论区告诉我，你对未来的ML算法讲座还有什么期待。</p><h1 id="c712" class="lq lr jf bd ls lt lu lv lw lx ly lz ma kl mb km mc ko md kp me kr mf ks mg mh bi translated"><strong class="ak">SVM背景</strong></h1><p id="374d" class="pw-post-body-paragraph ku kv jf kw b kx mi kg kz la mj kj lc ld mk lf lg lh ml lj lk ll mm ln lo lp ij bi translated">SVM群岛</p><ul class=""><li id="a798" class="mn mo jf kw b kx ky la lb ld mp lh mq ll mr lp ms mt mu mv bi translated">监督机器学习算法</li><li id="eaef" class="mn mo jf kw b kx mw la mx ld my lh mz ll na lp ms mt mu mv bi translated">用于<strong class="kw jg">分类</strong>和回归</li></ul><p id="0a64" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">SVM的真正优势在于分类，并且是高维向量(具有大量特征的数据点)的特定分类。虽然支持向量回归机(SVR)也使用与支持向量分类机相同的直觉和概念，但对于回归，线性或随机森林回归机也是SVR的强大竞争对手。我们将关注使用支持向量分类器的分类，而不是回归。</p><h1 id="dd48" class="lq lr jf bd ls lt lu lv lw lx ly lz ma kl mb km mc ko md kp me kr mf ks mg mh bi translated">SVM的优化</h1><p id="1295" class="pw-post-body-paragraph ku kv jf kw b kx mi kg kz la mj kj lc ld mk lf lg lh ml lj lk ll mm ln lo lp ij bi translated">优化取决于向量对的<strong class="kw jg">点积</strong></p><ul class=""><li id="5bf0" class="mn mo jf kw b kx ky la lb ld mp lh mq ll mr lp ms mt mu mv bi translated">推导(证明SVM分类器中的优化仅取决于向量对的乘积的事实)。</li></ul><p id="c93c" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">假设有一个如图1所示的二元分类问题，为了在2D区分两个类别，需要确定一个决策边界。如果划分类的边界是线性的，则称这些类是线性可分的。图1显示了两个不同类别+和-的向量(数据点)。</p><figure class="nc nd ne nf gt is gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/98b86c6025313cc3039abc88bae0f8c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*ICbtAl_QhIFY9bC4Owrf4w.jpeg"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图1</p></figure><p id="e65f" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">多行可以作为分隔两个类的判定边界(参见图2)。蓝线、黄线或绿线中的任何一条都可以作为决策边界，因为这三条线都分隔了数据。我们将如何决定这些边界中的哪一个最好地分隔数据？这个决定会给我们一个条件，利用这个条件，我们可以在所有的可能性中拒绝除了一个决定边界之外的所有可能性。</p><figure class="nc nd ne nf gt is gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/05be0c7b7a9ba75070c4a8a18138d957.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*5alNhw37i0Axn3KjYlnijA.jpeg"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图2</p></figure><p id="8a7d" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">为了定义最佳决策边界，我们使用最大裕度原则。离两个类的最近成员距离最大的边界被选为最佳边界。在图3中，散列线下面显示的是通过+和-类的最近点的线。从黄线到虚线的距离被定义为边距。假设在分隔两个类别的所有可能的线中，如果黄线在页边空白之间具有最宽的间隔，则黄线是最佳决策边界。散列黄线所经过的两个类别的点被称为支持向量。可以有单个支持向量(对于+类)或者可以有多个支持向量(对于-类)，散列线的斜率将与决策边界相同。</p><figure class="nc nd ne nf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ng"><img src="../Images/d71571b09c9b431be9f01a85e53833d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Od3UZ7MerG8iOgoC_fX9cg.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图3</p></figure><p id="f6cd" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">为了最大化两条散列线之间的边距或距离，我们需要创建一个表达式，并对其求导以找到最佳(极值)值。假设向量‘w’垂直于决策边界，向量‘u’未知，如图4所示。为了确定未知向量u位于-ive还是+ ive类的边上，我们测量u在w上的投影的长度，如果它大于某个长度，则它在+ive类中，否则它属于-ive类(或者位于边缘)。</p><figure class="nc nd ne nf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nh"><img src="../Images/20a8933feb5f1f8cea290b22c0441d26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uG4lM1wu_vLih7WlqlEpNA.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图4</p></figure><figure class="nc nd ne nf gt is gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/6645257545428ee79253d81448002498.png" data-original-src="https://miro.medium.com/v2/resize:fit:856/format:webp/1*z9amQrhEZtTandRJLXgA3g.jpeg"/></div></figure><p id="ecc6" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">为了区分+类和-类，我们假设如果未知向量是来自+类(x+)的向量，它应该给出大于1的值，或者如果它是来自-类(x)的向量，它应该给出小于-1的值。+1和-1值决定了两个类之间的清晰分隔。所以数学上的条件是:</p><figure class="nc nd ne nf gt is gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/33fcb7bf62fdb46f71953380f7d50aea.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*NRffDKbRDzkRZ_rjM_EX7Q.jpeg"/></div></figure><p id="941f" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">为了数学上的方便，我们引入一个变量yi，对于+样本yi = 1，对于-样本yi = -1。将yi乘以上面的等式，我们得到:</p><figure class="nc nd ne nf gt is gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/3ea3e22cc4c5cafb48add745226a4719.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*Q-solScVefiqUlA_4G3XWg.jpeg"/></div></figure><p id="e66e" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">来自+类(x+sv)和-类(x SV)的支持向量位于边缘本身。因此，矢量(x+SV x SV)⋅w/|w|在法线w上的投影就是两个边距之间的距离。该距离应在上述限制条件下最大化。</p><ul class=""><li id="47e4" class="mn mo jf kw b kx ky la lb ld mp lh mq ll mr lp ms mt mu mv bi translated">最大化:</li></ul><figure class="nc nd ne nf gt is gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/80f497faa32cdf329788c41a0058fa61.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*Ep_MPn2S6M2aWG32MDEDxw.jpeg"/></div></figure><p id="6223" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">从上面的等式我们可以破译:</p><figure class="nc nd ne nf gt is gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/8d067e73ff0e9b0e81a9b99431ab477a.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*vnXG8t5jQII9L7r_4hXCiw.jpeg"/></div></figure><p id="bbd4" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">因此，最大化条件变成最大值2/|w|,或者我们可以说最小值:</p><ul class=""><li id="fe3a" class="mn mo jf kw b kx ky la lb ld mp lh mq ll mr lp ms mt mu mv bi translated"><strong class="kw jg">最小化:</strong></li></ul><figure class="nc nd ne nf gt is gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/da6c899f580baf71b11c23ea329dde3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*20J8kDO9DG8spbEHELaLLg.jpeg"/></div></figure><ul class=""><li id="ce75" class="mn mo jf kw b kx ky la lb ld mp lh mq ll mr lp ms mt mu mv bi translated"><strong class="kw jg">约束:</strong></li></ul><figure class="nc nd ne nf gt is gh gi paragraph-image"><div class="gh gi no"><img src="../Images/edefad8d4d025126265dbe040b00e682.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*XMQlne3NGMWwJH68rjvtYQ.jpeg"/></div></figure><p id="3333" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">为了找到极值和约束，我们使用Langrangian，</p><figure class="nc nd ne nf gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi np"><img src="../Images/36c7c8e918aef3470a1781409ead039e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*2UYJkI5fUHnyn04yGVh-cw.jpeg"/></div></div></figure><p id="e690" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">拉格朗日的最小值是:</p><figure class="nc nd ne nf gt is gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/e476354e52f889b8e174ec62bd386ef9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*KklpyrEVSua5ltWr-4r5Ug.jpeg"/></div></figure><p id="14c4" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">在上面的表达式中，您可以观察到最小值(我们的优化)取决于两个向量对(数据点)的点积。</p><h1 id="8f38" class="lq lr jf bd ls lt lu lv lw lx ly lz ma kl mb km mc ko md kp me kr mf ks mg mh bi translated">如果类是线性不可分的</h1><p id="dee0" class="pw-post-body-paragraph ku kv jf kw b kx mi kg kz la mj kj lc ld mk lf lg lh ml lj lk ll mm ln lo lp ij bi translated">在这种情况下，添加额外的特征并将向量转换到更高维度可能有助于通过平面(超平面)分离向量。例如，假设有两个类c1(蓝色)和c2(橙色)，如下图5所示。这两类人似乎没有线性的决策界限。数据x_2 = (x1) 增加一个新的维度怎么办？</p><p id="1f88" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">非线性可分离数据:</p><figure class="nc nd ne nf gt is gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/568dfb2cf0af48aacec80c2566a00e16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1344/format:webp/1*zX0Mf6GSoqEbB0t-ry6vag.jpeg"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图5</p></figure><p id="d0e7" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">现在可线性分离的转换数据(如图6所示):</p><figure class="nc nd ne nf gt is gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/6a67ff1ba715ea8d3c0f566fd4389cc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1326/format:webp/1*Cav6_UaRwAU_2qNayl0Qwg.jpeg"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图6</p></figure><p id="668e" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">类似地，对于包含x1和x2维度的二维数据，可以创建像<strong class="kw jg"> (x1)、(x2)、(x1)3 </strong>这样的新维度，并且可以将数据转换到更高维度以分离类别。</p><p id="cf52" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">我们有一些函数可以帮助将数据转换到更高的维度。这些函数被称为内核。</p><h1 id="4481" class="lq lr jf bd ls lt lu lv lw lx ly lz ma kl mb km mc ko md kp me kr mf ks mg mh bi translated">SVM一些受欢迎的果仁</h1><ul class=""><li id="a4b0" class="mn mo jf kw b kx mi la mj ld nt lh nu ll nv lp ms mt mu mv bi translated"><strong class="kw jg">直线型</strong></li><li id="540e" class="mn mo jf kw b kx mw la mx ld my lh mz ll na lp ms mt mu mv bi translated">多项式</li><li id="8acb" class="mn mo jf kw b kx mw la mx ld my lh mz ll na lp ms mt mu mv bi translated">高斯的</li><li id="dfdb" class="mn mo jf kw b kx mw la mx ld my lh mz ll na lp ms mt mu mv bi translated">径向基函数</li><li id="e92e" class="mn mo jf kw b kx mw la mx ld my lh mz ll na lp ms mt mu mv bi translated">夸张的</li><li id="0214" class="mn mo jf kw b kx mw la mx ld my lh mz ll na lp ms mt mu mv bi translated">乙状结肠的</li></ul><h1 id="9144" class="lq lr jf bd ls lt lu lv lw lx ly lz ma kl mb km mc ko md kp me kr mf ks mg mh bi translated">讨论(面试QnA)</h1><ol class=""><li id="c87d" class="mn mo jf kw b kx mi la mj ld nt lh nu ll nv lp nw mt mu mv bi translated"><strong class="kw jg">任何算法通用问题:ADA？(优点、缺点和假设)</strong></li></ol><p id="6977" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg">优点:</strong></p><ul class=""><li id="21fb" class="mn mo jf kw b kx ky la lb ld mp lh mq ll mr lp ms mt mu mv bi translated">交钥匙算法，需要优化的参数很少。</li><li id="4831" class="mn mo jf kw b kx mw la mx ld my lh mz ll na lp ms mt mu mv bi translated">优化是在凸空间中进行的，因此可以达到全局最小值，这与神经网络不同，它不会遇到陷入局部最小值的问题。</li><li id="440d" class="mn mo jf kw b kx mw la mx ld my lh mz ll na lp ms mt mu mv bi translated">内存高效</li><li id="a1c4" class="mn mo jf kw b kx mw la mx ld my lh mz ll na lp ms mt mu mv bi translated">如果特征数很高(&gt; 1000)，线性SVM比其他算法更有效</li></ul><p id="e41c" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg">缺点:</strong></p><ul class=""><li id="0980" class="mn mo jf kw b kx ky la lb ld mp lh mq ll mr lp ms mt mu mv bi translated">特征缩放</li><li id="ecb7" class="mn mo jf kw b kx mw la mx ld my lh mz ll na lp ms mt mu mv bi translated">随着示例数量的增加，速度会变慢</li><li id="5401" class="mn mo jf kw b kx mw la mx ld my lh mz ll na lp ms mt mu mv bi translated">硬边界导致过度拟合</li></ul><p id="f707" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg">假设:</strong></p><ul class=""><li id="f278" class="mn mo jf kw b kx ky la lb ld mp lh mq ll mr lp ms mt mu mv bi translated">没有人</li></ul><p id="2857" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg"> 2。结合SVM解释硬利润和软利润。</strong></p><ul class=""><li id="ca16" class="mn mo jf kw b kx ky la lb ld mp lh mq ll mr lp ms mt mu mv bi translated"><strong class="kw jg">硬边界</strong>不允许有任何误差</li><li id="8a51" class="mn mo jf kw b kx mw la mx ld my lh mz ll na lp ms mt mu mv bi translated"><strong class="kw jg">软余量</strong>允许误差，它减少了过拟合的机会</li></ul><p id="727d" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg"> 3。为什么例子越多，SVM越慢？</strong></p><p id="406a" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">计算成本取决于向量对的选择(nC2，如优化部分所述)。</p><p id="119e" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">4.与SVM相关的超参数有哪些？</p><ul class=""><li id="f507" class="mn mo jf kw b kx ky la lb ld mp lh mq ll mr lp ms mt mu mv bi translated">核心</li><li id="81fd" class="mn mo jf kw b kx mw la mx ld my lh mz ll na lp ms mt mu mv bi translated">c:允许误差的数量。大C给出了一个狭窄的边界，反之亦然。</li></ul><p id="41d1" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg"> 5。为什么SVM算法被认为是内存高效的？</strong></p><p id="4edd" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">不是所有的数据点都需要作出决定，一旦支持向量被决定，只有支持向量和超平面的方程需要作出决定。</p><p id="4a34" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg"> 6 </strong>。陈述以下陈述是对还是错:“SVM判定边界是连接两个类的凸包的最近点的直线的垂直平分线”。</p><p id="0ba5" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">真实的陈述。几何解释:制作每个类别的凸包，尝试使用最近点连接凸包，分类边界将是垂直平分线。</p><p id="9464" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">7 .<strong class="kw jg">。</strong> <strong class="kw jg">举一个SVM将被用于RandomForest的典型例子。</strong></p><p id="6d89" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">文本分类:当特征数量很大时，SVM是高维空间的一种算法选择。</p><p id="14bf" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated"><strong class="kw jg"> 8。</strong> <strong class="kw jg">什么是正规化？SVM算法是如何正则化的？本质上是L1还是L2正规化？</strong></p><p id="850e" class="pw-post-body-paragraph ku kv jf kw b kx ky kg kz la lb kj lc ld le lf lg lh li lj lk ll lm ln lo lp ij bi translated">' c参数'实际上是<strong class="kw jg">正则化参数</strong>。C参数乘以误差之和，因此正则化本质上是L1。但是如果代价函数以这样的方式被修改，即SVM的代价是C乘以误差的平方和，则正则化本质上变成了L2。</p><figure class="nc nd ne nf gt is gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/ba986a35436147f0bc54e803404177da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/format:webp/1*5_YWJEdDZ5yEc7Ipyu4gSg.jpeg"/></div></figure><h1 id="b81e" class="lq lr jf bd ls lt lu lv lw lx ly lz ma kl mb km mc ko md kp me kr mf ks mg mh bi translated">推荐读物</h1><ul class=""><li id="5764" class="mn mo jf kw b kx mi la mj ld nt lh nu ll nv lp ms mt mu mv bi translated">一定要寻找其他的推导方法(线性代数方法)</li><li id="6f64" class="mn mo jf kw b kx mw la mx ld my lh mz ll na lp ms mt mu mv bi translated">默瑟定理</li></ul></div></div>    
</body>
</html>