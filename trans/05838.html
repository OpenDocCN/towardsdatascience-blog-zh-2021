<html>
<head>
<title>BERT For Next Sentence Prediction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于下一句预测的BERT</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bert-for-next-sentence-prediction-466b67f8226f?source=collection_archive---------9-----------------------#2021-05-25">https://towardsdatascience.com/bert-for-next-sentence-prediction-466b67f8226f?source=collection_archive---------9-----------------------#2021-05-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9e33" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">另一半用来训练伯特</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7c7b4f43b903e973913d12049b5b144d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rj8T6lczjL2QeFXI7QnjQw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">胸部+三与伯特-图片由作者(抱歉)</p></figure><p id="6f70" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi lu translated">N<span class="l lv lw lx bm ly lz ma mb mc di">N</span>ext句子预测(NSP)是BERT模型(另一个是掩蔽语言建模——MLM)背后的训练过程的一半。</p><p id="e880" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">MLM教伯特理解单词之间的关系，NSP教伯特理解句子之间的长期依存关系。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="md me l"/></div></figure><p id="79bd" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">没有NSP，伯特在每一个指标上都表现得更差——所以这很重要。</p><p id="54a6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，当我们使用预训练的伯特模型时，与NSP和MLM的训练已经完成，那么我们为什么需要了解它呢？</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mf me l"/></div></figure><p id="e4c1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">嗯，我们实际上可以微调这些预训练的BERT模型，以便它们更好地理解我们特定用例中使用的语言。为此，我们可以利用MLM和NSP。</p><p id="43d4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，在本文中，我们将深入探讨什么是NSP，它是如何工作的，以及我们如何用代码实现它。</p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><h1 id="754a" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">下一句预测</h1><p id="bfa1" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">NSP给伯特两个句子，句子A和句子B。然后我们说，“嗨，伯特，句子B是在句子A后面吗？”——伯特说要么是<code class="fe nk nl nm nn b">IsNextSentence</code>要么是<code class="fe nk nl nm nn b">NotNextSentence</code>。</p><p id="6a4a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">假设我们有三个句子:</p><ol class=""><li id="7d68" class="no np it la b lb lc le lf lh nq ll nr lp ns lt nt nu nv nw bi translated">找到神奇的绿色球体后，戴夫回家了。</li><li id="b9e7" class="no np it la b lb nx le ny lh nz ll oa lp ob lt nt nu nv nw bi translated">3.6百万年前，坦桑尼亚北部拉托里的火山灰上留下了类似人类的脚印。</li><li id="84b6" class="no np it la b lb nx le ny lh nz ll oa lp ob lt nt nu nv nw bi translated">回到家，戴夫吃完剩下的披萨，在沙发上睡着了。</li></ol><p id="66ee" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我问你是否相信(逻辑上)那句<strong class="la iu"> <em class="oc"> 2 </em> </strong>跟在句<strong class="la iu"> <em class="oc"> 1 </em> </strong>后面——你会说是吗？大概不会。</p><p id="19e3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">句子<strong class="la iu"> <em class="oc"> 3 </em> </strong>下面的句子<strong class="la iu"> <em class="oc"> 1 </em> </strong>怎么样？似乎更有可能。</p><p id="9942" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">伯特从NSP那里学到的正是这种逻辑风格——句子之间的长期依存关系。</p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><h1 id="77f5" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">代码中的NSP</h1><p id="a5bc" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">让我们看看如何用代码演示NSP。</p><p id="8394" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将使用拥抱脸的变形金刚和PyTorch，以及<code class="fe nk nl nm nn b">bert-base-uncased</code>模型。因此，让我们首先导入并初始化所有内容:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="md me l"/></div></figure><p id="e974" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注意，我们有两个单独的字符串——句子A的<code class="fe nk nl nm nn b">text</code>和句子b的<code class="fe nk nl nm nn b">text2</code>。将它们分开可以让我们的分词器正确地处理它们，我们稍后会解释这一点。</p><p id="c92b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们现在需要采取三个步骤:</p><ol class=""><li id="795a" class="no np it la b lb lc le lf lh nq ll nr lp ns lt nt nu nv nw bi translated">标记化</li><li id="8929" class="no np it la b lb nx le ny lh nz ll oa lp ob lt nt nu nv nw bi translated">创建分类标签</li><li id="1a9a" class="no np it la b lb nx le ny lh nz ll oa lp ob lt nt nu nv nw bi translated">计算损失</li></ol><p id="2e2b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">先说标记化。</p><p id="9a5c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi lu translated"><span class="l lv lw lx bm ly lz ma mb mc di"> 1。</span> <strong class="la iu">标记化</strong> —我们使用初始化的<code class="fe nk nl nm nn b">tokenizer</code>来执行标记化，同时传递<code class="fe nk nl nm nn b">text</code>和<code class="fe nk nl nm nn b">text2</code>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="md me l"/></div></figure><p id="fe3c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于NSP，有几件事我们应该知道。首先，我们的两个句子被合并到同一个张量集中——但是伯特有办法识别出它们实际上是两个独立的句子。</p><ul class=""><li id="32ec" class="no np it la b lb lc le lf lh nq ll nr lp ns lt od nu nv nw bi translated">两句之间加一个<em class="oc">【SEP】</em>记号。这个分隔符由上面的<em class="oc"> input_ids </em>张量中的<em class="oc"> 102 </em>表示。</li><li id="9ced" class="no np it la b lb nx le ny lh nz ll oa lp ob lt od nu nv nw bi translated"><em class="oc"> token_type_ids </em>张量包含段id，用于识别各个令牌属于哪个段。A句用<em class="oc"> 0 </em>表示，B句用<em class="oc"> 1 </em>表示。</li></ul><p id="8cd0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi lu translated"><span class="l lv lw lx bm ly lz ma mb mc di"> 2。</span> <strong class="la iu">创建类标签</strong>——下一步很简单，我们在这里需要做的就是创建一个新的<em class="oc">标签</em>张量来标识句子B是否跟在句子a后面</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="md me l"/></div></figure><p id="09cc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们用一个值<em class="oc"> 0 </em>来表示<code class="fe nk nl nm nn b">IsNextSentence</code>，用<em class="oc"> 1 </em>来表示<code class="fe nk nl nm nn b">NotNextSentence</code>。另外，我们必须使用<code class="fe nk nl nm nn b">torch.LongTensor</code>格式。</p><p id="aa41" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi lu translated"><span class="l lv lw lx bm ly lz ma mb mc di"> 3。最后，我们开始计算我们的损失。我们从通过我们的模型处理我们的<code class="fe nk nl nm nn b">inputs</code>和<code class="fe nk nl nm nn b">labels</code>开始。</span></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="md me l"/></div></figure><p id="e946" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们的模型将返回<em class="oc">损失</em>张量，这是我们将在训练期间优化的内容——我们很快将继续进行。</p><h2 id="cd0d" class="oe mo it bd mp of og dn mt oh oi dp mx lh oj ok mz ll ol om nb lp on oo nd op bi translated">预言；预测；预告</h2><p id="8265" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">我们可能也不需要训练我们的模型，而只是想使用模型进行推理。在这种情况下，我们将没有<em class="oc">标签</em>张量，我们将修改代码的最后一部分来提取<em class="oc"> logits </em>张量，如下所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="md me l"/></div></figure><p id="054c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们的模型将返回一个<em class="oc"> logits </em>张量，它包含两个值——索引<em class="oc"> 0 </em>中的<code class="fe nk nl nm nn b">IsNextSentence</code>类的激活，以及索引<em class="oc"> 1 </em>中的<code class="fe nk nl nm nn b">NotNextSentence</code>类的激活。</p><p id="5e6e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从这里开始，我们所做的就是使用输出<em class="oc">逻辑</em>的<em class="oc"> argmax </em>来返回我们模型的预测。在这种情况下，它返回<em class="oc"> 0 </em> —这意味着BERT相信句子B <strong class="la iu">跟随句子A(正确)。</strong></p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><p id="7b9f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这就是伯特的这篇关于NSP基本面的文章。我们已经介绍了NSP是什么，它是如何工作的，以及我们如何使用NSP提取损失和/或预测。</p><p id="20e5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我希望你喜欢这篇文章！如果你有任何问题，请通过<a class="ae oq" href="https://twitter.com/jamescalam" rel="noopener ugc nofollow" target="_blank"> Twitter </a>或在下面的评论中告诉我。如果你想要更多这样的内容，我也会在<a class="ae oq" href="https://www.youtube.com/c/jamesbriggs" rel="noopener ugc nofollow" target="_blank"> YouTube </a>上发布。</p><p id="434f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢阅读！</p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><h1 id="1480" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">参考</h1><p id="d6d0" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">[1] J .德夫林等人。艾尔。，<a class="ae oq" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank"> BERT:语言理解深度双向转换器预训练</a> (2019)，NAACL</p><p id="cd42" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae oq" href="https://www.udemy.com/course/nlp-with-transformers/?couponCode=MEDIUM" rel="noopener ugc nofollow" target="_blank">🤖带变压器的NLP课程</a></p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><p id="ac96" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你有兴趣了解更多关于使用NSP的另一半——MLM来微调伯特的知识，请阅读这篇文章:</p><div class="or os gp gr ot ou"><a rel="noopener follow" target="_blank" href="/masked-language-modelling-with-bert-7d49793e5d2c"><div class="ov ab fo"><div class="ow ab ox cl cj oy"><h2 class="bd iu gy z fp oz fr fs pa fu fw is bi translated">用BERT进行掩蔽语言建模</h2><div class="pb l"><p class="bd b dl z fp oz fr fs pa fu fw dk translated">towardsdatascience.com</p></div></div><div class="pc l"><div class="pd l pe pf pg pc ph ks ou"/></div></div></a></div></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><p id="f8bb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="oc">*除另有说明外，所有图片均出自作者之手</em></p></div></div>    
</body>
</html>