<html>
<head>
<title>Get started with MLOps</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">MLOps入门</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/get-started-with-mlops-fd7062cab018?source=collection_archive---------1-----------------------#2021-05-09">https://towardsdatascience.com/get-started-with-mlops-fd7062cab018?source=collection_archive---------1-----------------------#2021-05-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="594f" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="6cb0" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">包含开源工具的综合MLOps教程</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/d7888485a59b15cee5afa847f2a606b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dRurewSkFWJpboO5CVWhzA.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">pch.vector创建的人向量—【www.freepik.com T2】</p></figure><p id="3431" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">将机器学习(ML)模型投入生产是一项艰巨的工作。实际上，根据雄心的程度，这可能会异常困难。在这篇文章中，我将回顾一下我个人的想法(包括实现的例子),这些原则适合于在一个受监管的行业中将ML模型投入生产的旅程；也就是说，当一切都需要可审计、合规和受控时——在这种情况下，部署在EC2实例上的拼凑的API将无法满足要求。</p><p id="35f2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me">机器学习操作</em> (MLOps)指的是一种方法，其中DevOps和软件工程的组合以一种能够在生产中可靠有效地部署和维护ML模型的方式得到利用。可以在网上找到大量讨论MLOps的概念细节的信息，因此，本文将侧重于通过大量实际操作代码等来提高实用性。，基本上建立了一个基于开源工具的概念验证MLOps框架。你可以在GitHub 上找到最终代码<a class="ae lh" href="https://github.com/MathiasGruber/ProofOfConcept_MLOps" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="d281" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这一切都是为了将ML模型投入生产；但这意味着什么呢？在这篇文章中，我将考虑下面的概念列表，我认为它们应该被视为MLOps框架的一部分:</p><ol class=""><li id="94d1" class="mf mg it lk b ll lm lo lp lr mh lv mi lz mj md mk ml mm mn bi translated"><strong class="lk jd">开发平台:</strong>用于执行ML实验和授权数据科学家创建ML模型的协作平台应被视为MLOps框架的一部分。该平台应支持对数据源的安全访问(例如，从数据工程工作流)。我们希望从ML培训到部署的过渡尽可能平稳，这对于这样一个平台来说比在不同的本地环境中开发的ML模型更有可能。</li><li id="ef75" class="mf mg it lk b ll mo lo mp lr mq lv mr lz ms md mk ml mm mn bi translated"><strong class="lk jd">模型单元测试:</strong>每次我们创建、更改或重新训练模型时，我们都应该自动验证模型的完整性，例如<br/> -应该满足测试集的最低性能<br/> -应该在合成的用例特定数据集上表现良好</li><li id="27d4" class="mf mg it lk b ll mo lo mp lr mq lv mr lz ms md mk ml mm mn bi translated"><strong class="lk jd">版本化:</strong>应该可以及时返回，检查与给定模型相关的一切，例如，使用了什么数据&amp;代码。为什么？因为如果有东西坏了，我们需要能够回到过去，看看为什么。</li><li id="121e" class="mf mg it lk b ll mo lo mp lr mq lv mr lz ms md mk ml mm mn bi translated"><strong class="lk jd">模型注册:</strong>应该有已部署的&amp;退役ML模型、它们的版本历史以及每个版本的部署阶段的概述。为什么？如果出现问题，我们可以将之前归档的版本回滚到生产环境中。</li><li id="8ddc" class="mf mg it lk b ll mo lo mp lr mq lv mr lz ms md mk ml mm mn bi translated"><strong class="lk jd">模型治理:</strong>只有某些人应该有权查看与任何给定模型相关的培训。对于谁可以请求/拒绝/批准模型注册中心中部署阶段之间的转换(例如，从开发到测试到生产)，应该有访问控制。</li><li id="3406" class="mf mg it lk b ll mo lo mp lr mq lv mr lz ms md mk ml mm mn bi translated"><strong class="lk jd">部署:</strong>部署可以是很多事情，但是在这篇文章中，我考虑了我们想要将模型部署到云基础设施并公开API的情况，这使得其他人能够消费和使用该模型，也就是说，我没有考虑我们想要将ML模型部署到嵌入式系统中的情况。在适当的基础设施上高效的模型部署应该:<br/> -支持多个ML框架+定制模型<br/> -具有定义良好的API规范(例如，Swagger/OpenAPI) <br/> -支持容器化的模型服务器</li><li id="fa75" class="mf mg it lk b ll mo lo mp lr mq lv mr lz ms md mk ml mm mn bi translated"><strong class="lk jd">监控:</strong>跟踪性能指标(吞吐量、正常运行时间等。).为什么？如果一个模型突然开始返回错误或者出乎意料地慢，我们需要在最终用户抱怨之前知道，以便我们可以修复它。</li><li id="0f82" class="mf mg it lk b ll mo lo mp lr mq lv mr lz ms md mk ml mm mn bi translated"><strong class="lk jd">反馈:</strong>我们需要向模型反馈关于它表现如何的信息。为什么？通常，我们会对新样本进行预测，但我们还不知道基本事实。然而，当我们了解真相时，我们需要通知模型报告它实际做得有多好。</li><li id="119e" class="mf mg it lk b ll mo lo mp lr mq lv mr lz ms md mk ml mm mn bi translated"><strong class="lk jd"> A/B测试:</strong>无论我们认为我们在做多么可靠的交叉验证，我们永远不知道模型在实际部署之前会有怎样的表现。在MLOps框架内，使用真实模型进行A/B实验应该很容易。</li><li id="d572" class="mf mg it lk b ll mo lo mp lr mq lv mr lz ms md mk ml mm mn bi translated"><strong class="lk jd">漂移检测:</strong>通常，给定模型部署的时间越长，与模型被训练时相比，随着环境的变化，情况变得越糟。我们可以尝试对这些不同的情况进行监控和警告，或者在它们变得太严重之前进行“漂移”:<br/> <em class="me"> -概念漂移:</em>当输入和输出之间的关系已经改变时<br/> <em class="me"> -预测漂移:</em> <strong class="lk jd"> <em class="me"> </em> </strong>预测中的变化，但是模型仍然保持<br/> <em class="me"> -标签漂移:</em>模型的结果相对于训练数据的变化<br/> <em class="me"> -特征漂移:</em>分布中的变化</li><li id="53d1" class="mf mg it lk b ll mo lo mp lr mq lv mr lz ms md mk ml mm mn bi translated"><strong class="lk jd">异常值检测:</strong>如果部署的模型接收到与训练期间观察到的任何东西都显著不同的输入样本，我们可以尝试将该样本识别为潜在的异常值，并且返回的预测应该如此标记，这表明最终用户应该小心地信任该预测。</li><li id="0cd2" class="mf mg it lk b ll mo lo mp lr mq lv mr lz ms md mk ml mm mn bi translated"><strong class="lk jd">对抗性攻击检测:</strong>当对抗性样本攻击我们的模型时(例如，有人试图滥用/操纵我们算法的结果)，我们应该得到警告。</li><li id="9ef1" class="mf mg it lk b ll mo lo mp lr mq lv mr lz ms md mk ml mm mn bi translated"><strong class="lk jd">可解释性:</strong>ML部署应该支持端点返回我们预测的解释，例如，通过SHAP值。为什么？对于许多用例来说，预测是不够的，最终用户需要知道为什么要做出给定的预测。</li><li id="8516" class="mf mg it lk b ll mo lo mp lr mq lv mr lz ms md mk ml mm mn bi translated"><strong class="lk jd">部署的治理:</strong>我们不仅需要对谁可以查看数据、训练模型等进行访问限制。而且还取决于谁最终可以使用部署的模型。这些部署的模型通常就像它们被训练的数据一样保密。</li><li id="fbc9" class="mf mg it lk b ll mo lo mp lr mq lv mr lz ms md mk ml mm mn bi translated"><strong class="lk jd">以数据为中心:</strong>除了关注模型性能&amp;的改进，MLOps框架还能够更加关注终端用户如何提高数据质量和广度。</li></ol><p id="2810" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">当然，上述原则是对开发运维及软件工程的常规最佳实践的补充，包括代码、文档、单元测试、CI/CD等基础设施。在探索了MLOps的各种解决方案的应用之后(同时考虑到以上几点)，我将在这篇博客文章中探索的解决方案是<a class="ae lh" href="https://mlflow.org/" rel="noopener ugc nofollow" target="_blank">ml flow</a>+<a class="ae lh" href="https://www.seldon.io/tech/products/core/" rel="noopener ugc nofollow" target="_blank">Seldon-core</a>的组合——这些的优点是不需要昂贵的订阅。它们大部分都是开源的，这意味着我们可以直接设置和测试它们🙌 ...我将逐一讨论上述要点，看看这个堆栈如何应对每个挑战。这是一个很大的范围，所以也许在阅读之前喝杯咖啡。☕️</p><p id="1135" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><em class="me"> PS:我不属于mlflow或Seldon-core团队。虽然这篇文章包含了这些特定框架的大量代码，但这篇文章来自于一种技术不可知论者的心态，试图了解更多关于上述挑战的知识。</em></p><h2 id="aff2" class="mt mu it bd mv mw mx dn my mz na dp nb lr nc nd ne lv nf ng nh lz ni nj nk iz bi translated">1.发展</h2><p id="65d6" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">有多种工具可以解决数据科学家的ML实验管理和协作问题，例如<a class="ae lh" href="https://neptune.ai/" rel="noopener ugc nofollow" target="_blank"> Neptune.ai </a>、<a class="ae lh" href="https://wandb.ai/site" rel="noopener ugc nofollow" target="_blank">Weights&amp;bias</a>、<a class="ae lh" href="https://www.comet.ml/site/" rel="noopener ugc nofollow" target="_blank"> Comet.ml </a>和<a class="ae lh" href="https://mlflow.org/" rel="noopener ugc nofollow" target="_blank"> mlflow </a>。这里我们将重点关注mlflow，它既有一个<a class="ae lh" href="https://github.com/mlflow/mlflow" rel="noopener ugc nofollow" target="_blank">开源</a>版本，也有一个托管D <a class="ae lh" href="https://databricks.com/" rel="noopener ugc nofollow" target="_blank"> atabricks </a>解决方案。我们可以将Databricks部署为我们云基础架构的一部分(无前期成本，按使用付费)，从而为我们提供:</p><ul class=""><li id="4457" class="mf mg it lk b ll lm lo lp lr mh lv mi lz mj md nq ml mm mn bi translated">数据工程工作流的坚实基础(本文不会涉及)。这些数据可能是高度机密的，并且可能是我们不允许保存在本地计算机上的内容，需要这些数据来训练我们的模型。</li><li id="d6e2" class="mf mg it lk b ll mo lo mp lr mq lv mr lz ms md nq ml mm mn bi translated">一个可扩展的协作平台，用于在笔记本电脑上运行实际数据的数据科学实验，非常类似于jupyter笔记本电脑。</li><li id="7df6" class="mf mg it lk b ll mo lo mp lr mq lv mr lz ms md nq ml mm mn bi translated">用于训练给定ML模型的数据和代码的版本控制，以及与模型训练相关的信息(通过工具Delta Lake、Jobs API和mlflow)</li><li id="b066" class="mf mg it lk b ll mo lo mp lr mq lv mr lz ms md nq ml mm mn bi translated">用户管理，例如，在Azure Databricks的情况下，可以与许多公司使用的Azure AD链接。这使我们能够非常精细地控制谁可以看到什么，对我们的模型做什么。</li></ul><p id="861d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">要测试databricks+mlflow，首先要做的是创建一些基础设施。在这种情况下，我们将使用<a class="ae lh" href="https://www.terraform.io/" rel="noopener ugc nofollow" target="_blank"> terraform </a>部署到Azure，terraform是一个跨不同云提供商轻松配置基础设施的工具。我们将保持简单，只需使用以下脚本在一个文件夹中运行CLI命令<code class="fe nr ns nt nu b">terraform init &amp;&amp; terraform apply</code>:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nv nw l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">用于在Azure中设置Databricks工作空间和blob存储容器的Terraform脚本。作者代码。</p></figure><p id="1337" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">通过Terraform运行上面的脚本会在你的Azure帐户上设置一个Databricks工作区——如果你在Azure门户中导航到已创建的Databricks资源，你应该能够点击“启动工作区”，这会将你发送到新创建的Databricks工作区；到目前为止一切顺利。👌</p><p id="641c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在我们已经建立并运行了数据块，我们需要训练一个ML模型并将其登录到mlflow中-这可以对任何类型的自定义模型进行，但当我们使用流行的框架时，mlflow提供了许多方便的函数-在这种情况下，我们将利用mlflow的魔力，训练一个经典的Keras模型来对MNIST数据集中的手写数字进行分类。导航到Databricks左侧的“Workspace”选项卡，我们可以创建一个笔记本并运行以下代码片段:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nv nw l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">使用Keras训练MNIST分类器并将结果记录到mlflow的Python代码。作者代码。</p></figure><p id="f5f1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">注意:为了能够运行笔记本电脑，您必须将其连接到集群，但这可以使用UI轻松创建。</p><h2 id="e884" class="mt mu it bd mv mw mx dn my mz na dp nb lr nc nd ne lv nf ng nh lz ni nj nk iz bi translated">2.模型单元测试</h2><p id="690b" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">数据块使我们能够定义“作业”，这使我们能够用一组输入参数运行给定的笔记本。我们可以使用它来捆绑我们的训练代码，这样当我们希望重新训练我们的模型时，我们只需要重新运行我们的工作，这可以通过一个简单的REST请求来完成，而不是手动进入训练笔记本并重新训练模型。我们的模型训练的这种“可操作化”的一个关键部分是，我们在笔记本中包括检查，以确保模型仍然按照预期在例如测试数据集或合成数据集上工作。例如，可以将以下内容添加到我们之前的笔记本中:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nv nw l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">在测试数据集上评估我们的模型的代码片段，如果精度太低，则失败。作者代码</p></figure><p id="7b3a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在笔记本中添加了这些检查或“测试”之后，我们可以在Databricks中创建一个指向笔记本的“作业”。为了重新训练我们的模型，我们简单地告诉作业用笔记本创建一个“run”(这可以在UI中或者通过REST调用来完成)。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nx"><img src="../Images/f0f4a1ccec163bf86a07b7612bcb6627.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*18ywRANNI7GDJ_AkLWttrA.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">显示我们MNIST工作“运行”的屏幕截图——作者提供的屏幕截图。</p></figure><p id="f0f6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这正是我们想要的；例如，如果我们在已部署模型的性能开始下降太多时，自动向Databricks发送重新训练REST请求，但是由于某种原因，自上次训练以来，新的损坏数据被引入到数据集，从而破坏了模型，我们不希望该模型进入生产。</p><h2 id="b09e" class="mt mu it bd mv mw mx dn my mz na dp nb lr nc nd ne lv nf ng nh lz ni nj nk iz bi translated">3.版本控制</h2><p id="b752" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">一旦我们的模型训练完成，我们可以查看本笔记本附带的mlflow“实验”,在这里我们可以获得作为本笔记本一部分训练的模型的概述以及每个模型的指标和参数(例如，随时间推移的损耗等)。).我们可以点击“源”按钮(见下面的截图)，这将显示我们到底是什么代码被用来训练模型；这意味着我们将能够审计生产中的任何模型，以查看是谁以及如何训练它的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ny"><img src="../Images/2c30a8daddf0d54913fd7bd41cc81402.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HOfM55tClht1wD9DGytckA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">截图显示了我们的两个训练有素的MNIST模型的mlflow实验。作者截图。</p></figure><p id="9e85" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在上面的屏幕截图中，还有一个“Models”列，对于每次运行，mlflow都创建一个“mlflow model”对象，它在运行结束时封装模型。单击其中一个，我们将进入一个页面，其中包含有关模型的详细信息，包括有关训练期间的损失和准确性、训练参数的信息，最后是一个包含工件的部分，在这里我们可以看到封装的“mlflow model”，其中包含有关模型所需的<code class="fe nr ns nt nu b">conda.yaml</code>环境、训练的模型权重等信息。通过点击“注册模型”按钮，我们可以将这些模型中的任何一个输入到mlflow模型注册表中(当然，这也可以在代码中完成)</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nz"><img src="../Images/4dabf2a2c8011b7e42847ff7664983ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eS_IHS8jYTLPQrtsqMgqLA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">特定运行的mlflow工件的屏幕截图，特别关注于经过训练的mlflow模型。</p></figure><h2 id="132d" class="mt mu it bd mv mw mx dn my mz na dp nb lr nc nd ne lv nf ng nh lz ni nj nk iz bi translated">4.模型注册表</h2><p id="c760" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">一旦我们将一个或多个模型登录到我们的模型注册中心，我们可以在数据块中单击左侧的“模型”选项卡，以便获得我们的模型注册中心的概览，其中包含所有模型及其当前阶段:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oa"><img src="../Images/2db4880358def0b9941b81af7bff4044.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HoTTjv5ZUITGlddV6608ww.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">来自Databricks的模型注册表显示了每个模型、其最新版本、部署阶段等。</p></figure><p id="347e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这为我们提供了很多透明度，比如当前部署了什么，之前部署了哪些版本，以及到所有模型对象的链接，允许我们检查他们接受了哪些代码和数据的培训，等等。</p><h2 id="b49f" class="mt mu it bd mv mw mx dn my mz na dp nb lr nc nd ne lv nf ng nh lz ni nj nk iz bi translated">5.模型治理</h2><p id="4e5c" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">对于注册到我们的模型注册中心的每一个经过训练的模型，我们可以控制许可设置，这意味着我们可以控制谁可以看到模型注册中心中的哪些模型，以及谁可以授权从例如阶段到生产的转换；这正是我们想要的，因为我们希望有一个模型进入生产的控制门；例如，创建给定模型的数据科学家的经理可能需要在给定模型过渡到部署之前对其进行验证和批准，检查如下内容:</p><ul class=""><li id="df56" class="mf mg it lk b ll lm lo lp lr mh lv mi lz mj md nq ml mm mn bi translated">确保根据已知的测试集或通过交叉验证对模型进行了测试，以确认其有效。</li><li id="83ce" class="mf mg it lk b ll mo lo mp lr mq lv mr lz ms md nq ml mm mn bi translated">确保该模型已针对具有已知“信号”的合成数据集进行了测试如果它没有通过这种类型的测试，可能是引入了错误。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ob"><img src="../Images/256c37c36b105402ea1e6d066e634a3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kb7NlrLwL5rFuC5G3G9mFw.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">屏幕截图显示了模型注册表中给定模型的权限设置，演示了我们如何给不同的用户不同的权限。作者截图。</p></figure><h2 id="49dd" class="mt mu it bd mv mw mx dn my mz na dp nb lr nc nd ne lv nf ng nh lz ni nj nk iz bi translated">6.部署</h2><p id="abca" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">一旦模型注册中心中的模型已经转换到“生产”阶段，下一个阶段就是将该模型推送到部署服务器，以便该模型的消费者可以使用例如REST API来调用它。在本帖中，<a class="ae lh" href="https://github.com/SeldonIO/seldon-core/tree/v0.4.0" rel="noopener ugc nofollow" target="_blank">谢顿核心</a>将处理这些部署。简而言之，Seldon-core将ML模型包装并部署到一个<a class="ae lh" href="https://kubernetes.io/" rel="noopener ugc nofollow" target="_blank"> Kubernetes </a>集群上，Kubernetes是一个通用的开源系统，用于自动化部署、扩展和管理容器化的应用程序。为了测试所有这些，首先，我们需要一个Kubernetes集群，我们可以通过向terraform脚本添加一些配置来设置一个azure管理的Kubernetes集群:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nv nw l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">扩展以前的terraform脚本，在Azure上创建一个Kubernetes集群。作者代码。</p></figure><p id="9e15" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">将上述内容添加到我们之前的terraform脚本并重新运行<code class="fe nr ns nt nu b">terraform apply</code>,应该可以用一个非常简单的Kubernetes集群来更新我们的基础设施。既然我们已经启动并运行了一个集群，我们需要安装Seldon-core +相关的包，这可以通过以下命令来实现(取自官方文档):</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nv nw l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">在我们新创建的Kubernetes集群上安装Seldon-core的Bash命令</p></figure><p id="78e9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">就这样👌 ..现在，我们已经有了一个安装了Seldon-core的基本Kubernetes集群。作为设置的一部分，上面的命令还安装了<a class="ae lh" href="https://www.getambassador.io/" rel="noopener ugc nofollow" target="_blank"> Ambassador </a>，它控制对我们的Kubernetes集群的请求，当您运行上面的<code class="fe nr ns nt nu b">edgectl install</code>时，它应该已经打开了您的浏览器到您新设置的Ambassador域，这为我们提供了一个很好的界面来检查我们从我们的Kubernetes集群公开了什么，它是如何运行的，等等。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oc"><img src="../Images/3da7218a9779a895a912c35cd388660d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R_mh5zn_BGs2EvwcyQxv_w.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">我们新集群的大使界面截图。截图由作者拍摄。</p></figure><p id="61bc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">既然我们已经建立了Seldon-core的基础版本，我们可以开始探索如何部署我们的mlflow模型。可悲的是，我们将不得不做一些充实来实现这一点——即，尽管在Seldon-core中支持mlflow模型，但它不是以一个漂亮的大按钮的形式自动部署我们的模型。但是，请记住，部署模型所涉及的许多步骤都是可以自动化的——我将在下一篇博客文章中更全面地讨论这一点，可以在下面找到:</p><div class="od oe gp gr of og"><a rel="noopener follow" target="_blank" href="/making-mlops-easy-for-end-users-a3c22491e5e0"><div class="oh ab fo"><div class="oi ab oj cl cj ok"><h2 class="bd jd gy z fp ol fr fs om fu fw jc bi translated">让最终用户轻松使用MLOps</h2><div class="on l"><h3 class="bd b gy z fp ol fr fs om fu fw dk translated">使用开源工具简化MLOps的教程</h3></div><div class="oo l"><p class="bd b dl z fp ol fr fs om fu fw dk translated">towardsdatascience.com</p></div></div><div class="op l"><div class="oq l or os ot op ou lb og"/></div></div></a></div><p id="dc06" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">好吧。我们希望从Databricks模型注册表中获取mlflow模型，并将其部署到Seldon-core中。Seldon-core确实提供了一个开箱即用的mlflow包装器来部署mlflow模型，但对我来说，它总是与有问题的模型发生冲突。相反，我采用了一种更加定制的方法，我们需要创建一个与Seldon兼容的docker映像，我们可以将它部署到Seldon-core上。实际上，我发现这更好，因为它将允许我们在后面与漂移和异常值检测相关的章节中探索Seldon的更多边缘应用。首先，我们需要从Databricks mlflow服务器下载mlflow模型:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nv nw l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">获取一个Azure AD令牌，并使用它根据您的工作区对Databricks CLI进行身份验证。</p></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nv nw l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">用于将模型从数据块托管的mlflow下载到本地文件夹的脚本</p></figure><p id="405e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用上面的脚本，我们可以很容易地从mlflow下载一个给定的模型，例如，在我的例子中，我运行了<code class="fe nr ns nt nu b">python download_mlflow_model.py --model-name “Digit Classifier" --model_stage Production</code>。这将把训练好的mlflow模型下载到一个<code class="fe nr ns nt nu b">model/</code>文件夹中。从这里开始，我们只需要编写一个小的可重用python包装器，它让Seldon知道如何加载这个mlflow模型，并通过定义<code class="fe nr ns nt nu b">load()</code>和<code class="fe nr ns nt nu b">predict()</code>方法来执行预测。这个<code class="fe nr ns nt nu b">MyMlflowModel</code> python包装器是我们在mlflow和Seldon之间拥有的接口的核心部分，在这篇文章中我们会多次提到它。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nv nw l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">我们的mlflow的Python接口将由Seldon-core使用。模型是从我们的下载器/模型文件夹中加载的。</p></figure><p id="a942" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">有了下载的模型和python包装器，我们就可以用mlflow模型打包一个合适的docker映像了。这可以使用命令行工具<code class="fe nr ns nt nu b">s2i</code>轻松完成，它让我们将代码注入到由Seldon维护的docker映像中:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nv nw l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">将我们的mlflow模型捆绑在一个docker映像中，该映像被推送到dockerhub</p></figure><p id="73f9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">通读Seldon-core文档，有多种方法可以在部署之前本地测试这个docker映像，例如，通过启动docker映像并向其发送请求，或者使用Seldon-core提供的各种命令行工具。一旦确认它在本地工作，我们就必须将我们的映像推送到某个容器存储库——在这种情况下，我只使用公共dockerhub，但我们必须记住，模型也可以是高度机密的，因此在现实世界的设置中，docker映像存储库的安全性也必须考虑。最后，我们可以使用一个超级基本的Kubernetes <code class="fe nr ns nt nu b">mlflow.yaml</code>文件将我们训练好的mlflow模型推送到我们的Kubernetes集群，这个文件是用<code class="fe nr ns nt nu b">kubectl apply -f mlflow.yaml</code>应用的:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nv nw l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">我们在Kubernetes上部署mlflow模型的配置文件</p></figure><p id="cc67" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">一旦<code class="fe nr ns nt nu b">kubectl apply</code>命令成功，您应该能够转到您的大使端点(例如，<a class="ae lh" href="https://naughty-albattani-574.edgestack.me/seldon/seldon/mlflow-mnist/api/v1.0/doc/#/" rel="noopener ugc nofollow" target="_blank">https://YOUR _ URL/seldon/</a>mlflow models-namespace<a class="ae lh" href="https://naughty-albattani-574.edgestack.me/seldon/seldon/mlflow-mnist/api/v1.0/doc/#/" rel="noopener ugc nofollow" target="_blank">/ml flow-Mn ist/API/v 1.0/doc/</a>)来获取您新部署的模型的REST API的文档。该页面还将使您能够通过直接从浏览器向API发送请求来快速开始测试API:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ov"><img src="../Images/4a32e6da477d942fcd34c2e777f7c5c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iNDDwMjr4FHZrW4d9hTSNg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">显示我们部署的mlflow模型的Swagger UI的屏幕截图。作者截图。</p></figure><p id="308f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">或者，我们可以使用python通过整个测试数据集来访问我们的API:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nv nw l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">用于将整个测试MNIST数据集发送到我们部署的模型端点的Python脚本。</p></figure><h2 id="3ac4" class="mt mu it bd mv mw mx dn my mz na dp nb lr nc nd ne lv nf ng nh lz ni nj nk iz bi translated">7.监视</h2><p id="b986" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">我们部署的docker映像自动设置了许多监控样板文件，这些文件公开了大量的指标，这些指标可以被<a class="ae lh" href="https://prometheus.io/" rel="noopener ugc nofollow" target="_blank"> Prometheus </a>(一种常见的监控解决方案)抓取。Seldon-core附带了快速设置所有这些的工具，使我们能够在<a class="ae lh" href="https://grafana.com/" rel="noopener ugc nofollow" target="_blank"> Grafana </a>中可视化这些指标。本质上，我们所要做的就是在我们的Kubernetes集群上安装<code class="fe nr ns nt nu b">seldon-core-analytics</code>，这可以通过运行以下命令来完成:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nv nw l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">Bash命令用于安装Seldon-core-analytics并将Prometheus和Grafana端口转发到本地主机</p></figure><p id="94f9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">使用这些命令，我们在集群上安装了Prometheus和Grafana，并将其端口转发到我们的本地主机，这样我们就可以转到<code class="fe nr ns nt nu b">localhost:3001</code>查看Prometheus界面，该界面允许我们发送对所有记录的指标的查询，或者我们可以转到<code class="fe nr ns nt nu b">localhost:3000</code>查看预先填充的Grafana仪表板，该仪表板为我们提供请求率、延迟等的概述。，适用于所有当前部署的型号。本质上，使用Seldon-core设置监控非常简单，Grafana可以根据用户的需求和愿望方便地定制仪表盘。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ow"><img src="../Images/2b68f0fb3f8f57509b39744793a34ffe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cug7yAcdzIS1Z-72OYH6ig.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者提供的图片，展示了Grafana dashboard和我们部署的mlflow模型。</p></figure><h2 id="7a1e" class="mt mu it bd mv mw mx dn my mz na dp nb lr nc nd ne lv nf ng nh lz ni nj nk iz bi translated">8.反馈</h2><p id="eda5" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">您可能已经从Seldon公开的API文档中注意到，它自动公开了一个<code class="fe nr ns nt nu b">/feedback</code>端点和<code class="fe nr ns nt nu b">/predict</code>端点。此外，默认情况下，Grafana仪表板还提供了一个图表，显示来自我们模型的“奖励”信号。“反馈”和“奖励”使我们能够在部署模型后对其进行评估，因为我们逐渐开始获得带有已知标签的额外数据。在我们知道真实标签的地方获取新数据，我们可以给<code class="fe nr ns nt nu b">/feedback</code>端点发送一个给定的奖励，记录一个模型是表现得好(高奖励)还是差(低奖励)。例如，我们可以通过循环我们的测试数据来测试端点，并根据它的预测情况将奖励发送回模型:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nv nw l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">循环测试集，从模型中获得预测，基于性能向模型反馈奖励。</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ox"><img src="../Images/57f69161f9579f2093012a686daab6fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6A9HPRQQVGD6Pq71DR8-aQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">Grafana仪表板的屏幕截图，显示我们的模型表现良好，由奖励信号确定。</p></figure><p id="6053" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">除了直接发送奖励，值得注意的是，我们还可以在python模型包装器上覆盖<code class="fe nr ns nt nu b">send_feedback()</code>方法，使我们能够将数据样本和真实标签发送到<code class="fe nr ns nt nu b">/feedback</code>端点，然后在后端计算“奖励”(例如，在准确性方面)——本质上，在如何跟踪实时模型性能方面有很大的灵活性。</p><p id="4d3d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">注意，我们可以很容易地在Python包装器中实现一个阈值，当平均回报低于给定阈值时，它会自动向Databricks发送重新训练请求。🚀</p><h2 id="5810" class="mt mu it bd mv mw mx dn my mz na dp nb lr nc nd ne lv nf ng nh lz ni nj nk iz bi translated">9.A/B测试</h2><p id="5801" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">当引入新版本的模型时，您可能希望验证它是否比以前的模型更好。因此，我们可能希望在生产中运行AB测试，看看哪个模型的性能更好。Seldon-core使这一点变得非常容易，因为我们基本上只需要将<code class="fe nr ns nt nu b">traffic</code>关键字添加到我们之前的<code class="fe nr ns nt nu b">yaml</code>文件中，并添加另一个预测器。作为一个例子，让我们尝试添加另一个MNIST模型，该模型偶然只被训练了一个时期。在全面更换我们的旧型号之前，我们将80%的流量导向旧型号，20%导向新型号:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nv nw l"/></div></figure><p id="9527" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这样，80%的流量流向旧模型(版本0.1)，20%流向新模型(版本0.2)，两者都向我们的Grafana仪表板报告，在那里我们可以比较它们接收反馈信号时的表现。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/692ae5e567579ff9de27904567f8cfe1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1072/format:webp/1*zZAPudUOglRUoFDh-KVXow.png"/></div></figure><p id="d232" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这种情况下，两个模型实际上表现得差不多，因此仅训练1个时期的MNIST分类器不会导致模型更差，但也不会显著改善结果。然而，在某些情况下，模型实施或训练数据中的错误可能会通过我们之前的所有检查，因此为任务关键型部署执行这些AB测试可能是一个好主意，Seldon-core使它非常容易做到。</p><h2 id="c378" class="mt mu it bd mv mw mx dn my mz na dp nb lr nc nd ne lv nf ng nh lz ni nj nk iz bi translated">10.漂移检测</h2><p id="e80a" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">在这篇文章中选择Seldon-core的原因之一是他们维护并集成了库<a class="ae lh" href="https://github.com/SeldonIO/alibi-detect" rel="noopener ugc nofollow" target="_blank"> Alibi-Detect </a>，该库为漂移检测、敌对攻击检测和异常检测提供了预实现的算法——预实现这些算法可以节省大量时间，因为没有一种算法适合所有人，并且我们自己实现它们会很痛苦。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oz"><img src="../Images/e0f738f4743f9e9227c8bc597e2a042b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hpqVSiNCIZSNP5hSzTCwPg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">Alibi-Detect中的漂移检测算法支持(2021年5月4日)</p></figure><p id="30a1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">假设我们希望将一种漂移检测算法应用于我们的MNIST分类器。为了以Seldon-core推荐的方式完成这项工作，我们必须引入另一个工具，即“<a class="ae lh" href="https://knative.dev/docs/eventing/" rel="noopener ugc nofollow" target="_blank"> KNative </a>，它允许我们从Seldon-core API中提取有效载荷信息，并异步处理它，见下图:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pa"><img src="../Images/35adce00110a392b2387f8085b0a9cb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*Xzd8AEqfVZRcHXx-.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图片来自Seldon-Core文档</p></figure><p id="face" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">设置这个有点麻烦，因为它涉及安装Knative并设置它与Ambassador一起正常工作，训练Alibi-Detect模型并将其推送到blob存储，最后为我们的模型设置所需的Knative资源。不过，最终，在所有的设置之后，部署漂移检测算法并不比向Kubernetes应用另一个<code class="fe nr ns nt nu b">yaml</code>文件更复杂，指向预训练的漂移检测算法。</p><p id="e341" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了保持这篇文章的简洁，我选择不去经历这个棘手的过程，而是提到在谢顿核心文档中有关于如何继续的<a class="ae lh" href="https://docs.seldon.io/projects/seldon-core/en/latest/examples/outlier_cifar10.html" rel="noopener ugc nofollow" target="_blank">例子。作为替代，值得指出的是，我们可以选择直接在python模型包装类上实现自定义指标，这意味着我们可以在模型本身中包含预训练的漂移检测模型(基于Alibi-Detect ),从而让它与每个请求同步评估，并在Grafana中实时报告漂移。为此，我们可以回到Databricks笔记本，训练一个Kolmogorov-Smirnov漂移检测算法(检查数据分布的差异):</a></p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nv nw l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">为MNIST数据集训练KS漂移检测器并添加到mlflow模型伪影</p></figure><p id="d206" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这样，我们就可以将漂移检测算法训练并添加到我们的mlflow模型中，然后我们可以将它重新打包到Seldon-core docker映像中，并部署到我们的Kubernetes集群中。不过，在此之前，我们也将更新我们的python包装器类，以便对每个预测执行漂移检测:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nv nw l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">添加了漂移检测器的mlflow模型的Python包装器。</p></figure><p id="39ce" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">随着这个模型的部署，漂移现在由普罗米修斯刮出，我们可以很容易地在Grafana中创建一个显示漂移的图形。我们甚至可以添加警报阈值，这样，如果数据偏离训练数据太远，相关用户就会收到通知。这里有一个例子，最初我给模型输入了大量正常的MNIST图像，突然之间，输入了大量失真的样本，这些样本很容易被检测器检测到:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pb"><img src="../Images/231ee3d573a38eff556b6f1f83104a23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tMuLCXou4LB-NMzK3xaZFQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者截图来自Grafana仪表板</p></figure><p id="a2ff" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这种特定类型的漂移称为“特征漂移”或数据漂移，即输入数据特征的变化。然而，任何其他类型的漂移检测也可以直接在python包装器中实现。</p><h2 id="34a1" class="mt mu it bd mv mw mx dn my mz na dp nb lr nc nd ne lv nf ng nh lz ni nj nk iz bi translated">11.离群点检测</h2><p id="4c1b" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">与漂移检测类似，Alibi-Detect采用各种算法来识别输入样本是否为异常值。Seldon-core中推荐的实现类似于漂移检测，因为它涉及设置Knative并异步处理给定样本是否为异常值。为了简洁起见，我将再次跳过这个实现，而是链接到<a class="ae lh" href="https://docs.seldon.io/projects/seldon-core/en/latest/examples/outlier_cifar10.html" rel="noopener ugc nofollow" target="_blank">文档示例</a>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pc"><img src="../Images/f3d4a2094e3c235f70364d65a9c668a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SXC6LzTwX5TfcniEyfyZ7Q.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">Alibi-Detect中的异常值检测算法支持(2021年5月4日)</p></figure><p id="f854" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">除了设置Knative，值得一提的是，与我们如何在python模型包装器上实现自定义指标类似，我们也可以实现在预测时返回的自定义标记——因此，我们可以将异常值检测器与模型捆绑在一起，并同步处理进入异常值检测器中的<code class="fe nr ns nt nu b">/predict</code>端点的每个样本。一种想法是，如果样本是异常值，则返回一个标签，声明该样本与预测一起被认为是异常值。如果我们要尝试这样做，我们首先回到我们的Databricks笔记本，在那里我们可以训练异常值检测器并将其与我们的mlflow模型捆绑在一起:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nv nw l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">用于为MNIST数据集训练异常值预测值并将其绑定为mlflow模型工件的Python脚本。</p></figure><p id="a9c8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，我们只需要将离群点检测器添加到当前的python包装器中:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nv nw l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">我们的mlflow模型的Python包装器，增加了漂移和异常值检测器</p></figure><p id="250f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这样，我们可以将新的mlflow模型捆绑到一个新的Seldon-core docker映像中，并将其部署到Kubernetes。当使用明显是异常值的样本(例如，完全白色的图像)向预测端点发送REST请求时，我们现在得到以下响应:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nv nw l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">当我们发送白色图像时，来自部署具有漂移和异常值检测器的mlflow模型的样本响应。</p></figure><p id="b0c5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在，终端用户将立即能够看到，该预测是相当不确定的，因为与训练数据相比，输入数据被标记为异常值。</p><h2 id="cbb9" class="mt mu it bd mv mw mx dn my mz na dp nb lr nc nd ne lv nf ng nh lz ni nj nk iz bi translated">12.对抗性攻击检测</h2><p id="a375" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">机器学习算法可能很脆弱，因为通过非常轻微地改变我们给模型的输入，我们有时可以从根本上改变它的输出——我相信你们都在网上看到过这样的例子，对给定图像的细微改变可以改变其分类，例如从熊猫到长臂猿。在MLOps的上下文中，我们需要考虑一个坏演员可能会利用我们部署的一些机器学习模型的这一事实，试图确定如何优化算法的预测。根据不同的用例，这可能或多或少很关键。Alibi-Detect包再次派上了用场，它采用了一种<a class="ae lh" href="https://arxiv.org/abs/2002.09364" rel="noopener ugc nofollow" target="_blank">算法</a>来检测使用敌对自动编码器的“敌对”(被操纵)样本，使我们能够在部署的模型遇到异常数量的敌对数据时获得警报。</p><p id="04ea" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">与异常值和漂移检测一样，我们可以在模型代码中直接包含对抗性检测器；这样做的一个额外好处是，我们不仅可以使用对抗性检测器来检测我们的ML模型的攻击，还可以纠正对抗性示例，从而使攻击变得更加困难，详见<a class="ae lh" href="https://arxiv.org/abs/2002.09364" rel="noopener ugc nofollow" target="_blank">论文</a>。这一点的实现将非常简单地遵循异常值和漂移检测器的步骤，因此我选择在这里不包括它。</p><h2 id="ffbd" class="mt mu it bd mv mw mx dn my mz na dp nb lr nc nd ne lv nf ng nh lz ni nj nk iz bi translated">13.可解释性</h2><p id="37ae" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">除了Alibi-Detect，Seldon还维护和集成了一个库<a class="ae lh" href="https://github.com/SeldonIO/alibi" rel="noopener ugc nofollow" target="_blank"> Alibi </a>，它实现了各种用于模型解释的算法(例如，SHAP、集成梯度、锚点等)。这些都是以这样的方式实现的，它们在我们的REST API中提供了一个额外的<code class="fe nr ns nt nu b">/explain</code>端点，这样任何感兴趣的用户都可以很容易地查询给定预测的解释。这太棒了，因为这意味着当最终用户对为什么返回给定的预测有疑问时，他不需要手动执行大量的分析，而只需调用<code class="fe nr ns nt nu b">/explain</code>端点。</p><p id="16b4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们尝试向我们的MNIST模型添加一个“集成渐变”解释器——不同的解释器有不同的要求，但在这种情况下，解释器需要直接访问我们的Keras模型——因此，首先，我们将在我们的Databricks笔记本中保存模型，并上传到我们最初用Terraform创建的Azure blob:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nv nw l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">将我们的Keras模型保存到云中Azure blob存储的脚本。</p></figure><p id="2c14" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">注意:我们从Azure的存储帐户中获得的<code class="fe nr ns nt nu b">AZURE_CONNECT_STRING</code>。既然我们已经将解释器上传到blob存储器，我们可以扩展我们的Seldon部署的<code class="fe nr ns nt nu b">yaml</code>-文件以包含<code class="fe nr ns nt nu b">explainer</code>部分:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nv nw l"/></div></figure><p id="ab61" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">有了这个，我们就可以向一个端点发送格式为:https://[URL]/sel don/[space]/[name]-explainer/default/API/v 1.0/explain的请求，我们会收到一个包含每个像素对分类贡献的响应。对给定的样本进行可视化，我们会得到如下的图，显示哪些像素导致了给定标签的给定阳性分类。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nv nw l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">使用测试集中的示例图像调用/explain端点，并绘制返回的解释</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pd"><img src="../Images/41fa49dcc82a681eefa4b3ad22952fad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mYSJxBMSLmwQ5vdSVhSrZQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">结果图显示了发送到解释终点的样本(左)和像素属性(右)</p></figure><p id="d2ad" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这种方法非常有效，我们可以看到，该模型将重点放在了敏感像素上，以便进行预测。</p><h2 id="7813" class="mt mu it bd mv mw mx dn my mz na dp nb lr nc nd ne lv nf ng nh lz ni nj nk iz bi translated">14.部署治理</h2><p id="1ea3" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">重要的是，并非所有用户都可以访问给定的部署模型——基本上，最小特权原则应该始终适用:允许人们做他们需要做的事情，仅此而已。例如，我们可能不希望财务部门的人在一个模型周围闲逛，这个模型是根据R&amp;D部门的高度机密数据训练出来的。幸运的是，Kubernetes周围有很多工具可以解决这些问题，例如，在我们的例子中，我们使用托管Azure集群和Ambassador例如，我们可以使用Azure AD来认证我们的端点；<a class="ae lh" href="https://www.getambassador.io/docs/edge-stack/latest/howtos/sso/azure/" rel="noopener ugc nofollow" target="_blank">查看此链接</a>了解更多详情。这些安全方面的细节超出了本文的范围，但是解决方案肯定是有的。</p><p id="9a72" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">就治理而言，一个有趣的复杂情况是，最终端点上的访问权限应该与Databricks/mlflow中的模型注册中心内设置的权限一致，因此需要实现一些额外的工具来紧密同步这些权限。这种工具的细节也不在这篇博文的讨论范围之内。</p><h2 id="f74f" class="mt mu it bd mv mw mx dn my mz na dp nb lr nc nd ne lv nf ng nh lz ni nj nk iz bi translated">15.数据中心性</h2><p id="0748" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">最后但可能是最重要的一点是，我们如何改变MLOps的主要焦点，使其远离模型、性能、监控等。，并返回到数据。任何在数据科学领域工作过的人都知道，系统地提高数据质量、一致性和广度(涵盖边缘情况)将比追求更复杂和更花哨的模型带来更好的模型性能。</p><p id="b50b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如何构建以数据为中心的MLOps管道是一个有点悬而未决的问题，答案取决于使用情形，但一些观察结果如下:</p><ul class=""><li id="4d60" class="mf mg it lk b ll lm lo lp lr mh lv mi lz mj md nq ml mm mn bi translated">反馈监控、漂移检测和异常值识别的实现已经分别告诉了我们一些关于我们的模型在特定新样本上表现如何、它们与我们已经观察到的有多大不同以及我们是否应该怀疑新样本的准确性的信息。在重新训练我们的模型之前，这些是需要记住的重要指标；例如，如果我们确定某个样本是异常值(例如，传感器损坏)，我们就不应该将其包含在我们的重新训练中，对于我们知道旧模型表现不佳的样本，我们可能应该在添加到我们的数据集之前进行更彻底的调查。</li><li id="2a80" class="mf mg it lk b ll mo lo mp lr mq lv mr lz ms md nq ml mm mn bi translated">在一些我们可以选择收集什么数据的应用程序中，可以部署一个适合贝叶斯优化的附加模型；这样，部署的模型可以指导用户收集哪些额外的数据点。</li><li id="1518" class="mf mg it lk b ll mo lo mp lr mq lv mr lz ms md nq ml mm mn bi translated">类似于根据反馈进行重新训练，我们可以很容易地想象这样一种场景:我们收到了如此多的新数据，以至于重新训练模型实际上会提高性能，即使旧模型的性能并没有衰退。因此，不仅模型性能的变化将决定我们何时重新训练，而且可用训练数据的变化也将决定我们何时重新训练。</li></ul><h1 id="4190" class="pe mu it bd mv pf pg ph my pi pj pk nb ki pl kj ne kl pm km nh ko pn kp nk po bi translated">结论</h1><p id="df3b" class="pw-post-body-paragraph li lj it lk b ll nl kd ln lo nm kg lq lr nn lt lu lv no lx ly lz np mb mc md im bi translated">我经常被希望引入其最新和最棒的MLOps框架的公司接洽，仅基于营销材料来评估每种框架的利弊可能相当困难。正如我希望这篇文章所展示的那样，实现一个全面的MLOps框架绝不是一项简单的任务，有许多需要考虑的事项和缺陷需要考虑。</p><p id="e466" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">本帖中提出的概念验证解决方案涵盖了本帖中提出的15个原则的相当多的方面，除此之外，正在讨论的系统(mlflow + Seldon)支持许多其他概念，这些概念也可能与给定的MLOps解决方案相关。然而，mlflow+seldon的解决方案绝不是完美的，当我在这个框架中调试部署以使它们工作时，我不得不经历许多困难。在理想的场景中，我希望数据科学家能够专注于实现符合一些python接口实现方法的ML模型，如<code class="fe nr ns nt nu b">predict()</code>、<code class="fe nr ns nt nu b">feedback()</code>、<code class="fe nr ns nt nu b">drift()</code>、<code class="fe nr ns nt nu b">outlier()</code>、<code class="fe nr ns nt nu b">transform()</code>等。，然后能够按下一个绿色的“部署”按钮，启动一个自动化的管道，将接口容器化，并部署具有本文中讨论的所有功能的ML模型。要求普通市民数据科学家编写和调试Kubernetes部署(以及工件部署以保护blob存储等。)不太现实。此外，以这种手动方式进行新模型的每一次部署，对我来说似乎都不可扩展或者非常不容易，如果突然需要多人来将一个模型投入生产，就更不容易了。</p><p id="1b17" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">一种设想是实现一种工具，将mlflow与Seldon-core更紧密地联系起来。该工具将负责确保只有符合适当接口的模型才能被推送到mlflow模型注册中心中的“生产”阶段，并且一旦模型被提升到生产阶段，该工具将负责确保模型得到部署，权限从mlflow传播到Kubernetes，监控功能正常，等等。这种工具的愿景与我在博文中没有过多提及的最后一个原则联系在一起，那就是<em class="me">自动化的原则。</em>从本质上来说，MLOps框架要被认为是“完整的”，它应该包含我们在DevOps等领域看到的所有最佳实践，并自动化所有可以通过代码自动化的事情。</p><p id="8bbe" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">虽然我不认为mlflow + Seldon是一个“无缝”的MLOps框架，但我确实认为它代表了一个强有力的组合，并且我认为两者都是非常棒的工具。在一天结束的时候，我没有看到一个全面的MLOps系统是微不足道的，因此，在我的书中，所提出的解决方案是开源的这一事实是一个巨大的好处，因为这意味着我们可以很容易地自己填补任何缺失的漏洞。</p><p id="339e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在本系列的下一部分中，我将介绍如何结合使用Databricks和BentoML来为最终用户创建一个无缝的设置:</p><div class="od oe gp gr of og"><a rel="noopener follow" target="_blank" href="/making-mlops-easy-for-end-users-a3c22491e5e0"><div class="oh ab fo"><div class="oi ab oj cl cj ok"><h2 class="bd jd gy z fp ol fr fs om fu fw jc bi translated">让最终用户轻松使用MLOps</h2><div class="on l"><h3 class="bd b gy z fp ol fr fs om fu fw dk translated">使用开源工具简化MLOps的教程</h3></div><div class="oo l"><p class="bd b dl z fp ol fr fs om fu fw dk translated">towardsdatascience.com</p></div></div><div class="op l"><div class="oq l or os ot op ou lb og"/></div></div></a></div></div></div>    
</body>
</html>