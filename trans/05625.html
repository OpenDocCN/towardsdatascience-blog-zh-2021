<html>
<head>
<title>Contrasting Contrastive Learning Approaches</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对比对比学习方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/contrasting-contrastive-learning-approaches-c3eab8a4728c?source=collection_archive---------17-----------------------#2021-05-19">https://towardsdatascience.com/contrasting-contrastive-learning-approaches-c3eab8a4728c?source=collection_archive---------17-----------------------#2021-05-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="3d11" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="4b13" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">深入探讨哪些计算机视觉任务是很好的基准，数据集如何影响模型性能，以及哪种编码器是最好的通用主干。</h2></div><p id="5a6d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="lk">链接:</em><a class="ae ll" href="https://github.com/allenai/ViRB" rel="noopener ugc nofollow" target="_blank"><em class="lk">Github</em></a><em class="lk"/><a class="ae ll" href="https://arxiv.org/abs/2103.14005" rel="noopener ugc nofollow" target="_blank"><em class="lk">论文</em> </a></p><p id="97e7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">近年来，我们看到了计算机视觉领域新的自我监督学习方法的爆炸式增长——研究人员已经成功训练了神经网络，这些网络在公共基准上表现非常好，如使用大部分未标记数据的<a class="ae ll" href="https://imagenet.stanford.edu/" rel="noopener ugc nofollow" target="_blank"> ImageNet </a>分类。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi lm"><img src="../Images/d7304396790c09cdf0a802c24b62cd5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TKpDhS1emoqf6b_WYucatg.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">图文由<a class="ae ll" href="http://www.winsonhan.com/" rel="noopener ugc nofollow" target="_blank">温森韩</a></p></figure><p id="a086" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">事实证明，理解是什么使一幅图像与其他图像不同，就足以产生该图像的抽象表示，这种表示可用于现实世界的任务，如语义分类。这种方法的早期成功引发了大量描述这一主题的变体的出版物，这些变体彼此之间都有微小的改进。</p><p id="6961" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们现在有诸如<a class="ae ll" href="https://arxiv.org/abs/1912.01991" rel="noopener ugc nofollow" target="_blank"> PIRL </a>、<a class="ae ll" href="https://arxiv.org/abs/1807.03748" rel="noopener ugc nofollow" target="_blank"> CPC </a>、<a class="ae ll" href="https://arxiv.org/abs/2002.05709" rel="noopener ugc nofollow" target="_blank"> SimCLR </a>、<a class="ae ll" href="https://arxiv.org/abs/1911.05722" rel="noopener ugc nofollow" target="_blank"> MoCo </a>和<a class="ae ll" href="https://arxiv.org/abs/2006.09882" rel="noopener ugc nofollow" target="_blank"> SwAV </a>等方法，这些方法都使用一种称为对比学习的特定类型的自我监督学习来产生显著的结果，在对比学习中，编码器被训练来识别同一图像的稍微视觉增强版本，因为它们彼此相似，而与其他图像不同。</p><p id="ed4b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">虽然这种爆炸性的研究速度对于推进一个新的想法很有帮助，但它也产生了许多难以比较或整合的独立线索。在这篇博客中，我想谈谈自我监督的计算机视觉研究的现状和我最近发表的一篇论文的问题，以及旨在解决其中一些问题的<a class="ae ll" href="https://arxiv.org/abs/2103.14005" rel="noopener ugc nofollow" target="_blank">论文</a>和<a class="ae ll" href="http://gabrielilharco.com/" rel="noopener ugc nofollow" target="_blank"> Gabriel Ilharco </a>、<a class="ae ll" href="https://people.csail.mit.edu/ludwigs/" rel="noopener ugc nofollow" target="_blank"> Ludwig Schmidt </a>、<a class="ae ll" href="https://ehsanik.github.io/" rel="noopener ugc nofollow" target="_blank"> Kiana Eshani </a>和<a class="ae ll" href="https://roozbehm.info/" rel="noopener ugc nofollow" target="_blank"> Roozbeh Mottaghi </a>。</p><p id="ac87" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在我们深入探讨之前，让我们快速回顾几个关键术语，以及我将如何在本文中使用它们:</p><p id="5029" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">预训练算法:</strong>虽然术语“预训练算法”在深度学习中的定义相当松散，但在这篇文章中，我将使用它来描述最近流行的作品(如MoCo和SwAV)提出的整个预训练管道。</p><p id="840e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">预训练数据:</strong>这是用于计算机视觉编码器自我监督预训练的数据集。大部分作品为此使用ImageNet。</p><p id="e12d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">编码器:</strong>在计算机视觉中，我们通常将我们的网络分为两个组件:通用特征提取器，它将图像的原始像素数据编码为有用的抽象表示；以及终端任务网络，它使用该抽象表示来完成一些现实世界的任务。前者就是我在这篇博文中所说的编码器。</p><p id="8d38" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">终端任务网络:</strong>如上所述，终端任务网络是我们的模型的一部分，它被定制来执行特定的现实世界任务，如图像分类，因此它必须针对每个任务分别进行调整。</p><p id="faf9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">结束任务:</strong>结束任务是我们的模型可以执行的一些有用的任务。通常，这些都是实际的事情，比如从图像中估计房间的深度或者对狗的品种进行分类。最终任务是一种将我们的抽象模型与现实世界的工作联系起来的方式，人们可以从中受益。</p><p id="c694" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">终端任务数据:</strong>这是与特定终端任务相关联的训练数据集，用于训练终端任务网络对编码器产生的抽象图像表示做一些有用的事情。</p><p id="2d0c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">所以总而言之，像SwAV这样的<strong class="kq ja">预训练算法</strong>使用一个<strong class="kq ja">预训练数据集</strong>来训练一个<strong class="kq ja">编码器</strong>，这是一个从图像中提取抽象表示的通用工具。<strong class="kq ja">结束任务网络</strong>然后在<strong class="kq ja">结束任务数据</strong>上被训练，以使用这些抽象表示来执行一些有用的现实世界<strong class="kq ja">结束任务</strong>。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi mc"><img src="../Images/98fdd58e71942310ec20e3049318dcde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uqFAlWHprfGSQzjFdumLzg.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">图表来自<a class="ae ll" href="https://arxiv.org/abs/2103.14005" rel="noopener ugc nofollow" target="_blank">对比自我监督对比模型</a></p></figure></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><p id="1ef8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">既然我们都已经掌握了术语，让我们深入探讨一下自我监督视觉领域的快速创新带来的几个关键问题。</p><h1 id="6812" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated"><strong class="ak"> 1。苹果、橘子和香蕉</strong></h1><p id="ac97" class="pw-post-body-paragraph ko kp iq kq b kr nc ka kt ku nd kd kw kx ne kz la lb nf ld le lf ng lh li lj ij bi translated">虽然各种建议的训练算法都试图创建良好的通用图像编码器，但它们共享非常少的兼容数据点，这意味着将该算法应用于完全相同的模型架构，使用完全相同的端点使用完全相同的预训练数据。</p><p id="6b00" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">像这样的完全匹配的数据点集合通常会减少到只有一个:使用在ImageNet数据上训练的ResNet50的ImageNet分类性能。虽然这是一个很好的基准，但如果它是我们唯一关心的<strong class="kq ja">基准，它可能会变得极其危险。除此之外，不同的论文提供了最终任务、预训练数据集和模型架构的非重叠子集的结果，因此比较不同论文之间的数字常常会导致比较苹果和橙子。</strong></p><h1 id="3407" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated">2.我们到底在追求什么？</h1><p id="5752" class="pw-post-body-paragraph ko kp iq kq b kr nc ka kt ku nd kd kw kx ne kz la lb nf ld le lf ng lh li lj ij bi translated">由于ImageNet分类是大多数计算机视觉社区可以同步并达成一致的唯一基准，因此似乎真正追求的目标不是生产一个好的通用图像编码器，而是生产一个在ImageNet分类上表现良好的编码器，以及类似的最终任务。在某种程度上，任何开发新算法的研究人员都被迫追求这一基准，因为高分将给予算法更多的关注，但这无意中导致社区优化“ImageNet性能”的代理目标，而不是“良好的视觉编码器”的真正目标。结合大多数论文使用ImageNet作为训练数据的事实，我们有一个强大的反馈回路的配方，该反馈回路产生擅长学习数据集(如ImageNet)的底层分布统计的编码器，而不是擅长理解图像中的内容。</p><h1 id="b18e" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated"><strong class="ak">我们的工作</strong></h1><p id="ead8" class="pw-post-body-paragraph ko kp iq kq b kr nc ka kt ku nd kd kw kx ne kz la lb nf ld le lf ng lh li lj ij bi translated">希望这足以让你相信，在自我监督的计算机视觉领域存在着重大的不一致。现在让我们谈谈解决这些问题的方法。具体来说，我将谈论我最近的论文<a class="ae ll" href="https://arxiv.org/abs/2103.14005" rel="noopener ugc nofollow" target="_blank">对比对比自我监督表示学习模型</a>的方法和发现。</p><p id="e063" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了获得一个标准化的参考框架来比较各种自监督算法和预训练数据集，我们必须固定许多实验变量。我们所有的测试都是使用相同的编码器架构(ResNet50)完成的。在训练终端任务网络时，我们还冻结了编码器的权重。</p><p id="003b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">尽管冻结了这些变量，我们仍然使用数千小时的GPU时间运行了700多个实验。我们在4个不同的预训练数据集(<a class="ae ll" href="http://www.image-net.org/" rel="noopener ugc nofollow" target="_blank"> ImageNet </a>、<a class="ae ll" href="http://places2.csail.mit.edu/download.html" rel="noopener ugc nofollow" target="_blank"> Places </a>、<a class="ae ll" href="http://taskonomy.stanford.edu/" rel="noopener ugc nofollow" target="_blank"> Taskonomy </a>和<a class="ae ll" href="https://deepmind.com/research/open-source/kinetics" rel="noopener ugc nofollow" target="_blank"> Kinetics400 </a>)和4个组合上测试了由4个不同的预训练算法(<a class="ae ll" href="https://arxiv.org/abs/2006.09882" rel="noopener ugc nofollow" target="_blank"> SwAV </a>、<a class="ae ll" href="https://arxiv.org/abs/1911.05722" rel="noopener ugc nofollow" target="_blank"> MoCo v1 </a>、<a class="ae ll" href="https://arxiv.org/abs/2003.04297" rel="noopener ugc nofollow" target="_blank"> MoCo v2 </a>和<a class="ae ll" href="https://arxiv.org/abs/1912.01991" rel="noopener ugc nofollow" target="_blank"> PIRL </a>)产生的总共30个编码器。我们在20个最终任务训练集上为每个编码器训练了最终任务网络，并报告了这些编码器在最终任务测试集上产生的结果(参见下面的最终任务图)。</p><p id="2ef7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">更多详情请参考<a class="ae ll" href="https://www.semanticscholar.org/paper/Contrasting-Contrastive-Self-Supervised-Learning-Kotar-Ilharco/56f9922b8132000e3ab04e92c20a9218349354b7" rel="noopener ugc nofollow" target="_blank">文件</a>。</p><p id="b464" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在让我们深入研究结果…</p><h1 id="6b80" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated">ImageNet是一个好的基准吗？</h1><p id="ed8d" class="pw-post-body-paragraph ko kp iq kq b kr nc ka kt ku nd kd kw kx ne kz la lb nf ld le lf ng lh li lj ij bi translated">如上所述，在ImageNet分类结束任务中评估在ImageNet数据集上训练的模型似乎是相当循环的。为了衡量这一指标的好坏，我们对ImageNet上编码器的性能和其他终端任务进行了相关性分析。这意味着，对于任何给定的编码器，我们计算了ImageNet上的性能与其他终端任务的性能之间的关系。我们发现ImageNet根本不是一个很好的指标。虽然它在预测类似任务(如加州理工学院和CIFAR-100分类)的性能方面相当不错，但在预测不同任务(如深度预测)的性能方面确实很差。</p><p id="9365" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们根据任务类型(语义或结构)和输出形式(图像级或像素级)将任务大致分为四类。下面是我们所有最终任务及其相应分类的图示:</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi nh"><img src="../Images/a8817087b7e2dfa497fb2efe3a178bf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DnIMNmQSX90aOWwMYsItKw.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">来自<a class="ae ll" href="https://arxiv.org/abs/2103.14005" rel="noopener ugc nofollow" target="_blank">对比自我监督对比模型的图形</a></p></figure><p id="deca" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">下图描绘了ImageNet分类准确性与其他终端任务性能的关系。它说明了ImageNet性能对于其他图像级语义任务是一个很好的指标，但是对于所有其他终端任务类别是一个很弱的信号。此外，我们甚至看到一些负相关的结果，表明将编码器调优为非常擅长ImageNet分类会导致它忽略一些对其他任务类型至关重要的信息。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi ni"><img src="../Images/188e02dd34922e2eee4a1fbda038a61c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BYxL600WhAh2aq9D2PqlXQ.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">图片来自<a class="ae ll" href="https://arxiv.org/abs/2103.14005" rel="noopener ugc nofollow" target="_blank">对比自我监督对比模型</a></p></figure><p id="d9a5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">总而言之，这表明仅报告模型的ImageNet分类性能是非常有限的。</p><h1 id="0ad1" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated"><strong class="ak">所有预训练数据集都是平等的吗？</strong></h1><p id="8d0b" class="pw-post-body-paragraph ko kp iq kq b kr nc ka kt ku nd kd kw kx ne kz la lb nf ld le lf ng lh li lj ij bi translated">我们想探索的另一个领域是预训练数据对最终模型的质量有多大影响。由于这个领域的绝大多数工作都是在ImageNet上预先训练他们的编码器，所以在这个轴上没有太多的探索。我们在4个数据集上训练了MoCo v2和SwAV编码器:ImageNet、Places、Taskonomy和Kinetics400。我们对所有数据集进行子采样，以匹配ImageNet的大小，并对4个数据集的组合进行了训练。</p><p id="93c8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">首先，我们发现在ImageNet上训练的编码器倾向于最好地解决语义终端任务，而在Places上训练的编码器倾向于最好地解决结构终端任务。这很有意义，因为ImageNet包含许多不同的图像，而Places包含房间和建筑物的图像。此外，Places和ImageNet都对数据进行了策划、标记和组织，而Kinetics是从youtube视频中捕获的一系列帧，Taskonomy是一系列Matterport 3d扫描。这表明，尽管我们没有明确使用标签，但使用一个整洁有序的数据集仍然可能有一些优势。这就对在来自互联网的随机完全无监督数据上训练视觉模型的可行性提出了质疑——这是自我监督计算机视觉的伟大承诺之一。虽然最近的一些工作显示了在从互联网上收集的大型数据集上进行训练的成功，但尚不清楚这些数据有多干净和有组织。</p><p id="557c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">其次，我们测试了在类似于我们的终端任务数据的大型数据集上使用自我监督方法预训练我们的编码器是否会产生更好的编码器。对于我们的每个预训练数据集(ImageNet、Kinetics、Places和Taskonomy)，我们找到了使用相似数据集或相同数据集的子集(分别为加州理工学院分类、Kinetics动作预测、太阳场景分类和Taskonomy深度预测)的相应最终任务。我们绘制了我们在以下4个数据集上训练的所有编码器的最终任务性能:</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi nj"><img src="../Images/4806bccbe412237875f079c9a9db8777.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J5zZojFj2PYNnJjb5z-avA.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">来自<a class="ae ll" href="https://arxiv.org/abs/2103.14005" rel="noopener ugc nofollow" target="_blank">对比自我监督对比模型的图形</a></p></figure><p id="dc72" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这个结果对于监督学习来说有些明显，但是在我们的工作中，我们也验证了它对于对比学习也是成立的。有趣的是，我们发现组合数据集平均产生的编码器在所有任务中都相当好，但在任何任务中都不是最好的。事实上，我们还发现，在ImageNet和Places上训练的编码器平均性能优于组合编码器，因此混合数据集给我们带来的好处似乎少于缺点。</p><h1 id="25a5" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated">数据集平衡重要吗？</h1><p id="ef4d" class="pw-post-body-paragraph ko kp iq kq b kr nc ka kt ku nd kd kw kx ne kz la lb nf ld le lf ng lh li lj ij bi translated">除了上面提到的预训练数据集，我们还使用ImageNet的非平衡版本测试了预训练的效果，我们通过对每个类别中的图像数量进行对数采样来产生该非平衡版本(例如，我们从几个类别中获得许多样本，从许多类别中获得少量样本)。我们发现，如果我们在ImageNet的严重不平衡的子集上预训练我们的编码器，我们的最终任务性能不会比在相同大小的ImageNet的完全平衡的子集上预训练差。我们测试了每个数据集的3个不同样本，没有非常大的差异，这表明所有子采样都一样好，并且如果我们从子样本中的特定类获得许多样本，没有神奇的类会为我们提供巨大的性能提升。为了稍微破坏这一发现的兴奋，重要的是要提到我们只使用了200个时期训练的小数据集(250，000个样本)，因此需要进一步的工作来验证更大数据集和更长训练运行的这一趋势。</p><h1 id="da2d" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated">不同的预训练算法是否表现出不同的实力？</h1><p id="0f07" class="pw-post-body-paragraph ko kp iq kq b kr nc ka kt ku nd kd kw kx ne kz la lb nf ld le lf ng lh li lj ij bi translated">我们详细描述的两种训练算法是MoCo v2和SwAV。虽然不是我们工作的主要焦点，但我们的分析提出了两种算法的一些有趣的对比特性。</p><p id="3c67" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">MoCo v2往往更擅长于结构性任务，而SwAV在图像级任务上表现出色。关于为什么会发生这种情况，我的高层次假设是，由于SwAV在最后一层使用聚类方法，它往往会丢失一些空间图像信息。这个理论的一些支持来自我们对编码器进行的分层<a class="ae ll" href="https://arxiv.org/abs/1905.00414" rel="noopener ugc nofollow" target="_blank"> CKA </a>分析的结果。我们发现，平均而言，用MoCo v2训练的编码器在早期和晚期层表示之间具有更强的一致性，这表明在最终编码中保留了更多的空间信息。下图展示了MoCo和SwAV编码器在像素级和图像级任务中的性能差异:</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi nk"><img src="../Images/3a6f4659dff55bac9bb9656ed2a1624e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dKa3Bym4j1yxKouK79tXlA.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">图片来自<a class="ae ll" href="https://arxiv.org/abs/2103.14005" rel="noopener ugc nofollow" target="_blank">对比自我监督对比模型</a></p></figure><p id="39e9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如果我们试图为需要空间信息的任务构建自定义编码器，这可能是一个有用的数据点，因为我们现在有证据表明MoCo v2是这项工作的更好的预训练算法。在这里，我们看到了将ImageNet分类性能作为我们的基准的另一个缺点。由于SwAV在这一特定的最终任务上优于MoCo v2，许多人可能会认为它总体上更好，而我们的研究表明现实并不是如此清晰。</p><h1 id="c099" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated"><strong class="ak">自我监督编码器对所有下游任务都有用吗？</strong></h1><p id="d61e" class="pw-post-body-paragraph ko kp iq kq b kr nc ka kt ku nd kd kw kx ne kz la lb nf ld le lf ng lh li lj ij bi translated">简单来说，答案是肯定的。对于我们测试的每个任务，自我监督模型都表现得非常好，事实上，除了3个任务，它们都优于监督的ImageNet基线。受监督编码器表现较好的3个最终任务是ImageNet分类、ImageNet v2分类和Pets分类(与ImageNet非常相似)。由于我们没有为手头的任务微调编码器，这个结果一点也不奇怪，因为在ImageNet上训练的监督编码器在编码器训练期间有效地微调了任务。对于其他一切，自我监督的方法表现更好，这给了我们一个强烈的迹象，他们产生更好的通用编码器。</p><p id="0c25" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">此外，我们发现一些终端任务从使用自我监督模型中获得了比其他任务更大的提升，即结构性任务。下图显示，虽然一些自监督编码器在每个任务类别中都优于监督基准，但几乎所有的<strong class="kq ja">自监督编码器在结构化任务方面都优于监督基准，甚至那些预训练数据集和预训练算法与最终任务不匹配的编码器也是如此:</strong></p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi nl"><img src="../Images/30689e458a03bd2b33154a430906a923.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*baoPfd8kg6VHJOH55HG6AA.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">图片来自<a class="ae ll" href="https://arxiv.org/abs/2103.14005" rel="noopener ugc nofollow" target="_blank">对比自我监督对比模型</a></p></figure><h1 id="b366" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated">那么我应该使用什么编码器呢？</h1><p id="5b85" class="pw-post-body-paragraph ko kp iq kq b kr nc ka kt ku nd kd kw kx ne kz la lb nf ld le lf ng lh li lj ij bi translated">在考虑了所有上述结果之后，很明显当前的标准计算机视觉编码器(在ImageNet上用监督学习训练的ResNet50)通常不是最好的通用编码器。我们发现，一些经过自我监督学习训练的编码器在解决最终任务方面几乎总是更好，并且一个特定的编码器(在ImageNet上经过SwAV训练)在超过75%的最终任务方面更好。</p><p id="4023" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">下图显示了自监督模型相对于监督ImageNet的相对改进水平。它还表明，ImageNet和Places往往是实现上述最佳结果的两个数据集。</p><figure class="ln lo lp lq gt lr gh gi paragraph-image"><div role="button" tabindex="0" class="ls lt di lu bf lv"><div class="gh gi nm"><img src="../Images/f0f787fa9794cd40f4a21eee6e9c4c5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y2gTlzbKZ5-8rlEcXxMTyQ.png"/></div></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">图片来自<a class="ae ll" href="https://arxiv.org/abs/2103.14005" rel="noopener ugc nofollow" target="_blank">对比自我监督对比模型</a></p></figure></div><div class="ab cl md me hu mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="ij ik il im in"><h1 id="2b8b" class="mk ml iq bd mm mn nn mp mq mr no mt mu kf np kg mw ki nq kj my kl nr km na nb bi translated"><strong class="ak">更多未回答的问题</strong></h1><p id="635c" class="pw-post-body-paragraph ko kp iq kq b kr nc ka kt ku nd kd kw kx ne kz la lb nf ld le lf ng lh li lj ij bi translated">虽然我们在工作中进行了700多次实验，但我们只描绘了自我监督计算机视觉整体前景的一小部分。为了获得我们所做的详细结果，我们需要修正许多变量，这给我们留下了许多未解决的问题，例如:</p><ol class=""><li id="cb1e" class="ns nt iq kq b kr ks ku kv kx nu lb nv lf nw lj nx ny nz oa bi translated">模型架构如何影响不同自监督算法的性能？</li><li id="a588" class="ns nt iq kq b kr ob ku oc kx od lb oe lf of lj nx ny nz oa bi translated">微调整个编码器会显著影响性能吗？</li><li id="dac8" class="ns nt iq kq b kr ob ku oc kx od lb oe lf of lj nx ny nz oa bi translated">如果我们为更多的时代训练编码器，我们观察到的趋势会消失还是变得更明显？</li></ol><p id="e3f9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这些都是未来工作的良好起点，将进一步帮助我们理解自我监督计算机视觉的优点和缺点。</p><h1 id="3d0d" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated"><strong class="ak">结论</strong></h1><p id="91f2" class="pw-post-body-paragraph ko kp iq kq b kr nc ka kt ku nd kd kw kx ne kz la lb nf ld le lf ng lh li lj ij bi translated">虽然这篇博文指出了自我监督视觉领域当前工作的许多缺陷，但庆祝它的许多成就也很重要。我们的论文发现证据表明，对比学习方法在产生良好的通用编码器方面优于监督学习，进一步验证了这不仅仅是一个巧妙的技巧，而是一个真正的重要进步。然而，我们也已经表明，在单一维度(ImageNet分类)中衡量进展可能会导致我们忽略更大画面的某些部分(比如MoCo v2在我们测试的近一半最终任务中优于SwAV的事实)。</p><p id="0225" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">总之，我想从这项工作中提供4个关键的收获，可能对计算机视觉研究人员和工程师未来的计算机视觉项目有所帮助:</p><ol class=""><li id="4a21" class="ns nt iq kq b kr ks ku kv kx nu lb nv lf nw lj nx ny nz oa bi translated">自监督图像编码器是伟大的通用特征提取器，你应该考虑在你的下一个项目中使用它。我会推荐一个ResNet50，在ImageNet上用SwAV训练800个历元(或者Places，如果你的项目是结构化的)</li><li id="4b3e" class="ns nt iq kq b kr ob ku oc kx od lb oe lf of lj nx ny nz oa bi translated">如果您的领域中有大量的数据，考虑用它来训练一个自我监督的编码器，因为这可能会给您带来更大的性能提升。</li><li id="1c34" class="ns nt iq kq b kr ob ku oc kx od lb oe lf of lj nx ny nz oa bi translated">如果你正在开发一个新的自我监督模型，确保在各种各样的任务上对它进行评估。考虑使用我们在项目中发布的<a class="ae ll" href="https://github.com/allenai/ViRB" rel="noopener ugc nofollow" target="_blank"> ViRB </a>代码库。</li><li id="56bf" class="ns nt iq kq b kr ob ku oc kx od lb oe lf of lj nx ny nz oa bi translated">如果您正在开发一个新的数据集(或训练webly监督模式)，您的数据类的平衡可能不那么重要，但拥有一些不同的样本是重要的。</li></ol></div></div>    
</body>
</html>