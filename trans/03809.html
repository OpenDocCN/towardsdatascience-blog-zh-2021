<html>
<head>
<title>Featurizing text with Google’s T5 Text to Text Transformer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Google的T5文本到文本转换器来特征化文本</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/featurizing-text-with-googles-t5-text-to-text-transformer-a4855f49d8bd?source=collection_archive---------42-----------------------#2021-03-29">https://towardsdatascience.com/featurizing-text-with-googles-t5-text-to-text-transformer-a4855f49d8bd?source=collection_archive---------42-----------------------#2021-03-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6945" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何扩展<a class="ae ki" href="https://github.com/Featuretools/featuretools" rel="noopener ugc nofollow" target="_blank">特征工具</a> <a class="ae ki" href="https://docs.featuretools.com/automated_feature_engineering/primitives.html?__hstc=142826602.c56a6db62f4f0668decced094ba5d9fd.1608037254960.1616692672748.1616763943831.10&amp;__hssc=142826602.1.1616763943831&amp;__hsfp=3461249301" rel="noopener ugc nofollow" target="_blank"> <em class="kj">原语-函数</em> </a>从表格文本列自动创建<em class="kj">特征</em>。</h2></div><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi kk"><img src="../Images/e1e924aad18f39175481913f2c58b3ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*bXOWRCOUaho0Yzcj.gif"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated"><em class="kj">图片由</em> <a class="ae ki" href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html" rel="noopener ugc nofollow" target="_blank"> <em class="kj">谷歌</em> </a> <em class="kj">提供。</em>文本到文本框架图。每个任务都使用文本作为模型的输入，模型被训练生成一些目标文本。这允许相同的模型、损失函数和超参数跨越不同的任务集，包括翻译(绿色)、语言可接受性(红色)、句子相似性(黄色)和文档摘要(蓝色)。参见<a class="ae ki" href="https://arxiv.org/abs/1910.10683" rel="noopener ugc nofollow" target="_blank"> <em class="kj">用统一的文本到文本转换器探索迁移学习的极限</em> </a></p></figure></div><div class="ab cl la lb hx lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="im in io ip iq"><p id="aec0" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><em class="md">原载于2021年3月29日https://blog.ccganalytics.com</em><em class="md">的</em> <a class="ae ki" href="https://blog.ccganalytics.com/featurizing-text-with-googles-t5-text-to-text-transformer" rel="noopener ugc nofollow" target="_blank"> <em class="md">。</em></a></p></div><div class="ab cl la lb hx lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="im in io ip iq"><p id="46ef" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在本文中，我们将演示如何使用Google的T5文本到文本转换器来特征化表格数据中的文本。您可以使用这个库中的Jupyter笔记本继续学习。</p><p id="f2d7" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">当试图在机器学习管道中利用真实世界的数据时，经常会遇到书面文本，例如，在预测房地产估值时，有许多数字特征，例如:</p><ul class=""><li id="a9e0" class="me mf it lj b lk ll ln lo lq mg lu mh ly mi mc mj mk ml mm bi translated">“卧室数量”</li><li id="184d" class="me mf it lj b lk mn ln mo lq mp lu mq ly mr mc mj mk ml mm bi translated">“浴室数量”</li><li id="e313" class="me mf it lj b lk mn ln mo lq mp lu mq ly mr mc mj mk ml mm bi translated">"平方英尺的面积"</li><li id="9373" class="me mf it lj b lk mn ln mo lq mp lu mq ly mr mc mj mk ml mm bi translated">“纬度”</li><li id="c56c" class="me mf it lj b lk mn ln mo lq mp lu mq ly mr mc mj mk ml mm bi translated">“经度”</li><li id="2503" class="me mf it lj b lk mn ln mo lq mp lu mq ly mr mc mj mk ml mm bi translated">&amp;等等…</li></ul><p id="b720" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">但是也有大量的文字，比如在Zillow网站上的房地产列表描述中。这些文本数据可能包含许多有价值的信息，这些信息在其他情况下不会被考虑在内，例如:</p><ul class=""><li id="20af" class="me mf it lj b lk ll ln lo lq mg lu mh ly mi mc mj mk ml mm bi translated">提及开放式厨房/平面图</li><li id="6fdf" class="me mf it lj b lk mn ln mo lq mp lu mq ly mr mc mj mk ml mm bi translated">提到花岗岩柜台</li><li id="a24c" class="me mf it lj b lk mn ln mo lq mp lu mq ly mr mc mj mk ml mm bi translated">提及硬木地板</li><li id="7a9c" class="me mf it lj b lk mn ln mo lq mp lu mq ly mr mc mj mk ml mm bi translated">提及不锈钢器具</li><li id="91ca" class="me mf it lj b lk mn ln mo lq mp lu mq ly mr mc mj mk ml mm bi translated">提及最近的装修</li><li id="2d61" class="me mf it lj b lk mn ln mo lq mp lu mq ly mr mc mj mk ml mm bi translated">&amp;等等…</li></ul><p id="6bf1" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然而，令人惊讶的是，许多AutoML工具完全忽略了这些信息，因为流行的表格算法(如XGBoost)不能直接使用书面文本。</p><p id="996b" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这就是<a class="ae ki" href="https://github.com/Featuretools/featuretools" rel="noopener ugc nofollow" target="_blank">功能工具</a> <a class="ae ki" href="https://docs.featuretools.com/automated_feature_engineering/primitives.html?__hstc=142826602.c56a6db62f4f0668decced094ba5d9fd.1608037254960.1616692672748.1616763943831.10&amp;__hssc=142826602.1.1616763943831&amp;__hsfp=3461249301" rel="noopener ugc nofollow" target="_blank"> <em class="md">原始函数</em> </a>的用武之地。Featuretools旨在为不同类型的数据(包括文本)自动创建<em class="md">功能</em>，然后这些功能可以被表格形式的机器学习模型使用。</p><p id="b3ed" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在本文中，我们展示了如何扩展<a class="ae ki" href="https://github.com/FeatureLabs/nlp_primitives" rel="noopener ugc nofollow" target="_blank"> nlp-primitives库</a>以用于Google最先进的T5文本到文本转换器模型，在这样做的过程中，我们创建了最重要的nlp原语特性，这反过来又提高了Alteryx博客<a class="ae ki" href="https://innovation.alteryx.com/natural-language-processing-featuretools/" rel="noopener ugc nofollow" target="_blank">自动特征工程的自然语言处理</a>中展示的准确性。</p><h2 id="7ee9" class="ms mt it bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj nk bi translated">关于T5</h2><p id="39ad" class="pw-post-body-paragraph lh li it lj b lk nl ju lm ln nm jx lp lq nn ls lt lu no lw lx ly np ma mb mc im bi translated">对于任何不熟悉T5的读者来说，T5模型是在谷歌的论文中提出的，题为<a class="ae ki" href="https://arxiv.org/pdf/1910.10683.pdf" rel="noopener ugc nofollow" target="_blank">用统一的文本到文本转换器探索迁移学习的极限</a>，作者是<em class="md">科林·拉弗尔、诺姆·沙泽尔、、凯瑟琳·李、·纳朗、迈克尔·马特纳、周燕琪、和彼得·刘。</em>以下是摘要:</p><blockquote class="nq nr ns"><p id="ac96" class="lh li md lj b lk ll ju lm ln lo jx lp nt lr ls lt nu lv lw lx nv lz ma mb mc im bi translated">迁移学习已经成为自然语言处理(NLP)中的一种强大技术，在迁移学习中，模型首先在数据丰富的任务上进行预训练，然后在下游任务上进行微调。迁移学习的有效性已经产生了各种各样的途径、方法和实践。在本文中，我们通过引入一个统一的框架，将每一个语言问题转换成文本到文本的格式，来探索自然语言处理中迁移学习技术的前景。我们的系统研究比较了几十个语言理解任务的预训练目标、架构、未标记数据集、迁移方法和其他因素。通过将我们的探索与scale和我们新的“庞大干净的爬行语料库”相结合，我们在涵盖摘要、问题回答、文本分类等许多基准上实现了最先进的结果。为了促进NLP迁移学习的未来工作，我们发布了我们的数据集、预训练模型和代码。</p></blockquote><h1 id="ba6c" class="nw mt it bd mu nx ny nz mx oa ob oc na jz od ka nd kc oe kd ng kf of kg nj og bi translated">一个机器学习演示，使用拥抱脸T5来表征文本</h1><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/0d081b252e6b0f0e753f1bfcb3f88da5.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*mUboKLRV82vS6FN7leme_w.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">罗纳德·里根在1985年《变形金刚》英国年会上遇见擎天柱。在NLP的背景下——Hugging Face Transformers是一个自然语言处理库，一个hub现在对所有ML模型开放，支持库有<a class="ae ki" href="https://github.com/flairNLP/flair" rel="noopener ugc nofollow" target="_blank"> Flair </a>、<a class="ae ki" href="https://github.com/asteroid-team/asteroid" rel="noopener ugc nofollow" target="_blank"> Asteroid </a>、<a class="ae ki" href="https://github.com/espnet/espnet" rel="noopener ugc nofollow" target="_blank"> ESPnet </a>、<a class="ae ki" href="https://github.com/pyannote/pyannote-audio" rel="noopener ugc nofollow" target="_blank"> Pyannote </a>等等。</p></figure><p id="daca" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为了扩展用于T5的NLP原语库，我们将构建两个定制的<code class="fe oi oj ok ol b">TransformPrimitive</code>类。出于实验目的，我们测试了两种方法:</p><ul class=""><li id="4bbf" class="me mf it lj b lk ll ln lo lq mg lu mh ly mi mc mj mk ml mm bi translated">微调<a class="ae ki" href="https://huggingface.co/t5-base" rel="noopener ugc nofollow" target="_blank">抱紧面T5-底座</a></li><li id="30ba" class="me mf it lj b lk mn ln mo lq mp lu mq ly mr mc mj mk ml mm bi translated">一个现成的<a class="ae ki" href="https://huggingface.co/mrm8488/t5-base-finetuned-imdb-sentiment" rel="noopener ugc nofollow" target="_blank">拥抱脸T5模型为情绪分析预先调整</a></li></ul><p id="847b" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">首先，让我们加载基本模型。</p><pre class="kl km kn ko gt om ol on oo aw op bi"><span id="61c8" class="ms mt it ol b gy oq or l os ot">from simpletransformers.t5 import T5Model</span><span id="b06c" class="ms mt it ol b gy ou or l os ot">model_args = {<br/>    "max_seq_length": 196,<br/>    "train_batch_size": 8,<br/>    "eval_batch_size": 8,<br/>    "num_train_epochs": 1,<br/>    "evaluate_during_training": True,<br/>    "evaluate_during_training_steps": 15000,<br/>    "evaluate_during_training_verbose": True,<br/>    "use_multiprocessing": False,<br/>    "fp16": False,<br/>    "save_steps": -1,<br/>    "save_eval_checkpoints": False,<br/>    "save_model_every_epoch": False,<br/>    "reprocess_input_data": True,<br/>    "overwrite_output_dir": True,<br/>    "wandb_project": None,<br/>}</span><span id="d9f6" class="ms mt it ol b gy ou or l os ot">model = T5Model("t5", "t5-base", args=model_args)</span></pre><p id="e2f6" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">其次，让我们加载预调好的模型。</p><pre class="kl km kn ko gt om ol on oo aw op bi"><span id="5b9e" class="ms mt it ol b gy oq or l os ot">model_pretuned_sentiment = T5Model('t5',<br/>                                   'mrm8488/t5-base-finetuned-imdb-sentiment',<br/>                                   use_cuda=True)<br/>model_pretuned_sentiment.args</span></pre><p id="22f8" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为了微调<code class="fe oi oj ok ol b">t5-base</code>模型，我们需要重新组织和格式化用于训练的数据。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi ov"><img src="../Images/c610a8d14900e9d0873fc86981582267.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*38JAzo7-yp7Ft8EO.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">图片作者。原始Kaggle数据集</p></figure><p id="ed42" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">从Kaggle数据集中，我们将把<code class="fe oi oj ok ol b">review_text</code>列映射到一个名为<code class="fe oi oj ok ol b">input_text</code>的新列，并且我们将把<code class="fe oi oj ok ol b">review_rating</code>列映射到一个名为<code class="fe oi oj ok ol b">target_text</code>的新列，这意味着<code class="fe oi oj ok ol b">review_rating</code>是我们试图预测的。这些变化符合用于微调t5的Simpletransformers库接口，其中主要的额外需求是指定一个“前缀”，这意味着有助于多任务训练(注意:在本例中，我们关注的是单个任务，因此前缀是不必要的，但是为了便于使用，我们还是要定义它)。</p><pre class="kl km kn ko gt om ol on oo aw op bi"><span id="dfe7" class="ms mt it ol b gy oq or l os ot">dft5 = df[['review_text','review_rating']<br/>].rename({<br/>'review_text':'input_text',<br/>'review_rating':'target_text'<br/>},axis=1)</span><span id="23b1" class="ms mt it ol b gy ou or l os ot">dft5['prefix'] = ['t5-encode' for x in range(len(dft5))]</span><span id="ac15" class="ms mt it ol b gy ou or l os ot">dft5['target_text'] = dft5['target_text'].astype(str)</span><span id="c923" class="ms mt it ol b gy ou or l os ot">dft5</span></pre><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="ab gu cl ow"><img src="../Images/0f3e1ae6f7c0f444e068c2fe7903f7e6.png" data-original-src="https://miro.medium.com/v2/format:webp/1*6MgcvDh7ljoBLodmS-ObIQ.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">输出。图片作者。</p></figure><p id="3673" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">本例中的目标文本是消费者对给定餐馆的评分。我们可以通过以下方式轻松地微调此任务的T5模型:</p><pre class="kl km kn ko gt om ol on oo aw op bi"><span id="c3a6" class="ms mt it ol b gy oq or l os ot">from sklearn.model_selection import train_test_split</span><span id="ff17" class="ms mt it ol b gy ou or l os ot">train_df, eval_df = train_test_split(dft5)</span><span id="42e5" class="ms mt it ol b gy ou or l os ot">model.train_model(train_df, eval_data=eval_df)</span></pre><p id="f553" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">接下来，我们加载预调好的拥抱脸模型。</p><pre class="kl km kn ko gt om ol on oo aw op bi"><span id="5421" class="ms mt it ol b gy oq or l os ot">from sklearn.model_selection import train_test_split</span><span id="e573" class="ms mt it ol b gy ou or l os ot">train_df, eval_df = train_test_split(dft5)</span><span id="a969" class="ms mt it ol b gy ou or l os ot">model.train_model(train_df, eval_data=eval_df)</span></pre><p id="7ea8" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们测试这两个模型，以更好地了解它们将预测什么。</p><pre class="kl km kn ko gt om ol on oo aw op bi"><span id="819e" class="ms mt it ol b gy oq or l os ot">test = ['Great drinks and food', list(np.array(model.predict(test)).astype(float))<br/> 'Good food &amp;amp; beer',</span><span id="5252" class="ms mt it ol b gy ou or l os ot">Generating outputs: 0%| | 0/1 [00:00&lt;?, ?it/s] Generating outputs: 100%|██████████| 1/1 [00:00&lt;00:00, 3.17it/s] Generating outputs: 100%|██████████| 1/1 [00:00&lt;00:00, 3.16it/s] Decoding outputs: 0%| | 0/3 [00:00&lt;?, ?it/s] Decoding outputs: 33%|███▎ | 1/3 [00:00&lt;00:01, 1.14it/s] Decoding outputs: 100%|██████████| 3/3 [00:00&lt;00:00, 3.43it/s] Out[14]: [4.0, 4.0, 4.0] <br/> 'Pretty good beers']</span></pre><p id="c0b3" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们可以看到，微调后的模型输出了一个<code class="fe oi oj ok ol b">review_rankings</code>【4.0，4.0，4.0】的列表，这是试图预测我们问题的最终答案。</p><p id="bd1f" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">接下来，我们用预先调好的拥抱脸模型做一个<em class="md">测试预测</em>。</p><pre class="kl km kn ko gt om ol on oo aw op bi"><span id="8a92" class="ms mt it ol b gy oq or l os ot">test = ['Great drinks and food', <br/>     'Good food &amp;amp; beer', <br/>     'Pretty good beers']</span><span id="e03c" class="ms mt it ol b gy ou or l os ot">list(np.where(np.array(model_pretuned_sentiment.predict(test))=='positive', 1.0, 0.0))</span><span id="ff0f" class="ms mt it ol b gy ou or l os ot"> Generating outputs:   0%|          | 0/1 [00:00&lt;?, ?it/s] Generating outputs: 100%|██████████| 1/1 [00:00&lt;00:00,  7.57it/s] Generating outputs: 100%|██████████| 1/1 [00:00&lt;00:00,  7.56it/s]  Decoding outputs:   0%|          | 0/3 [00:00&lt;?, ?it/s] Decoding outputs:  33%|███▎      | 1/3 [00:00&lt;00:01,  1.17it/s] Decoding outputs: 100%|██████████| 3/3 [00:00&lt;00:00,  3.50it/s] Out[15]: [1.0, 1.0, 1.0]</span></pre><p id="7b54" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">请注意，预调优的模型输出一系列布尔真/假值，这些值指示语句是<code class="fe oi oj ok ol b">positive</code>还是<code class="fe oi oj ok ol b">negative</code>——我们将这些值转换为浮点值，以便更好地与表格建模集成。在这种情况下，所有值都为真，因此输出变为[1.0，1.0，1.0]。</p><p id="7315" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在我们已经加载了T5的两个版本，我们可以构建<code class="fe oi oj ok ol b">TransformPrimitive</code>类，它将集成NLP原语和Featuretools库。</p><pre class="kl km kn ko gt om ol on oo aw op bi"><span id="eb89" class="ms mt it ol b gy oq or l os ot">from featuretools.primitives.base import TransformPrimitive<br/>from featuretools.variable_types import Numeric, Text<br/></span><span id="8b76" class="ms mt it ol b gy ou or l os ot">class T5Encoder(TransformPrimitive):<br/>    <br/>    name = "t5_encoder"<br/>    input_types = [Text]<br/>    return_type = Numeric<br/>    default_value = 0<br/>    <br/>    def __init__(self, model=model):<br/>      self.model = model</span><span id="b711" class="ms mt it ol b gy ou or l os ot">    def get_function(self):</span><span id="a03e" class="ms mt it ol b gy ou or l os ot">        def t5_encoder(x):<br/>            model.args.use_multiprocessing = True<br/>            return list(np.array(model.predict(x.tolist())).astype(float))<br/>        return t5_encoder</span></pre><p id="71b0" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">上面的代码创建了一个名为<code class="fe oi oj ok ol b">T5Encoder</code>的新类，它将使用经过<strong class="lj iu"> <em class="md">微调的</em> </strong> T5模型，下面的代码创建了一个名为<code class="fe oi oj ok ol b">T5SentimentEncoder</code>的新类，它将使用经过<strong class="lj iu"> <em class="md">预调的</em> </strong> T5模型。</p><pre class="kl km kn ko gt om ol on oo aw op bi"><span id="8076" class="ms mt it ol b gy oq or l os ot">class T5SentimentEncoder(TransformPrimitive):<br/>    <br/>    name = "t5_sentiment_encoder"<br/>    input_types = [Text]<br/>    return_type = Numeric<br/>    default_value = 0<br/>    <br/>    def __init__(self, model=model_pretuned_sentiment):<br/>      self.model = model</span><span id="e300" class="ms mt it ol b gy ou or l os ot">    def get_function(self):</span><span id="b796" class="ms mt it ol b gy ou or l os ot">        def t5_sentiment_encoder(x):<br/>            model.args.use_multiprocessing = True<br/>            return list(np.where(np.array(model_pretuned_sentiment.predict(x.tolist()))=='positive',1.0,0.0))<br/>        return t5_sentiment_encoder</span></pre><p id="c44d" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">Featuretools现在将知道如何使用T5来特征化文本列，它甚至会使用T5输出来计算聚合，或者对其执行操作，例如从其他特性中减去该值。定义了这些新类后，我们只需将它们与默认类一起以所需的Featuretools格式进行汇总，这将使它们可用于自动化特征工程。</p><pre class="kl km kn ko gt om ol on oo aw op bi"><span id="a6cd" class="ms mt it ol b gy oq or l os ot">trans = [<br/>           T5Encoder,<br/>           T5SentimentEncoder,<br/>           DiversityScore,<br/>           LSA,<br/>           MeanCharactersPerWord,<br/>           PartOfSpeechCount,<br/>           PolarityScore, <br/>           PunctuationCount,<br/>           StopwordCount,<br/>           TitleWordCount,<br/>           UniversalSentenceEncoder,<br/>           UpperCaseCount<br/>        ]</span><span id="4f3e" class="ms mt it ol b gy ou or l os ot">ignore = {'restaurants': ['rating'],<br/>          'reviews': ['review_rating']}</span><span id="fe28" class="ms mt it ol b gy ou or l os ot">drop_contains = ['(reviews.UNIVERSAL']</span><span id="8eaa" class="ms mt it ol b gy ou or l os ot">features = ft.dfs(entityset=es,<br/>                  target_entity='reviews',<br/>                  trans_primitives=trans,<br/>                  verbose=True,<br/>                  features_only=True,<br/>                  ignore_variables=ignore,<br/>                  drop_contains=drop_contains,<br/>                  max_depth=4)</span></pre><p id="6daf" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">正如你在下面的输出中看到的，Featuretools库非常强大！事实上，除了这里展示的T5特性，它还使用指定的所有其他NLP原语创建了数百个特性，非常酷！</p><pre class="kl km kn ko gt om ol on oo aw op bi"><span id="1801" class="ms mt it ol b gy oq or l os ot">feature_matrix = ft.calculate_feature_matrix(features=features,<br/>                                             entityset=es,<br/>                                             verbose=True)</span><span id="b105" class="ms mt it ol b gy ou or l os ot">features</span></pre><ul class=""><li id="b479" class="me mf it lj b lk ll ln lo lq mg lu mh ly mi mc mj mk ml mm bi translated"><feature: t5_encoder=""/></li><li id="d2af" class="me mf it lj b lk mn ln mo lq mp lu mq ly mr mc mj mk ml mm bi translated"><feature: t5_sentiment_encoder=""/></li><li id="8f79" class="me mf it lj b lk mn ln mo lq mp lu mq ly mr mc mj mk ml mm bi translated"><feature: restaurants.max=""/></li><li id="2af1" class="me mf it lj b lk mn ln mo lq mp lu mq ly mr mc mj mk ml mm bi translated"><feature: restaurants.max=""/></li><li id="62f6" class="me mf it lj b lk mn ln mo lq mp lu mq ly mr mc mj mk ml mm bi translated"><feature: restaurants.mean=""/></li><li id="5bcb" class="me mf it lj b lk mn ln mo lq mp lu mq ly mr mc mj mk ml mm bi translated"><feature: restaurants.mean=""/></li><li id="07a9" class="me mf it lj b lk mn ln mo lq mp lu mq ly mr mc mj mk ml mm bi translated"><feature: restaurants.min=""/></li><li id="f710" class="me mf it lj b lk mn ln mo lq mp lu mq ly mr mc mj mk ml mm bi translated"><feature: restaurants.min=""/></li><li id="ffb2" class="me mf it lj b lk mn ln mo lq mp lu mq ly mr mc mj mk ml mm bi translated"><feature: restaurants.skew=""/></li><li id="9fbd" class="me mf it lj b lk mn ln mo lq mp lu mq ly mr mc mj mk ml mm bi translated"><feature: restaurants.skew=""/></li><li id="85c5" class="me mf it lj b lk mn ln mo lq mp lu mq ly mr mc mj mk ml mm bi translated"><feature: restaurants.std=""/></li><li id="81e5" class="me mf it lj b lk mn ln mo lq mp lu mq ly mr mc mj mk ml mm bi translated"><feature: restaurants.std=""/></li><li id="9ca2" class="me mf it lj b lk mn ln mo lq mp lu mq ly mr mc mj mk ml mm bi translated"><feature: restaurants.sum=""/></li><li id="91af" class="me mf it lj b lk mn ln mo lq mp lu mq ly mr mc mj mk ml mm bi translated"><feature: restaurants.sum=""/></li></ul><h1 id="dd6a" class="nw mt it bd mu nx ny nz mx oa ob oc na jz od ka nd kc oe kd ng kf of kg nj og bi translated">机器学习</h1><p id="f576" class="pw-post-body-paragraph lh li it lj b lk nl ju lm ln nm jx lp lq nn ls lt lu no lw lx ly np ma mb mc im bi translated">现在，我们使用包括新创建的T5原语的特征矩阵来创建和测试来自sklearn的各种机器学习模型。</p><p id="ae7e" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">提醒一下，我们将比较T5增强的准确性和Alteryx博客<a class="ae ki" href="https://innovation.alteryx.com/natural-language-processing-featuretools/" rel="noopener ugc nofollow" target="_blank">中展示的自动化特征工程的自然语言处理</a>的准确性。</p><h2 id="1927" class="ms mt it bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj nk bi translated">使用逻辑回归:</h2><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi ox"><img src="../Images/64455981b986c6acaf73cf88a3fb5434.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*FoNr0_4GdRL980__.png"/></div></div></figure><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi oy"><img src="../Images/9ed0e2ab0a92da94278ff0bf97e0e913.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*KqJjDff5WaOwsP3X.png"/></div></div></figure><p id="8e2d" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">请注意，上面的0.64逻辑回归分数显示了对Featuretools原生逻辑回归分数(0.63)的改进。</p><h2 id="889d" class="ms mt it bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj nk bi translated">使用随机森林分类器:</h2><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi oz"><img src="../Images/d0f789e790ca4fef65374683927e617b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*a63iSRQO7JRc6ycY.png"/></div></div></figure><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi pa"><img src="../Images/29c47ca1364f45b03d949129e5c50416.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UwzbmYZ4tJmXm-EZ.png"/></div></div></figure><p id="8dc2" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">请注意，上面的T5增强的0.65随机森林分类器分数显示了对Featuretools原生随机森林分类器分数(0.64)的改进。</p><h1 id="c7f0" class="nw mt it bd mu nx ny nz mx oa ob oc na jz od ka nd kc oe kd ng kf of kg nj og bi translated">随机森林分类器特征重要性</h1><p id="2023" class="pw-post-body-paragraph lh li it lj b lk nl ju lm ln nm jx lp lq nn ls lt lu no lw lx ly np ma mb mc im bi translated">我们可以使用sklearn随机森林分类器特征重要性将改进的分数归因于新的T5基元。</p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi ov"><img src="../Images/2d54f1daba33ba4072f8dec39f15d4fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*uD7TuFbHUN21hlYs.png"/></div></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">随机森林要素重要性。作者图片</p></figure><p id="d1e8" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">从上表中我们可以看出，随机森林模型的最高特征重要性是新创建的特征</p><p id="0df0" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj iu">T5 _情操_编码器(评论_标题)！</strong></p><figure class="kl km kn ko gt kp gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/84ec098ff1b8b666a799965190d4a218.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/0*E6DY3SxV2hXDmnHL.png"/></div></figure><p id="cb0c" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">随机森林分类器特征重要性，图片由作者提供</p><h2 id="e659" class="ms mt it bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj nk bi translated">关键要点</h2><figure class="kl km kn ko gt kp gh gi paragraph-image"><div role="button" tabindex="0" class="kq kr di ks bf kt"><div class="gh gi pc"><img src="../Images/c84e608e228b3c762e9fece975225027.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jeAmdpXxbokjPrx0eOqQSg.png"/></div></div></figure><ol class=""><li id="0759" class="me mf it lj b lk ll ln lo lq mg lu mh ly mi mc pd mk ml mm bi translated"><strong class="lj iu">T5模型是一个健壮、灵活的文本到文本转换器，它可以增强几乎任何nlp任务的结果，包括那些NLP原语库在处理文本数据时处理的结果。</strong>额外的准确性，虽然在这里是微不足道的，但几乎肯定可以通过实现情感分析之外的额外拥抱面部预调整模型来提高。此外，在这个例子中，我们的微调T5版本只在<code class="fe oi oj ok ol b">review_text</code>数据上进行训练，而不是在<code class="fe oi oj ok ol b">review_title</code>数据上进行训练，这似乎与Featuretools创建的功能不一致——这意味着所有创建的功能似乎只使用<code class="fe oi oj ok ol b">review_title</code>数据作为微调模型的输入，因此其性能更差。纠正这个问题很可能意味着更好的整体性能。</li><li id="a865" class="me mf it lj b lk mn ln mo lq mp lu mq ly mr mc pd mk ml mm bi translated">使用拥抱面部变形器和简单变形器库，扩展Featuretools框架很简单。通过加入一些额外的代码行，准确性提高了，而代码的复杂度保持不变。</li></ol><h2 id="35b8" class="ms mt it bd mu mv mw dn mx my mz dp na lq nb nc nd lu ne nf ng ly nh ni nj nk bi translated">最后的想法</h2><p id="483e" class="pw-post-body-paragraph lh li it lj b lk nl ju lm ln nm jx lp lq nn ls lt lu no lw lx ly np ma mb mc im bi translated">大多数企业都有大量的表格数据，其中大部分数据都是书面文本格式的。CCG是一家数据和分析公司，帮助组织变得更加洞察驱动。我们通过行业特定的解决方案解决复杂的挑战并加速增长。我们的数据科学团队使企业能够获得更大的可见性并做出明智的决策，从而获得竞争优势。我们的<a class="ae ki" href="https://ccganalytics.com/solutions" rel="noopener ugc nofollow" target="_blank">战略产品</a>旨在加快价值实现，改善业务成果，并围绕可信见解的共同观点团结团队。请联系我们，帮助您构建下一个定制的NLP解决方案…</p></div><div class="ab cl la lb hx lc" role="separator"><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf lg"/><span class="ld bw bk le lf"/></div><div class="im in io ip iq"><p id="761c" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><em class="md">原载于2021年3月29日</em><a class="ae ki" href="https://blog.ccganalytics.com/featurizing-text-with-googles-t5-text-to-text-transformer" rel="noopener ugc nofollow" target="_blank"><em class="md">【https://blog.ccganalytics.com】</em></a><em class="md">。</em></p><p id="b1b1" class="pw-post-body-paragraph lh li it lj b lk ll ju lm ln lo jx lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><a class="ae ki" href="http://ccganalytics.com" rel="noopener ugc nofollow" target="_blank"> <em class="md"> CCG </em> </a> <em class="md">是一家数据和分析公司，通过行业特定的解决方案，帮助组织变得更加洞察驱动，解决复杂的挑战并加速增长。我们让客户能够更好地了解他们的业务，做出明智的决策，从而获得竞争优势。我们的</em> <a class="ae ki" href="https://ccganalytics.com/solutions" rel="noopener ugc nofollow" target="_blank"> <em class="md">战略产品</em> </a> <em class="md">旨在加快价值实现、改善业务成果，并围绕可信见解的共同观点团结团队。</em></p></div></div>    
</body>
</html>