<html>
<head>
<title>Unlocking Inclusivity Possibilities with Polyglot-NER</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用多语言开启包容性的可能性-NER</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/unlocking-inclusivity-possibilities-with-polyglot-ner-9990baf03561?source=collection_archive---------40-----------------------#2021-06-28">https://towardsdatascience.com/unlocking-inclusivity-possibilities-with-polyglot-ner-9990baf03561?source=collection_archive---------40-----------------------#2021-06-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="00ab" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="a67b" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">海量多语言命名实体识别基础</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/42deb4f7a4f2105bb2fce606a67116e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dLkweh7H2N6LksyVvZZtSw.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">通过<a class="ae le" href="https://unsplash.com/photos/BAZejJdZ57w" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>由Nick Fewings拍摄的图像</p></figure><p id="f32f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">简介</strong></p><p id="97c8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">近年来，命名实体识别(NER)技术发展迅猛，新的模型不断出现。这是一个激动人心的地方，值得观看，也是其中的一部分。就在最近，我的同事<a class="ae le" rel="noopener" target="_blank" href="/whos-who-and-what-s-what-advances-in-biomedical-named-entity-recognition-bioner-c42a3f63334c"> Sybren Jansen </a>和<a class="ae le" rel="noopener" target="_blank" href="/doing-almost-as-much-with-much-less-a-case-study-in-biomedical-named-entity-recognition-efa4abe18ed"> Stéphan Tulkens </a>探索了生物医学命名实体识别(BioNER)的优势和局限性，作为一个例子。</p><p id="8818" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">然而，在利用NER能力构建值得信赖的人工智能时，仍然存在一个重大挑战:如何适应世界上的多样化语言。绝大多数NER模型、训练集、数据、开源代码等等都是英文的。然而，在全球近80亿人口中，只有3.75亿人的母语是英语。NER盛行的英语偏见实际上是将世界上相当一部分地区排除在这一重要的技术进步之外。</p><p id="1b69" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">多语言NER模型提供了语言之间的桥梁的可能性。</p><p id="556c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这第一篇博文中，我将进一步探讨多语言模型的重要性，并更详细地探讨特定的多语言模型。在后续的文章中，我将更深入地研究测试和结果。</p><p id="1ef7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">输入:多语</strong></p><p id="2d77" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">据估计，世界上有7000种活跃的语言。[1] NLP专家通常将语言分为所谓的低、中和高资源语言。高资源语言是这样一种语言，其中有许多不同体裁的在线标记和未标记文本，如英语。低资源语言是一种数据非常少的语言；例如，约鲁巴语。中等资源语言介于这两种类型之间。</p><p id="f905" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在NLP中，大多数研究是在英语和其他高级资源语言上完成的。因此，从语言技术中获益是不对称的。[2]</p><p id="054f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">有两个常用术语用来描述可用于不同语言的模型:多语言和多语种。在这一领域，专家们也使用“跨语言学习”这个短语，这是将一种语言的知识应用到另一种语言的过程。这种技术用于训练多语言模型，可以用来在资源较少的语言上表现得更好。</p><p id="36fc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">命名实体识别(NER) </strong></p><p id="6838" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们将通过NER的子任务探索多语言模型。在这项任务中，模型必须检测文本中哪些标记引用了命名实体(NE)。NE的定义因域而异，但NE最简单的定义是一个令牌或一组令牌，它指的是现实世界中的特定事物。然而，这并不完全准确，因为“哈利·波特”这个角色也是一个NE，但在现实世界中并不存在。</p><p id="109e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">没有必要对NE有严格的定义，因为哪个令牌属于NE将因域和数据集而异。例如，在医学领域中，像肺炎这样的特定情况可以被认为是NE，但是在商业领域中，它不会被认为是NE。</p><p id="0b12" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">当处理多种语言时，NER任务变得更加困难，因为不同的语言对于如何指示NE有不同的约定。例如，在英语中，句子中间以大写字母开头的单词很可能是NE。在德语中，这不太可靠，因为所有名词都以大写字母开头，但不是每个名词都是NE。此外，有许多语言不使用拉丁字母，也不区分大写字母和小写字母。语言中表示NE的这些不同约定对于多语言领域来说是额外的挑战。</p><p id="5f3e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">应用NER:匿名化</strong></p><p id="84cd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">能够识别100种不同语言的命名实体不同于用100种不同语言进行对话。然而，这仍然是一项有用且令人印象深刻的技能。匿名化领域可以通过使用NER模型获得显著的价值。随着开放数据趋势的增加[3]，匿名化扮演了一个关键角色，因为文档在发布前通常需要匿名化。此外，公众对透明政府的要求越来越高，政府官员提出了对隐私的关切。透明度和隐私之间的权衡是一个合理的考虑。通过NER的匿名化，可以帮助消除交易的负面影响。</p><p id="a227" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">还有几个其他的用例可以从NER文档匿名化中受益。欧盟(EU)对法院和医疗文件的匿名化表示了兴趣，并资助了<a class="ae le" href="https://mapa-project.eu/" rel="noopener ugc nofollow" target="_blank"> MAPA项目</a> [4]。该项目旨在创建一个工具，对欧盟所有24种官方语言的医学和法律文本进行匿名化处理。这个项目仍处于开发阶段。</p><p id="aa65" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">当前可用的型号</strong></p><p id="45c6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">今天有几种型号可供使用。首先，已在LENER-BR数据集上训练了由<a class="ae le" href="https://cic.unb.br/~teodecampos/LeNER-Br/luz_etal_propor2018.pdf" rel="noopener ugc nofollow" target="_blank"> Luz等人[5] </a>建立的单语巴西葡萄牙语模型。这个数据集是为了训练这个模型而专门开发的。LENER代表合法命名实体认可，BR表示该名称在巴西葡萄牙语中。LENER-BR数据集由66份法律文件组成，这些文件是用特定法律实体手动注释的。这些实体不同于标准命名实体，如个人、组织和位置。虽然这些类型的实体也包括在内，但数据集包括对特定法律实体和判例的注释，总共有6个类别。基于这些数据训练的模型是基于LSTM的CRF，并使用单词嵌入作为输入。在他们的论文中，他们报告了6个班级的平均92.5分。</p><p id="efd8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">此外，还提供了一种由<a class="ae le" href="https://arxiv.org/pdf/1911.02116.pdf" rel="noopener ugc nofollow" target="_blank">connaau等人</a>命名为XLM-R的多语种模型供使用。[6] XLM-R是基于Transformer [7]的模型，是XLM <a class="ae le" href="https://arxiv.org/pdf/1901.07291.pdf" rel="noopener ugc nofollow" target="_blank"> [ </a> 8]和罗伯塔[9]的组合。XLM-R使用掩蔽语言建模方法接受了100种语言的培训。下图显示了所包含的语言和每种语言的数据量。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mb"><img src="../Images/8e16f0519e2715dc31e8d164dddc8235.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*Yag-xFmJxpjUc1UtwaSy2w.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">表1:XLM-R模型训练数据，取自原论文[6]</p></figure><p id="c1fa" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">XLM-R模型已经在四个任务上进行了评估，其中之一是NER。虽然XLM-R模型是在100种语言上训练出来的，但原始的论文只报告了四种语言的NER分数:英语、荷兰语、德语和西班牙语。这些结果已在2003年CONLL NER数据集上报告。[10]这令人失望，因为在其他任务中，该模型表现良好，即使在资源较少的语言中也是如此。看到NER在更多语言上的表现会很有趣。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mc"><img src="../Images/8587dd8b69bbd475e08748fc125ad7f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_3mhdx79OYn-mqw19dOSgg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">表2:XLM-R在NER的性能与其他模型的比较取自原XLM-R论文[9]</p></figure><p id="9e47" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在上表中，我们看到了原始XLM-R文件中关于NER的结果报告。多语BERT (M-BERT) [11]是最成熟的多语种模型之一。M-BERT用于比较XLM-R在原始论文中的表现。前两个模型被用作基线，都是单语模型。对于这四种语言，我们可以看到XLM-R模型在所有语言上都优于M-BERT，在4种语言中的2种语言上优于单语基线。</p><p id="4dfd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">幸运的是，我们可以超越这四种语言，因为Plu [12]采用了XLM-R，并在40种语言上进行微调，并在所有语言上进行NER实验。它可用于拥抱脸。[12]该模型在PAN-X数据集[13]上进行了微调，该数据集也称为Wiki-Ann数据集，它利用了维基百科的数据。在维基百科上，有超过295种不同语言的文章。手工标记所有这些页面将是一项非常耗费资源的任务。手工标签被认为是黄金标准。潘等人[13]通过利用维基百科页面标记，使用维基百科页面创建了所谓的银标准命名实体语料库，命名实体通常链接到其他页面。根据语言的不同，性能会有很大的差异，但总体而言，该模型表现良好:对于许多语言，F值接近0.9。令人印象深刻的是，这40种语言来自非常不同的语系。</p><p id="41fa" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">让XLM-R更进一步的实验</strong></p><p id="e4a8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们已经看到了来自XLM-R的令人兴奋的结果。但是有两种方法可以将这种模式进一步发展，以获得更大的收益来执行NER:</p><ol class=""><li id="5229" class="md me iq lh b li lj ll lm lo mf ls mg lw mh ma mi mj mk ml bi translated">领域变更:XLM-R模型只在三个NE类上训练:人员、组织、位置。虽然XLM-R已经表明自己能够处理不同的语言，但我们还不知道它是否能够处理不同的领域。当我们在没有任何额外改变的情况下切换域时，看看结果会是什么将会是有趣的。这可能是因为该模型非常强大，可以开箱即用地跨领域应用。这将使它非常容易接近。</li><li id="3e3e" class="md me iq lh b li mm ll mn lo mo ls mp lw mq ma mi mj mk ml bi translated">匿名化:在这篇文章的开始，我们介绍了匿名化任务及其重要性。好消息是，我们可以通过将任务转换为二元任务，将3类模型应用于6类数据集。也就是预测某个东西是不是NE。</li></ol><p id="3393" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果您想匿名化一个文档，您想编校文档中的每一个NE。然而，在某些情况下，控制哪种类型的实体需要匿名，哪种类型的实体需要保留并进行实验可能是好的；对于被认为是NE的一部分的令牌，预测1，对于不是NE的一部分的令牌，预测0。通过这种方式，我们可以将XLM-R模型应用于这个数据集。</p><p id="9e46" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这样的实验中，有两件事需要测试:二值化的效果和XLM-R模型是否也可以在跨域设置中工作。为了将单语模型与二进制化的XLM-R进行比较，单语模型也应该以二进制方式进行转换。</p><p id="ad7a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">总结和下一步行动</strong></p><p id="f921" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们现在已经奠定了多语言NER的基础，作为构建更具包容性的自然语言处理的关键技术。在下一篇博文中，我们将看到二进制域转换实验的结果。你认为结果会是什么？你还希望看到什么样的实验？我很想收到你的来信！</p><p id="1717" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果你想了解更多关于更苗条的人工智能和我们人工智能创新团队的最新研究，请看:【https://medium.com/slimmerai/innovation/home】T2。</p></div><div class="ab cl mr ms hu mt" role="separator"><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw mx"/><span class="mu bw bk mv mw"/></div><div class="ij ik il im in"><p id="2b56" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">参考文献</strong></p><p id="4289" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[1]世界上有多少种语言？(2021年2月23日)。从https://www.ethnologue.com/guides/how-many-languages<a class="ae le" href="https://www.ethnologue.com/guides/how-many-languages" rel="noopener ugc nofollow" target="_blank">取回</a></p><p id="f5be" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[2]p . Joshi，s . Santy，a . Budhiraja，k . Bali和m . Choudhury(2020年)。NLP世界中语言多样性和包容性的状态和命运。arXiv，2004.09095。从https://arxiv.org/abs/2004.09095v3<a class="ae le" href="https://arxiv.org/abs/2004.09095v3" rel="noopener ugc nofollow" target="_blank">取回</a></p><p id="9127" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[3]欧洲联盟出版物处。，凯捷发明。，&amp;欧洲数据门户。(2020).开放数据和隐私。出版办公室。<a class="ae le" href="https://doi.org/10.2830/532195" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.2830/532195</a></p><p id="dece" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[4]Mapa——公共管理的多语种匿名化。(2021年6月03日)。检索自<a class="ae le" href="https://mapa-project.eu/" rel="noopener ugc nofollow" target="_blank">https://mapa-project . eu</a></p><p id="a1f2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[5] Luz de Araujo，P. H .，de Campos，T. E .，de Oliveira，R. R. R .，Stauffer，m .，Couto，s .，&amp; Bermejo，P. (2018年)。LeNER-Br:巴西法律文本中命名实体识别的数据集。葡萄牙语的计算处理。斯普林格。doi:10.1007/978–3–319–99722–3 _ 32</p><p id="d2f4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[6] Conneau，a .，Khandelwal，k .，Goyal，n .，Chaudhary，v .，Wenzek，g .，Guzmán，f .，…Stoyanov，V. (2019)。大规模无监督跨语言表征学习。arXiv，1911.02116。从https://arxiv.org/abs/1911.02116v2<a class="ae le" href="https://arxiv.org/abs/1911.02116v2" rel="noopener ugc nofollow" target="_blank">取回</a></p><p id="935a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[7] Vaswani，a .，Shazeer，n .，Parmar，n .，Uszkoreit，j .，Jones，l .，Gomez，A. N .，…Polosukhin，I. (2017)。你需要的只是关注。arXiv，1706.03762。从https://arxiv.org/abs/1706.03762v5<a class="ae le" href="https://arxiv.org/abs/1706.03762v5" rel="noopener ugc nofollow" target="_blank">取回</a></p><p id="816c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[8]兰普尔，g .，&amp;康瑙，A. (2019年)。跨语言语言模型预训练。arXiv，1901.07291。从https://arxiv.org/abs/1901.07291v1<a class="ae le" href="https://arxiv.org/abs/1901.07291v1" rel="noopener ugc nofollow" target="_blank">取回</a></p><p id="d7fe" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[9]刘，y .，奥特，m .，戈亚尔，n .，杜，j .，乔希，m .，陈，d，…斯托扬诺夫，V. (2019)。RoBERTa:稳健优化的BERT预训练方法。arXiv，1907.11692。从https://arxiv.org/abs/1907.11692v1取回</p><p id="7c94" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[10]桑，E. T. K .，&amp; De Meulder，F. (2003年)。CoNLL-2003共享任务简介:独立于语言的命名实体识别。ACL选集，142–147。从<a class="ae le" href="https://www.aclweb.org/anthology/W03-0419" rel="noopener ugc nofollow" target="_blank">https://www.aclweb.org/anthology/W03-0419</a>取回</p><p id="8cd4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[11] Devlin，j .，Chang，m-w .，Lee，k .，&amp; Toutanova，K. (2018年)。BERT:用于语言理解的深度双向转换器的预训练。arXiv，1810.04805。从https://arxiv.org/abs/1810.04805v2<a class="ae le" href="https://arxiv.org/abs/1810.04805v2" rel="noopener ugc nofollow" target="_blank">取回</a></p><p id="51be" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[12]jplu/TF-xlm-r-ner-40-郎抱脸。(2021年6月22日)。从https://huggingface.co/jplu/tf-xlm-r-ner-40-lang<a class="ae le" href="https://huggingface.co/jplu/tf-xlm-r-ner-40-lang" rel="noopener ugc nofollow" target="_blank">取回</a></p><p id="33a6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[13]潘，x .，张，b .，梅，j .，诺斯曼，j .，奈特，k .，&amp;纪，H. (2017)。282种语言的跨语言姓名标记和链接。ACL选集，1946-1958。doi:10.18653/v1/P17–117</p></div></div>    
</body>
</html>