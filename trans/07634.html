<html>
<head>
<title>Towards Automatic Discovery of Mathematical and Physical Laws: Algebraically-Informed Deep Networks (AIDN)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数学和物理定律的自动发现:代数信息深度网络(AIDN)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/towards-automatic-discovery-of-mathematical-laws-from-observed-data-algebraically-informed-deep-68eb0482e4b7?source=collection_archive---------43-----------------------#2021-07-12">https://towardsdatascience.com/towards-automatic-discovery-of-mathematical-laws-from-observed-data-algebraically-informed-deep-68eb0482e4b7?source=collection_archive---------43-----------------------#2021-07-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="306c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用深度学习发现任意代数方程组的解</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/167fc5e9ea67480905945bf6338872c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*snEsgjdpMVKOJqqn"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">莱纳斯·米米耶茨在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="6690" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">深度学习和数学接口的中心问题之一是建立能够从观察到的数据中自动揭示潜在数学规律的学习系统。在过去的几年里，深度学习技术已经被用于求解许多类型的方程，包括偏微分方程[2]，非线性隐式方程组[3]和超越方程[5]。</p><p id="29d0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章中，我们朝着在解代数方程和深度学习之间建立桥梁的方向迈出了一步，并介绍了<em class="ls"> AIDN </em>，<em class="ls">代数信息深度网络</em> [1]。给定一组满足一组约束或方程的未知函数，AIDN是旨在使用深度学习解决该问题的通用算法。</p><p id="adeb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了解释AIDN算法的主要思想及其广泛的应用，我们从一个例子开始，</p><h1 id="e4d7" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">杨-巴克斯特方程</h1><p id="306f" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">杨-巴克斯特方程是以下形式的方程:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/dc00e375c5c5c058dfd2ddf82b49de89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GOh0zrkexFknrOaDz-OAsA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">杨-巴克斯特方程是数学物理中的中心方程，具有广泛的应用，包括量子场论和统计力学</p></figure><p id="47d0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中函数</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/f5ce07ab8bb4d6cebedb5010c5aa89e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*k-KtdN7ucJvDNOxr9Ozw6A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">R矩阵是满足杨-巴克斯特方程的可逆映射</p></figure><p id="f8fa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">是我们喜欢寻找的未知函数。对于本文，我们可以将<em class="ls">和</em>视为欧几里德域<em class="ls">的子集。</em>任何满足杨-巴克斯特方程的可逆函数<em class="ls"> R </em>称为<em class="ls"> R矩阵</em>。寻找上述方程的解具有非常长的历史，具有广泛的应用，包括量子场论、低维拓扑和统计力学，并且通常被认为是一个困难的问题。</p><h1 id="4705" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">深度学习拯救世界</h1><p id="184f" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">我们现在解释AIDN算法在寻找上述杨-巴克斯特方程的解上的应用。为了求解杨-巴克斯特方程，AIDN算法将寻找解R的问题实现为优化问题。首先我们把函数R写成一个深度神经网络。我们将把这个网络称为<em class="ls"> f_R </em>或者简称为<em class="ls"> f </em>。</p><p id="23a6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因为我们要求<em class="ls"> f </em>是可逆的，那么我们创建另一个网络<em class="ls"> g </em>，我们将它训练成<em class="ls"> f </em>的逆。理论上我们可以选择网络<em class="ls"> f </em>是可逆的，但是在我们的论文中我们选择训练<em class="ls"> g </em>使得它是<em class="ls"> f </em>的逆。为了更直观地理解我们的解决方案，采用图形符号更容易。首先，我们用以下两个黑盒来表示网络<em class="ls"> f </em>和<em class="ls"> g </em>:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/1daea78b897e2d1150f5ce738594799e.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*ofPqaXyNo71fEb7kg0p_hw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">函数f和g的图形表示。图形来源[1]。</p></figure><p id="5f35" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">给定这个符号，我们可以通过下面的图表表示两个函数<em class="ls"> f </em>和<em class="ls"> g </em>彼此相反的事实:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/977a3e7c05602fe0b20652fb8ef3e432.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*XFnNDtO2HVVXxUnuvAg-JQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">函数f的可逆性之一的图示图来源[1]。</p></figure><p id="be84" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里，垂直连接意味着网络的组合。上图简单的说就是<em class="ls"> f </em>后面跟着<em class="ls"> g </em>和<em class="ls"> g </em>后面跟着<em class="ls"> f </em>就是身份图。</p><p id="cf68" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">类似地，杨-巴克斯特方程可以通过以下图表表示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/fda470888e1f0396d96948edc0739893.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*cX-ik6fR0VWyRCTCvJS0Bg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">杨-巴克斯特方程的图示。图表的</p></figure><p id="07f7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了训练这两个网络，我们定义了以下目标:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mv"><img src="../Images/9fa4557f7ea5a4ba41a51703acf9b144.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cq5jtun6khGrUq2YUrF8fw.png"/></div></div></figure><p id="cd2c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上述损失函数基本上有两个目的:满足杨-巴克斯特方程以及强制函数f的可逆性。</p><p id="cef1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">更实际地说，为了训练f，g，我们创建了一个辅助神经网络来帮助我们进行训练。具体来说，用损失函数训练辅助网络:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/5edbe9c0e63b96a5be59d0b23e01de23.png" data-original-src="https://miro.medium.com/v2/resize:fit:604/format:webp/1*eG12W7zbRl5EjGueO1vdMA.png"/></div></figure><p id="a6cf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在哪里</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mx"><img src="../Images/332e81f497ba786a3ab573c71b20f570.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8WhE3E_dbSAX4cLyBZA3hA.png"/></div></div></figure><p id="5e09" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">和</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi my"><img src="../Images/1a14fbc6250046d03940f027a72406e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*31JwZzOLY0Y5xctvgivBTA.png"/></div></div></figure><p id="83ed" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中{xi，易，子}表示从A × A × A均匀采样的点，其中A⊂ R^n，一般为单位区间[0，1]。仅此而已！在训练函数<em class="ls"> f </em>和<em class="ls"> g </em>之后，假设存在解，则满足杨-巴克斯特方程(参见我们的<a class="ae kv" href="https://github.com/mhajij/Algebraically_Informed_Deep_Nets" rel="noopener ugc nofollow" target="_blank"> repo </a>以获得关于实现的更多细节)。</p><h1 id="c60b" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">AIDN算法的通用版本</h1><p id="b18a" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">我们为杨-巴克斯特方程所解释的思想可以推广应用于更大范围的问题。具体来说，给定具有<em class="ls"> n </em>个变量的<em class="ls"> k </em>个方程(或我们在论文中称之为<em class="ls">关系</em>)，然后AIDN通过创建一组<em class="ls"> n </em>神经网络来训练这些网络，通过将方程转换为MSE损失函数来满足关系。详细情况如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mz"><img src="../Images/32d89d986103660f243bfe07cd088d2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wx-UjNLRNwDznp6cJExTpA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">AIDN算法</p></figure><h1 id="44f9" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">AIDN用在哪里？</h1><p id="3a8e" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">我们提出的算法是相当通用的，它可以应用于许多领域。我们只是触及了表面，并解释了它对杨-巴克斯特方程的适用性。今天，杨-巴克斯特方程被认为是应用于量子力学、代数几何和量子群的几个物理和数学领域的基石。理论上，如果给定足够的计算能力，AIDN能够找到任何代数方程组的解，只要这种系统的解存在。在我们的论文[1]中，我们展示了许多其他的应用，包括寻找新的量子不变量。</p><h1 id="d7fe" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">局限性和<strong class="ak">结论</strong></h1><p id="4985" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">关于AIDN性能的一个重要注释是当具有许多生成器和关系时训练的难度。虽然诸如SGD的现代优化范例允许人们训练高维数据的模型，但是我们发现训练与具有许多生成器和关系以及高维数据的代数结构相关联的多个网络是困难的。这可以通过使用更好的超参数搜索和更合适的优化方案来解决。</p><p id="0ac5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然没有理论保证神经网络存在，但我们在实验中始终发现，在给定网络的足够表达能力和来自这些网络的域的足够样本点的情况下，AIDN能够实现良好的结果并找到期望的函数。这一观察结果与理论深度学习[4]中关于深度网络损失情况的其他公开研究问题是一致的。</p><h1 id="860a" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">密码</h1><p id="e850" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">在我们的<a class="ae kv" href="https://github.com/mhajij/Algebraically_Informed_Deep_Nets" rel="noopener ugc nofollow" target="_blank"> repo </a>中详细给出了上述示例和许多其他示例的存储库。</p><h1 id="7b9f" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated"><strong class="ak">参考文献</strong></h1><p id="c561" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">[1] Hajij，Mustafa等.“<a class="ae kv" href="https://arxiv.org/abs/2012.01141" rel="noopener ugc nofollow" target="_blank">代数信息深度网络</a> (AIDN):一种表示代数结构的深度学习方法。”<em class="ls"> arXiv预印本arXiv:2012.01141 </em> (2020)。</p><p id="fd31" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2] Maziar Raissi、Paris Perdikaris和George Em Karniadakis。物理学通知深度学习(第一部分):非线性偏微分方程的数据驱动的解决方案。arXiv预印本arXiv:1711.10561，2017年</p><p id="1b49" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[3]，，孟，，廖，和斯特凡诺·埃尔蒙。非线性方程求解:前馈计算的一种更快的替代方法。arXiv预印本:2002.03629，2020</p><p id="6b02" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[4]拉维德·施瓦茨-齐夫和纳夫塔利·蒂什比。通过信息打开深度神经网络的黑匣子。arXiv预印本arXiv:1703.00810，2017</p><p id="ed91" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[5] SK Jeswal和Snehashish Chakraverty。用人工神经网络求解超越方程。应用软计算，73:562–571，2018</p></div></div>    
</body>
</html>