<html>
<head>
<title>Bagging Decision Trees — Clearly Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">包装决策树——解释清楚</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bagging-decision-trees-clearly-explained-57d4d19ed2d3?source=collection_archive---------10-----------------------#2021-05-07">https://towardsdatascience.com/bagging-decision-trees-clearly-explained-57d4d19ed2d3?source=collection_archive---------10-----------------------#2021-05-07</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><div class=""/><div class=""><h2 id="575d" class="pw-subtitle-paragraph jr it iu bd b js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki dk translated">整体技术-装袋</h2></div><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj kj"><img src="../Images/693f422cadcbdd9b81ae58c24e8fee51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ntvCqJJe_DcrClklP_qFhg.jpeg"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">照片由<a class="ae kz" href="https://www.pexels.com/@pixabay?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">皮查拜</a>从<a class="ae kz" href="https://www.pexels.com/photo/scenic-view-of-rice-paddy-247599/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">派克斯</a>拍摄</p></figure><h1 id="3875" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">决策树</h1><p id="0f46" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">决策树是监督机器学习算法，用于分类和回归任务。</p><p id="0629" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">在本文中，我介绍了以下概念。</p><ul class=""><li id="6faf" class="mt mu iu lu b lv mo ly mp mb mv mf mw mj mx mn my mz na nb bi translated">如何建立决策树？用什么标准来分割决策树？</li><li id="01bf" class="mt mu iu lu b lv nc ly nd mb ne mf nf mj ng mn my mz na nb bi translated">如何克服决策树中的过度拟合？</li><li id="667b" class="mt mu iu lu b lv nc ly nd mb ne mf nf mj ng mn my mz na nb bi translated">什么是装袋？包装决策树是如何工作的？</li></ul></div><div class="ab cl nh ni hy nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="in io ip iq ir"><p id="a195" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">决策树是一种树状模型，可用于预测目标变量的类别/值。决策树有效地处理非线性数据。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj no"><img src="../Images/7fbd9889975aac9bdaf0ca8ba8ba2ab7.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*ceSj7b9d6VBvV5x05163BQ.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><p id="23fc" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">假设我们有难以线性分类的数据点，决策树提供了一种简单的方法来确定决策边界。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj np"><img src="../Images/89dcb74f4e56ead6c303be5aedc74172.png" data-original-src="https://miro.medium.com/v2/resize:fit:964/format:webp/1*MOPb__w3Vw3TCpng6i_WkA.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">作者图片</p></figure><p id="8564" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">它通过递归分割整个区域来实现。让我们看看分裂是如何进行的。</p><h1 id="09ee" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">节点类型</h1><ol class=""><li id="c8c7" class="mt mu iu lu b lv lw ly lz mb nq mf nr mj ns mn nt mz na nb bi translated">根节点—决策树中最顶端的节点称为根节点。</li><li id="9063" class="mt mu iu lu b lv nc ly nd mb ne mf nf mj ng mn nt mz na nb bi translated">内部节点—进一步分裂的子节点称为内部节点。</li><li id="8c22" class="mt mu iu lu b lv nc ly nd mb ne mf nf mj ng mn nt mz na nb bi translated">叶节点—不会进一步拆分的节点称为叶节点</li></ol><h1 id="2f1b" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">如何在决策树中进行拆分？</h1><p id="95ff" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">在决策树中，在每个节点，基于特征进行分割。如果特征是连续变量，则在某个阈值上形成分裂条件。【比如年龄&gt; 50】。如果特征是分类变量，则对所有值进行分割。【例如。性别=男性】。<br/>选择杂质减少更多的分流。</p><p id="114c" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">用于计算最佳分割的标准是</p><ol class=""><li id="0ac3" class="mt mu iu lu b lv mo ly mp mb mv mf mw mj mx mn nt mz na nb bi translated">分类误差</li><li id="fd3f" class="mt mu iu lu b lv nc ly nd mb ne mf nf mj ng mn nt mz na nb bi translated">熵</li><li id="59cb" class="mt mu iu lu b lv nc ly nd mb ne mf nf mj ng mn nt mz na nb bi translated">基尼指数</li></ol><h1 id="d05b" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">分类误差</h1><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj nu"><img src="../Images/e04a33ccc7f6d936e84fe3fc715b61f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:520/format:webp/1*Y_PuY57NNOjmdEy-1FA8bg.png"/></div></figure><h2 id="79a1" class="nv lb iu bd lc nw nx dn lg ny nz dp lk mb oa ob lm mf oc od lo mj oe of lq og bi translated"><strong class="ak">如何计算分类误差？</strong></h2><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj oh"><img src="../Images/940d9e59f86a2a813f3290c093de41bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*Z9LGUUrISBG8OSAgw_wI0w.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">根据拆分条件，节点N1被拆分为N2和N3[图片由作者提供]</p></figure><p id="1bcd" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><strong class="lu iv">步骤1:计算节点N1处的分类误差【分割前】</strong></p><p id="ba08" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">P[类0]= 3/8<br/>P[类1]=5/8</p><p id="ebc0" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">1- max[p(c)]=1-[3/8，5/8]= 1–5/8 = 3/8</p><p id="b4cb" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><code class="fe oi oj ok ol b">classification error before split =3/8</code></p><p id="c0a1" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><strong class="lu iv">步骤2:计算节点N2的分类误差</strong></p><p id="23fe" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">P[0级]= 1/4<br/>P[1级]=3/4</p><p id="25d9" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">1- max[p(c)]=1-[1/4，3/4]= 1–3/4 = 1/4</p><p id="3436" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><strong class="lu iv"> <em class="om">第三步。在节点N3 </em> </strong>计算分类误差</p><p id="4af2" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">P[0级] = 2/4<br/>P[1级]= 2/4</p><p id="f2f6" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">1- max[p(c)]=1-[2/4，2/4]= 1–2/4 = 2/8</p><p id="c5f6" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><strong class="lu iv"> <em class="om">第四步。子节点的平均分类误差。</em> </strong></p><p id="d3fc" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">节点n2中数据点的比例= 4/8<br/><em class="om">【8个数据点中有4个在节点N2】</em></p><p id="d7a3" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">节点n3中数据点的比例= 4/8<br/><em class="om">【8个数据点中有4个在节点n3】</em></p><p id="9ba4" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">加权分类误差= 4/8 * 1/4 + 4/8 * 2/4 = 3/8</p><p id="2061" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><code class="fe oi oj ok ol b">classification error after split =3/8</code></p><p id="719c" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><strong class="lu iv"> 5。分类错误变更:</strong></p><p id="1515" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">分割前的分类误差= 3/8 <br/>分割后的分类误差= 3/8</p><p id="2607" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">分类误差变化= 3/8–3/8 = 0</p><p id="f7c6" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><code class="fe oi oj ok ol b">Classification error for the above split is 0</code></p><p id="65db" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">分割是基于每个特征进行的。对于每个特征，计算分类误差的变化，选择产生最大<em class="om">分类误差变化</em>的特征进行分割。</p><p id="6abe" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">但有时，分类误差变化将为零。[在上面的例子中，分类误差变化为零]。所以，这不是首选。<em class="om">基尼指数和熵优于分类误差。</em></p><h1 id="3a3d" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">熵</h1><p id="8792" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">熵用于度量决策树中节点的同质性。如果节点是同质的，熵是0。熵指的是杂质。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj on"><img src="../Images/365a18259223a4da7cc4cb559176bde3.png" data-original-src="https://miro.medium.com/v2/resize:fit:528/format:webp/1*cMJ9ue8sMTcZn3DDzy-jGA.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">熵公式</p></figure><p id="89b3" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><strong class="lu iv">熵的范围从0到1【针对2类问题】</strong></p><ol class=""><li id="eb02" class="mt mu iu lu b lv mo ly mp mb mv mf mw mj mx mn nt mz na nb bi translated">如果节点是同质的，并且所有数据点都属于类0 p[class=1]=0。熵将为0。[纯节点-无杂质]</li><li id="fe76" class="mt mu iu lu b lv nc ly nd mb ne mf nf mj ng mn nt mz na nb bi translated">如果节点是同质的，并且所有数据点都属于类1，p[class=1]=1。熵将为0。[纯节点]</li><li id="915c" class="mt mu iu lu b lv nc ly nd mb ne mf nf mj ng mn nt mz na nb bi translated">如果节点包含相同数量的属于类0和类1的数据点，则p[c=1]=0.5。熵将是1。</li></ol><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj oo"><img src="../Images/105abccbc19e37b89fa64ad318203438.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*TMflBxgBrqJzTM-eI32pFw.png"/></div></figure><h1 id="d60d" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">信息增益</h1><p id="9962" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">熵(杂质)的减少被称为信息增益。如果特定拆分的信息增益更大，则首先执行该拆分条件。</p><p id="8b77" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><code class="fe oi oj ok ol b">Information gain= Entropy before split — Entropy after split</code></p><h2 id="73be" class="nv lb iu bd lc nw nx dn lg ny nz dp lk mb oa ob lm mf oc od lo mj oe of lq og bi translated"><strong class="ak">为上述决策树计算熵。</strong></h2><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj op"><img src="../Images/c758c92856901bf1c706d9938696ee4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*3AU2q_f9EFL1mW_CaOgJUg.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">计算分裂前后的熵[图片由作者提供]</p></figure><h2 id="f69c" class="nv lb iu bd lc nw nx dn lg ny nz dp lk mb oa ob lm mf oc od lo mj oe of lq og bi translated">计算信息增益。</h2><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj oq"><img src="../Images/3b1d0b21d5dc724dbdd2ffffbd0044dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/format:webp/1*lBlTG5arxv2RVDo60oFi4Q.png"/></div></figure><p id="9070" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">分割是基于所有特征完成的。对于每个特征，计算信息增益，并选择给出更多信息增益[杂质减少更多]的特征用于第一次分离。</p><h1 id="d53d" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">基尼指数</h1><p id="c46a" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">基尼指数是衡量杂质的另一个标准。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj or"><img src="../Images/33a96bfce1862b2e5a9a3e4ca343675a.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/format:webp/1*8n1b3OGTomUPIoqjfzjSWA.png"/></div></figure><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj os"><img src="../Images/a6b4c8ddb72fce91bd5e6f0f0377993c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*X-VGh4_KKf82VYdZXX1w9w.png"/></div></figure><p id="1a38" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><strong class="lu iv">基尼指数下降</strong></p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj ot"><img src="../Images/efe27c2c4f5011edf6472a119858916c.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*9IduXHr5wZc8N7iSD2UwhA.png"/></div></figure><blockquote class="ou ov ow"><p id="652b" class="ls lt om lu b lv mo jv lx ly mp jy ma ox mq md me oy mr mh mi oz ms ml mm mn in bi translated">基尼系数的范围从0到0.5。<br/>熵的范围从0到1。</p></blockquote><h1 id="aea4" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">决策树中的问题</h1><p id="f40b" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">决策树往往会过度拟合。它将继续增长，直到所有的叶节点都是纯的。所有叶节点都是同质的[属于一个类]。这将产生一个精确符合训练数据的树。所以，它不会很好地概括，也不会在测试数据中表现良好。</p><h1 id="d9b2" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">如何克服决策树中的过拟合？</h1><p id="7a53" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">为了防止过度拟合，应该有一些停止标准。</p><p id="592c" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">避免过度拟合的两种方法。</p><ol class=""><li id="f4a2" class="mt mu iu lu b lv mo ly mp mb mv mf mw mj mx mn nt mz na nb bi translated">截断或预修剪→有许多超参数来截断树。这是一种自上而下的方法。</li><li id="e931" class="mt mu iu lu b lv nc ly nd mb ne mf nf mj ng mn nt mz na nb bi translated">让树完全长大，然后修剪。这是一种自下而上的方法。</li></ol><h2 id="84e0" class="nv lb iu bd lc nw nx dn lg ny nz dp lk mb oa ob lm mf oc od lo mj oe of lq og bi translated"><strong class="ak">预剪枝→决策树停止准则—决策超参数</strong></h2><ol class=""><li id="67e9" class="mt mu iu lu b lv lw ly lz mb nq mf nr mj ns mn nt mz na nb bi translated"><strong class="lu iv">最大深度</strong> →树被允许生长到提到的最大深度。</li><li id="487b" class="mt mu iu lu b lv nc ly nd mb ne mf nf mj ng mn nt mz na nb bi translated"><strong class="lu iv"> min_samples_split </strong> →拆分决策节点所需的最小样本数。[决策节点是具有进一步拆分的节点]。如果决策节点的样本数少于min_samples_split中提到的样本数，则不会进一步分割。</li><li id="7820" class="mt mu iu lu b lv nc ly nd mb ne mf nf mj ng mn nt mz na nb bi translated"><strong class="lu iv"> min_samples_leaf </strong> →一个叶节点所需的最小样本数。</li><li id="c7dd" class="mt mu iu lu b lv nc ly nd mb ne mf nf mj ng mn nt mz na nb bi translated"><strong class="lu iv"> max_features </strong> →进行分割时要考虑的特征数量。</li></ol><p id="9c97" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">通过调整这些参数，它可以防止树过度拟合。</p><h2 id="0ace" class="nv lb iu bd lc nw nx dn lg ny nz dp lk mb oa ob lm mf oc od lo mj oe of lq og bi translated"><strong class="ak">后期修剪→减少错误修剪</strong></h2><p id="fbe4" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">这是一种自下而上的方法。一棵树完全长大后，再修剪。</p><p id="45dd" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">训练数据分为训练集和验证集。迭代地修剪节点，并在确认集和训练集中检查修剪树的性能。如果验证集中被修剪的树的准确度大于训练集的平均值，则该节点被修剪。修剪意味着删除以该节点为根的子树。</p></div><div class="ab cl nh ni hy nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="in io ip iq ir"><h1 id="be49" class="la lb iu bd lc ld pa lf lg lh pb lj lk ka pc kb lm kd pd ke lo kg pe kh lq lr bi translated">为什么要用装袋？</h1><p id="35bd" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">决策树是高方差模型。这意味着训练数据的微小变化，将导致完全不同的模型。决策树通常会过拟合。为了克服这种情况，可以使用集成技术——bagging。</p><h1 id="6b7c" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">什么是装袋？</h1><p id="1672" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">Bagging表示<strong class="lu iv">自举聚合。<br/> </strong> Bagging是指利用样本子集建立不同的模型，然后聚合不同模型的预测值以减少方差。</p><h1 id="d09b" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">装袋如何减少差异？</h1><p id="8a42" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">假设我们有一组'<strong class="lu iv"> n' </strong>的独立观测值，比如说<strong class="lu iv"> Z1，Z2…Zn </strong>。个体观察的方差为<strong class="lu iv"> σ2 </strong>。</p><p id="f2c1" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">所有数据点的平均值将为(<strong class="lu iv"> Z1+Z2+…)。+Zn)/n <br/> </strong>类似地，该平均值的方差将是<strong class="lu iv"> σ2/n. </strong></p><p id="4c3c" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">因此，如果我们增加数据点的数量，均值的方差就会减小。这就是装袋决策树背后的概念。</p><p id="1e8d" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">同样，如果我们在给定的数据集上训练多个决策树，然后聚合预测。方差将会减小。</p><h1 id="121b" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">引导样本</h1><p id="2237" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">装袋分两步</p><ol class=""><li id="d86b" class="mt mu iu lu b lv mo ly mp mb mv mf mw mj mx mn nt mz na nb bi translated">拔靴带</li><li id="7b38" class="mt mu iu lu b lv nc ly nd mb ne mf nf mj ng mn nt mz na nb bi translated">聚合</li></ol><h2 id="5bbe" class="nv lb iu bd lc nw nx dn lg ny nz dp lk mb oa ob lm mf oc od lo mj oe of lq og bi translated">拔靴带</h2><p id="9a5c" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">我们已经知道，如果我们对一组观察值进行平均，方差将会减少。但是我们只有一个训练集，我们必须建立多个决策树。</p><p id="ca9d" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">引导有助于从训练数据中创建多个子集。然后，我们可以在自举样本上构建多个树。</p><p id="ad8b" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">Bootstrapping将从训练集中随机选择重复的数据点，并创建多个子集-替换采样。</p><p id="583f" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">如果在训练集中有<strong class="lu iv">‘n’</strong>个数据点，</p><p id="7767" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">在bootstrap样本中选择特定数据点的概率= <strong class="lu iv"> 1/n </strong></p><p id="1959" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">bootstrap样本中特定数据点未被选择的概率= <br/> <strong class="lu iv"> 1-1/n </strong></p><p id="f33b" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">所以，自举样本中每个点未被选中的概率=<br/><strong class="lu iv">(1–1/n)^n</strong></p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div class="gi gj pf"><img src="../Images/15bcc25a4f388f4717664e7881a0b34c.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*YgXQcz9qySoZDKL8Pf_jPg.png"/></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">引导样本中未选择的数据点的百分比-按作者分类的图像</p></figure><p id="f7da" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">这种方法也被称为<strong class="lu iv"> 0.632自举</strong>。表示bootstrap样本中每个数据点被选中的概率=63.2%。</p><p id="144d" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">每个引导样本将包含原始训练数据的<strong class="lu iv"> 63.25% </strong>。剩余的数据点将是重复的。<br/>每个引导样本将不包含<strong class="lu iv"> 36.8% </strong>的训练数据。这用作从该样本构建的模型的测试数据。</p><h1 id="ec47" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">汇总输出</h1><p id="04c4" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">现在，我们有了不同的引导样本，并为每个引导样本构建了'<strong class="lu iv"> B' </strong>决策树。下一步是汇总输出。</p><p id="a5ad" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><strong class="lu iv">套袋分类树</strong></p><p id="2bf8" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">对于测试集中的每个数据点，通过'<strong class="lu iv"> B' </strong>树来预测输出类。基于多数表决机制，计算最终类。</p><p id="48e4" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">另一种方法是从“B”树中获得类的概率，并且根据概率的平均值对最终的类进行分类。</p><p id="9317" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><strong class="lu iv">装袋回归树</strong></p><p id="2538" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">对于回归→对于测试集中的每个数据点，从“B”回归树的所有预测的平均值计算目标值。</p><blockquote class="pg"><p id="3f34" class="ph pi iu bd pj pk pl pm pn po pp mn dk translated">Bagging是引导聚集</p></blockquote><p id="62e2" class="pw-post-body-paragraph ls lt iu lu b lv pq jv lx ly pr jy ma mb ps md me mf pt mh mi mj pu ml mm mn in bi translated">通过从训练数据集创建引导样本，然后在引导样本上构建树，然后聚合所有树的输出并预测输出，来完成决策树的打包。</p><figure class="kk kl km kn gu ko gi gj paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gi gj pv"><img src="../Images/91884b8574158616e36dabe0ddcb9e7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4ivswXnBC6TFDMFDuyVeRg.png"/></div></div><p class="kv kw gk gi gj kx ky bd b be z dk translated">bagging[图片由作者提供]</p></figure><h1 id="2986" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">如何计算装袋误差？</h1><p id="a7de" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">我们知道，在每个bootstrap样本中，大约有三分之一的数据点被遗漏(36.8%)。来自训练集的大约只有三分之二的原始数据点被包括在引导样本中(63.2%)</p><p id="8616" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">因此，对于基于自举样本构建的每棵树，误差是根据该特定自举样本的未使用样本计算的。测量所有误差的平均值。这就是所谓的<strong class="lu iv">出BagError。</strong></p><h1 id="a07b" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">如何计算特征重要性？</h1><p id="a544" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">通过使用基尼指数/熵来计算特征重要性。</p><p id="09df" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">在为每个引导样本构建树时，基于信息增益在节点中进行分裂。我们将查看所有树中该特征的信息增益。然后在所有树中平均该特征的信息增益。</p><h1 id="723c" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">袋装决策树的优势</h1><ol class=""><li id="eaf0" class="mt mu iu lu b lv lw ly lz mb nq mf nr mj ns mn nt mz na nb bi translated">模型的方差减小。</li><li id="4c88" class="mt mu iu lu b lv nc ly nd mb ne mf nf mj ng mn nt mz na nb bi translated">可以同时训练多个树。</li></ol><h1 id="537f" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">包装决策树的问题。</h1><p id="3135" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">如果一个特征是最强的预测值，并且它对目标变量均值有更大的影响，则基于不同引导样本构建的所有树将首先基于该最强的预测值进行分割。因此，在不同引导样本上训练的所有树将是相关的。所以，它不会减少模型的方差。</p><p id="6fa4" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">为了克服这种情况，使用了随机森林。同样在随机森林中，我们将训练多棵树。但是数据点和特征都是随机选择的。通过这样做，树不相关，这将改善方差。</p><h1 id="d201" class="la lb iu bd lc ld le lf lg lh li lj lk ka ll kb lm kd ln ke lo kg lp kh lq lr bi translated">结论</h1><p id="e743" class="pw-post-body-paragraph ls lt iu lu b lv lw jv lx ly lz jy ma mb mc md me mf mg mh mi mj mk ml mm mn in bi translated">决策树使用像基尼指数/熵这样的分裂标准来分裂节点。决策树往往会过度拟合。为了克服过度拟合，使用预修剪或后修剪方法。装袋决策树也被用来防止过度拟合。</p><p id="8744" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated">我希望这篇文章对你有所帮助。感谢阅读！</p></div><div class="ab cl nh ni hy nj" role="separator"><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm nn"/><span class="nk bw bk nl nm"/></div><div class="in io ip iq ir"><h1 id="444b" class="la lb iu bd lc ld pa lf lg lh pb lj lk ka pc kb lm kd pd ke lo kg pe kh lq lr bi translated">进一步阅读:</h1><div class="pw px gq gs py pz"><a href="https://betterprogramming.pub/understanding-decision-trees-in-machine-learning-86d750e0a38f" rel="noopener  ugc nofollow" target="_blank"><div class="qa ab fp"><div class="qb ab qc cl cj qd"><h2 class="bd iv gz z fq qe fs ft qf fv fx it bi translated">理解机器学习中的决策树</h2><div class="qg l"><h3 class="bd b gz z fq qe fs ft qf fv fx dk translated">决策树背后的数学以及如何使用Python和sklearn实现它们</h3></div><div class="qh l"><p class="bd b dl z fq qe fs ft qf fv fx dk translated">better编程. pub</p></div></div><div class="qi l"><div class="qj l qk ql qm qi qn kt pz"/></div></div></a></div><p id="52cc" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><em class="om">如果你喜欢看我的更多教程，就关注我的</em> <a class="ae kz" href="https://medium.com/@IndhumathyChelliah" rel="noopener"> <strong class="lu iv"> <em class="om">中</em> </strong> </a>，<a class="ae kz" href="https://www.linkedin.com/in/indhumathy-chelliah/" rel="noopener ugc nofollow" target="_blank"><strong class="lu iv"><em class="om">LinkedIn</em></strong></a><strong class="lu iv"><em class="om">，</em></strong><a class="ae kz" href="https://twitter.com/IndhuChelliah" rel="noopener ugc nofollow" target="_blank"><strong class="lu iv"><em class="om">Twitter</em></strong></a><strong class="lu iv"><em class="om">。</em> </strong></p><p id="9e28" class="pw-post-body-paragraph ls lt iu lu b lv mo jv lx ly mp jy ma mb mq md me mf mr mh mi mj ms ml mm mn in bi translated"><em class="om">点击这里成为中等会员:</em><a class="ae kz" href="https://indhumathychelliah.medium.com/membership" rel="noopener"><em class="om">https://indhumathychelliah.medium.com/membership</em></a></p></div></div>    
</body>
</html>