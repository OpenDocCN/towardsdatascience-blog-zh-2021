<html>
<head>
<title>The Complete Guide to Neural Network multi-class Classification from scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络多类分类完全指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-complete-guide-to-neural-networks-multinomial-classification-4fe88bde7839?source=collection_archive---------0-----------------------#2021-08-14">https://towardsdatascience.com/the-complete-guide-to-neural-networks-multinomial-classification-4fe88bde7839?source=collection_archive---------0-----------------------#2021-08-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="73a3" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">神经网络到底是什么？本文将向您全面完整地介绍如何从头开始编写神经网络，并使用它们进行多项式分类。包括python源代码。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/08c85dbc247fae3f276b6ccb43e8d8cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JD27RGBIg4eL47hLHs5STQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者照片:与朋友一起骑山地车2018</p></figure><p id="2f43" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">神经网络反映了人脑的行为。它们允许程序识别模式并解决机器学习中的常见问题。这是执行分类而不是逻辑回归的另一种选择。在<a class="ae lr" href="http://www.rapidtrade.com" rel="noopener ugc nofollow" target="_blank"> Rapidtrade </a>，我们使用神经网络对数据进行分类并运行回归场景。这篇文章的源代码可以在<a class="ae lr" href="https://github.com/shaunenslin/machinelearning/tree/master/python/neural%20networks" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到。</p><p id="c43a" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们将使用来自Kaggle的数据集，你可以在这里下载<a class="ae lr" href="https://www.kaggle.com/abisheksudarshan/customer-segmentation" rel="noopener ugc nofollow" target="_blank">。所以为了<strong class="kx ir">可视化</strong>我们将在本文中使用的数据，见下文。我们将使用它来训练网络根据j列对我们的客户进行分类。我们还将使用突出显示的3个特征对我们的客户进行分类。我需要3个特征来适应我的神经网络，这是最好的3个特征。</a></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lt"><img src="../Images/631d14dbf11a3f1c8c96bb3c109b5e94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e-5fcg_aEL9RXDHvxP2OMw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1:我们的数据集</p></figure><blockquote class="lu lv lw"><p id="9058" class="kv kw ls kx b ky kz jr la lb lc ju ld lx lf lg lh ly lj lk ll lz ln lo lp lq ij bi translated">请记住，我们将把所有的字母字符串值转换成数字。毕竟我们不能把字符串塞进方程<em class="iq">；-) </em></p></blockquote><p id="072b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">这是一篇相当长的文章，分为两个部分:</strong></p><ul class=""><li id="6b00" class="ma mb iq kx b ky kz lb lc le mc li md lm me lq mf mg mh mi bi translated">介绍</li><li id="6634" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq mf mg mh mi bi translated">把所有的放在一起</li></ul><p id="ac88" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">好运；-) </strong></p></div><div class="ab cl mo mp hu mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="ij ik il im in"><h1 id="917f" class="mv mw iq bd mx my mz na nb nc nd ne nf jw ng jx nh jz ni ka nj kc nk kd nl nm bi translated">介绍</h1><p id="e351" class="pw-post-body-paragraph kv kw iq kx b ky nn jr la lb no ju ld le np lg lh li nq lk ll lm nr lo lp lq ij bi translated">神经网络总是由层组成，如图2所示。这看起来很复杂，但是让我们把它拆开，让它更容易理解。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/5783147841f091ff86d7cc5b7024705b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zakUzWg-gim_Ynwv2kYlYg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2:神经网络</p></figure><p id="0e2b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">一个神经网络有6个重要的概念，我将在这里简单解释一下，但在这一系列文章中会详细介绍。</p><p id="d700" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">- <strong class="kx ir">权重</strong> — <em class="ls">这些就像我们在其他算法中使用的</em> <strong class="kx ir"> <em class="ls"> theta的</em></strong><em class="ls"><br/>-<strong class="kx ir">层</strong> — <em class="ls">我们的网络将有3层</em> <br/> - <strong class="kx ir">前向传播</strong> — <em class="ls">使用特征/权重来获得Z和一个</em> <br/> - <strong class="kx ir">反向传播</strong></em></p><p id="5edc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这个系列中，我们将建立一个三层的神经网络。在进入正题之前，让我们快速讨论一下这些层次。</p><h2 id="ad33" class="nt mw iq bd mx nu nv dn nb nw nx dp nf le ny nz nh li oa ob nj lm oc od nl oe bi translated">-输入层</h2><p id="29ef" class="pw-post-body-paragraph kv kw iq kx b ky nn jr la lb no ju ld le np lg lh li nq lk ll lm nr lo lp lq ij bi translated">参考上面的图2，我们将这一层的结果称为<strong class="kx ir"> A1 </strong>。该图层的<strong class="kx ir">大小</strong> (# units) <strong class="kx ir"> </strong>取决于我们数据集中要素的<strong class="kx ir">数量。</strong></p><p id="7cb0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">构建我们的输入层并不困难你只需简单地<strong class="kx ir">将X复制到A1 </strong>，但是添加一个所谓的<strong class="kx ir">偏向层</strong>，默认为“1”。</p><p id="2334" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">第1栏:<strong class="kx ir">偏向的</strong>图层默认为“1”<br/>第2栏:<strong class="kx ir">“曾经结婚”</strong>我们的第一个特征，并被重新标记为1/2 <br/>第3栏:<strong class="kx ir">“毕业”</strong>我们的第二个特征，并被重新标记为1/2 <br/>第4栏:<strong class="kx ir">“家庭规模”</strong>我们的第三个特征</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/0312847abd49eaaa08640f2481ddcd50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hQEace1f3W-bjgivBWGnYA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3:可视化A1 —输入层</p></figure><h2 id="14eb" class="nt mw iq bd mx nu nv dn nb nw nx dp nf le ny nz nh li oa ob nj lm oc od nl oe bi translated">-隐藏层</h2><p id="04ea" class="pw-post-body-paragraph kv kw iq kx b ky nn jr la lb no ju ld le np lg lh li nq lk ll lm nr lo lp lq ij bi translated">参考上面的图2，我们只有<strong class="kx ir"> 1个隐藏层</strong>，但是你可以为每个特征设置一个隐藏层。如果您有比我下面提到的逻辑更多的隐藏层，您将为每个隐藏层复制计算。</p><p id="c1d2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">大小(单位数)由你决定，我们选择了#features * 2 <strong class="kx ir"> ie。6台</strong>。</p><p id="b401" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">该层是在向前和向后传播期间计算的。运行<strong class="kx ir">和</strong>这两个步骤后，我们<strong class="kx ir">计算每个单元的Z2、A2和S2 </strong>。运行每个步骤后的输出如下。</p><p id="a1f5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">正向传播</strong></p><p id="2f65" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">参考图1，在这个步骤中，我们计算Z2，然后是A2。</p><ul class=""><li id="eb42" class="ma mb iq kx b ky kz lb lc le mc li md lm me lq mf mg mh mi bi translated"><strong class="kx ir"> Z2 </strong>包含我们对隐藏层中的<strong class="kx ir">假设</strong>的计算结果。</li><li id="d3be" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq mf mg mh mi bi translated">而<strong class="kx ir"> A2 </strong>也包括偏置层(col 1)并且具有应用于来自Z2的每个单元的sigmoid函数。</li></ul><p id="0b93" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，根据图4，Z2具有6列，A2具有7列。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/3462020433b082506900444a82cd4072.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_PThR1N-HVW_GUjkRwKebQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图4:可视化Z2和A2-隐藏层</p></figure><p id="5248" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">反向传播</strong></p><p id="fe27" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，在前向传播穿过所有层之后，我们然后执行<strong class="kx ir">反向传播</strong>步骤来计算<strong class="kx ir"> S2 </strong>。S2被称为每个单元假设计算的<strong class="kx ir">δ</strong>。这用于计算该θ的<strong class="kx ir">梯度</strong>，随后，结合该单元的<strong class="kx ir">成本</strong>，帮助梯度下降计算出最佳θ/重量。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/8ffdeb6162f5b28f70b641c27cf0ee65.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2nHO_lX7VKNpJB6fFjmv-g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图5:可视化S2的梯度</p></figure><h2 id="5f14" class="nt mw iq bd mx nu nv dn nb nw nx dp nf le ny nz nh li oa ob nj lm oc od nl oe bi translated">-输出层</h2><p id="10fe" class="pw-post-body-paragraph kv kw iq kx b ky nn jr la lb no ju ld le np lg lh li nq lk ll lm nr lo lp lq ij bi translated">我们的输出层给出了我们假设的结果。即。如果应用这些thetas，我们对这些客户分类的最佳猜测是什么。<strong class="kx ir">大小</strong>(#单位)源自y的数字标签。如图1所示，有7个标签，因此输出层的大小为7。</p><p id="b6f9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">与隐藏层一样，这个结果是在<strong class="kx ir">向前和向后</strong>传播的2个步骤中计算出来的。运行这两个步骤后，结果如下:</p><p id="b0c9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">正向传播</strong></p><p id="7ddb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在向前推进期间，我们将为输出层计算<strong class="kx ir"> Z3 </strong>和<strong class="kx ir"> A3 </strong>，就像我们为隐藏层所做的那样。参考上面的图1，可以看到不需要偏置柱，下面可以看到Z3和A3的结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oi"><img src="../Images/203319c49b23f8c00c77825c9ff55635.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kSmNvQqoLsH1GbTJhOtW5Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图6:可视化Z3和A3</p></figure><p id="e743" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">反向传播</strong></p><p id="8e70" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在(参考图1)我们有了Z3和A3，让我们计算<strong class="kx ir"> S3 </strong>。由于S3只是一个基本的成本计算，从Y中减去A3，所以我们将在接下来的文章中探索这些等式，但我们仍然可以看到下面的结果</p></div><div class="ab cl mo mp hu mq" role="separator"><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt mu"/><span class="mr bw bk ms mt"/></div><div class="ij ik il im in"><h1 id="e68c" class="mv mw iq bd mx my mz na nb nc nd ne nf jw ng jx nh jz ni ka nj kc nk kd nl nm bi translated">把所有的放在一起</h1><p id="de1c" class="pw-post-body-paragraph kv kw iq kx b ky nn jr la lb no ju ld le np lg lh li nq lk ll lm nr lo lp lq ij bi translated">因此，上面是一个有点尴尬，因为它可视化的输出在每一层。我们在神经网络中的主要焦点是计算神经网络成本的函数。该函数的编码将采取以下步骤。</p><ol class=""><li id="18cc" class="ma mb iq kx b ky kz lb lc le mc li md lm me lq oj mg mh mi bi translated"><strong class="kx ir">准备</strong>数据</li><li id="3c58" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq oj mg mh mi bi translated"><strong class="kx ir">设置</strong>神经网络</li><li id="a301" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq oj mg mh mi bi translated"><strong class="kx ir">初始化</strong>一组权重/θ</li><li id="fa4a" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq oj mg mh mi bi translated">创建我们的<strong class="kx ir">成本函数</strong>，它将<br/> 4.1执行<strong class="kx ir">正向传播</strong> <br/> 4.2计算正向传播<br/>的<strong class="kx ir">成本</strong>4.3执行<strong class="kx ir">反向传播<br/> </strong> 4.4计算<strong class="kx ir">增量</strong>，然后从反向道具计算<strong class="kx ir">梯度</strong>。</li><li id="95d4" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq oj mg mh mi bi translated">执行<strong class="kx ir">成本优化</strong></li><li id="a9e4" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq oj mg mh mi bi translated"><strong class="kx ir">预测</strong>结果以检查准确性</li></ol><h1 id="7123" class="mv mw iq bd mx my ok na nb nc ol ne nf jw om jx nh jz on ka nj kc oo kd nl nm bi translated">1.准备数据</h1><p id="4852" class="pw-post-body-paragraph kv kw iq kx b ky nn jr la lb no ju ld le np lg lh li nq lk ll lm nr lo lp lq ij bi translated">为了开始这个探索性的分析，首先导入库并定义使用matplotlib绘制数据的函数。根据数据的不同，并不是所有的图都要绘制。</p><blockquote class="lu lv lw"><p id="6aab" class="kv kw ls kx b ky kz jr la lb lc ju ld lx lf lg lh ly lj lk ll lz ln lo lp lq ij bi translated">嘿，我只是一个简单的内核机器人，而不是卡格尔比赛的大师！</p></blockquote><pre class="kg kh ki kj gt op oq or os aw ot bi"><span id="e449" class="nt mw iq oq b gy ou ov l ow ox">from mpl_toolkits.mplot3d import Axes3D<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.preprocessing import LabelEncoder<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt # plotting<br/>import numpy as np # linear algebra<br/>import os # accessing directory structure<br/>import pandas as pd # data processing, CSV file I/O (e.g. <br/>from scipy import optimize as opt</span><span id="9f95" class="nt mw iq oq b gy oy ov l ow ox">pd.read_csv)<br/>import matplotlib.pyplot as plt</span></pre><p id="0ed2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，让我们阅读我们的数据并快速浏览一下。</p><pre class="kg kh ki kj gt op oq or os aw ot bi"><span id="60e2" class="nt mw iq oq b gy ou ov l ow ox">df = pd.read_csv(‘customertrain.csv’)<br/>df.head()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oz"><img src="../Images/b7842cc787a98c9c4242d828a9ca7442.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*brbQyX0twUdqKoqfONywWA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图4:可视化测向</p></figure><p id="fc25" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">做一个信息，我们可以看到我们有一些工作要做的空值以及一些对象字段转换成数字。</p><pre class="kg kh ki kj gt op oq or os aw ot bi"><span id="946d" class="nt mw iq oq b gy ou ov l ow ox">df.info()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pa"><img src="../Images/44162ab41591603db2b5d6720df75d6f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3bkK4V48iLslQp-vqHolgw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图8</p></figure><p id="3923" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，让我们将对象字段转换为数字，并删除我们不需要的列。</p><pre class="kg kh ki kj gt op oq or os aw ot bi"><span id="f3c1" class="nt mw iq oq b gy ou ov l ow ox">columns = [“Gender”,”Ever_Married”,”Graduated”,”Profession”,”Spending_Score”]<br/>for feature in columns:<br/>  le = LabelEncoder()<br/>  df[feature] = le.fit_transform(df[feature])</span><span id="395c" class="nt mw iq oq b gy oy ov l ow ox">df = df.drop([“ID”,”Gender”,”Age”,”Profession”,”Work_Experience”,”Spending_Score”], axis=1)<br/>df.dropna(subset=['Var_1'], inplace=True)</span><span id="2012" class="nt mw iq oq b gy oy ov l ow ox">df.head()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pb"><img src="../Images/4c08d10fd48826495bbab8296b99ca81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VVVoWhYOdfiN11sSZJpxCg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图9准备数据后的测向结果</p></figure><p id="9164" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">使用fit_transform将我们的多项式类别编码成我们可以处理的数字。</p><pre class="kg kh ki kj gt op oq or os aw ot bi"><span id="09b5" class="nt mw iq oq b gy ou ov l ow ox">yle = LabelEncoder()<br/>df[“Var_1”] = yle.fit_transform(df[“Var_1”])<br/>df.head()</span></pre><p id="4295" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">填写缺失的特征</strong></p><p id="f710" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">回归的一个重要部分是理解哪些特征缺失了。我们可以选择忽略所有缺少值的行，或者用mode、median或mode填充它们。</p><ul class=""><li id="3a3a" class="ma mb iq kx b ky kz lb lc le mc li md lm me lq mf mg mh mi bi translated">模式=最常见的值</li><li id="3b74" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq mf mg mh mi bi translated">中位数=中间值</li><li id="5cac" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq mf mg mh mi bi translated">平均值=平均</li></ul><p id="c6f0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里有一个方便的函数，你可以调用它，用你想要的方法来填充缺失的特性。我们将选择用平均值填充值。</p><p id="0ec5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">经过下面的搞笑，你应该看到7992没有空值。</p><pre class="kg kh ki kj gt op oq or os aw ot bi"><span id="d4f4" class="nt mw iq oq b gy ou ov l ow ox">def fillmissing(df, feature, method):<br/>  if method == “mode”:<br/>    df[feature] = df[feature].fillna(df[feature].mode()[0])<br/>  elif method == “median”:<br/>    df[feature] = df[feature].fillna(df[feature].median())<br/>  else:<br/>    df[feature] = df[feature].fillna(df[feature].mean())</span><span id="c59a" class="nt mw iq oq b gy oy ov l ow ox"><br/>features_missing= df.columns[df.isna().any()]<br/>for feature in features_missing<br/>  fillmissing(df, feature= feature, method= “mean”)</span><span id="54d4" class="nt mw iq oq b gy oy ov l ow ox">df.info()</span></pre><p id="c559" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">提取Y </strong></p><p id="0466" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们将Y列提取到一个单独的数组中，并将其从dataframe中移除。</p><pre class="kg kh ki kj gt op oq or os aw ot bi"><span id="8809" class="nt mw iq oq b gy ou ov l ow ox">Y = df[“Var_1”]</span><span id="b7b0" class="nt mw iq oq b gy oy ov l ow ox">df = df.drop([“Var_1”], axis=1</span></pre><p id="d76c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在把我们的X和y列复制到矩阵中，以便于以后的矩阵操作。</p><pre class="kg kh ki kj gt op oq or os aw ot bi"><span id="a4e6" class="nt mw iq oq b gy ou ov l ow ox">X = df.to_numpy() # np.matrix(df.to_numpy())<br/>y = Y.to_numpy().transpose() # np.matrix(Y.to_numpy()).transpose()<br/>m,n = X.shape</span></pre><p id="c1c5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">规格化特征</strong></p><p id="50f4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，让我们将X归一化，使其值介于-1和1之间。我们这样做是为了让所有的特征都在一个相似的范围内。我们使用下面的等式</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/f2ab8326b9c7cf27dc2fde54c6255d4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1264/format:webp/1*mNvT07f0QpxxSyvwWSeSag.png"/></div></figure><p id="1750" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">执行标准化的目标是将所有特征降低到一个共同的比例，而不扭曲值范围的差异。这一重定要素等级的过程使得它们的均值为0，方差为1。</p><h1 id="9412" class="mv mw iq bd mx my ok na nb nc ol ne nf jw om jx nh jz on ka nj kc oo kd nl nm bi translated">2.设置神经网络</h1><p id="e1a4" class="pw-post-body-paragraph kv kw iq kx b ky nn jr la lb no ju ld le np lg lh li nq lk ll lm nr lo lp lq ij bi translated">现在，我们可以设置我们的神经网络的大小，首先，下面是我们要放在一起的神经网络。</p><p id="4a13" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在下面的初始化中，确保实现了上述网络。所以，现在你会问“合理的数字是多少？”</p><ul class=""><li id="dbd0" class="ma mb iq kx b ky kz lb lc le mc li md lm me lq mf mg mh mi bi translated">输入层=设置为尺寸的大小</li><li id="23f5" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq mf mg mh mi bi translated">隐藏层=设置为输入层* 2</li><li id="5465" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq mf mg mh mi bi translated">输出层=设置为y的标签大小。在我们的例子中，这是7个类别</li></ul><pre class="kg kh ki kj gt op oq or os aw ot bi"><span id="08cb" class="nt mw iq oq b gy ou ov l ow ox">input_layer_size = n                      # Dimension of features<br/>hidden_layer_size = input_layer_size*2    # of units in hidden layer<br/>output_layer_size = len(yle.classes_)     # number of labels</span></pre><h1 id="38d7" class="mv mw iq bd mx my ok na nb nc ol ne nf jw om jx nh jz on ka nj kc oo kd nl nm bi translated">3.初始化重量(θs)</h1><p id="c729" class="pw-post-body-paragraph kv kw iq kx b ky nn jr la lb no ju ld le np lg lh li nq lk ll lm nr lo lp lq ij bi translated">事实证明，对于梯度下降来说，这是一个相当重要的话题。如果你没有处理过梯度下降，那么先查一下<a class="ae lr" href="/geekculture/gradient-descent-in-matlab-octave-954160e2d3fa" rel="noopener ugc nofollow" target="_blank">这篇文章</a>。从上面可以看出，我们需要2组砝码。(用表示)。</p><blockquote class="lu lv lw"><p id="c0b0" class="kv kw ls kx b ky kz jr la lb lc ju ld lx lf lg lh ly lj lk ll lz ln lo lp lq ij bi translated">我们仍然经常称这些重量为θ，它们的意思是一样的。</p></blockquote><p id="9734" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们需要一套2级的thetass和一套3级的theta。每个θ是一个矩阵，大小为(L) * size(L-1)。因此如上所述:</p><ul class=""><li id="5d05" class="ma mb iq kx b ky kz lb lc le mc li md lm me lq mf mg mh mi bi translated">θ1 = 6×4矩阵</li><li id="6a56" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq mf mg mh mi bi translated">θ2 = 7×7矩阵</li></ul><p id="5c89" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们现在必须猜测哪个初始thetas应该是我们的起点。这里，epsilon来拯救我们，下面是matlab代码，可以轻松地为我们的初始权重生成一些随机的小数字。</p><pre class="kg kh ki kj gt op oq or os aw ot bi"><span id="8e9d" class="nt mw iq oq b gy ou ov l ow ox">def initializeWeights(L_in, L_out):<br/>  epsilon_init = 0.12<br/>  W = np.random.rand(L_out, 1 + L_in) * 2 * \<br/>     epsilon_init - epsilon_init<br/>  return W</span></pre><p id="c631" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如上所述，在对每个θ的大小运行上述函数后，我们将得到一些好的小随机初始值，如图7所示。对于上面的图1，我们提到的权重指的是下面矩阵的第1行。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pd"><img src="../Images/7e2b4a41d8d6fcd4f846a7f11cf40435.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sjYM5fx1kP4EDGf9-j3hcA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图7:初始θ</p></figure><h1 id="386b" class="mv mw iq bd mx my ok na nb nc ol ne nf jw om jx nh jz on ka nj kc oo kd nl nm bi translated">4.成本函数</h1><p id="36e3" class="pw-post-body-paragraph kv kw iq kx b ky nn jr la lb no ju ld le np lg lh li nq lk ll lm nr lo lp lq ij bi translated">我们需要一个函数来实现用于执行分类的两层神经网络的神经网络成本函数。</p><p id="e3a1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在GitHub代码中，checknn.py我们名为<strong class="kx ir"> nnCostFunction </strong>的成本函数将返回:</p><ul class=""><li id="07af" class="ma mb iq kx b ky kz lb lc le mc li md lm me lq mf mg mh mi bi translated">梯度应该是神经网络偏导数的“展开”向量</li><li id="0be1" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq mf mg mh mi bi translated">最后的J是这个重量的成本。</li></ul><p id="28fd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们的成本函数需要执行以下操作:</p><ul class=""><li id="1eb9" class="ma mb iq kx b ky kz lb lc le mc li md lm me lq mf mg mh mi bi translated">将nn_params重新整形为参数θ1和θ2，这是我们的2层神经网络的权重矩阵</li><li id="b97f" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq mf mg mh mi bi translated">执行正向传播以计算(a)和(z)</li><li id="e023" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq mf mg mh mi bi translated">执行反向传播以使用(a)计算(s)</li></ul><p id="a0ea" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，我们的成本函数首先需要将theta重新整形为隐藏层和输出层的theta。</p><pre class="kg kh ki kj gt op oq or os aw ot bi"><span id="d1f2" class="nt mw iq oq b gy ou ov l ow ox"># Reshape nn_params back into the parameters Theta1 and Theta2, <br/># the weight matrices for our 2 layer neural network</span><span id="d4a7" class="nt mw iq oq b gy oy ov l ow ox">Theta1 = nn_params[:hidden_layer_size * \<br/>   (input_layer_size + 1)].reshape( \<br/>   (hidden_layer_size, input_layer_size + 1))<br/>Theta2 = nn_params[hidden_layer_size * \<br/>   (input_layer_size + 1):].reshape( \<br/>   (num_labels, hidden_layer_size + 1))</span><span id="6993" class="nt mw iq oq b gy oy ov l ow ox"># Setup some useful variables<br/>m = X.shape[0]</span></pre><h2 id="f7a5" class="nt mw iq bd mx nu nv dn nb nw nx dp nf le ny nz nh li oa ob nj lm oc od nl oe bi translated">4.1正向传播</h2><p id="b4fa" class="pw-post-body-paragraph kv kw iq kx b ky nn jr la lb no ju ld le np lg lh li nq lk ll lm nr lo lp lq ij bi translated">前向传播是神经网络的重要组成部分。这并不像听起来那么难。</p><p id="1005" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在图7中，我们可以看到去掉了很多细节的网络图。我们将重点介绍第二级的一个单元和第三级的一个单元。这种理解可以复制到所有单位。注意我们可以做的矩阵乘法(图7中蓝色部分)来执行正向传播。</p><blockquote class="lu lv lw"><p id="eb15" class="kv kw ls kx b ky kz jr la lb lc ju ld lx lf lg lh ly lj lk ll lz ln lo lp lq ij bi translated">我在每一层显示一个单元的细节，但是你可以重复所有层的逻辑。</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pe"><img src="../Images/ffee7e5567ac4121fb3aeb910f7589a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*99Uw-WwzoxOa4M_SzyqInA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图7:神经网络正向传播</p></figure><blockquote class="lu lv lw"><p id="39ec" class="kv kw ls kx b ky kz jr la lb lc ju ld lx lf lg lh ly lj lk ll lz ln lo lp lq ij bi translated">在我们展示正向推进代码之前，让我们谈一谈正向推进过程中需要的两个概念。</p></blockquote><p id="d057" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> 4.1.1乙状结肠功能</strong></p><p id="5cc3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">既然是做分类，我们就用sigmoid来评估我们的预测。sigmoid函数是一种数学函数，具有特征性的“S”形曲线或sigmoid曲线。sigmoid函数的一个常见示例是逻辑函数，如图一所示，由以下公式定义</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pf"><img src="../Images/2ebbb720c9108f772e37f198f06c1da0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4KcQ9cZhWRebVi97_BpW4A.png"/></div></div></figure><p id="eade" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在github中，checknn.py创建了以下方便的函数:</p><ul class=""><li id="11a1" class="ma mb iq kx b ky kz lb lc le mc li md lm me lq mf mg mh mi bi translated">sigmoid是计算输入参数Z的sigmoid的简便函数</li><li id="e20c" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq mf mg mh mi bi translated">sigmoidGradient计算在z处计算的sigmoid函数的梯度。无论z是矩阵还是向量，这都应该有效。</li></ul><pre class="kg kh ki kj gt op oq or os aw ot bi"><span id="2f64" class="nt mw iq oq b gy ou ov l ow ox">def sigmoid(z):<br/>  g = np.frompyfunc(lambda x: 1 / (1 + np.exp(-x)), 1, 1)<br/>  return g(z).astype(z.dtype)</span><span id="9949" class="nt mw iq oq b gy oy ov l ow ox">def sigmoidGradient(z)<br/>  return sigmoid(z) * (1 - sigmoid(z))</span></pre><p id="e892" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> 4.1.2正规化</strong></p><p id="80da" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们将实现正则化，因为数据科学专业人员面临的最常见问题之一是避免过度拟合。过度拟合会给你一种情况，你的模型在训练数据上表现得非常好，但是不能预测测试数据。神经网络很复杂，使它们更容易过度拟合。正则化是一种对学习算法进行轻微修改以使模型更好地概括的技术。这反过来也提高了模型在不可见数据上的性能。</p><p id="8cd6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你研究过机器学习中正则化的概念，你会有一个公平的想法，正则化惩罚系数。在深度学习中，它实际上惩罚了节点的权重矩阵。</p><p id="681f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们在nnCostFunction中通过传递一个lambda来实现正则化，该lambda用于惩罚计算的梯度和成本。</p><p id="7cd0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir"> 4.1.3实施正向推进</strong></p><p id="4aba" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">根据图1，让我们计算A1。你可以看到它几乎是我的X的特征，我们在前面添加了硬编码为“1”的bias列。下面是实现这一点的python代码:</p><pre class="kg kh ki kj gt op oq or os aw ot bi"><span id="6512" class="nt mw iq oq b gy ou ov l ow ox"># Add ones to the X data matrix<br/>a1 = np.insert(X, 0, 1, axis=1)</span></pre><p id="bdb9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在，结果将为您提供图4中A1的结果。特别注意前面增加的偏差栏“1”。</p><p id="906f" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">很好，A1完成了，让我们继续A2。在我们得到A2之前，我们将首先运行一个假设来计算Z2。一旦你有了假设，你就可以通过sigmoid函数得到A2。同样，根据图1，将bias列添加到前面。</p><pre class="kg kh ki kj gt op oq or os aw ot bi"><span id="14c7" class="nt mw iq oq b gy ou ov l ow ox"># Perform forward propagation for layer 2<br/>z2 = np.matmul(a1, Theta1.transpose())<br/>a2 = sigmoid(z2)<br/>a2 = np.insert(a2, 0, 1, axis=1)</span></pre><p id="b81b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">好了，我们快到了…现在在A3上，让我们做与A2相同的事情，但是这一次，我们不担心添加偏差列。</p><pre class="kg kh ki kj gt op oq or os aw ot bi"><span id="f1ea" class="nt mw iq oq b gy ou ov l ow ox">z3 = np.matmul(a2, Theta2.transpose())<br/>a3 = sigmoid(z3)</span></pre><p id="7b62" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">你可能会问，<strong class="kx ir">“我们为什么要保留Z2&amp;Z3】</strong>。嗯，我们在反向传播中需要这些。所以我们最好把它们放在手边；-).</p><h2 id="446c" class="nt mw iq bd mx nu nv dn nb nw nx dp nf le ny nz nh li oa ob nj lm oc od nl oe bi translated">4.2计算前进支柱的成本</h2><p id="e9a8" class="pw-post-body-paragraph kv kw iq kx b ky nn jr la lb no ju ld le np lg lh li nq lk ll lm nr lo lp lq ij bi translated">在我们继续之前，如果您了解我们的Y列(图9 ),其中包含用于对客户进行分类的标签。然后，为了计算成本，我们需要将Y重新格式化为一个与标签数量相对应的矩阵。在我们的案例中，我们的客户有7个类别。</p><p id="8496" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">图8显示了如何将<strong class="kx ir"> Y </strong>转换为矩阵y_one_hot，并且标签现在在适当的列中显示为二进制。</p><pre class="kg kh ki kj gt op oq or os aw ot bi"><span id="db91" class="nt mw iq oq b gy ou ov l ow ox"># turn Y into a matrix with a new column for each category and marked with 1<br/>y_one_hot = np.zeros_like(a3)<br/>for i in range(m):<br/>  y_one_hot[i, y[i] - 1] = 1</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pg"><img src="../Images/1fd422f3ec87c7ff7feabf4c69416431.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pWVoDP59ylaT-N8mGCLziw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图8:将Y从vector映射到矩阵y_one_hot</p></figure><p id="d4a6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">现在我们有了矩阵形式的Y，让我们看看计算成本的等式。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ph"><img src="../Images/6680b1e95f81459234371615d8f906dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q_Dc_qd4FR0nIP1Ie5t8kg.png"/></div></div></figure><p id="0a51" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">嗯，这非常复杂，但好消息是，通过一些矩阵操作，我们可以用如下几行python代码来完成。</p><pre class="kg kh ki kj gt op oq or os aw ot bi"><span id="8c80" class="nt mw iq oq b gy ou ov l ow ox"># Calculate the cost of our forward prop<br/>ones = np.ones_like(a3<br/>A = np.matmul(y_one_hot.transpose(), np.log(a3)) + \<br/>  np.matmul((ones - y_one_hot).transpose(), np.log(ones - a3))</span><span id="8077" class="nt mw iq oq b gy oy ov l ow ox">J = -1 / m * A.trace()<br/>J += lambda_ / (2 * m) * \<br/>  (np.sum(Theta1[:, 1:] ** 2) + np.sum(Theta2[:, 1:] ** 2))</span></pre><h2 id="1c91" class="nt mw iq bd mx nu nv dn nb nw nx dp nf le ny nz nh li oa ob nj lm oc od nl oe bi translated">4.3执行反向传播</h2><p id="5cab" class="pw-post-body-paragraph kv kw iq kx b ky nn jr la lb no ju ld le np lg lh li nq lk ll lm nr lo lp lq ij bi translated">因此，我们简化了图1中的神经网络，首先只显示了细节:</p><ul class=""><li id="4816" class="ma mb iq kx b ky kz lb lc le mc li md lm me lq mf mg mh mi bi translated">从Y减去A1(3)计算S3</li><li id="f5cb" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq mf mg mh mi bi translated">此后，使用下面提到的θ乘以S3来计算线性方程。S2。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/558f49b6fe64c4ef63367d21e5679c85.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*mhlJux4P4D4YU0PFpsQFWA.png"/></div></figure><p id="5ee5" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">由于一张图片描绘了1000个单词，图9应该解释我们用什么来计算S3以及随后的S2(用红色标记)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pj"><img src="../Images/5974445f5318dd78460922cd67ace880.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I8rly0VynBGl8utQBNkNDw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图9:反向传播</p></figure><p id="29ed" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">从(3)中，我们了解了我们的权重(θs)是如何初始化的，所以只是为了可视化图9所指的权重(φ),参见下面的图10。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pk"><img src="../Images/1d48f21b0a374a34d51dece8e0e48ab3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i2oI_z_3l_kbGeDwldsF6A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图9:反向传播中使用的权重</p></figure><p id="42bc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">因此，有了矩阵操作的帮助，正向传播在python中并不是一件困难的事情</p><pre class="kg kh ki kj gt op oq or os aw ot bi"><span id="13d0" class="nt mw iq oq b gy ou ov l ow ox"># Perform backward propagation to calculate deltas<br/>s3 = a3 - yv<br/>s2 = np.matmul(s3, Theta2) * \<br/>  sigmoidGradient(np.insert(z2, 0, 1, axis=1))</span><span id="8cbb" class="nt mw iq oq b gy oy ov l ow ox"># remove z2 bias column<br/>s2 = s2[:, 1:]</span></pre><h2 id="c2db" class="nt mw iq bd mx nu nv dn nb nw nx dp nf le ny nz nh li oa ob nj lm oc od nl oe bi translated"><strong class="ak"> 4.4从反向道具计算坡度</strong></h2><p id="bdd0" class="pw-post-body-paragraph kv kw iq kx b ky nn jr la lb no ju ld le np lg lh li nq lk ll lm nr lo lp lq ij bi translated">我们需要返回梯度作为我们的成本函数的一部分，这是需要的，因为梯度下降是在<strong class="kx ir">反向属性</strong>中发生的过程，其中目标是基于权重<em class="ls"> w </em>在相反方向上连续重新采样模型参数的梯度，持续更新，直到我们达到函数<em class="ls"> J(w) </em>的<strong class="kx ir">全局最小值</strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/558f49b6fe64c4ef63367d21e5679c85.png" data-original-src="https://miro.medium.com/v2/resize:fit:916/format:webp/1*mhlJux4P4D4YU0PFpsQFWA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">反向推进方程</p></figure><p id="bfdd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">简单来说，我们用梯度下降最小化代价函数，<em class="ls"> J(w) </em>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pl"><img src="../Images/e1865c60e436e2b3fd3cf034e9ee8489.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S3ty3gMD13_Kr7j9jGlaEQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图10</p></figure><p id="f6bd" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">再一次，矩阵操作拯救了它，只需要几行代码。</p><p id="1fd9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们的第一步是计算一个可以用来调整我们的成本的惩罚。如果你想要一个关于正规化的解释，那么看看这篇<a class="ae lr" href="/geekculture/logistics-regression-regularisation-2-3-4a0d8b85564c" rel="noopener ugc nofollow" target="_blank">文章</a>。</p><pre class="kg kh ki kj gt op oq or os aw ot bi"><span id="869f" class="nt mw iq oq b gy ou ov l ow ox"># calculate regularized penalty, replace 1st column with zeros<br/>p1 = (lambda_/m) * np.insert(Theta1[:, 1:], 0, 0, axis=1)<br/>p2 = (lambda_/m) * np.insert(Theta2[:, 1:], 0, 0, axis=1)</span></pre><p id="dd26" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于成本优化，我们需要反馈这组特定权重的梯度。图2显示了梯度一旦被绘制出来。对于输入到我们的成本函数的权重集，这将是绘制线的梯度。</p><pre class="kg kh ki kj gt op oq or os aw ot bi"><span id="aeed" class="nt mw iq oq b gy ou ov l ow ox"># gradients / partial derivitives<br/>Theta1_grad = delta_1 / m + p1<br/>Theta2_grad = delta_2 / m + p2<br/>grad = np.concatenate((Theta1_grad.flatten(), <br/>  Theta2_grad.flatten()), axis=None)</span></pre><p id="55e7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，成本优化函数不知道如何处理2θ，所以让我们把它们展开成一个向量，结果如图5所示。</p><pre class="kg kh ki kj gt op oq or os aw ot bi"><span id="0657" class="nt mw iq oq b gy ou ov l ow ox">grad = np.concatenate((Theta1_grad.flatten(),  <br/>   Theta2_grad.flatten()), axis=None)</span></pre><p id="16d2" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">好了，哇，这是很多信息，但我们的成本函数已经完成，让我们继续运行梯度下降和成本优化。</p><h1 id="1757" class="mv mw iq bd mx my ok na nb nc ol ne nf jw om jx nh jz on ka nj kc oo kd nl nm bi translated">5.执行成本优化</h1><h2 id="ccc1" class="nt mw iq bd mx nu nv dn nb nw nx dp nf le ny nz nh li oa ob nj lm oc od nl oe bi translated">5.1验证我们的成本函数</h2><p id="045b" class="pw-post-body-paragraph kv kw iq kx b ky nn jr la lb no ju ld le np lg lh li nq lk ll lm nr lo lp lq ij bi translated">一件很难理解的事情是我们的成本函数是否表现良好。检查这一点的一个好方法是运行一个名为checknn的函数。</p><p id="5d69" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">创建一个小的神经网络来检查反向传播梯度，它将输出由反向传播代码产生的分析梯度和数值梯度(使用computeNumericalGradient计算)。这两个梯度计算应该产生非常相似的值。</p><p id="a8e9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你想更深入地研究这种技术背后的理论，这在吴恩达的机器学习课程第4周是很难的。</p><p id="7824" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">您不需要每次都运行，只需在第一次设置成本函数时运行即可。</p><p id="2b99" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这里就不放代码了，但是在checknn.py里面检查一下github项目有没有以下功能:</p><ul class=""><li id="bfc6" class="ma mb iq kx b ky kz lb lc le mc li md lm me lq mf mg mh mi bi translated">检查梯度</li><li id="a6bf" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq mf mg mh mi bi translated">去基尼化权重</li><li id="9724" class="ma mb iq kx b ky mj lb mk le ml li mm lm mn lq mf mg mh mi bi translated">计算美国梯度</li></ul><p id="3e17" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">运行cheecknn之后，您应该会得到以下结果</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pm"><img src="../Images/3693b2b24eb79d4b1cca9c0959c58c8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Up1hHvVZX0a3mTs3XzafdA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图11:验证我们的成本函数的结果</p></figure><h2 id="b563" class="nt mw iq bd mx nu nv dn nb nw nx dp nf le ny nz nh li oa ob nj lm oc od nl oe bi translated">5.2梯度下降</h2><p id="f0cb" class="pw-post-body-paragraph kv kw iq kx b ky nn jr la lb no ju ld le np lg lh li nq lk ll lm nr lo lp lq ij bi translated">梯度下降是一种优化算法，主要用于寻找函数的最小值。在机器学习中，梯度下降用于更新模型中的参数。参数可以根据算法而变化，例如线性回归中的<em class="ls">系数</em>和神经网络中的权重。我们将使用SciPy优化模块来运行我们的梯度下降。</p><pre class="kg kh ki kj gt op oq or os aw ot bi"><span id="3133" class="nt mw iq oq b gy ou ov l ow ox">from scipy import optimize as opt</span><span id="3736" class="nt mw iq oq b gy oy ov l ow ox">print('Training Neural Network... ')<br/>#  Change the MaxIter to a larger value to see how more <br/>#  training helps.<br/>options = {'maxiter': 50, 'disp': True}</span><span id="6c9d" class="nt mw iq oq b gy oy ov l ow ox"># You should also try different values of lambda<br/>lambda_ = 1;</span><span id="3151" class="nt mw iq oq b gy oy ov l ow ox"># Create cost function shortcuts to be minimized<br/>fun = lambda nn_params: nnCostFunction2(nn_params, input_layer_size, hidden_layer_size, output_layer_size, xn, y, lambda_)[0]</span><span id="33a4" class="nt mw iq oq b gy oy ov l ow ox">jac = lambda nn_params: nnCostFunction2(nn_params, input_layer_size, hidden_layer_size, output_layer_size, xn, y, lambda_)[1]</span><span id="1820" class="nt mw iq oq b gy oy ov l ow ox"># Now, costFunction is a function that takes in only one <br/># argument (the neural network parameters)</span><span id="e0d1" class="nt mw iq oq b gy oy ov l ow ox">res = opt.minimize(fun, nn_params, method='CG', jac=jac, options=options)</span><span id="f1b6" class="nt mw iq oq b gy oy ov l ow ox">nn_params = res.x<br/>cost = res.fun<br/>print(res.message)</span><span id="20da" class="nt mw iq oq b gy oy ov l ow ox">print(cost)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pe"><img src="../Images/6807161ff1113520ba8dca84f1f9ed20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B9skP3UQa5DJVNg1zLN9mw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图12:运行梯度下降的结果</p></figure><p id="c6de" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">通过使用reshape为每一层获取我们的thetas</p><pre class="kg kh ki kj gt op oq or os aw ot bi"><span id="02da" class="nt mw iq oq b gy ou ov l ow ox"># Obtain Theta1 and Theta2 back from nn_params</span><span id="2097" class="nt mw iq oq b gy oy ov l ow ox">Theta1 = nn_params[:hidden_layer_size * (input_layer_size + <br/>   1)].reshape((hidden_layer_size, input_layer_size + 1))</span><span id="47dc" class="nt mw iq oq b gy oy ov l ow ox">Theta2 = nn_params[hidden_layer_size * (input_layer_size + <br/>   1):].reshape((output_layer_size, hidden_layer_size + 1))</span></pre><h1 id="3b6c" class="mv mw iq bd mx my ok na nb nc ol ne nf jw om jx nh jz on ka nj kc oo kd nl nm bi translated">6.预测结果以检查准确性</h1><p id="fdf0" class="pw-post-body-paragraph kv kw iq kx b ky nn jr la lb no ju ld le np lg lh li nq lk ll lm nr lo lp lq ij bi translated">现在我们有了最佳权重(thetas ),让我们使用它们来进行预测，以检查准确性。</p><pre class="kg kh ki kj gt op oq or os aw ot bi"><span id="8711" class="nt mw iq oq b gy ou ov l ow ox">pred = predict(Theta1, Theta2, X)</span><span id="872c" class="nt mw iq oq b gy oy ov l ow ox">print(f’Training Set Accuracy: {(pred == y).mean() * 100:f}’)</span></pre><p id="f728" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">你应该得到65.427928% <br/>的准确率，是的，有点低，但那是我们正在处理的数据集。我用物流回归&amp; SVM测试了这个数据集，得到了相同的结果。</p><h1 id="4c8b" class="mv mw iq bd mx my ok na nb nc ol ne nf jw om jx nh jz on ka nj kc oo kd nl nm bi translated">结论</h1><p id="7705" class="pw-post-body-paragraph kv kw iq kx b ky nn jr la lb no ju ld le np lg lh li nq lk ll lm nr lo lp lq ij bi translated">我希望这篇文章能让你对神经网络有更深层次的理解，以及你如何用它来分类数据。让我知道你怎么走…</p></div></div>    
</body>
</html>