<html>
<head>
<title>Comprehensive guide for Principal Component Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">主成分分析综合指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/comprehensive-guide-for-principal-component-analysis-7bf2b4a048ae?source=collection_archive---------6-----------------------#2021-09-02">https://towardsdatascience.com/comprehensive-guide-for-principal-component-analysis-7bf2b4a048ae?source=collection_archive---------6-----------------------#2021-09-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="eae4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用python实现主成分分析的理论和实践部分</h2></div><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="9af3" class="kr ks it kn b gy kt ku l kv kw"><strong class="kn iu"><em class="kx">Table of Contents<br/></em>1. Introduction<br/>2. Principal Component Analysis (PCA)<br/>3. Theory<br/>3.1. Calculating PCA<br/>3.1.1. Rescaling (Standardization)<br/>3.1.2. Covariance Matrix<br/>3.1.3. Eigenvalues and Eigenvectors<br/>3.1.4. Sorting in Descent Order<br/>3.2. Is PCA one of the feature extraction&amp;feature selection methods?<br/>4. Implementation<br/>4.1. Traditional Machine Learning Approaches<br/>4.2. Deep Learning Approaches<br/>5. PCA Types<br/>5.1. Kernel PCA<br/>5.2. Sparse PCA<br/>5.3. Randomized PCA<br/>5.4. Incremental PCA</strong></span></pre><h1 id="358e" class="ky ks it bd kz la lb lc ld le lf lg lh jz li ka lj kc lk kd ll kf lm kg ln lo bi translated"><strong class="ak"> 1。简介</strong></h1><p id="1fff" class="pw-post-body-paragraph lp lq it lr b ls lt ju lu lv lw jx lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">本文涵盖了PCA的定义，没有Sklearn库的PCA理论部分的Python实现，PCA与特征选择&amp;特征提取的区别，机器学习&amp;深度学习的实现，并举例说明了PCA的类型。</p><figure class="ki kj kk kl gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi ml"><img src="../Images/e5afcf335b444b45adb671ed84277b9b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sJoKMNnf4Nx1Qggg"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">由<a class="ae mx" href="https://unsplash.com/@nate_dumlao?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">内森·杜姆劳</a>在<a class="ae mx" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="51d7" class="ky ks it bd kz la lb lc ld le lf lg lh jz li ka lj kc lk kd ll kf lm kg ln lo bi translated">2.主成分分析</h1><p id="4538" class="pw-post-body-paragraph lp lq it lr b ls lt ju lu lv lw jx lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">主成分分析是一种基于数学和统计学的非常有用的方法，它通过从不同角度对数据集进行评价来进行降维。它在机器学习中的任务是减少数据集中输入的维数，并通过算法或根据无监督方法中的特征对数据集进行分组来促进学习。这个降维过程是各种数学运算的结果。以坐标平面中具有两个特征的2D数据集(x，y)为例。当我们用主成分分析将数据集转换成1D时，数据集的分类变得容易得多。现在让我们用PCA实现并可视化降维:</p><figure class="ki kj kk kl gt mm"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="1bff" class="pw-post-body-paragraph lp lq it lr b ls na ju lu lv nb jx lx ly nc ma mb mc nd me mf mg ne mi mj mk im bi translated">癌症数据集(编码中定义为cancer_data)由596个样本和30个特征组成。首先使用StandardScaler对这些数字特征进行缩放，然后使用Sklearn库导入的PCA方法对数据集进行二维处理，并对“恶性”和“良性”目标进行着色，如图1所示。X轴代表8个分量中的第一个，y轴代表8个分量中的第二个分量。</p><figure class="ki kj kk kl gt mm gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/89d439fb7e991312d62493441bf00344.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/1*PhwoCFRF5loq27Yi9aBj8w.png"/></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">图一。第一主成分图和第二主成分图，作者图像</p></figure><p id="74b6" class="pw-post-body-paragraph lp lq it lr b ls na ju lu lv nb jx lx ly nc ma mb mc nd me mf mg ne mi mj mk im bi translated">从图1中可以看出，在PCA过程之后，不使用任何算法就可以进行分类，这几乎是人眼可以预测的。然而，考虑到30个特征的数值数据集，这对于人类来说是根本不可能的。</p><p id="ec55" class="pw-post-body-paragraph lp lq it lr b ls na ju lu lv nb jx lx ly nc ma mb mc nd me mf mg ne mi mj mk im bi translated">看各个分量的方差值，看到有<strong class="lr iu">【0.44272026，0.18971182，0.09393163，0.06602135，0.05495768，0.04024522，0.02250734，0.01588724】</strong>。第一和第二分量对应于整个数据集的63%。8个组成部分的累积方差图如图2所示。</p><figure class="ki kj kk kl gt mm gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/0c4cee3bf6f4a1be4d9696639cf26061.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*yiiJI2Y-VirVCqp50jAW4A.png"/></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">图二。组件数量的累积方差，按作者排序的图像</p></figure><p id="831c" class="pw-post-body-paragraph lp lq it lr b ls na ju lu lv nb jx lx ly nc ma mb mc nd me mf mg ne mi mj mk im bi translated">将数据集转换为不同维度时，在新维度中定位数据的过程称为<strong class="lr iu">投影</strong>。在图3中，根据新创建的维度和与<em class="kx"> mglearn </em>库中PCA图的差异，可以看出区别。</p><figure class="ki kj kk kl gt mm gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/cb7af4020b24822745ae6719c6cd6f2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*kQmvzBfEFsjr0UqqP74xiQ.png"/></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">图3。使用mglearn库的PCA可视化，图片由作者提供</p></figure><p id="065c" class="pw-post-body-paragraph lp lq it lr b ls na ju lu lv nb jx lx ly nc ma mb mc nd me mf mg ne mi mj mk im bi translated">那么这个将30维转化为2维的神奇过程背后到底隐藏着什么呢？</p><h1 id="9c70" class="ky ks it bd kz la lb lc ld le lf lg lh jz li ka lj kc lk kd ll kf lm kg ln lo bi translated">3.理论</h1><p id="36fa" class="pw-post-body-paragraph lp lq it lr b ls lt ju lu lv lw jx lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">PCA改变组件的方向以实现最大方差，并以此方式降低数据集的维数。</p><blockquote class="ni nj nk"><p id="62c7" class="lp lq kx lr b ls na ju lu lv nb jx lx nl nc ma mb nm nd me mf nn ne mi mj mk im bi translated">方差:给出关于数据集分布的信息。例如，让我们举一个将5cl液体装入瓶子的例子。假设第一种情况下的瓶子是4cl、5cl、5cl、5cl、6cl，第二种情况下的瓶子是2cl、3cl、5cl、7cl、8cl。虽然两者的平均值都是5cl，但是第一种情况下的填充物将比第二种情况下的填充物更均匀，因为第一种情况下的样本分布方差低于第二种情况下的样本分布方差。这表明分配更加成功。</p></blockquote><h2 id="9fe7" class="kr ks it bd kz no np dn ld nq nr dp lh ly ns nt lj mc nu nv ll mg nw nx ln ny bi translated">3.1.<strong class="ak">计算PCA </strong></h2><p id="6745" class="pw-post-body-paragraph lp lq it lr b ls lt ju lu lv lw jx lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">图4显示了如何使用PCA进行降维的流程图。</p><figure class="ki kj kk kl gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi nz"><img src="../Images/b38c9b64b009d7da02ff915f7794aaa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*E7QGSmI2c9_j1q_FOvZWJA.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">图4。降维流程图，作者图片</p></figure><p id="e7f8" class="pw-post-body-paragraph lp lq it lr b ls na ju lu lv nb jx lx ly nc ma mb mc nd me mf mg ne mi mj mk im bi translated">PCA是在不使用sklearn库的情况下通过数学运算创建的，并且与sklearn库的组件进行比较。</p><figure class="ki kj kk kl gt mm"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="5c3e" class="pw-post-body-paragraph lp lq it lr b ls na ju lu lv nb jx lx ly nc ma mb mc nd me mf mg ne mi mj mk im bi translated">每一步的输出都逐步显示在表格中。</p><p id="0881" class="pw-post-body-paragraph lp lq it lr b ls na ju lu lv nb jx lx ly nc ma mb mc nd me mf mg ne mi mj mk im bi translated">3.1.1。重新缩放(标准化)</p><p id="41f0" class="pw-post-body-paragraph lp lq it lr b ls na ju lu lv nb jx lx ly nc ma mb mc nd me mf mg ne mi mj mk im bi translated">在第一阶段，对数值数据集应用缩放。为此，计算每个特征的平均值和标准差。使用这些计算，按照公式x _ new =(x-mean(x的列))/STD(x的列)创建新的数据集。对于此操作，每个特征的平均值= 0，标准值= 1(用标准定标器标准化)</p><figure class="ki kj kk kl gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi oa"><img src="../Images/759b364e7c4096af317f45b4f2535e1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QWx8OAWYc3eaqTUMspCubw.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">图5。数据集(左)和缩放数据集(右)，按作者分类的图像</p></figure><p id="2ea0" class="pw-post-body-paragraph lp lq it lr b ls na ju lu lv nb jx lx ly nc ma mb mc nd me mf mg ne mi mj mk im bi translated"><strong class="lr iu"> 3.1.2。协方差矩阵</strong></p><p id="33a1" class="pw-post-body-paragraph lp lq it lr b ls na ju lu lv nb jx lx ly nc ma mb mc nd me mf mg ne mi mj mk im bi translated">协方差矩阵根据以下公式创建，缩放后的数据集根据彼此之间的关系完全重建:</p><figure class="ki kj kk kl gt mm gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/21b43f8315539e7c5d02817c7cc1ef6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*BasVp8zcISSOLfntMb5LDg.png"/></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">图6。协方差矩阵公式，<a class="ae mx" href="https://medium.com/analytics-vidhya/understanding-principle-component-analysis-pca-step-by-step-e7a4bb4031d9" rel="noopener">来源</a></p></figure><p id="5bf1" class="pw-post-body-paragraph lp lq it lr b ls na ju lu lv nb jx lx ly nc ma mb mc nd me mf mg ne mi mj mk im bi translated">根据该方程计算所有协方差值后，得到(n_features，n_features)的矩阵。主要目标是重新排列数据集，以最大化数据集中的方差。为了检测这一点，需要协方差矩阵。</p><blockquote class="ni nj nk"><p id="1e25" class="lp lq kx lr b ls na ju lu lv nb jx lx nl nc ma mb nm nd me mf nn ne mi mj mk im bi translated">协方差是相关性的度量。通过协方差，我们知道两个变量一起变化的方向(正的话方向相同，负的话方向相反)。然后我们可以用相关性来找出这种变化的程度。协方差以单位来度量。在数据科学中，协方差涵盖两个变量或数据集的关系。</p></blockquote><figure class="ki kj kk kl gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi oc"><img src="../Images/27b1bfd9c5eca83220498c9e123b3406.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0_2S-0nPDRzaMoozHudujw.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">图7。协方差矩阵，图片由作者提供</p></figure><p id="3a35" class="pw-post-body-paragraph lp lq it lr b ls na ju lu lv nb jx lx ly nc ma mb mc nd me mf mg ne mi mj mk im bi translated">3.1.3。特征值和特征向量</p><p id="75ab" class="pw-post-body-paragraph lp lq it lr b ls na ju lu lv nb jx lx ly nc ma mb mc nd me mf mg ne mi mj mk im bi translated">从具有协方差矩阵的数据集中计算特征值，并且获得相应的特征向量作为特征的总数。</p><figure class="ki kj kk kl gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi od"><img src="../Images/601dc5859887d386d0c79d4a9420fa85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h7F0-_qVsVBe0IZnlGoJSQ.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">图8。特征值(左)和相应的特征向量(右)，作者图片</p></figure><p id="5193" class="pw-post-body-paragraph lp lq it lr b ls na ju lu lv nb jx lx ly nc ma mb mc nd me mf mg ne mi mj mk im bi translated"><strong class="lr iu"> 3.1.4。按降序排序</strong></p><p id="9630" class="pw-post-body-paragraph lp lq it lr b ls na ju lu lv nb jx lx ly nc ma mb mc nd me mf mg ne mi mj mk im bi translated">特征值从最高到最低排序。希望为PCA选择多少分量，选择对应于该数量的特征值的特征向量，并降低数据集维数。</p><blockquote class="ni nj nk"><p id="05c2" class="lp lq kx lr b ls na ju lu lv nb jx lx nl nc ma mb nm nd me mf nn ne mi mj mk im bi translated">关于特征值的提示:</p><p id="9bfa" class="lp lq kx lr b ls na ju lu lv nb jx lx nl nc ma mb nm nd me mf nn ne mi mj mk im bi translated">矩阵x的迹等于其特征值之和。</p><p id="1335" class="lp lq kx lr b ls na ju lu lv nb jx lx nl nc ma mb nm nd me mf nn ne mi mj mk im bi translated">矩阵x的行列式等于其特征值的乘积。</p><p id="b20d" class="lp lq kx lr b ls na ju lu lv nb jx lx nl nc ma mb nm nd me mf nn ne mi mj mk im bi translated">矩阵x的秩等于矩阵x的非零特征值的个数。</p></blockquote><figure class="ki kj kk kl gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi oe"><img src="../Images/c8b225324ee78d2b2bb6efe9e4167d73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t2boOYy8E8VI6CyaAWlAyw.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">图9。无Sklearn的PCA结果无Sklearn(左)，有Sklearn的PCA结果(右)，图片作者</p></figure><p id="fcf3" class="pw-post-body-paragraph lp lq it lr b ls na ju lu lv nb jx lx ly nc ma mb mc nd me mf mg ne mi mj mk im bi translated">用数学方程计算数据集的第一和第二主成分，可以看出，结果与导入Sklearn库的结果相同。</p><h2 id="5855" class="kr ks it bd kz no np dn ld nq nr dp lh ly ns nt lj mc nu nv ll mg nw nx ln ny bi translated"><strong class="ak"> 3.2。PCA是特征选择&amp;特征提取方法之一吗？</strong></h2><p id="f40f" class="pw-post-body-paragraph lp lq it lr b ls lt ju lu lv lw jx lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">既可以是也可以不是。由于主成分分析降低了特征的维数，因此它可以被理解为提取特征或选择影响结果的最有效的特征。但是理解了上面提到的理论部分，这个就清楚了。在PCA机器学习应用之外，它是关于在另一个坐标系中解释数据集。我们可以认为这是用傅里叶变换将信号从时间轴转换到频率轴。</p><p id="8061" class="pw-post-body-paragraph lp lq it lr b ls na ju lu lv nb jx lx ly nc ma mb mc nd me mf mg ne mi mj mk im bi translated">通过考虑数值和连续变量的方差值，对它们进行重新评估，并使用PCA从不同的窗口查看数据集。虽然在技术上可以实现，但对分类变量使用PCA不会产生可靠的结果。同样，当理解上述理论部分时，<strong class="lr iu">用PCA执行特征选择的条件是合理的，因为影响结果的最重要的特征具有最大的方差。</strong>当然，技术上还是可以实现的，只是选择权在开发者。</p><h1 id="34fd" class="ky ks it bd kz la lb lc ld le lf lg lh jz li ka lj kc lk kd ll kf lm kg ln lo bi translated">4.履行</h1><h2 id="5f67" class="kr ks it bd kz no np dn ld nq nr dp lh ly ns nt lj mc nu nv ll mg nw nx ln ny bi translated">4.1.传统的机器学习方法</h2><p id="0a2a" class="pw-post-body-paragraph lp lq it lr b ls lt ju lu lv lw jx lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">有人提到，主成分分析是一种非常有用的方法，尽管会丢失信息，但会降低维数减少和特征值。在影像数据集中，每个像素都被视为一个要素。换句话说，对于128x128 RGB (3通道)图像，有128*128*3 = 49152个特征。这个数字对于监督学习模型来说是相当高的。在这一部分中，在由81个杯子、74个盘子和78个盘子组成的厨房用具图像数据集上，在利用PCA对数据集进行图像增强和维度缩减之后，XGBoost被应用如下:</p><figure class="ki kj kk kl gt mm"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="4ed4" class="pw-post-body-paragraph lp lq it lr b ls na ju lu lv nb jx lx ly nc ma mb mc nd me mf mg ne mi mj mk im bi translated">数据集从本地文件夹导入后，使用定义的Imagedatagenerator复制15次，获得3495个样本。在编码中，<em class="kx"> x:代表数据集，y:代表标签。</em>然后，为了测量来自不同源的模型的泛化性能，下载5个杯子、5个盘子和5个盘子，并且这些也从本地文件夹中导入。在必要的数据预处理之后，将图像添加到x的末端，将标签添加到y的末端。<strong class="lr iu">将获取的图像添加到训练和测试数据集中以评估模型泛化性能的原因是相同的PCA过程应用于所有图像。</strong>数据集合并后，使用sklearn库导入PCA，49152像素(特征)减少到300。此时，使用NumPy再次提取具有15个样本的模型泛化性能数据集，并且3495个数据集被分离为训练数据集和测试数据集。这里的问题不是是否使用五氯苯甲醚，而只是作为一种应用。也可以使用SelectPercentile进行特征选择。然后，在标签适应XGBoost模型之后，训练数据集被训练，并且用测试数据集评估该模型。最后，用分离的15个样本的外部数据集检验了模型的预测。</p><p id="f1e4" class="pw-post-body-paragraph lp lq it lr b ls na ju lu lv nb jx lx ly nc ma mb mc nd me mf mg ne mi mj mk im bi translated">结果如图10所示。</p><figure class="ki kj kk kl gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi of"><img src="../Images/7d27d4bac89b32223bd21f8c4f8fbe68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0b9OQmnkhA0FIfiQHBlzoA.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">图10。外部数据集的混淆矩阵(左)和测试数据集的混淆矩阵(右)，图片由作者提供</p></figure><h2 id="f061" class="kr ks it bd kz no np dn ld nq nr dp lh ly ns nt lj mc nu nv ll mg nw nx ln ny bi translated">4.2.深度学习方法</h2><p id="0984" class="pw-post-body-paragraph lp lq it lr b ls lt ju lu lv lw jx lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">编码器和解码器是深度学习处理的首选。然而，应用五氯苯甲醚在技术上是可行的。让我们对用上述数据导入和预处理操作以及用PCA降维准备的数据集进行分类，用密集层。</p><figure class="ki kj kk kl gt mm"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="bb0e" class="pw-post-body-paragraph lp lq it lr b ls na ju lu lv nb jx lx ly nc ma mb mc nd me mf mg ne mi mj mk im bi translated">结果如图11所示:</p><figure class="ki kj kk kl gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi og"><img src="../Images/ba05d2332d94f6145951bbc888eb29f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t1TckM2OCqDAH0O68iTI0Q.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">图11。外部数据集的混淆矩阵(左)和测试数据集的混淆矩阵(右)，图片由作者提供</p></figure><h1 id="49d1" class="ky ks it bd kz la lb lc ld le lf lg lh jz li ka lj kc lk kd ll kf lm kg ln lo bi translated">5.PCA类型</h1><h2 id="edd0" class="kr ks it bd kz no np dn ld nq nr dp lh ly ns nt lj mc nu nv ll mg nw nx ln ny bi translated">5.1.核主成分分析</h2><p id="b6d7" class="pw-post-body-paragraph lp lq it lr b ls lt ju lu lv lw jx lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">虽然PCA是一个线性模型，但在非线性情况下可能不会给出成功的结果。内核PCA是一种方法，也称为内核技巧，可以非线性地分离数据。</p><figure class="ki kj kk kl gt mm"><div class="bz fp l di"><div class="my mz l"/></div></figure><figure class="ki kj kk kl gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi oh"><img src="../Images/9d191533f3a4e61add8e73b831e93a19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SLpVYjT5cBEnZkOW9CXWOA.png"/></div></div><p class="mt mu gj gh gi mv mw bd b be z dk translated">图12。数据集(左)、含PCA的数据集(中)、含内核PCA的数据集(右)、按作者分类的图像</p></figure><h2 id="ece2" class="kr ks it bd kz no np dn ld nq nr dp lh ly ns nt lj mc nu nv ll mg nw nx ln ny bi translated">5.2.稀疏主成分分析</h2><p id="80fc" class="pw-post-body-paragraph lp lq it lr b ls lt ju lu lv lw jx lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">其目的是在稀疏主成分分析中更容易解释模型。虽然整个数据集的线性组合是PCA中的每个主成分，但是每个主成分是稀疏PCA中数据集子集的线性组合。</p><h2 id="c146" class="kr ks it bd kz no np dn ld nq nr dp lh ly ns nt lj mc nu nv ll mg nw nx ln ny bi translated">5.3.随机化主成分分析</h2><p id="eed6" class="pw-post-body-paragraph lp lq it lr b ls lt ju lu lv lw jx lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">随机化PCA与随机梯度下降一起工作，被称为随机化PCA。通过查找前x个主成分来加快PCA过程。</p><h2 id="dd14" class="kr ks it bd kz no np dn ld nq nr dp lh ly ns nt lj mc nu nv ll mg nw nx ln ny bi translated">5.4.增量PCA</h2><p id="2130" class="pw-post-body-paragraph lp lq it lr b ls lt ju lu lv lw jx lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">它通过将大规模数据集以小批量保存在内存中来执行PCA方法。</p><p id="11c1" class="pw-post-body-paragraph lp lq it lr b ls na ju lu lv nb jx lx ly nc ma mb mc nd me mf mg ne mi mj mk im bi translated">方法在上面的Sklearn库中给出，并且可以根据数据集容易地实现。</p></div><div class="ab cl oi oj hx ok" role="separator"><span class="ol bw bk om on oo"/><span class="ol bw bk om on oo"/><span class="ol bw bk om on"/></div><div class="im in io ip iq"><h2 id="ada5" class="kr ks it bd kz no np dn ld nq nr dp lh ly ns nt lj mc nu nv ll mg nw nx ln ny bi translated">回到指引点击<a class="ae mx" href="https://ibrahimkovan.medium.com/machine-learning-guideline-959da5c6f73d" rel="noopener">此处</a>。</h2><div class="op oq gp gr or os"><a href="https://ibrahimkovan.medium.com/machine-learning-guideline-959da5c6f73d" rel="noopener follow" target="_blank"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd iu gy z fp ox fr fs oy fu fw is bi translated">机器学习指南</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">所有与机器学习相关的文章</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">ibrahimkovan.medium.com</p></div></div><div class="pb l"><div class="pc l pd pe pf pb pg mr os"/></div></div></a></div></div></div>    
</body>
</html>