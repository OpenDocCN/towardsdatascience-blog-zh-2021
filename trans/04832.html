<html>
<head>
<title>AdaBoost from Scratch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/adaboost-from-scratch-37a936da3d50?source=collection_archive---------2-----------------------#2021-04-27">https://towardsdatascience.com/adaboost-from-scratch-37a936da3d50?source=collection_archive---------2-----------------------#2021-04-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4148" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">构建数据科学中最流行的“现成”算法之一的Python实现</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/5d45e4d341c1e68e9120103248848572.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oKyfMtQvDoBwJXdwCmSMXw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Javier Allegue Barros 在<a class="ae kv" href="https://unsplash.com/s/photos/decision?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="a0dc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有个同事曾经跟我说过，一个算法，直到你能在NumPy上从头开始写，你才算真正理解了它。这种说法可能很大胆，但打开一本关于机器学习的教科书或论文，并试图将数学转化为代码，仍然有一些美好的东西。这是我开始做这篇文章背后的工作的全部动机，在这篇文章中，我描述了如何构建您自己的AdaBoost算法实现。我们的算法版本来自<em class="ls">《统计学习的要素</em> (TESL)，第10章【1】。</p><p id="e74d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">AdaBoost是Ada的缩写，已经成为数据科学中最流行的“现成”算法之一。它已经成为Kaggle竞赛中获胜解决方案的一部分(有时与其年轻的堂兄弟XGBoost和Light GBM结合使用)，现在它甚至成为人脸检测问题的一部分。它是由Freund和Shapire在1996年提出的[2]。</p><p id="febd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一个小小的警告:本文的目的不是详细推导和解释AdaBoost的数学——已经有很多这样的好帖子了。相反，这篇文章是关于算法的实现如何在幕后工作的。它是为那些想了解AdaBoost的现成实现是做什么的人准备的，比如Python的<a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a>或r的<a class="ae kv" href="https://cran.r-project.org/web/packages/adabag/adabag.pdf" rel="noopener ugc nofollow" target="_blank"> Adabag </a>上的那个。最后，我希望它也能启发你构建其他算法的实现，以了解它们的更多信息！</p><p id="1f2b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文的其余部分，我将遵循TESL第10章中的符号和公式。我们将实施AdaBoost。M1算法，在Friedman等人(2000) [3]中称为“离散AdaBoost”，在TESL中称为算法10.1。所有代码示例都是用Python编写的。你可以在Github库中找到代码。</p><h1 id="4618" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">升压概述</h1><p id="df61" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">AdaBoost算法基于“增强”的概念。boosting背后的思想是，使用投票机制，一组“弱”分类器可以组成一个健壮的分类器。弱分类器只能产生比投掷(公平的)硬币稍微好一点的结果。换句话说，如果通过随机猜测一个二进制标签，我们将有50%的机会是正确的，那么弱分类器将是正确的，比如说，55%(或者任何其他接近50%的数字)。</p><p id="9233" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用boosting，我们在样本的连续修改版本上训练一系列弱分类器。让我们举一个例子来看看这是如何工作的。假设我们有一个有五个观察值的样本。每个观察值被标记为-1或1，因此我们的标记是<em class="ls"> y = [1，1，-1，-1] </em>。我们也有一些解释变量<em class="ls"> x. </em>在第一轮提升中，我们训练一个弱分类器<em class="ls"> G₁(x) </em>，它产生下面的预测:<em class="ls"> G₁(x) = [1，-1，-1，1，-1]。</em>我们可以看到第二个和第四个观察值被错误分类了。在第二轮提升中，我们将使用一些权重对样本中的每个观察值进行加权<em class="ls"> wᵢ.</em>我们将设置<em class="ls"> wᵢ </em>，使得这两个错误分类的观察值在新分类器<em class="ls"> G </em> ₂ <em class="ls"> (x) </em>的学习过程中比其他三个具有更大的影响。新的分类器现在可能会错过一些以前分类良好的观察结果，但它会更好地预测困难的观察结果。</p><p id="7b53" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在重复上述过程M次之后，我们将得到一组M个弱分类器，我们可以将它们组合成最终的健壮元分类器<em class="ls"> G(x)。</em>该元分类器将根据加权多数投票为每个观察值分配一个预测，如下式所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/088602f18abbaee486ca65613a493281.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/format:webp/1*7H2nn8GFReMGjPZlNXlq_A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">我们最后的分类器。资料来源:统计学习的要素。10</p></figure><p id="60bd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">看到在多数表决中每个弱<strong class="ky ir">分类器</strong>的<strong class="ky ir">权重</strong>用<em class="ls"> α </em>表示。这些不是我们在每一轮助推中应用于每个<strong class="ky ir">观察</strong>的权重，我们称之为<em class="ls"> wᵢ </em>(我知道，这么多权重！).</p><h1 id="62f9" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">弱分类器</h1><p id="25b7" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">请记住，我们粗略地将弱分类器定义为只能产生比随机猜测稍好的结果的分类器。这个定义的更正式的版本是分类器的错误率小于但接近50%。在这种情况下，样本内错误率就是总样本量中错误分类的观察值的数量(即<em class="ls">yᵢ</em>≦<em class="ls">g(xᵢ】</em>)，如下所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/b0dc92763d970a2289ffca16a3e8803b.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/format:webp/1*mCMMoeNApKcnYqT_eesocg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">样本内误差率。资料来源:统计学习的要素。10</p></figure><p id="39c2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">许多算法可以作为弱分类器，但在AdaBoost的情况下，我们通常使用“树桩”；也就是说，决策树只包含两个终端节点。直观地说，在二元分类问题中，树桩会试图通过数据集的多个解释变量中的一个来分割样本。正如你所想象的，通常这不足以简单地分离类，尤其是在复杂的数据集中。</p><p id="230b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，如果我们对相同的数据训练一个决策树分类器1000次，我们将总是得到相同的切割(对于给定的随机种子，如果你使用近似启发式)。在画线时，决策树会最小化一个基于目标值分布的指标，通常是基尼系数或来自结果类别的信息增益。这就是我们的观察权重<em class="ls"> wᵢ </em>发挥作用的地方。在每一次boosting迭代中，我们告诉我们的stump在估计它最小化的度量时对每个观察值进行不同的加权。这样，在每次迭代中，我们都会得到不同的决策树。</p><p id="329b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我们不会详细讨论决策树(事实上，我在算法中使用了Scikit-learn实现)，但是如果您想了解更多关于决策树的知识，我建议您阅读TESL的第9、10和15章。</p><h1 id="6d86" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">编写我们的算法</h1><p id="0b02" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">是时候回顾一下了。为了解决二元分类问题，我们的AdaBoost算法将通过一系列提升迭代来拟合一系列弱分类器(树桩)。这些分类器将形成元分类器，该元分类器将基于加权多数投票机制产生预测。在每一次提升迭代中，我们将给予那些在前一次迭代中被错误分类的观察更多的权重。我们可以将这个过程形式化，如下面的算法所示。人们可以证明，<em class="ls"> α </em>和<em class="ls"> w </em>的公式来自最小化指数损失函数。解释有点长，超出了本文的范围，但是你可以在第10.4节的TESL中找到。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ms"><img src="../Images/69bcef4f778e2f31e8341d87bdfcaf08.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TcrnpBGsi5MsFog-fSdRMQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">资料来源:统计学习的要素。10</p></figure><p id="8c5d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们将通过识别独立的流程块来开始我们的实现，我们可以在步骤2的循环之外对这些独立的流程块进行编码。步骤2(b)、2(c)和2(d)包含在每次迭代中重复的操作，所以我们将它们定义为Python函数。让我们看看怎么做。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="fb4d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，这些函数与算法无关。人们可以用<em class="ls"> compute_error </em>函数计算任何分类器的错误率。类似地，我们可以为其他上下文估计<em class="ls"> α </em>和<em class="ls"> w </em>。</p><p id="5cdc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">是时候定义我们的AdaBoost分类器了。为此，我们将构建一个名为<em class="ls"> AdaBoost </em>的Python类，它将由几个方法组成，以适应模型并进行预测。它还将包含关于我们拟合的模型的其他相关信息，例如弱分类器的序列和它们在最终投票中的权重，以及训练和预测误差。下面的代码片段显示了我们的类的初始化和fit方法:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="f5f1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">的。<em class="ls"> fit() </em>方法的结构与典型的Scikit-learn类非常相似。作为必要的参数，它以数组形式的独立变量矩阵<em class="ls"> X </em>和带有目标变量的向量<em class="ls"> y </em>。目标变量必须是二进制的，并且必须进行编码，使其值为-1或1(这使得投票更容易)。增强舍入参数是可选的，默认值为100。</p><p id="df80" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">AdaBoost的步骤1和2。对于循环，M1算法在<em class="ls">内被特征化。开始时的<em class="ls"> if </em>语句在第一轮中将每个观察值的权重初始化为1/N(步骤1)，然后在拟合树桩并计算出该迭代的<em class="ls"> α </em>后更新它们(步骤2(d))。该方法的输出是一组树桩、alphas和训练错误，我们将其存储起来以供在预测方法中进一步使用。算法的步骤3在一个单独的方法中定义，我们称之为<em class="ls">。预测()。</em></em></p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="4a20" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">类似于。<em class="ls">合体()</em>法，<em class="ls">。predict() </em>将解释变量的类似数组的矩阵作为参数。然后，它为每个观察值和树桩计算一个预测值。这些数据存储在一个数据帧中，该数据帧将用于该过程的多数表决部分，以计算每个观察的最终预测。</p><h1 id="7df1" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">测试我们的算法</h1><p id="73ab" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">我们现在已经定义了AdaBoost的实现，但是我们如何知道它能工作呢？为了找到答案，我们将在Spambase数据集上进行测试，这是一个由4，601封电子邮件组成的集合，根据它们是否是垃圾邮件进行标记。自变量由每封电子邮件中的术语和字符频率组成。这个数据集可以在加州大学欧文分校的公共存储库中找到，你可以在这里下载。它还被用于TESL的几个章节，包括关于AdaBoost的章节，因此我们可以将我们的实现与其他经过验证的算法进行比较。</p><p id="3bd1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就像在TESL中一样，为了确保我们的训练集尽可能相似，我们不会对解释变量应用任何清理或特征工程。TESL没有说明它的随机种子，所以我们不能复制训练/测试分裂。然而，我们仍然可以使用相同的集合大小，使得至少两种算法都在相同数量的观察值上被训练。下面的代码片段显示了我们如何读取和分割数据:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="1306" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们的AdaBoost类在这里实现非常简单。我们只需要将训练集传递给<em class="ls">。fit() </em>方法和测试设置为<em class="ls">。</em>预测()【方法:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mt mu l"/></div></figure><p id="fe51" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">就是这样！如果你在你的机器上尝试上述步骤，你会发现你会得到5.6%的错误率。相比之下，在TESL的第10.8章中，他们使用梯度推进得到的错误率为4.5%，使用加法逻辑回归得到的错误率为5.5%，使用CART树得到的错误率为8.7%，使用MARS得到的错误率为5.5%。请注意，我们使用的不是完全相同的样本，而是相同数量的观察值。但还不错，对吧？</p><p id="68ad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作为第二个测试，您还可以将这个定制实现与Scikit-learn的实现进行比较。我们上面产生的一组预测产生了93.99%的ROC-AUC分数。在相同的数据集上使用Scikit-learn，我得到了92.79%。由于随机种子和近似试探法，数字可能略有不同，但它真的很接近！</p><h1 id="c69a" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">回到树桩</h1><p id="12ad" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">记住AdaBoost适合一组弱分类器。为了了解这在实践中意味着什么，让我们来看看上面训练的每个树桩的错误率。在下面的图表中，我们可以看到大多数树桩在训练集上的错误率非常接近，但低于0.5。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mv"><img src="../Images/da8f2ce6958f2260c7c57e02b66ba11d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5d0Btlr8AL0V3gD871ZoEQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:作者创作</p></figure><p id="7415" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">决策树的一个众所周知的特性是它们倾向于过度拟合训练数据。我们的树桩也不例外，如下图所示。其实很多比乱猜还要烂。然而，他们的组合只有5.6%的错误率！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mv"><img src="../Images/54f16508d212b6a8f2e50d89086260c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fvu-Y6QTWc9aCzvrOQ5WNA.png"/></div></div></figure><h1 id="8d2a" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">主要外卖</h1><p id="0e65" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">本文的目标是介绍如何用Python构建AdaBoost分类算法的自定义实现。为此，我们遵循了<em class="ls">的第十章《统计学习的要素》。</em>在这一过程中，我们也给出了boosting的高级概述，boosting是一种将弱学习者结合到一个健壮的元算法中的技术。</p><p id="ac14" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望这是有用的，并启发您构建自己的自定义实现。这不仅是一种有趣的学习方式，而且非常有效！正如纳西姆·n·塔勒布所说:“我自己学到的东西我仍然记得”[5]。</p><p id="aeeb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">喜欢吗？您可以访问这个<a class="ae kv" href="https://github.com/AlvaroCorrales/AdaBoost" rel="noopener ugc nofollow" target="_blank"> Github库</a>中的代码。</p><h1 id="efbc" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">参考</h1><p id="13d7" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">[1] Hastie，t .等人(2009)，<em class="ls">《统计学习的要素》，第二版，</em> DOI 10.1007/b94608_10，施普林格科学+商业媒体有限责任公司。</p><p id="d8a4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2] Freund，y .和Schapire，R. (1996)，<em class="ls">用新的Boosting算法进行的实验，</em>机器学习:第十三届国际会议论文集，可在http://rob.schapire.net/papers/FreundSc96.pdf<a class="ae kv" href="http://rob.schapire.net/papers/FreundSc96.pdf" rel="noopener ugc nofollow" target="_blank">获得</a></p><p id="5c79" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[3]弗里德曼J，哈斯蒂T，蒂布希拉尼R (2000年)。<em class="ls">加法逻辑回归:助推的统计学观点。</em>统计年鉴，28卷2期，337–407页。</p><p id="0054" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[4]杜瓦和格拉夫(2019年)。UCI机器学习知识库[http://archive . ics . UCI . edu/ml]。加州欧文:加州大学信息与计算机科学学院。</p><p id="06c4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[5]纳西姆·尼古拉斯·塔勒布(2016)，《普洛克罗斯特斯之床:哲学和实践格言》。</p></div></div>    
</body>
</html>