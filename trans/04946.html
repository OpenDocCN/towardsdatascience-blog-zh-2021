<html>
<head>
<title>Understanding Google’s Switch Transformer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解谷歌的Switch Transformer</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-googles-switch-transformer-904b8bf29f66?source=collection_archive---------7-----------------------#2021-04-30">https://towardsdatascience.com/understanding-googles-switch-transformer-904b8bf29f66?source=collection_archive---------7-----------------------#2021-04-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="432c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">谷歌如何无成本地创建了世界上最大的语言模型</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/9c5778f9dfcf56921271bf28f0ef4341.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MdaIgzDZdspWogRK"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">乔纳森在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="76ff" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当GPT-3在2020年5月由<a class="ae kv" href="https://openai.com/" rel="noopener ugc nofollow" target="_blank"> OpenAI </a>在<a class="ae kv" href="https://arxiv.org/abs/2005.14165" rel="noopener ugc nofollow" target="_blank">推出</a>时，消息不胫而走。不仅在人工智能社区，甚至在主流媒体上也有类似于“<a class="ae kv" href="https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3" rel="noopener ugc nofollow" target="_blank">一个机器人写了这篇文章</a>”和“<a class="ae kv" href="https://www.nbcnews.com/tech/tech-news/have-you-read-something-written-gpt-3-probably-not-it-n1240384" rel="noopener ugc nofollow" target="_blank">你读过GPT 3写的东西吗？</a>”。人们很兴奋！</p><p id="445c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在GPT-3之前，最大的语言模型是2020年2月发布的拥有170亿个参数的图灵-NLG模型。那年晚些时候，OpenAI用1750亿个参数把它炸出了公园。突然，出现了一种语言模型，它可以产生与人类通常无法区分的内容。</p><p id="d4af" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2021年初，谷歌发布了一篇<a class="ae kv" href="https://arxiv.org/abs/2101.03961" rel="noopener ugc nofollow" target="_blank">论文</a>，题为“<em class="ls">开关变压器:用简单高效的稀疏性</em>扩展到万亿参数模型”。虽然这确实得到了一些人工智能社区之外的小媒体报道，但它并不接近GPT-3，尽管创建了一个参数几乎十倍多的模型！</p><p id="814f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种覆盖面的缺乏并不令人惊讶。谷歌的1.6万亿参数模型在性能方面不是最先进的，也没有一个整洁的API前端来演示人工智能的未来。然而，这并不意味着动机或结果对未来的人工智能和机器学习研究不重要。</p><p id="0965" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我们将尝试理解开关变压器背后的动机，它是如何工作的，以及为什么这些结果对机器学习研究和应用的未来如此重要。</p><h1 id="0474" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">动机</h1><p id="7cae" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated"><a class="ae kv" href="https://arxiv.org/abs/2001.08361" rel="noopener ugc nofollow" target="_blank">经验表明</a>语言模型的性能随着参数数量(模型大小)、数据集大小和计算预算的增加而呈幂律增长。</p><p id="8e54" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，随着这些费用的增加，培训的财务费用也在增加。这导致开源、预先训练的语言模型越来越受欢迎，如谷歌的<a class="ae kv" href="https://github.com/google-research/bert" rel="noopener ugc nofollow" target="_blank"> BERT </a>，它可以在特定的下游语言任务上进行微调，如分类、问答或总结，使数据科学从业者受益于谷歌、脸书和OpenAI等公司掌握的大量资源。</p><p id="ebdf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，GPT-3证明，如果这些模式继续增长，即使是大型企业和组织也可能开始陷入困境。GPT-3花费了<a class="ae kv" href="https://venturebeat.com/2020/06/01/ai-machine-learning-openai-gpt-3-size-isnt-everything/" rel="noopener ugc nofollow" target="_blank">据称</a>1200万美元训练。这个<a class="ae kv" href="https://www.forbes.com/sites/robtoews/2020/06/17/deep-learnings-climate-change-problem/" rel="noopener ugc nofollow" target="_blank">包括</a>近5000个模型版本的训练，使用了将近10000天的GPU时间。</p><p id="3014" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">同样值得注意的是，成本不仅是经济的。据估计，这次培训产生了78，000磅的二氧化碳排放量。类似于一个美国成年人两年生产的东西！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/6f2201cf386f0ab2a05edf5648f51899.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FzxLeOsS2ya3CiBv"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@ismailenesayhan?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">i̇smail·埃内斯·艾汉</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="5b32" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">OpenAI的研究表明，用于训练人工智能模型的计算量每3.4个月翻一番，这在经济和环境方面产生了令人担忧的预测。</p><p id="9033" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这就是Switch Transformer背后的动机，在不增加计算成本的情况下，创建更大的机器学习模型。</p><h1 id="69d3" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">开关变压器</h1><p id="f303" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">开关变压器是一个开关前馈神经网络(FFN)层，取代了<a class="ae kv" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">变压器</a>架构中的标准FFN层。关键区别在于，每个交换层包含多个FFN，称为专家，而不是包含单个FFN。</p><p id="87c5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当每个令牌通过这一层时，它首先通过路由器功能，然后将令牌路由到特定的FFN专家。</p><blockquote class="mr"><p id="0e6d" class="ms mt iq bd mu mv mw mx my mz na lr dk translated">由于每个令牌只通过一个专家FFN，浮点运算(FLOPS)的数量保持不变，而参数的数量随着专家数量的增加而增加。</p></blockquote><p id="5638" class="pw-post-body-paragraph kw kx iq ky b kz nb jr lb lc nc ju le lf nd lh li lj ne ll lm ln nf lp lq lr ij bi translated">这创建了一个稀疏模型，其中不是每个参数都用于每个令牌。在这方面，开关变压器解决了上述动机，在不增加计算量的情况下增加模型参数的数量，计算量以FLOPs计。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/04cce914bf0a17b1ba1746b8d5000b24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QMLNn7AFlwuirryw1Qdpaw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在开关变压器前馈神经网络层中，每个令牌通过路由器功能将其发送到单个前馈神经网络，称为专家。由于每个令牌仅通过单个FFN，所以计算不会增加，但是参数的数量随着专家的数量而增加。图片来自原开关变压器<a class="ae kv" href="https://arxiv.org/pdf/2101.03961.pdf" rel="noopener ugc nofollow" target="_blank">纸</a>。</p></figure><h2 id="4c9c" class="nh lu iq bd lv ni nj dn lz nk nl dp md lf nm nn mf lj no np mh ln nq nr mj ns bi translated">专家混合</h2><p id="7bf6" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">使用专家来增加模型参数数量的概念对于开关变压器来说并不新奇。一篇描述专家混合层的<a class="ae kv" href="https://arxiv.org/abs/1701.06538" rel="noopener ugc nofollow" target="_blank">论文</a>于2017年发布，其架构与开关变压器几乎相同。</p><p id="aebd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">关键区别在于路由器功能会将每个令牌发送给多个FFN专家。作者假设路由器功能将无法学习如何路由令牌，除非它可以通过路由到k&gt;1个专家来比较至少两个令牌。</p><p id="c575" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另一方面，开关变压器仅使用k=1，提供三大优势:</p><ol class=""><li id="f07b" class="nt nu iq ky b kz la lc ld lf nv lj nw ln nx lr ny nz oa ob bi translated">路由器的计算量减少了，因为它只路由到一个FFN专家。</li><li id="05a0" class="nt nu iq ky b kz oc lc od lf oe lj of ln og lr ny nz oa ob bi translated">每个专家的批量<em class="ls">至少减半</em>(即k=1而不是k ≥2)。</li><li id="1215" class="nt nu iq ky b kz oc lc od lf oe lj of ln og lr ny nz oa ob bi translated">降低了设备之间(即路由器到专家)的通信成本。</li></ol><h2 id="d71c" class="nh lu iq bd lv ni nj dn lz nk nl dp md lf nm nn mf lj no np mh ln nq nr mj ns bi translated">功率</h2><p id="cb1e" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">第二个好处需要进一步分析。每个专家的批量大小也称为专家能力，即专家在给定通道中能够处理的令牌数量。</p><p id="587d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">理想情况下，这将等于令牌数除以专家数，称为容量因子1。这样，在给定的步骤中不会浪费专家的能力。</p><p id="ad34" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，这假设路由器功能在专家之间平均分配令牌。实际上，一些专家FFN会溢出，导致某些令牌在该步骤中没有被FFN专家处理。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/0e8cae08f20746de5391ea9de7d30b5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lmv9_cnCay-E83ztEty-rg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">为了避免溢出，导致令牌在该步骤中不被任何FFN处理，必须增加容量因子。这增加了计算和通信成本，因此辅助损耗不利于不平衡路由。图片来自原开关变压器<a class="ae kv" href="https://arxiv.org/pdf/2101.03961.pdf" rel="noopener ugc nofollow" target="_blank">纸</a>。</p></figure><p id="e610" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">容量因子可以增加，但是这将导致一些专家具有未使用的容量，增加计算和通信成本。</p><p id="4493" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作为一种折衷，作者在总损失函数中增加了一个辅助损失，通过路由器功能来惩罚令牌的不平衡路由。他们根据经验发现，容量系数为1.25时性能最佳。</p><h1 id="5fda" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">结果</h1><p id="b7e3" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">为了衡量Switch Transformer的性能，他们在<em class="ls">庞大干净的爬虫语料库</em>(<a class="ae kv" href="https://www.tensorflow.org/datasets/catalog/c4" rel="noopener ugc nofollow" target="_blank"/>)上训练了几个模型，用<a class="ae kv" href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html" rel="noopener ugc nofollow" target="_blank"> T5 </a>语言模型作为基准，对比了负log困惑度。</p><p id="0a96" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">考虑到动机，为了在不增加所需计算的情况下增加参数的数量(从而增加性能)，他们训练的模型与T5相当，即每个令牌的计算量保持相等。</p><p id="0cca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">他们发现了以下情况:</p><ul class=""><li id="fa20" class="nt nu iq ky b kz la lc ld lf nv lj nw ln nx lr oi nz oa ob bi translated">经过100，000步后，开关变压器模型比触发器匹配的T5等效模型具有更大的负损耗困惑。</li><li id="2a83" class="nt nu iq ky b kz oc lc od lf oe lj of ln og lr oi nz oa ob bi translated">开关变压器达到质量阈值(负。日志perp。=-1.495)比同等的T5更快。在T5基的情况下，这个阈值从未达到过！</li><li id="215f" class="nt nu iq ky b kz oc lc od lf oe lj of ln og lr oi nz oa ob bi translated">开关变压器每秒能够处理更多的样本。</li></ul><h2 id="9d67" class="nh lu iq bd lv ni nj dn lz nk nl dp md lf nm nn mf lj no np mh ln nq nr mj ns bi translated">缩放属性</h2><p id="6dbd" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">除了分析整体性能，作者还在预训练期间查看了缩放属性。也就是说，给定一个无限的计算预算，如何缩放模型是最好的？</p><p id="7028" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对三个不同的维度进行了分析，以了解模型的缩放特性:步骤、时间和针对更大的密集模型。</p><p id="cbe9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">步长调整:</strong>对于固定数量的训练步长，开关变压器将优于触发器匹配的T5模型。增加专家数量将进一步提高性能，而不会增加失败次数。拥有64名专家的开关变压器模型在60k步中实现了与T5基模型在450k步中相同的性能，相当于7.5倍的加速。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi gj"><img src="../Images/1010ae6409cfe2d24175b141638cce45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t1BcUNJanvDeA3Xddrn_mQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">T5基极与触发器匹配的等效开关变压器模型的步进调整，专家人数不同。图片来自原开关变压器<a class="ae kv" href="https://arxiv.org/pdf/2101.03961.pdf" rel="noopener ugc nofollow" target="_blank">纸</a>。</p></figure><p id="9ed4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">时间缩放:</strong>直观上，时间缩放应该等同于步长缩放。然而，跨设备的额外通信成本和路由器功能的计算意味着这需要被明确地分析。结果显示，对于固定的训练时间，开关变压器优于触发器匹配的等效T5基极模型。在T5基模型和64专家开关变压器模型之间观察到7倍的加速。</p><p id="fc7d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">更大的密集模型:</strong>作者最后考虑了这样一种情况，为了匹配开关变压器的性能，使用T5-大模型代替T5-Base。然而，他们表明，这导致每个令牌的FLOPS增加了3.5倍，并且FLOPS匹配的开关变压器模型将优于此。</p><p id="1aa0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些缩放结果表明，对于任何可用的计算增加，较大的开关变压器模型将优于较大的密集模型。考虑到减少语言模型的计算足迹的动机，这是一个极其重要的结果。</p><h2 id="f06e" class="nh lu iq bd lv ni nj dn lz nk nl dp md lf nm nn mf lj no np mh ln nq nr mj ns bi translated">下游结果</h2><p id="7fe2" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">为了测量下游性能，将T5-Base(223m参数和124B触发器)和T5-Large (739M参数和425B触发器)与匹配开关-Base (7.4B参数)和开关-Large (26.3B参数)模型的触发器进行了比较。</p><p id="d315" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些模型在11个不同的语言任务中进行了比较，包括分类、问题回答和总结等任务。除了<a class="ae kv" href="https://allenai.org/data/arc" rel="noopener ugc nofollow" target="_blank">电弧</a>之外，开关变压器模型在所有任务中都优于触发器匹配的等效T5模型。</p><h2 id="d242" class="nh lu iq bd lv ni nj dn lz nk nl dp md lf nm nn mf lj no np mh ln nq nr mj ns bi translated">多语言学习</h2><p id="9d05" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">作为性能的最终测试，作者在101种不同语言的预训练中测量了模型的质量。在<em class="ls">所有</em>语言中，基于开关的模型比基于T5的模型具有更大的负对数困惑，并且观察到平均5倍的训练加速。</p><h1 id="6fb0" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">万亿参数模型</h1><p id="ed9e" class="pw-post-body-paragraph kw kx iq ky b kz ml jr lb lc mm ju le lf mn lh li lj mo ll lm ln mp lp lq lr ij bi translated">在文章的结尾，作者介绍了两个大型开关变压器模型Switch-XXL和Switch-C的设计和训练，它们分别具有3950亿和1.571万亿个参数。</p><p id="94e1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">开关-XXL模型与T5-XXL模型是匹配的。但是，由于Switch-C模型的大小，架构大小(层大小、深度、注意力头数量等。)，因此一偏，是减少了。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/88f5016300affe69cc88a1b183fdad56.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3UrdYOp2xwUABWYW"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae kv" href="https://unsplash.com/@kaffeebart?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Kaffeebart </a>拍摄</p></figure><p id="5994" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这可能是Switch-XXL模型的负对数困惑度大于Switch-C的部分原因，表明增加专家人数的收益递减，特别是以牺牲其他模型维度为代价。然而，在预训练后，两种模型都优于T5-XXL。</p><p id="7cf2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些模型也在下游任务中被测量。然而，这些结果都不是最先进的。</p><p id="b1f1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这也许是为什么开关变压器没有得到与GPT-3相同的媒体曝光率的原因。万亿参数模型虽然令人印象深刻，但也需要有令人印象深刻的性能做后盾。</p><p id="a770" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，这并没有降低这项研究的重要性。迄今为止，增加密集语言模型的规模是生产最先进模型阻力最小的途径；但很明显，这在经济和环境上都是不可持续的。</p><p id="cef5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">谷歌已经表明，可以创建创新的模型架构，在不增加计算成本的情况下提高模型性能，这是数据科学和人工智能社区在不久的将来肯定会看到的事情。</p></div></div>    
</body>
</html>