<html>
<head>
<title>Self-Supervised Voice Emotion Recognition Using Transfer Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于迁移学习的自监督语音情感识别</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/self-supervised-voice-emotion-recognition-using-transfer-learning-d21ef7750a10?source=collection_archive---------19-----------------------#2021-04-02">https://towardsdatascience.com/self-supervised-voice-emotion-recognition-using-transfer-learning-d21ef7750a10?source=collection_archive---------19-----------------------#2021-04-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="79d7" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="de17" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">从语音音频构建自我监督的二元情感分类器</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/737bacf15ff84752f5d82fbf16b6a393.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*g_GwZWTgnh5w4Mvq"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">由<a class="ae le" href="https://unsplash.com/@pawel_czerwinski?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">paweczerwi ski</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="205e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">你有没有注意到当听到有人用快乐的语调说一些非常可怕的事情时的不安感？我们习惯于听到声音的情感与演讲的内容相匹配。可以利用这种现象来自动生成用于训练语音情感分类器的弱标记数据。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><p id="3e22" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我在Metis数据科学训练营的最终项目的目标是从语音音频中构建一个自我监督的二进制情感分类器。情绪是复杂的多维概念，但在这个项目中，我建立了一个模型，给定一个音频剪辑，预测声音的情绪是积极还是消极。</p><p id="dfd2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">从声音中进行情感分类最困难的事情之一是标签数据的可用性，它非常稀缺。即使标记的数据是可用的，它要么被表现出来，听起来可能不同于真实的情绪，要么被独立地标记，这是非常耗时和/或主观的。</p><p id="1b9e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在我们说什么和我们怎么说相关的假设下，我使用了一种自我监督的方法来使用音频抄本生成弱标记数据。</p><h2 id="ab67" class="mi mj iq bd mk ml mm dn mn mo mp dp mq lo mr ms mt ls mu mv mw lw mx my mz iw bi translated">方法学</h2><p id="6760" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">下图显示了构建分类器所需的处理步骤。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nf"><img src="../Images/a7475e69b45fcab4dd682322a4b8c41c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N1M6N3mz1xfRQ5o4_KUbwg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">构建分类器的解决方案架构</p></figure><p id="0b89" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">使用上述解决方案架构，构建模型所需的唯一数据是转录的音频文件，没有任何情感标签。左侧是处理音频文件所需的步骤，右侧是使用预先构建的情感分析库从脚本数据生成标签的步骤。</p><p id="9112" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这个项目中，我使用了<a class="ae le" href="http://multicomp.cs.cmu.edu/resources/cmu-mosei-dataset/" rel="noopener ugc nofollow" target="_blank">卡内基梅隆大学的多模态意见情绪和情感强度数据集</a>。这可以说是关于这个主题的最全面的数据集，涵盖了语言、视觉和听觉形态。该数据集由超过1000名在线YouTube发言人的超过23，500个句子话语视频组成。这是完全转录和适当的标点符号。虽然这种方法不需要任何情绪标签，但我使用这个数据集的原因是为了让我在未来对结果进行基准测试和比较。</p><h2 id="124c" class="mi mj iq bd mk ml mm dn mn mo mp dp mq lo mr ms mt ls mu mv mw lw mx my mz iw bi translated">数据预处理</h2><p id="c2fb" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">数据集中的音频文件非常大，包含多个句子和非语音音频，如音乐。因此，第一个预处理步骤是从完整的音频文件中提取每个句子的音频片段。为了提取音频剪辑，我使用了抄本的时间戳。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/ea23de5c25da1ac7ed11015f56882b43.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/1*UydmCpVk3Ze9Lj1h7ue3_Q.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">使用抄本时间戳按句子分割音频文件</p></figure><p id="215d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我使用的建模技术需要相同长度的特征，在我的例子中是相同长度的音频剪辑。因此，我决定根据数据中剪辑的平均持续时间，将我的音频文件裁剪或填充到7秒。</p><p id="7752" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">长于7秒的音频剪辑是我用随机偏移剪辑的。对于较短的剪辑，我也用随机偏移填充了静音(零)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nh"><img src="../Images/f94b8455e32fc2cba81e3d675b592703.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EyihEg64ZxJtlPnjFquLqw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">将音频文件裁剪或填充到固定长度</p></figure><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="f0fc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在分割和填充我的音频文件后，我使用了<a class="ae le" href="https://librosa.org/doc/latest/index.html" rel="noopener ugc nofollow" target="_blank"> Librosa </a>库将音频转换成Mel比例光谱图。这些是信号频谱随时间变化的直观表示，广泛用于音频分类。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="e3d2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">下面你可以看到数据集中两个随机音频文件的Mel光谱图。右边图两边的黑色区域是我用来填充音频剪辑的静音，因为它短于7秒。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nk"><img src="../Images/72853676f5fd6b7fca579c752fa1f810.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bPpVGqqhXeK03vTHmJYvKg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">来自数据集的两个随机音频文件的Mel光谱图</p></figure><p id="d147" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">对于文字记录，我使用了两个不同的现成文本情感分析库——<a class="ae le" href="https://textblob.readthedocs.io/en/dev/quickstart.html" rel="noopener ugc nofollow" target="_blank">text blob</a>和<a class="ae le" href="https://pypi.org/project/vaderSentiment/" rel="noopener ugc nofollow" target="_blank"> Vader </a>，来测量句子的二元情感。我对每次观察的情绪得分进行了平均，并在此基础上对我的观察进行了二元分类——积极的和消极的。我排除了平均情绪绝对值非常低的观察结果。</p><h2 id="9e91" class="mi mj iq bd mk ml mm dn mn mo mp dp mq lo mr ms mt ls mu mv mw lw mx my mz iw bi translated">情感识别建模</h2><p id="36b1" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">在数据预处理步骤之后，我将数据集分成80%的训练集、10%的验证集和10%的测试集。</p><p id="a0b7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我使用<a class="ae le" href="https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc#:~:text=An%20ROC%20curve%20(receiver%20operating,False%20Positive%20Rate" rel="noopener ugc nofollow" target="_blank"> ROC AUC </a>作为我的二元分类器的主要性能指标。这是一种通常用于具有不平衡类的二元分类器的性能度量。它的值范围从0到1，无信息分类器产生0.5，完美分类器产生1。</p><p id="2bcb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">由于我的训练集只有略多于11，000个观察值，为了减少特征的数量，我对跨时间戳的Mel谱图值进行了平均。正如所料，许多谱图特征彼此高度相关。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/59de541d21a97f8bf42ca13eb7498bb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*mrRle4iYsIu4O3YuC1rYmg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">特征之间的相关性</p></figure><p id="965d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我建立的一些经典机器学习模型产生了以下ROC AUC分数:随机森林0.51，K-最近邻0.54，深度神经网络0.6。</p><p id="606b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了使用我的光谱图中的全部数据，而不求平均值，我决定使用迁移学习从我的光谱图中提取低维特征。为此，我使用了<a class="ae le" href="https://keras.io/api/applications/vgg/" rel="noopener ugc nofollow" target="_blank"> VGG16 </a> Keras预训练网络。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nm"><img src="../Images/2758dfae5f81f87b5de29dd0ffe8e38f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e3O7GgTtSnMqsQ35QaIKGw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">VGG16是由K. Simonyan和A. Zisserman提出的卷积神经网络</p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nm"><img src="../Images/50dadcbef76d8b161f3efdf1c58ac704.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SFEHDmH3E-gERk_9XRIJzQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作为DNN输入的低维特征</p></figure><p id="717b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我用这些提取的特征用Keras训练了一个模型。该模型在验证集上的性能为0.61，在测试集上的性能为0.63。这不是一个显著的改进，但由于它使用了整个光谱图，随着更多的观察，它有最大的改进潜力。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/2a4cafa2ceb538f2f82e4d742fe59017.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*UwakmN4K_uSn4UoCIDxCUA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">DNN用VGG16提取特征测试集ROC曲线</p></figure><h2 id="62c2" class="mi mj iq bd mk ml mm dn mn mo mp dp mq lo mr ms mt ls mu mv mw lw mx my mz iw bi translated">用我自己的声音测试</h2><p id="6457" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">作为一个有趣的实验，我决定测试这个模型是否可以推广到其他语言。我录下了8段话语:我说的四种语言——英语、法语、俄语和亚美尼亚语——各有一段正面和一段负面的话语。然后我从这些声音片段中预测了情绪。</p><p id="30dd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">结果值非常接近，因此我使用最小最大缩放在0和1之间缩放它们。基于这些极其有限的数据，似乎语言起了很大的作用。然而，在每种语言中，积极的情绪记录比消极的情绪记录得分高，这是一个很好的结果。下一步将是用适当的统计严谨性来研究这一现象。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="no nj l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">回放录制的话语</p></figure><h2 id="d548" class="mi mj iq bd mk ml mm dn mn mo mp dp mq lo mr ms mt ls mu mv mw lw mx my mz iw bi translated">结论</h2><p id="4acc" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">总之，我已经创建了一个自我监督的机器学习分类器，它使用转移学习从语音中预测情感，并根据音频转录本生成的标签进行训练。</p><p id="9011" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">像任何其他方法一样，这也有它的优点和缺点。优点主要归结为大量未标记数据的可用性和推广到其他语言的潜力。缺点大多与这样一个事实有关:我们所说的和我们所说的方式并不总是一致，这导致了大量不准确的标签。</p><p id="af45" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这种类型的模型可以有各种业务应用，其中之一是在呼叫中心使用它来衡量和提高代理提供的服务质量。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><p id="a2cd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">请访问我的<a class="ae le" href="https://github.com/SatenikS" rel="noopener ugc nofollow" target="_blank"> Github </a>帐户，看看这个和我的其他项目的代码。也可以随时通过<a class="ae le" href="https://www.linkedin.com/in/satenik-safaryan/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>联系我。</p></div></div>    
</body>
</html>