<html>
<head>
<title>Running multiple GPU ImageNet experiments using Slurm with Pytorch Lightning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Slurm和Pytorch Lightning运行多个GPU ImageNet实验</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/running-multiple-gpu-imagenet-experiments-using-slurm-with-pytorch-lightning-ac90f3db5cf9?source=collection_archive---------21-----------------------#2021-03-30">https://towardsdatascience.com/running-multiple-gpu-imagenet-experiments-using-slurm-with-pytorch-lightning-ac90f3db5cf9?source=collection_archive---------21-----------------------#2021-03-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/def039750e52b715c2aa441550113494.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UgDh5Ng1fqTjNtNq"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">照片由<a class="ae jd" href="https://unsplash.com/@imgix?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> imgix </a>在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h2 id="5a4f" class="je jf jg bd b dl jh ji jj jk jl jm dk jn translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/making-sense-of-big-data" rel="noopener" target="_blank">理解大数据</a></h2><div class=""/><p id="4608" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi lk translated">从MNIST和CIFAR的沙坑梦幻世界毕业后，是时候转向ImageNet实验了。也许你也站在那里盯着那百万以上的数据集，问自己应该从哪个方向接近这头野兽。在这里，我将给出我采取的一步一步的方法，希望它能帮助你与怪物搏斗。</p><p id="4305" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">首先，警告:</p><p id="393f" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="lt">不要低估运行ImageNet实验所需的计算:</em> <strong class="ko jq"> <em class="lt">经常需要多个GPU每个实验需要几个小时。</em>T13】</strong></p><p id="d2f0" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如果你正在读这一行，那么你已经决定你有足够的计算和耐心继续，让我们看看我们需要采取的核心步骤。我的方法在使用SLURM(我的大学集群)、Pytorch和Lightning的计算集群上使用多个GPU。本教程假设一个基本的能力，导航他们所有的❤</p></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><h1 id="66f4" class="mb mc jg bd md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my bi translated">关键步骤</h1><p id="2755" class="pw-post-body-paragraph km kn jg ko b kp mz kr ks kt na kv kw kx nb kz la lb nc ld le lf nd lh li lj ij bi translated">1.在Lightning中设置DDP</p><p id="27fa" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">2.访问ImageNet数据集</p><p id="8941" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">3.Bash脚本指令Slurm</p></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><h1 id="3630" class="mb mc jg bd md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my bi translated">在Lightning中设置DDP</h1><p id="d327" class="pw-post-body-paragraph km kn jg ko b kp mz kr ks kt na kv kw kx nb kz la lb nc ld le lf nd lh li lj ij bi translated"><em class="lt">等等，DDP是什么？</em></p><p id="b370" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">问得好，DDP代表分布式数据并行，是一种允许不同GPU和集群中不同节点之间进行通信的方法。有很多方法可以做到这一点，但是我们只讨论DDP，因为它是推荐的，并且是用Lightning实现的。</p><p id="2a24" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">DDP在每个可用的GPU上训练模型的副本，并为每个GPU将一个小批量分成独占的片。向前传球非常简单。每个GPU在其子小批量上进行预测，并且预测被合并。简单。</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ne"><img src="../Images/9db9474a410158fc7eacef5d2a52fa28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2OQB-LKFJzXj8Bqr1cG9fA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated"><em class="nj">DDP的前传。小批量与相同的模型参数一起在每个GPU之间平均分配。然后，每个GPU计算正向传递，并汇总输出预测</em></p></figure><p id="28a3" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">向后传球有点棘手。DDP的非分布式版本(叫做，你猜对了，DP)要求你有一个“主节点”,它收集所有的输出，计算梯度，然后将这个传递给所有的模型。</p><p id="00e5" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">但是，DDP对中央集权的官僚体制说不。相反，每个GPU负责将模型权重梯度(使用其子小批量计算)发送到其他每个GPU。收到完整的梯度集后，每个GPU都会汇总结果。结果如何？每个GPU上的每个模型副本都有相同的更新。这种操作的名称是<em class="lt">全归约</em>操作。</p><figure class="nf ng nh ni gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nk"><img src="../Images/b691413549957131ebe046489746a65f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wBItxpitaPSjNBkK9HCJKw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated"><em class="nj">用于更新每个GPU上的模型的all-reduce操作。所有的GPU都向所有其他的GPU发送它们自己的梯度(记住它们是在一个子小批量上训练的),然后它们各自聚合。</em></p></figure><h1 id="2944" class="mb mc jg bd md me nl mg mh mi nm mk ml mm nn mo mp mq no ms mt mu np mw mx my bi translated">好吧，告诉我闪电帮我设置了这个？</h1><p id="0884" class="pw-post-body-paragraph km kn jg ko b kp mz kr ks kt na kv kw kx nb kz la lb nc ld le lf nd lh li lj ij bi translated">是的，确实如此。我假设你有一点闪电经验的读者，所以将只专注于关键的事情去做:</p><p id="0f67" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko jq">同步记录</strong></p><p id="bb66" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">就像确保梯度更新是相同的一样，您还需要更新您必须考虑到通信需求的任何度量记录。如果不这样做，您的准确性将取决于GPU，仅基于GPU看到的数据子集。</p><p id="57d6" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为多个GPU进行转换非常简单。只需在所有日志中添加“sync_dist = True”即可。</p><figure class="nf ng nh ni gt is"><div class="bz fp l di"><div class="nq nr l"/></div></figure><h2 id="e642" class="ns mc jg bd md nt nu dn mh nv nw dp ml kx nx ny mp lb nz oa mt lf ob oc mx jm bi translated">设置GPU设备和DDP后端</h2><p id="efa7" class="pw-post-body-paragraph km kn jg ko b kp mz kr ks kt na kv kw kx nb kz la lb nc ld le lf nd lh li lj ij bi translated">现在我们需要更新我们的训练器，以匹配我们正在使用的GPU数量。或者，您可以让Lightning计算出您有多少个GPU，并将GPU的数量设置为-1。</p><p id="d5b4" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如前所述，我使用DDP作为我的分布式后端，因此设置我的加速器。这里没什么可做的&gt;&gt;</p><pre class="nf ng nh ni gt od oe of og aw oh bi"><span id="f1bc" class="ns mc jg oe b gy oi oj l ok ol">trainer = Trainer(gpus=-1, accelerator='ddp')</span></pre><p id="be86" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这就是Python代码。根据您设置模型的方式，您可能还需要删除任何。到()或。cuda()调用——这会导致问题。</p><p id="6cc0" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如果遇到什么困难:<a class="ae jd" href="https://pytorch-lightning.readthedocs.io/en/stable/advanced/multi_gpu.html" rel="noopener ugc nofollow" target="_blank">https://py torch-lightning . readthedocs . io/en/stable/advanced/multi _ GPU . html</a></p></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><h1 id="b966" class="mb mc jg bd md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my bi translated">ImageNet</h1><p id="d149" class="pw-post-body-paragraph km kn jg ko b kp mz kr ks kt na kv kw kx nb kz la lb nc ld le lf nd lh li lj ij bi translated">闪电代码准备好了，是时候抓ImageNet了。通过torchvision下载数据集不再像以前那么简单。相反，我将给出我发现有效的两个选项</p><p id="c4a1" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko jq">长版</strong></p><p id="5e7a" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><a class="ae jd" href="http://image-net.org/download" rel="noopener ugc nofollow" target="_blank">http://image-net.org/download</a></p><p id="225a" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">点击上面的并请求访问。这可能需要几天时间才能被批准用于非商业用途。</p><p id="b6f6" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko jq">短版</strong></p><div class="ip iq gp gr ir om"><a href="https://www.kaggle.com/c/imagenet-object-localization-challenge/data" rel="noopener  ugc nofollow" target="_blank"><div class="on ab fo"><div class="oo ab op cl cj oq"><h2 class="bd jq gy z fp or fr fs os fu fw jp bi translated">ImageNet对象本地化挑战</h2><div class="ot l"><h3 class="bd b gy z fp or fr fs os fu fw dk translated">识别图像中的物体</h3></div><div class="ou l"><p class="bd b dl z fp or fr fs os fu fw dk translated">www.kaggle.com</p></div></div><div class="ov l"><div class="ow l ox oy oz ov pa ix om"/></div></div></a></div><p id="7b80" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">转到Kaggle，加入comp并使用下面的bash命令下载数据。</p><pre class="nf ng nh ni gt od oe of og aw oh bi"><span id="80d2" class="ns mc jg oe b gy oi oj l ok ol">kaggle competitions download -c imagenet-object-localization-challenge</span></pre><p id="9f4f" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在这两种情况下，当下载到您的集群实例时，您可能希望下载到scratch而不是您的主文件空间，因为，嗯，ImageNet是一头野兽，很快就会超出最大的存储空间。</p><p id="08ec" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="ko jq">创建您的lightning数据模块</strong></p><figure class="nf ng nh ni gt is"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="5b35" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">您可以像连接其他模块一样连接该数据模块，这样培训就可以按照以下方式进行:</p><pre class="nf ng nh ni gt od oe of og aw oh bi"><span id="6134" class="ns mc jg oe b gy oi oj l ok ol">data = ImageNet_Module()<br/>model = YourModel()<br/>trainer = Trainer(gpus=-1, accelerator='ddp', max_epochs=90)<br/>trainer.fit(model, data)<br/>trainer.test()</span></pre><p id="0f12" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">当然，您会希望将它放入一个漂亮的Python文件中，其中包含您希望bash脚本调用的所有铃声、哨声和定制模型。好了，我想我们已经准备好了最后一块胶水，SLURM脚本。</p></div><div class="ab cl lu lv hu lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="ij ik il im in"><h1 id="9d74" class="mb mc jg bd md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my bi translated">SLURM脚本</h1><p id="0a6a" class="pw-post-body-paragraph km kn jg ko b kp mz kr ks kt na kv kw kx nb kz la lb nc ld le lf nd lh li lj ij bi translated">至此，所有艰难的工作都完成了。下面我将给出我在我的大学集群上运行的示例脚本作为示例:</p><figure class="nf ng nh ni gt is"><div class="bz fp l di"><div class="nq nr l"/></div></figure><p id="be9a" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">当然，你会受到分配的资源和限制的约束，但这应该有助于给你一个基本的轮廓来开始。要使用这个大纲，您需要设置conda环境，并在集群上安装所需的库。</p><h1 id="b3ac" class="mb mc jg bd md me nl mg mh mi nm mk ml mm nn mo mp mq no ms mt mu np mw mx my bi translated">这是所有的乡亲</h1><p id="1083" class="pw-post-body-paragraph km kn jg ko b kp mz kr ks kt na kv kw kx nb kz la lb nc ld le lf nd lh li lj ij bi translated">好了，就这样了。</p><p id="d664" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如果一切按计划进行，你现在应该在训练过程中。</p><p id="d511" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">对于我的设置，一个使用4x RTX 8000的现成ResNet18模型每个时期大约需要30分钟，批量大小为128。</p><p id="8c2e" class="pw-post-body-paragraph km kn jg ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这是一个n=1的例子，说明如何使用SLURM和Lightning进行ImageNet实验，所以我确信在资源、库和版本略有不同的情况下会出现障碍和问题，但希望这将有助于您开始驯服野兽。</p><h1 id="69d3" class="mb mc jg bd md me nl mg mh mi nm mk ml mm nn mo mp mq no ms mt mu np mw mx my bi translated"><strong class="ak">感谢您阅读❤ </strong></h1><h1 id="33ff" class="mb mc jg bd md me nl mg mh mi nm mk ml mm nn mo mp mq no ms mt mu np mw mx my bi translated">使用的工具</h1><ul class=""><li id="2248" class="pb pc jg ko b kp mz kt na kx pd lb pe lf pf lj pg ph pi pj bi translated">Pytorch (1.7)</li><li id="d989" class="pb pc jg ko b kp pk kt pl kx pm lb pn lf po lj pg ph pi pj bi translated">Pytorch闪电(1.2)</li><li id="53e7" class="pb pc jg ko b kp pk kt pl kx pm lb pn lf po lj pg ph pi pj bi translated">SLURM管理器(Uni计算集群)</li><li id="c0cb" class="pb pc jg ko b kp pk kt pl kx pm lb pn lf po lj pg ph pi pj bi translated">4辆原始的夸德罗RTX 8000</li></ul></div></div>    
</body>
</html>