<html>
<head>
<title>What is Residual Connection?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">什么是剩余连接？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55?source=collection_archive---------0-----------------------#2021-12-18">https://towardsdatascience.com/what-is-residual-connection-efb07cab0d55?source=collection_archive---------0-----------------------#2021-12-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="cc9b" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">一种训练深度神经网络的技术</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/161555158ce15cd9bb28c25248c578ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gp1gtP2-JSSuo0bg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@yiranding?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">丁</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="4a67" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">训练神经网络的一个困境是，我们通常希望更深的神经网络具有更好的准确性和性能。但是，网络越深，训练越难收敛。在本文中，我们将讨论剩余连接(也称为跳过连接)，这是一种简单但非常有效的技术，可以使训练深度神经网络更加容易。它被不同的模型广泛采用，从计算机视觉中首次引入的 ResNet，自然语言处理中的 Transformer，一直到强化学习中的 AlphaZero 和蛋白质结构预测的 AlphaFold。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="e358" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">公式</h2><p id="79c9" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">在传统的前馈神经网络中，数据按顺序流经每一层:一层的输出是下一层的输入。</p><p id="8f31" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">剩余连接通过跳过一些层为数据到达神经网络的后面部分提供了另一条路径。考虑一个层序列，层<em class="mx"> i </em>到层<em class="mx"> i + n </em>，设<em class="mx"> F </em>为这些层所代表的函数。用<em class="mx"> x </em>表示层<em class="mx"> i </em>的输入。在传统的前馈设定中，<em class="mx"> x </em>会简单的一个一个的经过这些层，层<em class="mx"> i + n </em>的结果是<em class="mx"> F </em> ( <em class="mx"> x </em>)。绕过这些层的剩余连接通常如下工作:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi my"><img src="../Images/079e6a4e56871fd2d93d2a873278e9f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/format:webp/1*RTYKpn1Vqr-8zT5fqa8-jA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图一。残留块。由作者创作。</p></figure><p id="ec8c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">剩余连接首先对<em class="mx"> x </em>应用身份映射，然后执行元素加法<em class="mx">F</em>(<em class="mx">x</em>)+<em class="mx">x</em>。在文献中，取一个输入<em class="mx"> x </em>并产生输出<em class="mx">F</em>(<em class="mx">x</em>)+<em class="mx">x</em>的整个架构通常被称为剩余块或积木块。通常，残差块还会包括一个激活函数，例如应用于<em class="mx">F</em>(<em class="mx">x</em>)+<em class="mx">x</em>的 ReLU。</p><p id="beb4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">强调上图中看似多余的身份映射的主要原因是，如果需要，它可以作为更复杂函数的占位符。例如，只有当<em class="mx"> F </em> ( <em class="mx"> x </em>)和<em class="mx"> x </em>具有相同的维度时，元素相加<em class="mx"> F </em> ( <em class="mx"> x </em> ) + <em class="mx"> x </em>才有意义。如果它们的维数不同，我们可以用线性变换(即乘以矩阵<em class="mx"> W </em>)代替恒等式映射，改为执行<em class="mx">F</em>(<em class="mx">x</em>)+<em class="mx">Wx</em>。</p><p id="7a48" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通常，在整个神经网络中使用多个残差块，这些残差块可以具有相同或不同的架构。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="dd46" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">它如何帮助训练深度神经网络</h2><p id="7753" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">对于前馈神经网络，由于诸如爆炸梯度和消失梯度的问题，训练深度网络通常是非常困难的。另一方面，经验表明，具有剩余连接的神经网络的训练过程更容易收敛，即使网络有几百层。像深度学习中的许多技术一样，我们仍然没有完全理解关于剩余连接的所有细节。然而，我们确实有一些有趣的理论，这些理论得到了强有力的实验结果的支持。</p><h2 id="7709" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">表现得像浅层神经网络的集合</h2><p id="94e5" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">对于前馈神经网络，正如我们上面提到的，输入将依次通过网络的每一层。更专业地说，输入通过一个长度等于层数的单一路径。另一方面，具有剩余连接的网络由许多不同长度的路径组成。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mz"><img src="../Images/f68e849ebe59e18b42abe757e549f095.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EMHk3-4qk-MK6O34BTQhxA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图二。剩余连接的未解视图。由作者创作。</p></figure><p id="b59d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作为例子，考虑具有 3 个剩余块的网络。我们可以尝试逐步扩展这个网络的公式:</p><pre class="kg kh ki kj gt na nb nc nd aw ne bi"><span id="7b1e" class="lz ma iq nb b gy nf ng l nh ni"><em class="mx">x</em>₃ = <em class="mx">H</em>(<em class="mx">x</em>₂) + <em class="mx">x</em>₂<br/>   = <em class="mx">H</em>(<em class="mx">G</em>(<em class="mx">x</em>₁) + <em class="mx">x</em>₁) + <em class="mx">G</em>(<em class="mx">x</em>₁) + <em class="mx">x</em>₁<br/>   = <em class="mx">H</em>(<em class="mx">G</em>(<em class="mx">F</em>(<em class="mx">x</em>₀) + <em class="mx">x</em>₀) + <em class="mx">F</em>(<em class="mx">x</em>₀) + <em class="mx">x</em>₀) + <em class="mx">G</em>(<em class="mx">F</em>(<em class="mx">x</em>₀) + <em class="mx">x</em>₀) + <em class="mx">F</em>(<em class="mx">x</em>₀) + <em class="mx">x</em>₀</span></pre><p id="d5de" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有 2 = 8 项输入<em class="mx"> x </em> ₀对输出<em class="mx"> x </em> ₃.有贡献因此，我们也可以将此网络视为长度为 0、1、2 和 3 的 8 条路径的集合，如上图所示。</p><p id="73cf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据这种“未阐明的观点”，[6]表明，具有剩余连接的网络的行为类似于不强烈相互依赖的网络的集合体。而且梯度下降训练时的梯度大部分来自于短路径。换句话说，剩余连接不能解决爆炸或消失梯度问题。相反，它通过在“系综”中使用浅层网络来避免这些问题。</p><h2 id="af55" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">解决破碎渐变问题</h2><p id="c60b" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">虽然剩余连接不能解决爆炸或消失梯度问题，但它可以解决另一个与梯度相关的问题。</p><p id="632a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[1]表明，对于深度前馈神经网络，梯度类似于白噪声。这不利于训练，这个问题被称为破碎梯度问题。残差连接通过向梯度引入一些空间结构来解决这个问题，从而帮助训练过程。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="4f81" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">实现示例</h2><ul class=""><li id="f616" class="nj nk iq ky b kz ms lc mt lf nl lj nm ln nn lr no np nq nr bi translated">TensorFlow:一个<a class="ae kv" href="https://www.tensorflow.org/tutorials/customization/custom_layers#models_composing_layers" rel="noopener ugc nofollow" target="_blank"> ResNet 块示例</a></li><li id="14df" class="nj nk iq ky b kz ns lc nt lf nu lj nv ln nw lr no np nq nr bi translated">PyTorch: <a class="ae kv" href="https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L100" rel="noopener ugc nofollow" target="_blank"> ResNet 源代码</a></li></ul></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="4ba5" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">进一步阅读</h2><ol class=""><li id="c5de" class="nj nk iq ky b kz ms lc mt lf nl lj nm ln nn lr nx np nq nr bi translated">残差连接的命名来源于以下:有些函数<em class="mx"> H </em> ( <em class="mx"> x </em>)对于一个层的序列来说是非常难学的，但是学习相应的残差函数<em class="mx">H</em>(<em class="mx">x</em>)-<em class="mx">x</em>就容易多了。例如，恒等函数<em class="mx">H</em>(<em class="mx">x</em>)=<em class="mx">id</em>(<em class="mx">x</em>)很难拟合，但它的残差函数只是零函数，通过将所有层中的所有权重都设为 0，可以很容易地学习到零函数。<br/>有了残差连接，我们的层只需要学习<em class="mx">F</em>(<em class="mx">x</em>)=<em class="mx">H</em>(<em class="mx">x</em>)-<em class="mx">x</em>，残差块的最终输出就是<br/><em class="mx">F</em>(<em class="mx">x</em>)+<em class="mx">x = H</em>(<em class="mx">x</em>)-<em class="mx">x 关于这个动机的更多细节，我们可以参考 ResNet 的原始论文[3]。</em></li><li id="eba9" class="nj nk iq ky b kz ns lc nt lf nu lj nv ln nw lr nx np nq nr bi translated">[5]示出了残差块能够执行迭代特征细化，这意味着每个块略微改善了前一层的表示。这部分解释了为什么具有残差块的非常深的神经网络具有更好的准确性:残差块越多，表示越精细。</li><li id="2574" class="nj nk iq ky b kz ns lc nt lf nu lj nv ln nw lr nx np nq nr bi translated">[2]表明 ResNet 模型架构，通过改进的训练和扩展策略，能够匹配最近的最新性能。这展示了剩余连接的有用性和相关性。</li><li id="7f6f" class="nj nk iq ky b kz ns lc nt lf nu lj nv ln nw lr nx np nq nr bi translated">剩余连接的关键特征是它提供了从早期层到后期层的短路径。[4]通过将所有层直接相互连接，进一步发展了这一思想:每一层从所有前面的层获得输入，并将其输出传递给所有后面的层。这表明在设计神经网络体系结构方面有很大的创造性空间。</li></ol></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h2 id="0861" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">参考</h2><ol class=""><li id="ff3b" class="nj nk iq ky b kz ms lc mt lf nl lj nm ln nn lr nx np nq nr bi translated">D.Balduzzi，M. Frean，L. Leary，J.P. Lewis，k . w-d . Ma 和 B. McWilliams。破碎的渐变问题:如果结果是答案，那么问题是什么？ (2017)，ICML 2017。</li><li id="0f42" class="nj nk iq ky b kz ns lc nt lf nu lj nv ln nw lr nx np nq nr bi translated">I .贝洛、w .费杜斯、x .杜、E.D .库布克、a .斯里尼瓦斯、t-y .林、j .施伦斯和 b .佐夫。<a class="ae kv" href="https://arxiv.org/abs/2103.07579" rel="noopener ugc nofollow" target="_blank">重温 ResNets:改进的培训和扩展策略</a> (2021)，NIPS 2021。</li><li id="a1aa" class="nj nk iq ky b kz ns lc nt lf nu lj nv ln nw lr nx np nq nr bi translated">K.何，张，任，孙军。<a class="ae kv" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank">用于图像识别的深度残差学习</a> (2016)，CVPR 2016。</li><li id="1c03" class="nj nk iq ky b kz ns lc nt lf nu lj nv ln nw lr nx np nq nr bi translated">G.黄，刘，范德马腾，温伯格。<a class="ae kv" href="https://arxiv.org/abs/1608.06993" rel="noopener ugc nofollow" target="_blank">密集连接的卷积网络</a> (2017)，CVPR 2017。</li><li id="5b37" class="nj nk iq ky b kz ns lc nt lf nu lj nv ln nw lr nx np nq nr bi translated">南亚斯特日布斯基、达皮特、巴拉斯、维尔马、切和本吉奥。<a class="ae kv" href="https://arxiv.org/abs/1710.04773" rel="noopener ugc nofollow" target="_blank">剩余连接鼓励迭代推理</a> (2018)，ICLR 2018。</li><li id="c691" class="nj nk iq ky b kz ns lc nt lf nu lj nv ln nw lr nx np nq nr bi translated">A.韦特、m .威尔伯和 s .贝隆吉。<a class="ae kv" href="https://arxiv.org/abs/1605.06431" rel="noopener ugc nofollow" target="_blank">残差网络的行为类似于相对较浅网络的集合</a> (2016)，NIPS 2016。</li></ol></div></div>    
</body>
</html>