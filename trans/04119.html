<html>
<head>
<title>Stacking machine learning models for speech sentiment analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于语音情感分析的堆叠机器学习模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/stacking-machine-learning-models-for-speech-sentiment-analysis-adf433488845?source=collection_archive---------15-----------------------#2021-04-07">https://towardsdatascience.com/stacking-machine-learning-models-for-speech-sentiment-analysis-adf433488845?source=collection_archive---------15-----------------------#2021-04-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="3d0c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如何建立一个从音频和文本记录中识别人类情感的模型？合著者:<a class="kl km ep" href="https://medium.com/u/c84d51f06925?source=post_page-----adf433488845--------------------------------" rel="noopener" target="_blank">关</a>，<a class="kl km ep" href="https://medium.com/u/422736fe9020?source=post_page-----adf433488845--------------------------------" rel="noopener" target="_blank">亚历山大洛朗</a>。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi kn"><img src="../Images/77af527c9abef54ac4025e1b3fb8955d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*J4Y5AkvR9a_ejcbq"/></div></div><p class="kz la gj gh gi lb lc bd b be z dk translated">照片由<a class="ae ld" href="https://unsplash.com/@helloimnik?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">你好我是Nik </a>上<a class="ae ld" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="b62a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在Le Wagon的bootcamp的最终项目的背景下，我和我的团队决定承担一项令人着迷的任务:语音情感识别。</p><p id="5487" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是一个巨大的挑战，因为情感在文化、性别、语言甚至个人层面上都是主观的，因此很难对人类情感进行普遍分类。</p><h2 id="a6eb" class="le lf iq bd lg lh li dn lj lk ll dp lm jy ln lo lp kc lq lr ls kg lt lu lv lw bi translated">我们的数据</h2><p id="0fa3" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">我们从卡耐基甜瓜大学找到了一个名为<a class="ae ld" href="https://github.com/A2Zadeh/CMU-MultimodalSDK" rel="noopener ugc nofollow" target="_blank"><em class="mc">CMU-莫塞</em> </a>的数据集，这是在线视频中句子级情感分析和情感识别的最大数据集。它包含超过65小时的注释视频，来自1000多名演讲者和250个主题。</p><p id="ef50" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">数据被分成不同长度的片段，每个片段代表一个完整的口语句子(特征)和情感，我们的目标，它在值-3到3之间变化(从负到正，0是中性的)。</p><p id="36b9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们决定分析录音和文字记录来预测一个人的句子背后的情绪。我们的直觉是，结合两个不同来源的两个模型，使用多模态学习，可以提高我们的性能。</p><h2 id="7459" class="le lf iq bd lg lh li dn lj lk ll dp lm jy ln lo lp kc lq lr ls kg lt lu lv lw bi translated">数据预处理</h2><p id="5ba7" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">我们工作的第一步是清理文本和音频数据。</p><p id="3dab" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">虽然已经从视频中提取了文本，但编辑主要是对文本文件进行基本格式化(删除标点、数字和大写字母)。然而，在自然语言处理(NLP)中，很难选择去除文本的哪些部分以及保留哪些部分(单个单词、句子、整个会话)。我试图对单词进行词汇化和词干化，但是没有发现性能上的改进。</p><p id="703d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">音频格式稍微复杂一些，我们尝试了两种方法:</p><p id="e003" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">1.首先是使用Python的库<a class="ae ld" href="https://librosa.org/doc/latest/index.html" rel="noopener ugc nofollow" target="_blank"> <em class="mc"> librosa </em> </a>进行音频特征提取。它能够从每个记录中提取5个主要特征(平均值):MFCC、色度、梅尔谱图、光谱质心&amp; Tonnetz。从中，我获得了大约190个特征，这些特征可以作为表格数据用于建模。</p><p id="5035" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">2.第二个是音频到Mel声谱图，它允许我们将音频解释为图像，并从视觉角度对其进行建模。请看下图，x轴代表时间(s)，y轴代表频率(Hz)，颜色强度代表信号幅度(dB)。在这种情况下，图像允许在深度学习设置(卷积网络)中进行特征提取。</p><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi md"><img src="../Images/53c9ee90e7200c5e818c96af7b7a79e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2LT4_6i6K6ie8EqJpZ8uWw.png"/></div></div><p class="kz la gj gh gi lb lc bd b be z dk translated">作者图片</p></figure></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="55ce" class="ml lf iq bd lg mm mn mo lj mp mq mr lm ms mt mu lp mv mw mx ls my mz na lv nb bi translated">堆叠机器学习模型</h1><h2 id="6939" class="le lf iq bd lg lh li dn lj lk ll dp lm jy ln lo lp kc lq lr ls kg lt lu lv lw bi translated">ML文本模型</h2><p id="640b" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">我首先尝试看看我们可以用一个简单的单词袋NLP模型实现什么结果。</p><p id="fd77" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">单词袋表示包括对文本中每个单词的出现次数进行计数。每个单词的计数变成一列。我决定使用<a class="ae ld" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> <em class="mc"> scikit-learn </em> </a>的CountVectorizer，并执行网格搜索来寻找最佳的超参数。</p><p id="d588" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些超参数包括:</p><ul class=""><li id="b65f" class="nc nd iq jp b jq jr ju jv jy ne kc nf kg ng kk nh ni nj nk bi translated">忽略数据集中出现频率高于指定阈值(<code class="fe nl nm nn no b"><strong class="jp ir">max_df</strong></code>)的单词</li><li id="0a2b" class="nc nd iq jp b jq np ju nq jy nr kc ns kg nt kk nh ni nj nk bi translated">指定矢量化时要保留的顶部特征的数量(<code class="fe nl nm nn no b"><strong class="jp ir">max_features</strong></code>)</li><li id="4db7" class="nc nd iq jp b jq np ju nq jy nr kc ns kg nt kk nh ni nj nk bi translated">指定要考虑的序列长度(<code class="fe nl nm nn no b"><strong class="jp ir">ngram_range</strong></code>)。</li></ul><p id="917d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">令人惊讶的是，给出最好结果的<code class="fe nl nm nn no b"><strong class="jp ir">ngram_range</strong></code>是在训练中只保持一个单词长度的那个(<code class="fe nl nm nn no b"><strong class="jp ir">(1,1)</strong></code>)。因此，我们的模型可能无法检测出“不开心”是一种负面情绪。我们的解释是，大多数时候，我们的模型会关注关键词(“好”、“灾难”)来检测正确的情绪。</p><p id="67d0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">矢量化之后，我使用了一个具有岭正则化的回归模型:其想法是通过在基于回归系数(betas)的损失函数中添加一个惩罚项来避免我们的模型过度拟合。我们决定使用L2罚函数，因为我们假设所有系数对预测都有相似的影响。</p><figure class="ko kp kq kr gt ks"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="8447" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了评估模型的性能，我使用了平均绝对误差(MAE ),一种预测值和“真实”观察值之间的误差度量。在这里，这个基本的NLP模型给了我们0.87 MAE，这意味着情绪的预测值和真实值之间的差异平均为0.87，其中情绪等级为[-3，3]。</p><p id="6fb8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">作为比较，我使用来自均匀分布的随机样本创建了一个基线模型，它给出了1.77 MAE。</p><h2 id="ffd5" class="le lf iq bd lg lh li dn lj lk ll dp lm jy ln lo lp kc lq lr ls kg lt lu lv lw bi translated">ML音频模型</h2><p id="7d54" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">如上所述，音频的输入变量(X)是从音频文件中提取的声学特征。</p><p id="709a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了预测情绪，我们使用scikit-learn构建了一个随机森林(RF)模型。RF是一种集成方法，它在数据集的子样本上打包一组决策树。这种方法的优点是RF使用平均来提高预测精度和控制过拟合。</p><p id="a602" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后，我们进行了网格搜索，以优化RF的超参数，包括:</p><ul class=""><li id="b21d" class="nc nd iq jp b jq jr ju jv jy ne kc nf kg ng kk nh ni nj nk bi translated">森林中的树木数量(<code class="fe nl nm nn no b"><strong class="jp ir">n_estimators</strong></code>)</li><li id="5f7a" class="nc nd iq jp b jq np ju nq jy nr kc ns kg nt kk nh ni nj nk bi translated">分割内部节点所需的最小样本数(<code class="fe nl nm nn no b"><strong class="jp ir">min_samples_split</strong></code></li><li id="1ccf" class="nc nd iq jp b jq np ju nq jy nr kc ns kg nt kk nh ni nj nk bi translated">树的最大深度(<code class="fe nl nm nn no b"><strong class="jp ir">max_depth</strong></code>)。</li></ul><figure class="ko kp kq kr gt ks"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="9398" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在这里，这个安装在音频功能上的RF模型给了我们0.91 MAE。</p><h2 id="42ce" class="le lf iq bd lg lh li dn lj lk ll dp lm jy ln lo lp kc lq lr ls kg lt lu lv lw bi translated">堆叠最大似然模型以改进我们的预测</h2><p id="c6fe" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">一旦这两个模型建立起来，我们打算把它们的预测结合起来，看看是否能改进我们的结果。</p><p id="169d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，我创建了一个定制的特性选择器，使我们的管道能够在堆叠它们之前为每个模型选择正确的特性:</p><figure class="ko kp kq kr gt ks"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="acfd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后，我使用scikit-learn的堆叠回归和MLP回归来创建我们的堆叠模型的结构。想法是增加神经元层，在管道执行中结合两种模型。迭代之后，我们选择了一个有五个神经元的单层。</p><p id="8277" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该模型作为一个深度学习神经网络工作:我实现了500个纪元(<code class="fe nl nm nn no b"><strong class="jp ir">max_iter</strong></code>)，这是一个在每个神经元中校正的线性激活函数(<code class="fe nl nm nn no b"><strong class="jp ir">activation='relu'</strong></code> ) &amp;，一个限制过度拟合的早期停止工具。</p><figure class="ko kp kq kr gt ks"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="766b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">堆叠ML模型的过程能够显著改善我们的预测:我们的最终模型达到了0.78的MAE。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="686b" class="ml lf iq bd lg mm mn mo lj mp mq mr lm ms mt mu lp mv mw mx ls my mz na lv nb bi translated">堆叠深度学习模型</h1><h2 id="a6dc" class="le lf iq bd lg lh li dn lj lk ll dp lm jy ln lo lp kc lq lr ls kg lt lu lv lw bi translated">NLP CNN模型</h2><p id="fa44" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">为了用神经网络分析文本，我选择了一个带有自定义嵌入的卷积网络模型。</p><p id="38d0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">嵌入包括将我们的训练集的每个单词放置在我们创建的多维空间中。我们决定创造自己的词汇，潜在地更加强调每个单词的情感‘价值&amp;因此比例如<a class="ae ld" href="https://www.tensorflow.org/tutorials/text/word2vec" rel="noopener ugc nofollow" target="_blank"> <em class="mc"> Word2Vec </em> </a> <em class="mc"> </em>更加精确。为此，我们创建了一个“词汇”类来训练&amp;保存这个词汇以备将来预测。</p><p id="a5a8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">至于模型，我们实现了一个卷积神经网络(CNN):这些类型的深度学习模型广泛用于图像，也执行某些NLP任务，这是情感预测的情况。</p><p id="5941" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面的代码展示了我们用<a class="ae ld" href="https://www.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> <em class="mc"> Tensorflow </em> </a>的<a class="ae ld" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"><em class="mc">keras</em></a><em class="mc"/>库构建的神经网络。在集成嵌入之后，训练数据通过一个卷积层。然后将它压平，得到一个由32个神经元组成的致密层。所有神经元都具有校正的线性激活函数(<code class="fe nl nm nn no b"><strong class="jp ir">activation='relu'</strong></code>)。</p><figure class="ko kp kq kr gt ks"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="f5fa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个CNN模型在我们的测试数据中给了我们0.75 MAE，成为我们最好的模型。</p><h2 id="b296" class="le lf iq bd lg lh li dn lj lk ll dp lm jy ln lo lp kc lq lr ls kg lt lu lv lw bi translated">音频CNN模型</h2><p id="e564" class="pw-post-body-paragraph jn jo iq jp b jq lx js jt ju ly jw jx jy lz ka kb kc ma ke kf kg mb ki kj kk ij bi translated">我们的下一个方法是使用音频Mel声谱图，这在深度学习中被广泛采用。我们将频率转换为Mel标度，结果成为Mel频谱图，它将成为CNN模型的输入(作为图像)。</p><p id="186c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因为人类不能在线性标度上感知频率，Mel标度接近人类对音高的感知。因此，在我们的研究中，所有的频率都被映射到128个Mel波段。</p><p id="bdee" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">由于CNN的所有输入应该具有相同的输入形状，因此我们为较短的音频填充静音，为较长的音频进行剪辑，以便获得唯一的输入形状(128，850，1)，其中128表示128个Mel带，850表示长度，1表示1通道(灰度图像)。</p><p id="48b9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是我们使用Mel声谱图为情绪预测建立的最终CNN模型。</p><figure class="ko kp kq kr gt ks"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="d9b3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这种对Mel光谱图的图像分析给出了0.89 MAE。</p><h2 id="7986" class="le lf iq bd lg lh li dn lj lk ll dp lm jy ln lo lp kc lq lr ls kg lt lu lv lw bi translated">堆叠DL模型</h2><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi nw"><img src="../Images/08f9702ba991a1717be28b79ddd32af4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*iMzu75hiUbeG7mgF"/></div></div><p class="kz la gj gh gi lb lc bd b be z dk translated">两个原始模型后具有两层神经元的DL堆叠的视觉表示—图片由作者提供</p></figure><p id="37e5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从ML结果中，我们了解到堆叠NLP和音频模型可以改进我们的预测。因此，由于使用了<em class="mc">keras</em>‘Concatenate’方法，我们在输出层之前堆叠了两个DL模型输出和一个密集层。</p><figure class="ko kp kq kr gt ks"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="56ea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">使用与ML桩中相同的方法，模型本质上基于两个模型输出进行回归。不幸的是，它并没有改善我们对测试数据的预测，因为这个叠加模型也给出了0.75 MAE。</p></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><h1 id="3593" class="ml lf iq bd lg mm mn mo lj mp mq mr lm ms mt mu lp mv mw mx ls my mz na lv nb bi translated">结果和展望</h1><figure class="ko kp kq kr gt ks gh gi paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><div class="gh gi nx"><img src="../Images/f3baf4d3a71ca6b5ad8cbebcd4078da9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gaMdI44lHlD-jy_ztSbmyw.png"/></div></div><p class="kz la gj gh gi lb lc bd b be z dk translated">作者图片</p></figure><p id="00b2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">总的来说，我们任务的最佳模型是我们的NLP深度学习模型。我们发现最终的0.75 MAE是可以接受的，反映了项目所花费的时间、数据集的大小及其质量:</p><p id="4b9f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先，它是由人类注释的:由于情绪和情感是高度主观的(文化、意义的不同解释、讽刺等等)，模型的质量受到了损害。其次，大多数时候情绪是中性的，这意味着数据可能会被扭曲。这导致我们的模型不成比例地预测情绪为中性(尽管它是积极的或消极的)。在不牺牲数据集大小(数据平衡)的情况下，解决这个问题的方法是收集更多的负面和正面数据，以便提取预测更广泛情感的特征。</p><p id="9e77" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">此外，我们认为我们的结果可以在几个方面得到改进:</p><ul class=""><li id="f731" class="nc nd iq jp b jq jr ju jv jy ne kc nf kg ng kk nh ni nj nk bi translated">在模型调整上花费更多时间，关注超参数、文本清理步骤或内核大小。</li><li id="a501" class="nc nd iq jp b jq np ju nq jy nr kc ns kg nt kk nh ni nj nk bi translated">尝试堆叠其他深度学习模型，看看它们是否可以改善我们的预测。Mel spectrogram似乎没有找到可以与我们的NLP CNN上分析的文本互补的模式。我们的结论是，这个问题值得深入挖掘，我们绝对欢迎来自社区的评论和建议！</li><li id="d978" class="nc nd iq jp b jq np ju nq jy nr kc ns kg nt kk nh ni nj nk bi translated">用不同的方法建立一个情绪分类器，能够预测快乐、愤怒、惊喜等，而不是情绪评级。</li></ul><p id="a3c3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">源代码可以在<a class="ae ld" href="https://github.com/Celine-Guan/backinthessr" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到。我们期待听到任何反馈或问题。</p><p id="7b94" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">作为结论，我们相信从音频和文本中进行情感识别有一个非常令人兴奋的未来，因为它允许从人们那里收集很好的见解。如果进一步推进，并与情感分类相结合，这类项目的一些用例可以为社会增加巨大的价值。例如:</p><ul class=""><li id="d012" class="nc nd iq jp b jq jr ju jv jy ne kc nf kg ng kk nh ni nj nk bi translated">改善电话客户服务。根据客户的情绪/观点重新引导客户。满意的客户可以被引导到销售部门，不满意的客户可以被引导到保留部门，困惑的客户可以被引导到技术支持部门，等等。</li><li id="c192" class="nc nd iq jp b jq np ju nq jy nr kc ns kg nt kk nh ni nj nk bi translated">这也是一种评估服务质量和品牌监控的好方法。我们绝对欢迎来自社区的评论和建议，并期待看到这一领域的改进。</li></ul></div><div class="ab cl me mf hu mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="ij ik il im in"><p id="a07f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[1]A，梁PP，茯苓S，Vij P，Cambria E，Morency L-P (2018)，面向人类交流理解的多注意递归网络，<em class="mc">第三十二届AAAI人工智能大会。</em></p><p id="7b38" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[2]:塔哈·宾胡拉依布。(2020年10月13日)。美国有线电视新闻网<a class="ae ld" rel="noopener" target="_blank" href="/nlp-with-cnns-a6aa743bdc1e">https://towardsdatascience.com/nlp-with-cnns-a6aa743bdc1e</a>NLP<em class="mc"/></p></div></div>    
</body>
</html>