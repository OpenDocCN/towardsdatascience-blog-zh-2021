<html>
<head>
<title>Regularization and Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">正则化和线性回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/regularization-and-linear-regression-bcaeba547c46?source=collection_archive---------15-----------------------#2021-03-25">https://towardsdatascience.com/regularization-and-linear-regression-bcaeba547c46?source=collection_archive---------15-----------------------#2021-03-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="db76" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">我们纠正过度拟合的那个</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/36b0c443aa3201016a79f6da875cefc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*sNx2S1HbkqJRJoQ4o8rRfQ.jpeg"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">正则化和线性回归|照片由Jr Korpa 拍摄</p></figure><p id="14b1" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这篇文章是我关于<a class="ae ku" rel="noopener" target="_blank" href="/linear-regression-with-bootstrapping-4924c05d2a9">线性回归和bootstrap </a>和<a class="ae ku" rel="noopener" target="_blank" href="/bayesian-statistics-11f225174d5a">贝叶斯统计</a>系列的延续。</p><p id="4307" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">之前我详细地讨论了线性回归，现在我将继续这个话题。正如我先前暗示的，我将提出正规化的话题。正则化的作用是简化模型。你想要一个更简单的模型的原因是，通常，一个更简单的will模型会在大多数测试中表现得更好。</p><p id="590c" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">名词（noun的缩写）用训练数据作为测试数据是不合适的。当您使用测试数据(模型尚未看到的数据)测试您的模型时，您只能清楚地看到正则化的全部效果。</p><p id="ec2d" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我的写作主要来自对机器学习效果较差的事物的统计建模。为了简单起见，下面的许多例子打破了这个规则，我们使用训练数据而不是测试数据来评估我们的模型。</p><p id="b38b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">因此，<strong class="kx iu">我们模型中的决定系数并不能真正代表模型的有效性，相反，它们只是相关系数</strong>。</p><h2 id="dbbf" class="lr ls it bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">寻找最佳模型</h2><p id="95ba" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">模型选择——有时也称为特征选择，是指我们使用不同的特征/属性/可用数据列为现有数据选择最佳模型。为模型选择不同的特征实质上创建了不同的模型。</p><p id="d9d7" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">哪一个都可以。选择我们的模型后，我们将调整它。正则化就是简化它:<strong class="kx iu">正则化=简化</strong>。</p><p id="4816" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们之所以想要一个更简单的模型，是因为我们希望避免一种叫做过拟合或过参数化的现象。过度拟合或过度参数化是一种经常发生的现象，尤其是当您有很多参数时。</p><p id="5e06" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">有多种正则化技术可供使用，包括:</p><ul class=""><li id="ebd2" class="mp mq it kx b ky kz lb lc le mr li ms lm mt lq mu mv mw mx bi translated">逐步地</li><li id="9aa1" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">里脊回归</li><li id="2e94" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">套索</li><li id="6d04" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">PCR — PCA +回归(主成分分析和正则化)</li></ul><p id="8ac8" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">PCR是用于正则化的最重要的技术。</p><h2 id="070f" class="lr ls it bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">模型适合度</h2><p id="5d28" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">概括地说，一个好的模型“很好地符合数据”然而，这是有细微差别的，因为不同的型号有不同的用途。如果你有一个数据集，你想知道这个数据集是否可以建模；如果你可以用一个数学公式来解释模型，你所拥有的数据——这就是你所想做的——那么使用相同的数据并使用该数据评估你的模型，或者使用我们的训练数据评估我们的模型，这是非常好的和合法的。</p><p id="abea" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">然而，如果你希望能够做任何类型的预测，并确定这个模型在预测某事方面有多强大——这通常是人们希望对模型做的事情:他们希望实际利用它——那么<strong class="kx iu">我们需要用测试数据</strong>测试模型。</p><p id="f1e2" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">好的模型需要很好的概括。例如，它需要从您的训练数据推广到您的测试数据。考虑下面的例子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nd"><img src="../Images/e4575ec4c1354af523929f98ac3dd8df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZYO4utfl2FcmDG-acgNvpg.jpeg"/></div></div></figure><ul class=""><li id="cb6d" class="mp mq it kx b ky kz lb lc le mr li ms lm mt lq mu mv mw mx bi translated"><strong class="kx iu">过度拟合</strong>:模型对训练数据非常有效，但不能推广到现实世界</li><li id="8907" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated"><strong class="kx iu">欠拟合</strong>:模型过于简单，笼统但没有意义。更少的差异，但更多的偏向错误的答案</li></ul><p id="36e1" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">为什么我把最左边的图像称为非常适合，也就是刚刚好的那个？因为当我们看这个的时候，黑线似乎很好的解释了那些蓝点。如果我们要做一个简单的线性回归，我们可能会得到类似中间图像上的线。我们称之为不合身。我们的模型会缺少方差，我们的模型会有太多的偏差；这就是我们要说的。</p><p id="ca75" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">现在，在右边，我们过度拟合。在这里，我们在模型中有太多的方差。所以模型方差非常明显，因为它上下波动，到处跳跃。这种差异有助于拟合这个特定的数据集。但是我们称之为过度拟合，我们说它对数据拟合得太好了。你可能会问，怎么/为什么这是一件坏事？嗯，这是一件坏事，因为如果我加入测试数据，测试数据可能会比其他两个模型更不适合。因为测试数据不会正好在这条黑线的最大值或最小值的这些波峰或波谷处。</p><h2 id="24fc" class="lr ls it bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">过度拟合</h2><p id="fa0b" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">过度参数化通常会导致过度拟合，并且还会带来其他一些问题:</p><ul class=""><li id="65d2" class="mp mq it kx b ky kz lb lc le mr li ms lm mt lq mu mv mw mx bi translated">参数数量&gt;数据维度</li><li id="2a27" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">输入要素的线性相关性</li><li id="226d" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">在训练数据集上效果很好，但在真实世界(测试数据)中性能很差(如果不是很糟糕的话)</li></ul><p id="8ddb" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们所说的输入要素的线性相关性存在一个问题。因此，如果您的一个输入要素可以用其他输入要素以线性方式解释，那么我们称之为线性依赖。也就是说，如果你可以线性组合(也就是说，通过加法、减法和乘法，添加一些列来得出或确定另一个已经存在的列)，那么这意味着你有一个线性依赖；这意味着其中一列，一列或多列只是其他列的组合。当你计算和确定模型时，这会导致问题。</p><h2 id="f2cf" class="lr ls it bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">防止过度拟合</h2><p id="4834" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">防止过度拟合的工具是正则化。正规化有不同的方式。从早期开始，逐步回归就是正则化的一种形式。什么是逐步回归？逐步删除/添加功能。有两种味道，向前和向后。向前逐步回归差不多是这样工作的:你选择你认为是你最好的特征。假设你有输入特征x_1，x_2，x_3，x_4等等；你选择一个你认为最好的(有很多种方法可以选择。)然后你来，你会创建一个线性回归，基于那个特征，假设是x_2，现在是输入，y是输出。然后你选择下一个最好的，比如说x_4，你用x_2来测试那个。如果模型变得更好，有所改善，那么你保留第二个x值。如果它不好，如果模型性能没有提高，那么你就不要保留它，然后你又回到只有x_2的状态。然后你尝试这些功能中的另一个。这被称为向前逐步回归。</p><p id="ffa9" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">向后逐步回归是更常见的方法。你得到所有的值，比如x_1，x_2，x_3，x_4，然后从这四个输入中建立一个线性回归，得到一个解释y的函数；计算/预测y，然后你去掉其中一列，看看模型是否有所改进。通常情况下，你会不太适应。但是您的模型可能会基于标准而改进，这些标准不仅仅是纯粹的拟合，它们可能会更好，因为您还会对参数进行惩罚，或者使用一种称为信息标准的测量方法，该方法要求您需要从任何给定的输入参数中获取如此多的信息。继续移除特征，直到模型性能停止提高。这被称为向后逐步回归。</p><p id="983c" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">其他的正则化技术是你为你的每一列设置一个惩罚；与该列相关的系数的罚值。这些是其他正规化技术。</p><h2 id="29ee" class="lr ls it bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">测量模型</h2><p id="e0dd" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">我们如何度量模型？有Akaike信息标准(AIC):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi ni"><img src="../Images/05fadcddbb90b0e22537013093b84702.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9Sk_TbNABYgz2ZLO"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">AIC公式</p></figure><ul class=""><li id="a64a" class="mp mq it kx b ky kz lb lc le mr li ms lm mt lq mu mv mw mx bi translated">k:估计参数的数量</li><li id="b848" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">可能性</li></ul><p id="382c" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这是确定一个模型是否有所改进的最受欢迎的标准。随着可比模型的AIC变得越来越小，你说AIC在改进，你的模型也在改进。因此，通过添加更多的列，或者删除更多的列，如果你的AIC有所改善，那么你就知道你在向正确的方向前进。如果你的AIC没有提高，那意味着没有下降，那么你知道你在错误的方向上迈出了一步；这意味着最后一次添加列或最后一次删除列应该被撤消，您可以返回并尝试其他操作。</p><p id="bdc5" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">另一个模型度量是贝叶斯信息标准(BIC)，但它很少在这种情况下使用，它非常类似于AIC:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nj"><img src="../Images/5b5c04f454191bce47f3322fefd8d0a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*IS9Bl9YtageWSsf3"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">BIC公制</p></figure><ul class=""><li id="b14b" class="mp mq it kx b ky kz lb lc le mr li ms lm mt lq mu mv mw mx bi translated">k:估计参数的数量</li><li id="176e" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">可能性</li><li id="f292" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">n:记录的测量次数</li></ul><p id="c951" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">不管你选择的标准是什么，你的最佳拟合模型是具有最小AIC或BIC的模型。</p><h2 id="1f8e" class="lr ls it bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">费用</h2><p id="af8f" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">当你确定任何函数时，任何机器学习算法都有一个所谓的代价函数。这是一个非常基本的概念。一个成本函数基本上是说，“你的模型有多差？”</p><p id="996b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我之前讨论过线性回归，差是残差的平方。提醒一下，残差是输出y的预测值和实际值之间的差值；然后你把它平方，然后你把所有的平方相加。这是一个成本函数，但以前没有明确地调用过。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nk"><img src="../Images/2d2357c764d94c3e521512f66ae7ac82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*akcxQuKHkl-xDhwK"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">RSS也被称为SSR，但它们并不总是一回事！</p></figure><p id="5bc6" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">名词（noun的缩写）b:在某些文献中，SSR不是残差平方和，而是平方和回归——一个不同的量。在本文的其余部分，我坚持使用RSS来避免对上述数量的混淆。</p><p id="1d93" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们在这里和那里改变了一些细节，我们可以用案例的数量来划分，这意味着正在使用的值的数量—数据点的数量。我们可以在这里放入一个常数，但总的来说，我们希望将它最小化:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nj"><img src="../Images/1f18be2636b36ca1e6f84277867a38df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*GOlW2DeKjr7Dk88Z"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">最小化成本函数</p></figure><p id="ad04" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这可能是最常见的成本函数之一。还有其他的，特别是逻辑回归。然而，在很大程度上，这是我们的主要成本函数——它可能会以一些小的方式进行修改。</p><p id="0522" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">成本函数计算惩罚。所以这里的惩罚就是这些平方和:计算事情有多糟糕。对于给定的训练建模数据集，这是一个损失。我们的想法是最小化这个损失或者最小化这个成本。所以你需要注意一些事情:</p><ul class=""><li id="3e43" class="mp mq it kx b ky kz lb lc le mr li ms lm mt lq mu mv mw mx bi translated">模型需要相互比较</li><li id="3c68" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">成本函数决定方差</li><li id="6157" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">根据数据集、目标变量和目的(要回答的业务问题)，选择正确的成本函数是至关重要的</li></ul><p id="50d2" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">相似的模型必须相互比较，这一点很重要。因此，你无法比较均方根误差(RMSE)，它与上面的成本函数密切相关，你无法真正比较一个模型与下一个模型，除非它们非常非常相似，除非有很多相同的东西。</p><p id="8fb6" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">如果你只有两个不同来源的不同模型，做不同的事情，有不同的目的，<strong class="kx iu">比较RMSE没有任何意义</strong>。这些均方根误差，或这些成本，只有在与一个非常相似的模型的另一个成本相比较时，才是可以理解的。</p><h2 id="92c7" class="lr ls it bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">方差和偏差</h2><p id="0ce0" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">一个好的成本函数试图平衡方差和偏差，使其成为方差和偏差的最佳组合。</p><p id="c16a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">什么是方差和偏差？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nd"><img src="../Images/e4575ec4c1354af523929f98ac3dd8df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZYO4utfl2FcmDG-acgNvpg.jpeg"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">展示方差与偏差</p></figure><p id="11e6" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在右边的过度拟合例子中，有很强的方差，方差太大；虽然在中心欠拟合的例子中有很大的偏差，但是偏差太大了。</p><h2 id="98fb" class="lr ls it bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">回归成本</h2><p id="5ac9" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">拟合度是最小化成本函数的另一种说法。到目前为止，我们一直在最小化残差平方和。非常容易混淆的是，这可以被称为SSR和RSS，我将使用RSS。总成本的另一个组成部分是我们系数的权重，只是系数的另一种说法。</p><p id="2811" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">提醒你一下，这个特殊情况下的系数，假设我们有两列:x _ 1列和x_2列。好吧，那我们就有:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nl"><img src="../Images/35cd3fbd29ea34f64d8a0097abbd8bba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*BHYbeG6Bv6c7WE_h"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">b和c是系数</p></figure><p id="c936" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">由于系数是我们回归的一部分(我们之前把它们当作1)，我们应该把它们包括在总成本函数中。实际上，我们把它们的平方加进去，我们会因为有这个系数而得到另一个惩罚。</p><p id="04e6" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">如果这个系数变为零，基本上就等于零了。这样的话，我们实际上可以去掉列。或者去掉这个惩罚，或者这个惩罚的一部分。很自然，如果我们去掉所有的列，就什么都没有了，那么我们只有y本身的平均值。</p><p id="16d3" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">所以我们需要，当我们添加一列时，我们会说，“好的，我们可以添加一列，我们可以使用这些信息。但我们将接受因添加该栏而受到的处罚。”<strong class="kx iu">不管那列得到什么系数，都将是我们的惩罚</strong>。这意味着如果它是一个相当大的，重要的列，它将有一个大的系数，所以它最好是值得的。因为这个系数也将被算作这个所谓的惩罚的一部分，它将增加总成本。通过把这个放入我们的计算中，我们实际上不再寻找最佳拟合，T4，我们寻找最佳拟合和较小系数之间的平衡。一个非常非常小的系数就是零。负系数也被认为是大的。</p><p id="c021" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><strong class="kx iu">总成本</strong> =拟合度+系数大小:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nj"><img src="../Images/51a0377b9648b59315c92131710336ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Q_hvMX_GIufdILyK"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">总成本，而不是“总成本减去数量成本”</p></figure><ul class=""><li id="4629" class="mp mq it kx b ky kz lb lc le mr li ms lm mt lq mu mv mw mx bi translated">当拟合度(RSS)较小时=良好拟合</li><li id="ef7a" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">当系数的幅度(误差平方和)很小时=良好拟合</li></ul><p id="65f4" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">如果你知道你的线性代数，这个重量(w)实际上叫做L2范数。对这个故事来说，重要的是把它想成系数的平方，有时叫做权重的平方，但它确实是一个系数。</p><p id="9bc0" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在<a class="ae ku" rel="noopener" target="_blank" href="/an-introduction-to-linear-regression-9cbb64b52d23">之前的一篇文章</a>中，我们引入了误差项ε。m和b项是系数(斜率和y截距)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nm"><img src="../Images/c7c855a357f4d64a4f523715e8f4e723.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*wrv1rlRy7l88NetW"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">ε表示误差，I表示它是哪个数据点(第一、第二等。)</p></figure><p id="0a3d" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">看起来我们只有一个输入列，也就是x和一个系数b，但实际上，取决于我们如何看待这个问题，x可以是列的一个完整向量，也可以是系数的一个完整向量。</p><h2 id="7643" class="lr ls it bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">特征选择</h2><p id="34a3" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">选择作为目标的良好预测器的特征子集的过程被称为特征选择。如上所述，它是型号选择的另一个术语。两者相同的原因是，如果你有不同的功能集，你实际上有不同的模型。</p><p id="2f94" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">它有助于:</p><ul class=""><li id="b339" class="mp mq it kx b ky kz lb lc le mr li ms lm mt lq mu mv mw mx bi translated">控制模型的复杂性</li><li id="8800" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">提高模型性能</li><li id="9079" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">提高泛化能力</li><li id="9294" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">加速模型学习而不降低精度</li></ul><p id="c000" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我个人的偏好是尽可能多地使用所有的特性，然后一旦我有了所有的特性，我就可以精简并只找到重要的特性。所以我想尽可能多地去掉一些特性。我为什么要这么做？我想要一个更简单的模型。为什么我想要一个更简单的模型？因为一个简单的模型概括得更好。泛化是什么意思？<strong class="kx iu">这意味着你可以在现实世界中使用它</strong>。该模型不仅解释了训练数据，而且也有更好的机会解释我们以前没有见过的数据。首先是您的测试数据，然后是您从未见过的数据，即您的运营数据。</p><p id="9f3b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">如前所述，逐步回归是一个重要的正则化工具。向后是最常见的，从所有特征开始，删除解释最小差异的列，直到模型性能达到峰值。向前是从平均值开始添加特性，直到模型性能达到峰值。然后是“两者”——在每一步检查是添加一个特性还是删除一个特性。说你往前走，你加了一个栏目。当你倒退的时候。现在，从这一点来看，最好删除的列可能不是您刚刚添加的列。可能是另一个。其原因是，列之间存在复杂的相互作用，这通常是无法预测的。这可能不是我正在做的线性回归的情况。但是在其他类型的回归中:树和神经网络，情况肯定是这样的。</p><p id="9e0c" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我敢说每个人都使用逐步回归。这并不意味着你只用逐步回归。其他类型的正则化方法包括脊，套索，以及称为弹性网的东西，这是脊和套索的组合。然后在此基础上，我做逐步回归。所以我认为结合使用这些是一个人进步的典型方式，是一个人工作得最好的方式。</p><h1 id="9c0c" class="nn ls it bd lt no np nq lw nr ns nt lz jz nu ka mc kc nv kd mf kf nw kg mi nx bi translated">逐步回归:Python示例</h1><p id="51fc" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">接下来的部分使用<a class="ae ku" href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/T0HSJ1" rel="noopener ugc nofollow" target="_blank">高尔顿的身高数据集</a>，逐步回归的一个例子，我在我的<a class="ae ku" rel="noopener" target="_blank" href="/bootstrap-resampling-2b453bb036ec"> bootstrap重采样</a>文章中使用了相同的数据集。</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="4d6c" class="lr ls it nz b gy od oe l of og"># imports</span><span id="684a" class="lr ls it nz b gy oh oe l of og">import pandas as pd<br/>import numpy as np<br/>import statsmodels.formula.api as sm<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt</span><span id="514d" class="lr ls it nz b gy oh oe l of og">%matplotlib inline</span><span id="9a29" class="lr ls it nz b gy oh oe l of og"># load data</span><span id="6316" class="lr ls it nz b gy oh oe l of og"># I previously saved the Galton data as data<br/>family_data = pd.read_csv(data, delimiter=’\t’)</span><span id="0dd3" class="lr ls it nz b gy oh oe l of og"># column labels<br/>family_data.columns = [“family”,”father”,”mother”,”gender”,”childHeight”, “kids”]</span><span id="66a3" class="lr ls it nz b gy oh oe l of og"># check data is well-imported<br/>family_data.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/2e07f3f664702c3114e03e86a5a809d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*-5NQJDaKfnkRlM08soAwPw.jpeg"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">前5行，数据看起来不错</p></figure><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="1501" class="lr ls it nz b gy od oe l of og"># check data types<br/>family_data.dtypes</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/599f646a7007d795027cfbdbfa7738e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/1*8EdIPaIWohrNzh-QOsz-ow.jpeg"/></div></figure><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="a6bc" class="lr ls it nz b gy od oe l of og"># subset the data with a Boolean flag, capture male children<br/>isMale = family_data.loc[:,”gender”] == “M”</span><span id="99b7" class="lr ls it nz b gy oh oe l of og"># create just the male children df<br/>male_only = family_data[isMale].copy()<br/>male_only.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/d24ece391c923429c469ec0e1ad4222b.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*wT0cR1MgeGgjoRD9Cw2F5g.jpeg"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">在此df中仅捕获(M)名男性儿童</p></figure><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="0d69" class="lr ls it nz b gy od oe l of og">print(‘Number of rows: {}, Number of Males: {}’.format(len(family_data), len(male_only)))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/ab592f3f322113c4f5173d32c2eebe2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/1*I5aQv_etNTZb0Xv51t8NsQ.jpeg"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">数据很平衡！M = F</p></figure><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="ad99" class="lr ls it nz b gy od oe l of og"># create new df for new feature<br/>male_df = male_only.copy()</span><span id="cd84" class="lr ls it nz b gy oh oe l of og"># add in squares of mother and father heights<br/>male_df[‘father_sqr’] = male_df[‘father’] **2<br/>male_df[‘mother_sqr’] = male_df[‘mother’] **2</span><span id="5bd8" class="lr ls it nz b gy oh oe l of og"># drop columns family, gender, kids<br/>Drop = [“family”, “gender”, “kids”]<br/>for x in Drop:<br/> male_df = male_df.drop(x, axis=1)<br/> <br/># reset index<br/>male_df=male_df.reset_index(drop=True)<br/>male_df.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/dc1c8440ac41cd1705d4d5a0dfe30fc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*CwsIeu0V1dLgNcx_rP_cig.jpeg"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">成功添加新功能</p></figure><p id="5028" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">名词（noun的缩写）childHeight特性只是针对儿子的，我们之前只是复制并捕捉了儿子。我们稍后用女儿做PCR分析。</p><p id="a905" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">接下来，我们将z-score标准化数据。我们为什么要标准化数据？较大的值，比如father_sqr，会有较小的系数，较小的值会有较大的系数。这是不“公平”的。为了公平起见，我们对数据进行了缩放，使它们彼此相等，其中一种方法是z分数标准化，即从身高中减去儿童身高的平均值，然后除以儿童身高的标准差。这是z值:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="e771" class="lr ls it nz b gy od oe l of og"># scale all columns but the individual height (childHeight)<br/># normalization function for a column in a pandas df<br/>def scale(col):<br/> mean_col = np.mean(col)<br/> sd_col = np.std(col)<br/> std = (col — mean_col) / sd_col<br/> return std</span><span id="887e" class="lr ls it nz b gy oh oe l of og"># add scaled x to data frame.<br/>male_df[‘father’] = scale(male_df[‘father’])<br/>male_df[‘mother’] = scale(male_df[‘mother’])<br/>male_df[‘father_sqr’] = scale(male_df[‘father_sqr’])<br/>male_df[‘mother_sqr’] = scale(male_df[‘mother_sqr’])<br/>male_df.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/8dd5e5a8144a2c896bc251414a037ee7.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*xBLgvMI6ZnmSx9MaRb_C2A.jpeg"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">值根据列平均值的标准差进行缩放</p></figure><p id="8dd0" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">注意第一个父亲的身高是四。四个什么？离平均值四个标准差，这已经很高了。</p><h2 id="7797" class="lr ls it bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">计算具有所有功能的模型</h2><p id="3b78" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">下面的代码使用所有可用的功能计算所有男孩的身高。到目前为止，这是我展示的全部内容，但今天的特别之处在于，现在我们可以找到可以扔掉的功能。为什么我们要找到可以抛弃的特性？我们想让模型更简单。为什么我们要让模型更简单？事实证明，越简单的模型越好。</p><p id="6f2a" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">名词（noun的缩写）b:<code class="fe oo op oq nz b">ols_model</code>中的<code class="fe oo op oq nz b">+ 1</code>表示将会有一个偏移。<code class="fe oo op oq nz b">sklearn</code>这是自动的。总有截距，偏移量的另一个术语。因为一切都是按比例缩放的，所以在这种情况下，截距就是孩子的平均身高。</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="fbde" class="lr ls it nz b gy od oe l of og">ols_model = sm.ols(formula = ‘childHeight ~ father + mother + father_sqr + mother_sqr + 1’, data=male_df)</span><span id="2508" class="lr ls it nz b gy oh oe l of og">results = ols_model.fit()<br/>n_points = male_df.shape[0]<br/>y_output = male_df[‘childHeight’].values.reshape(n_points, 1)<br/>print(‘Intercept, Slopes : \n{}’.format(results.params))</span><span id="0da6" class="lr ls it nz b gy oh oe l of og"># print  hypothesis test stats<br/>print(‘Intercept t-value, Slope t-values: \n{}’.format(results.tvalues))<br/>print(‘\nHypothesis test summary for each coefficient if they differ from zero:’)<br/>print(results.pvalues)</span><span id="64e5" class="lr ls it nz b gy oh oe l of og">print(‘\nSSE, SST, SSR, and RMSE:’)<br/>mean_y = np.mean(y_output)<br/>sst = np.sum((y_output — mean_y)**2)<br/>sse = sst — results.ssr<br/>print(‘SSE: {}’.format(sse))<br/>print(‘SST: {}’.format(sst))<br/>print(‘SSR: {}’.format(results.ssr))<br/>print(‘RMSE: {}’.format(np.sqrt(results.mse_resid))) # ESHx</span><span id="39c8" class="lr ls it nz b gy oh oe l of og"># print summary stats<br/>print(results.summary())</span><span id="de4c" class="lr ls it nz b gy oh oe l of og"># plot a histogram of the residuals<br/>sns.distplot(results.resid, hist=True)<br/>plt.xlabel(‘Residual’)<br/>plt.ylabel(‘Frequency’)<br/>plt.title(‘Residual Histogram’)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/9a4ca347a442fb684137a9c562c9b7cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*QO_NFeGUiIt9X-yDs-OlYA.jpeg"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi os"><img src="../Images/7bcae1a508f2d53b43dd3900ed11e8b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4-QiUd1VOQ-s0zvpUYtI5g.jpeg"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">基线模型</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/bb0542d33689b31e1182db580e906b42.png" data-original-src="https://miro.medium.com/v2/resize:fit:844/format:webp/1*UELULmiMy8O4Zz-AYdbwCQ.jpeg"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">残差呈正态分布</p></figure><p id="df02" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">现在我们有了一个基线模型，我们能用正则化技术改进它吗？</p><h2 id="2595" class="lr ls it bd lt lu lv dn lw lx ly dp lz le ma mb mc li md me mf lm mg mh mi mj bi translated">应用逐步回归</h2><p id="db32" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">在前面，我定义了用于定义模型拟合度的指标:AIC</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi ni"><img src="../Images/05fadcddbb90b0e22537013093b84702.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9Sk_TbNABYgz2ZLO"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">AIC:模型对数似然调整的参数数量</p></figure><ul class=""><li id="1835" class="mp mq it kx b ky kz lb lc le mr li ms lm mt lq mu mv mw mx bi translated">k:估计参数的数量</li><li id="73ad" class="mp mq it kx b ky my lb mz le na li nb lm nc lq mu mv mw mx bi translated">可能性</li></ul><p id="f73d" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">我们可以使用后向特征选择对该函数应用逐步回归:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="d75c" class="lr ls it nz b gy od oe l of og">def backward_selected(data, response):<br/> “””Linear model designed by backward selection. Feature selection based on AIC</span><span id="85fa" class="lr ls it nz b gy oh oe l of og">Parameters:<br/> — — — — — -<br/> data : pandas dfwith all possible predictors and response</span><span id="7599" class="lr ls it nz b gy oh oe l of og">response: string, name of response column in data</span><span id="2e53" class="lr ls it nz b gy oh oe l of og">Returns:<br/> — — — — <br/> model: an “optimal” fitted statsmodels linear model<br/> with an intercept<br/> selected by backward selection<br/> “””<br/> # begin with all factors and intercept<br/> possible_factors = set(data.columns)<br/> possible_factors.remove(response)<br/> formula = “{} ~ {} + 1”.format(response, ‘ + ‘.join(possible_factors))<br/> best_aic = sm.ols(formula, data).fit().aic<br/> current_aic = best_aic<br/> <br/> # create a non-empty set of columns that will be labeled as “to remove and try”<br/> to_try_remove = possible_factors<br/> <br/> # check which features remain<br/> while to_try_remove and current_aic == best_aic:<br/> aic_candidates = []<br/> for candidate in to_try_remove:<br/> <br/> columns = possible_factors — set([candidate])<br/> # removing the candidate column<br/> formula = “{} ~ {} + 1”.format(response, ‘ + ‘.join(columns))<br/> # print AIC<br/> aic = sm.ols(formula, data).fit().aic<br/> <br/> # append tuple of the form (aic, response)<br/> aic_candidates.append((aic, candidate))<br/> <br/> # sort all the pairs by the first entry of tuple <br/> aic_candidates.sort()<br/> # change sort/pop order!<br/> best_new_aic, best_candidate = aic_candidates.pop(0)<br/> <br/> # check if we have something better:<br/> if best_new_aic &lt; current_aic:<br/> # Remove the best candidate’s name from possible_factors<br/> <br/> possible_factors.remove(best_candidate)<br/> current_aic = best_new_aic<br/><br/> <br/> # repeat the process with all the remaining candidate columns</span><span id="02e0" class="lr ls it nz b gy oh oe l of og"># final formula<br/> formula = “{} ~ {} + 1”.format(response, ‘ + ‘.join(possible_factors))<br/> # model object<br/> model = sm.ols(formula, data).fit()<br/> return model</span><span id="e00a" class="lr ls it nz b gy oh oe l of og">backwards_model = backward_selected(male_df, ‘childHeight’)</span><span id="833b" class="lr ls it nz b gy oh oe l of og">print(backwards_model.model.formula)</span><span id="73df" class="lr ls it nz b gy oh oe l of og">print(‘Adjusted R-Squared: {}’.format(backwards_model.rsquared_adj))<br/>print(‘AIC: {}’.format(backwards_model.aic))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/0db46e183cf587a72729491175b89289.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*78yKxli-f6-JaxErB1A0Xw.jpeg"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">向后逐步回归，AIC较好</p></figure><p id="56e6" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">反向选择的公式<code class="fe oo op oq nz b">childHeight ~ father + mother + mother_sqr + 1</code>现在被输入到标准<code class="fe oo op oq nz b">sm</code>模板中:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="eff6" class="lr ls it nz b gy od oe l of og">ols_model_forward = sm.ols(formula = ‘childHeight ~ father + mother + mother_sqr + 1’, data=male_df)<br/>results = ols_model_forward.fit()<br/>n_points = male_df.shape[0]<br/>y_output = male_df[‘childHeight’].values.reshape(n_points, 1)</span><span id="6749" class="lr ls it nz b gy oh oe l of og"># slope (m) and y-intercept (b)<br/>print(‘Intercept, Slopes : \n{}’.format(results.params))</span><span id="780a" class="lr ls it nz b gy oh oe l of og"># t-values (hypothesis test statistics) <br/>print(‘Intercept t-value, Slope t-values: \n{}’.format(results.tvalues))</span><span id="f18b" class="lr ls it nz b gy oh oe l of og"># p-values for above t-value statistics<br/>print(‘\nHypothesis test summary for each coefficient if they differ from zero:’)<br/>print(results.pvalues)</span><span id="8341" class="lr ls it nz b gy oh oe l of og">print(‘\nSSE, SST, SSR, and RMSE:’)<br/>mean_y = np.mean(y_output)<br/>sst = np.sum((y_output — mean_y)**2)<br/>sse = sst — results.ssr<br/>print(‘SSE: {}’.format(sse))<br/>print(‘SST: {}’.format(sst))<br/>print(‘SSR: {}’.format(results.ssr))<br/>print(‘RMSE: {}’.format(np.sqrt(results.mse_resid))) # ESHx<br/>print(results.summary())</span><span id="aefa" class="lr ls it nz b gy oh oe l of og"># plot a histogram of the residuals:<br/>sns.distplot(results.resid, hist=True)<br/>plt.xlabel(‘Residual’)<br/>plt.ylabel(‘Frequency’)<br/>plt.title(‘Residual Histogram’)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/8a1e17e5fd9336dd552aa3203c9afb88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*UAIFfIIwmqxKeBs4VmnUiw.jpeg"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">部分斜率是系数</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi ow"><img src="../Images/01a78d3a89d11456e23990137f6a0683.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j01CKgY6kYF_pj6k9M40AA.jpeg"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">其余的更好</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/835f4c3cf9cfff4965c5e4a12ce6348c.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*gW9ye8PVq7n1GpgENzJ_nQ.jpeg"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">残差呈正态分布</p></figure><p id="7250" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">名词（noun的缩写）逐步回归似乎是一种简单的特征选择方法；请注意<strong class="kx iu">逐步回归的伸缩性不好</strong>。与任何多重比较方法一样，逐步回归法有很高的假阳性结果概率。在这种情况下，由于低p值或AIC，不应删除的要素可能不会被删除。</p><h1 id="4647" class="nn ls it bd lt no np nq lw nr ns nt lz jz nu ka mc kc nv kd mf kf nw kg mi nx bi translated">主成分回归:Python示例</h1><p id="9982" class="pw-post-body-paragraph kv kw it kx b ky mk ju la lb ml jx ld le mm lg lh li mn lk ll lm mo lo lp lq im bi translated">为了简洁起见，我省略了PCR背后的大量线性代数。诀窍就是做PCA，主成分分析。PCA将帮助你确定哪个主成分是最好的。这是通过创建初始变量的线性组合并创建一个解释最大方差的新组合来实现的。然后另一个解释了稍微少一点的差异，等等。</p><p id="cce8" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">从模型的角度来看，我们已经把它正规化了。没有任何信息被丢弃，每台电脑都包含所有功能的线性组合。但是模型看到了新的特性，使得确定哪些特性应该被丢弃变得容易。</p><p id="fd17" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">名词（noun的缩写）b:这个靠的是幕后的SVD。</p><p id="9752" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">PCA中的变换创建线性独立的特征，它们之间具有最佳的独立性。具有少量差异的PC可能对我们的模型不重要，因此我们通过移除差异极小的特征来简化我们的模型。</p><p id="f55c" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">首先，在将PCR应用于高尔顿数据集之前，我将对其进行演示:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="1014" class="lr ls it nz b gy od oe l of og">from sklearn.decomposition import PCA<br/>from sklearn.preprocessing import StandardScaler</span><span id="288b" class="lr ls it nz b gy oh oe l of og">np.random.seed(21)</span><span id="1fd3" class="lr ls it nz b gy oh oe l of og"># right-multiply a 2x2 matrix to a 2x100 matrix to transform the points<br/>X = np.dot(np.random.randn(100, 2), np.random.rand(2, 2))<br/># scale data<br/>X = StandardScaler().fit_transform(X)</span><span id="6188" class="lr ls it nz b gy oh oe l of og">plt.scatter(X[:, 0], X[:, 1])<br/>plt.grid()<br/>plt.xlabel(‘x’)<br/>plt.ylabel(‘y’)<br/>plt.title(‘Generated x-y data with dependence’)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/a771c2817647ffce4f398d031ab89a61.png" data-original-src="https://miro.medium.com/v2/resize:fit:814/format:webp/1*Rukv8ccwG-QRdhRJmJsO7A.jpeg"/></div></figure><p id="13c0" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">现在，我们将创建变量的线性组合来创建PC1和PC2，即主成分1和主成分2。PC1的方差最大。请记住，在PCA中，输入特征与创建的主成分之间存在1:1的映射</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="b179" class="lr ls it nz b gy od oe l of og">pca = PCA(n_components=2)<br/>pca_result = pca.fit_transform(X)<br/>pca_df = pd.DataFrame(data = pca_result , columns = [‘pc1’, ‘pc2’])</span><span id="6656" class="lr ls it nz b gy oh oe l of og">print(pca_df.head())<br/>print(pca_df.shape)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/6106ac27d08a44223743280d41a6512d.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*y8t5KMM15XdK3TWP695P6A.jpeg"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">100双</p></figure><p id="620f" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">绘制输出</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="8207" class="lr ls it nz b gy od oe l of og">plt.axis(‘equal’) <br/>plt.scatter(pca_df.loc[:, ‘pc1’], pca_df.loc[:, ‘pc2’])<br/>plt.ylabel(‘Component 2’)<br/>plt.xlabel(‘Component 1’)<br/>plt.title(‘Two Dimensional PCA’)<br/>plt.grid()<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/5549b21d1c29a2912e4d0f9539626e66.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*6c6Hjum490bBcVRyIxooPw.jpeg"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">数据在我们新的坐标系中重新绘制。</p></figure><p id="dccf" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">PC1和PC2是原始要素的线性组合，在我们的新坐标系(PC1和PC2)中附加了一些权重，而不是x和y。</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="34f3" class="lr ls it nz b gy od oe l of og">print(pca.explained_variance_)<br/>print(pca.components_)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi pa"><img src="../Images/3d4ae5096c2597102628fde8d9b025c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*hZKjf5zsYqdGM1mYNz1MiA.jpeg"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">这些电脑解释了100%的差异！</p></figure><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="cfbe" class="lr ls it nz b gy od oe l of og">exp_var = pca.explained_variance_<br/>components = pca.components_</span><span id="0480" class="lr ls it nz b gy oh oe l of og"># plot vector<br/># = plot scale * component direction * component length<br/>v1 = 2 * components[0] * np.sqrt(exp_var[0])<br/>v2 = 2 * components[1] * np.sqrt(exp_var[1])<br/>c = (0, 0) # Center is at 0,0 because of our standardization</span><span id="121e" class="lr ls it nz b gy oh oe l of og">plt.scatter(X[:, 0], X[:, 1], alpha=0.5)<br/>plt.annotate(‘’, c + v1, c, arrowprops={‘arrowstyle’: ‘-&gt;’, ‘shrinkA’: 0,<br/> ‘shrinkB’: 0, ‘linewidth’: 3})<br/>plt.annotate(‘’, c + v2, c, arrowprops={‘arrowstyle’: ‘-&gt;’, ‘shrinkA’: 0,<br/> ‘shrinkB’: 0, ‘linewidth’: 3})<br/>plt.axis(‘equal’)<br/>plt.grid()<br/>plt.title(‘F-vector functions (Principal Components)’)<br/>plt.xlabel(‘x’)<br/>plt.ylabel(‘y’)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/dd4a96a9cc23ed6b215bfb7848dffe8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:792/format:webp/1*0MhrlWw9L9-Qpg_FrVyDkQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">矢量在新的坐标系中确实是垂直的</p></figure><p id="bd3f" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">根据PCR，我们创建了两个线性独立的特征，如上图所示。因为PC1比PC2解释了更多的差异，这个模型可以通过去掉PC2来简化。</p><p id="1e17" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">这可以扩展到更高维的数据集。通过计算主成分，我们消除了数据中的多重共线性问题。</p><p id="5720" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">现在，让我们在高尔顿数据集上使用PCR看看女儿。由于df中有四个特征，因此PCA之后将有四个主要成分。</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="b1ec" class="lr ls it nz b gy od oe l of og"># subset the data with a Boolean flag to capture daughters<br/>isFemale = family_data.loc[:,”gender”] == “F”</span><span id="fe16" class="lr ls it nz b gy oh oe l of og"># create just the female df<br/>female_data = family_data[isFemale].copy()</span><span id="9929" class="lr ls it nz b gy oh oe l of og"># create new df for new feature set<br/>female_df = female_data.copy()</span><span id="26ad" class="lr ls it nz b gy oh oe l of og"># feature engineer squares of mother and father heights<br/>female_df[‘father_sqr’] = female_df[‘father’] **2<br/>female_df[‘mother_sqr’] = female_df[‘mother’] **2</span><span id="4074" class="lr ls it nz b gy oh oe l of og"># drop columns for family, gender, kids<br/>Drop= [“family”, “gender”, “kids”]<br/>for x in Drop:<br/> female_df = female_df.drop(x, axis=1)<br/> <br/># reset the index<br/>female_df=female_df.reset_index(drop=True)</span><span id="e1ea" class="lr ls it nz b gy oh oe l of og"># add scaled x to df<br/>female_df[‘father’] = scale(female_df[‘father’])<br/>female_df[‘mother’] = scale(female_df[‘mother’])<br/>female_df[‘father_sqr’] = scale(female_df[‘father_sqr’])<br/>female_df[‘mother_sqr’] = scale(female_df[‘mother_sqr’])<br/>female_df.head()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/98cffc2d0dd128f5358a216225abe6db.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*2N8QnNoNjN2-QmIo2-jf9Q.jpeg"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">仅子数据集看起来很好</p></figure><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="f0ee" class="lr ls it nz b gy od oe l of og"># calculate all the principal components (4)<br/>X = female_df[[‘father’, ‘mother’, ‘father_sqr’, ‘mother_sqr’]].values<br/>y = female_df[‘childHeight’]<br/>pca = PCA()<br/>pca_result = pca.fit_transform(X)<br/>pca_df = pd.DataFrame(data = pca_result, columns=[‘pc1’, ‘pc2’, ‘pc3’, ‘pc4’])</span><span id="2da5" class="lr ls it nz b gy oh oe l of og"># our data projected onto the four principal components.<br/>print(pca_df.head())<br/>print(pca_df.shape)</span><span id="d0a3" class="lr ls it nz b gy oh oe l of og">pca_df[‘childHeight’] = female_df[‘childHeight’]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/de094f3635198ca901f09bb759d8716c.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*t_kQqgEyttDeaJlHI-rBEQ.png"/></div></figure><p id="2e2b" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">方差图:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="87ee" class="lr ls it nz b gy od oe l of og">plt.plot([i + 1 for i in range(4)], pca.explained_variance_)<br/>plt.title(‘Scree plot for PCA decoposition’)<br/>plt.xlabel(‘Component’)<br/>plt.ylabel(‘Variance explained’)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/730683ed152ba192985f3999b8c28cd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*tcqLGOPPznVR1buBWSkmDQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">前两个部分解释了最大的差异</p></figure><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="d8d4" class="lr ls it nz b gy od oe l of og">pca.explained_variance_</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/870e6fcd694b97427e93aa0edf189d7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1108/format:webp/1*5-8WJl37_UaKIzWsfnzfZg.jpeg"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">解释了四个PCs的差异</p></figure><p id="b585" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">既然我们已经将所有的数据都投影到了4个主成分上，我们可以看看解释的方差了。前两个PC解释了最大的方差，因此我们可能会删除PC3和PC4，因为它们解释的方差如此之低。</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="1117" class="lr ls it nz b gy od oe l of og">pcr_model = sm.ols(formula = ‘childHeight ~ pc1 + pc2 + pc3 + pc4’, data=pca_df)</span><span id="c51d" class="lr ls it nz b gy oh oe l of og">results = pcr_model.fit()<br/>n_points = pca_df.shape[0]<br/>y_output = pca_df[‘childHeight’].values.reshape(n_points, 1)<br/>print(results.summary())</span><span id="f125" class="lr ls it nz b gy oh oe l of og"># plot a histogram of the residuals<br/>sns.distplot(results.resid, hist=True)<br/>plt.xlabel(‘Residual’)<br/>plt.ylabel(‘Frequency’)<br/>plt.title(‘Residual Histogram’)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi pg"><img src="../Images/d4b299d1a303492fd5a57a77803c469b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0PpRvq1D7DpGClmmWMrxig.jpeg"/></div></div></figure><p id="8b9c" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在这里，你可以看到，最后两个主要组成部分并不需要一个重要的拟合。你也可能已经注意到，从每个组件的“解释方差”中-以及最后两个组件如何解释近4个数量级的方差。</p><p id="faef" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">现在，我们可以只使用两个分量来尝试PCA回归；解释大部分差异的因素包括:</p><pre class="kj kk kl km gt ny nz oa ob aw oc bi"><span id="d0ea" class="lr ls it nz b gy od oe l of og">pcr_model = sm.ols(formula = ‘childHeight ~ pc1 + pc2’, data=pca_df)</span><span id="ffc0" class="lr ls it nz b gy oh oe l of og">results = pcr_model.fit()<br/>n_points = pca_df.shape[0]<br/>y_output = pca_df[‘childHeight’].values.reshape(n_points, 1)<br/>print(results.summary())</span><span id="f758" class="lr ls it nz b gy oh oe l of og"># plot a histogram of the residuals<br/>sns.distplot(results.resid, hist=True)<br/>plt.xlabel(‘Residual’)<br/>plt.ylabel(‘Frequency’)<br/>plt.title(‘Residual Histogram’)<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi ph"><img src="../Images/81ed9335cf6c7b5d8b303d6b2d9b0267.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EW1KKoQoKUG_LEhYirxocA.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">请注意，所有模型系数现在都很重要</p></figure><p id="a061" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">PC3和PC4的p值较大，因此不显著。此外，他们各自的95%置信区间跨越零。因此，他们没有帮助模型解释女儿的身高，我们怀疑他们解释的低方差。如果我们没有执行PCR，我们很可能不会丢弃任何初始特征(父、母、父_sqr、母_sqr)。</p><p id="a201" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">如果您注意到R在改进的模型上并没有更好，请记住我们犯了一个非犹太行为:我们在训练数据上测试了我们的模型。一个大禁忌。几乎可以肯定的是，如果我们把以前看不到的测试数据输入到最后一个模型，只有两台电脑的模型，R就会增加。这是因为这是一个更简单的模型，不容易过度拟合。如果数据是训练/调整/测试分割的，那么模型的改进将是可论证的。</p><p id="48d5" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">降维起到了预期的效果！调整后的R和AIC是该模型的最低值。正则化帮助我们创建了更好、更简单、更通用的模型。</p><p id="99cc" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">如果你想知道为什么父母的身高不是孩子身高的好指标，这就是“回归平均值”这个短语的来源。与18世纪的普遍看法相反，高尔顿的家庭分析表明，高个子父母的孩子往往身高回归到人口平均水平。</p><p id="1d24" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">接下来，我将用<a class="ae ku" href="https://james-andrew.medium.com/ridge-lasso-and-elasticnet-regression-b1f9c00ea3a3" rel="noopener"> Ridge、LASSO和Elasticnet regressions </a>来讨论正则化！</p><p id="baed" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated">在<a class="ae ku" href="https://www.linkedin.com/in/james-a-w-godwin/" rel="noopener ugc nofollow" target="_blank"> Linkedin </a>上找到我</p><p id="a999" class="pw-post-body-paragraph kv kw it kx b ky kz ju la lb lc jx ld le lf lg lh li lj lk ll lm ln lo lp lq im bi translated"><em class="pi">物理学家兼数据科学家——可用于新机遇| SaaS |体育|初创企业|扩大规模</em></p></div></div>    
</body>
</html>