<html>
<head>
<title>Reinforcement Learning in Python with Flappy Bird</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中带Flappy Bird的强化学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/reinforcement-learning-in-python-with-flappy-bird-37eb01a4e786?source=collection_archive---------12-----------------------#2021-05-20">https://towardsdatascience.com/reinforcement-learning-in-python-with-flappy-bird-37eb01a4e786?source=collection_archive---------12-----------------------#2021-05-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7411" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Q-learning和RL考虑事项的演练</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/46136f8b6335392b59086f0f1ed91924.png" data-original-src="https://miro.medium.com/v2/resize:fit:566/0*QVsfpDQDKnqnLPAB.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">活着，活着。(<em class="ku">图片来自作者，</em>资产来自<a class="ae kv" href="https://github.com/sourabhv/FlapPyBird" rel="noopener ugc nofollow" target="_blank"> FlapPyBird </a>)</p></figure><h1 id="9737" class="kw kx it bd ky kz la lb lc ld le lf lg jz lh ka li kc lj kd lk kf ll kg lm ln bi translated">介绍</h1><p id="9931" class="pw-post-body-paragraph lo lp it lq b lr ls ju lt lu lv jx lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">2014年，热门游戏《沉睡者》席卷了移动游戏世界。它已经在PyGame 中实现，但最有趣的是它非常适合强化学习。代理(鸟)只能执行2个动作(拍打或什么都不做)，并且只对1个环境变量(即将到来的管道)感兴趣。这个问题非常简单，非常适合从头开始用Python实现强化学习。</p><p id="d94a" class="pw-post-body-paragraph lo lp it lq b lr mk ju lt lu ml jx lw lx mm lz ma mb mn md me mf mo mh mi mj im bi translated"><strong class="lq iu">本文对该项目进行了更高层次的概述。代码和结果可以在GitHub </strong> <a class="ae kv" href="https://github.com/anthonyli358/FlapPyBird-Reinforcement-Learning" rel="noopener ugc nofollow" target="_blank"> <strong class="lq iu">这里</strong> </a> <strong class="lq iu">找到。</strong></p></div><div class="ab cl mp mq hx mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="im in io ip iq"><h1 id="5b82" class="kw kx it bd ky kz mw lb lc ld mx lf lg jz my ka li kc mz kd lk kf na kg lm ln bi translated">q学习</h1><p id="dcbd" class="pw-post-body-paragraph lo lp it lq b lr ls ju lt lu lv jx lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">Q-learning是一种无模型强化学习算法，通常用于学习给定特定状态下代理采取的最佳行动。当代理人在特定状态下采取行动时，它会收到奖励(或惩罚)，代理人的目标是使其总奖励最大化，因此在采取行动时，它还必须考虑潜在的未来奖励。对于Flappy Bird，代理是其可能的动作是不做任何事情或拍打翅膀的鸟，并且其当前状态由以下各项定义:</p><ul class=""><li id="c659" class="nb nc it lq b lr mk lu ml lx nd mb ne mf nf mj ng nh ni nj bi translated">与即将到来的管道的距离</li><li id="d050" class="nb nc it lq b lr no lu np lx nq mb nr mf ns mj ng nh ni nj bi translated"><code class="fe nk nl nm nn b">y</code>到即将到来的底部管道的距离</li><li id="5ce8" class="nb nc it lq b lr no lu np lx nq mb nr mf ns mj ng nh ni nj bi translated">鸟的当前y轴速度<code class="fe nk nl nm nn b">v</code></li><li id="c1e8" class="nb nc it lq b lr no lu np lx nq mb nr mf ns mj ng nh ni nj bi translated">顶部和底部即将到来的管道之间的距离<code class="fe nk nl nm nn b">y1</code></li></ul><p id="ba57" class="pw-post-body-paragraph lo lp it lq b lr mk ju lt lu ml jx lw lx mm lz ma mb mn md me mf mo mh mi mj im bi translated">第一次达到一个新的状态时，一个Q值被初始化(0就可以了)，代理将执行默认的动作并获得奖励。如果以前遇到过一个状态，代理将执行具有最高Q值的动作(奖励最多的动作)。然后更新Q值，并将该值插入状态×动作Q表中的相应单元。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/fb60a6950769aad4d1cf2c4d34f36cd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*DV9S3zEyS9SEk_yzCirZUg.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">当代理对不同的状态采取不同的动作时，Q表被更新。下一次代理遇到状态1时，它将摆动，对于状态2，它将什么也不做。(<em class="ku">图片作者)</em></p></figure><p id="8a7b" class="pw-post-body-paragraph lo lp it lq b lr mk ju lt lu ml jx lw lx mm lz ma mb mn md me mf mo mh mi mj im bi translated">Q值根据几个参数更新:</p><pre class="kj kk kl km gt nu nn nv nw aw nx bi"><span id="3a34" class="ny kx it nn b gy nz oa l ob oc"># Update q values in a 2d array<br/>Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[new_state, :]) — Q[state, action])</span></pre><ul class=""><li id="3ff9" class="nb nc it lq b lr mk lu ml lx nd mb ne mf nf mj ng nh ni nj bi translated">αα是步长，它决定了旧信息在多大程度上被遗忘以利于新信息。</li><li id="ecce" class="nb nc it lq b lr no lu np lx nq mb nr mf ns mj ng nh ni nj bi translated">γγ决定了未来奖励的重要性。例如，γ=0意味着代理将只考虑其当前报酬。</li><li id="e97a" class="nb nc it lq b lr no lu np lx nq mb nr mf ns mj ng nh ni nj bi translated">Q[状态，动作]是当前的Q值。</li></ul></div><div class="ab cl mp mq hx mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="im in io ip iq"><h1 id="58fa" class="kw kx it bd ky kz mw lb lc ld mx lf lg jz my ka li kc mz kd lk kf na kg lm ln bi translated">结果</h1><h2 id="0e2d" class="ny kx it bd ky od oe dn lc of og dp lg lx oh oi li mb oj ok lk mf ol om lm on bi translated">初步训练</h2><p id="7e5f" class="pw-post-body-paragraph lo lp it lq b lr ls ju lt lu lv jx lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">奖励函数被定义为死亡罚-1000分，否则罚0分，这样代理的关注点就是得到尽可能高的分数。这确保了奖励功能在每一集之后都有足够的影响，而在实现中，分数增加奖励+1意味着惩罚几乎没有影响。</p><p id="b70d" class="pw-post-body-paragraph lo lp it lq b lr mk ju lt lu ml jx lw lx mm lz ma mb mn md me mf mo mh mi mj im bi translated">在α=0.7和γ=0.95的情况下，代理最初在没有任何探索的情况下被训练大约10，000集，并且几乎能够达到100万的分数。已经记录了y轴，以便可以更清楚地看到分数的滚动增加。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="op oq di or bf os"><div class="gh gi oo"><img src="../Images/2bf75e3dd40badea90ea39a31196e6d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*NNL3bJiTN5sn3Zl5.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">特工在学习生存。(<em class="ku">图片作者)</em></p></figure><h2 id="0674" class="ny kx it bd ky od oe dn lc of og dp lg lx oh oi li mb oj ok lk mf ol om lm on bi translated">经验回放:灾难性遗忘</h2><p id="644e" class="pw-post-body-paragraph lo lp it lq b lr ls ju lt lu lv jx lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">尽管代理在初始训练后表现良好，但要进一步提高却需要更长的时间。这是因为代理需要很长时间才能到达死亡的场景，即使这样它也只能从死亡中学习一次。通过引入经验回放，代理可以尝试多次困难场景，直到克服困难或陷入恢复循环(此处设置为100次尝试)。在通过困难场景时，在失败时，它从开始重新开始，以避免连续增加所达到的最大分数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="op oq di or bf os"><div class="gh gi ot"><img src="../Images/aa15129706e4c483703ab04c44eab96a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ukq1tCy6iCkZdI2q.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">加载Q表，从初始训练开始继续，但进行经验回放。(<em class="ku">图片作者)</em></p></figure><p id="c92e" class="pw-post-body-paragraph lo lp it lq b lr mk ju lt lu ml jx lw lx mm lz ma mb mn md me mf mo mh mi mj im bi translated">代理的性能有了初步的提高，因为它能够从更困难的场景中学习，但是它的性能很快下降到以前的最大值以下。</p><p id="c6b6" class="pw-post-body-paragraph lo lp it lq b lr mk ju lt lu ml jx lw lx mm lz ma mb mn md me mf mo mh mi mj im bi translated">这被称为灾难性遗忘，它通常会导致代理性能的振荡，因为它会忘记和重新学习要采取的最佳行动。通过从相同的场景失败中反复学习，代理过度拟合，并且忘记了先前可概括的Q值。</p><h2 id="3267" class="ny kx it bd ky od oe dn lc of og dp lg lx oh oi li mb oj ok lk mf ol om lm on bi translated">体验回放:回放缓冲区</h2><p id="0e0d" class="pw-post-body-paragraph lo lp it lq b lr ls ju lt lu lv jx lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">为了克服灾难性遗忘，在训练智能体时会添加阿尔法衰减，帮助它保留早期学习的信息，同时仍然从更少的场景中学习。此外，被视为陷入循环的尝试次数减少到50次，并且在体验重放期间，会创建一个“重放缓冲区”，其中包含所有采取的操作。然后以小批量的方式更新Q表，一旦体验重放完成，就对5次尝试进行采样。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="op oq di or bf os"><div class="gh gi ou"><img src="../Images/4405ef02c36fcb37b10aa9e3ace88725.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*HFHSR4Y48SOMfent.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">这只鸟的记忆力更好。(<em class="ku">图片作者)</em></p></figure><p id="f058" class="pw-post-body-paragraph lo lp it lq b lr mk ju lt lu ml jx lw lx mm lz ma mb mn md me mf mo mh mi mj im bi translated">这个代理差不多能达到一千万的评分。虽然随着训练持续超过10，000集，观察到表现下降，但它能够从这种最初的遗忘中恢复。虽然没有花费更多的时间来训练这个代理，但是可以预期代理的性能会随着它忘记和重新学习要采取的最佳动作而波动。随着alpha继续衰减，这将最终使代理在其最大值附近保持稳定。</p><h2 id="bd07" class="ny kx it bd ky od oe dn lc of og dp lg lx oh oi li mb oj ok lk mf ol om lm on bi translated">ε贪婪政策</h2><p id="b094" class="pw-post-body-paragraph lo lp it lq b lr ls ju lt lu lv jx lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">我们现在尝试新训练的代理，引入探索率ε，这提供了探索随机动作的机会，直到它在10，000集后从0.1衰减到0，以及α衰减，它在20，000集后从0.7衰减到0.1。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="op oq di or bf os"><div class="gh gi ov"><img src="../Images/654f29da8fd691ed73732cbaf3542bcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ei7xBJ5SOqP0YRYd.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">经典的sigmoid增长曲线。(<em class="ku">图片作者)</em></p></figure><p id="777e" class="pw-post-body-paragraph lo lp it lq b lr mk ju lt lu ml jx lw lx mm lz ma mb mn md me mf mo mh mi mj im bi translated">由于α和ε衰减，该代理比初始训练中学习得更慢，但是一旦它达到略低于100万的最佳性能，就稳定得多。这个最高分数比我们没有探索时的分数要低，可能是因为alpha衰减提高了稳定性，但代价是代理学习得更慢。</p></div><div class="ab cl mp mq hx mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="im in io ip iq"><h1 id="4e84" class="kw kx it bd ky kz mw lb lc ld mx lf lg jz my ka li kc mz kd lk kf na kg lm ln bi translated">最终验证和想法</h1><p id="a7d1" class="pw-post-body-paragraph lo lp it lq b lr ls ju lt lu lv jx lw lx ly lz ma mb mc md me mf mg mh mi mj im bi translated">从上面我们可以看到，最终表现最好的代理是用经验重放和重放缓冲区训练出来的。代理性能经过25次运行验证。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="op oq di or bf os"><div class="gh gi ot"><img src="../Images/12b82c4408cb2d599d7e9b0ef4ef9aca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*NzG-JK_VEiqWa4gu.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">🐤那是个不错的高分！(<em class="ku">图片作者)</em></p></figure><p id="9bc8" class="pw-post-body-paragraph lo lp it lq b lr mk ju lt lu ml jx lw lx mm lz ma mb mn md me mf mo mh mi mj im bi translated">代理表现不错，持续过10万，最高分达到500万以上。它能够通过大多数情况，但当更困难的情况出现时，它不能永远活着，这意味着它有时会很早就死去。</p><ul class=""><li id="2846" class="nb nc it lq b lr mk lu ml lx nd mb ne mf nf mj ng nh ni nj bi translated">稳定性:变异系数(标准差/平均值)为0.967。这是预料之中的，因为在随机环境中，只有当智能体能够克服所有情况并且永不死亡时，它才能完全稳定。</li><li id="7def" class="nb nc it lq b lr no lu np lx nq mb nr mf ns mj ng nh ni nj bi translated">平均分:平均分2001434。这是代理人始终能够达到的一个很高的分数，超过了任何人类游戏。</li><li id="52f5" class="nb nc it lq b lr no lu np lx nq mb nr mf ns mj ng nh ni nj bi translated">最高分:25轮最高分是6，720，279。这是一个接近默认最大训练值1000万的高分，再次超过任何人类游戏。</li></ul><h2 id="ec43" class="ny kx it bd ky od oe dn lc of og dp lg lx oh oi li mb oj ok lk mf ol om lm on bi translated">未来的工作</h2><ul class=""><li id="3ad1" class="nb nc it lq b lr ls lu lv lx ow mb ox mf oy mj ng nh ni nj bi translated">更长的训练时间，表现最好的代理总共训练了15个小时，只达到了10，674集</li><li id="b080" class="nb nc it lq b lr no lu np lx nq mb nr mf ns mj ng nh ni nj bi translated">实施<a class="ae kv" href="https://arxiv.org/abs/1511.05952" rel="noopener ugc nofollow" target="_blank">优先体验回放</a></li><li id="3673" class="nb nc it lq b lr no lu np lx nq mb nr mf ns mj ng nh ni nj bi translated">训练一个在飞鸟环境中永不死亡的特工</li></ul></div><div class="ab cl mp mq hx mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="im in io ip iq"><h1 id="d58f" class="kw kx it bd ky kz mw lb lc ld mx lf lg jz my ka li kc mz kd lk kf na kg lm ln bi translated">参考</h1><ul class=""><li id="c64e" class="nb nc it lq b lr ls lu lv lx ow mb ox mf oy mj ng nh ni nj bi translated">许宏涛，用强化学习训练一只永远不会死的flappy bird(2020)，<a class="ae kv" rel="noopener" target="_blank" href="/use-reinforcement-learning-to-train-a-flappy-bird-never-to-die-35b9625aaecc">https://towards data science . com/Use-reinforcement-learning-to-train-a-flappy-bird-NEVER-to-die-35b 9625 aa ECC</a></li></ul></div></div>    
</body>
</html>