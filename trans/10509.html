<html>
<head>
<title>Maximum Likelihood Estimation (MLE) and the Fisher Information</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">最大似然估计(MLE)和Fisher信息</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/maximum-likelihood-estimation-mle-and-the-fisher-information-1dd53faa369?source=collection_archive---------3-----------------------#2021-10-07">https://towardsdatascience.com/maximum-likelihood-estimation-mle-and-the-fisher-information-1dd53faa369?source=collection_archive---------3-----------------------#2021-10-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4705" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">极大似然估计置信区间的构造</h2></div><p id="c773" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">媒体上关于MLE的文章数量巨大，从理论到实现都使用不同的语言。关于费希尔的资料，也有不少教程。然而，费雪信息和最大似然估计之间的联系很少被提及。因此，我想就这个话题发表一篇文章。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi le"><img src="../Images/d6ebb59234257ed14bca6155b26f93a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XQX55F5kl5p1RGYI-A8tVA.jpeg"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">图片来自<a class="ae lu" href="https://unsplash.com/photos/RSsqjpezn6o" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><h1 id="a0f6" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">最大似然估计</h1><p id="a88d" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">MLE的主要思想很简单。它回答了这个问题:</p><blockquote class="ms"><p id="c84f" class="mt mu it bd mv mw mx my mz na nb ld dk translated">W <!-- -->什么参数最有可能使模型产生我们现有的样本？</p></blockquote><p id="d2e1" class="pw-post-body-paragraph ki kj it kk b kl nc ju kn ko nd jx kq kr ne kt ku kv nf kx ky kz ng lb lc ld im bi translated">形式上，我们考虑随机变量序列X₁，…，Xₙ，这样它们是相同的独立分布(iid)随机变量。它们都来自同一个分布f(x；θ)，其中θ是一个参数向量(我们用这个大θ来表示一个参数向量，这意味着θ∈ℝᵖ，如果模型只有一个参数，我们将在本文中使用θ来表示)和θ∈ω，其中ω是参数的样本空间。这听起来很奇怪，但是在MLE中，我们从样本空间中选取参数，并且我们想要最可能的一个。我们如何做到这一点？我们最大化一个似然函数，其定义为</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/15d67331071e2b5f91deab5d2f986655.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*bGyNtH468YfT0jL18fYBgg.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">等式1.1似然函数</p></figure><p id="3c10" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">每个事件的概率可以相乘，因为我们知道这些观察是独立的。在等式1.1中，每个Aⱼ是一个事件，它可以是一个区间或包含单个点的集合。当截尾在某一特定值u时，观测事件Aⱼ为区间[u，∞]。在没有删截的情况下，Aⱼ包含一个单点，并且给定参数θ，该观察的可能性为</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/d11eed1c39130c71e58ac457bc8fd97e.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*0-qDqaaw0ev-cHc6OVgaJQ.png"/></div></figure><p id="f838" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不考虑删失，似然函数可以写成</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/e90778c1edd33acba7a8d87fb5309622.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/1*PGoXWNn_PeatCXJlDjrsfw.png"/></div></figure><p id="91f1" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">也就是我们平时在课本上看到的。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/8452a6bdd2b51c1a48cad681745c521b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1158/format:webp/1*Sd9G1218Z_hug4jyyslSKA.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">作者图片</p></figure><p id="cd15" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们想对L做什么？我们想找出θ，L的最大值。例如，如果L可以解析地最大化，并且它是凹的，我们可以计算关于θ的导数，并让它为零。此时，L的值将是全局和局部最大值。让我们看一个正态分布的多元数据的例子。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi nl"><img src="../Images/9b1e7b17685a320fed426f92d0314ec5.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*zUkvBqwm6vzcnsAYyvu67Q.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">等式1.2正态分布</p></figure><p id="00e3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">考虑随机变量X = (X₁，X₂，…，Xₙ)，均值μ = (μ₁，μ₂，…，μₙ)；我们假设标准方差是一个常数σ，这个性质也被称为<strong class="kk iu">同方差</strong>。因此，似然函数是</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/1d345586a8627d8614dd7447631ef57d.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*XkbKn7I2FJ0Pq53dOz0_Hw.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">等式1.3</p></figure><p id="fd8c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这些指数看起来有点混乱，但是考虑到每个观察值被安排在矩阵x的列中的事实，等式1.3实际上是非常简单的。xᵢⱼ只是第j次观察的第I个成分。并且等式[ex1]用于估计每个μᵢ.我们知道对数可以把乘积变成求和，而通常，求和更容易处理。所以我们可以试着取等式1.3两边的对数。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/19be4d8d7bb67600eb77fbfa88212c7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*Fc2nglkgtjrbeqIf9FFmWw.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">等式1.4</p></figure><p id="f027" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">(应该很明显<em class="no"> log </em>指的是自然对数)剩下的就好办了；我们需要对方程1.4做一些代数运算。由于σ是一个常数，我们可以把它分解出来；然后我们到达</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi np"><img src="../Images/6b73a310cfc07eb8726e3c3966c3aee5.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*uDQmt5jSTE5Z0X9VQOcYmg.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">等式1.5</p></figure><p id="b13f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">记住我们想要最大化L，这相当于最大化等式1.5，因为<em class="no"> log </em>单调增加。这可以归结为最小化下面的表达式</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/4e074119d572289860bb566ca16c6685.png" data-original-src="https://miro.medium.com/v2/resize:fit:258/format:webp/1*p9QRhLW5oF26CGxoQUx0cg.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">公式1.6</p></figure><p id="210f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你熟悉普通的线性模型，这应该会让你想起最小二乘法。我们可以看到，最小二乘法和正态假设下的MLE是一样的(误差项具有正态分布)。</p><p id="32fa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于实际应用中的最大似然估计，我们看下面的例子:一个高中学生获得的奖励数量的数据集(此处<a class="ae lu" href="https://www.sheffield.ac.uk/mash/statistics/datasets" rel="noopener ugc nofollow" target="_blank">可用</a>)。如果我们画出奖励的数量</p><pre class="lf lg lh li gt nr ns nt nu aw nv bi"><span id="7c24" class="nw lw it ns b gy nx ny l nz oa">awards &lt;- read.csv2(file='data/Awards_R.csv', header=TRUE, sep=',')<br/>summary(awards)<br/>awards.num &lt;- awards$num_awards</span><span id="f51d" class="nw lw it ns b gy ob ny l nz oa">plot(table(awards.num), main='Awards in math', ylab='Frequency', xlab='Number of awards')</span></pre><p id="0628" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从图中我们可以看出，它遵循泊松分布</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/1c8b46dfbd8c1156a6bc3a91f6c01bd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*vA9ADtJFx_6Oyq9I5gwFAg.png"/></div></figure><p id="8f8e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">泊松分布的最大似然函数定义为</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi od"><img src="../Images/4becaf09af0979454ccc76290a0a724f.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/format:webp/1*qlsVkguPyeWHR3k9kLw4MA.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">等式1.8泊松分布的最大似然函数</p></figure><p id="7334" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以这样实现</p><pre class="lf lg lh li gt nr ns nt nu aw nv bi"><span id="f49c" class="nw lw it ns b gy nx ny l nz oa">L=function(x){<br/>    a=1<br/>    for (i in 1:length(awards.num)){<br/>        # Poisson probability mass function<br/>        a=a*dpois(awards.num[i],x)<br/>    }<br/>    return(a)<br/>}</span></pre><p id="7d08" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">并优化这个函数</p><pre class="lf lg lh li gt nr ns nt nu aw nv bi"><span id="1145" class="nw lw it ns b gy nx ny l nz oa"># find the value for which L is maximized<br/>sol &lt;- optimize(L, c(0,2), maximum=TRUE)<br/>curve(L, from=0, to = 2)<br/>x1 &lt;- sol$maximum<br/>x2 &lt;- sol$objective<br/>points(x1, x2, col="#FF0000", pch=19)<br/>abline(v=x1, col="#FF0000")</span></pre><p id="909d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">搜索最大值的间隔是根据L图选择的(见图1.8)。该函数通过<code class="fe oe of og ns b">optimize</code>进行数值最大化，它在给定的区间内(以预定的精度)搜索优化值。这给了我们</p><pre class="lf lg lh li gt nr ns nt nu aw nv bi"><span id="53fc" class="nw lw it ns b gy nx ny l nz oa">$maximum<br/>[1] 0.970013</span><span id="f4d0" class="nw lw it ns b gy ob ny l nz oa">$objective<br/>[1] 1.853119e-113</span></pre><p id="7bf9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这意味着最大值是1.853119e-113，L(0.970013)= 1.853119 e-113—λ= 0.970013是优化的参数。如图表所示</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/527ae3b9c6a6292f03c634290baebc92.png" data-original-src="https://miro.medium.com/v2/resize:fit:996/format:webp/1*tShJ7CWmrgj8v6kPE_0gCg.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">图1.8似然函数</p></figure><p id="1f85" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">结果表明，样本均值与优化L值非常接近</p><pre class="lf lg lh li gt nr ns nt nu aw nv bi"><span id="a09d" class="nw lw it ns b gy nx ny l nz oa">mean(awards.num)<br/># --&gt; 0.97<br/># sol$maximum = 0.970013</span></pre><p id="b76c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是有意义的，因为泊松分布中的参数λ与期望值相同。为了正式证明这一点，我们可以对对数似然函数求导</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/1eca07953d9ca90a613f8c23de20cbb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*B5CcmD3jZFyJMmcJZaYl-g.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">等式1.9 log(L)的推导，L在等式1.8中定义</p></figure><p id="d79b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">将这个导数设为零，我们得到</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/be74fc344c917fcda730c8e4c429d9d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:224/format:webp/1*ajWoCrpuTd76-DwEjDrQ9w.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">等式1.10估计量</p></figure><p id="7ff3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中λ和hat表示估计量。这告诉我们，在这个例子中，最大似然估计量是由样本均值给出的。</p><h1 id="ca48" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">极大似然估计的置信区间</h1><h2 id="b2a2" class="nw lw it bd lx ok ol dn mb om on dp mf kr oo op mh kv oq or mj kz os ot ml ou bi translated">费希尔信息矩阵</h2><p id="781e" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">假设随机变量X来自参数为θ的分布<em class="no"> f </em>，费希尔信息<em class="no">测量X </em>携带的关于θ的信息量。为什么这种量化很重要？事实证明，在贝叶斯和频率主义方法的统计，费雪信息的应用。贝叶斯主义者用这个来求初始概率，频率主义者在MLE中构造置信区间。(要阅读更多关于贝叶斯和频率主义方法的内容，请参见<a class="ae lu" href="https://medium.com/science-and-philosophy/subjectivism-in-decision-science-926c29feb7bb" rel="noopener">这里</a>)费希尔信息重要性的一个具体例子在【2】中谈到:</p><p id="05f2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个例子是连续投掷硬币十次，观察结果是一个10维的数组，一个可能的结果是X = (1，1，1，1，1，0，0，0，0)。我们要估计得到人头的概率，θ。x有2个⁰ = 1024个可能的结果，我们可以让另一个随机变量，t是x中的人头数，t是x的函数，它被称为一个<strong class="kk iu">统计量</strong>。在一些文献中，统计被描述为“一条信息”这是真的，但更准确地说，它是观察值(数据集)的函数，它<strong class="kk iu">总结了</strong>数据。</p><p id="e066" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这个例子中，T具有由概率密度函数给出的二项式分布</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi ov"><img src="../Images/2652d2e581846ca985c7cdcf68f9d3f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*AjW5JPre-keWRvZqDQzcmQ.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">等式2.1</p></figure><p id="ba12" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在本例中，n = 10。x有1024种可能的结果，然而T只能取11个不同的值。是否意味着X比T包含更多的信息？一般来说，是的，既然X把抛硬币的顺序考虑进去了，但是T没有。但是关于θ，<strong class="kk iu">没有</strong>，因为抛硬币的输出顺序不影响θ。同样，统计量T是<strong class="kk iu">充分的</strong>，这意味着给定T的值，X取特定值的概率不取决于θ。这意味着，条件概率分布P(X | T = t，θ)是均匀的，由下式给出</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/b149ff15b64d5fe8680140fc1f7a513b.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*ysGwYAdCYBlBEtkOGIz_NA.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">等式2.2</p></figure><p id="e378" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这也可以这样解释:给定T的值，X中不再有关于θ的<strong class="kk iu">信息</strong>。为了量化统计量T和原始数据X中关于参数θ的信息，Fisher信息开始发挥作用</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/1ed7016df6101d25fb161726ebda52f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/format:webp/1*rzBvJszSyGXci-DdxWNQtA.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">定义2.3 (a)费希尔信息(离散)</p></figure><p id="dfdf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中ω表示样本空间。在连续分发的情况下</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/086205c12169a8fdce2ddd6611d3a911.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*FnQqdarD0DPAAH4RKDudmA.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">定义2.3 (b)费希尔信息(续)</p></figure><p id="17f2" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">log f(x|θ)的偏导数称为<em class="no">得分函数</em>。我们可以看到，费希尔信息就是得分函数的<strong class="kk iu">方差</strong>。如果有多个参数，我们就有了带有元素的矩阵形式的Fisher信息</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/7e6d48b90a1947eab976765a7ddfafd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*hbR0yjmYSrwS58EGXy_95Q.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">定义2.4费希尔信息矩阵</p></figure><p id="0b65" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这也可以写成</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/dca23f41b3667ef8cb311729f5b6ed43.png" data-original-src="https://miro.medium.com/v2/resize:fit:694/format:webp/1*Za-yMFnq_jj3RUHtdVZgKQ.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">等式2.5费希尔信息矩阵</p></figure><p id="6df0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">定义2.4和方程2.5之间的等价性不是微不足道的。这是费雪信息的一个重要性质，我们现在就来证明一维情况(θ是单参数):先从恒等式开始:</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/dfbd42dea50884fb59a23fd2f653500f.png" data-original-src="https://miro.medium.com/v2/resize:fit:392/format:webp/1*wwq7uCowJOlAPHEm3x0qYg.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">等式2.6</p></figure><p id="8562" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">也就是密度函数f(x；θ),θ为参数。注意f(x|θ)和f(x；θ).第一个表示条件概率——概率分布函数在给定参数的条件下。然而，后者意味着θ是函数的参数，仅此而已。然后我们对两边的θ求导。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/5b1dfac0c1c31b68e7e923f1f884dd93.png" data-original-src="https://miro.medium.com/v2/resize:fit:416/format:webp/1*K1Gn5BxFhjQhYa8VwWQ-_g.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">等式2.7</p></figure><p id="946c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在我们需要尝试让<em class="no">日志</em>出现。诀窍如下</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/753a9a445c4aa504c03229b774568f92.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*dde9_MpmlWPY6DHCAvCVCg.png"/></div></figure><p id="2b33" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在等式2.7中，我们使用乘一技术(“乘一”、“加零”——数学中著名的把戏)，这意味着我们乘以f(x；θ)然后除以f(x；θ).等式2.8中红色部分的组合给出了f(x；θ).</p><p id="a73c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">等式2.9给出了费雪信息的另一个重要性质— <em class="no">费雪信息的期望等于零</em>。(是旁注，本帖没有用到这个性质)回到Def 2.4和方程2.5等价的证明。我们重新对方程2.9关于θ求导</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/d4175bcbf69d04bcc53ade9ce37372a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1262/format:webp/1*LLpmYswKDOQcNzQoI6fs2Q.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">再次对等式2.9求导</p></figure><p id="2383" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因为在等式2.10中，灰色和黑色部分都是正的(f(x；θ)终究是<a class="ae lu" rel="noopener" target="_blank" href="/measure-theory-in-probability-c8aaf1dea87c">概率测度</a>，唯一可能的场景就是方程(2.11)。从方程2.11开始，我们移动f(x；θ)从LHS(左侧)到RHS(右侧)</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/25b2f3da2423bdc753ed53f4e2b77dce.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*YCf6nvOQmpQg5-tJzZlWxg.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">等式2.12</p></figure><p id="8f77" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">就像等式2.8一样，在等式2.12中，红色部分的组合再次给我们f(x；θ).</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/ef7a8ea3bbf7f0028b0eb5475b99d88d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*f95jD_nEiumo0MwEfOl8Qg.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">等式2.13</p></figure><p id="3fd7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">证明已经完成了。</p><h2 id="01fe" class="nw lw it bd lx ok ol dn mb om on dp mf kr oo op mh kv oq or mj kz os ot ml ou bi translated">构建置信区间</h2><p id="9370" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">首先，我们将介绍最大似然估计的渐近分布定理，它告诉我们估计量的渐近分布:</p><blockquote class="ph pi pj"><p id="a6dd" class="ki kj no kk b kl km ju kn ko kp jx kq pk ks kt ku pl kw kx ky pm la lb lc ld im bi translated">L <!-- --> et X₁，…，Xₙ是由f(x)给定的分布中大小为n的一个样本，参数θ未知。设真实参数为θ₀，θ₀的最大似然法为θhat，则</p></blockquote><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/8c38cb9902a67fd3678b6d7fbed0ee74.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*Ib_aThdmXsjkgHYdbyWzLw.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">等式2.5</p></figure><p id="737a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这表明</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div role="button" tabindex="0" class="lk ll di lm bf ln"><div class="gh gi po"><img src="../Images/b9f2587a518e10ffe58ead45d46cbfdf.png" data-original-src="https://miro.medium.com/v2/resize:fit:358/format:webp/1*My2Xg6hUBn9E67a1xeOjYw.png"/></div></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">等式2.6估计量的分布</p></figure><p id="fb4b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因为当样本量接近无穷大时，最大似然估计接近真实参数，这也被称为最大似然估计的一致性</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/7b9a661eb02546d11e2a55233b143511.png" data-original-src="https://miro.medium.com/v2/resize:fit:392/format:webp/1*GTRL3yQK_cl8oxpfyC3ZiQ.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">性质2.7最大似然估计的一致性性质</p></figure><p id="1544" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们也可以论证方程2.8也成立(参考方程2.5)。</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/9f9a9aa1a2da9764cb63784066f96e2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*EUXyKL1TGU62gjw9z8yogA.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">等式2.8</p></figure><p id="e79b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">(关于这个定理的证明，见<a class="ae lu" href="https://people.missouristate.edu/songfengzheng/Teaching/MTH541/Lecture%20notes/Fisher_info.pdf" rel="noopener ugc nofollow" target="_blank">此处</a>，第5页。)然后我们可以从下面的等式建立置信区间</p><figure class="lf lg lh li gt lj gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/3b7fe8f432c80981f61000874f01572b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1388/format:webp/1*7PsHuGldOgdsPDH-SWnA_g.png"/></div><p class="lq lr gj gh gi ls lt bd b be z dk translated">不等式2.8置信区间</p></figure><p id="1a73" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">其中z是累积函数的倒数，α是临界值。接下来的事情是找到费希尔信息矩阵。这很容易，因为根据等式2，5和Hessian的定义，对数似然函数的负Hessian就是我们要找的东西。</p><p id="b409" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">你可能会问为什么方程2.5中的费希尔信息矩阵与海森信息矩阵相同，尽管它是一个期望值？这是因为函数L是在最大似然估计下求值的，也就是说L是针对特定的θ求值的。而这就是使l最大化的θ，因此，加权平均(我们知道期望是加权平均)就不再必要了——观察到的<strong class="kk iu">费希尔信息</strong>只是二阶微分。“观察到的”意味着费希尔信息是观察到的数据的函数。(这个话题在<a class="ae lu" href="https://stats.stackexchange.com/questions/68080/basic-question-about-fisher-information-matrix-and-relationship-to-hessian-and-s?rq=1" rel="noopener ugc nofollow" target="_blank"> MathStackExchange </a>上也有讨论)。</p><p id="9651" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以用下面的代码找到置信区间，用同样的数据集。</p><pre class="lf lg lh li gt nr ns nt nu aw nv bi"><span id="59c7" class="nw lw it ns b gy nx ny l nz oa">L.log = function(x){<br/>    a=1<br/>    for (i in 1:length(awards.num)){<br/>        # Poisson probability mass function<br/>        a=a+dpois(awards.num[i],x, log=TRUE)<br/>    }<br/>    return(a)<br/>}</span><span id="3ecd" class="nw lw it ns b gy ob ny l nz oa"># numerical approach<br/>opt.log = optim(par=1, L.log, method="Brent", control=list(fnscale=-1), hessian=TRUE, lower=0, upper=2)</span><span id="aa6d" class="nw lw it ns b gy ob ny l nz oa">opt.log<br/>I.log &lt;- opt.log$hessian</span><span id="3429" class="nw lw it ns b gy ob ny l nz oa"># since we have only one parameter, there's no inverse of matrix calculated<br/>est.log &lt;- qnorm(1 - alpha/2) / sqrt(n * (-I.log[1,1]))<br/>l.est.log &lt;- x1 - est.log<br/>h.est.log &lt;- x1 + est.log<br/>l.est.log<br/>h.est.log<br/># --&gt; CI = [0.9603613, 0.9796647]</span></pre><p id="239d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="no">总结:</em></p><p id="fed6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这篇文章中，我们快速介绍了最大似然估计，然后我们来看看Fisher信息及其矩阵形式。记住这两个概念，然后我们探索如何构建置信区间。在这篇文章中，只使用了一个例子:一所高中的获奖人数。该分析完全在r中实现。</p></div><div class="ab cl pr ps hx pt" role="separator"><span class="pu bw bk pv pw px"/><span class="pu bw bk pv pw px"/><span class="pu bw bk pv pw"/></div><div class="im in io ip iq"><p id="7e70" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">资源:</p><p id="83ce" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[1]p . m . e . Altham(2005年)。<a class="ae lu" href="http://www.statslab.cam.ac.uk/~pat/All.pdf" rel="noopener ugc nofollow" target="_blank">R中的广义线性建模介绍</a>。<em class="no">统计实验室，朱尼奥</em>。</p><p id="8f28" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2] Ly，a .，Marsman，m .，Verhagen，j .，Grasman，R. P .，&amp; Wagenmakers，E. J. (2017)。<a class="ae lu" href="https://www.sciencedirect.com/science/article/pii/S0022249617301396?casa_token=xnfLaHuZcvQAAAAA:cLkteYOqVl7VBwX-QHyKjEyv9RrHbzmi6DXQZtzbH0JR4sQqo1v8LqhT7PLRO0Em5crfGEbbX1k" rel="noopener ugc nofollow" target="_blank">关于费希尔信息的教程</a>。<em class="no">数学心理学杂志</em>，<em class="no"> 80 </em>，40–55。</p><p id="440d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[3]马尔科·塔博加(2017)。“泊松分布——最大似然估计”，概率论与数理统计讲座，第三版。Kindle直接出版。在线附录。<a class="ae lu" href="https://www.statlect.com/fundamentals-of-statistics/Poisson-distribution-maximum-likelihood." rel="noopener ugc nofollow" target="_blank">https://www . stat lect . com/fundamentals-of-statistics/泊松分布-最大似然法。</a></p><p id="f7f0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[4] Klugman，S. A .，Panjer，H. H .，&amp; Willmot，G. E. (2012年)。<em class="no">损失模型:从数据到决策</em>(第715卷)。约翰·威利的儿子们。</p><p id="d91d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[5]什么是删失数据？可从以下网址获得:<a class="ae lu" href="https://reliability.readthedocs.io/en/latest/What%20is%20censored%20data.html" rel="noopener ugc nofollow" target="_blank">https://reliability . readthedocs . io/en/latest/What % 20 is % 20 reviewed % 20 data . html</a>。于2021年10月13日访问</p></div><div class="ab cl pr ps hx pt" role="separator"><span class="pu bw bk pv pw px"/><span class="pu bw bk pv pw px"/><span class="pu bw bk pv pw"/></div><div class="im in io ip iq"><p id="88bc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">错误日志</p><ol class=""><li id="5e0f" class="py pz it kk b kl km ko kp kr qa kv qb kz qc ld qd qe qf qg bi translated">增加等式1.1中符号的解释</li></ol></div></div>    
</body>
</html>