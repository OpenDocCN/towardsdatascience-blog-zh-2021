<html>
<head>
<title>Understanding Convolutional Neural Networks (CNNs)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解卷积神经网络(CNN)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-convolutional-neural-networks-cnns-81dffc813a69?source=collection_archive---------17-----------------------#2021-12-30">https://towardsdatascience.com/understanding-convolutional-neural-networks-cnns-81dffc813a69?source=collection_archive---------17-----------------------#2021-12-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="fb3e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">对最强大的深度学习工具之一及其构建模块的温和介绍</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/657c3602777098a6c19cf66eac992fa8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rhV3URvLEWEfBWu4S07vCw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">原文由<a class="ae kv" href="https://unsplash.com/s/photos/andrew-schultz" rel="noopener ugc nofollow" target="_blank">安德鲁·舒尔茨</a>在<a class="ae kv" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="a407" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本文将涵盖卷积神经网络(CNN)的所有主要方面，它们如何工作以及这项技术的主要构件。本文中使用的参考资料可以在我的<a class="ae kv" href="https://github.com/vinyluis/Articles/tree/main/Como%20funcionam%20as%20CNNs" rel="noopener ugc nofollow" target="_blank"> github 资源库</a>中找到。</p><p id="f817" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">卷积神经网络(CNN)或简称为卷积网络是一种至少在其一层中使用卷积运算而不是矩阵乘法的神经网络。</p><p id="7576" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种类型的网络有效地用于数据元素与其相邻元素具有某种关系的应用中，如图像(由二维像素阵列表示)或时间序列或音频文件(由规则时间帧中的一维数据点样本序列表示)。</p><p id="9b9d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于能够有效地提取图像特征，细胞神经网络被广泛应用于目标检测、人脸识别、语义分割、图像处理和操作等任务中。一些摄像头也有基于 CNN 的智能过滤器，自动驾驶汽车使用这些网络来导航和检测障碍物，许多其他系统都基于这种方法。</p><h1 id="9db7" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">卷积运算</h1><p id="1eb6" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">卷积是两个函数之间的线性数学运算。给出了函数<strong class="ky ir"> <em class="mp"> x(t) </em> </strong>和<strong class="ky ir"> <em class="mp"> w(a) </em> </strong>，分别称为<em class="mp">输入</em>和<em class="mp">内核</em>。函数<strong class="ky ir"> <em class="mp"> x </em> </strong>与<strong class="ky ir"> <em class="mp"> w </em> </strong>的卷积超过<strong class="ky ir"> <em class="mp"> a </em> </strong>，对于给定的<strong class="ky ir"><em class="mp"/></strong>为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/187b798b4683d21fa5e9711292f9cca4.png" data-original-src="https://miro.medium.com/v2/resize:fit:620/format:webp/0*Suup_PmXjkTn45GV.png"/></div></figure><p id="2d0c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通配符运算符<strong class="ky ir"> <em class="mp"> (x*w) </em> </strong>代表卷积，输出<strong class="ky ir"><em class="mp">【s(t)</em></strong>通常称为<em class="mp">特征图</em>。</p><p id="eaa6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该算子可以扩展到离散函数，这是在一维 CNN 上使用的情况(例如用于时间序列的情况):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/929fea2d9be4271753f7fd8aad37bfb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:626/format:webp/0*zTC159i8iM8QEri_.png"/></div></figure><p id="e6f3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">而在 2D 卷积的情况下，非常用于图像处理，并且考虑到将<strong class="ky ir"><em class="mp"/></strong>I 作为输入图像，并且将<strong class="ky ir"> <em class="mp"> K </em> </strong>作为二维核，卷积可以表示为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/a76be74295d2deea0fb4b3bca4dd9c4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/0*Oi_iZUV3sorD3r5w.png"/></div></figure><p id="5de8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通常核比图像小，卷积使用一个核来处理整个图像。由于简化的表示，每个卷积核(或层)需要学习少量的参数。</p><p id="b5f4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另一方面，由于标准 ann(人工神经网络)使用权重矩阵将给定层的神经元连接到前一层的输出，每个权重将代表一个单一的连接，因此对于相同的任务，ann 将需要学习比等效 CNN 多得多的参数。</p><h1 id="d64a" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">图像卷积</h1><p id="9418" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">正如我已经说过的，卷积广泛用于图像处理。通过改变使用的内核，有可能操纵模糊和形状，改变图像的风格或检测其边缘。下图显示了一些不同内核在基础映像上的应用。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mt"><img src="../Images/dcd51888397ff4ea4814c77e21ef05be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C_VhJRGqe-pVkmthkKUt9Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">在基础映像上使用不同的内核。内核是在它们各自的图像下表示的矩阵。图片作者。</p></figure><p id="2c2a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">核应用于图像的每个像素，核和图像受影响区域之间的卷积运算的结果将成为输出图像上的新像素，如下图所示。但是，内核不能应用于边界像素，因为内核的一部分会在图像之外，因此，生成的图像比原始图像稍小。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mu"><img src="../Images/4cfe7d61f13a4b28fa97ea2c8a300e3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UFj0-Npfz58nKmUmzx2taw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图像上的二维卷积。内核(蓝色)应用于图像(红色)。操作的结果将成为输出图像的新像素(绿色)。由于内核不能应用于真实图像之外，因此绿色区域是卷积发生的地方，从而产生一个稍小的图像。在卷积过程中，内核将滑过原始图像。图片作者。</p></figure><p id="9169" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上图中，5x5 内核(蓝色)与 16x12 图像(红色)的卷积将产生 12x8 图像(绿色)。新的宽度和高度可以用下面的公式计算:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/b83a0a936720ca0f546ff0ab238a3def.png" data-original-src="https://miro.medium.com/v2/resize:fit:414/format:webp/0*dzFk_eldIaj8_Ovm.png"/></div></figure><p id="75d4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在 CNN 上，我们认为卷积是网络的一层，但是每层可以有一个以上的核。输出将具有与内核数量相同的特征映射。</p><p id="9100" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如，具有 4 个内核的卷积层接收尺寸为(<strong class="ky ir"><em class="mp">win</em></strong>x<strong class="ky ir"><em class="mp">hin</em></strong>x 1)的灰度图像，将产生尺寸为(<strong class="ky ir"><em class="mp">wout</em></strong>x<strong class="ky ir"><em class="mp">hout</em></strong>x 4)的矩阵作为输出。该矩阵将不再被认为是图像，而是对应于由该层的四个内核检测的特征的一组四个特征图。</p><p id="55be" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过在构成网络结构的层上链接特征地图和内核，CNN 可以捕捉图像上存在的各种特征，这是允许这些网络学习识别物体和其他特征的过程。</p><h1 id="914b" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">多核过滤器</h1><p id="f093" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在多通道图像中，如 RGB 图像或由网络内层产生的特征图，卷积将在每个通道一个核的情况下发生。所有这些内核一起形成一个<em class="mp">滤波器</em>，每个滤波器的内核数量与输入图像的通道数量相同。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/78905e9e7ffae3cbbfa9a175c9bb6dd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:404/format:webp/1*AQ2DAxWY3tDlFz0v1w766Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">多核过滤器示例。图片作者。</p></figure><p id="b55e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">每个滤波器将产生一个单一的特征图，因此我们需要通过求和或求平均值来聚集各个内核的输出，从而产生该滤波器的单一输出。</p><p id="929f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用<strong class="ky ir"> <em class="mp"> n </em> </strong>滤镜的图层将生成<strong class="ky ir"> <em class="mp"> n </em> </strong>特征地图作为图层的输出。</p><h1 id="5217" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">填料</h1><p id="e7ed" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">为了避免图像边缘像素上的内核卷积问题，我们可以在输入上应用<em class="mp">填充</em>。我们可以通过简单地在边框上添加更多的像素来放大它。</p><p id="39bd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下图说明了三种类型的填充:零填充<em class="mp"/>(a)在输入图像的周界上插入值 0；将内部像素反射到边界上的<em class="mp">反射填充</em>；以及用常数值(包括 0)填充新边框的<em class="mp">常量填充</em> (c)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mt"><img src="../Images/bdd029b85eeb09b36905d18dc8c08d45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dSD1T5JX7d8FHKFsa3iW6w.png"/></div></div></figure><p id="a43a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">填充边界的大小可以由网络的创建者决定，但通常我们选择一个值来消除输出图像上的缩减效果。我们可以用下面的公式计算新的尺寸:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/03944b0224787b524c9993b2f47c803d.png" data-original-src="https://miro.medium.com/v2/resize:fit:648/format:webp/0*STtKUHuQqPnWx1f5.png"/></div></figure><h1 id="1dac" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">联营</h1><p id="f346" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">当网络学习最佳滤波器以仅从图像中提取相关特征时，每个滤波器也将产生噪声和与后续层不相关的信息。我们可以在卷积示例中看到，边缘检测器发现了一些不属于狗的像素。</p><p id="550c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">池技术用代表像素组的值来代替像素组。下图说明了大小为 2 的<code class="fe my mz na nb b">MaxPool</code>，它将像素分组为 2x2 的正方形，并用最强像素的值替换整个组。类似地，我们可以执行<code class="fe my mz na nb b">AvgPool</code>，其中产生的像素是组的平均强度。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/4aa89c1836747deaff4b1c65dd506735.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w6ZPThhCo6OZqTc9fUOqZA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">最大池操作。图片作者。</p></figure><p id="8cd0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作为池层的直接结果，信息被压缩，只保留对后续层最重要的信息并减少干扰。输出也减少了一个因子，该因子取决于池大小(在本例中，输出的宽度和高度是输入的一半)，因此每个卷积需要处理的像素更少，结果网络变得更有效。</p><p id="f4f5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下图说明了<code class="fe my mz na nb b">MaxPool</code>和<code class="fe my mz na nb b">AvgPool</code>在应用于狗图像的边缘检测器结果中的应用。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mt"><img src="../Images/23c9ab6fdca85fc1c55a9025a4b55a4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cymgVsYHUrV4A-whbfbBvg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">池技术在边缘图像上的应用。图片作者。</p></figure><p id="356b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe my mz na nb b">MaxPool</code>的另一个重要方面是，它使网络对输入端的小转换更加健壮。由于相邻像素被所有像素中的最高值替换，如果转换将最重要的像素保留在同一组中，结果将保持不变。</p><h1 id="0629" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">进展</h1><p id="c014" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">我们还可以使用步长卷积来降低图像(或特征图)在各层中的维数。stride 参数表示每次迭代后内核将行走多少像素(或矩阵元素)。</p><p id="20f6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在前面的例子中，我们使用了 stride = 1，这意味着内核将移动到下一个像素，当一行结束时，它将继续下一行。下图的上半部分通过对红色输入应用 5x5 内核来说明这种情况。蓝色方块代表每次迭代中内核的中心。</p><p id="240d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">图片的下半部分代表步幅= 2 的情况。在这种情况下，当在行上行走时，内核将跳过一个像素，当行结束时，它也将在继续之前跳过一整行。正如我们所看到的，内核停止的像素比前一种情况少，所以生成的图像更小。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/17474c023fa6a631a255136e874d6875.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4jed2GHkVq-Zr3Ek8Mi52A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">比较跨距= 1(上)和跨距= 2(下)的卷积。产生的输出大小不同。图片作者。</p></figure><p id="cb33" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当我们想要恢复小于输入的输出时，维数减少是很重要的，例如编码器想要压缩图像，或者二进制分类器有一个单一的值作为输出。</p><p id="e159" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以为宽度(<strong class="ky ir"> <em class="mp"> wstride </em> </strong>)和高度(<strong class="ky ir"> <em class="mp"> hstride </em> </strong>)定义不同的步幅，因此可以使用下面的表达式来计算得到的尺寸。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/48db37518c2d195d1914f1ea6b121caa.png" data-original-src="https://miro.medium.com/v2/resize:fit:530/format:webp/0*5aFUduooOfWP_se3.png"/></div></figure><h1 id="26ab" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">转置卷积</h1><p id="b1fe" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在其他应用中，我们可能希望在一层之后增加输出的维度。一些生成方法通常从压缩表示(如潜在向量)开始，并通过层增加维度，直到输出生成最终图像。</p><p id="4aba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">实现这一点的一种方法是使用转置卷积(很多时候会被反卷积误操作)，即在输入图像通过“传统”卷积之前，结合使用填充和步幅来放大输入图像，从而使输出大于输入。</p><p id="1f9e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下图说明了通过使用核为 3x3 且步幅= 1 的转置卷积，从 3x3 输入生成 5x5 图像的过程。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nf"><img src="../Images/a0e501003eb19d6cdf4884f9386c216d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HE0IHCX-rrpYbn3TxdsSxw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">转置卷积。图片作者。</p></figure><p id="308e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在 5x5 图像中，内核为 3x3 且步幅= 1 的“传统”卷积将生成 3x3 特征图作为输出，因此我们需要使用相同的参数执行转置卷积，以实现相反的操作。</p><p id="6dfb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它分两步走:</p><ol class=""><li id="f1a7" class="ng nh iq ky b kz la lc ld lf ni lj nj ln nk lr nl nm nn no bi translated">图像将在其原始像素(红色)之间以及边界上插入零值像素(灰色)。</li><li id="aa91" class="ng nh iq ky b kz np lc nq lf nr lj ns ln nt lr nl nm nn no bi translated">现在，卷积将应用于放大的图像，内核为 3x3，步幅= 1。</li></ol><p id="8fa4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了计算转置卷积后的输出维数，我们可以使用以下公式:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/b96b7042ed2c29014d6d76ddb7c48afa.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/0*mDgRoFc02HIWBI3U.png"/></div></figure><p id="6173" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">转置卷积并不是增加图层输出维度的唯一方法。其他架构使用传统的调整大小技术，如最近邻插值，然后是传统的卷积层。</p><h1 id="583d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">棋盘文物</h1><p id="21d5" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">转置卷积的一个缺点是，当通过图像时，内核会重叠，从而加强这些区域中像素的重要性。结果，生成的图像(或特征图)将具有周期性出现的<em class="mp">棋盘状伪像</em>。这种机制可以在下图中看到。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/7c17667c7ac97e6b915d63d798c3647e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pbAhIk4FAO47LiWZN438aA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">产生棋盘状伪像的机制。图片作者。</p></figure><p id="8bab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通过使用其他技术，如上面提到的调整卷积大小，我们可以避免这种影响。</p></div><div class="ab cl nw nx hu ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="ij ik il im in"><h1 id="f2fd" class="ls lt iq bd lu lv od lx ly lz oe mb mc jw of jx me jz og ka mg kc oh kd mi mj bi translated">结论和意见</h1><p id="b959" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">关于卷积有很多东西要学，但第一步是理解基础知识，我希望这篇文章对很多人有帮助。</p><p id="6551" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇课文改编自我的硕士学位论文。</p><p id="b759" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">特别感谢我的狗布鲁斯，他好心地让我给他拍照并用作例子。他是最好的男孩。</p><h1 id="f3b8" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">如果你喜欢这个帖子…</h1><p id="823e" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">支持我一杯咖啡！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><a href="https://www.buymeacoffee.com/vinitrevisan"><div class="gh gi oi"><img src="../Images/acf4154cfebdc13859934db49fd502cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*h_y4o6IwDDFFWIyKQE7Rww.png"/></div></a><p class="kr ks gj gh gi kt ku bd b be z dk translated">给我买杯咖啡！</p></figure><p id="1e4b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">看看这个很棒的帖子</p><div class="oj ok gp gr ol om"><a rel="noopener follow" target="_blank" href="/5-tips-to-start-a-career-in-data-211ad15a7ca8"><div class="on ab fo"><div class="oo ab op cl cj oq"><h2 class="bd ir gy z fp or fr fs os fu fw ip bi translated">开始数据职业生涯的 5 个技巧</h2><div class="ot l"><h3 class="bd b gy z fp or fr fs os fu fw dk translated">让我成为一名跨国公司数据科学家的步骤</h3></div><div class="ou l"><p class="bd b dl z fp or fr fs os fu fw dk translated">towardsdatascience.com</p></div></div><div class="ov l"><div class="ow l ox oy oz ov pa kp om"/></div></div></a></div><h1 id="e893" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">参考</h1><p id="6c92" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">奥登纳等。艾尔。<a class="ae kv" href="https://distill.pub/2016/deconv-checkerboard/" rel="noopener ugc nofollow" target="_blank">反卷积和棋盘状工件</a></p><p id="1fe3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">古德费勒等人。艾尔。<a class="ae kv" href="https://www.deeplearningbook.org/contents/convnets.html" rel="noopener ugc nofollow" target="_blank">深度学习。</a>第九章:卷积网络</p><p id="f6e8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一般化和网络设计策略。技术报告 CRG-TR-89–4</p><p id="51a6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Dumoulin，v .，Visin，F.: <a class="ae kv" href="https://arxiv.org/pdf/1603.07285.pdf" rel="noopener ugc nofollow" target="_blank">深度学习卷积算法指南。arXiv:1603.07285 (2016 年)</a></p><p id="21e8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">拉德福德，a .，梅斯，l .，钦塔拉，S.: <a class="ae kv" href="https://arxiv.org/pdf/1511.06434.pdf" rel="noopener ugc nofollow" target="_blank">深度卷积生成对抗网络的无监督表示学习</a>。</p></div></div>    
</body>
</html>