<html>
<head>
<title>GloVe Research Paper Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">手套研究论文解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/glove-research-paper-explained-4f5b78b68f89?source=collection_archive---------15-----------------------#2021-09-08">https://towardsdatascience.com/glove-research-paper-explained-4f5b78b68f89?source=collection_archive---------15-----------------------#2021-09-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8600" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">手套模型背后数学的直观理解和解释</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/0ea987440ca60553b38ed4ddbf19556d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*782-GD0wEDwPEpwMfmRYZw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自Unsplash </p></figure><p id="8e91" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">继我的<a class="ae kv" rel="noopener" target="_blank" href="/word2vec-research-paper-explained-205cb7eecc30"> <strong class="ky ir"> <em class="ls"> word2vectors研究论文讲解</em> </strong> </a>博客之后，我又拿起<a class="ae kv" href="https://nlp.stanford.edu/pubs/glove.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> GloVe研究论文</strong></a><strong class="ky ir"/>【Pennington等人】来讲解我对一篇非常详细而全面的研究论文的理解。</p><blockquote class="lt lu lv"><p id="d669" class="kw kx ls ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated"><strong class="ky ir"> GloVe </strong>代表<strong class="ky ir">Global</strong><strong class="ky ir">Vectors</strong>其中Global是指语料库的全局统计，Vectors是单词的表示。早期的单词嵌入方法，如LSA，word2vec，能够学习单词之间的句法和语义关系，但其起源尚不清楚。手套模型旨在明确归纳单词向量中的规则，并找到单词之间学习关系的来源。</p></blockquote><p id="48a9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 1。手套型号概述</strong></p><p id="294c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">GloVe是一个<strong class="ky ir">全局对数双线性模型</strong>。嗯，我相信你想知道术语' '<em class="ls">全局对数双线性</em>' '是什么意思。<em class="ls">全局</em>是指训练语料的全局统计。<em class="ls">对数双线性</em>是指输出的对数是两类字向量乘积的线性回归。在手套的情况下，两种类型是<em class="ls"> </em>单词向量(w)和上下文向量(w^).如下所述，手套模型结合了两种广泛采用的用于训练单词向量的方法。</p><h2 id="ca03" class="lz ma iq bd mb mc md dn me mf mg dp mh lf mi mj mk lj ml mm mn ln mo mp mq mr bi translated">1.1矩阵分解</h2><p id="fd03" class="pw-post-body-paragraph kw kx iq ky b kz ms jr lb lc mt ju le lf mu lh li lj mv ll lm ln mw lp lq lr ij bi translated">这些方法分解单词-单词或文档-术语共现矩阵。<strong class="ky ir">哈尔</strong>(语言的超空间模拟)【隆德&amp;伯吉斯】、<strong class="ky ir"> LSA </strong>(潜在语义分析)【兰道尔、福尔茨、&amp;拉哈姆】是使用矩阵分解的流行模型。<strong class="ky ir">特征值分解</strong>用于方阵，<strong class="ky ir">奇异值分解</strong> (SVD)用于矩阵分解的矩形矩阵。在词-词矩阵中，行代表词，列代表上下文词。矩阵Mᵢⱼ中的值表示特定单词Wᵢ在Wⱼ.上下文中出现的次数在文档术语矩阵中，行代表单词，列代表文档。一行表示特定单词在所有文档中的分布，一列表示该文档中所有单词的分布。</p><p id="ef92" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">矩阵分解方法通常包括以下步骤。</p><ol class=""><li id="911b" class="mx my iq ky b kz la lc ld lf mz lj na ln nb lr nc nd ne nf bi translated">为每个单词定义上下文，并生成单词-单词或文档术语矩阵m。</li><li id="3826" class="mx my iq ky b kz ng lc nh lf ni lj nj ln nk lr nc nd ne nf bi translated">按行、列或长度规范化矩阵中的值。</li><li id="aa5e" class="mx my iq ky b kz ng lc nh lf ni lj nj ln nk lr nc nd ne nf bi translated">删除方差较小的列，以降低矩阵的维数。</li><li id="1232" class="mx my iq ky b kz ng lc nh lf ni lj nj ln nk lr nc nd ne nf bi translated">对M进行SVD矩阵分解，生成U，S，V矩阵。</li><li id="d0d8" class="mx my iq ky b kz ng lc nh lf ni lj nj ln nk lr nc nd ne nf bi translated">在通过取第一个r以降序对矩阵M的奇异值进行排序之后，&lt; R (rank of matrix M), low rank of a matrix M can be obtained. Product of low rank matrices U^, S^ and V^ is close approximation to the original matrix M.</li><li id="f027" class="mx my iq ky b kz ng lc nh lf ni lj nj ln nk lr nc nd ne nf bi translated">Vector of a word Wⱼ is the jᵗʰ vector from reduced rank space matrix U^.</li></ol><p id="6ad5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">In such methods, the most common words in the corpus such as (<em class="ls">和</em>贡献了单词之间不成比例的相似性。出现在“The”、“and”上下文中的单词Wᵢ和Wⱼ导致比其真实相似性得分更大的相似性得分。类似地，Wᵢ和Wⱼ不经常出现在“the”、“and”的上下文中，但具有真正的相似性，它们的相似性得分会更低。诸如<strong class="ky ir">煤</strong>【t . Rohde等人】的方法通过使用基于矩阵的<em class="ls">相关性</em>或<em class="ls">熵</em>归一化来克服这个问题。当使用基于熵的归一化时，在所有上下文或文档中出现的单词，例如“the”、“and”，将具有高熵。原始计数用高熵项归一化，导致矩阵中这些项的高值缩小。这解决了向单词相似性分数添加不成比例的度量的问题。</p><p id="8d86" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 1.2基于窗口的方法</strong></p><p id="bef4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些方法在<strong class="ky ir">局部上下文窗口</strong>上工作，而不是使用语料库的全局统计。[Bengio等人]中的语言模型训练神经网络，神经网络又训练单词向量表示。该模型以N个单词的过去历史和预测下一个单词作为语言建模的目标。因此，该方法中的每个示例都是作为输入的单词的局部窗口，而输出是下一个单词。后来word2vec模型【Mikolov et al .】如<strong class="ky ir"> skip-gram和CBOW </strong>解耦语言建模，训练单隐层神经网络。更详细的了解，可以去翻翻<a class="ae kv" rel="noopener" target="_blank" href="/word2vec-research-paper-explained-205cb7eecc30"> <strong class="ky ir"> <em class="ls">我之前的文章</em> </strong> </a>就可以了。</p><p id="144e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与矩阵分解方法相比，这些方法没有利用可用的全局统计，导致单词的次优向量表示。通过使用跨语料库的重复局部窗口，可以减少模型的训练时间和复杂度。</p><p id="7c39" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 2。手套模型背后的详细数学运算</strong></p><blockquote class="lt lu lv"><p id="b85e" class="kw kx ls ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">接下来是你们都期待阅读的部分。理解任何算法背后的数学是非常重要的。它有助于我们理解算法的公式和掌握一个概念。也就是说，让我们定义一些符号。</p></blockquote><p id="07ad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">x是共现矩阵(单词-单词)，其中Xᵢⱼ是单词Wⱼ在单词Wᵢ.的上下文中的计数上下文对一个特定的单词意味着什么？单词周围的上下文可以被定义为由过去的N个单词和未来的N个单词组成的对称上下文。不对称上下文只包含过去的N个历史单词。简单计数或加权计数可用于计算矩阵中的值。简单计数将1作为出现次数。在加权计数中，1/d用作出现次数，其中d是与给定单词的距离。使用权重背后的动机是，作为有意义的上下文，与给定单词距离较近的上下文单词比距离较远的单词更重要。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/5619140064e5521e8a23333c342d061e.png" data-original-src="https://miro.medium.com/v2/resize:fit:194/0*kbDiQ2PG5SPmQ6Uh"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">iᵗʰ行-eqⁿ的共现值(1)</p></figure><p id="8605" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在Eqⁿ (1)中,“Xᵢ”是在“Wᵢ.”这个词的上下文中出现的所有词的总和</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/2afaba3bfeae628c68e2849349c3a6ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:278/0*FMRUcTd6KJnXk_v_"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Eqⁿ (2)</p></figure><p id="3bdc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在Eqⁿ (2)中，p是共现概率，其中Pᵢⱼ是单词Wⱼ在单词Wᵢ.的上下文中出现的概率</p><p id="98fe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">GloVe建议根据概率而不是原始计数来寻找两个单词之间的关系。通过寻找<em class="ls">与一些探测词</em> (Wₖ).)的共现概率来检查两个词(Wᵢ和Wⱼ)之间的关系</p><p id="78a1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设我们有两个词，Wᵢ是'<em class="ls">冰</em>，Wⱼ是'<em class="ls">蒸汽</em>，还有一些探测词Wₖ是'固体'、'气体'、'水'、'时尚'。从基本的理解中，我们知道“固体”更多地与'<em class="ls">冰</em>'有关(Wᵢ)and“气体”更多地与'<em class="ls">蒸汽</em> ' (Wⱼ)有关，而时尚与'<em class="ls">冰</em>'和'<em class="ls">蒸汽</em>'都无关，而水与'<em class="ls">冰'</em>'和'<em class="ls">蒸汽</em>'都有关现在我们的目标是在探测词中找到给定词的相关词。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/a120ad5e39177eb4e5b1d37654d100fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*ylmpBqzYYHROX3XN5fU_zA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd no">表1: </strong>来自60亿令牌语料库的同现概率(来源——GloVe研究论文)</p></figure><p id="2398" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据表1，对于作为“固体”的探测词(Wₖ)，p(固体|冰)是“固体”在“冰”的上下文中出现的概率，是(1.9*10^–4)大于p(固体|蒸汽)。P(k |冰)/P(k |蒸汽)之比是&gt; &gt; 1。对于探测词‘气’，比率为&lt;&lt;1. For probe words ‘water’ and ‘fashion’ the ratio is nearly equal to 1. The ratio of co-occurrence probability distinguishes words (solid and gas) which are more relevant to given words than irrelevant words (fashion and water). The words having ratio nearly equal to 1 either appear in the context of given words or not, hence causing no impact in learning relationship between given words. This proves <strong class="ky ir"> <em class="ls">的同现概率比率</em> </strong>是学习词表征的起点。</p><p id="4a84" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Pᵢₖ/Pⱼₖ的同现概率比取决于三个词Wᵢ、Wⱼ、Wₖ.函数F的最一般形式可以在单词和上下文向量上定义如下。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi np"><img src="../Images/c72f2fe23a35fb50942a649a3b03a705.png" data-original-src="https://miro.medium.com/v2/resize:fit:324/0*7jV7EkxECJ-YIDme"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Eqⁿ (3)</p></figure><p id="3e0f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中w ∈ Rᵈ是单词向量，w∞∈rᵈ是上下文向量。Eqⁿ (3)的右侧是从训练语料库中获得的概率。尽管f函数有大量的可能性，但它应该对存在于Pᵢₖ/Pⱼₖ.的信息进行编码目标词Wᵢ和Wⱼ之间的关系可以通过向量差来获得，因为这些向量来自d维的线性向量空间。因此等式变成了，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/e672b852be9e5c70b4749f6e0d4406c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:350/0*rbPZentF68N5STQB"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Eqⁿ (4)</p></figure><p id="9fb4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，方程的右边是定标器，而F的输入是d维向量。f可以被复杂的神经网络参数化，这最终将打破向量空间中的线性结构。为了避免这种情况，我们可以取F的输入的点积，这样可以防止向量维数与其他维数的混合。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/2afd66129de15c0d3be9952c5f16f3b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:396/0*YWPcShGvvcKasr2b"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Eqⁿ (5)</p></figure><p id="5223" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">单词和上下文单词可以相互交换，因为出现在“ice”上下文中的“solid”等同于出现在“solid”上下文中的“ice”。因此w ↔ w和X ↔ Xᵀ的替换可以在Eqⁿ完成(5)。基本上，我们将左边的输入改为f，然后希望对Eqⁿ的右边产生类似的影响(5)。这也被称为保持两个组之间的结构相似性(同态函数)。我们在Eqⁿ (5)的左手边有组g，用减法作为组运算，在右手边有组h，用除法作为组运算。现在，当我们替换w ↔ w和X ↔ Xᵀ时，为了保持结构的相似性，g中两个向量的相减应该反映h中这两个字向量的相除。假设X= wᵢᵀwₖ，Y= wⱼᵀwₖ ∈ G，z =(x y)。函数F在组(R)和(R&gt;0)之间应该是同态的。根据同态的定义，如果G中的Z = X-Y减去op，那么在组H<strong class="ky ir"/>F(Z)= F(X)÷F(Y)但是Z = X-Y因此我们得到F(X-Y)=F(x) ÷ F(y)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/5b20e1223155449f4e0bcb5be8fb7cbf.png" data-original-src="https://miro.medium.com/v2/resize:fit:656/0*s0E2XhDHnYWKTSYM"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Eqⁿ (6)</p></figure><p id="f9a6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">那么函数F应该是什么呢？你猜对了，应该是指数函数。F (x)= e ˣ.比较Eqⁿ (5)和Eqⁿ (6)，我们得到，F(wᵢᵀ*wₖ)=Pᵢₖ = Xᵢₖ / Xᵢ.因此，取两边对数，我们得到，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/f166702a55ecfe39343902ec6e5a9377.png" data-original-src="https://miro.medium.com/v2/resize:fit:460/0*JtlUI0igrz5qfUuP"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Eqⁿ (7)</p></figure><p id="3903" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Eqⁿ (7)在单词和上下文方面展示了对称性，除了术语log(Xᵢ).由于这个术语独立于Wₖ，我们可以认为这是bᵢ的偏见术语。再加上一个偏差项bₖ，我们得到</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/8c8fc77df20a42b21b92663f2993c68e.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/0*uCeRK0_x3Gcpyess"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Eqⁿ (8)</p></figure><p id="fad7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于log(x)在0附近发散，我们可以在log(Xᵢₖ上使用加法移位，使得对数的输入总是≥1，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/4a21913852e2e8b7f22becca96d91edd.png" data-original-src="https://miro.medium.com/v2/resize:fit:526/0*Wd7vpoDz03WZ9fdy"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Eqⁿ (9)</p></figure><p id="a5d1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Eqⁿ (9)类似于同现 <strong class="ky ir">矩阵</strong>的对数的<strong class="ky ir">分解，这是<strong class="ky ir"> LSA算法</strong>背后的主要思想。但是上面的等式有一个问题。如果我们使用平方误差函数来定义成本函数，它将对矩阵中的所有项给予相同的权重，即使对于值接近0的罕见频率项也是如此。这样的术语很吵，携带的信息也不多。因此，加权最小二乘法可以用作手套模型的成本函数。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/07537e9607508b1dd89a3c8dc9069d29.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/0*v6gVvIy-x4zGckIV"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Eqⁿ (10)</p></figure><p id="7429" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意，加法移位已经在Eqⁿ (10)所示的矩阵x上完成。j是我们想要在具有词汇大小的共现矩阵上最小化的成本函数。f(Xᵢⱼ)给矩阵中的每一项加权。wᵢ是单词向量，wⱼ是上下文向量。bᵢ和bⱼ是每个向量的偏差项，Xᵢⱼ是共生矩阵中的项。</p><p id="1459" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在问题是如何选择函数f，它应该遵循一定条件。f(0)=0，f应该是非递减的，并且对于x的大值，f(x)不应该非常高，因为不应该过度加权频繁出现的单词。来自Eqⁿ的xₘ(11)与图1中的Xₘₐₓ相同。在用Eqⁿ的Eqⁿ (11)代替f(xᵢⱼ(10)后，我们得到手套模型的最终成本函数。然后，使用优化器对一批训练样本训练模型，以最小化成本函数，从而为每个单词生成单词和上下文向量。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/1ca3e4b443e03d9bda3ac285cc07f468.png" data-original-src="https://miro.medium.com/v2/resize:fit:546/0*eh1oq-wpT-hPARk1"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Eqⁿ (11)</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/029dbff56eecc9fc3cf75d69ec2fdde3.png" data-original-src="https://miro.medium.com/v2/resize:fit:858/format:webp/1*pK8Zu8gZOKC3tbRVr2parQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd no">图1</strong>-α= 3/4的权重函数。(来源——手套研究论文)</p></figure><p id="22a9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我们完成了手套模型的完整推导。到目前为止，我们所有人都取得了很大的成就！！！现在，如果您想找出GloVe模型与skip-gram (word2vec模型)之间的关系，并理解GloVe模型的整体复杂性，您可以浏览接下来的部分。在这些部分之后，手套模型的实验和结果被讨论。</p><p id="fbc1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 3。手套和Skip-Gram之间的等效性(word2vec模型)</strong></p><p id="482d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">GloVe模型基于全局共现矩阵，而skip-gram通过局部窗口扫描，并且不考虑全局统计。这两种方法可以被认为是训练单词向量的两种不同的思想流派。本节找出了GloVe和skip-gram之间的相似性，即使乍一看这两种类型的模型有不同的解释。</p><p id="8f51" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们定义跳格模型，其中Qᵢⱼ定义单词j出现在单词I的上下文中的概率。Qᵢⱼ是给定Wⱼ的上下文单词的概率分布，并且可以被认为是softmax函数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/cd7bf79fe2c7bf535ce92fed6fe14e67.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/0*83xy4uxptr4BTQEV"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Eqⁿ (12)</p></figure><p id="c0ba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里，wᵢ和wⱼ分别是跳格模型的语境向量和词向量。skip-gram模型的目标是最大化训练语料上所有局部窗口扫描的对数概率。通过由局部窗口生成的示例，经由随机/在线发生训练，但是全局目标函数可以在来自语料库的所有局部扫描上公式化为</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/0faf440c5b8a977f34e44213f966bb65.png" data-original-src="https://miro.medium.com/v2/resize:fit:518/0*M85nyWFx5wla5Cml"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Eqⁿ (13)</p></figure><p id="8840" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了使成本函数为+ve，当Qᵢⱼ值在0和1之间时添加负号，并且该值范围的对数为-ve。上述成本函数和原始跳转程序之间的唯一区别是前者是全局的，后者是局部的。当遍历每个局部窗口时，我们可以将相同的单词-上下文对(Xᵢⱼ)组合在一起，并将它们直接乘以log(Qᵢⱼ项。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/eb30a454d48e99614e7e8c856f13c173.png" data-original-src="https://miro.medium.com/v2/resize:fit:438/0*pfs4uGYyjbF89YVl"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Eqⁿ (14)</p></figure><p id="e53f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们知道Pᵢⱼ=Xᵢⱼ/Xᵢ.因此代入Xᵢⱼ，我们得到</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/832217a0a894bf70ec697b3ec0f8f824.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/0*kd230qE0stJlzTUt"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Eqⁿ (15)</p></figure><p id="9748" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Xᵢ独立于j，所以它可以在j上求和之外。H(Pᵢ,Qᵢ)是Pᵢ和Qᵢ.概率分布之间的交叉熵注意，在交叉熵公式中考虑了负号。代价函数变成交叉熵误差的加权和，权重为Xᵢ.上述目标函数可以解释为全局跳过程序的目标。使用交叉熵作为误差度量有一定的局限性。p是长尾分布，当使用交叉熵作为误差度量时，不太可能/罕见的事件具有较高的权重。此外，Q必须用词汇V上的求和来归一化，这在使用交叉熵时是一个巨大的瓶颈。因此，可以使用不同成本误差度量来代替Eqⁿ (15)中的交叉熵，其中之一是最小平方目标。q中的分母项可以通过仅取分子项Q^.来忽略，新的成本函数被定义为，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/441c633065e4fcd5a32ce8a9f833abf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/0*Xrq9Nw5F_F6Kibkm"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Eqⁿ (16)</p></figure><p id="c834" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里，p^ᵢⱼ = Xᵢⱼ，Q^ᵢⱼ= exp(wᵢᵀ * wⱼ)是非正态分布。与Q^ᵢⱼ相比，Xᵢⱼ采用较大的值，导致优化中的大梯度和大步骤，导致模型的不稳定学习。因此，使用对数的平方误差。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/27b19a661105b2e6692a3ef8d6271ea8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1056/0*S-fhdIqhXCH3T0kA"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Eqⁿ (17)</p></figure><p id="7324" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">代替Xᵢ，f(Xᵢⱼ)被用作权重函数，如图1所示。方括号中的项可以颠倒，没有任何符号变化。Eqⁿ (17)的成本函数与Eqⁿ (10)的GloVe模型的成本函数等价，这表明skip-gram模型最终基于语料库的共现矩阵，并且与GloVe模型具有相似性。</p><p id="0772" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 4。手套模型的复杂性</strong></p><p id="1f0e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">手套模型的复杂性可以从Eqⁿ (10)的成本函数中找到。它取决于共生矩阵中非零元素的总数。求和在I和j上运行，vocab大小为V。手套模型的复杂度不超过O(|V|)。对于几十万字的语料库，|V|超过几十亿。因此，为了获得模型的精确复杂度，可以对x中非零项的总数设置更严格的界限。共生矩阵Xᵢⱼ中的项可以被建模为单词j和上下文I的频率等级(rᵢⱼ)的幂律函数</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/29cd7b7e69bbb1505f6056ee7284e9e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:192/0*zP5TrvYsDb9gMWRY"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Eqⁿ (18)</p></figure><p id="fac6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">语料库|C|中的单词总数与共现矩阵x的所有项的总和成比例</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi og"><img src="../Images/5a416afe3dbd08347a2f5f97b5af75e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:574/0*gKE7-Ypk9B6J9qQ1"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Eqⁿ (19)</p></figure><p id="e410" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">矩阵中的所有元素，无论是单词I还是上下文j，都可以通过将矩阵中的所有项(单词、上下文)放在一起考虑，从X中获得其频率排名。|X|是矩阵中任何单词/上下文的最大秩，与矩阵中非零元素的数量相同。ₓ,α是一个调和级数，比值为α，元素个数为|X|。通过将Xᵢⱼ设置为其最小值1，Eqⁿ (18)可以获得频率秩r的最大值。因此我们得到，|X|=k ^( /α)。在Eqⁿ (19)中使用这个，我们得到</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/49d1b7638eca31b62460a8c6da0fa4af.png" data-original-src="https://miro.medium.com/v2/resize:fit:314/0*wErLHs3U_mJhnfhg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Eqⁿ (20)</p></figure><p id="04c9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用广义调和数(Apostol，1976)在右侧展开H项，我们得到:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/caa8cd7bc49ca3d4681b68460088d16c.png" data-original-src="https://miro.medium.com/v2/resize:fit:550/0*hlDrpqZU9aoyYJ-3"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Eqⁿ (21岁)</p></figure><p id="876d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中ζ (α)是黎曼ζ函数。当|X|很大时，两项中只有一项是相关的，这取决于我们得到的是α&gt;1还是α&lt;1. Hence we finally arrive at,</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/71338febc8f8644c696309cd5f1530d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:624/0*4x-T7EJQo_VAfvKU"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Eqⁿ (22)</p></figure><p id="6a35" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Authors of GloVe observed that Xᵢⱼ is well modelled by setting α=1.25. When α&gt;1 |x|=o(|c|^0.8).因此，该模型的总体复杂度比O(|V|)好得多，并且略微好于用O(|C|)缩放的原始skip-gram模型。</p><p id="6789" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 5。实验</strong></p><p id="5c84" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在以下三个NLP任务上测试训练的手套单词向量。</p><p id="fdd6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">一、词语类比</strong></p><p id="d3a9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">任务是找到单词d，它回答了“a对于b就像c对于”这样的问题。数据集包含句法和语义问题。为了找到单词d，根据余弦相似性，预测最接近(Wᵇ− Wᵃ+ Wᶜ)的Wᵈ作为输出。</p><p id="ce3f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">二。单词相似度</strong></p><p id="d7c6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">任务是按照相似性的降序排列与给定单词相似的单词。Spearman等级相关用于测量模型的性能。</p><p id="ac2b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">三。命名实体识别(NER) </strong></p><p id="1cf0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">任务是为语料库中的每个标记分配实体类型。CoNLL-2003英语基准数据集有四种实体类型，即人员、位置、组织和杂项。连同记号特征一起，从手套模型训练的词向量作为输入被添加到NER模型，以生成实体类型的概率分布。</p><p id="902d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 6。手套模型训练详情</strong></p><p id="c299" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">手套模型在五个不同的语料库上训练:2010年具有10亿个标记的维基百科转储，2014年具有16亿个标记的维基百科转储，具有43亿个标记的Gigaword 5，具有60亿个标记的Gigaword5 + Wikipedia2014的组合，来自普通爬行的420亿个标记的网络数据。</p><p id="f577" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="ls">预处理步骤</em> </strong> :-语料库文本小写。<a class="ae kv" href="https://nlp.stanford.edu/software/tokenizer.html" rel="noopener ugc nofollow" target="_blank">斯坦福标记器</a>用于标记化。使用前400，000个常用词的词汇来构建共现矩阵。在构造矩阵之前，必须定义单词的上下文。对于相距d的单词，使用权重为1/d的递减加权窗口来计算矩阵值。GloVe使用单词的左标记(历史)以及使用单词的左和右标记(历史和未来)的对称上下文来探索不对称上下文的影响。</p><p id="b3af" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以下数值在手套模型训练中设定。</p><p id="5a55" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于图1所示的加权函数，Xₘ= 100，α=3/4。</p><p id="6eea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="ls">优化器</em> </strong> - <a class="ae kv" href="https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf" rel="noopener ugc nofollow" target="_blank">阿达格拉德</a>(杜奇等人，2011)初始学习率为0.05的优化器用于从x随机采样的一批非零项。对于小于300维的向量，使用50次训练数据迭代，而对于大于300维的向量，使用100次迭代。</p><p id="984f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">手套模型为每个令牌生成两个向量，W(单词向量)和W~(上下文向量)。两个向量的和(W+W~)被用作最终向量。</p><p id="22cc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 7。结果</strong></p><p id="ccaf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">一、词语类比</strong></p><p id="01dd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">各种模型的单词类比实验结果如表2所示。SG模型代表skip-gram，CBOW代表连续词袋模型，SVD代表奇异值分解。表2中显示了语义和句法以及总的准确率百分比。<strong class="ky ir">手套</strong>型号的<strong class="ky ir">性能优于</strong>两种<strong class="ky ir"> word2vec </strong>型号。在语料库大小1.6B和向量维数300上训练的手套模型已经实现了近70%的准确度，超过了skip-gram的61%和CBOW的36%。在word2vec模型中，随着向量维数从300增加到1000，语料库大小从1B增加到6B，模型准确性的提高非常显著，但与向量大小为300且语料库大小为1.6B的GloVe模型相比，仍然表现不佳。这表明即使在较小数据集上训练了300维的GloVe模型词向量，也比在大得多的数据集上训练的1000维的word2vec模型具有更有意义的信息。与主体大小的增加(从6B到42B增加了7倍)相比，手套模型的准确性增加了小幅度(75%)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/9885a00c21b3e74adb01298110a57df5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*-i_xAXQhHS_90s8WSYW7TA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd no">表2 </strong>:词语类比任务的语义、句法和整体模型准确性(Source- GloVe研究论文)</p></figure><p id="9387" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">关于对称和非对称上下文、窗口大小和向量维数的手套模型的准确性如图2所示。所有模型都在6B语料库上进行训练。在(a)中，上下文是对称的，在向量维数的所有变化中，窗口大小为10。随着向量维数从50增加到300，句法、语义以及整体准确度稳步增加，之后准确度的增加与向量维数的增加相比可以忽略不计。在(b)中，向量维数在对称上下文窗口大小的所有变化中保持为100。随着对称窗口大小的增加，与句法相比，所有的准确性随着语义的急剧增加而增加，这表明理解语义需要更宽的上下文，而短的上下文对于句法问题就足够了。在(c)中，向量大小在所有非对称窗口模型中保持为100。类似的行为在(c)中被注意到作为来自(b)的对称上下文。在对称窗口大小为3时，语义准确性超过语法准确性，而在非对称窗口中，语义准确性需要5个窗口。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/d9d0261c36858853ec7ca4c69e1bef01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ld4yL4EQGqFBlVf5dHmSdg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd no">图2 </strong> -词语类比模型精度vs向量维数、上下文和窗口大小。(来源——手套研究论文)</p></figure><p id="8791" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在图3中，显示了不同语料库上的手套模型准确度。在所有训练语料库中保持300的向量维数。句法准确率随着语料库规模的增大而稳步提高，而语义准确率却没有这种规律。因此，通过将语料库的大小从1B增加到6B和42B，总体准确度没有显著提高。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/b4799e91628ab68c9be66eb2905dc51c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1380/format:webp/1*352u4WAJKx_C2zmd_tAgBg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd no">图3 </strong> -词语类比模型准确率vs训练语料库。(来源——手套研究论文)</p></figure><p id="eb51" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">二世。</strong> <strong class="ky ir">词语相似度</strong>——各种数据集(<a class="ae kv" href="http://alfonseca.org/eng/research/wordsim353.html" rel="noopener ugc nofollow" target="_blank"> WordSim-353 </a>、<a class="ae kv" href="https://aclweb.org/aclwiki/MC-28_Test_Collection_(State_of_the_art)" rel="noopener ugc nofollow" target="_blank"> MC </a>、<a class="ae kv" href="https://aclweb.org/aclwiki/RG-65_Test_Collection_(State_of_the_art)" rel="noopener ugc nofollow" target="_blank"> RG </a>、<a class="ae kv" href="http://www.bigdatalab.ac.cn/benchmark/bm/dd?data=Stanford%20Contextual%20Word%20Similarity" rel="noopener ugc nofollow" target="_blank"> SCEW </a>和<a class="ae kv" href="https://nlp.stanford.edu/~lmthang/morphoNLM/" rel="noopener ugc nofollow" target="_blank"> RW </a>)与其他模型一起用于测试GloVe 300维词语向量。通过对词汇表中所有单词的每个向量维度进行归一化来获得相似性得分，然后对归一化向量使用余弦相似性来找到与给定单词相似的前n个单词。Spearman的等级相关系数是根据从相似性得分和人类判断获得的前n个单词的等级来计算的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/6f377dac78129bcde5a7c719c96f1154.png" data-original-src="https://miro.medium.com/v2/resize:fit:314/0*u27mMFl3cfkqntN5"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Spearman等级相关公式，其中d =模型预测等级和输出等级之间的差异，n =观察值数量-eqⁿ(23)</p></figure><p id="f1ce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在单词相似性任务的各种数据集上测试的各种模型的结果如表3所示。在WS353、MC、RG和RW数据集上，<strong class="ky ir"> GloVe </strong> model <strong class="ky ir">优于</strong> SVD、CBOW和SG。在42B语料上训练的GloVe模型比在SCWS数据集上训练的CBOW模型提高了spearman等级相关性。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi on"><img src="../Images/320afb955e0d28870a0cb352729c37a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1356/format:webp/1*eoc7hqKm0o2J3aXEcyipQQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd no">表3 </strong>:跨各种数据集的不同模型的单词相似性任务的Spearman等级相关性。(来源——手套研究论文)</p></figure><p id="6955" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">三。</strong><strong class="ky ir"/>——NER任务在不同数据集上的模型性能(验证测试集<a class="ae kv" href="https://huggingface.co/datasets/conll2003" rel="noopener ugc nofollow" target="_blank"> CoNLL-2003 </a>、<a class="ae kv" href="https://www.ldc.upenn.edu/sites/www.ldc.upenn.edu/files/edt-guidelines-v2-5.pdf" rel="noopener ugc nofollow" target="_blank"> ACE </a>和<a class="ae kv" href="https://aclweb.org/aclwiki/MUC-7_(State_of_the_art)" rel="noopener ugc nofollow" target="_blank"> MUC7 </a>)如表4所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/d1f07acf2096d8e631d388006e3ee110.png" data-original-src="https://miro.medium.com/v2/resize:fit:970/format:webp/1*SDa8t3gneSDj4o1ID2xqSQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd no">表4 </strong>:各种模型在不同数据集上的NER任务F1得分。(来源——手套研究论文)</p></figure><p id="da98" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">以上所有模型都是基于CRF的，具有不同的特征集。离散模型使用来自<a class="ae kv" href="https://nlp.stanford.edu/software/CRF-NER.html" rel="noopener ugc nofollow" target="_blank">斯坦福NER模型</a>的特征集，而其他模型使用基本特征集及其经过训练的词向量作为特征。F1分数与其他数据集的测试数据一起用于CoNLL-2003的验证和测试数据集，以比较模型性能。<strong class="ky ir"> GloVe </strong>模型在<strong class="ky ir"> NER任务</strong>中的<strong class="ky ir">表现优于</strong>所有其他模型(离散、SVD和两种word2vec模型)。</p><p id="e451" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">8。结论</p><p id="37ef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">手套模型讨论了两类算法，基于计数的算法和基于预测的算法。手套模型显示这两类方法差别不大，并且最终都使用<strong class="ky ir">共现矩阵</strong>作为底层概念来训练词向量。GloVe捕获数据中存在的全局统计和线性子结构。因此<strong class="ky ir"> GloVe </strong>是一个<strong class="ky ir">全局对数双线性模型</strong>在各种下游NLP任务上优于这两类模型。</p><p id="fc53" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你想了解更多关于如何使用python实现手套模型的知识，请在评论中告诉我。</p><p id="a67c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> 9。资源</strong></p><p id="9b64" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[1]原始研究论文——GloVe:单词表征的全局向量:<a class="ae kv" href="https://nlp.stanford.edu/pubs/glove.pdf" rel="noopener ugc nofollow" target="_blank">https://nlp.stanford.edu/pubs/glove.pdf</a><br/>【2】从词汇共现中产生高维语义空间:<a class="ae kv" href="https://link.springer.com/article/10.3758/BF03204766" rel="noopener ugc nofollow" target="_blank">https://link.springer.com/article/10.3758/BF03204766</a><br/>【3】潜在语义分析研究论文简介:<a class="ae kv" href="https://mainline.brynmawr.edu/Courses/cs380/fall2006/intro_to_LSA.pdf" rel="noopener ugc nofollow" target="_blank">https://mainline . brynmawr . edu/Courses/cs 380/fall 2006/intro _ to _ LSA . pdf</a><br/>【4】向量空间中单词表征的高效估计:<a class="ae kv" href="https://arxiv.org/pdf/1301.3781.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1301.3781.pdf【T10</a></p></div><div class="ab cl op oq hu or" role="separator"><span class="os bw bk ot ou ov"/><span class="os bw bk ot ou ov"/><span class="os bw bk ot ou"/></div><div class="ij ik il im in"><blockquote class="lt lu lv"><p id="7625" class="kw kx ls ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated"><em class="iq">感谢您抽出时间阅读帖子。我希望你喜欢这本书。请在评论中告诉我你的想法。随时联系我关于</em><a class="ae kv" href="https://www.linkedin.com/in/nikhil-birajdar/" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir"><em class="iq">LinkedIn</em></strong></a><strong class="ky ir"><em class="iq"/></strong><em class="iq">和</em><strong class="ky ir"><em class="iq"/></strong><a class="ae kv" href="mailto:nikhilbirajdar123@gmail.com" rel="noopener ugc nofollow" target="_blank"><strong class="ky ir"><em class="iq">Gmail</em></strong></a><em class="iq">。</em></p></blockquote></div></div>    
</body>
</html>