<html>
<head>
<title>Transformers Meet Active Learning: Less Data, Better Performance</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变形金刚满足主动学习:更少的数据，更好的性能</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/transformers-meet-active-learning-less-data-better-performance-4cf931517ff6?source=collection_archive---------16-----------------------#2021-10-04">https://towardsdatascience.com/transformers-meet-active-learning-less-data-better-performance-4cf931517ff6?source=collection_archive---------16-----------------------#2021-10-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9910" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">关于如何对Transformer模型使用主动学习的实践教程</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7c3a8ee1f5144769e1c8d7881d404582.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vVsHU5_es2EjchC8"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">丹·伯顿在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="41eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最近，大型语言模型(LLM)推动了许多自然语言处理(NLP)任务的发展。通常，这些LLM遵循两步框架:预训练步骤，随后是微调步骤。预训练使用大量未标记的数据来创建预训练的权重。然后，微调步骤加载这些权重，并根据来自下游任务的标记数据进行训练。LLMs可以用少量的标记数据获得良好的结果，从而缩短训练时间。然而，在现实世界中，即使注释一个很小的数据集也是很昂贵的。这不仅是一项漫长的人工工作，而且是一项复杂的任务(即，用30个类别或一个复杂的领域进行分类)，标记也不是微不足道的。例如，学习任务领域可能具有挑战性(医疗、财务)或者处理多个注释者之间的差异。因此，减少注释的数量是非常有益的，这也是主动学习(al)可以发挥作用的地方。</p><p id="e94b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇文章将向你展示如何将AL与Transformer模型一起使用，以更少的标记数据获得相似或更好的结果。然而，这篇文章中的技术适用于任何概率分类器。另外，如果你对在多任务学习环境中使用人工智能感兴趣，可以看看<a class="ae ky" href="https://openreview.net/forum?id=de11dbHzAMF" rel="noopener ugc nofollow" target="_blank">我在ICLR21 </a>上发表的论文。</p><h1 id="d5db" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">主动学习:高层次的概述</h1><p id="7f92" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在我们进入教程之前，我将概述一些高层次的概念，它们将帮助你更好地理解这篇文章的剩余部分。然而，如果你正在寻找更多关于AL的细节，我推荐<a class="ae ky" href="https://minds.wisconsin.edu/handle/1793/60660" rel="noopener ugc nofollow" target="_blank">这个文献调查。</a></p><p id="4f17" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">AL背后的核心思想是，如果算法可以选择标记样本，那么它可以使用更少的标记样本来实现更好的性能，从而减少标注工作和训练时间。此外，在标记数据昂贵或难以收集而未标记数据大量可用的情况下，人工智能是有益的。后者是大多数真实世界应用的情况，如产品评论、对话数据等。</p><p id="3dc9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在机器学习(ML)文献中有三种主要的人工智能方法:</p><ul class=""><li id="d205" class="ms mt it lb b lc ld lf lg li mu lm mv lq mw lu mx my mz na bi translated">成员查询学习</li><li id="c9f9" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">基于池的采样</li><li id="0d94" class="ms mt it lb b lc nb lf nc li nd lm ne lq nf lu mx my mz na bi translated">基于流的选择性采样</li></ul><p id="83e0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">出于本教程的考虑，我将只概述基于池的抽样，但是如果您想深入了解其他技术的细节，我邀请您查看上面链接的调查。</p><h2 id="0c04" class="ng lw it bd lx nh ni dn mb nj nk dp mf li nl nm mh lm nn no mj lq np nq ml nr bi translated">基于池的采样</h2><p id="7140" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">基于池的采样假设我们有一个用于训练初始分类器的小的初始标记数据集和一个大的未标记数据集。初始分类器使用<em class="ns">信息量</em>度量<em class="ns">从未标记集合中选择样本。</em>选定的样本随后由oracle(自动化流程或人工)标记，并添加到标记的数据中。最后，使用新的标记集学习新的模型。重复该过程，直到达到期望的性能。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/5e363516d08cf39638897d8a2a590fbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u6Or0_S_aCb2ZuWzmEarQQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由Amine Elhattami拍摄，灵感来自https://minds.wisconsin.edu/handle/1793/60660<a class="ae ky" href="https://minds.wisconsin.edu/handle/1793/60660" rel="noopener ugc nofollow" target="_blank"/></p></figure><h2 id="7b73" class="ng lw it bd lx nh ni dn mb nj nk dp mf li nl nm mh lm nn no mj lq np nq ml nr bi translated">信息含量指标</h2><p id="02a5" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">也被称为查询策略，信息量度量是一种试图帮助选择值得标记的样本<em class="ns">的度量。有多种查询策略。选择正确的将取决于您的数据和任务。在这篇文章中，我们将使用不确定性抽样。</em></p><h2 id="2f35" class="ng lw it bd lx nh ni dn mb nj nk dp mf li nl nm mh lm nn no mj lq np nq ml nr bi translated">不确定抽样</h2><p id="fd74" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">不确定性采样是一种启发式方法，用于概率分类器，以帮助选择最不确定的样本。核心直觉是，高度的不确定性预示着一个决策边界。澄清这个界限将导致学习更好的分类器。</p><p id="4115" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有多种方法可以应用不确定性采样。在这里，我将介绍熵采样，在本教程中使用的。</p><p id="bfd9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">熵采样</strong>选择使用下面的等式计算的具有最高香农熵[2]的未标记样本，其中<em class="ns"> qc </em>是样本属于类别<em class="ns"> c的概率</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/37dbfcb44ed8e2f449ff97babc8cb2d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:466/format:webp/1*LRSS_cFJsBUkGi102TKg6w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">香农熵方程</p></figure><p id="1835" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，在二进制分类中，熵采样将选择类概率接近0.5的实例，其中熵最大，如下图所示。直观上，如果该分类器预测类1的概率为0.99，类2的概率为0.01，则该模型是合理可信的。因此，注释这个特殊的样本可能对模型没有好处。相反，如果两个类别的类别概率都是0.5，则分类器是非常不确定的。因此，注释这个样本将是非常有益的(定义决策边界)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/b54c0d4308d3ee53ebb0de10153278d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*dK97epy7_DJrMwUhyuo9GA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">基于Amine Elhattami的二元分类熵</p></figure><h1 id="cffe" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">实践教程</h1><p id="1763" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在本教程中，我们将在<a class="ae ky" href="https://www.microsoft.com/en-us/download/details.aspx?id=52398" rel="noopener ugc nofollow" target="_blank">微软研究院释义语料库</a> (MRPC)任务[4]中使用带有cased BERT [3]基础模型的al。我选择了一个小模型和一个小任务，让每个人都能在几个小时左右的时间内<strong class="lb iu">复制这个教程。</strong>BERT base可以安装在几乎任何GPU上，包括免费提供的GPU(如Google colab)。至于任务，MRPC数据集并不庞大，三个历元的训练最多需要30分钟。</p><p id="e86d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于BERT实现和数据集管理，我使用了流行的<a class="ae ky" href="https://huggingface.co/transformers/" rel="noopener ugc nofollow" target="_blank"> huggingface </a>库。此外，我在这个<a class="ae ky" href="https://github.com/Am1n3e/active-learning-transformer" rel="noopener ugc nofollow" target="_blank"> git资源库</a>中共享了全部源代码，并创建了一个<a class="ae ky" href="https://colab.research.google.com/github/Am1n3e/active-learning-transformer/blob/main/active_learning_transformer.ipynb" rel="noopener ugc nofollow" target="_blank">公共Google colab笔记本</a>。</p><h2 id="2783" class="ng lw it bd lx nh ni dn mb nj nk dp mf li nl nm mh lm nn no mj lq np nq ml nr bi translated">数据集</h2><p id="7126" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">MRPC数据集由从在线新闻中提取的5801个句子对组成。这项任务的目标是预测一对句子在语义上是否等价。</p><p id="5d15" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">句子对示例:</p><p id="8910" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一句话:“纳斯达克综合指数小幅上涨1.28点，涨幅0.1%，至1766.60点，此前一周上涨3.7%。”</p><p id="1498" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第二句话:科技股纳斯达克综合指数。IXIC下跌24.44点，或1.39%，至1，739.87点。</p><p id="85df" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">MRPC任务使用两个指标。准确性和F1分数。本教程将比较评估数据集中两个分数的组合(平均值),如<a class="ae ky" href="https://gluebenchmark.com/leaderboard" rel="noopener ugc nofollow" target="_blank"> GLUE排行榜</a>所示，因为测试集标签不公开。</p><h2 id="568d" class="ng lw it bd lx nh ni dn mb nj nk dp mf li nl nm mh lm nn no mj lq np nq ml nr bi translated">基线</h2><p id="200b" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在我们开始实现和试验人工智能之前，我们需要使用整个训练数据集创建一个基线分数，以比较后来的结果。因为我们的目标是尝试al，所以我没有做任何超参数搜索。我使用了<a class="ae ky" href="https://github.com/Am1n3e/multitask-transformer/blob/initial-version/run_glue.py" rel="noopener ugc nofollow" target="_blank"> huggingface run glue示例</a>中的默认值。因此，这个基线分数绝不是我们使用所选模型所能达到的最好成绩。以下是所有实验中使用的超参数:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="0570" class="ng lw it nx b gy ob oc l od oe">per_device_train_batch_size: 32<br/>per_device_eval_batch_size: 32<br/>learning_rate: 2e-5<br/>num_train_epochs: 3<br/>seed: 12<br/>random_seed: 123</span></pre><p id="679b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下图显示了模型在所有训练样本上训练后的评估结果。最大综合得分是85.7，这是一个相当不错的分数，因为使用一个大得多的模型，当前的最先进水平是92.85(在写这篇文章的时候)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/bf9e8326cdd500f57ce22778f42e82be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*KWjI_fiORE1rYRYZKJOE4A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Amine Elhattami使用所有训练数据集的评估结果</p></figure><h2 id="e45a" class="ng lw it bd lx nh ni dn mb nj nk dp mf li nl nm mh lm nn no mj lq np nq ml nr bi translated">主动学习设置</h2><p id="9972" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">现在我们有了基线，让我们看看主动学习是否可以使用更少的标记样本获得类似或更好的结果。概括一下，我们将使用基于池的采样和Shanon熵。</p><p id="7556" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们将原始训练数据集分成两部分:一个小的初始训练数据集和一个较大的“未标记”数据集。由于我们的数据集已经被标记，我们将只从训练集中移除样本。在这个例子中，我选择初始训练数据集为原始数据集的30%。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="715b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第二步是使用我们之前创建的初始训练集来训练初始模型。然后，在评估集上记录分数，并在未标记的数据集上进行预测。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="eb46" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们计算未标记数据集预测的熵，并选择具有最高Shanon熵的<em class="ns"> topk </em>样本。<code class="fe oi oj ok nx b">query_samples_count</code>是我们想要选择的样本数量。理想情况下，这个数字应该是批量的倍数。这里我选了64。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="305e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦选择了样本，我们就请求oracle(或注释器)提供标签(在我们的例子中，我们已经有了标签)，扩展训练数据集，并从未标记的集合中删除这些样本。</p><p id="bb00" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在现实世界的应用程序中，从oracle请求标签可能需要将未标记的数据发送到您的标记平台，并保持该过程，直到数据被标记。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="6a08" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，从第二步开始重复，直到当前分数超过目标分数，或者未标记数据集为空。后一种情况，意味着你没有足够的数据来达到想要的分数，或者你可能需要做一些超参数搜索。</p><p id="f15a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">整个代码将如下所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og oh l"/></div></figure><h2 id="1c89" class="ng lw it bd lx nh ni dn mb nj nk dp mf li nl nm mh lm nn no mj lq np nq ml nr bi translated">结果呢</h2><p id="1665" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">下图显示了AL实验的结果。AL结果中的每个点(蓝色)是使用训练数据集百分比(x轴)的不同运行。橙色中的基线分数(使用100%的训练样本)用作参考。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/c2bb5a0ed8083a48d28e71e5a00c39b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*fX7Z8kNc78ez-EJch2msqA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Amine Elhattami将所有结果与基线进行比较</p></figure><p id="8da9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，所有结果都超过了使用少21%训练样本的基线结果。知道训练数据集包含3668个样本，AL避免了我们标记776个样本。如果我们假设一个注释者平均在每个样本上花费3分钟，那么就节省了38.8个工作小时(T4)。当然，这个时间将取决于任务的复杂性和注释者的知识。</p><h1 id="fa83" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">结论</h1><p id="9397" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在这篇文章中，我们看到了如何使用transformer模型进行主动学习，以更少的标记数据获得更好的结果。</p><p id="613f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里展示的用例很简单，它可能需要您为自己的应用程序做更多的调整。然而，这篇文章旨在向您展示，这很容易做到，而且值得努力，因为从长远来看，它可以节省大量时间，尤其是在数据域封闭的应用程序中。例如，当使用会话数据时，您不需要注释所有可能的打招呼方式</p><p id="cc14" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样，我已经在这个<a class="ae ky" href="https://github.com/Am1n3e/active-learning-transformer" rel="noopener ugc nofollow" target="_blank"> git仓库</a>中分享了全部源代码，并创建了一个<a class="ae ky" href="https://colab.research.google.com/github/Am1n3e/active-learning-transformer/blob/main/active_learning_transformer.ipynb" rel="noopener ugc nofollow" target="_blank">公共Google colab笔记本</a>供你尝试代码和不同的参数。</p><h1 id="9752" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">在你走之前</h1><p id="49c6" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在Twitter上关注我，我经常在Twitter上发布关于软件开发和机器学习的消息。</p><h1 id="e142" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">参考</h1><p id="4912" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">[1]毛刺解决。主动学习文献调查。计算机科学技术报告1648，威斯康星大学麦迪逊分校。2009.</p><p id="fa82" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2] C. E. Shannon，“通信的数学理论”，《贝尔系统技术杂志》，第27卷，第3期，第379-423页，1948年7月，会议名称:《贝尔系统技术杂志》。</p><p id="51f6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3] Jacob Devlin、张明蔚、Kenton Lee和Kristina Toutanova。BERT:用于语言理解的深度双向转换器的预训练。<em class="ns"> CoRR </em>，abs/1810.04805，2018。网址<a class="ae ky" href="http://arxiv.org/abs/1810.04805." rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/1810.04805.</a></p><p id="ae25" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[4]威廉·多兰和克里斯·布罗克特。自动构建句子释义语料库。《第三届国际释义研讨会论文集》(IWP2005) ，2005年。网址<a class="ae ky" href="https://www.aclweb.org/anthology/I05-5002." rel="noopener ugc nofollow" target="_blank">https://www.aclweb.org/anthology/I05-5002.</a></p></div></div>    
</body>
</html>