<html>
<head>
<title>Entropy in Soft Actor-Critic (Part 2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">软演员-评论家中的熵(下)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/entropy-in-soft-actor-critic-part-2-59821bdd5671?source=collection_archive---------29-----------------------#2021-06-07">https://towardsdatascience.com/entropy-in-soft-actor-critic-part-2-59821bdd5671?source=collection_archive---------29-----------------------#2021-06-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="2f79" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">SAC算法执行在<em class="kl">策略评估</em>和<em class="kl">策略改进</em>之间交替的迭代。在策略评估步骤中，算法根据<em class="kl">最大熵</em>和软贝尔曼方程计算<strong class="jp ir">中策略<strong class="jp ir"> 𝜋 </strong>的值。在策略改进步骤中，使用<em class="kl">最小交叉熵将目标策略更新为尽可能接近先前策略的softmax函数。</em>在该步骤中，对于每个状态，通过Kullback-Leiber散度来更新策略。</strong></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/8f3819add33b6d177e6348b6fc449fdb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8aYjIWmBUgJIgmM_ZBiDjA.jpeg"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">资料来源:123rf.com</p></figure><p id="9f39" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">查找策略</strong></p><p id="5c6f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在<a class="ae lc" rel="noopener" target="_blank" href="/entropy-in-soft-actor-critic-part-1-92c2cd3a3515">第一部分</a>中，按照SAC算法，利用<em class="kl">最大熵原理，</em>我们构造了<em class="kl">软</em>状态值函数<strong class="jp ir"><em class="kl">V</em>(<em class="kl">s</em>)</strong>，<em class="kl">软</em>动作值函数<strong class="jp ir"> <em class="kl"> Q </em> (s，<em class="kl">a【T32)</em></strong>和<strong class="jp ir">T35下面的目标是建立一个寻找最佳策略的算法<strong class="jp ir">𝜋(<em class="kl">a</em>|<em class="kl">s</em>)</strong>。为此，SAC算法使用<em class="kl"/>KL-divergence和softmax函数实现了<em class="kl">最小交叉熵原理</em>。</strong></p><h2 id="1df5" class="ld le iq bd lf lg lh dn li lj lk dp ll jy lm ln lo kc lp lq lr kg ls lt lu lv bi translated"><strong class="ak"> KL-divergence </strong></h2><p id="f1b1" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated"><strong class="jp ir">交叉熵<em class="kl"> H </em> ( <em class="kl"> p，q </em>)和KL-散度</strong></p><p id="5e91" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">设<strong class="jp ir"> <em class="kl"> p </em> </strong>和<strong class="jp ir"> <em class="kl"> q </em> </strong> <em class="kl"> </em>为事件集合上的离散概率分布{ <strong class="jp ir"> <em class="kl"> x </em> ₁ </strong>，…，<strong class="jp ir"> <em class="kl"> x_n </em> </strong> }与某个离散变量<strong class="jp ir"> <em class="kl"> X </em> </strong>分布<strong class="jp ir"><em class="kl"/></strong>相对于分布<strong class="jp ir"><em class="kl"/></strong>的<em class="kl">交叉熵</em>在给定的一组事件上定义如下:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mb"><img src="../Images/59a246e3a8135bb17e58487f7b461e38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g3S2hkew-YeFd3T4s5Au-g.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated"><strong class="bd mc">交叉熵</strong>(图片由作者提供)</p></figure><p id="a93d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里<strong class="jp ir"> E </strong> <em class="kl"> p </em> [∙]是相对于分布<strong class="jp ir"> <em class="kl"> p </em> </strong>的期望值算子。</p><p id="2430" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于概率空间相同的离散概率分布<strong class="jp ir"> <em class="kl"> p </em> </strong>和<strong class="jp ir"><em class="kl">q</em></strong><em class="kl"/><strong class="jp ir"><em class="kl">X</em></strong><em class="kl">kull back-lei bler散度(</em>或<em class="kl"> KL-divergence) </em>从<strong class="jp ir"> <em class="kl"> q </em> </strong> <em class="kl"> </em>到<strong class="jp ir"> <em class="kl"> p </em> </strong> <em class="kl"/></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi md"><img src="../Images/cd9b41d20ea574772264a46a68b92708.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gaHCB18NUc-S8Lrh0BvmNw.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated"><strong class="bd mc"> KL-divergence </strong>(图片作者提供)</p></figure><p id="c313" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">熵和交叉熵通过KL-divergence联系起来:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi me"><img src="../Images/dfcc5ead59536c102407e10384b7d7b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zJZMNmEkaqGjmlfQtd8Slg.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated"><strong class="bd mc">熵、交叉熵和KL散度</strong>(图片由作者提供)</p></figure><p id="76ba" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">KL- <em class="kl">散度</em>仅当对于所有<strong class="jp ir"> <em class="kl"> x </em> </strong>从<strong class="jp ir"><em class="kl">q</em>(<em class="kl">x</em>)= 0</strong>遵循<strong class="jp ir"> <em class="kl"> p </em> (x)=0 </strong>时定义。如果<strong class="jp ir"> <em class="kl"> p </em> (x)=0 </strong>，<strong class="jp ir"> </strong>那么根据洛必达法则，和中的相应项解释为<strong class="jp ir"> 0 </strong></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mf"><img src="../Images/dcda7debb69f157e46d411f1a3a40630.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3z8mn5s1CmaTNfJeQU4RVA.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated"><strong class="bd mc">p(x)= 0的项的贡献</strong>(图片由作者提供)</p></figure><p id="5c51" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">设<strong class="jp ir"> <em class="kl"> p </em> </strong>是固定的参考分布，那么交叉熵和KL-散度直到一个加法常数都是相同的(因为<strong class="jp ir"> <em class="kl"> p </em> </strong>是固定的)。当<strong class="jp ir"> <em class="kl"> p=q，</em> </strong> KL-divergence取<em class="kl">最小值</em> = 0，交叉熵<strong class="jp ir"> <em class="kl"> H(p，q) </em> </strong>取<em class="kl">最小值</em> <strong class="jp ir"> <em class="kl"> H(p，q) = H(p) </em> </strong>。</p><p id="0bae" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">KL-散度和距离度量</strong></p><p id="551a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">交叉熵<strong class="jp ir"> <em class="kl"> H </em> ( <em class="kl"> p </em>，<em class="kl"> q </em> ) </strong>不能作为距离度量，因为它有一个缺点就是<strong class="jp ir"><em class="kl">H</em></strong>(<strong class="jp ir"><em class="kl">p</em></strong>，<strong class="jp ir"><em class="kl">p</em></strong>)<strong class="jp ir">=<em class="kl">H</em>(KL散度不能是距离度量，因为它不是对称的，即<strong class="jp ir"><em class="kl">d</em></strong>ᴋʟ(<strong class="jp ir"><em class="kl">p</em>∩<em class="kl">q</em>)≦<em class="kl">d</em></strong>ᴋʟ<strong class="jp ir">(<em class="kl">q</em>∩<em class="kl">p【t128</em></strong></strong></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mg"><img src="../Images/ccd947439995366f72070af780c6f501.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rvO7gHS6gfLNq7TJ6pDFlA.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated"><strong class="bd mc">高斯对的KL散度，由应用程序2.1的python代码生成</strong></p></figure><p id="935c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">KL散度可以被认为是分布<strong class="jp ir"> <em class="kl"> q </em> </strong>离分布<strong class="jp ir"><em class="kl"/></strong>有多远的度量，但是它不能被用作距离度量。然而，KL散度的<em class="kl">无穷小形式</em>，所有二阶偏导数(Hessian)的矩阵构成了被称为<em class="kl">费希尔信息度量</em>的度量。根据<a class="ae lc" href="https://arxiv.org/abs/1701.08895" rel="noopener ugc nofollow" target="_blank">陈佐夫定理</a>、<a class="ae lc" href="https://arxiv.org/abs/1701.08895" rel="noopener ugc nofollow" target="_blank">、【Do17】</a>费希尔信息度量是黎曼度量的一种变体，是黎曼几何中必不可少的部分。</p><p id="7849" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">KL-散度估计的偏差</strong></p><p id="1e3a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">KL-散度可能高度依赖于样本的数量，并且当样本的数量趋于无穷大时，KL-散度估计可能不会收敛到好的结果。这方面的一个例子如图(a)所示，我们有两个固定的高斯分布:<strong class="jp ir"><em class="kl">p</em></strong><strong class="jp ir">~<em class="kl">N</em>(0，2) </strong>和<strong class="jp ir"> <em class="kl"> q ~ N </em> ( <em class="kl"> 8，2 </em> ) </strong>。然而，样本数量在区间<strong class="jp ir">【500，5000】</strong>内变化。KL散度随着样本数量的增加而线性增加。</p><p id="8ca7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果样本数量是固定的，而分布参数是变化的，则KL-散度估计可能会有很大的偏差，并且该偏差可能高度依赖于分布。在图(b)中，我们又有了两个高斯分布:<strong class="jp ir"><em class="kl">p</em></strong><strong class="jp ir">~<em class="kl">N</em>(0，2) </strong>和<strong class="jp ir"> <em class="kl"> q~ N </em> (3，std) </strong> : <code class="fe mh mi mj mk b"><strong class="jp ir"><em class="kl">mean</em></strong></code> <em class="kl"> = </em> 3和<code class="fe mh mi mj mk b"><strong class="jp ir"><em class="kl">std</em></strong></code>在区间<strong class="jp ir">【1，20】</strong>上运行。<strong class="jp ir"> </strong>在这个区间内，KL-散度变化很大。参见<a class="ae lc" href="https://ieeexplore.ieee.org/document/4595271" rel="noopener ugc nofollow" target="_blank">【Pe08】</a>、<a class="ae lc" href="https://ieeexplore.ieee.org/document/8850627" rel="noopener ugc nofollow" target="_blank">【第18期】</a>了解更多关于KL-散度估计的详细信息。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ml"><img src="../Images/baa24e408889c7ea55169f348e5662f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mLrki-vUGV0G0U3jgFFZzQ.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated"><strong class="bd mc">(a)<em class="mm">d</em></strong>ᴋʟ(<strong class="bd mc"><em class="mm">p</em>∩<em class="mm">q</em>)对样本数的依赖性，(b)<em class="mm">d</em></strong>ᴋʟ(<strong class="bd mc"><em class="mm">p</em>∩<em class="mm">q</em>对标准差的依赖性(图(a)和(b)为</strong></p></figure><h2 id="2819" class="ld le iq bd lf lg lh dn li lj lk dp ll jy lm ln lo kc lp lq lr kg ls lt lu lv bi translated">演员兼评论家</h2><p id="7b01" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated"><strong class="jp ir">确定性策略与随机策略</strong></p><p id="ea60" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于离散动作空间的情况，有一个成功的算法<em class="kl">【DQN】</em><strong class="jp ir"><em class="kl"/></strong>(深Q网)。将<em class="kl"> DQN </em>方法转移到演员-评论家架构的连续动作空间的成功尝试之一是算法<a class="ae lc" href="https://arxiv.org/abs/1509.02971" rel="noopener ugc nofollow" target="_blank"><em class="kl">【DDPG】</em></a>，其关键组件是<em class="kl"/><a class="ae lc" href="https://arxiv.org/abs/1509.02971" rel="noopener ugc nofollow" target="_blank">【Li15】</a><em class="kl">。</em>确定性策略的主要优点是，与随机策略相比，可以更有效地计算策略梯度，随机策略需要更多的样本来计算，<a class="ae lc" href="http://proceedings.mlr.press/v32/silver14.html" rel="noopener ugc nofollow" target="_blank">【Si14】</a><em class="kl">。</em></p><p id="71a7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而<em class="kl"> DDPG </em>的<em class="kl">、</em>一个<em class="kl">、一个</em>常见故障模式是学习到的<em class="kl"> Q </em>函数经常高估<em class="kl"> Q </em>值。众所周知的事实是，如果探索不够有效，就无法学习到好的政策。在[ <a class="ae lc" href="https://arxiv.org/abs/1911.11679" rel="noopener ugc nofollow" target="_blank"> Ma19 </a> ]中确立了一个不那么微不足道的事实:</p><blockquote class="mn mo mp"><p id="9bf8" class="jn jo kl jp b jq jr js jt ju jv jw jx mq jz ka kb mr kd ke kf ms kh ki kj kk ij bi translated">“…如果探索确实始终如一地发现了奖励，但发现得不够早，演员-评论家算法可能会陷入一种配置，奖励的样本会被忽略。”</p></blockquote><p id="23d6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如<a class="ae lc" href="https://arxiv.org/abs/1911.11679" rel="noopener ugc nofollow" target="_blank">【Ma19】</a>所示，对于一个确定性的策略梯度更新，即使在非常简单的环境(比如1D-托伊)的情况下，即使行为策略定期检测到奖励，演员更新也会停止。</p><p id="1361" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">随机因素和术语“软”</strong></p><p id="5f3f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在<a class="ae lc" rel="noopener" target="_blank" href="/entropy-in-soft-actor-critic-part-1-92c2cd3a3515">第1部分</a>中，我们考虑了统计力学中使用的<em class="kl">玻尔兹曼分布</em>。这是一个概率分布，给出了系统状态<strong class="jp ir"><em class="kl"/></strong>的概率<strong class="jp ir"><em class="kl"/></strong>:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mt"><img src="../Images/903b716e5ea2c66f4b29df6a38573b48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zzFE4THa5GlRTx0uvgNNKw.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated"><strong class="bd mc"> <em class="mm">玻尔兹曼分布和p </em>分区函数</strong>(图片由作者提供)</p></figure><p id="831c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在统计力学中，方程(4)的归一化分母称为<em class="kl">配分函数</em>，用<strong class="jp ir"> <em class="kl"> Z </em> </strong>表示。为了更精确地表达配分函数，我们需要使用积分，而不是求和，但是对于我们的目的，使用离散的情况就足够了。</p><p id="c839" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在概率论中，<em class="kl"> softmax函数</em> <strong class="jp ir"> <em class="kl"> p(x) </em> </strong> <em class="kl">，</em>如同玻尔兹曼分布，<em class="kl"> </em>是一个取<strong class="jp ir"><em class="kl"/></strong><em class="kl"/>实数<strong class="jp ir"><em class="kl"/></strong>其中<strong class="jp ir"> <em class="kl"> i </em> </strong>遍历事件空间<strong class="jp ir">{ 1<strong class="jp ir"/></strong></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mu"><img src="../Images/88961699d8815c5c2b7f9bce48e5257a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YuB92Ti_PW9zji1ZvuCrVQ.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated"><strong class="bd mc"> Softmax函数p(x)和分区函数<em class="mm"> Z </em> </strong>(图片由作者提供)</p></figure><p id="d5ac" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">函数<strong class="jp ir"><em class="kl"/></strong>映射<em class="kl"/><strong class="jp ir"><em class="kl">【xᵢ】</em></strong><em class="kl"/>到半开区间<strong class="jp ir"> (0，1】</strong>，所有<strong class="jp ir"><em class="kl">【p(xᵢ】</em></strong><em class="kl"/>之和为<strong class="jp ir"> 1 </strong>。由此可见，函数<strong class="jp ir"> <em class="kl"> p </em> </strong> <em class="kl"> </em>是一个<strong class="jp ir"> <em class="kl"> </em> </strong> <em class="kl">概率分布在事件空间</em><strong class="jp ir"><em class="kl"/></strong><strong class="jp ir">{ 1，<em class="kl"> … </em>，<em class="kl"> n </em> } </strong>。softmax函数也称为<em class="kl">归一化指数变换。Softmax是一种在事件之间创造一种竞争的方式。特别地，在软行动者-批评家中，随机代理使用softmax执行选择动作。Softmax将选择最大值的<a class="ae lc" href="https://proceedings.neurips.cc/paper/1989/hash/0336dcbab05b9d5ad24f4333c7658a0e-Abstract.html" rel="noopener ugc nofollow" target="_blank"> <em class="kl">赢者通吃</em> </a> <em class="kl"> </em>策略改为随机策略，其中每个动作都可以按一定概率选择。这就是“软”这个词的动机<a class="ae lc" href="https://proceedings.neurips.cc/paper/1989/hash/0336dcbab05b9d5ad24f4333c7658a0e-Abstract.html" rel="noopener ugc nofollow" target="_blank">【Br89】</a>。显然，在软演员-评论家中，术语“<em class="kl">软</em>”指的是使用了<em class="kl"> softmax </em>函数的事实。</em></p><p id="a653" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">配分函数<strong class="jp ir"> <em class="kl"> Z </em> </strong>也出现在神经网络中，并且具有与统计力学中相同的含义:离散项的和，其是如等式(5)中的softmax函数中的归一化分母。</p><p id="a0d9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">政策改进步骤</strong></p><p id="0d8b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在软行动者-批评家中，策略通过以下公式更新:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mv"><img src="../Images/f908683fec195c651c34751758e10f81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0kkLCpfTFVc-GdJfdHcqvQ.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated"><strong class="bd mc">更新政策</strong>(图片作者提供)</p></figure><p id="281e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">等式(6)中的第一个分布是分布<strong class="jp ir">𝜋</strong>'(∙|<strong class="jp ir"><em class="kl">s _ t</em></strong>)。这是高斯分布π集合中的一个分布。本质上，这种分布提供了<em class="kl">最小交叉熵。</em>与等式(2’)中KL散度的最小值<em class="kl">相同。</em></p><blockquote class="mn mo mp"><p id="a0e2" class="jn jo kl jp b jq jr js jt ju jv jw jx mq jz ka kb mr kd ke kf ms kh ki kj kk ij bi translated">“由于在实践中我们更喜欢易处理的策略，因此我们将额外将策略限制为某个策略集<em class="iq">π</em>，例如，它可以对应于一个参数化的分布族，如高斯分布、<a class="ae lc" href="https://arxiv.org/abs/1801.01290" rel="noopener ugc nofollow" target="_blank"><em class="iq">【SAC-1】</em></a><em class="iq">。</em></p></blockquote><p id="55a5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">等式(6)中的第二分布是使用softmax函数构建的，分母是配分函数(5)。为前一个策略函数<strong class="jp ir"> 𝜋_old </strong>获得的状态值函数<strong class="jp ir"> <em class="kl"> Q </em> ( <em class="kl"> s_t，aᵢ </em> ) </strong>的值构成了等式(5)中softmax函数的一组<strong class="jp ir"><em class="kl">【xᵢ</em></strong>值<strong class="jp ir"> <em class="kl"> </em> </strong>。</p><p id="aacb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">预期KL-散度</strong></p><p id="f29c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们从方程(6)转换KL散度。根据(2)，因为比率的对数是对数的差，所以KL散度看起来像这样:</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mw"><img src="../Images/7ce9bfb88e5237d83e071bb192cfd91c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1ycgj-kDQgYaHaciTIlbsA.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated"><strong class="bd mc">将KL-divergence转换为对数差</strong>(图片由作者提供)</p></figure><p id="6c45" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们的目标是最小化高斯分布π集上的KL散度。对数分区函数不依赖于动作<strong class="jp ir"><em class="kl"/></strong>和策略<strong class="jp ir">𝜋'∈</strong>π的选择，那么对数分区函数<strong class="jp ir">log<em class="kl">z</em>(<em class="kl">s</em>_<em class="kl">t</em>)</strong>可以被丢弃。此外，可以通过直接最小化方程(7)中的<em class="kl">预期KL-散度</em>乘以温度参数<em class="kl"> α </em>来学习策略参数</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mw"><img src="../Images/07969c10e4b8ec4e378be203c011527b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EyhlYWv8W0iDiBQaPrj2NQ.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated"><strong class="bd mc">最小化预期KL散度的目标函数</strong>(图片由作者提供)</p></figure><p id="7c18" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">等式(7’)用于计算<code class="fe mh mi mj mk b"><strong class="jp ir"><em class="kl">policy_loss</em></strong></code> <strong class="jp ir"> <em class="kl"> </em> </strong>张量。</p><p id="3c5c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">策略计算步骤</strong></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi mx"><img src="../Images/195be9c4ceb882e505689878137f5bf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uP_ZzCyfgRlbzBHQUyVKuQ.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated"><strong class="bd mc">策略计算步骤</strong>(图片由作者提供)</p></figure><h2 id="ff4a" class="ld le iq bd lf lg lh dn li lj lk dp ll jy lm ln lo kc lp lq lr kg ls lt lu lv bi translated">SAC实施提示</h2><p id="ab9d" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated"><strong class="jp ir">SAC代理的初始化函数</strong></p><p id="f761" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在SAC代理<code class="fe mh mi mj mk b"><strong class="jp ir">__init__</strong></code>函数中，<em class="kl"/><code class="fe mh mi mj mk b"><strong class="jp ir"><em class="kl">self.critic</em></strong></code><code class="fe mh mi mj mk b"><strong class="jp ir"><em class="kl">self.critic_target</em></strong></code><strong class="jp ir"><em class="kl"/></strong>成员<strong class="jp ir"> <em class="kl"> </em> </strong>属于类型<em class="kl">QNetwork</em><strong class="jp ir"><em class="kl"/></strong>和<code class="fe mh mi mj mk b"><strong class="jp ir"><em class="kl">self.policy</em></strong></code>属于类型<em class="kl">高斯策略</em>。<a class="ae lc" href="https://github.com/Rafael1s/Deep-Reinforcement-Learning-Algorithms/blob/master/HopperBulletEnv-v0-SAC/model.py" rel="noopener ugc nofollow" target="_blank"> model.py </a>中定义的<em class="kl"> QNetwork </em>和<strong class="jp ir"> </strong> <em class="kl">高斯策略</em>类<em class="kl"> </em>为神经网络。</p><figure class="kn ko kp kq gt kr"><div class="bz fp l di"><div class="my mz l"/></div><p class="ky kz gj gh gi la lb bd b be z dk translated"><strong class="ak">soft _ actor _ critic _ agent类的函数_ _ init _ _</strong></p></figure><p id="29f9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">政策性亏损和批评性亏损</strong></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi na"><img src="../Images/fd4d3744db4580e36fe89d8b86035557.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sAMbNZWNZFyPdvkVZoTX-A.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated"><strong class="bd mc">张量policy_loss代表eq。(7)出自</strong><a class="ae lc" href="https://arxiv.org/abs/1812.05905" rel="noopener ugc nofollow" target="_blank"><strong class="bd mc">【SAC-2】</strong></a></p></figure><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div class="ab gu cl nb"><img src="../Images/3e3ec242afc562abb33648bf29ab29ab.png" data-original-src="https://miro.medium.com/v2/format:webp/1*ZPkiza7lh0Rtq47d_eWlUg.png"/></div><p class="ky kz gj gh gi la lb bd b be z dk translated">张量Q1 _损耗和Q2 _损耗代表eq。(5)出自<a class="ae lc" href="https://arxiv.org/abs/1812.05905" rel="noopener ugc nofollow" target="_blank"><strong class="bd mc">【SAC-2】</strong></a></p></figure><ul class=""><li id="782f" class="nc nd iq jp b jq jr ju jv jy ne kc nf kg ng kk nh ni nj nk bi translated">以<code class="fe mh mi mj mk b"><strong class="jp ir"><em class="kl">state_batch</em></strong></code>为输入的<code class="fe mh mi mj mk b"><strong class="jp ir"><em class="kl">policy.sample</em></strong></code>函数返回张量<code class="fe mh mi mj mk b"><strong class="jp ir"><em class="kl">action_batch_pi</em></strong></code>和<code class="fe mh mi mj mk b"><strong class="jp ir"><em class="kl">log_pi</em></strong></code> <strong class="jp ir"> <em class="kl">。</em> </strong>两个张量都用于计算<code class="fe mh mi mj mk b"><strong class="jp ir"><em class="kl">policy_loss</em></strong></code> <strong class="jp ir"> <em class="kl">的<em class="kl"> </em>。</em> </strong></li><li id="869c" class="nc nd iq jp b jq nl ju nm jy nn kc no kg np kk nh ni nj nk bi translated">有两个(<em class="kl">评论家)</em>神经网络<code class="fe mh mi mj mk b"><strong class="jp ir"><em class="kl">Q1</em></strong></code>和<code class="fe mh mi mj mk b"><strong class="jp ir"><em class="kl">Q2</em></strong></code> <strong class="jp ir"> <em class="kl">，</em></strong><code class="fe mh mi mj mk b"><a class="ae lc" href="https://github.com/Rafael1s/Deep-Reinforcement-Learning-Algorithms/blob/master/HopperBulletEnv-v0-SAC/sac_agent.py" rel="noopener ugc nofollow" target="_blank"><strong class="jp ir"><em class="kl">QNetwork</em></strong></a></code>，<a class="ae lc" href="https://github.com/Rafael1s/Deep-Reinforcement-Learning-Algorithms/blob/master/HopperBulletEnv-v0-SAC/sac_agent.py" rel="noopener ugc nofollow" target="_blank">【hop 20】</a>两个实例。请注意，SAC(如在TD3算法中)使用两个<em class="kl">评论家</em>神经网络<em class="kl"> </em>来减轻策略改进步骤中的正偏差。</li><li id="3709" class="nc nd iq jp b jq nl ju nm jy nn kc no kg np kk nh ni nj nk bi translated"><code class="fe mh mi mj mk b"><strong class="jp ir"><em class="kl">policy.sample</em></strong></code> <strong class="jp ir"> <em class="kl"> </em> </strong>函数，以<code class="fe mh mi mj mk b"><strong class="jp ir"><em class="kl">next_state_batch</em></strong></code>为输入，返回张量<code class="fe mh mi mj mk b"><strong class="jp ir"><em class="kl">next_state_action</em></strong></code>和<code class="fe mh mi mj mk b"><strong class="jp ir"><em class="kl">next_state_log_pi</em></strong></code> <strong class="jp ir"> <em class="kl">。</em> </strong>两个张量都用于两个Q损失值<code class="fe mh mi mj mk b"><strong class="jp ir"><em class="kl"> Q1_loss, Q2_loss</em></strong></code>的计算<em class="kl"> </em>。</li><li id="3aea" class="nc nd iq jp b jq nl ju nm jy nn kc no kg np kk nh ni nj nk bi translated">两个张量<code class="fe mh mi mj mk b"><strong class="jp ir"><em class="kl">Q1_loss</em></strong></code>和<code class="fe mh mi mj mk b"><strong class="jp ir"><em class="kl">Q2_loss</em></strong></code> <strong class="jp ir"> <em class="kl"> </em> </strong>由<em class="kl"> PyTorch </em> <code class="fe mh mi mj mk b"><strong class="jp ir"><em class="kl">mse_loss() </em></strong></code>函数计算。<strong class="jp ir"><em class="kl"/></strong><strong class="jp ir"><em class="kl"/></strong>对<code class="fe mh mi mj mk b"><strong class="jp ir"><em class="kl">mse_loss()</em></strong></code>的<a class="ae lc" rel="noopener" target="_blank" href="/a-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4">是通过两种方式</a>得到的:(一)动作值函数<em class="kl"> Q(s，a) </em> <strong class="jp ir">，</strong>(二)软贝尔曼方程(<a class="ae lc" rel="noopener" target="_blank" href="/entropy-in-soft-actor-critic-part-1-92c2cd3a3515">第一部分</a>，【10】)。与<code class="fe mh mi mj mk b"><strong class="jp ir"><em class="kl">mse_loss()</em></strong></code>的自变量之差称为<em class="kl">软更夫残差</em>。</li></ul><figure class="kn ko kp kq gt kr"><div class="bz fp l di"><div class="my mz l"/></div><p class="ky kz gj gh gi la lb bd b be z dk translated"><strong class="ak">计算张量保单_损失和Q1 _损失，Q2 _损失</strong></p></figure><p id="6920" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">实施方案</strong></p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nq"><img src="../Images/d8ad767c10b038c0de63ce962093cdcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ssU6mDcSwF61GOddVpomww.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated"><strong class="bd mc">函数update_parameters </strong>中发生了什么(图片由作者提供)</p></figure><p id="f783" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">上图基本上反映了SAC算法的核心<code class="fe mh mi mj mk b"><strong class="jp ir"><em class="kl">update_parameters </em></strong></code>函数中发生的事情。</p><p id="5be9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">结论</strong> <br/>与交叉熵相关的KL散度几乎可以认为是概率分布空间上的距离度量。在无穷小形式中，KL散度甚至与黎曼几何中的某些特定距离度量相一致。这种由KL-divergence生成的“几乎”度量用于考虑高斯分布空间中的接近度，其中在改进的每一步都寻求最佳策略。事实证明，随机策略比确定性策略工作得更好，特别是存在这样的例子，其中确定性策略的最优策略梯度更新被停止，并且即使对于非常简单的环境也不能改进策略。</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nr"><img src="../Images/81972f895fd4d3d76bb17e5b84706274.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*djtBZdRVZg_sjZdQsYYe_w.jpeg"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">资料来源:123rf.com</p></figure><p id="ebc7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">第三部分会有什么？</p><p id="c16b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如何计算<strong class="jp ir">log 𝜋(<em class="kl">a</em>|<em class="kl">s</em>)</strong>，SAC上下文中的重新参数化技巧，以及其他类似的东西…</p><p id="219d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir"> App 1。吉布斯不等式</strong></p><p id="abb9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">设<strong class="jp ir"> <em class="kl"> p= </em> { <em class="kl"> p </em> ₁，<em class="kl"> … </em>，<em class="kl"> p_ </em> n} </strong>和<strong class="jp ir"><em class="kl">q =</em>{<em class="kl">q</em>₁<em class="kl">，…，q_ </em> n} <em class="kl"> </em> </strong>为两个概率分布。假设<strong class="jp ir"> <em class="kl"> I </em> </strong>是所有<strong class="jp ir"> <em class="kl"> i </em> </strong>的集合，其中<strong class="jp ir"><em class="kl"/></strong>为非零。然后</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi ns"><img src="../Images/69befbc577d6d765fbc23d5e9f0bbaee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Tgr4AkKF7O1PQf41tNsf0A.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated"><strong class="bd mc">吉布斯不等式</strong></p></figure><p id="1272" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们看看为什么吉布斯不等式成立。首先，</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nt"><img src="../Images/04f8378ce2d96cfc0856e0e34ea9f32a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CZn_Row3GAx5huPL8KcOnA.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated"><strong class="bd mc">这里，y = ln(x) </strong></p></figure><p id="e36e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然后</p><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi nu"><img src="../Images/3aaccba92054d838daa70972e345a015.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0qqWEEIi7s8eh8shC8E9Lg.png"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated"><strong class="bd mc">吉布斯不等式的证明</strong></p></figure><p id="110e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所有非零值<strong class="jp ir"> <em class="kl"> pᵢ </em> </strong>之和为<strong class="jp ir"> 1 </strong>。一些非零的<strong class="jp ir"><em class="kl">【qᵢ】</em></strong>可能被排除在总和之外，因为根据定义，我们只选择那些<strong class="jp ir"><em class="kl"/></strong>不等于零的指数。从<strong class="jp ir"> ln( <em class="kl"> x </em> ) </strong>到<strong class="jp ir">log</strong>₂<strong class="jp ir">(<em class="kl">x</em>)</strong>我们只需要用<strong class="jp ir"> ln2除所有项。</strong></p><h1 id="ecd9" class="nv le iq bd lf nw nx ny li nz oa ob ll oc od oe lo of og oh lr oi oj ok lu ol bi translated">App 2。<strong class="ak"> Python代码:KL-divergence </strong></h1><p id="f436" class="pw-post-body-paragraph jn jo iq jp b jq lw js jt ju lx jw jx jy ly ka kb kc lz ke kf kg ma ki kj kk ij bi translated"><strong class="jp ir"> App 2.1。高斯对</strong></p><figure class="kn ko kp kq gt kr"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="0c9b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">App 2.2 KL-散度对标准偏差的依赖性</strong></p><figure class="kn ko kp kq gt kr"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="1846" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><strong class="jp ir">参考文献</strong></p><p id="7f86" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[Br89] J. Bridly，<a class="ae lc" href="https://proceedings.neurips.cc/paper/1989/hash/0336dcbab05b9d5ad24f4333c7658a0e-Abstract.html" rel="noopener ugc nofollow" target="_blank">将随机模型识别算法训练为网络可以导致参数的最大互信息估计</a>，1989，NeurIPS Proceedings</p><p id="687c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[Do17] J.Dowty，<a class="ae lc" href="https://arxiv.org/abs/1701.08895" rel="noopener ugc nofollow" target="_blank">指数族的陈佐夫定理</a>，2017，arXiv</p><p id="650a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[Hop20] <a class="ae lc" href="https://github.com/Rafael1s/Deep-Reinforcement-Learning-Algorithms/blob/master/HopperBulletEnv-v0-SAC/sac_agent.py" rel="noopener ugc nofollow" target="_blank">项目—HopperBulletEnv with Soft Actor-critical(SAC)</a>，2020，github</p><p id="708b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[Li15] T.Lillicrap等，<a class="ae lc" href="https://arxiv.org/abs/1509.02971" rel="noopener ugc nofollow" target="_blank">深度强化学习的连续控制</a>，2015，arXiv</p><p id="a526" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[Ma19] G. Matheron等人，<a class="ae lc" href="https://arxiv.org/abs/1911.11679" rel="noopener ugc nofollow" target="_blank">DDPG的问题:理解具有稀疏回报的确定性环境中的失败</a>，2019，arXiv</p><p id="8c73" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[No18] Y.-K. Noh等人，<a class="ae lc" href="https://ieeexplore.ieee.org/document/8850627" rel="noopener ugc nofollow" target="_blank">kull back-lei bler散度最近邻估计的偏差减少和度量学习</a>，2018，IEEE <em class="kl"> Xplore </em></p><p id="15ff" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[Pe08] F. Perez-Cruz，<a class="ae lc" href="https://ieeexplore.ieee.org/document/4595271" rel="noopener ugc nofollow" target="_blank">连续分布的Kullback-Leibler散度估计</a>，2008，IEEE <em class="kl"> Xplore </em></p><p id="cbfb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">【Sa18】Y . Sako，<a class="ae lc" href="https://medium.com/@u39kun/is-the-term-softmax-driving-you-nuts-ee232ab4f6bd" rel="noopener">是“softmax”这个词把你逼疯了</a>，2018，Medium</p><p id="c844" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[SAC-1] T.Haarnoja等人，<a class="ae lc" href="https://arxiv.org/abs/1801.01290" rel="noopener ugc nofollow" target="_blank">软行动者-批评家:带随机行动者的离策最大熵深度强化学习</a>，2018，arXiv</p><p id="f737" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[SAC-2] T.Haarnoja等，<a class="ae lc" href="https://arxiv.org/abs/1812.05905" rel="noopener ugc nofollow" target="_blank">软演员-评论家算法及应用</a> s，2019，arXiv</p><p id="7494" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[Si14] D.Silver等人，<a class="ae lc" href="http://proceedings.mlr.press/v32/silver14.html" rel="noopener ugc nofollow" target="_blank">确定性策略梯度算法</a>，2014，DeepMind Technologies</p><p id="8126" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[St20a] R. Stekolshchik，<a class="ae lc" rel="noopener" target="_blank" href="/a-pair-of-interrelated-neural-networks-in-dqn-f0f58e09b3c4">深度Q网络中一对相互关联的神经网络</a>，2020，TDS</p><p id="b53f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[St20b] R. Stekolshchik，<a class="ae lc" rel="noopener" target="_blank" href="/three-aspects-of-deep-rl-noise-overestimation-and-exploration-122ffb4bb92b">深层RL的三个方面:噪声、高估和勘探</a>，2020，TDS</p><p id="4166" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[St21] R. Stekolshchik，<a class="ae lc" rel="noopener" target="_blank" href="/entropy-in-soft-actor-critic-part-1-92c2cd3a3515#6f9e-408b2cef46eb">软演员-评论家中的熵(上)</a>，2021年，TDS</p></div></div>    
</body>
</html>