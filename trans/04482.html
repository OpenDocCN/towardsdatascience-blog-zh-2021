<html>
<head>
<title>Implementing the Perceptron Algorithm in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Python实现感知器算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/perceptron-algorithm-in-python-f3ac89d2e537?source=collection_archive---------1-----------------------#2021-04-17">https://towardsdatascience.com/perceptron-algorithm-in-python-f3ac89d2e537?source=collection_archive---------1-----------------------#2021-04-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="dacf" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从零开始的机器学习:第6部分</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/82c4ea35674cc6b8602359a9c9ae4bba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uzm-62Wq3J1JF1HwTMY4mg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="7488" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这篇文章中，我们将看看感知器算法，它是用于<strong class="la iu">二进制分类</strong>的最<strong class="la iu">基本单层神经网络</strong>。首先，我们将查看<strong class="la iu">单位阶跃函数</strong>并查看感知器算法如何分类，然后查看<strong class="la iu">感知器更新规则</strong>。</p><p id="4abc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最后，我们将为我们的数据绘制<strong class="la iu">决策边界</strong>。我们将使用只有两个特征的数据，由于感知器是一个二元分类器，因此将有两个类。我们将使用<strong class="la iu"> Python NumPy </strong>实现所有代码，使用<strong class="la iu"> Matplotlib </strong>可视化/绘图。</p><div class="lu lv gp gr lw lx"><a rel="noopener follow" target="_blank" href="/logistic-regression-from-scratch-in-python-ec66603592e2"><div class="ly ab fo"><div class="lz ab ma cl cj mb"><h2 class="bd iu gy z fp mc fr fs md fu fw is bi translated">Python中从头开始的逻辑回归</h2><div class="me l"><h3 class="bd b gy z fp mc fr fs md fu fw dk translated">从零开始的机器学习:第5部分</h3></div><div class="mf l"><p class="bd b dl z fp mc fr fs md fu fw dk translated">towardsdatascience.com</p></div></div><div class="mg l"><div class="mh l mi mj mk mg ml ks lx"/></div></div></a></div><div class="lu lv gp gr lw lx"><a rel="noopener follow" target="_blank" href="/polynomial-regression-in-python-b69ab7df6105"><div class="ly ab fo"><div class="lz ab ma cl cj mb"><h2 class="bd iu gy z fp mc fr fs md fu fw is bi translated">用Python从头开始实现多项式回归</h2><div class="me l"><h3 class="bd b gy z fp mc fr fs md fu fw dk translated">从零开始的机器学习:第4部分</h3></div><div class="mf l"><p class="bd b dl z fp mc fr fs md fu fw dk translated">towardsdatascience.com</p></div></div><div class="mg l"><div class="mm l mi mj mk mg ml ks lx"/></div></div></a></div></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="4093" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">介绍</h1><p id="6812" class="pw-post-body-paragraph ky kz it la b lb nm ju ld le nn jx lg lh no lj lk ll np ln lo lp nq lr ls lt im bi translated">感知器算法的灵感来自大脑中的基本处理单元，称为神经元，以及它们如何处理信号。它是由弗兰克·罗森布拉特利用麦卡洛克-皮茨神经元和赫布的发现发明的。<a class="ae nr" href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.335.3398&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">感知器研究论文</a>。</p><p id="10e6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感知器算法并没有在实践中广泛使用。我们研究它主要是因为历史原因，也因为它是最基本和最简单的单层神经网络。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/4ae554fa44e2f2afea3bfc0a0b0decb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*_3whNwk2XukXjpvZ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">神经元；来源<a class="ae nr" href="https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fsimple.wikipedia.org%2Fwiki%2FNeuron&amp;psig=AOvVaw11_hIa-2-Z0MEcaHoK0ruy&amp;ust=1618665454908000&amp;source=images&amp;cd=vfe&amp;ved=0CAIQjRxqFwoTCNCU--_sgvACFQAAAAAdAAAAABAO" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="a37d" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">感知器</h1><p id="99b0" class="pw-post-body-paragraph ky kz it la b lb nm ju ld le nn jx lg lh no lj lk ll np ln lo lp nq lr ls lt im bi translated">让我们用下面的数据作为激励的例子来理解感知器算法。</p><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="bce5" class="ny mv it nu b gy nz oa l ob oc"><strong class="nu iu">from sklearn import datasets</strong></span><span id="75ef" class="ny mv it nu b gy od oa l ob oc"><strong class="nu iu">X, y = datasets.make_blobs(n_samples=150,n_features=2,<br/>                           centers=2,cluster_std=1.05,<br/>                           random_state=2)</strong></span><span id="cdbe" class="ny mv it nu b gy od oa l ob oc">#Plotting</span><span id="5b49" class="ny mv it nu b gy od oa l ob oc"><strong class="nu iu">fig = plt.figure(figsize=(10,8))</strong></span><span id="a962" class="ny mv it nu b gy od oa l ob oc"><strong class="nu iu">plt.plot(X[:, 0][y == 0], X[:, 1][y == 0], 'r^')<br/>plt.plot(X[:, 0][y == 1], X[:, 1][y == 1], 'bs')<br/>plt.xlabel("feature 1")<br/>plt.ylabel("feature 2")<br/>plt.title('Random Classification Data with 2 classes')</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/ea5bbdb62e17fd65ab0821a7323e636b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UBTpcmzpXnl1DznTAcFWGw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="2c9b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">有两个类，红色和绿色，我们想通过在它们之间画一条直线来区分它们。或者，更正式地说，我们想要学习一组参数<code class="fe of og oh nu b">theta</code>来找到一个最佳超平面(我们的数据的直线),它将两个类分开。</p><p id="a9ae" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于<a class="ae nr" href="https://medium.com/analytics-vidhya/linear-regression-from-scratch-in-python-b6501f91c82d" rel="noopener"> <strong class="la iu">线性回归</strong> </a>我们的假设(<code class="fe of og oh nu b">y_hat</code>)是<code class="fe of og oh nu b">theta.X</code>。然后，对于<a class="ae nr" rel="noopener" target="_blank" href="/logistic-regression-from-scratch-in-python-ec66603592e2"> <strong class="la iu"> Logistic回归</strong> </a>中的二元分类，我们需要输出0到1之间的概率，所以我们将假设修改为——<code class="fe of og oh nu b">sigmoid(theta.X)</code>。我们对输入要素和参数的点积应用了sigmoid函数，因为我们需要将输出压缩在0和1之间。</p><p id="7bef" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于感知器算法，我们对<code class="fe of og oh nu b">theta.X</code>应用不同的函数，即单位阶跃函数，其定义为—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/79770364938358ac5871524eeddb6f5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:372/format:webp/1*e1p4fPMT5fO5xgNi0pAvsg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:吴恩达课程</p></figure><p id="292e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在哪里，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/6ed3ec0c552acf0b877b20d7f854740c.png" data-original-src="https://miro.medium.com/v2/resize:fit:922/format:webp/1*sSxVjxnTEvGQTSHASOD68Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:吴恩达课程</p></figure><p id="6921" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">与输出0和1之间概率的<a class="ae nr" rel="noopener" target="_blank" href="/logistic-regression-from-scratch-in-python-ec66603592e2">逻辑回归</a>不同，感知器输出的值恰好是0或1。</p><p id="0799" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该函数表示，如果输出(<code class="fe of og oh nu b">theta.X</code>)大于或等于零，则模型将分类为<strong class="la iu"> 1 </strong>(例如红色)，如果输出小于零，则模型将分类为<strong class="la iu"> 0 </strong>(例如绿色)。<strong class="la iu"> </strong>感知算法就是这样分类的。</p><p id="44e8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们用图形来看单位阶跃函数—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/f3580123233e6dbe73ca590a1754d8aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:720/format:webp/0*cF7NmzL19MwQxBpO.jpg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">单位阶跃函数；<a class="ae nr" href="http://wwwf.imperial.ac.uk/metric/metric_public/functions_and_graphs/particular_functions/heaviside_im1.jpg" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="5245" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以看到对于<strong class="la iu"> z≥0 </strong>，<strong class="la iu"> g(z) = 1 </strong>和对于<strong class="la iu"> z &lt; 0 </strong>，<strong class="la iu"> g(z) = 0 </strong>。</p><p id="d910" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们编写步骤函数。</p><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="54e1" class="ny mv it nu b gy nz oa l ob oc"><strong class="nu iu">def step_func(z):<br/>        return 1.0 if (z &gt; 0) else 0.0</strong></span></pre></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="6155" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">作为神经网络的感知器</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/794a4b8bec1a2af35c88cb7f7d70ccff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xcEOFhZLyyo3uxzAt6KndQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd om">一个感知器</strong>；作者图片</p></figure><p id="002a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以通过上图直观的了解感知器。对于每个训练示例，我们首先取输入特征和参数的点积，<code class="fe of og oh nu b">theta</code>。然后，我们应用单位阶跃函数进行预测(<code class="fe of og oh nu b">y_hat</code>)。</p><p id="7ede" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果预测是错误的，或者换句话说，模型对这个例子进行了错误分类，我们对参数θ进行更新。当预测正确时(或者与真实/目标值<code class="fe of og oh nu b">y</code>相同)，我们不更新。</p><p id="2e39" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们来看看更新规则是什么。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="dc6c" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">感知器更新规则</h1><p id="3b03" class="pw-post-body-paragraph ky kz it la b lb nm ju ld le nn jx lg lh no lj lk ll np ln lo lp nq lr ls lt im bi translated">感知更新规则非常类似于梯度下降更新规则。以下是更新规则—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/02d0f1f7d9511558c2c5f6edd33251d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*E-GGbrUh1IQQGk9kmbG4OA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来源:吴恩达课程</p></figure><blockquote class="oo op oq"><p id="5611" class="ky kz or la b lb lc ju ld le lf jx lg os li lj lk ot lm ln lo ou lq lr ls lt im bi translated">请注意，即使感知器算法可能看起来类似于逻辑回归，但它实际上是一种非常不同的算法，因为很难赋予感知器的预测有意义的概率解释，或导出感知器作为最大似然估计算法。</p><p id="b33b" class="ky kz or la b lb lc ju ld le lf jx lg os li lj lk ot lm ln lo ou lq lr ls lt im bi translated"><strong class="la iu">(出自吴恩达教程)</strong></p></blockquote><p id="62c7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们来编码吧。<strong class="la iu">见评论(#)。</strong></p><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="8a15" class="ny mv it nu b gy nz oa l ob oc"><strong class="nu iu">def perceptron(X, y, lr, epochs):</strong><br/>    <br/>    # X --&gt; Inputs.<br/>    # y --&gt; labels/target.<br/>    # lr --&gt; learning rate.<br/>    # epochs --&gt; Number of iterations.<br/>    <br/>    # m-&gt; number of training examples<br/>    # n-&gt; number of features <br/><strong class="nu iu">    m, n = X.shape</strong><br/>    <br/>    # Initializing parapeters(theta) to zeros.<br/>    # +1 in n+1 for the bias term.<br/>   <strong class="nu iu"> theta = np.zeros((n+1,1))<br/>    </strong><br/>    # Empty list to store how many examples were <br/>    # misclassified at every iteration.<br/>    <strong class="nu iu">n_miss_list = []<br/>    </strong><br/>    # Training.<br/>    <strong class="nu iu">for epoch in range(epochs):</strong><br/>        <br/>        # variable to store #misclassified.<br/>        <strong class="nu iu">n_miss = 0</strong><br/>        <br/>        # looping for every example.<br/>        <strong class="nu iu">for idx, x_i in enumerate(X):</strong><br/>            <br/>            # Insering 1 for bias, X0 = 1.<br/>            <strong class="nu iu">x_i = np.insert(x_i, 0, 1).reshape(-1,1)</strong><br/>            <br/>            # Calculating prediction/hypothesis.<br/>            <strong class="nu iu">y_hat = step_func(np.dot(x_i.T, theta))<br/>            </strong><br/>            # Updating if the example is misclassified.<br/>            <strong class="nu iu">if (np.squeeze(y_hat) - y[idx]) != 0:<br/>                theta += lr*((y[idx] - y_hat)*x_i)<br/>                </strong><br/>                # Incrementing by 1.<br/>               <strong class="nu iu"> n_miss += 1</strong><br/>        <br/>        # Appending number of misclassified examples<br/>        # at every iteration.<br/>        <strong class="nu iu">n_miss_list.append(n_miss)</strong><br/>        <br/>  <strong class="nu iu">  return theta, n_miss_list</strong></span></pre></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="5867" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">绘制决策边界</h1><p id="eca2" class="pw-post-body-paragraph ky kz it la b lb nm ju ld le nn jx lg lh no lj lk ll np ln lo lp nq lr ls lt im bi translated">我们知道这个模型预测了—</p><blockquote class="ov"><p id="c6d0" class="ow ox it bd oy oz pa pb pc pd pe lt dk translated"><em class="pf"> y=1时</em>T0】</p><p id="016b" class="ow ox it bd oy oz pa pb pc pd pe lt dk translated"><em class="pf"> y=0时</em>T1】</p></blockquote><p id="67bb" class="pw-post-body-paragraph ky kz it la b lb pg ju ld le ph jx lg lh pi lj lk ll pj ln lo lp pk lr ls lt im bi translated">所以，<code class="fe of og oh nu b"><strong class="la iu">theta.X = 0</strong></code>将是我们的决策边界。</p><blockquote class="oo op oq"><p id="74ab" class="ky kz or la b lb lc ju ld le lf jx lg os li lj lk ot lm ln lo ou lq lr ls lt im bi translated">以下用于绘制决策边界的代码仅在<code class="fe of og oh nu b">X</code>中只有两个特征时有效。</p></blockquote><p id="5a0e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">见注释(#)。</strong></p><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="696b" class="ny mv it nu b gy nz oa l ob oc"><strong class="nu iu">def plot_decision_boundary(X, theta):</strong><br/>    <br/>    # X --&gt; Inputs<br/>    # theta --&gt; parameters<br/>    <br/>    # The Line is y=mx+c<br/>    # So, Equate mx+c = theta0.X0 + theta1.X1 + theta2.X2<br/>    # Solving we find m and c<br/>   <strong class="nu iu"> x1 = [min(X[:,0]), max(X[:,0])]<br/>    m = -theta[1]/theta[2]<br/>    c = -theta[0]/theta[2]<br/>    x2 = m*x1 + c</strong><br/>    <br/>    # Plotting<br/>    <strong class="nu iu">fig = plt.figure(figsize=(10,8))<br/>    plt.plot(X[:, 0][y==0], X[:, 1][y==0], "r^")<br/>    plt.plot(X[:, 0][y==1], X[:, 1][y==1], "bs")<br/>    plt.xlabel("feature 1")<br/>    plt.ylabel("feature 2")<br/>    plt.title(’Perceptron Algorithm’)</strong></span><span id="3487" class="ny mv it nu b gy od oa l ob oc"><strong class="nu iu">    plt.plot(x1, x2, 'y-')</strong></span></pre></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="0827" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">训练和绘图</h1><pre class="kj kk kl km gt nt nu nv nw aw nx bi"><span id="ed96" class="ny mv it nu b gy nz oa l ob oc"><strong class="nu iu">theta, miss_l = perceptron(X, y, 0.5, 100)</strong></span><span id="7ae9" class="ny mv it nu b gy od oa l ob oc"><strong class="nu iu">plot_decision_boundary(X, theta)</strong></span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pl"><img src="../Images/eb1058a9c40bc53ec25dab33e3931372.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b7kNF1-TcrcogZAh2i5l4Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="a741" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以从上面的决策边界图中看到，我们能够完美地分离绿色和蓝色类。也就是说，我们得到了100%的准确率。</p></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><h1 id="04ac" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">感知器算法的局限性</h1><ol class=""><li id="d36e" class="pm pn it la b lb nm le nn lh po ll pp lp pq lt pr ps pt pu bi translated">它只是一个线性分类器，不能分离不可线性分离的数据。</li><li id="a57b" class="pm pn it la b lb pv le pw lh px ll py lp pz lt pr ps pt pu bi translated">该算法仅用于二元分类问题。</li></ol></div><div class="ab cl mn mo hx mp" role="separator"><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms mt"/><span class="mq bw bk mr ms"/></div><div class="im in io ip iq"><p id="0403" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢阅读。对于问题、评论、顾虑，请在回复部分进行讨论。更多的ML从零开始即将推出。</p><p id="6e69" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">看看从零开始学习的机器系列— </strong></p><ul class=""><li id="a718" class="pm pn it la b lb lc le lf lh qa ll qb lp qc lt qd ps pt pu bi translated">第1部分:<a class="ae nr" href="https://medium.com/analytics-vidhya/linear-regression-from-scratch-in-python-b6501f91c82d?source=your_stories_page-------------------------------------" rel="noopener"><strong class="la iu">Python中从零开始的线性回归</strong> </a></li><li id="6fbc" class="pm pn it la b lb pv le pw lh px ll py lp pz lt qd ps pt pu bi translated">第二部分:<a class="ae nr" rel="noopener" target="_blank" href="/locally-weighted-linear-regression-in-python-3d324108efbf?source=your_stories_page-------------------------------------"><strong class="la iu">Python中的局部加权线性回归</strong> </a></li><li id="f057" class="pm pn it la b lb pv le pw lh px ll py lp pz lt qd ps pt pu bi translated">第三部分:<a class="ae nr" rel="noopener" target="_blank" href="/normal-equation-in-python-the-closed-form-solution-for-linear-regression-13df33f9ad71?source=your_stories_page-------------------------------------"> <strong class="la iu">使用Python的正规方程:线性回归的封闭解</strong> </a></li><li id="6225" class="pm pn it la b lb pv le pw lh px ll py lp pz lt qd ps pt pu bi translated">第四部分:<a class="ae nr" rel="noopener" target="_blank" href="/polynomial-regression-in-python-b69ab7df6105"><strong class="la iu">Python中的多项式回归从零开始</strong> </a></li><li id="f70b" class="pm pn it la b lb pv le pw lh px ll py lp pz lt qd ps pt pu bi translated">第五部分:<a class="ae nr" rel="noopener" target="_blank" href="/logistic-regression-from-scratch-in-python-ec66603592e2"><strong class="la iu">Python中的逻辑回归从零开始</strong> </a></li></ul></div></div>    
</body>
</html>