<html>
<head>
<title>BERT For Measuring Text Similarity</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于测量文本相似性的伯特</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bert-for-measuring-text-similarity-eec91c6bf9e1?source=collection_archive---------1-----------------------#2021-05-05">https://towardsdatascience.com/bert-for-measuring-text-similarity-eec91c6bf9e1?source=collection_archive---------1-----------------------#2021-05-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b19a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">高性能的BERT语义相似度</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a55deb0c3fdf5e13b17c4ada80a76ae7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wvcA77k94ImM0n_7ysijEQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者形象</p></figure><p id="86ab" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi lu translated"><span class="l lv lw lx bm ly lz ma mb mc di"> A </span>我们今天似乎要谈论的是BERT这个，BERT那个。我想写点别的东西，但是伯特太<em class="md">太</em>好了——所以这篇文章将是关于伯特<em class="md">和</em>序列相似性的！</p><p id="ac7c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">自然语言处理的很大一部分依赖于高维空间中的相似性。通常，NLP解决方案会获取一些文本，对其进行处理以创建一个表示所述文本的大向量/数组，然后执行几个转换。</p><p id="1309" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是高度立体的魔法。</p><p id="9191" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">句子相似性是高度维度魔法有多强大的最明显的例子之一。</p><p id="1716" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">逻辑是这样的:</p><ul class=""><li id="85d5" class="me mf it la b lb lc le lf lh mg ll mh lp mi lt mj mk ml mm bi translated">取一个句子，把它转换成一个向量。</li><li id="0d99" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt mj mk ml mm bi translated">取许多其他句子，并把它们转换成向量。</li><li id="c7aa" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt mj mk ml mm bi translated">找到它们之间具有最小距离(欧几里德)或最小角度(余弦相似度)的句子——更多信息请参见这里的<a class="ae ms" rel="noopener" target="_blank" href="/similarity-metrics-in-nlp-acc0777e234c"/>。</li><li id="5baa" class="me mf it la b lb mn le mo lh mp ll mq lp mr lt mj mk ml mm bi translated">我们现在有了一个衡量句子之间语义相似度的方法——简单！</li></ul><p id="8520" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从更高的层面来说，它没有太多其他的东西。当然，我们想更详细地了解正在发生的事情，并在Python中实现它！让我们开始吧。</p></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><h1 id="b23c" class="na nb it bd nc nd ne nf ng nh ni nj nk jz nl ka nm kc nn kd no kf np kg nq nr bi translated">BERT为什么会帮忙</h1><p id="ed27" class="pw-post-body-paragraph ky kz it la b lb ns ju ld le nt jx lg lh nu lj lk ll nv ln lo lp nw lr ls lt im bi translated">正如我们已经提到的，伯特是自然语言处理的最有价值球员。这在很大程度上归功于贝尔特将单词的意思嵌入到密集的向量中的能力。</p><p id="0dc3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们称它们为<em class="md">密集</em>向量，因为向量中的每个值都有一个值，并且有成为该值的原因——这与<em class="md">稀疏</em>向量形成对比，例如单热编码向量，其中大多数值为<strong class="la iu"> 0 </strong>。</p><p id="b4ee" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">BERT在创建这些密集向量方面非常出色，每个编码器层(有几个)都会输出一组密集向量。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/3f4be0847204016321c521e90da5ac81.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KOwZLqjbuxqMJAcjHzFuDA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">BERT基础网络-隐藏层表示以绿色突出显示。</p></figure><p id="8be7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于BERT基，这将是一个包含768的向量。这768个值包含我们对单个标记的数字表示，我们可以将其用作上下文单词嵌入。</p><p id="2fd7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因为这些向量中有一个用于表示每个令牌(由每个编码器输出)，所以我们实际上看到的是大小为768的张量乘以<em class="md">令牌的数量</em>。</p><p id="4d95" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以得到这些张量，并对它们进行变换，以创建输入序列的语义表示。然后，我们可以采用我们的相似性度量，计算不同序列之间的相似性。</p><p id="68db" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最简单和最常用的提取张量是<em class="md"> last_hidden_state </em>张量，它由BERT模型方便地输出。</p><p id="eeb2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当然，这是一个相当大的张量——在512x768处——我们想要一个向量来应用我们的相似性度量。</p><p id="0f92" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为此，我们需要将<em class="md"> last_hidden_states </em>张量转换为768维的向量。</p><h2 id="5239" class="ny nb it bd nc nz oa dn ng ob oc dp nk lh od oe nm ll of og no lp oh oi nq oj bi translated">创建矢量</h2><p id="222e" class="pw-post-body-paragraph ky kz it la b lb ns ju ld le nt jx lg lh nu lj lk ll nv ln lo lp nw lr ls lt im bi translated">为了将我们的<em class="md"> last_hidden_states </em>张量转换成我们的向量，我们使用了一个<strong class="la iu">均值池</strong>运算。</p><p id="7c3b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这512个标记中的每一个都有各自的768个值。这种汇集操作将取所有标记嵌入的平均值，并将它们压缩到单个768向量空间中，从而创建一个“句子向量”。</p><p id="89d8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">同时，我们也不能只把平均激活当做一回事。我们需要考虑空填充标记(我们不应该包括它)。</p></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><h1 id="1fd6" class="na nb it bd nc nd ne nf ng nh ni nj nk jz nl ka nm kc nn kd no kf np kg nq nr bi translated">用代码</h1><p id="68bd" class="pw-post-body-paragraph ky kz it la b lb ns ju ld le nt jx lg lh nu lj lk ll nv ln lo lp nw lr ls lt im bi translated">这在过程背后的理论和逻辑上是很棒的——但是我们如何在现实中应用它呢？</p><p id="acf9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将概述两种方法——简单的方法和稍微复杂一点的方法。</p><h2 id="b7f3" class="ny nb it bd nc nz oa dn ng ob oc dp nk lh od oe nm ll of og no lp oh oi nq oj bi translated">简单句子变形金刚</h2><p id="e345" class="pw-post-body-paragraph ky kz it la b lb ns ju ld le nt jx lg lh nu lj lk ll nv ln lo lp nw lr ls lt im bi translated">对我们来说，实现我们刚刚讨论的所有内容的最简单的方法是通过<code class="fe ok ol om on b">sentence-transformers</code>库——它将这个过程的大部分封装到几行代码中。</p><p id="d015" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，我们使用<code class="fe ok ol om on b">pip install sentence-transformers</code>安装句子转换器。这个库在幕后使用了HuggingFace的<em class="md">变形金刚</em>——所以我们实际上可以在这里找到<em class="md">句子——变形金刚</em>模型<a class="ae ms" href="https://huggingface.co/sentence-transformers" rel="noopener ugc nofollow" target="_blank">。</a></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="4e71" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将利用<code class="fe ok ol om on b"><a class="ae ms" href="https://huggingface.co/sentence-transformers/bert-base-nli-mean-tokens" rel="noopener ugc nofollow" target="_blank">bert-base-nli-mean-tokens</a></code>模型——它实现了我们到目前为止讨论过的相同逻辑。</p><p id="a17c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="md">(它也使用128个输入令牌，而不是512个)。</em></p><p id="3dc9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们创建一些句子，初始化我们的模型，并对句子进行编码:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oq op l"/></div></figure><p id="3e45" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">很好，我们现在有四个句子嵌入—每个包含768个值。</p><p id="f27e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们要做的是找出这些嵌入，并找出它们之间的余弦相似性。所以对于第0句:</p><blockquote class="or os ot"><p id="8377" class="ky kz md la b lb lc ju ld le lf jx lg ou li lj lk ov lm ln lo ow lq lr ls lt im bi translated">三年后，棺材里仍然装满了果冻。</p></blockquote><p id="ea89" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以用下面的句子找到最相似的句子:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oq op l"/></div></figure><p id="93d3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，这是一个更简单——更抽象的方法。七行代码来比较我们的句子。</p><h2 id="395f" class="ny nb it bd nc nz oa dn ng ob oc dp nk lh od oe nm ll of og no lp oh oi nq oj bi translated">参与—变形金刚和PyTorch</h2><p id="6944" class="pw-post-body-paragraph ky kz it la b lb ns ju ld le nt jx lg lh nu lj lk ll nv ln lo lp nw lr ls lt im bi translated">在进入第二种方法之前，值得注意的是它和第一种方法做同样的事情——但是低了一个层次。</p><p id="4a91" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用这种方法，我们需要执行到<em class="md"> last_hidden_state </em>的转换来创建句子嵌入。为此，我们执行平均池操作。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oo op l"/></div></figure><p id="8c81" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">此外，在平均池操作之前，我们需要创建<em class="md"> last_hidden_state </em>，我们这样做:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oq op l"/></div></figure><p id="72c5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我们生成密集向量<code class="fe ok ol om on b">embeddings</code>之后，我们需要执行一个<em class="md">平均池</em>操作来创建一个单一的向量编码(嵌入的<strong class="la iu">句子)。</strong></p><p id="c527" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了进行这种平均池操作，我们需要将我们的<code class="fe ok ol om on b">embeddings</code>张量中的每个值乘以其各自的<code class="fe ok ol om on b">attention_mask</code>值——这样我们就可以忽略非实数标记。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oq op l"/></div></figure><p id="dd3b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一旦我们有了密集向量，我们就可以计算每个向量之间的余弦相似性，这与我们之前使用的逻辑相同:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oq op l"/></div></figure><p id="4ceb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们返回几乎相同的结果——唯一的区别是索引<strong class="la iu">三</strong>的余弦相似性已经从<code class="fe ok ol om on b">0.5547</code>变为<code class="fe ok ol om on b">0.5548</code> —由于四舍五入的原因，这是一个微小的差异。</p></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><p id="b794" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以上是关于使用BERT测量句子语义相似性的介绍——同时使用了<em class="md">句子转换器</em>和一个带有<em class="md"> PyTorch </em>和<em class="md">转换器</em>的底层实现。</p><p id="1f13" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">您可以在此处和<a class="ae ms" href="https://github.com/jamescalam/transformers/blob/main/course/similarity/03_calculating_similarity.ipynb" rel="noopener ugc nofollow" target="_blank">处</a>找到两种方法的完整笔记本。</p><p id="c12a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我希望你喜欢这篇文章。如果您有任何问题或建议，请通过推特<a class="ae ms" href="https://twitter.com/jamescalam" rel="noopener ugc nofollow" target="_blank">或在下面的评论中告诉我。如果你对更多类似的内容感兴趣，我也会在YouTube上发布。</a></p><p id="184f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢阅读！</p></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><h1 id="bfab" class="na nb it bd nc nd ne nf ng nh ni nj nk jz nl ka nm kc nn kd no kf np kg nq nr bi translated">参考</h1><p id="8c03" class="pw-post-body-paragraph ky kz it la b lb ns ju ld le nt jx lg lh nu lj lk ll nv ln lo lp nw lr ls lt im bi translated">名词（noun的缩写）Reimers，I. Gurevych，<a class="ae ms" href="https://arxiv.org/abs/1908.10084" rel="noopener ugc nofollow" target="_blank">句子-伯特:使用连体伯特网络的句子嵌入</a> (2019)，2019年自然语言处理实证方法会议录</p><p id="8ad6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae ms" href="https://bit.ly/nlp-transformers" rel="noopener ugc nofollow" target="_blank">🤖带变压器的NLP课程</a></p></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><p id="286d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您有兴趣了解关于NLP相似性度量的更多信息(包括我们在这里使用的余弦相似性)，请查看我写的这篇解释最流行的度量的文章:</p><div class="ox oy gp gr oz pa"><a rel="noopener follow" target="_blank" href="/similarity-metrics-in-nlp-acc0777e234c"><div class="pb ab fo"><div class="pc ab pd cl cj pe"><h2 class="bd iu gy z fp pf fr fs pg fu fw is bi translated">自然语言处理中的相似性度量</h2><div class="ph l"><h3 class="bd b gy z fp pf fr fs pg fu fw dk translated">欧几里德距离、点积和余弦相似度</h3></div><div class="pi l"><p class="bd b dl z fp pf fr fs pg fu fw dk translated">towardsdatascience.com</p></div></div><div class="pj l"><div class="pk l pl pm pn pj po ks pa"/></div></div></a></div></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><p id="829d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="md">*所有图片均由作者提供，除非另有说明</em></p></div></div>    
</body>
</html>