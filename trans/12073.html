<html>
<head>
<title>Hands-On Reinforcement Learning Course: Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">实践强化学习课程:第2部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hands-on-reinforcement-learning-course-part-2-1b0828a1046b?source=collection_archive---------10-----------------------#2021-12-06">https://towardsdatascience.com/hands-on-reinforcement-learning-course-part-2-1b0828a1046b?source=collection_archive---------10-----------------------#2021-12-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="6a28" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="083f" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">表格Q学习</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/5f65daaec8c768f2bf78e1172268c9f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FyE2fZZB0sQKRAq0ULf8hg.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">来自<a class="ae lh" href="https://www.pexels.com/photo/blue-and-black-boat-on-dock-5870314/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> Pexels </a>的海伦娜·扬科维奇</p></figure><h1 id="e62d" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">欢迎来到我的强化学习课程❤️</h1><p id="96e6" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">这是我强化学习实践课程的第二部分，带你从零到英雄🦸‍♂️.</p><p id="baa2" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">如果您错过了<a class="ae lh" rel="noopener" target="_blank" href="/hands-on-reinforcement-learning-course-part-1-269b50e39d08"> <strong class="mc jd"> part 1 </strong> </a>，请阅读它，以便将强化学习术语和基础知识掌握到位。</p><p id="7987" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">今天我们要解决第一个学习问题…</p><p id="3542" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我们将训练一名代理人驾驶出租车🚕🚕🚕！</p><p id="66c3" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">嗯，一个简化版的出租车环境，但是一天结束的时候出租车。</p><p id="d94f" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我们将使用Q-learning，这是最早和最常用的RL算法之一。</p><p id="b32d" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">当然，还有Python🐍。</p><p id="e762" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">本课所有代码在<a class="ae lh" href="https://github.com/Paulescu/hands-on-rl" rel="noopener ugc nofollow" target="_blank"> <strong class="mc jd">本Github repo </strong> </a> <strong class="mc jd">中。</strong> Git克隆它，跟随今天的问题。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><a href="https://github.com/Paulescu/hands-on-rl"><div class="gh gi kr"><img src="../Images/6697f67a2e526c0b07270a54f0b47cae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Aef4_9CUysYfv18dulK5uw.jpeg"/></div></a></figure><h1 id="9389" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">第二部分</h1><h2 id="df82" class="nb lj it bd lk nc nd dn lo ne nf dp ls mj ng nh lu mn ni nj lw mr nk nl ly iz bi translated">内容</h2><ol class=""><li id="91a6" class="nm nn it mc b md me mg mh mj no mn np mr nq mv nr ns nt nu bi translated">出租车驾驶问题🚕</li><li id="cdc5" class="nm nn it mc b md nv mg nw mj nx mn ny mr nz mv nr ns nt nu bi translated">环境、行动、状态、奖励</li><li id="4384" class="nm nn it mc b md nv mg nw mj nx mn ny mr nz mv nr ns nt nu bi translated">随机代理基线🤖🍷</li><li id="9a02" class="nm nn it mc b md nv mg nw mj nx mn ny mr nz mv nr ns nt nu bi translated">q-学习代理🤖🧠</li><li id="1205" class="nm nn it mc b md nv mg nw mj nx mn ny mr nz mv nr ns nt nu bi translated">超参数调谐🎛️</li><li id="2698" class="nm nn it mc b md nv mg nw mj nx mn ny mr nz mv nr ns nt nu bi translated">重述✨</li><li id="bef9" class="nm nn it mc b md nv mg nw mj nx mn ny mr nz mv nr ns nt nu bi translated">家庭作业📚</li><li id="5aec" class="nm nn it mc b md nv mg nw mj nx mn ny mr nz mv nr ns nt nu bi translated">下一步是什么？❤️</li></ol><h1 id="f3af" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">1.出租车驾驶问题🚕</h1><p id="6b82" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">我们将使用强化学习来教一个智能体驾驶出租车。</p><p id="6e04" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">首先，在现实世界中驾驶出租车是一项非常复杂的任务。因此，我们将在一个简化的环境中工作，该环境包含一名优秀出租车司机所做的三件重要事情，即:</p><ul class=""><li id="c082" class="nm nn it mc b md mw mg mx mj oa mn ob mr oc mv od ns nt nu bi translated">接载乘客并把他们送到他们想要的目的地。</li><li id="5ee6" class="nm nn it mc b md nv mg nw mj nx mn ny mr nz mv od ns nt nu bi translated">安全驾驶，意味着没有碰撞。</li><li id="cf16" class="nm nn it mc b md nv mg nw mj nx mn ny mr nz mv od ns nt nu bi translated">在尽可能短的时间内驾驶它们。</li></ul><p id="8d96" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我们将使用OpenAI Gym中的一个环境，称为<code class="fe oe of og oh b"><a class="ae lh" href="https://gym.openai.com/envs/Taxi-v3/" rel="noopener ugc nofollow" target="_blank"><strong class="mc jd">Taxi-v3</strong></a></code>环境。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oi"><img src="../Images/05b183c5e74139748888b7f8ee1a89f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*toX5ZWcYxXtsaXE6hQJ1ag.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">出租车环境(图片由作者提供)</p></figure><p id="c433" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">在网格世界中有四个指定的位置，分别用R(ed)、G(reen)、Y(ellow)和B(lue)表示。</p><p id="1f35" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">当这一集开始时，出租车在一个随机的广场出发，乘客在一个随机的位置(R，G，Y或B)。</p><p id="7aab" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">出租车开到乘客所在地，接乘客，开到乘客的目的地(四个指定地点中的另一个)，然后让乘客下车。在这样做的时候，我们的出租车司机需要小心驾驶，以避免撞到任何墙壁，标记为<strong class="mc jd"> | </strong>。一旦乘客下车，这一集就结束了。</p><p id="3e55" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这就是我们今天将构建的q-learning代理如何驱动:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oj ok l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">优秀的出租车司机</p></figure><p id="6378" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">在我们到达那里之前，让我们很好地理解什么是这个环境的行动、状态和回报。</p><h1 id="32a6" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">2.环境、行动、状态、奖励</h1><p id="21a6" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated"><a class="ae lh" href="https://github.com/Paulescu/hands-on-rl/blob/main/01_taxi/notebooks/00_environment.ipynb" rel="noopener ugc nofollow" target="_blank">👉🏽notebooks/00 _ environment . ipynb</a></p><p id="aa47" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">让我们首先加载环境:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ol"><img src="../Images/16f8447145887398e50f7d4baff29586.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N1m2vqJWvBqSbSzib-rPqg.png"/></div></div></figure><p id="60a5" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">在每个步骤中，代理可以选择哪些<strong class="mc jd">操作</strong>？</p><ul class=""><li id="7476" class="nm nn it mc b md mw mg mx mj oa mn ob mr oc mv od ns nt nu bi translated"><code class="fe oe of og oh b">0</code>往下开</li><li id="64fd" class="nm nn it mc b md nv mg nw mj nx mn ny mr nz mv od ns nt nu bi translated"><code class="fe oe of og oh b">1</code>开车过来</li><li id="7643" class="nm nn it mc b md nv mg nw mj nx mn ny mr nz mv od ns nt nu bi translated"><code class="fe oe of og oh b">2</code>向右行驶</li><li id="4085" class="nm nn it mc b md nv mg nw mj nx mn ny mr nz mv od ns nt nu bi translated"><code class="fe oe of og oh b">3</code>向左行驶</li><li id="d3b5" class="nm nn it mc b md nv mg nw mj nx mn ny mr nz mv od ns nt nu bi translated"><code class="fe oe of og oh b">4</code>搭载乘客</li><li id="cfed" class="nm nn it mc b md nv mg nw mj nx mn ny mr nz mv od ns nt nu bi translated"><code class="fe oe of og oh b">5</code>让乘客下车</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi om"><img src="../Images/d0949dec89e91b919de021498caab137.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Pzbq6efaMcgpDtLNa80TTw.png"/></div></div></figure><p id="f942" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated"><strong class="mc jd">表示</strong>？</p><ul class=""><li id="50aa" class="nm nn it mc b md mw mg mx mj oa mn ob mr oc mv od ns nt nu bi translated">25个可能的滑行位置，因为世界是一个5x5的网格。</li><li id="e85d" class="nm nn it mc b md nv mg nw mj nx mn ny mr nz mv od ns nt nu bi translated">乘客的5个可能位置，即R、G、Y、B，加上乘客在出租车中的情况。</li><li id="8afa" class="nm nn it mc b md nv mg nw mj nx mn ny mr nz mv od ns nt nu bi translated">4个目的地位置</li></ul><p id="ad9f" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这给了我们25×5×4 = 500个状态</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi on"><img src="../Images/0322904923700571aae0acc4fbf4c645.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OJtU-V4bz8PmY4s5G6NDIw.png"/></div></div></figure><p id="bb3a" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">那么<strong class="mc jd">奖励</strong>呢？</p><ul class=""><li id="7293" class="nm nn it mc b md mw mg mx mj oa mn ob mr oc mv od ns nt nu bi translated"><strong class="mc jd"> -1 </strong>默认每步奖励。<br/> <em class="oo">为什么-1，而不是简单的0？因为我们希望通过惩罚每一个额外的步骤来鼓励代理花费最短的时间。这就是你对出租车司机的期望，不是吗？</em></li><li id="f3bb" class="nm nn it mc b md nv mg nw mj nx mn ny mr nz mv od ns nt nu bi translated"><strong class="mc jd"> +20 </strong>将乘客送到正确目的地的奖励。</li><li id="737b" class="nm nn it mc b md nv mg nw mj nx mn ny mr nz mv od ns nt nu bi translated"><strong class="mc jd"> -10 </strong>在错误地点执行取货或卸货的奖励。</li></ul><p id="b318" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">可以从<code class="fe oe of og oh b">env.P.</code>读取奖励和环境转换<em class="oo">(状态，动作)→ next_state </em></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi op"><img src="../Images/7d43a986faf594b71a6dc78843e54ee0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jYyrEwM0CfFsw7qmBhnv8g.png"/></div></div></figure><p id="db22" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">顺便说一句，你可以在每个状态下渲染环境来仔细检查这个<code class="fe oe of og oh b">env.P</code>向量是否有意义:</p><p id="5767" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">来自<code class="fe oe of og oh b">state=123</code></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oq"><img src="../Images/fc6b4982083ec500ea4086e72b35749a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*emheFKcptnVD2KrM7DrQ1g.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">州= 123</p></figure><p id="ef05" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">代理向南移动<code class="fe oe of og oh b">action=0</code>到达<code class="fe oe of og oh b">state=223</code></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi or"><img src="../Images/517169f2a93806f2f59aaaf27ba979b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t9urLESKVNKcwBKqWxmmlA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">州=223</p></figure><p id="377e" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">奖励是-1，因为这一集既没有结束，也没有司机错误地选择或放弃。</p><h1 id="37a8" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">3.随机代理基线🤖🍷</h1><p id="0990" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated"><a class="ae lh" href="https://github.com/Paulescu/hands-on-rl/blob/main/01_taxi/notebooks/01_random_agent_baseline.ipynb" rel="noopener ugc nofollow" target="_blank">👉🏽notebooks/01 _ random _ agent _ baseline . ipynb</a></p><p id="0636" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">在您开始实现任何复杂的算法之前，您应该总是建立一个基线模型。</p><p id="e47e" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这个建议不仅适用于强化学习问题，也适用于一般的机器学习问题。</p><p id="e7f8" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">直接跳到复杂/花哨的算法中是非常有诱惑力的，但是除非你真的很有经验，否则你会失败得很惨。</p><p id="b00f" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">让我们使用一个随机代理🤖🍷作为基准模型。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi os"><img src="../Images/524b70523bc6d54a042d701bc337151e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6I9oN8TQ6-cETliGzCo-yg.png"/></div></div></figure><p id="a875" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我们可以看到这个代理对于给定的初始<code class="fe oe of og oh b">state=198</code>的表现</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ot"><img src="../Images/38f2ba532e3179f49668b437966aea3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BhfZjfQ-q4Gnzcl501RUNg.png"/></div></div></figure><p id="99cb" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">3804步很多！😵</p><p id="0ab2" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">请在此视频中亲自观看:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oj ok l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">行动中的随机代理</p></figure><p id="3408" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">为了获得更有代表性的性能度量，我们可以重复相同的评估循环<code class="fe oe of og oh b">n=100</code>次，每次从随机状态开始。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ou"><img src="../Images/a632b63b82c716f40f5c1f4c536f2e14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rGG0oYxuUioE-bVIgc2jhg.png"/></div></div></figure><p id="2b3d" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">如果你画出<code class="fe oe of og oh b">timesteps_per_episode</code>和<code class="fe oe of og oh b">penalties_per_episode</code>，你可以观察到它们都没有随着代理完成更多的剧集而减少。换句话说，代理没有学到任何东西。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ov"><img src="../Images/fec383908ccfd20b2a919c81775d81a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aPZL0OuLh-1XX6RNgwVL0Q.png"/></div></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ow"><img src="../Images/3fe59f8076fb744e29e7cec39ceafa0b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KT4vHaoewkPMm7dO-3mrRg.png"/></div></div></figure><p id="58f0" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">如果您想要性能的汇总统计，您可以取平均值:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ox"><img src="../Images/72700665566343b02e6ab50c941cb606.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G_3Efv821ncbYJUSuNKJAg.png"/></div></div></figure><p id="903c" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">实现代理学习是强化学习的目标，也是本课程的目标。</p><p id="8011" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">让我们使用Q-learning实现我们的第一个“智能”代理，Q-learning是现存的最早和最常用的RL算法之一。</p><h1 id="4d9f" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">4.q-学习代理🤖🧠</h1><p id="48e8" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated"><a class="ae lh" href="https://github.com/Paulescu/hands-on-rl/blob/main/01_taxi/notebooks/02_q_agent.ipynb" rel="noopener ugc nofollow" target="_blank">👉🏽notebooks/02_q_agent.ipynb </a></p><p id="bd82" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated"><a class="ae lh" href="https://www.gatsby.ucl.ac.uk/~dayan/papers/cjch.pdf" rel="noopener ugc nofollow" target="_blank"> Q-learning </a> (by <a class="ae lh" href="http://www.cs.rhul.ac.uk/~chrisw/" rel="noopener ugc nofollow" target="_blank"> <strong class="mc jd">克里斯·沃金斯</strong> </a> 🧠和<a class="ae lh" href="https://en.wikipedia.org/wiki/Peter_Dayan" rel="noopener ugc nofollow" target="_blank"> <strong class="mc jd">彼得·达扬</strong> </a> 🧠)是一种寻找最优q值函数的算法。</p><p id="f9e2" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">正如我们在<a class="ae lh" rel="noopener" target="_blank" href="/hands-on-reinforcement-learning-course-part-1-269b50e39d08">第1部分</a>中所说的，与策略<strong class="mc jd"> π <em class="oo"> </em> </strong>相关联的Q值函数<strong class="mc jd"> Q(s，a) </strong>是代理人在状态<strong class="mc jd"> s </strong>采取行动<strong class="mc jd"> a </strong>并随后遵循策略<strong class="mc jd"> π </strong>时期望得到的总报酬。</p><p id="3576" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">最优Q值函数<strong class="mc jd"> Q*(s，a) <em class="oo"> </em> </strong>是与最优策略<strong class="mc jd"> π*相关联的Q值函数。</strong></p><p id="55bb" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">如果你知道<strong class="mc jd"> Q*(s，a) </strong>你可以推断π*:也就是说，你选择一个最大化当前状态s的Q*(s，a)的动作作为下一个动作。</p><p id="b4bb" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">Q-learning是一种迭代算法，从任意初始猜测<strong class="mc jd"> Q⁰(s，a) </strong>开始，计算最佳q值函数<strong class="mc jd"> Q*(s，a) </strong>的越来越好的近似值</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oy"><img src="../Images/d97f3e15ad511f3710c19f43827725be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*323DNjYPk23v3Mpq.png"/></div></div></figure><p id="29a5" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">在类似于<code class="fe oe of og oh b">Taxi-v3</code>的表格环境中，状态和动作的数量有限，q函数本质上是一个矩阵。它的行数与状态数一样多，列数与动作数一样多，即500 x 6。</p><p id="1f3c" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">好的，<em class="oo">但是你如何准确地从Q⁰(s，a)计算下一个近似值Q (s，a)？</em></p><p id="0530" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这是Q-learning的关键公式:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oz"><img src="../Images/9cbf9c91fb5a1010f80bf759fdf702de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ODDuOVNWybX6g0CI4i2Uiw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">q-学习公式(图片由作者提供)</p></figure><p id="0ab5" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">当我们的q-agent导航环境并观察下一个状态<strong class="mc jd"><em class="oo">s’</em></strong>和奖励<strong class="mc jd"> <em class="oo"> r </em> </strong>时，你用这个公式更新你的q-值矩阵。</p><p id="770a" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated"><em class="oo">这个公式中的学习率</em> <strong class="mc jd"> 𝛼 </strong> <em class="oo">是多少？</em></p><p id="e00a" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated"><strong class="mc jd">学习率</strong>(通常在机器学习中)是一个小数字，它控制q函数的更新量。你需要调整它，因为太大的值会导致不稳定的训练，太小的值可能不足以避开局部最小值。</p><p id="26f0" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated"><em class="oo">还有这个折现因子</em> <strong class="mc jd"> <em class="oo"> 𝛾 </em> </strong> <em class="oo">？</em></p><p id="7a05" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated"><strong class="mc jd">折扣因子</strong>是一个介于0和1之间的(超)参数，它决定了相对于近期的回报，我们的代理人对远期回报的关心程度。</p><ul class=""><li id="6831" class="nm nn it mc b md mw mg mx mj oa mn ob mr oc mv od ns nt nu bi translated">当𝛾=0时，代理人只关心眼前报酬的最大化。正如生活中发生的那样，最大化眼前回报并不是获得最佳长期结果的最佳方法。这也发生在RL特工身上。</li><li id="1494" class="nm nn it mc b md nv mg nw mj nx mn ny mr nz mv od ns nt nu bi translated">当𝛾=1时，代理人根据其所有未来报酬的总和来评估其每一个行为。在这种情况下，代理人同等重视眼前的回报和未来的回报。</li></ul><p id="ba70" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">折扣因子通常是中间值，例如0.6。</p><p id="e4bb" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">总而言之，如果你</p><ul class=""><li id="6be9" class="nm nn it mc b md mw mg mx mj oa mn ob mr oc mv od ns nt nu bi translated">训练足够长的时间</li><li id="751d" class="nm nn it mc b md nv mg nw mj nx mn ny mr nz mv od ns nt nu bi translated">有着不错的学习率和折扣系数</li><li id="cd7f" class="nm nn it mc b md nv mg nw mj nx mn ny mr nz mv od ns nt nu bi translated">代理在状态空间中探索了足够多的时间</li><li id="4848" class="nm nn it mc b md nv mg nw mj nx mn ny mr nz mv od ns nt nu bi translated">你用Q学习公式更新Q值矩阵</li></ul><p id="b823" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">你的初始近似最终会收敛到最优q矩阵。瞧啊。</p><p id="d17b" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">那么让我们为Q-agent实现一个Python类。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pa"><img src="../Images/6f6384bacc5d518980a833400ef9c9ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QGxq4MhDX3FNCEhmdp6ukQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">Q-agent的API</p></figure><p id="d222" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">它的API与上面的<code class="fe oe of og oh b">RandomAgent</code>相同，但是多了一个方法<code class="fe oe of og oh b">update_parameters()</code>。该方法采用转移向量<code class="fe oe of og oh b">(state, action, reward, next_state)</code>，并使用上面的Q学习公式更新Q值矩阵近似值<code class="fe oe of og oh b">self.q_table</code>。</p><p id="1dcd" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">现在，我们需要将这个代理插入到一个训练循环中，并在代理每次收集新的经验时调用它的<code class="fe oe of og oh b">update_parameters()</code>方法。</p><p id="0890" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">此外，记住我们需要保证代理充分探索状态空间。还记得我们在<a class="ae lh" rel="noopener" target="_blank" href="/hands-on-reinforcement-learning-course-part-1-269b50e39d08">第一部分</a>中谈到的勘探-开采参数吗？这就是<code class="fe oe of og oh b">epsilon</code>参数进入游戏的时候。</p><p id="1016" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">让我们为<code class="fe oe of og oh b">n_episodes = 10,000</code>培训代理，并使用<code class="fe oe of og oh b">epsilon = 10%</code></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pb"><img src="../Images/bda3358d5ba06a4ba7b90b950b8c13a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mKmMUC5n_5NDT6WBt6_ivw.png"/></div></div></figure><p id="0854" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">还有剧情<code class="fe oe of og oh b">timesteps_per_episode</code>和<code class="fe oe of og oh b">penalties_per_episode</code></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pc"><img src="../Images/f67f2abb5a1c62f2bd33cb7442db9eb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*omYCRiNueImrF2blCCYt2Q.png"/></div></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pd"><img src="../Images/11df653c641fd4e2846cca77fc3cc7d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wr77nLXa396g4G9B7WH3Xg.png"/></div></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pe"><img src="../Images/0d9ee35b8a34aabb9db46a2315a9b968.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fOX9IH5wWRGOV4tX9EXWMQ.png"/></div></div></figure><p id="eb91" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">不错！这些图表看起来比<code class="fe oe of og oh b">RandomAgent</code>好得多。这两个指标都随着训练而下降，这意味着我们的代理正在学习🎉🎉🎉。</p><p id="e20f" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我们实际上可以看到代理是如何从与我们用于<code class="fe oe of og oh b">RandomAgent</code>相同的<code class="fe oe of og oh b">state = 123</code>开始驱动的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pf"><img src="../Images/0b7b1d2284810094dd66b4ca460e4155.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_MLHTX9OKPc9yxDgZUofMA.png"/></div></div></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="oj ok l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">好棒的车！</p></figure><p id="a599" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">如果你想比较硬数字，你可以评估q-agent的性能，比如说，100个随机事件，并计算时间戳和招致的惩罚的平均数。</p><h2 id="5536" class="nb lj it bd lk nc nd dn lo ne nf dp ls mj ng nh lu mn ni nj lw mr nk nl ly iz bi translated">一点点关于ε贪婪政策</h2><p id="1475" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">评估代理时，使用正的<code class="fe oe of og oh b">epsilon</code>值而不是<code class="fe oe of og oh b">epsilon = 0.</code>值仍然是一个好的做法</p><p id="755f" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">为什么会这样？我们的特工不是训练有素吗？为什么我们在选择下一个行动时需要保留这种随机性的来源？</p><p id="ea23" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">原因是防止过度拟合。即使对于这样一个小州，在<code class="fe oe of og oh b">Taxi-v3</code>(即500 x 6)中的行动空间，很可能在培训期间我们的代理没有访问足够的特定州。</p><p id="463f" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">因此，它在这些状态下的性能可能不是100%最佳的，导致代理“陷入”次优操作的几乎无限循环中。</p><p id="f750" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">如果epsilon是一个小正数(例如5%)，我们可以帮助代理摆脱这些次优行为的无限循环。</p><p id="5a9e" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">通过在评估时使用小ε，我们采用了所谓的<strong class="mc jd">ε贪婪策略</strong>。</p><p id="b577" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">让我们使用<code class="fe oe of og oh b">epsilon = 0.05.</code>来评估我们在<code class="fe oe of og oh b">n_episodes = 100</code>上训练过的代理，观察这个循环看起来几乎与上面的训练循环完全一样，但是没有调用<code class="fe oe of og oh b">update_parameters()</code></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pg"><img src="../Images/2e28c9d59d7813eba159961633cd515c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bOzCK6mGz1JQatxTw2H0mQ.png"/></div></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ph"><img src="../Images/d3dd83693c5f80b250e75f489afe3235.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RVG8QSITN4SMI78eJmvMag.png"/></div></div></figure><p id="8a34" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这些数字看起来比<code class="fe oe of og oh b">RandomAgent.</code>好得多</p><p id="80ba" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我们可以说我们的代理已经学会开出租车了！</p><p id="2f48" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">q学习为我们提供了一种计算最优q值的方法。但是，<em class="oo">超参数</em> <code class="fe oe of og oh b">alpha</code>、<code class="fe oe of og oh b">gamma</code>和<code class="fe oe of og oh b">epsilon</code>呢？</p><p id="4d0f" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我为你选择了它们，相当随意。但是在实践中，您将需要针对您的RL问题来调整它们。</p><p id="9d75" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">让我们探索它们对学习的影响，以获得对正在发生的事情的更好的直觉。</p><h1 id="86a7" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">5.超参数调谐🎛️</h1><p id="8b95" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated"><a class="ae lh" href="https://github.com/Paulescu/hands-on-rl/blob/main/01_taxi/notebooks/03_q_agent_hyperparameters_analysis.ipynb" rel="noopener ugc nofollow" target="_blank">👉🏽notebooks/03 _ q _ agent _ hyperparameters _ analysis . ipynb</a></p><p id="888b" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">让我们使用不同的值<code class="fe oe of og oh b">alpha</code>(学习率)和<code class="fe oe of og oh b">gamma</code>(折扣因子)来训练我们的q-agent。至于T2，我们保持在10%。</p><p id="9fa6" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">为了保持代码整洁，我将q-agent定义封装在<code class="fe oe of og oh b"><a class="ae lh" href="https://github.com/Paulescu/hands-on-rl/blob/50c61a385bbd511a6250407ffb1fcb59fbfb983f/01_taxi/src/q_agent.py#L4" rel="noopener ugc nofollow" target="_blank">src/q_agent.py</a></code>中，并将训练循环封装在<code class="fe oe of og oh b"><a class="ae lh" href="https://github.com/Paulescu/hands-on-rl/blob/50c61a385bbd511a6250407ffb1fcb59fbfb983f/01_taxi/src/loops.py#L9" rel="noopener ugc nofollow" target="_blank">src/loops.py</a></code>的<code class="fe oe of og oh b">train()</code>函数中</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pi"><img src="../Images/07ef700f8ea9c381c14a2768cc373284.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZM9Kdp6873PF-9Xaj_Si7w.png"/></div></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pi"><img src="../Images/9c2ee14629a0f0b40edf5a5628af1477.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DTHu3SJZAliaBF8GFllwaA.png"/></div></div></figure><p id="ef74" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">让我们为每个超参数组合绘制每集的<code class="fe oe of og oh b">timesteps</code>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pj"><img src="../Images/b18dbe938755fe97670fb9d355c117b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v43PqgL_ALNjaujkqF5OzQ.png"/></div></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pk"><img src="../Images/29a61ad93d809d507f2aa88af59f91a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W5W7_XdTYUGnwaKh5infjA.png"/></div></div></figure><p id="8429" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这张图看起来很艺术，但有点太吵杂了😵。</p><p id="c07e" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">不过你可以观察到，当<code class="fe oe of og oh b">alpha = 0.01</code>时，学习速度变慢。<code class="fe oe of og oh b">alpha</code>(学习率)控制我们在每次迭代中更新多少q值。值太小意味着学习速度较慢。</p><p id="d9d4" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">让我们放弃<code class="fe oe of og oh b">alpha = 0.01</code>，对每个超参数组合进行10次训练。我们使用这10次运行，对从1到1000的每个剧集编号的<code class="fe oe of og oh b">timesteps</code>进行平均。</p><p id="dd31" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我在<code class="fe oe of og oh b">src/loops.py</code>中创建了函数<code class="fe oe of og oh b"><a class="ae lh" href="https://github.com/Paulescu/hands-on-rl/blob/50c61a385bbd511a6250407ffb1fcb59fbfb983f/01_taxi/src/loops.py#L121" rel="noopener ugc nofollow" target="_blank">train_many_runs()</a></code>来保持笔记本代码的整洁:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pl"><img src="../Images/33e00aabdf4604f4c319eabeb8da3906.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*V7E3BAwAHLQ55usXcU93eQ.png"/></div></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pm"><img src="../Images/5c03b083786ed30400f638e9eaa7d776.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qoEuloRv2VNAKtq7ux_XVQ.png"/></div></div></figure><p id="dfda" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">看起来<code class="fe oe of og oh b">alpha = 1.0</code>是效果最好的值，而<code class="fe oe of og oh b">gamma</code>似乎影响较小。</p><p id="2b28" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">恭喜你！🥳，你已经在本课程中调整了你的第一个学习率</p><p id="242f" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">调整超参数既费时又乏味。有一些优秀的库可以自动完成我们刚刚完成的手动过程，比如<a class="ae lh" href="https://optuna.org/" rel="noopener ugc nofollow" target="_blank"><strong class="mc jd">【Optuna】</strong></a>，但是这是我们将在课程的后面部分使用的。暂时享受一下我们刚刚发现的训练提速。</p><p id="587b" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">等等，我告诉你要相信我的这个<code class="fe oe of og oh b">epsilon = 10%</code>怎么了？</p><p id="b8cf" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">现在的10%值是最好的吗？</p><p id="d150" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我们自己检查一下。</p><p id="5423" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我们选择找到的最好的<code class="fe oe of og oh b">alpha</code>和<code class="fe oe of og oh b">gamma</code>，即</p><ul class=""><li id="e85e" class="nm nn it mc b md mw mg mx mj oa mn ob mr oc mv od ns nt nu bi translated"><code class="fe oe of og oh b">alpha = 1.0</code></li><li id="51e8" class="nm nn it mc b md nv mg nw mj nx mn ny mr nz mv od ns nt nu bi translated"><code class="fe oe of og oh b">gamma = 0.9</code>(我们也可以选择<code class="fe oe of og oh b">0.1</code>或<code class="fe oe of og oh b">0.6</code>)</li></ul><p id="dd6d" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">和训练用不同的<code class="fe oe of og oh b">epsilons = [0.01, 0.1, 0.9]</code></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pn"><img src="../Images/777a87e67f9cced1102be66d1d6fcaa8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NzaRMAkn6l7e6j-8S9fNDg.png"/></div></div></figure><p id="5c46" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">并绘制产生的<code class="fe oe of og oh b">timesteps</code>和<code class="fe oe of og oh b">penalties</code>曲线:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi po"><img src="../Images/946fa325223ad2390ddb6b33a55a203e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x5Mv-TOmD4EmVeKYf4r6xw.png"/></div></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pp"><img src="../Images/f912a644abb25726d8a1032316f3b7e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gpEZua6ILiJ2QR4wClD3Dg.png"/></div></div></figure><p id="a552" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">如你所见，<code class="fe oe of og oh b">epsilon = 0.01</code>和<code class="fe oe of og oh b">epsilon = 0.1</code>似乎都工作得很好，因为它们在勘探和开发之间取得了恰当的平衡。</p><p id="bcfb" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">另一方面，<code class="fe oe of og oh b">epsilon = 0.9</code>是一个太大的值，导致训练过程中“太多”的随机性，并阻止我们的q矩阵收敛到最优值。观察性能如何稳定在每集大约<code class="fe oe of og oh b">250 timesteps</code>的水平。</p><p id="fe77" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">一般来说，选择<code class="fe oe of og oh b">epsilon</code>超参数的最佳策略是<strong class="mc jd">渐进ε衰变</strong>。也就是说，在训练开始时，当代理对其q值估计非常不确定时，最好访问尽可能多的州，为此，大的<code class="fe oe of og oh b">epsilon</code>是很好的(例如50%)</p><p id="d3f5" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">随着训练的进行，代理改进了它的q值估计，探索那么多不再是最佳的。相反，通过减少<code class="fe oe of og oh b">epsilon</code>，代理可以学习完善和微调q值，使它们更快地收敛到最优值。太大的<code class="fe oe of og oh b">epsilon</code>会导致我们看到的<code class="fe oe of og oh b">epsilon = 0.9</code>的收敛问题。</p><p id="cddc" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我们会在课程中不断调整，所以我暂时不会坚持太多。再次，享受我们今天所做的。这是非常了不起的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pq"><img src="../Images/bd4d99931917f29d2858316ce1294acf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MZkWEiKneJzfnbNLafj6sg.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">B-R-A-V-O！(图片由作者提供)</p></figure><h1 id="1dcb" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">6.重述✨</h1><p id="a888" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">恭喜你(可能)解决了你的第一个强化学习问题。</p><p id="d194" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这些是我希望你能记住的关键知识:</p><ul class=""><li id="335b" class="nm nn it mc b md mw mg mx mj oa mn ob mr oc mv od ns nt nu bi translated">强化学习问题的难度与可能的动作和状态的数量直接相关。<code class="fe oe of og oh b">Taxi-v3</code>是一个表格环境(即有限数量的状态和动作)，所以它是一个简单的环境。</li><li id="3810" class="nm nn it mc b md nv mg nw mj nx mn ny mr nz mv od ns nt nu bi translated">Q-learning是一种学习算法，非常适合表格环境。</li><li id="c985" class="nm nn it mc b md nv mg nw mj nx mn ny mr nz mv od ns nt nu bi translated">无论您使用什么RL算法，都有一些超参数需要调整，以确保您的代理了解最佳策略。</li><li id="c89f" class="nm nn it mc b md nv mg nw mj nx mn ny mr nz mv od ns nt nu bi translated">调整超参数是一个耗时的过程，但必须确保我们的代理了解。随着课程的进展，我们会在这方面做得更好。</li></ul><h1 id="09a5" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">7.家庭作业📚</h1><p id="c75f" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated"><a class="ae lh" href="https://github.com/Paulescu/hands-on-rl/blob/main/01_taxi/notebooks/04_homework.ipynb" rel="noopener ugc nofollow" target="_blank">👉🏽notebooks/04_homework.ipynb </a></p><p id="ac0b" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">这是我要你做的:</p><ol class=""><li id="d79f" class="nm nn it mc b md mw mg mx mj oa mn ob mr oc mv nr ns nt nu bi translated"><a class="ae lh" href="https://github.com/Paulescu/hands-on-rl" rel="noopener ugc nofollow" target="_blank"> <strong class="mc jd"> Git克隆</strong> </a>把repo到你的本地机器上。</li><li id="237d" class="nm nn it mc b md nv mg nw mj nx mn ny mr nz mv nr ns nt nu bi translated"><a class="ae lh" href="https://github.com/Paulescu/hands-on-rl/tree/main/01_taxi#quick-setup" rel="noopener ugc nofollow" target="_blank"> <strong class="mc jd">设置</strong> </a>本课的环境<code class="fe oe of og oh b">01_taxi.</code></li><li id="1fb3" class="nm nn it mc b md nv mg nw mj nx mn ny mr nz mv nr ns nt nu bi translated">打开<code class="fe oe of og oh b"><a class="ae lh" href="https://github.com/Paulescu/hands-on-rl/blob/main/01_taxi/notebooks/04_homework.ipynb" rel="noopener ugc nofollow" target="_blank">01_taxi/otebooks/04_homework.ipynb</a></code>并尝试完成2个挑战。</li></ol><p id="94a0" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我称之为挑战(不是练习)，因为它们并不容易。我希望你尝试它们，弄脏你的手，并且(可能)成功。</p><p id="fc22" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">在第一个挑战中，我谅你也不敢更新<code class="fe oe of og oh b">train()</code>函数<code class="fe oe of og oh b">src/loops.py</code>来接受一个依赖于剧集的epsilon。</p><p id="173b" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">在第二个挑战中，我希望您升级Python技能并实现并行处理，以加快超参数实验。</p><p id="5339" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">像往常一样，如果你遇到困难，需要反馈，请给我写信<code class="fe oe of og oh b">plabartabajo@gmail.com.</code></p><p id="a5e1" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">我将非常乐意帮助你。</p><h1 id="9113" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">8.下一步是什么？❤️</h1><p id="39e8" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">在下一部分，我们将解决一个新的RL问题。</p><p id="a78c" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">更难的一个。</p><p id="893e" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">使用新的RL算法。</p><p id="187f" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">有很多蟒蛇。</p><p id="234d" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">还会有新的挑战。</p><p id="197d" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">而且好玩！</p><p id="5521" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">回头见！</p><h1 id="9b59" class="li lj it bd lk ll lm ln lo lp lq lr ls ki lt kj lu kl lv km lw ko lx kp ly lz bi translated">想支持我吗？</h1><p id="0e29" class="pw-post-body-paragraph ma mb it mc b md me kd mf mg mh kg mi mj mk ml mm mn mo mp mq mr ms mt mu mv im bi translated">你喜欢阅读和学习关于ML、AI和数据科学的知识吗？</p><p id="76e6" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">无限制地访问我在Medium上发布的所有内容，并支持我的写作。</p><p id="774d" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">👉🏽今天使用我的<a class="ae lh" href="https://pau-labarta-bajo.medium.com/membership" rel="noopener"> <strong class="mc jd">推荐链接</strong> </a>成为会员。</p><div class="pr ps gp gr pt pu"><a href="https://pau-labarta-bajo.medium.com/membership" rel="noopener follow" target="_blank"><div class="pv ab fo"><div class="pw ab px cl cj py"><h2 class="bd jd gy z fp pz fr fs qa fu fw jc bi translated">成为一个媒体成员来阅读我在媒体上分享的一切。</h2><div class="qb l"><h3 class="bd b gy z fp pz fr fs qa fu fw dk translated">你的会员费的一部分给了所有你喜欢阅读的作家。希望是我。</h3></div><div class="qc l"><p class="bd b dl z fp pz fr fs qa fu fw dk translated">pau-labarta-bajo.medium.com</p></div></div><div class="qd l"><div class="qe l qf qg qh qd qi lb pu"/></div></div></a></div><p id="ccdc" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">👉🏽订阅<a class="ae lh" href="https://datamachines.xyz/subscribe/" rel="noopener ugc nofollow" target="_blank"> <strong class="mc jd"> <em class="oo"> datamachines </em>简讯</strong> </a> <strong class="mc jd">。</strong></p><p id="cb00" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">👉🏽<a class="ae lh" href="https://medium.com/@pau-labarta-bajo" rel="noopener"> <strong class="mc jd">跟着我</strong> </a>上媒。</p><p id="8ca4" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">👉🏽给我很多掌声👏🏿👏🏽👏在下面</p><p id="7bce" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">祝你愉快🤗</p><p id="39b3" class="pw-post-body-paragraph ma mb it mc b md mw kd mf mg mx kg mi mj my ml mm mn mz mp mq mr na mt mu mv im bi translated">避寒胜地</p></div></div>    
</body>
</html>