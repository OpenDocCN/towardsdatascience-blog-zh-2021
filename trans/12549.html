<html>
<head>
<title>Adaptive Boosting: A stepwise Explanation of the Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自适应增强:算法的逐步解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/adaptive-boosting-a-stepwise-explanation-of-the-algorithm-50b75c3729c1?source=collection_archive---------18-----------------------#2021-12-23">https://towardsdatascience.com/adaptive-boosting-a-stepwise-explanation-of-the-algorithm-50b75c3729c1?source=collection_archive---------18-----------------------#2021-12-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/cb267277231d23a48eefcdf72de42449.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4gdgyOJ9UKssAFma56x5Rw.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">索耶·本特森在<a class="ae jd" href="https://unsplash.com/s/photos/jets?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><div class=""/><p id="6a97" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">自适应 Boosting(或 AdaBoost)是一种监督集成学习算法，是第一种用于实践的 Boosting 算法，由 Freund 和 Schapire 在 1995 年开发。</p><p id="371e" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">简而言之，自适应提升有助于通过将许多弱分类器顺序转化为一个强分类器来减少任何分类学习算法(如决策树或支持向量机)的错误。这可以通过顺序的权重调整、单个投票权和最终算法分类器的加权和来实现。</p><p id="6526" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我想尽可能用最简单快捷的方式向你解释自适应增强过程。我们将根据一个简单的数据集手工计算算法的每一步。</p><p id="1064" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">请注意，我不会触及升压的基本理论，因为已经有大量令人惊叹的文献了。更确切地说，这篇文章的附加价值在于关注算法的数学部分。这将让你更好地理解 Boosting，从而更自由地使用它。</p><p id="8f36" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以，享受这篇帖子，让你未来的机器学习模型更进一步！</p></div><div class="ab cl lb lc hu ld" role="separator"><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg lh"/><span class="le bw bk lf lg"/></div><div class="ij ik il im in"><h1 id="b973" class="li lj jg bd lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">自适应升压算法步骤</h1><p id="2bc5" class="pw-post-body-paragraph kd ke jg kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">执行自适应增强时，我们必须反复执行以下每个步骤(除了步骤 1，它只在开始时执行):</p><figure class="mm mn mo mp gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ml"><img src="../Images/73ba55bfac462c7894036a16b5cb479e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*Myd92ba1iDRj4wIdDrMWJQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">自适应增压的步骤</p></figure><p id="ec28" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">迭代意味着自适应增强执行多轮步骤 2 至 4，并最终将所有选择的分类器组合成一个整体公式(即强分类器):</p><figure class="mm mn mo mp gt is gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/4516958a9cf8465c85f684507302a2c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:702/format:webp/1*54wAfH2nhjyfKFQmcDcV_w.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">所有递归分类器的总和</p></figure><blockquote class="mr ms mt"><p id="68c4" class="kd ke mu kf b kg kh ki kj kk kl km kn mv kp kq kr mw kt ku kv mx kx ky kz la ij bi translated">注意:signum 函数用于获取-1 和+1 之间的最终值。</p></blockquote><p id="47f2" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">到目前为止一切顺利，让我们快速浏览一下计算所基于的模型设置！</p><h1 id="06c5" class="li lj jg bd lk ll my ln lo lp mz lr ls lt na lv lw lx nb lz ma mb nc md me mf bi translated">模型设置</h1><p id="1346" class="pw-post-body-paragraph kd ke jg kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">对于以下演示，我将使用后续数据:</p><figure class="mm mn mo mp gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nd"><img src="../Images/d9dd090e325c2fb196059ba6e493f6fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*9su86R1lKWr-E6UAKuCEhQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">模型数据(图片由作者提供)</p></figure><p id="6be9" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf jh">数据包含两类</strong>(蓝色加号和红色减号)。目标是以一种优化的方式正确地划分这两个类别。这样，我们就面临一个<strong class="kf jh">二元分类问题</strong>。</p><p id="a412" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是，我们将使用以下决策树树桩</p><figure class="mm mn mo mp gt is gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/3292f198d634b2047749c922351556d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:504/format:webp/1*-EKyz7rdNxaZ_hzVU7erCA.png"/></div></figure><p id="f1cf" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">并试图找到那些能够正确划分这些数据点的弱分类器(即决策树树桩)(注意我随机选择了决策树树桩)。</p><p id="ff42" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们设想一下我们将执行自适应增强的最终模型设置:</p><figure class="mm mn mo mp gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nd"><img src="../Images/3c3120e39da66c09a527eb551d86d64c.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*G6-_geWmbW2A2pw7Od02mw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">数据和决策树树桩(图片由作者提供)</p></figure><h1 id="50eb" class="li lj jg bd lk ll my ln lo lp mz lr ls lt na lv lw lx nb lz ma mb nc md me mf bi translated">逐渐执行自适应增压</h1><p id="973e" class="pw-post-body-paragraph kd ke jg kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">对于第一轮执行步骤，我将详细解释每个步骤。之后只会看每一轮的结果，看事情会怎么变化！</p><figure class="mm mn mo mp gt is gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/9d9240c5b5a906e3582e1fb73852f137.png" data-original-src="https://miro.medium.com/v2/resize:fit:632/format:webp/1*FHc25JIWVeWC1oz2AFmN_Q.png"/></div></figure><p id="3a3c" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<strong class="kf jh">步骤 1 </strong>中，每个数据点具有相同的权重 1/N</p><p id="d328" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">之所以这样做，是因为没有理由一开始就要给不同的权重。因为我们有 10 个数据点，初始权重是 1/10，因此是 0.1。</p><figure class="mm mn mo mp gt is gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/8e2346821f33075ea7fa984bff94d0b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*qiG1DTeVY4rglP5vX3beeg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">初始重量(图片由作者提供)</p></figure><figure class="mm mn mo mp gt is gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/5bbf4cee596ede5449b43a7a6a321941.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*RF-nmD1HcgkTjt_JYq1Jsg.png"/></div></figure><p id="9640" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<strong class="kf jh">步骤 2 </strong>中，我们绘制第一个决策树残肢并计算误差。</p><p id="f1ff" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该误差用于两件事:首先，我们需要每个决策树树桩的误差，以便挑选具有最低误差率的树桩(步骤 3a)。其次，我们使用误差来计算新的权重(步骤 4)。</p><p id="7ee9" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">加权误差能够集中在那些难以预测的数据点上。换句话说，我们对先前分类器表现差的部分给予较高的权重。</p><p id="9bf6" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们看看第一个决策树树桩 y = 2 的错误:</p><figure class="mm mn mo mp gt is gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/798c638e999c2908df6ffc3af03be16c.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/1*7SAAxsxmrqgOi3cygq5KsA.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">第一个决策树树桩(图片由作者提供)</p></figure><p id="dcc8" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你能看到，我们错误地分类了四个数据点，这导致了 0.4 (4/10)的误差。这个过程应该使用上面选择的所有分类器来完成。所以让我们开始吧！</p><figure class="mm mn mo mp gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ni"><img src="../Images/d7f3d9178e3f7311fb4d54e99c9f4b03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zodHjJQZZpO0sQmChZRBtg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">Boosting 算法的第一轮(图片由作者提供)</p></figure><p id="f594" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可以看到所有的分类器和它们各自的错误。</p><figure class="mm mn mo mp gt is gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/e6892f5098cf19e4eb041f71fa5bb7c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*aC1wa9eIhqj86VsjkF4IRw.png"/></div></figure><p id="2a70" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">根据<strong class="kf jh">步骤 3a </strong>，我们必须选择误差最小的分类器。在我们的例子中，分类器 3 的错误率最低，为 0.2。</p><figure class="mm mn mo mp gt is gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/321403ce039dd45707d32bdd093f6e2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*PloCNTJwe1GUH2ki-VB1jQ.png"/></div></figure><p id="4c60" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<strong class="kf jh">步骤 3b </strong>中，我们要计算 alpha。</p><p id="b081" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Alpha 用于展示所选分类器的投票能力。误差越低，投票权越高。这是有用的，因为更准确的分类器也应该有更高的投票权。</p><blockquote class="mr ms mt"><p id="c00f" class="kd ke mu kf b kg kh ki kj kk kl km kn mv kp kq kr mw kt ku kv mx kx ky kz la ij bi translated">注意:公式使用 log，因为如果我们有一个非常小的误差，那么 alpha 就会变得相当高。</p></blockquote><p id="5722" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们将使用 Python 来计算 alpha。</p><pre class="mm mn mo mp gt nl nm nn no aw np bi"><span id="0139" class="nq lj jg nm b gy nr ns l nt nu">import math</span><span id="21f9" class="nq lj jg nm b gy nv ns l nt nu">def get_alpha(error):<br/>    return (1/2)*math.log(((1-error)/error), math.e)</span><span id="11ac" class="nq lj jg nm b gy nv ns l nt nu">print("Alpha value: " + str(get_alpha(error=0.2)))</span><span id="3822" class="nq lj jg nm b gy nv ns l nt nu">Alpha value: 0.6931471805599453</span></pre><p id="48c4" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，我们的第一个分类者的投票权约为 0.69。</p><figure class="mm mn mo mp gt is gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/d14af75f7ca90945cb9dd8b3fc18a75b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*8w9iT7BBxCCIvFt4SgMqLQ.png"/></div></figure><p id="c3d6" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在<strong class="kf jh">步骤 4 </strong>中，我们需要计算新的权重。</p><p id="fec5" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">新的权重在每一轮结束时计算，用于识别难以分类的数据点。我们再次使用 Python 来计算新的权重。</p><pre class="mm mn mo mp gt nl nm nn no aw np bi"><span id="4f05" class="nq lj jg nm b gy nr ns l nt nu">def new_weight_correct(error):<br/>    return 0.1/(2*(1-error))</span><span id="0d42" class="nq lj jg nm b gy nv ns l nt nu">def new_weight_incorrect(error):<br/>    return 0.1/(2*error)</span><span id="d963" class="nq lj jg nm b gy nv ns l nt nu">print("Weights for correct classified datapoints: " + str(new_weight_correct(0.2)))<br/>print("Weights for incorrect classified datapoints: " + str(new_weight_incorrect(0.2)))</span><span id="c3e8" class="nq lj jg nm b gy nv ns l nt nu">Weights for correct classified datapoints: 0.0625<br/>Weights for incorrect classified datapoints: 0.25</span></pre><p id="3461" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您可能会注意到，分类不正确的数据点的权重(0.25)高于分类正确的数据点的权重(0.0625)。通过查看数据点的大小，可以在下一轮中看到更高的权重(我使数据点的大小与彼此的权重成比例)。</p><p id="fc3d" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因此，在第一轮结束时，我们有了 Boosting 算法中的第一个分类器(分类器 3)，他各自的投票权和新的权重。</p><p id="66fe" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在每一轮结束时，我们可以考虑是否已经有足够的弱分类器来正确地分离所有数据点。显然，一轮是不够的，所以我们用新的重量进行第二轮！</p><p id="81b8" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于每一轮的计算都是相同的，我将只从这一点来解释结果。</p><figure class="mm mn mo mp gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ni"><img src="../Images/f5f7c3caa3d29e326a2a71cb2e52c5e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B410CztV8fACybPZeZejhg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">Boosting 算法的第二轮(图片由作者提供)</p></figure><p id="60b2" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在第 2 轮之后，分类器 4 是最小化误差的那个。注意，分类器 6 也具有相同的错误率，但是我们现在将选择第一个。然而，我们将继续计算分类器 4 的α和新的权重。</p><pre class="mm mn mo mp gt nl nm nn no aw np bi"><span id="4d4e" class="nq lj jg nm b gy nr ns l nt nu">print("Alpha value: " + str(get_alpha(error=0.1875)))<br/>print("Weights for correct classified datapoints: " + str(new_weight_correct(error=0.1875)))<br/>print("Weights for incorrect classified datapoints: " + str(new_weight_incorrect(error=0.1875)))</span><span id="5712" class="nq lj jg nm b gy nv ns l nt nu">Alpha value: 0.7331685343967135<br/>Weights for correct classified datapoints: 0.06153846153846154<br/>Weights for incorrect classified datapoints: 0.26666666666666666</span></pre><p id="1161" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">到目前为止一切顺利，让我们继续第三轮:</p><figure class="mm mn mo mp gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ni"><img src="../Images/6172428d6826bb8439470d4c8e9e152e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5TiBOUJP8ddOl1C0eE7G7Q.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">Boosting 算法的第三轮(图片由作者提供)</p></figure><p id="1eba" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">似乎分类器 3 还是最好的。让我们添加它并计算 alpha，新的权重，从第 4 轮开始:</p><pre class="mm mn mo mp gt nl nm nn no aw np bi"><span id="1740" class="nq lj jg nm b gy nr ns l nt nu">print("Alpha value: " + str(get_alpha(error=0.1231)))<br/>print("Weights for correct classified datapoints: " + str(new_weight_correct(error=0.1231)))<br/>print("Weights for incorrect classified datapoints: " + str(new_weight_incorrect(error=0.1231)))</span><span id="498d" class="nq lj jg nm b gy nv ns l nt nu">Alpha value: 0.9816979637974511<br/>Weights for correct classified datapoints: 0.05701904436081651<br/>Weights for incorrect classified datapoints: 0.4061738424045492</span></pre><figure class="mm mn mo mp gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ni"><img src="../Images/86e43653b0e353907a51d29cab982954.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TLGp5CfcjvUS1miSWi3s4w.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">Boosting 算法的第四轮(图片由作者提供)</p></figure><p id="5fbc" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">同样，分类器 4 和 6 是最好的。这一次，我们选择分类器 6！</p><pre class="mm mn mo mp gt nl nm nn no aw np bi"><span id="2e92" class="nq lj jg nm b gy nr ns l nt nu">print("Alpha value: " + str(get_alpha(error=0.1711)))<br/>print("Weights for correct classified datapoints: " + str(new_weight_correct(error=0.1711)))<br/>print("Weights for incorrect classified datapoints: " + str(new_weight_incorrect(error=0.1711)))</span><span id="3dec" class="nq lj jg nm b gy nv ns l nt nu">Alpha value: 0.7889256698496664<br/>Weights for correct classified datapoints: 0.06032090722644469<br/>Weights for incorrect classified datapoints: 0.29222676797194624</span></pre><p id="7233" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在等等！如果我们组合分类器，似乎我们可以对所有数据点进行分类。让我们看看最终分类:</p><figure class="mm mn mo mp gt is gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/90f87b3aa88e2bc68578713cca713bf0.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*l1mF90mfTWSvXNE-F4F4xg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">最终分类(图片由作者提供)</p></figure><p id="ee0d" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在那里！我们把一切都分类对了！让我们尝试添加一个新的数据点，并让分类器投票！</p><p id="55c5" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">x = 7 <br/> y = 3.5</p><figure class="mm mn mo mp gt is gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/248c123f31755049aa4104e651fb709a.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*isNP6eEs8VdpT_t13h80tg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">添加新的数据点(图片由作者提供)</p></figure><p id="be7c" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">根据最终模型，我们有以下算法:</p><figure class="mm mn mo mp gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ny"><img src="../Images/15d01d6ffeb339b443513ab1e0f6de39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FTtnFZcDR4k1kibFLlebgw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">最终模型</p></figure><ul class=""><li id="3277" class="nz oa jg kf b kg kh kk kl ko ob ks oc kw od la oe of og oh bi translated"><strong class="kf jh">分类器 1 </strong>以 0.6931 的投票权为 plus 投票</li><li id="174a" class="nz oa jg kf b kg oi kk oj ko ok ks ol kw om la oe of og oh bi translated"><strong class="kf jh">分类器 2 </strong>以 0.7332 的投票权投反对票</li><li id="0805" class="nz oa jg kf b kg oi kk oj ko ok ks ol kw om la oe of og oh bi translated"><strong class="kf jh">分类器 3 </strong>以 0.9816 的投票权为 plus 投票</li><li id="b9dc" class="nz oa jg kf b kg oi kk oj ko ok ks ol kw om la oe of og oh bi translated"><strong class="kf jh">分类器 4 </strong>以 0.7889 的投票权投负号。</li></ul><p id="da01" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你计算投票权的加权和，算法会将数据点归类为加号并猜测…算法完全正确！</p><h1 id="f1d2" class="li lj jg bd lk ll my ln lo lp mz lr ls lt na lv lw lx nb lz ma mb nc md me mf bi translated">Python 中的自适应增强</h1><p id="144c" class="pw-post-body-paragraph kd ke jg kf b kg mg ki kj kk mh km kn ko mi kq kr ks mj ku kv kw mk ky kz la ij bi translated">为了向您展示如何在 Python 中实现自适应增强，我使用了来自 scikit-learn 的快速示例，其中他们基于支持向量机对数字图像进行分类。您可以在这里找到示例和相应的代码:</p><div class="ip iq gp gr ir on"><a href="https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html" rel="noopener  ugc nofollow" target="_blank"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd jh gy z fp os fr fs ot fu fw jf bi translated">识别手写数字-sci kit-学习 0.24.2 文档</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">这个例子展示了如何使用 scikit-learn 来识别手写数字(从 0 到 9)的图像。打印(__doc__ )…</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">scikit-learn.org</p></div></div><div class="ow l"><div class="ox l oy oz pa ow pb ix on"/></div></div></a></div><p id="c1ba" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是对比这个例子，我们会用 AdaBoost 对数字进行分类。之后，我们再看表演。</p><blockquote class="mr ms mt"><p id="e219" class="kd ke mu kf b kg kh ki kj kk kl km kn mv kp kq kr mw kt ku kv mx kx ky kz la ij bi translated">注意，在 AdaBoost 中，默认的分类器是决策树分类器。如果你想要其他的基本估计量，你必须在模型中阐明它们。</p></blockquote><pre class="mm mn mo mp gt nl nm nn no aw np bi"><span id="0241" class="nq lj jg nm b gy nr ns l nt nu"># Importing the libraries<br/>import matplotlib.pyplot as plt<br/>from sklearn import datasets, metrics<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.tree import DecisionTreeClassifier<br/>from sklearn.ensemble import AdaBoostClassifier</span><span id="16d1" class="nq lj jg nm b gy nv ns l nt nu">digits = datasets.load_digits()</span><span id="8efc" class="nq lj jg nm b gy nv ns l nt nu">_, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 3))<br/>for ax, image, label in zip(axes, digits.images, digits.target):<br/>    ax.set_axis_off()<br/>    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')<br/>    ax.set_title('Training: %i' % label)</span></pre><figure class="mm mn mo mp gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi pc"><img src="../Images/a270617e8fd483ce116941e7b5359dc0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*hPvsyJlleqbyx1GMFKYqQw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">我们将尝试分类的样本数字图像</p></figure><pre class="mm mn mo mp gt nl nm nn no aw np bi"><span id="fddc" class="nq lj jg nm b gy nr ns l nt nu"># flatten the images<br/>n_samples = len(digits.images)<br/>data = digits.images.reshape((n_samples, -1))</span><span id="62fd" class="nq lj jg nm b gy nv ns l nt nu"># Split data into 50% train and 50% test subsets<br/>X_train, X_test, y_train, y_test = train_test_split(<br/>    data, digits.target, test_size=0.5, shuffle=False)</span></pre><p id="9daf" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">好了，我们现在准备开始使用 AdaBoost 分类器。我们将使用 accuracy_score 度量来定义分类器的性能。</p><pre class="mm mn mo mp gt nl nm nn no aw np bi"><span id="9eca" class="nq lj jg nm b gy nr ns l nt nu">clf = AdaBoostClassifier()<br/>clf.fit(X_train, y_train)<br/>y_pred = clf.predict(X_test)</span><span id="93eb" class="nq lj jg nm b gy nv ns l nt nu">acc_sc = metrics.accuracy_score(y_test, y_pred)</span><span id="0cc9" class="nq lj jg nm b gy nv ns l nt nu">print("Accuracy Score: " + str(acc_sc))</span><span id="31bc" class="nq lj jg nm b gy nv ns l nt nu">Accuracy Score: 0.7808676307007787</span></pre><p id="1949" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在那里！我们能够以大约 78%的准确率对数字进行分类。请注意，我们没有执行超参数优化，并且还使用 50%作为测试大小(这相当高)…因此，该模型可以进一步改进！</p><p id="9364" class="pw-post-body-paragraph kd ke jg kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我希望你喜欢这篇文章，如果你喜欢…然后砸一些拍手:)编码快乐！</p></div></div>    
</body>
</html>