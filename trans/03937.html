<html>
<head>
<title>Detecting objects in urban scenes using YOLOv5</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用YOLOv5检测城市场景中的物体</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/detecting-objects-in-urban-scenes-using-yolov5-568bd0a63c7?source=collection_archive---------12-----------------------#2021-04-02">https://towardsdatascience.com/detecting-objects-in-urban-scenes-using-yolov5-568bd0a63c7?source=collection_archive---------12-----------------------#2021-04-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="117e" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/making-sense-of-big-data" rel="noopener" target="_blank">理解大数据</a></h2><div class=""/><p id="a46d" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">作为我在<a class="ae ku" href="https://mila.quebec/en/mila/" rel="noopener ugc nofollow" target="_blank"> MILA </a>(魁北克的人工智能研究所)攻读机器学习硕士学位的一部分，在蒙特利尔市<em class="kv">工作期间，</em> I <em class="kv"> </em>开发了一个人工智能城市物体检测解决方案，用于来自平移-倾斜-缩放(PTZ)交通摄像机的视频馈送。该原型可以检测五种不同类别的对象，即车辆、行人、公共汽车、骑自行车的人和建筑对象。这是一个检测一帧的例子，拍摄于蒙特利尔市中心的De La Montagne街和René-Levesque大道的交叉口。</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div class="gh gi kw"><img src="../Images/b70ed0d4b15830afa8e9c23ed49a84b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1248/format:webp/1*vnt4DlXUo_oYYYbtQu9qXw.png"/></div><p class="le lf gj gh gi lg lh bd b be z dk translated">显示所有五个对象类别的对象检测示例-按作者分类的图像</p></figure><p id="807a" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">为了显示该模型如何推广到另一个城市环境，这里有一个离线检测(即，非实时)的例子，其中为这个<a class="ae ku" href="https://youtu.be/ufK2XRGUjuc" rel="noopener ugc nofollow" target="_blank">测试youtube视频</a>的每一帧产生一个推理。</p><figure class="kx ky kz la gt lb"><div class="bz fp l di"><div class="li lj l"/></div><p class="le lf gj gh gi lg lh bd b be z dk translated">来自youtube视频的<a class="ae ku" href="https://youtu.be/ufK2XRGUjuc" rel="noopener ugc nofollow" target="_blank">测试城市场景的离线检测示例——视频作者</a></p></figure><p id="e1c7" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">感谢蒙特利尔市的开放数据政策和开源软件政策，我很高兴地宣布我的项目正在变成开源项目！本周，我们发布了<a class="ae ku" href="https://github.com/VilledeMontreal/urban-detection" rel="noopener ugc nofollow" target="_blank">代码</a>、<a class="ae ku" href="https://github.com/VilledeMontreal/urban-detection/releases" rel="noopener ugc nofollow" target="_blank">训练模型</a>以及所有带注释的图片(<a class="ae ku" href="https://donnees.montreal.ca/ville-de-montreal/images-annotees-cameras-circulation" rel="noopener ugc nofollow" target="_blank">数据集</a>)。</p><p id="57b6" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我还写了一份详细的技术报告，等待MILA的审查和批准，我也将能够分享(敬请期待！).与此同时，我写这篇博客的目的是简要概述这个项目及其成果，让人们了解这个新开放的数据集。</p><p id="81e4" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">重要提示</strong>:在继续之前，我想澄清一下这个项目的动机，让那些可能对这项技术的使用有疑虑的人放心。T2城市交通管理中心(CGMU)是蒙特利尔市智能交通系统的心脏和大脑。CGMU运营商在整个地区安装了500多个交通摄像头(见<a class="ae ku" href="https://ville.montreal.qc.ca/circulation/" rel="noopener ugc nofollow" target="_blank">地图</a>),可以监控道路网络上的交通状况，并在出现问题时(如事故或车辆故障)进行处理。为了帮助他们更快地检测事故，蒙特利尔市希望建立一个自动道路异常检测系统。我们现在发布的这个对象检测解决方案构成了一个重要的构建模块，将有助于实现这一长期目标。</p><p id="4474" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><strong class="jy ja">同样重要的是</strong>:在整个项目中，尊重公民隐私是非常重要的，尤其是现在我们正在发布注释。值得注意的是，这些注释与已经通过<a class="ae ku" href="https://donnees.montreal.ca/ville-de-montreal/cameras-observation-routiere" rel="noopener ugc nofollow" target="_blank">城市开放数据网站</a>公开发布的图像相关联。此外，虽然安装用于获取这些图像的摄像机是为了协助CGMU操作员执行日常交通管理任务，但它们也被调整为限制收集的信息。例如，相机分辨率被设置为既不能识别人脸也不能识别车牌，并且图像只以5分钟的间隔保存和发布。<a class="ae ku" href="https://laburbain.montreal.ca/sites/villeintelligente.montreal.ca/files/25817-charte_donnees_numeriques_ang.pdf" rel="noopener ugc nofollow" target="_blank">蒙特利尔的《数字数据宪章》</a>是一份很好的参考资料，有助于了解该市如何管理和规范数字数据的生命周期。</p><p id="3995" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">既然我们已经弄清楚了这一点，让我们开始吧！</p><h1 id="1cc5" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">关键特征</h1><p id="ff3c" class="pw-post-body-paragraph jw jx iq jy b jz mi kb kc kd mj kf kg kh mk kj kk kl ml kn ko kp mm kr ks kt ij bi translated">以下是最终目标检测模型的一些关键特征。他们:</p><ul class=""><li id="ce46" class="mn mo iq jy b jz ka kd ke kh mp kl mq kp mr kt ms mt mu mv bi translated">我们在单个GPU上使用近19k幅图像进行了训练(来自蒙特利尔市的7007幅训练图像+来自<a class="ae ku" href="http://podoce.dinf.usherbrooke.ca/challenge/dataset/" rel="noopener ugc nofollow" target="_blank"> MIO-TCD数据集</a>的11877幅图像)</li><li id="aa67" class="mn mo iq jy b jz mw kd mx kh my kl mz kp na kt ms mt mu mv bi translated">包括较小和较大的神经网络体系结构，它们可以分别在CPU上以167 ms的延迟和在GPU上以16.6 ms的延迟运行。</li><li id="40e3" class="mn mo iq jy b jz mw kd mx kh my kl mz kp na kt ms mt mu mv bi translated">兼容不同的相机品牌和型号、图像分辨率以及不同的变焦和方向设置。</li><li id="ecef" class="mn mo iq jy b jz mw kd mx kh my kl mz kp na kt ms mt mu mv bi translated">对视觉伪像(例如失焦、镜头上的雨滴或灰尘、阳光眩光等)以及不同的天气和天气条件具有鲁棒性。</li></ul><h1 id="fd50" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">资料组</h1><p id="1d8f" class="pw-post-body-paragraph jw jx iq jy b jz mi kb kc kd mj kf kg kh mk kj kk kl ml kn ko kp mm kr ks kt ij bi translated">蒙特利尔市数据集由10，000幅分辨率为350x288、352x240和704x480的图像组成，带有Pascal VOC格式的车辆、公共汽车、行人、骑自行车者和建筑物体的相关物体检测注释。所有注释都是使用<a class="ae ku" href="https://github.com/openvinotoolkit/cvat" rel="noopener ugc nofollow" target="_blank"> CVAT </a>(计算机视觉注释工具)生成的。然后，数据集被分成四部分，如下所示:</p><ul class=""><li id="9248" class="mn mo iq jy b jz ka kd ke kh mp kl mq kp mr kt ms mt mu mv bi translated">70%的图像用于训练，</li><li id="d634" class="mn mo iq jy b jz mw kd mx kh my kl mz kp na kt ms mt mu mv bi translated">10%用于验证，</li><li id="2382" class="mn mo iq jy b jz mw kd mx kh my kl mz kp na kt ms mt mu mv bi translated">10%用于域内测试</li><li id="2650" class="mn mo iq jy b jz mw kd mx kh my kl mz kp na kt ms mt mu mv bi translated">10%用于域外测试</li></ul><p id="bf54" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">训练集用于学习模型权重和偏差，而验证集在训练期间用于监控性能并确定模型何时开始过度拟合。它还用于评估超参数的不同配置。测试集在项目结束时用于报告绩效。</p><p id="0f97" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">训练集、验证集和域内测试集都使用相同的摄像机子群体(即交叉点)，而域外测试分割使用在训练期间不可见的保留摄像机群体。</p><p id="8ec9" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">下表显示了每个集合中包含的每个类的实例数量。我们可以看到，阶层分布不均衡，车辆远远多于骑自行车的人和公交车。</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nb"><img src="../Images/d2ae2d1aef676d8e25eeb01ca33a2e26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zNVb4e5CSP0E7s2YTLwshg.png"/></div></div><p class="le lf gj gh gi lg lh bd b be z dk translated">每个数据分割中不同类别的影像和对象总数-蒙特利尔市数据集-按作者分类的影像</p></figure><p id="6893" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">为了帮助提供更多代表性不足的类别的训练示例，使用了额外的数据集，即<a class="ae ku" href="http://podoce.dinf.usherbrooke.ca/challenge/dataset/" rel="noopener ugc nofollow" target="_blank"> MIOvision交通摄像机数据集</a> (MIO-TCD)。因为它包含了视角非常相似、分辨率非常相似的图像，所以非常符合我们的需求。它还包含冬季月份的图像，考虑到蒙特利尔市数据集只包含夏季月份的图像，这是一个有趣的补充。从总共11877幅附加图像中选择了包含骑自行车的人、行人和公共汽车的所有图像。因此，我们能够将2260名骑自行车的人和8319辆公共汽车添加到训练示例中。但是，MIO-TCD不包含任何带标签的构造对象。</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/ea6a01ddd62f5793ff3bcc029a3a1ea7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1254/format:webp/1*sSun1b2ZNNKXHg95yWqfrg.png"/></div><p class="le lf gj gh gi lg lh bd b be z dk translated">来自<a class="ae ku" href="http://podoce.dinf.usherbrooke.ca/challenge/dataset/" rel="noopener ugc nofollow" target="_blank"> MIO的样本图片-TCD数据集</a> —作者图片</p></figure><h1 id="b836" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">模型</h1><p id="4ad0" class="pw-post-body-paragraph jw jx iq jy b jz mi kb kc kd mj kf kg kh mk kj kk kl ml kn ko kp mm kr ks kt ij bi translated">YOLOv5 是一个对象检测模型，于2020年5月发布，作为github上的Pytorch实现，并被选为该项目的基础。在评估我们的选项时，YOLOv5是可用的最快和最准确的对象检测模型之一。此外，它受益于一个非常大的用户社区，这意味着它正在积极开发中，每周都有改进。为了稳定起见，commit<a class="ae ku" href="https://github.com/ultralytics/yolov5/tree/bb8872ea5f2afb7b6b2c81034b1d399d61d5157a" rel="noopener ugc nofollow" target="_blank"><strong class="jy ja">bb 8872</strong></a>(2020年9月8日发布)被分叉用于项目具体开发。YOLOv5包括4种不同的网络架构大小:小型(S)、中型(M)、大型(L)和X-大型(X)。</p><p id="dca2" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><a class="ae ku" href="https://arxiv.org/pdf/1512.02325.pdf" rel="noopener ugc nofollow" target="_blank">单发多箱探测器</a> (SSD)被选为基准型号，与YOLOv5进行比较。在2015年创建时，SSD是最快的型号之一，这使得它非常适合实时应用程序。它不再被认为是最先进的，但由于其简单性，仍然经常被用作基线模型。</p><h1 id="420f" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">迁移学习</h1><p id="e180" class="pw-post-body-paragraph jw jx iq jy b jz mi kb kc kd mj kf kg kh mk kj kk kl ml kn ko kp mm kr ks kt ij bi translated">在非常大的数据集上预先训练模型以学习有意义的表示，并随后在感兴趣的任务上对其进行微调，通常有利于性能。这种策略被用于YOLOv5(在MS COCO对象检测数据集上预先训练)和SSD(在ImageNet图像分类数据集上预先训练)。</p><h1 id="1075" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">增加</h1><p id="fb83" class="pw-post-body-paragraph jw jx iq jy b jz mi kb kc kd mj kf kg kh mk kj kk kl ml kn ko kp mm kr ks kt ij bi translated">YOLOv5和SSD都在训练时使用数据扩充来获得不容易过度拟合的解决方案。从一个时期到另一个时期，各种增强被采样并应用于相同的输入图像，这导致数据集大小和输入图像可变性的人为增加。光度和几何放大的一些例子如下所示:</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/61ad06faf8df53d4b1531d1a0f74f837.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*U0JkWj15RDjFSR9wcmYgbA.png"/></div><p class="le lf gj gh gi lg lh bd b be z dk translated">光度扩展示例-作者提供的图片</p></figure><figure class="kx ky kz la gt lb gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/ed6eaf99e5daf3f9b710b2ddf036be25.png" data-original-src="https://miro.medium.com/v2/resize:fit:1120/format:webp/1*_a_3AC70EZUKU0BsDohTsA.png"/></div><p class="le lf gj gh gi lg lh bd b be z dk translated">光度扩展的其他示例-图片由作者提供</p></figure><figure class="kx ky kz la gt lb gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/fff19d71de7b57b9e01048f0f1883bc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/format:webp/1*lZdDAluzoi9Ai1k7Y1vuEg.png"/></div><p class="le lf gj gh gi lg lh bd b be z dk translated">几何增强的例子——作者图片</p></figure><p id="639b" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">马赛克增强是一种新的图像增强方法，在YOLOv4中引入，并在YOLOv5中保留。镶嵌增强包括混合4个不同的训练图像，这具有允许检测正常背景之外的对象的效果，因此提高了泛化能力。另一个好处是，它减少了使用大的小批量的需要。</p><h1 id="05bd" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated"><strong class="ak">计算资源</strong></h1><p id="d45d" class="pw-post-body-paragraph jw jx iq jy b jz mi kb kc kd mj kf kg kh mk kj kk kl ml kn ko kp mm kr ks kt ij bi translated">所有实验和训练都是使用谷歌Colab Pro实例(包括英伟达特斯拉P100或V100)进行的，预期超参数搜索是使用谷歌云平台(GCP)上的GPU实例进行的。这里是有用的<a class="ae ku" href="https://grondin-js.medium.com/" rel="noopener">教程</a>为训练我们的模型设置GCP。</p><h1 id="37e9" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">结果</h1><p id="178f" class="pw-post-body-paragraph jw jx iq jy b jz mi kb kc kd mj kf kg kh mk kj kk kl ml kn ko kp mm kr ks kt ij bi translated">乍一看，与基准SSD模型相比，YOLOv5的检测性能(以<a class="ae ku" href="https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173" rel="noopener">平均精度(mAP) </a>表示)远远优于基准SSD模型。当仅在蒙特利尔市数据集上训练时，具有26.7 M参数的SSD实现了0.466的mAP，而yolov 5‘M’模型在大致相同的模型大小下实现了0.663的mAP。事实上，所有四种YOLOv5变体在推理和训练过程中都更快。</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nk"><img src="../Images/1dfaf3addc3189b279080049145e73a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EU21fpoDJawECy1EdjgmmA.png"/></div></div><p class="le lf gj gh gi lg lh bd b be z dk translated">模型大小对训练时间、性能和延迟的影响。输入图像尺寸:320x320。仅在蒙特利尔市数据集上训练，300个时期-图片由作者提供</p></figure><p id="d62e" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">除了模型架构大小之外，另一个被发现会显著影响性能和延迟的配置设置是输入图像大小。基准SSD使用300x300的输入图像大小，而320x320、512x512和704x704的大小在YOLOv5 'X '型号上进行了测试。蒙特利尔市<em class="kv">数据集</em>的最大图像分辨率为704x480，因此在输入图像尺寸为704x704的情况下，不会丢失任何像素信息。这可能会对小物体的检测性能产生很大影响，例如行人和建筑圆锥体，因为它们往往非常狭窄。下表显示，随着图像输入大小的增加，检测性能会提高，但GPU延迟和训练时间会增加。</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nl"><img src="../Images/50fcfd6620018b5c86fa79d4ff0605c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CC8fCx0rwTqLAgLn8nEDnQ.png"/></div></div><p class="le lf gj gh gi lg lh bd b be z dk translated">输入图像大小对训练时间、性能和延迟的影响。批量大小1和32分别用于CPU和GPU推断。数据集:蒙特利尔市，经过300个纪元的训练-图片由作者提供</p></figure><p id="4b25" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在固定架构和输入图像大小的情况下，增加MIO-TCD数据集可以提高YOLOv5模型的性能，但不能提高SDD的性能。下表显示了每个类别的增量收益。所有的职业都有进步，除了建筑类的表现有所下降。这并不奇怪，因为在MIO-TCD数据集中没有构造对象。组合两个数据集的一个缺点是训练时间的增加，考虑到训练样本的数量增加了一倍以上，这并不奇怪。</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nm"><img src="../Images/2d85d39a5a8c7804e2b0b12499a793d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j2si8wfGt6m5HxcUZqmQJQ.jpeg"/></div></div><p class="le lf gj gh gi lg lh bd b be z dk translated">添加MIO-TCD数据集对成绩和训练时间的影响。经过300个纪元的训练—作者提供的图片</p></figure><p id="0ddf" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">使用<a class="ae ku" href="https://github.com/Epistimio/orion" rel="noopener ugc nofollow" target="_blank"> Orion </a>进行了严格的超参数搜索，Orion 是一种用于黑盒函数优化的异步框架，已经集成为蒙特利尔市刚刚发布的代码的一部分。这方面的更多细节将在报告中提供。一个有趣的发现是，通过超参数搜索获得的性能提升与使用原始YOLOv5存储库中的基线超参数获得的性能提升相比微不足道。这意味着使用基线超参数已经可以预期非常好的性能，并且节省了他们自己的探索过程。</p><p id="3b48" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在测试集上评估了最佳的最终模型。下表显示了域内(训练期间看到的交叉点)和域外(新交叉点)测试图像的最终性能。</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/e25ab30855fd39b810f4b6f48b253a11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*5_krwf2ZUHCF5kQEQJ-uXA.png"/></div><p class="le lf gj gh gi lg lh bd b be z dk translated">在域内和域外测试集上的最终模型性能—图片由作者提供</p></figure><p id="884c" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">正如预期的那样，在看不见的交叉路口，性能会下降，但对人眼来说，检测质量仍然很好，如下图所示的小模型。</p><figure class="kx ky kz la gt lb gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi no"><img src="../Images/76ecc03881b70529b29d139159fcc8b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b15G1LCVwcGlLcCEjdR2HQ.png"/></div></div><p class="le lf gj gh gi lg lh bd b be z dk translated">YOLOv5 Small，输入图像大小为512x512:四个域外测试集图像—图像由作者提供</p></figure><h1 id="7ad4" class="lk ll iq bd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh bi translated">结论</h1><p id="65bd" class="pw-post-body-paragraph jw jx iq jy b jz mi kb kc kd mj kf kg kh mk kj kk kl ml kn ko kp mm kr ks kt ij bi translated">由于未来的目标是检测道路网络上的事件和其他异常，蒙特利尔市在不久的将来可以遵循的自然下一步是将一个预先训练的YOLOv5架构纳入多对象跟踪神经解决方案。为了帮助生成训练该解决方案所需的多对象跟踪数据集，可以使用我们提出的最佳模型作为预注释图像的手段，并最小化人类注释者的负担。</p></div></div>    
</body>
</html>