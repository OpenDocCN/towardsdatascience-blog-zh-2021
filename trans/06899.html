<html>
<head>
<title>10 Promising Synthesis Papers from CVPR 2021</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">来自CVPR 2021的10篇有前途的综合论文</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/10-promising-synthesis-papers-from-cvpr-2021-5dc670981872?source=collection_archive---------22-----------------------#2021-06-22">https://towardsdatascience.com/10-promising-synthesis-papers-from-cvpr-2021-5dc670981872?source=collection_archive---------22-----------------------#2021-06-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="51b8" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">我们从顶级计算机视觉会议上挑选的关于人类、场景和运动合成的最有前途的最先进的论文。</h2></div><p id="5874" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2021年计算机视觉和模式识别大会(CVPR)刚刚开始。主题涵盖从自动驾驶到医疗成像的整个领域，CVPR 2021展示了一系列令人兴奋的最新技术，展示了巨大的实际应用潜力。在信息高度密集的文件迷宫中穿行可不是在公园里散步。因此，我们精选了十篇有前途的CVPR论文，涉及三大主题——图像合成、场景重建和运动合成。</p><h1 id="3ea9" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">图像合成中对GANs的控制</h1><p id="8a60" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">“我们没有更好的算法。我们有更多的数据，”谷歌的研究主管彼得·诺维格说。这恰当地反映了机器学习社区中普遍流行的“数据越多(通常)越好”的口号。在计算机视觉领域，近年来在合成和编辑真实感图像方面已经看到了生成式对抗网络(GANs)的出现和发展。</p><p id="8c4d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，计算机视觉从业者哀叹对这种生成模型合成的图像缺乏控制——这是正确的。在这里，我们探索一些CVPR 2021论文，这些论文吹嘘在生成图像的过程中提供更多的控制。这些进步代表了向GAN的广泛采用迈进了一步。</p><p id="ac91" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 1。长颈鹿:将场景表示为合成生成神经特征场</strong> [ <a class="ae ly" href="https://autonomousvision.github.io/giraffe/" rel="noopener ugc nofollow" target="_blank"> Github </a> ]</p><p id="8938" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">控制生成图像中的对象的任务需要模型在3D中推理。这对于在2D空间中运行的模型来说绝不是微不足道的。为了解决这个问题，本文在模型中引入了一种组合的3D场景表示，允许独立地控制对象的位置、方向和视角。</p><p id="d7a5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该模型能够首先识别前景和背景中的对象，然后改变它们的位置、方向和视角。然后，它将所有对象组合起来，生成一个输出图像。在该示例中，在汽车图像集合上训练的模型可以生成不同尺寸、位置和方向的汽车图像。长颈鹿也能够改变背景，甚至改变场景中汽车的数量。</p><p id="269c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 2。想象:通过图像引导模型反演</strong> <br/>的图像合成想象，图像引导模型反演(IMAGINE)方法从一个单一的图像生成高质量和多样的图像，而不需要训练生成器。它的一个关键特性是在合成过程中对语义(如对象的形状和位置)施加约束的能力，并为用户提供对控件的直观控制。不像它的前辈像SinGAN那样努力创造非重复的物体，IMAGINE创造了非重复图像的高质量图像。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi lz"><img src="../Images/daf1331bac9bbd1ebb9ce1223bdbba0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wm51do_bTmXE-XsidrFTcA.jpeg"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">由想象生成的结果(<a class="ae ly" href="https://arxiv.org/pdf/2104.05895.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="1955" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 3。HistoGAN:通过颜色直方图控制GAN生成的图像和真实图像的颜色</strong><a class="ae ly" href="https://arxiv.org/pdf/2011.11731.pdf" rel="noopener ugc nofollow" target="_blank">纸张</a></p><p id="6faf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">GAN难以控制的另一个方面是图像的颜色。受基于类直方图的颜色传递方法的启发，慕尼黑工业大学和谷歌的研究人员开发了HistoGAN。</p><p id="1449" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">HistoGAN通过修改StyleGAN的最后两个块来整合直方图特征，从而利用了StyleGAN2的架构。因此，通过将目标颜色直方图作为直方图输入的一部分，可以控制图像的色调。</p><p id="b39c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用直方图作为改变图像颜色的杠杆是直观的和独立于领域的，允许来自不同领域的图像和从业者潜在地使用直方图。在上面的HistoGAN图中可以看到目标颜色对生成图像的影响。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi mp"><img src="../Images/691e9e2be89135d6ec2484c2b1992761.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gTbH4C1xNEJ_TQQUFXnj2Q.jpeg"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">HistoGAN生成的图像。每一行的图像都有不同的目标图像，目标颜色以直方图的形式从中导出(显示在左栏的左上角)。</p></figure><p id="2732" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 4。用于交互式图像合成和编辑的any cost GANs[Github | Paper]</strong></p><p id="f47b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">像StyleGAN2这样的先进网络的高计算成本已经阻止了用户即时和交互式地生成图像。今天，一个完整的生成器大约需要3秒来渲染一幅图像，这对于许多用途来说都太慢了。这个问题预示着交互式自然图像编辑的任何代价。</p><p id="ef34" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用Anycost GAN，用户可以控制以较低的通道和分辨率配置生成图像，以换取更快的结果和更低的计算成本。有了这样的控制，用户可以以快5倍的速度生成视觉上相似的图像。</p><p id="f23b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这一发现为用户在各种成本预算和硬件要求下生成图像铺平了道路，代表了向没有计算资源的用户普及GANs的一步。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi mq"><img src="../Images/8f03bfe6301b83173c62a7d2f8f3c78c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DcwnjtvThREag4DEXE0SVg.jpeg"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated">AnycostGan演示，重点介绍通道和分辨率设置。<a class="ae ly" href="https://raw.githubusercontent.com/mit-han-lab/anycost-gan/master/assets/figures/demo.gif" rel="noopener ugc nofollow" target="_blank">播放gif </a>！</p></figure><h1 id="addd" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">动态物体和人的合成</h1><p id="b201" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">我们已经看到了GANs如何允许静态图像被重新创建和控制。但是动态图像呢？在下一节中，我们将看到在运动中建模和合成物体方面的进步。</p><p id="1790" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 5。通过隐式模块化视听表示生成姿态可控的说话人脸</strong></p><p id="5cfc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">“我们正在进入一个时代，在这个时代，我们的敌人可以让任何人在任何时间点说任何话，”这是一位<a class="ae ly" href="https://www.youtube.com/watch?v=AmUC4m6w1wo" rel="noopener ugc nofollow" target="_blank">总统奥巴马根据2018年</a>几个小时的镜头重建的话。从那时起，计算机视觉社区已经实现了从音频剪辑中创建具有精确同步嘴唇运动的说话头部的壮举。然而，控制正在说话的头部的姿态的问题是具有挑战性的，并且是产生具有个性化的有节奏的头部运动的正在说话的头部的瓶颈。</p><p id="b7ae" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">姿势可控视听系统(PC-AVS)承诺通过实现自由姿势控制，同时创建带有音频的任意说话人脸来解决这一问题。PC-AVS可以将一个演员的单张照片、音频剪辑和另一个演员姿势的源视频转换成一个逼真的说话头。这种方法优于现有的方法，如节奏头(陈等人)和MakeitTalk(周等人)，特别是在音频的极端条件下，如当音频有噪声时。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi mr"><img src="../Images/51fc14bc3e3e47434ee3728bc5fc6e7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*e4xZBo8MddOxwpKjsD-HOA.jpeg"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated"><a class="ae ly" href="https://www.youtube.com/watch?v=lNQQHIggnUg&amp;t=1s" rel="noopener ugc nofollow" target="_blank">玩说话脸夹！</a></p></figure><p id="caaa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 6。神经体:具有结构化潜在代码的隐式神经表示，用于动态人的新颖视图合成[</strong><a class="ae ly" href="https://zju3dv.github.io/neuralbody/" rel="noopener ugc nofollow" target="_blank"><strong class="kh ir">GitHub</strong></a><strong class="kh ir">]</strong></p><p id="af69" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一个人需要多少台摄像机才能精确地再现人类的3D动作？过去，一个像样的机会需要一屋子的摄像机，尤其是像跳舞这样复杂的身体动作。多亏了神经体，一种带有结构化潜在代码的隐式神经表示，现在只需要四个摄像头。</p><p id="8cef" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这项任务，正式名称为“新视图合成”，在密集的单目摄像机阵列中是可能的，但在稀疏的多视图视频中似乎是不可能的。该模型不能完全依赖拼接多个2D图像来创建3D视图。相反，该模型学习在可变形网格上拼凑多个2D图像，并跨帧整合观察结果。</p><h1 id="6c59" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">新观点综述</h1><p id="38f1" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">去年引入了非常流行的神经网络领域(也称为NeRF)。NeRF是一个经过训练的简单全连接网络，被认为是使用多张图片合成复杂场景的新颖视图的最佳解决方案之一，具有最先进的效果。今年，研究人员通过实现更快更高级的场景重建扩展了NeRF。</p><p id="6763" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 7。pixelNeRF:来自一个或几个图像的神经辐射场[ </strong> <a class="ae ly" href="https://alexyu.net/pixelnerf/" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">网站</strong></a><strong class="kh ir">】</strong><br/>使用NeRF，需要在同一场景的多个视图上优化模型几个小时，如果不是几天，以追求合成高质量的图像，如下所示。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi ms"><img src="../Images/98d7bf91b77ea125145c471420901b24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WbYxhNDiUaPyFsnIVwFO6g.jpeg"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated"><a class="ae ly" href="http://source:%20https//arxiv.org/pdf/2003.08934.pdf" rel="noopener ugc nofollow" target="_blank">使用100张图像优化NeRF渲染一个打击乐器组新视图的过程。</a></p></figure><p id="09f7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">pixelNeRF解决了NeRF的缺点，承诺在更短的时间内用更少的场景视图再现新的视图。它通过引入一种架构来实现这一点，该架构使网络能够跨多个场景进行训练，以学习先前的场景。例如，在只有三个输入视图的情况下，pixelNeRF比NeRF产生了更好的结果。这可以通过需要更少的计算和更少的输入图像来潜在地降低合成视图的成本，使这种技术更容易获得。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi mt"><img src="../Images/a5407f4c1e4a21cc8031692ce6d3e64f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L7sgYk8e9rPsWX_Eo-qOrg.jpeg"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated"><a class="ae ly" href="https://www.youtube.com/watch?v=voebZx7f32g&amp;t=44s" rel="noopener ugc nofollow" target="_blank"> pixelNeRF </a>在用少量图像合成3D物体方面优于NeRF。</p></figure><p id="2c45" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> 8。NeRV:用于重新照明和视图合成的神经反射和可见性场[ </strong> <a class="ae ly" href="https://arxiv.org/pdf/2012.03927v1.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">纸张</strong> </a> <strong class="kh ir"> | </strong> <a class="ae ly" href="https://www.youtube.com/watch?v=nRyOzHpcr4Q" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">视频</strong></a><strong class="kh ir">|</strong><a class="ae ly" href="https://pratulsrinivasan.github.io/nerv/" rel="noopener ugc nofollow" target="_blank"><strong class="kh ir">GitHub</strong></a><strong class="kh ir">]</strong>【T20]虽然NeRF允许从新颖的视图渲染相同的场景，但它不提供用新颖的照明条件再现相同场景的方法。因此，谷歌、麻省理工和伯克利的研究人员提出了用于重新照明和视图合成的NeRV、神经反射和可见度场。NeRV在已知光照的情况下获取物体的图像，并在任意光照下从新的视点输出其3D表示。这可以重新照亮对象，甚至改变对象的材质。</p><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi mu"><img src="../Images/6286975794f49a5ca16b1e01a2d976e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AdyNLkrosjeUZkVuAeZ67A.jpeg"/></div></div></figure><figure class="ma mb mc md gt me gh gi paragraph-image"><div role="button" tabindex="0" class="mf mg di mh bf mi"><div class="gh gi mv"><img src="../Images/bbada947d6f0b0abd9d5e79c19b11519.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2FcMvOB5rKyIdYVjt8mTmA.jpeg"/></div></div><p class="ml mm gj gh gi mn mo bd b be z dk translated"><a class="ae ly" href="https://pratulsrinivasan.github.io/nerv/" rel="noopener ugc nofollow" target="_blank"> NeRV </a>可以改变照明条件和物体的材质。</p></figure><h1 id="6961" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">场景重建</h1><p id="c6ab" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">今天，实时3D场景重建已经应用于创建沉浸式增强现实和虚拟现实环境。今年，研究人员对用比以前少得多的资源重建场景产生了兴趣。</p><p id="1506" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">9。NeuralRecon:从单目视频[<a class="ae ly" href="https://zju3dv.github.io/neuralrecon/" rel="noopener ugc nofollow" target="_blank"><strong class="kh ir">Github</strong></a><strong class="kh ir">|</strong><a class="ae ly" href="https://arxiv.org/pdf/2104.00681.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="kh ir">纸</strong></a><strong class="kh ir">】</strong></p><p id="7bee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当前的重建技术只有在具有深度传感器的昂贵的专用相机存在的情况下才是实用的。如果我们可以只使用像智能手机上那样的单目相机来执行3D场景和对象重建，会怎么样？NeuralRecon可以通过已知相机姿态的单目相机实时捕捉视频来实现这一点。</p><p id="ca65" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">NeuralRecon的实时重建之所以成为可能，是因为它的计算成本比Atlas等现有的最先进方法相对更低。令人印象深刻的是，根据室内数据训练的NeuralRecon模型可以有效地重建墙壁等同质纹理，并很好地推广到室外场景。</p><p id="849e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">可以想象，智能手机可以利用NeuralRecon网络实时创建逼真的场景，有可能在不久的将来催化AR和VR的主流采用。</p><h1 id="8e6b" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">蛋白质相互作用的预测</h1><p id="0d5c" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">当DeepMind宣布其基于注意力的神经网络算法AlphaFold成功解决了生物学中50年的重大挑战——蛋白质折叠问题时，科学界感到敬畏。这证明了深度学习是解决蛋白质科学现存问题的有效方法。</p><p id="eea3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">10。蛋白质表面的快速端到端学习[  <a class="ae ly" href="https://www.biorxiv.org/content/10.1101/2020.12.28.424589v1" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">论文</strong></a><strong class="kh ir"/></p><p id="4859" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">蛋白质科学中的另一个问题是识别蛋白质的相互作用位点，以及通过扩展蛋白质如何彼此结合(由于它们的结构和化学特征)。在这篇论文中，作者表明，计算机视觉可以用来高精度地解决这两个问题，而无需预先计算蛋白质的手工化学和几何特征。因此，计算可以在占用很少内存的情况下进行，这使得在更大的蛋白质结构集合上进行这种预测变得切实可行。<br/>本文实现了最先进的性能，运行时间比以前的模型更快。蛋白质建模的这一突破可以预测蛋白质的功能，并催化合成蛋白质的设计。这种发展，加上其他深度学习方法，可能会被证明是解决蛋白质科学中许多开放问题的关键之一。</p><h1 id="5855" class="lb lc iq bd ld le lf lg lh li lj lk ll jw lm jx ln jz lo ka lp kc lq kd lr ls bi translated">CVPR 2021:图像合成是未来</h1><p id="07b3" class="pw-post-body-paragraph kf kg iq kh b ki lt jr kk kl lu ju kn ko lv kq kr ks lw ku kv kw lx ky kz la ij bi translated">在这次会议上，我们了解到计算机视觉领域取得的令人印象深刻的进展。特别是，从业者可以期待在生成新图像和视频的过程中得到更好的控制，而不是将控制委托给黑盒。我们也惊叹于社区在更精确地再现运动方面的进步及其在将计算机视觉应用于蛋白质科学方面的创造力。</p><p id="5127" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2021年CVPR是计算机视觉社区在推进图像生成技术方面足智多谋的证明。我们相信，在未来的几年中，这个致力于实践的社区将使图像生成技术保持快速突破。</p></div><div class="ab cl mw mx hu my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="ij ik il im in"><p id="c691" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nd">原载于2021年6月22日</em><a class="ae ly" href="https://www.datagen.tech/10-promising-synthesis-papers-from-cvpr-2021/" rel="noopener ugc nofollow" target="_blank"><em class="nd">https://www . data gen . tech</em></a><em class="nd">。</em></p></div></div>    
</body>
</html>