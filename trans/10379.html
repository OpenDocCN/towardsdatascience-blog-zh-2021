<html>
<head>
<title>Explanations and Features importance through robustness analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过稳健性分析说明和特性的重要性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/explanations-and-features-importance-through-robustness-analysis-396a7907777?source=collection_archive---------15-----------------------#2021-10-03">https://towardsdatascience.com/explanations-and-features-importance-through-robustness-analysis-396a7907777?source=collection_archive---------15-----------------------#2021-10-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="38fb" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">一种更精确、更可靠的方式来解释你的预测</h2></div><p id="21c9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本文基于Hsieh等人在ICLR 2021上发表的<a class="ae lb" href="https://arxiv.org/abs/2006.00442" rel="noopener ugc nofollow" target="_blank">评估和通过稳健性分析</a>进行解释的方法。</p><h1 id="ec26" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">一组重要的特性意味着什么？</h1><p id="9626" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">目前有许多不同的算法来为神经网络分类器预测提供解释。我们希望这些算法能够区分重要特征和不重要特征。如果小扰动可以改变预测，则一组特征是重要的，而如果需要大扰动来改变预测，则一组特征是不重要的。</p><p id="b7cd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">例如，考虑一个极其简单的分类问题，它有两个预测类(下面是红色和蓝色)和两个特征(<em class="lz"> x </em>和<em class="lz"> y </em>):</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/15dc3c3a1b8a2e24ba56fd3290976b8c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*4CqGHk8fyNe4SR7HFiuZCQ.png"/></div></figure><p id="af22" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">特性<em class="lz"> y </em>更重要，因为一个小的扰动就能轻易改变等级:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/8530c0e7c3765efb5da540536af50a69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*SX-6-GW7tXyqRCQ1g6vJ5Q.png"/></div></figure><p id="f645" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">虽然<em class="lz"> x </em>不那么重要，因为需要更大的扰动来改变类别:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi ma"><img src="../Images/2c6a96cea8c9156797e2cecbf725ff17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1062/format:webp/1*xwniamZGglUxk6Ab1MvyNg.png"/></div></figure><p id="52ec" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在MNIST数据集上，人们可以看到重要的要素是那些可以通过添加或删除数字线来更容易地更改预测的要素。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi mi"><img src="../Images/5d0c9c97291b13fd498358338eab2d03.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xLjwm9D-hk97LW6LTsUsiA.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated"><a class="ae lb" href="https://arxiv.org/abs/2006.00442" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="69fe" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于给定的分类器<em class="lz"> f </em>，输入<em class="lz"> x </em>和特征集<em class="lz"> S </em>，我们定义了“鲁棒性-S”(表示为<em class="lz"> g(f，x，S)) </em>，其量化了对<em class="lz"> S </em>上<em class="lz"> x </em>的最小扰动δ(<em class="lz">δ</em>在<em class="lz"> S </em>外等于零)，该扰动改变了预测:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/edd63a3ef01d1400618d9c1a14b7983c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*2HpDnzL91RmlIOpfgSdCgQ.png"/></div><p class="mn mo gj gh gi mp mq bd b be z dk translated"><a class="ae lb" href="https://arxiv.org/abs/2006.00442" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="752e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果我们把<em class="lz"> S_r </em>表示为重要特征的集合，把<em class="lz"> \not{S_r} </em>表示为它的补集，那么必然是这样的:g(f，x，S_r) 小而<em class="lz"> g(f，x，\not{S_r}) </em>大。<a class="ae lb" href="https://arxiv.org/abs/2006.00442" rel="noopener ugc nofollow" target="_blank">评估和通过鲁棒性分析进行解释的方法</a>的作者设计了一种算法“Greedy-AS ”,可以为S_r找到一个很好的候选。他们的实现可在<a class="ae lb" href="https://github.com/ChengYuHsieh/explanation_robustness" rel="noopener ugc nofollow" target="_blank"> Github </a>上获得。我们首先显示结果(下一节),然后更详细地介绍算法。</p><h1 id="ef96" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">结果</h1><p id="2d8f" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">对于给特征赋予重要性值的任何算法<em class="lz"> f </em>以及对于任何数量的特征<em class="lz"> k </em>，可以计算由<em class="lz"> f </em>标识的前<em class="lz"> k </em>特征定义的<em class="lz"> S </em>的<em class="lz"> g </em>函数。通过改变<em class="lz"> k </em>，可以为<em class="lz"> f </em>计算<em class="lz"> g </em>函数的AUC(下表中称为鲁棒性)。</p><p id="f963" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这样，人们可以通过算法的鲁棒性来比较不同的算法:</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi ms"><img src="../Images/f1e76bde1887fc9ce69d24d9cdbbfcd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JNPZEX_K78ki_CGBtbcHPQ.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated"><a class="ae lb" href="https://arxiv.org/abs/2006.00442" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="07bc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">可以看出，Greedy-AS能够比当前的SOTA算法更精确地识别特征，如IG(集成梯度)或EG(预期梯度)，这两种算法都在Captum(py torch的可解释库)中实现。</p><p id="82aa" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们想象一下所提供的不同解释(点击放大):</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi mt"><img src="../Images/9f8f94e1a963f13807f6d5dd797269ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mvufJm7S6std0Ce6dcU47w.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated"><a class="ae lb" href="https://arxiv.org/abs/2006.00442" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mj mk di ml bf mm"><div class="gh gi mu"><img src="../Images/257b44aac4eac76059d0a4d508b2f570.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UF8Lwn_qt2m-WRKoDOtldA.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated"><a class="ae lb" href="https://arxiv.org/abs/2006.00442" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><h1 id="7fa1" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">贪婪算法</h1><p id="7b49" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">为了计算<em class="lz"> S_r </em>，第一个简单的方法是以贪婪的方式来做:从空集<em class="lz"> S=∅ </em>开始，然后迭代地选择最大化<em class="lz"> g(f，x，s</em>∩<em class="lz">{ I })的最佳特征<em class="lz"> i </em>，当你选择k个特征时</em>停止。</p><p id="1eae" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然而，这种方法不能很好地处理功能的交互，因为功能的子集在同时添加时可能是重要的，而在逐个添加时可能是不重要的。为了处理这个问题，我们最大化期望g(f，x，S∩<em class="lz">{ I }</em>∪R<em class="lz">)</em>其中<em class="lz"> R </em>是期望所基于的一组随机特征。如果<em class="lz"> i </em>与其他特征相关，当<em class="lz"> R </em>包含时<em class="lz"> g </em>的值将会很大，因此期望值会更大。</p><p id="615a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这篇中型文章旨在快速简单地介绍这种算法。请随意阅读<a class="ae lb" href="https://arxiv.org/abs/2006.00442" rel="noopener ugc nofollow" target="_blank">评估和通过鲁棒性分析</a>进行解释的方法，以获得更多详细信息。</p></div></div>    
</body>
</html>