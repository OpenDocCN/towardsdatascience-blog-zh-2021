<html>
<head>
<title>Just How Do We Plot Multinomial Features?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">我们如何绘制多项式特征？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/just-how-do-we-plot-multinomial-features-d504fc5d2cff?source=collection_archive---------23-----------------------#2021-07-25">https://towardsdatascience.com/just-how-do-we-plot-multinomial-features-d504fc5d2cff?source=collection_archive---------23-----------------------#2021-07-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="fe97" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">是的，您可以绘制多项式特征。如果您的数据集有3个以上的独立要素，那么使用主成分分析(PCA)是一个赢家。让我们看看如何在Matlab中做到这一点。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/0554853f775221542a5e259b2b9fb407.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B7Ico_APscnVU723TFyMog.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">伯格和布什2017</p></figure><h1 id="ac70" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">我们的数据集</h1><p id="8e35" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">使用PCA，我们可以将我们的特征从(n)降低到2或3维，然后可以绘制这些特征。我们将从查看从<a class="ae mj" href="https://www.kaggle.com/arshid/iris-flower-dataset" rel="noopener ugc nofollow" target="_blank"> kaggle </a>下载的数据集开始。</p><p id="03bc" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">我们可以看到4个属性，这对于预测来说是超级的，但是不允许我们绘制可视化图。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/86a147555e5858c7d3e7382ab5c2322b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VdKsN3L1_vidRF9K23CxAQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图Kaggle的屏幕截图</p></figure><h1 id="fbbf" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">谅解</h1><p id="1dba" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">当我们使用PCA时，我们的主要目标是将我们的多项式特征(x1，x2，x3，…xn)带到二维(z1，z2)。下面的图1是将am2维(x1，x2)向下转换为一维(Z)的简化示例。</p><p id="ee32" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">注意<em class="mq">(类似于梯度下降)</em>我们如何在将x1 &amp; x2投影到z上时计算投影误差</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mr"><img src="../Images/f066030845f175062fa9cd0e3e8cc5f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RR7Ac0iAkui2923bEK1AFQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2:在<a class="ae mj" href="https://www.desmos.com/calculator" rel="noopener ugc nofollow" target="_blank"> Desmos </a>中绘制</p></figure><blockquote class="ms mt mu"><p id="c32f" class="ln lo mq lp b lq mk jr ls lt ml ju lv mv mm ly lz mw mn mc md mx mo mg mh mi ij bi translated">PCA的目标是找到数据投影的方向(向量),以便最小化投影误差。</p></blockquote><p id="ffe1" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">现在我们有了一个基本的理解，让我们开始编码…</p><h1 id="08dc" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">加载我们的数据集</h1><p id="dfa1" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">让我们加载数据，并确保正常化。一般来说，我们归一化是为了更好的ML性能。让我们也使用grp2idx将Y标签转换成数字。</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="3441" class="nd kw iq mz b gy ne nf l ng nh">%% Load our data<br/>clear;<br/>tbl = readtable(‘IRIS.csv’);<br/>[m,n] = size(tbl);<br/>X = tbl{:,1:n-1};<br/>[y,labels] = grp2idx(tbl{:,n});</span><span id="cd33" class="nd kw iq mz b gy ni nf l ng nh">[X_norm, mu, sigma] = featureNormalize(X);</span></pre><h1 id="d994" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">运行PCA</h1><p id="422f" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">PCA函数为<em class="mq"> m乘n </em>数据矩阵<code class="fe nj nk nl mz b">X</code>返回主成分系数，也称为负载。有了这个系数，我们现在可以投影“Z”了。</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="fb82" class="nd kw iq mz b gy ne nf l ng nh">%% use pca to reduce to 2 dimensions<br/>K = 2;<br/>[coef, S] = pca(X_norm);</span></pre><p id="76e0" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">如图3所示，我们的系数将是一个4x4矩阵，因为我们有4个特征。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/a6134f737dbe795ad3774307f6d4a18d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1332/format:webp/1*WcRZX5L6_7Qu6Do4goc6pQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图Matlab中pca的coef结果截图</p></figure><p id="844f" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">现在让我们创建一个coef_reduce，只包含我们喜欢的列数(K)。使用简单矩阵乘法将我们的特征从4减少到2。</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="fe31" class="nd kw iq mz b gy ne nf l ng nh">% calculate Z with 2 features<br/>coef_reduce = coef(:, 1:K);<br/>Z = X_norm * coef_reduce;</span></pre><p id="ee71" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">就这样，我们有一个包含4个特征的<strong class="lp ir"> X_norm </strong>矩阵，但现在已经减少到2个特征，存储在<strong class="lp ir"> Z </strong>矩阵中。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/f1fca9044eb22b69acfb74096f6da190.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eAWiCEpGEcIn9CZk3bCumw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图4:Z在Matlab中的结果截图</p></figure><h1 id="79a0" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">绘制结果</h1><p id="5667" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">在Matlab中，绘图现在成为一个相当标准的过程。因为我们有一个多元数据集，所以让我们创建一个托盘，用不同的颜色给每个类别着色。</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="2356" class="nd kw iq mz b gy ne nf l ng nh">%% Create colors pallette<br/>figure;<br/>palette = hsv(K + 1);<br/>colors = palette(y, :);</span><span id="1b8f" class="nd kw iq mz b gy ni nf l ng nh">% Plot the data, with different Y labels being different colors<br/>scatter(Z(:,1), Z(:,2), 30, colors);<br/>title(‘Pixel dataset plotted in 2D, using PCA for dimensionality reduction’);</span></pre><p id="4655" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">然后<strong class="lp ir">咔嘣</strong> …我们绘制了数据集。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/f5615ea089c0f147931468b644546d9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X4u121qICpBlzIeUs5__Sg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图5:在Matlab中绘制Z的结果截图</p></figure><h1 id="6d18" class="kv kw iq bd kx ky kz la lb lc ld le lf jw lg jx lh jz li ka lj kc lk kd ll lm bi translated">结论</h1><p id="3fb7" class="pw-post-body-paragraph ln lo iq lp b lq lr jr ls lt lu ju lv lw lx ly lz ma mb mc md me mf mg mh mi ij bi translated">因此，正如我们所见，绘制多项式数据集并不太难。自然，您不能像在绘制2个要素时那样理解单个要素，但我们可以了解数据集标注是否有意义。这在逻辑回归或K均值等分类算法中非常有用。</p><p id="5587" class="pw-post-body-paragraph ln lo iq lp b lq mk jr ls lt ml ju lv lw mm ly lz ma mn mc md me mo mg mh mi ij bi translated">ps。下面是featureNormalize函数</p><pre class="kg kh ki kj gt my mz na nb aw nc bi"><span id="83d8" class="nd kw iq mz b gy ne nf l ng nh">function [X_norm, mu, sigma] = featureNormalize(X)<br/>mu = mean(X);<br/>X_norm = bsxfun(@minus, X, mu);<br/>sigma = std(X_norm);<br/>X_norm = bsxfun(@rdivide, X_norm, sigma);<br/>end</span></pre></div></div>    
</body>
</html>