<html>
<head>
<title>Gentle introduction to 2D Hand Pose Estimation: Approach Explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">温和介绍2D手姿态估计:方法解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/gentle-introduction-to-2d-hand-pose-estimation-approach-explained-4348d6d79b11?source=collection_archive---------8-----------------------#2021-04-21">https://towardsdatascience.com/gentle-introduction-to-2d-hand-pose-estimation-approach-explained-4348d6d79b11?source=collection_archive---------8-----------------------#2021-04-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4b19" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><em class="kf">让自己舒服点，这是一篇很长的文章</em></h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/af2b3aff7cda01b91b7e6f4689f02d24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*CvYL8OI0js7MWUlM"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">作者图片</p></figure><p id="31c3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2018年，我花了6个月时间完成了关于手部姿态估计的硕士论文。那是我生命中充满挑战和深刻见解的一段时期，由此产生了40页的研究报告。直到现在，我还会收到工作面试邀请，以及感谢信和问题信，这让我认为这项研究仍然有意义，即使3年已经过去了——对于深度学习来说，这是一段很长的时间。</p><p id="4bd5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我的论文是关于2D和从一个单一的RGB图像的三维手姿态估计。3D部分是…嗯…我不会说这是一个彻底的失败，但接近于失败: )然而，2D部分(令人惊讶地)可与当时最先进的方法相媲美。</p><p id="2f5e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在接下来的几个月里，我计划将我的论文转化为一系列关于2D手部姿态估计的人性化教程:今天我们从温和地介绍这种方法开始，下一次我将展示如何在代码中实现它，稍后我们将介绍几种更高级的技术来提高模型性能。</p><p id="8b48" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">好奇吗？我们走吧！</p><p id="88b8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">内容</strong></p><h1 id="172d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">在手部姿势估计任务中，我们到底在估计什么</h1><p id="b076" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">一个好问题开始。</p><p id="27d8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">手的姿态由其关键点的位置来定义。因此，在手部姿态估计任务中，我们正在寻找关键点位置。</p><p id="a82f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">手有21个关键点:手腕+ 5个手指* (3个手指关节+ 1个指尖)= 21。想想看:知道了每个关键点的位置，我们就很容易理解一个人是否表现出“手掌向下”、“拇指向上”、“拳头”或“和平”的手势。</p><p id="e6c4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">秩序很重要。我们不仅仅需要“位置云”，我们需要关键点-位置对应。手腕在哪里？食指的指尖在哪里？等等。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi mp"><img src="../Images/3c7884a36f3d3062a1051dc8bb807c3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PrMoyDC0-fFt2_mnmw_aGQ.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><em class="kf">图片1。手关键点。这里秩序很重要。作者图片</em></p></figure><p id="b350" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一个典型的2D手姿态估计器看起来像这样:</p><ul class=""><li id="a3d3" class="mq mr iq ky b kz la lc ld lf ms lj mt ln mu lr mv mw mx my bi translated"><strong class="ky ir">输入</strong>:手形图像。对于许多手部姿态估计器来说，这是一个重要的假设:输入图像中只有一只手，并且图像被裁剪以排除任何其他身体部位和背景项目。</li><li id="2076" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated"><strong class="ky ir">输出</strong>:( x，y)关键点位置列表。位置可以表示为像素位置，其中x和y在范围[0，image_size]内，或者表示为归一化位置-在范围[0，1]内。归一化位置是像素位置除以图像大小。希望，下面的例子能说清楚。</li></ul><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ne"><img src="../Images/09f97d458d37f96722c2608657d821dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mThXrH5ioCkZNHAOvtfYaA.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><em class="kf">图片2。典型2D手姿态估计器的可视化</em>。作者图片</p></figure><p id="48ee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">还有几件事需要澄清。</p><p id="def5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">二手怎么样？通常，我们训练姿态估计器只为右手(或左手)工作。如果我们想对左手进行推理，我们只需给出一个看起来完全像右手的镜像模型。</p><p id="1f6f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通常，在论文(或演示)中<strong class="ky ir">手的姿势被可视化为一个骨架</strong>，有时还有多色手指。不要让它误导你，骨骼的绘制是为了可视化的目的，通过用线连接关键点。我们知道关键点的顺序，所以我们知道连接哪些关键点对来绘制骨架，对吗？</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nf"><img src="../Images/67f7bd861da18e1a7b6637558c4cd9c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZxLvJprsRlLu8h-Hxqn1dw.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><em class="kf">图片3。左图:手部姿势是如何被可视化的。右图:如何估计手部姿势。作者图片</em></p></figure><p id="1f13" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本教程中，我们将学习<strong class="ky ir">如何从单一的RGB图像</strong>中估计2D的手部姿态。这意味着，我们将训练一个输入单个RGB图像并输出图像平面上的关键点位置的模型。然而，手姿态估计是一个不同任务的大家族:你可以估计具有单个RGB、单个深度、单个RGBD(RGB+深度)图像或者甚至多个图像的2D或者甚至3D手姿态。也许，我会写一个帖子展示所有这些任务的多样性。但是现在，让我们继续。</p><h1 id="0725" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">哪里可以找到数据集(以及如何理解它)</h1><p id="af09" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">尽管收集和(更重要的是)标记手部姿势数据并不是一件容易的事情，但互联网上有几十个数据集。根据您的研究或业务目标，您可能需要:</p><ul class=""><li id="35bb" class="mq mr iq ky b kz la lc ld lf ms lj mt ln mu lr mv mw mx my bi translated">真实或合成数据</li><li id="0e8b" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated">照片或视频序列</li><li id="2f8e" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated">图像类型:RGB、深度、RGBD或立体</li><li id="af4f" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated">显示符号或与物体互动的手</li><li id="b4e6" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated">自我中心或第三人称观点</li><li id="5687" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated">标签:2D位置，三维位置，网格，手面具，...</li><li id="0dc1" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated">标记的关键点数量</li><li id="d4ce" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated">被遮挡的关键点是否被标记，图像中是否存在所有关键点等</li></ul><p id="436e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以在Github资源库[2]中找到的所有开源数据集的完整列表。出于学习和研究的目的，您可以随意使用这些数据集，但是，如果您计划为企业培训模型，请确保数据集许可允许这样做。</p><p id="8627" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于本教程，我选择了FreiHAND数据集[3]。它包含33k个右手实像，并有21个关键点的2D标签。这就是我们目前所需要的。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ng"><img src="../Images/263edb3efd8abb6cf079bb3184ceeaca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t0V4Q9Bv3X8_EuSuxPm1ZA.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><em class="kf">图片4。带有2D关键点标签的弗赖汉德数据集的随机样本。<br/>颜色:拇指绿色，食指青色，中指蓝色，戒指粉色，小指红色。</em></p></figure><p id="7151" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">FreiHAND数据集干净且结构良好，但是，请注意:</p><ul class=""><li id="313d" class="mq mr iq ky b kz la lc ld lf ms lj mt ln mu lr mv mw mx my bi translated">关键点位置的顺序和我在图1中展示的一样。标签存储在2D numpy数组中，其中手腕的位置在第0行，小指指尖的位置在第20行。</li><li id="2a3e" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated">你需要自己计算2D的位置，使用3D位置和相机矩阵。使用我在FreiHAND数据集Github repo [4]中找到的公式:</li></ul><pre class="kh ki kj kk gt nh ni nj nk aw nl bi"><span id="38b9" class="nm lt iq ni b gy nn no l np nq">def projectPoints(xyz, K): <br/>    xyz = np.array(xyz) <br/>    K = np.array(K) <br/>    uv = np.matmul(K, xyz.T).T <br/>    return uv[:, :2] / uv[:, -1:]</span></pre><ul class=""><li id="ff16" class="mq mr iq ky b kz la lc ld lf ms lj mt ln mu lr mv mw mx my bi translated">在训练部分，有130，240个图像，只有32，560个标签。这些标签仅适用于前32，560张原始图像。如果您想要在所有图像(原始图像+增强图像)上训练模型，以下是获取标签的方法:</li></ul><pre class="kh ki kj kk gt nh ni nj nk aw nl bi"><span id="b29c" class="nm lt iq ni b gy nn no l np nq">image_labels = labels_array[image_id % 32560]</span></pre><p id="d9d3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">那是因为图像32560看起来和图像0一模一样，以此类推。</p><h1 id="dbc4" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">如何对训练数据进行预处理</h1><p id="87fd" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">下面是一个逐步说明:</p><p id="d020" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">1.将数据集拆分为训练、验证和测试部分。像往常一样，训练集将用于训练模型，验证部分我们选择何时停止训练，我们在测试集上评估模型性能。</p><p id="3641" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2.将图像大小调整为128×128像素。手是一个简单的物体，所以这样的尺寸应该没问题。如果关键点位置是像素格式，请确保您也“调整”它们的大小。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nr"><img src="../Images/2dfcac8a4b8c62691b558a7f1ec30ed1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dnATfJDH_TPnLuGJpxB22w.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><em class="kf">图片5。调整图像大小时，关键点位置也应该“调整大小”。作者图片</em></p></figure><p id="1c85" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3.原始图像值在范围[0，255]内，最小-最大缩放它们到范围[0，1]内。</p><p id="ed4e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">4.标准使用训练集平均值和标准偏差归一化图像。每个通道(R，G，B)分别归一化，所以总共有3个均值和3个标准差。通道均值(和标准差)是在一个颜色通道内的所有图像的所有像素中计算的。</p><p id="9cdc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">5.从一系列关键点位置创建热图。用热图估计姿势是2D手(和人类)姿势估计中广泛使用的方法，你会在任何论文中看到它(稍作修改)。<br/>我相信研究[5]是最早使用热图的领域之一。</p><p id="99ee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们需要为每个关键点创建一个单独的热图，因此总共有21个热图。详情请看下图。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ns"><img src="../Images/2f03182dff912c8e1d6a0ef96b81a9ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tcMFmcFRG7o3xiZ-9b6azg.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><em class="kf">图片6。如何为关键点创建热图？作者图片</em></p></figure><p id="de65" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">热图是模糊的，以防止模型过度拟合，并使学习稳定和更快。实际的模糊参数并不重要，这里唯一的规则是不要让关键点“点”太大或太小。需要最终的最小-最大缩放，因为我们将在神经网络的最后一层使用Sigmoid变换，因此热图和模型预测在相同的范围内。</p><p id="f775" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">总结一下:</p><ul class=""><li id="7c4d" class="mq mr iq ky b kz la lc ld lf ms lj mt ln mu lr mv mw mx my bi translated">x是大小为3x128x128的图像。</li><li id="196a" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated">y是一个大小为21x128x128的数组，包含21个与输入图像大小相同的堆叠热图。确保热图顺序与图1中的关键点顺序相同。</li></ul><h1 id="ca7d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">使用什么模型架构</h1><p id="d706" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">我们需要某种编码器-解码器模型，因为输出与输入的大小相同——128×128。我个人在这里的偏好是UNet [6]。我和UNets一起工作过很多次，用它们完成各种分割任务，它们的表现总是令人惊叹。</p><p id="d20c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们不需要像原始论文[6]中那样复杂的UNet，因为手是一个简单的对象。让我们从这样的事情开始:</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nt"><img src="../Images/56623a50bf8ebe2058133b7d3c997b7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ovqDOUIAhpJ4hNlMHa8gqg.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><em class="kf">图7。我的简单的自定义UNet-like模型2D手姿态估计</em>。作者图片</p></figure><h1 id="2eac" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">培训详情</h1><p id="9fdb" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated"><strong class="ky ir">损失。</strong>大多数论文使用热图的MSE损失，例如，这两篇关于2D人体姿态估计的流行论文— [7]，[8]。我花了一些时间玩MSE损失，但它就是不起作用。</p><p id="f9be" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后我找到了论文[9]，其中作者使用交集/并集(IoU)损失训练了一个语义分割模型。2D手部姿态估计类似于分割，唯一的区别是热图具有范围[0，1]内的连续值，而不仅仅是二进制标签0/1作为分割掩码。然而，我们可以使用[9]中的公式，对热图稍加修改。而且很管用！</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/98cba4031f4cdd48433b9c0ba8c9d7de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*78bZt9dM2FDukswdsYVopg.png"/></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><em class="kf">形象8。如何计算热图的IoU损失？yi —预测值，ti —热图</em>中像素的目标值。<em class="kf">分别为每个热图计算损失，然后在所有21个热图中求平均值，再在批中的图像中求平均值。</em></p></figure><p id="2605" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">训练。</strong>对于本教程，我用batch_size=48和batches_per_epoch=50对模型进行了训练。我从学习率=0.1开始，每次训练损失停止减少时，就减少一半。当验证集上的损失停止下降时，我完成了培训。收敛花了大约200个历元(在GPU上花了2个小时)，我最终的训练和验证损失分别是0.437和0.476。</p><p id="abf9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这些数字只是地标。在训练您的模型时，您可能会得出不同数量的要收敛的历元和略有不同的最终损失。此外，随意增加/减少批量大小以适应您的机器内存，并增加/减少学习率。</p><h1 id="4d09" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">理解并可视化模型预测</h1><p id="a28e" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">所以现在我们有了一个输出热图的训练过的模型。然而，热图不是关键点位置，因此需要额外的后处理。通过查看热图，我们可以很容易地理解模型认为关键点位于何处。是的，它是“白色”区域的中心，一个具有最大值的区域。因此，让我们在后期处理中加入相同的逻辑。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nv"><img src="../Images/6ceff772fa118801471e00150e7252ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2J2RWoSlEGzq1Eg7J_QMKg.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><em class="kf">图片9。测试集中随机图像的模型输出。作者图片</em></p></figure><p id="d7b0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有两个选项:</p><ol class=""><li id="a990" class="mq mr iq ky b kz la lc ld lf ms lj mt ln mu lr nw mw mx my bi translated">最简单的是，我们可以在热图中找到一个值最大的像素。这个像素的(x，y)位置是关键点位置。</li><li id="0198" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr nw mw mx my bi translated">但是更可靠的方法是计算所有热图值的平均值。关于如何做到这一点的详细信息，请参见下图。</li></ol><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nx"><img src="../Images/dc11f1145e9b68436d5ab099772a26e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1d4R7Dv-W4zyY1aWp2Vi5w.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><em class="kf">图片10。如何通过平均计算热图中的关键点位置？作者图片</em></p></figure><p id="59e3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们准备评估模型并可视化预测。该模型适用于大多数姿势，但是，它不适用于具有严重关键点遮挡的姿势。好吧，标记被遮挡的关键点即使对于人类注释者来说也不是一件容易的事情。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi ny"><img src="../Images/75a8cd726487359bbb9cbc121df55508.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MiVDWs2MTZlHO3dOnl0VCg.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated"><em class="kf">图11。可视化测试集上的预测。</em></p></figure><p id="3433" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是测试集上的模型精确度。通过对图像中所有关键点的误差进行平均，然后对所有图像的误差进行平均，来计算平均误差。4.5% —还不错！</p><ul class=""><li id="6a09" class="mq mr iq ky b kz la lc ld lf ms lj mt ln mu lr mv mw mx my bi translated">平均误差:图像尺寸的4.5%</li><li id="c3bc" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated">平均误差:图像128×128为6个像素</li><li id="ad42" class="mq mr iq ky b kz mz lc na lf nb lj nc ln nd lr mv mw mx my bi translated">平均误差:图像224×224为10像素</li></ul><h1 id="cc04" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">下一步是什么</h1><p id="6875" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">我很高兴你已经读到这里了。希望，现在2D手的姿势估计对你清楚多了。但是…</p><p id="8d6b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当你能编码的时候你就知道了，对吗？: )所以在本教程的下一部分，我将分享和解释所有的代码，你将学习如何自己训练一个2D手姿态估计器。不要关闭你的Jupyter笔记本！</p><p id="e4af" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="nz">更新:第二部已经在</em> <a class="ae oa" href="https://notrocketscience.blog/gentle-introduction-to-2d-hand-pose-estimation-lets-code-it/" rel="noopener ugc nofollow" target="_blank"> <em class="nz">这里可用。</em> </a></p></div><div class="ab cl ob oc hu od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="ij ik il im in"><h2 id="5f98" class="nm lt iq bd lu oi oj dn ly ok ol dp mc lf om on me lj oo op mg ln oq or mi os bi translated">参考</h2><p id="ceba" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">[1]如果你想检查我的论文，这里有<a class="ae oa" href="https://er.ucu.edu.ua/bitstream/handle/1/1327/Chernytska%20-%203D%20Hand%20Pose%20Estimation%20from%20Single%20RGB%20Camera%20-%20master%20thesis.pdf" rel="noopener ugc nofollow" target="_blank">正文</a>和<a class="ae oa" href="https://github.com/OlgaChernytska/3D-Hand-Pose-Estimation" rel="noopener ugc nofollow" target="_blank">代码</a>。</p><p id="4ed4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2]"<a class="ae oa" href="https://github.com/xinghaochen/awesome-hand-pose-estimation" rel="noopener ugc nofollow" target="_blank">Awesome Hand Pose Estimation</a>"，这是一个Github知识库，包含一系列开源数据集和论文。</p><p id="6a3f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[3] FreiHAND数据集，<a class="ae oa" href="https://lmb.informatik.uni-freiburg.de/resources/datasets/FreihandDataset.en.html" rel="noopener ugc nofollow" target="_blank">这里</a>可以下载。</p><p id="4187" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[4] FreiHAND数据集<a class="ae oa" href="https://github.com/lmb-freiburg/freihand" rel="noopener ugc nofollow" target="_blank"> Github知识库</a>。</p><p id="b9db" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[5]托马斯·菲斯特，詹姆斯·查尔斯，安德鲁·齐塞曼。"<a class="ae oa" href="https://arxiv.org/abs/1506.02897" rel="noopener ugc nofollow" target="_blank">视频中人体姿态估计的流动变换</a>"</p><p id="6a14" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[6]奥拉夫·龙内贝格，菲利普·菲舍尔，托马斯·布罗克斯。"<a class="ae oa" href="https://arxiv.org/abs/1505.04597" rel="noopener ugc nofollow" target="_blank"> U-Net:生物医学图像分割的卷积网络</a>"</p><p id="c33b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[7]施-韦恩，瓦伦·罗摩克里希纳，金德武夫，亚塞尔·谢赫。<a class="ae oa" href="https://arxiv.org/abs/1602.00134" rel="noopener ugc nofollow" target="_blank">回旋摆姿机</a></p><p id="5bbb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[8]亚历杭德罗·纽维尔，·杨，贾登."<a class="ae oa" href="https://arxiv.org/abs/1603.06937" rel="noopener ugc nofollow" target="_blank">用于人体姿态估计的堆叠沙漏网络</a>"</p><p id="86a0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[9]阿蒂古尔·拉赫曼博士和王洋。"<a class="ae oa" href="http://cs.umanitoba.ca/~ywang/papers/isvc16.pdf" rel="noopener ugc nofollow" target="_blank">优化用于图像分割的深度神经网络中的交并运算</a>"</p></div><div class="ab cl ob oc hu od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="ij ik il im in"><p id="2199" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="nz">原载于2021年4月21日</em><a class="ae oa" href="https://notrocketscience.blog/" rel="noopener ugc nofollow" target="_blank"><em class="nz">https:notrocketseconomy . blog</em></a><em class="nz">。</em></p><p id="445a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="nz">如果你想阅读更多类似的教程，请订阅我的博客“不是火箭科学”——</em><a class="ae oa" href="https://t.me/notrocketscienceblog" rel="noopener ugc nofollow" target="_blank"><em class="nz">电报</em> </a> <em class="nz">和</em> <a class="ae oa" href="https://twitter.com/nRocketScience" rel="noopener ugc nofollow" target="_blank"> <em class="nz">推特</em> </a> <em class="nz">。</em></p></div></div>    
</body>
</html>