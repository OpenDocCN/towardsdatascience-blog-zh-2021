<html>
<head>
<title>Bisecting K-Means Algorithm — Clustering in Machine Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习中的二分K均值聚类算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bisecting-k-means-algorithm-clustering-in-machine-learning-1bd32be71c1c?source=collection_archive---------11-----------------------#2021-06-16">https://towardsdatascience.com/bisecting-k-means-algorithm-clustering-in-machine-learning-1bd32be71c1c?source=collection_archive---------11-----------------------#2021-06-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b3c2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">理解对分K-均值聚类算法(视觉和代码)</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/9eadf55d6ee4aa12a1f763455d20df68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nsoruAOXa-peSeHFKXF4rQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来自<a class="ae ky" href="https://unsplash.com/photos/dnGgAIRNnsE" rel="noopener ugc nofollow" target="_blank">源</a>的修改图像</p></figure><p id="d861" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di"> B </span> isecting K-means聚类技术是对常规K-means算法的一点修改，其中您固定了将数据划分为聚类的过程。所以，类似于K-means，我们首先初始化K个质心<em class="me">(你可以随机地做或者可以有一些先验)</em>。之后我们应用规则的<strong class="lb iu"> K-means，K=2 </strong> <em class="me">(这就是为什么单词平分)。</em>我们<strong class="lb iu">不断重复这一对分步骤，直到达到期望的聚类数</strong>。在第一次对分<em class="me">(当我们有2个聚类)</em>完成<em class="me">，</em>之后，人们可以想到多种策略来选择其中一个聚类，并在该聚类内重复对分和分配的整个过程——例如:<em class="me">选择具有最大方差的聚类或展开的聚类，选择具有最大数量数据点的聚类，等等。</em></p><p id="b583" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="me">没有时间看完整个博客？然后看这个快速的&lt; 60秒的YouTube短片—</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mf mg l"/></div></figure><p id="8d8d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以想象整个流程，如下图所示— </p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/f7fa0b9f070a0ff8900c00df85fcae38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*EudBrzz8JnN7ZbM_CqQI_Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">按作者划分K-Means |图像的步骤</p></figure><p id="ed0e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如上图所示，我们首先假设所有数据都在一个聚类<em class="me">(第一张图)</em>中，在第一步之后，我们得到2个<em class="me">(二分法)</em>聚类，然后我们检查是否达到了期望的聚类数。如果没有，我们从上一步的两个聚类中选择一个<em class="me">(红色)</em>，再次应用K=2的K-means，并且我们<strong class="lb iu">重复“<em class="me">检查</em>和“<em class="me">二等分</em>”步骤。</strong></p><p id="1530" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可能已经猜到了，这看起来像是层次聚类和K均值聚类的混合。因为在这里您正在构建一个树，这是一个层次结构，其中一个节点根据K-means策略和赋值被分成两个子节点。</p><h2 id="66f8" class="mi mj it bd mk ml mm dn mn mo mp dp mq li mr ms mt lm mu mv mw lq mx my mz na bi translated">对K-Means的改进</h2><p id="d979" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">与常规K-Means不同，在常规K-Means中，我们在每一步计算每个数据点和质心之间的距离，直到满足收敛标准，在这里，我们只做一次<em class="me">(第一步)</em>，之后，我们只使用特定聚类中的<strong class="lb iu">数据点来计算距离</strong>和进一步细分，使其比常规K-Means更有效。它还可以<strong class="lb iu">识别任何形状和大小的簇</strong>，不像<strong class="lb iu"> K-Means假设的是球形簇。</strong> <em class="me">我还发现了一篇有趣的研究文章，比较了K-Means和二分法K-Means在分析网络日志数据时的性能——在这里阅读</em><a class="ae ky" href="https://ijeter.everscience.org/Manuscripts/Volume-4/Issue-8/Vol-4-issue-8-M-23.pdf" rel="noopener ugc nofollow" target="_blank"><em class="me"/></a><em class="me">。</em></p><h2 id="c336" class="mi mj it bd mk ml mm dn mn mo mp dp mq li mr ms mt lm mu mv mw lq mx my mz na bi translated">限制</h2><p id="6986" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">因为基本聚类技术在这里仍然是K-Means，所以在离群值的情况下，这个<strong class="lb iu">算法也可能遭受对聚类中心</strong>的错误估计。</p><blockquote class="ng nh ni"><p id="b2af" class="kz la me lb b lc ld ju le lf lg jx lh nj lj lk ll nk ln lo lp nl lr ls lt lu im bi translated"><strong class="lb iu">提示:</strong><strong class="lb iu">二等分K-Medoid </strong>聚类技术可以帮助你解决上述限制。</p></blockquote><h2 id="18e9" class="mi mj it bd mk ml mm dn mn mo mp dp mq li mr ms mt lm mu mv mw lq mx my mz na bi translated">密码</h2><p id="a032" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated"><em class="me">我将使用Python的Sklearn库来实现目的— </em></p><pre class="kj kk kl km gt nm nn no np aw nq bi"><span id="65ec" class="mi mj it nn b gy nr ns l nt nu"><em class="me">from sklearn.cluster import KMeans<br/>import numpy as np</em></span><span id="01ef" class="mi mj it nn b gy nv ns l nt nu"><em class="me">X = np.array([[1, 2], [2, 1], [1, 1.5], [1.5, 1],<br/>               [10, 2], [10, 4], [10, 0], [10, 1],<br/>               [1, 10], [2, 11], [1.5, 9], [1, 10.5],<br/>               [10.5, 9], [9, 9.5], [9.5, 9], [10, 10]])</em></span><span id="5554" class="mi mj it nn b gy nv ns l nt nu"><em class="me">K = 4<br/>current_clusters = 1</em></span><span id="f104" class="mi mj it nn b gy nv ns l nt nu"><em class="me">while current_clusters != K:</em></span><span id="d0f4" class="mi mj it nn b gy nv ns l nt nu"><em class="me">    kmeans = KMeans(n_clusters=2).fit(X)<br/>    current_clusters += 1<br/>    split += 1</em></span><span id="5e7d" class="mi mj it nn b gy nv ns l nt nu"><em class="me">    cluster_centers = kmeans.cluster_centers_</em></span><span id="f523" class="mi mj it nn b gy nv ns l nt nu"><em class="me">    sse = [0]*2<br/>    for point, label in zip(X, kmeans.labels_):<br/>        sse[label] += np.square(point-cluster_centers[label]).sum()</em></span><span id="29e6" class="mi mj it nn b gy nv ns l nt nu"><em class="me">    chosen_cluster = np.argmax(sse, axis=0)<br/>    chosen_cluster_data = X[kmeans.labels_ == chosen_cluster]<br/>    X = chosen_cluster_data</em></span></pre><p id="8ec4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="me">下图展示了上述示例代码的图示演练— </em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/bb3cf34767dc26f4dc7ea6c45648a059.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*ClwknaPQLulchvYxvOVzIw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="nx">上例演练|作者图片</em></p></figure><p id="736f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简而言之，当考虑将K-Means作为算法的选择时，尝试平分K-Means也是一个好主意。由于上面讨论的原因，很有可能你会得到更好的结果。</p></div><div class="ab cl ny nz hx oa" role="separator"><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od oe"/><span class="ob bw bk oc od"/></div><div class="im in io ip iq"><p id="4ff7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">希望你喜欢阅读这篇博客。感谢您的宝贵时间！</p></div></div>    
</body>
</html>