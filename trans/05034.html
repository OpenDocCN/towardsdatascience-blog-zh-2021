<html>
<head>
<title>ElegantRL: Mastering PPO Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">ElegantRL:掌握PPO算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/elegantrl-mastering-the-ppo-algorithm-part-i-9f36bc47b791?source=collection_archive---------11-----------------------#2021-05-03">https://towardsdatascience.com/elegantrl-mastering-the-ppo-algorithm-part-i-9f36bc47b791?source=collection_archive---------11-----------------------#2021-05-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="d00d" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="73f6" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">近似策略优化算法(PPO)教程</h2></div><p id="cb24" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">由<a class="ae lk" href="https://twitter.com/XiaoYangLiu10" rel="noopener ugc nofollow" target="_blank"/>和Steven Li撰写的这篇文章描述了在ElegantRL库中实现近似策略优化(PPO)算法(<a class="ae lk" href="https://twitter.com/softraeh" rel="noopener ugc nofollow" target="_blank"> Twitter </a>和<a class="ae lk" href="https://github.com/AI4Finance-LLC/ElegantRL" rel="noopener ugc nofollow" target="_blank"> Github </a>)。PPO算法是目前广泛使用的深度学习算法，被许多研究机构和学者选为基准。</p><p id="46e0" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">请查看ElegantRL库的介绍性<a class="ae lk" rel="noopener" target="_blank" href="/elegantrl-a-lightweight-and-stable-deep-reinforcement-learning-library-95cef5f3460b">文章</a>。</p></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><h1 id="dd11" class="ls lt iq bd lu lv lw lx ly lz ma mb mc kf md kg me ki mf kj mg kl mh km mi mj bi translated"><strong class="ak">PPO算法概述</strong></h1><p id="2686" class="pw-post-body-paragraph ko kp iq kq b kr mk ka kt ku ml kd kw kx mm kz la lb mn ld le lf mo lh li lj ij bi translated"><strong class="kq ja">符合策略和不符合策略的算法有什么区别？</strong></p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi mp"><img src="../Images/16b30e461729069a8c5f85d495083c8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rngO1crkBFMLCNb3"/></div></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">图一。策略外和策略内算法。【图片由作者提供。]</p></figure><p id="2217" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">对于基于策略的算法，它们根据当前策略网络生成的转换来更新策略网络。评论家网络将在普通环境下对当前策略网络做出更准确的价值预测。</p><p id="34bb" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">对于非策略算法，它们允许使用旧策略的转换来更新当前策略网络。因此，可以重新利用旧的转换，如图1所示，点分散在由不同策略生成的轨迹上，这提高了采样效率并减少了总的训练步骤。</p><p id="8e9b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">有没有一种方法可以提高基于策略的算法的样本效率而不损失它们的好处？</strong></p><p id="fbc9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">答案是<strong class="kq ja">是的</strong>。作为一种基于策略的算法，PPO利用替代目标来避免新策略与旧策略相差太远，从而解决了样本效率问题。代理目标是PPO的关键特征，因为它既规范了策略更新，又支持训练数据的重用。</p><p id="5ea9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">标准PPO有一个削波目标函数[1]:</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi nf"><img src="../Images/ce5045cc1fa2491d32b64e99a49764df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*69iXbtfrnGs9_C-f"/></div></div></figure><p id="8b9a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">PPO-Clip只是在概率比项上加了一个剪辑区间，剪辑成一个范围[1 — ϶，1 + ϶]，其中϶是一个超参数。则该函数取原始比率和限幅比率之间的最小值。</p><p id="7c3c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">目标函数内部的概率比项是什么？</strong></p><p id="1c19" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">由于替代目标，PPO允许使用梯度上升的多个时期，而不会与旧政策相差太远。在这种情况下，我们必须使用重要性抽样估计器来补偿训练数据分布和当前策略状态分布之间的差距，这是概率比项:</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/de3eb68bc8b9c3dc675dead4b02d98d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:684/0*06kcnitr7OKraJeS"/></div></figure></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><h1 id="e2b5" class="ls lt iq bd lu lv lw lx ly lz ma mb mc kf md kg me ki mf kj mg kl mh km mi mj bi translated"><strong class="ak">伪代码和算法细节</strong></h1><p id="8f86" class="pw-post-body-paragraph ko kp iq kq b kr mk ka kt ku ml kd kw kx mm kz la lb mn ld le lf mo lh li lj ij bi translated">对于策略正则化，标准PPO算法使用剪切目标；对于策略参数化，标准PPO算法在连续动作空间使用高斯分布，在离散动作空间使用Softmax函数。此外，它遵循一个经典的演员-评论家框架，由四个部分组成:</p><ul class=""><li id="ff65" class="nh ni iq kq b kr ks ku kv kx nj lb nk lf nl lj nm nn no np bi translated"><strong class="kq ja">初始化</strong>:初始化相关属性和网络。</li><li id="9c98" class="nh ni iq kq b kr nq ku nr kx ns lb nt lf nu lj nm nn no np bi translated"><strong class="kq ja">探索</strong>:通过演员网络与环境的互动来探索过渡。</li><li id="ee23" class="nh ni iq kq b kr nq ku nr kx ns lb nt lf nu lj nm nn no np bi translated"><strong class="kq ja">计算</strong>:计算相关变量，如比率项、确切报酬、估计优势等。</li><li id="17f5" class="nh ni iq kq b kr nq ku nr kx ns lb nt lf nu lj nm nn no np bi translated"><strong class="kq ja">更新</strong>:基于损失函数和目标函数更新演员和评论家网络。</li></ul><p id="425d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">接下来，我们解释Alg。1以逐步的方式:</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi nv"><img src="../Images/966ff73e93acc4435fa7568f2ebc1798.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9KP9fMyqHfQs2tD3"/></div></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">Alg。1:PPO-Clip算法。来自[1]。</p></figure><ul class=""><li id="2ee2" class="nh ni iq kq b kr ks ku kv kx nj lb nk lf nl lj nm nn no np bi translated"><strong class="kq ja">步骤1 </strong>:初始化演员和评论家网络以及参数϶.</li><li id="967f" class="nh ni iq kq b kr nq ku nr kx ns lb nt lf nu lj nm nn no np bi translated"><strong class="kq ja">第三步</strong>:从最新的演员策略中收集一批轨迹。</li><li id="96a9" class="nh ni iq kq b kr nq ku nr kx ns lb nt lf nu lj nm nn no np bi translated"><strong class="kq ja">第4步</strong>:计算每一步中每条轨迹的确切奖励。</li><li id="ce12" class="nh ni iq kq b kr nq ku nr kx ns lb nt lf nu lj nm nn no np bi translated"><strong class="kq ja">步骤5 </strong>:根据最新的Critic网络计算每个轨迹的估计优势。</li><li id="417f" class="nh ni iq kq b kr nq ku nr kx ns lb nt lf nu lj nm nn no np bi translated"><strong class="kq ja">步骤6 </strong>:通过在K个时期内最大化PPO-CLIP目标函数，通过随机梯度上升来更新演员的参数。当我们开始运行优化时，旧策略与更新后的策略相同，这意味着比率为1。随着我们不断更新政策，该比率将开始触及上限。</li><li id="9a83" class="nh ni iq kq b kr nq ku nr kx ns lb nt lf nu lj nm nn no np bi translated"><strong class="kq ja">步骤7 </strong>:通过均方误差的梯度下降来更新评论家的参数。</li></ul></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><h1 id="f9d6" class="ls lt iq bd lu lv lw lx ly lz ma mb mc kf md kg me ki mf kj mg kl mh km mi mj bi translated"><strong class="ak">雅致放射中的多酚氧化酶</strong></h1><p id="05af" class="pw-post-body-paragraph ko kp iq kq b kr mk ka kt ku ml kd kw kx mm kz la lb mn ld le lf mo lh li lj ij bi translated">ElegantRL中的每个DRL代理都遵循其基类的层次结构，以实现轻量级编码。在本节中，我们将讨论标准PPO在<strong class="kq ja"> agent.py </strong>中的设计和实现。</p><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi nw"><img src="../Images/05f696a7041f5d635000c17cadf62888.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zlcOXg9oJX2_wuzq6hwQGg.png"/></div></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">图二。PPO算法的继承层次结构。【图片由作者提供。]</p></figure><h1 id="d828" class="ls lt iq bd lu lv nx lx ly lz ny mb mc kf nz kg me ki oa kj mg kl ob km mi mj bi translated"><strong class="ak">代理库:</strong></h1><p id="52a3" class="pw-post-body-paragraph ko kp iq kq b kr mk ka kt ku ml kd kw kx mm kz la lb mn ld le lf mo lh li lj ij bi translated">基本上，大多数经典的DRL算法都继承自AgentBase类。在AgentBase内部，我们初始化公共的<strong class="kq ja">变量</strong>:</p><ul class=""><li id="8f69" class="nh ni iq kq b kr ks ku kv kx nj lb nk lf nl lj nm nn no np bi translated">学习率</li><li id="771f" class="nh ni iq kq b kr nq ku nr kx ns lb nt lf nu lj nm nn no np bi translated">环境状态</li></ul><p id="61e0" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">和<strong class="kq ja">功能</strong>:</p><ul class=""><li id="db9a" class="nh ni iq kq b kr ks ku kv kx nj lb nk lf nl lj nm nn no np bi translated"><strong class="kq ja"> select_action() </strong>:给定环境的状态，获取相应的动作。</li><li id="366e" class="nh ni iq kq b kr nq ku nr kx ns lb nt lf nu lj nm nn no np bi translated"><strong class="kq ja"> explore_env() </strong>:使用<strong class="kq ja"> select_action() </strong>，通过演员网络和环境之间的交互来探索过渡。</li><li id="0ce3" class="nh ni iq kq b kr nq ku nr kx ns lb nt lf nu lj nm nn no np bi translated"><strong class="kq ja"> update_net() </strong>:从ReplayBuffer中采样批量数据，更新神经网络。</li><li id="21d9" class="nh ni iq kq b kr nq ku nr kx ns lb nt lf nu lj nm nn no np bi translated"><strong class="kq ja"> save_load_model() </strong>:保存模型用于训练，或者加载模型用于推理。</li><li id="53c5" class="nh ni iq kq b kr nq ku nr kx ns lb nt lf nu lj nm nn no np bi translated"><strong class="kq ja"> soft_update() </strong>:如果需要，从当前网络更新目标网络。</li></ul><h1 id="763f" class="ls lt iq bd lu lv nx lx ly lz ny mb mc kf nz kg me ki oa kj mg kl ob km mi mj bi translated">AgentPPO:</h1><p id="7eec" class="pw-post-body-paragraph ko kp iq kq b kr mk ka kt ku ml kd kw kx mm kz la lb mn ld le lf mo lh li lj ij bi translated">在AgentPPO中，我们添加了一些与PPO算法相关的新变量，并重新定义了几个函数。</p><p id="c7a4" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们遵循Alg中的步骤。1、先讨论变量(第1行):</p><ul class=""><li id="8a73" class="nh ni iq kq b kr ks ku kv kx nj lb nk lf nl lj nm nn no np bi translated"><strong class="kq ja"> ratio_clip </strong>:限幅后的区间范围。</li><li id="8abd" class="nh ni iq kq b kr nq ku nr kx ns lb nt lf nu lj nm nn no np bi translated"><strong class="kq ja">λ_熵</strong>:探测参数。</li><li id="6b87" class="nh ni iq kq b kr nq ku nr kx ns lb nt lf nu lj nm nn no np bi translated"><strong class="kq ja"> act = ActorPPO() </strong>:演员网。</li><li id="bfc0" class="nh ni iq kq b kr nq ku nr kx ns lb nt lf nu lj nm nn no np bi translated"><strong class="kq ja"> cri = CriticAdv() </strong>:评论家网。</li></ul><figure class="mq mr ms mt gt mu gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi oc"><img src="../Images/ff0379521942832d0286d50d4dd220d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4G0WpM5hjcKFh2r87RwTfg.png"/></div></div><p class="nb nc gj gh gi nd ne bd b be z dk translated">图3。AgentPPO中的函数结构。【图片由作者提供。]</p></figure><p id="3730" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们在Alg中拆分for循环(第3–7行)。1分为两个核心功能:</p><ul class=""><li id="b000" class="nh ni iq kq b kr ks ku kv kx nj lb nk lf nl lj nm nn no np bi translated"><strong class="kq ja"> explore_env </strong>(第3行):使用Actor-network来探索环境，并将结果转换存储到ReplayBuffer中。</li></ul><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="a929" class="oi lt iq oe b gy oj ok l ol om">def <strong class="oe ja">explore_env</strong>(self, env, buffer, target_step, reward_scale, gamma):<br/>    """    <br/>    :<strong class="oe ja">env:</strong> RL training environment.<br/>    :<strong class="oe ja">buffer:</strong> Experience Replay Buffer.<br/>    :<strong class="oe ja">int target_step:</strong> explored target_step number of step in env.<br/>    :<strong class="oe ja">float reward_scale:</strong> scale reward, 'reward * reward_scale'.<br/>    :<strong class="oe ja">float gamma:</strong> discount factor, 'mask = 0.0 if done else gamma'.<br/>    :<strong class="oe ja">return</strong> <strong class="oe ja">int target_step:</strong> collected target_step number of step in <br/>                             env.<br/>    """<br/>    <strong class="oe ja"># empty the ReplayBuffer (necessary for on-policy algorithm)</strong><br/>    buffer.empty_buffer_before_explore()<br/>    actual_step = 0 <br/>    <br/>    while actual_step &lt; target_step:<br/>        <strong class="oe ja"># for each trajectory, reset the environment at beginning </strong><br/>        state = env.reset()</span><span id="e63e" class="oi lt iq oe b gy on ok l ol om">        for _ in range(env.max_step): <br/>            <strong class="oe ja"># get the action from Actor network</strong><br/>            action, noise = self.select_action(state) <br/>            <br/>            <strong class="oe ja"># step the environment forward based on the action</strong><br/>            next_state, reward, done, _ = env.step(np.tanh(action))<br/>            <br/>            ...</span><span id="493b" class="oi lt iq oe b gy on ok l ol om">            other = (reward * reward_scale, 0.0 if done else gamma,      <br/>                     *action, *noise) <br/>            <strong class="oe ja"># append transition into ReplayBuffer</strong><br/>            buffer.append_buffer(state, other)</span><span id="17bb" class="oi lt iq oe b gy on ok l ol om">            ...</span><span id="7e1e" class="oi lt iq oe b gy on ok l ol om">            state = next_state</span><span id="7bb9" class="oi lt iq oe b gy on ok l ol om">    return actual_step</span></pre><ul class=""><li id="53d7" class="nh ni iq kq b kr ks ku kv kx nj lb nk lf nl lj nm nn no np bi translated"><strong class="kq ja"> update_policy </strong>(第4–7行):从ReplayBuffer中抽取一批转换，并计算更新网络的目标。</li></ul><pre class="mq mr ms mt gt od oe of og aw oh bi"><span id="dc28" class="oi lt iq oe b gy oj ok l ol om">def <strong class="oe ja">update_net</strong>(self, buffer, _target_step, batch_size, repeat_times=4): <br/>   """<br/>   <strong class="oe ja">:buffer:</strong> Experience replay buffer.<br/>   :<strong class="oe ja">int target_step:</strong> explore target_step number of step in env.<br/>   <strong class="oe ja">:int batch_size:</strong> sample batch_size of data for Stochastic<br/>                    Gradient Descent        <br/>   <strong class="oe ja">:float repeat_times:</strong> the times of sample batch.<br/>   <strong class="oe ja">:return float obj_a:</strong> the objective value of Actor.<br/>   <strong class="oe ja">:return float obj_c:</strong> the objective value of Critic.<br/>   """</span><span id="8ebb" class="oi lt iq oe b gy on ok l ol om">    ...</span><span id="24bc" class="oi lt iq oe b gy on ok l ol om">    with torch.no_grad(): <br/>       <strong class="oe ja"> # sample data from ReplayBuffer</strong><br/>        buf_reward, buf_mask, buf_action, buf_noise, buf_state = <br/>        buffer.sample_all() <br/>        <strong class="oe ja"># compute the expected value from critic network</strong><br/>        bs = 2 ** 10 <br/>        buf_value = torch.cat([self.cri(buf_state[i:i + bs]) for i <br/>                    in range(0, buf_state.size(0), bs)], dim=0) <br/>        <strong class="oe ja"># compute the log of probability, which is the denominator <br/>        of the probability ratio in the surrogate objectve</strong><br/>        buf_logprob = -(buf_noise.pow(2).__mul__(0.5) + <br/>                      self.act.a_std_log +                              <br/>                      self.act.sqrt_2pi_log).sum(1) <br/>       <strong class="oe ja"> # compute the reward and advantage</strong><br/>        buf_r_sum, buf_advantage = self.compute_reward(buf_len, <br/>        buf_reward, buf_mask, buf_value) </span><span id="6aa2" class="oi lt iq oe b gy on ok l ol om">    ...</span><span id="eae3" class="oi lt iq oe b gy on ok l ol om">    for _ in range(int(repeat_times * buf_len / batch_size)):  <br/>        <strong class="oe ja"># randomly select a transition</strong><br/>        indices = torch.randint(buf_len, size=(batch_size,), <br/>                  requires_grad=False, device=self.device) <br/>        state = buf_state[indices] <br/>        action = buf_action[indices] <br/>        r_sum = buf_r_sum[indices] <br/>        logprob = buf_logprob[indices] <br/>        advantage = buf_advantage[indices] </span><span id="695e" class="oi lt iq oe b gy on ok l ol om">        <strong class="oe ja"># compute the new log of probability and probability ratio</strong><br/>        new_logprob = self.act.compute_logprob(state, action)      <br/>        ratio = (new_logprob — logprob).exp() <br/>        <br/>        <strong class="oe ja"># compute the surrogate objective with entropy exploration</strong><br/>        obj_surrogate1 = advantage * ratio <br/>        obj_surrogate2 = advantage * ratio.clamp(1 — <br/>                         self.ratio_clip, 1 + self.ratio_clip) <br/>        obj_surrogate = -torch.min(obj_surrogate1, <br/>                        obj_surrogate2).mean() <br/>        obj_entropy = (new_logprob.exp() * new_logprob).mean()<br/>        obj_actor = obj_surrogate + obj_entropy * <br/>                    self.lambda_entropy </span><span id="a181" class="oi lt iq oe b gy on ok l ol om">        <strong class="oe ja"># compute the loss of Critic network</strong><br/>        value = self.cri(state).squeeze(1)<br/>        obj_critic = self.criterion(value, r_sum) <br/>        <br/>        <strong class="oe ja"># update Actor and Critic networks together</strong><br/>        obj_united = obj_actor + obj_critic / (r_sum.std() + 1e-5)<br/>                     self.optimizer.zero_grad() <br/>        obj_united.backward()<br/>        self.optimizer.step() </span><span id="5c07" class="oi lt iq oe b gy on ok l ol om">    return self.act.a_std_log.mean().item(), obj_critic.item()</span></pre></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><h1 id="5223" class="ls lt iq bd lu lv lw lx ly lz ma mb mc kf md kg me ki mf kj mg kl mh km mi mj bi translated">培训渠道</h1><p id="4f54" class="pw-post-body-paragraph ko kp iq kq b kr mk ka kt ku ml kd kw kx mm kz la lb mn ld le lf mo lh li lj ij bi translated">培训代理的两个主要步骤:</p><ol class=""><li id="e0aa" class="nh ni iq kq b kr ks ku kv kx nj lb nk lf nl lj oo nn no np bi translated"><strong class="kq ja">初始化</strong>:</li></ol><ul class=""><li id="9608" class="nh ni iq kq b kr ks ku kv kx nj lb nk lf nl lj nm nn no np bi translated">超参数<em class="op"> args </em>。</li><li id="d1d2" class="nh ni iq kq b kr nq ku nr kx ns lb nt lf nu lj nm nn no np bi translated"><em class="op"> env = PreprocessEnv() </em>:创建一个环境(以OpenAI gym格式)。</li><li id="37e8" class="nh ni iq kq b kr nq ku nr kx ns lb nt lf nu lj nm nn no np bi translated"><em class="op"> agent = AgentXXX() </em>:为DRL算法创建一个代理。</li><li id="0fd0" class="nh ni iq kq b kr nq ku nr kx ns lb nt lf nu lj nm nn no np bi translated"><em class="op"> evaluator = Evaluator() </em>:评估并存储训练好的模型。</li><li id="3061" class="nh ni iq kq b kr nq ku nr kx ns lb nt lf nu lj nm nn no np bi translated"><em class="op"> buffer = ReplayBuffer() </em>:存储过渡。</li></ul><p id="faf0" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">2.然后，<strong class="kq ja">训练过程</strong>由while循环控制:</p><ul class=""><li id="c005" class="nh ni iq kq b kr ks ku kv kx nj lb nk lf nl lj nm nn no np bi translated"><em class="op"> agent.explore_env(…): </em>代理浏览目标步骤中的环境，生成转换，并将它们存储到ReplayBuffer中。</li><li id="fab3" class="nh ni iq kq b kr nq ku nr kx ns lb nt lf nu lj nm nn no np bi translated"><em class="op"> agent.update_net(…) </em>:代理使用ReplayBuffer中的批处理更新网络参数。</li><li id="94d9" class="nh ni iq kq b kr nq ku nr kx ns lb nt lf nu lj nm nn no np bi translated"><em class="op">evaluator . evaluate _ save(…)</em>:评估代理的表现，保留最高分的训练好的模型。</li></ul><p id="73d1" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">当条件满足时，while循环将终止，例如，达到目标分数、最大步数或手动中断。</p></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><h1 id="6cca" class="ls lt iq bd lu lv lw lx ly lz ma mb mc kf md kg me ki mf kj mg kl mh km mi mj bi translated">测试示例</h1><p id="800b" class="pw-post-body-paragraph ko kp iq kq b kr mk ka kt ku ml kd kw kx mm kz la lb mn ld le lf mo lh li lj ij bi translated">欢迎感兴趣的用户测试<a class="ae lk" href="https://github.com/AI4Finance-Foundation/ElegantRL/blob/master/eRL_demo_PPOinSingleFile.py" rel="noopener ugc nofollow" target="_blank"> PPO单档</a>。</p><p id="1d69" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[1] OpenAI在深度RL中加速旋转，<a class="ae lk" href="https://spinningup.openai.com/en/latest/algorithms/ppo.html#id3" rel="noopener ugc nofollow" target="_blank">近端策略优化</a>。</p><p id="3d2d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[2]政策梯度算法，<a class="ae lk" href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#ppo" rel="noopener ugc nofollow" target="_blank"> PPO </a>。</p></div></div>    
</body>
</html>