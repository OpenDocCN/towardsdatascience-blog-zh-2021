<html>
<head>
<title>Use pre-trained Huggingface models in TensorFlow Serving</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在TensorFlow服务中使用预先训练的Huggingface模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/use-pre-trained-huggingface-models-in-tensorflow-serving-d2761f7e69f6?source=collection_archive---------15-----------------------#2021-04-28">https://towardsdatascience.com/use-pre-trained-huggingface-models-in-tensorflow-serving-d2761f7e69f6?source=collection_archive---------15-----------------------#2021-04-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="625f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">将数千个社区NLP模型投入生产</h2></div></div><div class="ab cl kf kg hu kh" role="separator"><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk kl"/><span class="ki bw bk kj kk"/></div><div class="ij ik il im in"><figure class="kn ko kp kq gt kr gh gi paragraph-image"><div role="button" tabindex="0" class="ks kt di ku bf kv"><div class="gh gi km"><img src="../Images/41ddc6736f665081e0a16bf03cedac19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vk_3ycXeez11vOUK"/></div></div><p class="ky kz gj gh gi la lb bd b be z dk translated">克里斯托夫·高尔在<a class="ae lc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="a442" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated"><strong class="lf ir"> HuggingFace </strong>简化了NLP，只需几行代码，你就拥有了一个完整的管道，能够执行从情感分析到文本生成的任务。作为一个预训练模型的中心，加上它的开源框架<strong class="lf ir"> Transformers，</strong>我们过去做的许多艰苦工作都得到了简化。这使我们能够编写能够解决复杂的NLP任务的应用程序，但缺点是我们不知道幕后发生了什么。尽管<strong class="lf ir">拥抱脸</strong>和<strong class="lf ir">变形金刚</strong>执行了这种惊人的简化，但我们可能希望从所有代码中提取一些抽象，并简单地使用许多可用的预训练模型中的一个。在这篇文章中，我们将学习如何使用TensorFlow服务中的许多预训练模型之一，这是一种将机器学习模型投入生产的流行服务。</p><p id="ac43" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我将使用这个<a class="ae lc" href="https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english?text=I+like+you.+I+love+you" rel="noopener ugc nofollow" target="_blank"> Distilbert </a>预训练模型进行情感分析，它将预测给定文本是正面还是负面的。不幸的是，Transformers没有直接导出到TensorFlow Serve的功能，因此我们必须做一些工作来实现我们的目标。首先，我们需要安装Tensorflow、Transformers和NumPy库。</p><pre class="kn ko kp kq gt lz ma mb mc aw md bi"><span id="3f96" class="me mf iq ma b gy mg mh l mi mj">pip install transformers<br/>pip install tensorflow<br/>pip install numpy</span></pre><p id="8650" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在第一部分代码中，我们将从Transformers加载模型和标记器，然后以正确的格式将其保存在磁盘上，以便在TensorFlow Serve中使用。</p><pre class="kn ko kp kq gt lz ma mb mc aw md bi"><span id="03ef" class="me mf iq ma b gy mg mh l mi mj">from transformers import TFAutoModelForSequenceClassification<br/>import tensorflow as tf</span><span id="ed4b" class="me mf iq ma b gy mk mh l mi mj">MAX_SEQ_LEN = 100</span><span id="63f9" class="me mf iq ma b gy mk mh l mi mj">model = TFAutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")</span><span id="6e2e" class="me mf iq ma b gy mk mh l mi mj">callable = tf.function(model.call)<br/>concrete_function = callable.get_concrete_function([tf.TensorSpec([None, MAX_SEQ_LEN], tf.int32, name="input_ids"), tf.TensorSpec([None, MAX_SEQ_LEN], tf.int32, name="attention_mask")])</span><span id="f16f" class="me mf iq ma b gy mk mh l mi mj">model.save('saved_model/distilbert/1', signatures=concrete_function)</span></pre><p id="3920" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">首先，我们用TFAutoModelForSequenceClassification加载模型。加载变形金刚模型的一个关键方面是选择正确的类。由于我们使用预训练模型进行情感分析，我们将使用TensorFlow的加载器(这就是为什么我们导入了<strong class="lf ir"> TF </strong> AutoModel类)进行序列分类。如果你不确定加载什么类，只需检查模型卡或“在变形金刚中使用”页面上的“拥抱脸模型”信息，以确定使用哪个类。</p><p id="ca75" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在代码片段中，我们声明了一个签名函数，它是TensorFlow Serve所必需的。这个函数对于向TF Serve中的可服务模型声明我们的数据的输入形状是必要的，它由变量<em class="ml"> MAX_SEQ_LEN </em>定义。在这种情况下，我已经定义了模型将接受两个输入，两个大小为200的列表，分别用于input_ids(我们从tokenizer获得的id)和attention_mask(如果输入序列长度小于最大输入序列长度时使用)。记住我们为每个输入声明的名称也很重要，因为它是我们稍后将在发送给模型的HTTP请求中定义的参数。在执行这几行之后，我们应该在工作目录中有一个包含以下文件的新目录:</p><figure class="kn ko kp kq gt kr"><div class="bz fp l di"><div class="mm mn l"/></div></figure><p id="8ab0" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">如果你还没有<a class="ae lc" href="https://github.com/tensorflow/serving/blob/master/tensorflow_serving/g3doc/docker.md" rel="noopener ugc nofollow" target="_blank">安装TensorFlow Serve </a>(我推荐和Docker一起使用)现在就做吧。安装完成后，我们可以从Docker中提取映像，并使用加载了以下命令的Distilbert模型开始运行我们的服务:</p><pre class="kn ko kp kq gt lz ma mb mc aw md bi"><span id="0ec8" class="me mf iq ma b gy mg mh l mi mj">docker run -p 8501:8501 --mount type=bind, source=/PATH_TO_YOUR_DIRECTORY/saved_models/distilbert, target=/models/distilbert -e MODEL_NAME=distilbert -t tensorflow/serving</span></pre><p id="6921" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">这将导致TensorFlow Serve在端口8501加载Distilbert(模型文件必须位于用数字命名的目录下，否则TF Serve不会加载模型),我们可以通过HTTP请求该端口。在下一节中，我将展示如何对我们的加载模型进行预测。</p><pre class="kn ko kp kq gt lz ma mb mc aw md bi"><span id="6176" class="me mf iq ma b gy mg mh l mi mj">import requests<br/>from transformers import AutoTokenizer, AutoConfig</span><span id="e04c" class="me mf iq ma b gy mk mh l mi mj">tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")</span><span id="f5ed" class="me mf iq ma b gy mk mh l mi mj">text = "I like you. I love you"<br/>encoded_input = tokenizer(text, pad_to_max_length=MAX_SEQ_LEN, max_length=MAX_SEQ_LEN)</span><span id="4fc6" class="me mf iq ma b gy mk mh l mi mj">#TF Serve endpoint<br/>url = "http://localhost:8501/v1/models/distilbert:predict"<br/><br/>payload={"instances": [{"input_ids": encoded_input['input_ids'], "attention_mask": encoded_input['attention_mask']}]}</span><span id="19a3" class="me mf iq ma b gy mk mh l mi mj">print(payload)<br/>&gt;&gt; { "input_ids": [101, 1045, 2066, 2017, 1012, 1045, 2293, 2017,  102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], "attention_mask": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] }</span><span id="aa17" class="me mf iq ma b gy mk mh l mi mj">headers = {<br/>  'Content-Type': 'application/json'<br/>}<br/><br/>response = requests.request("POST", url, headers=headers, data=json.dumps(payload))<br/><br/>print(json.loads(response.text)['predictions'])<br/>&gt;&gt;[[-4.2159214, 4.58923769]]</span></pre><p id="607c" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我们首先从transformers调用模型标记器，将输入文本转换成id列表及其相应的注意掩码，两者都有填充，以获得发送到我们加载的模型所需的格式。令牌化后，我们对TensorFlow执行POST请求，使用正确的有效负载提供服务，在这里我们获得呈现的值。但是等等，这个模型不是应该对文本中的情感进行分类吗？嗯，我们还缺少最后一步，将<a class="ae lc" href="https://machinelearningmastery.com/softmax-activation-function-with-python/" rel="noopener ugc nofollow" target="_blank"> <strong class="lf ir"> softmax </strong> </a>函数应用于我们的结果向量。</p><pre class="kn ko kp kq gt lz ma mb mc aw md bi"><span id="a13e" class="me mf iq ma b gy mg mh l mi mj">import numpy as np</span><span id="6929" class="me mf iq ma b gy mk mh l mi mj">def softmax(x):<em class="ml"><br/>    </em>e_x = np.exp(x - np.max(x))<br/>    return e_x / e_x.sum(axis=0)</span><span id="fc74" class="me mf iq ma b gy mk mh l mi mj">print(softmax(json.loads(response.text)['predictions'][0]))<br/>&gt;&gt;[0.0001 0.9999]</span></pre><p id="a4da" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">得到的向量是输入文本为负或正的概率。在这种情况下，0.0001的概率为负，0.9999的概率属于正类。如果我们将结果与用变压器加载管道的输出进行比较，我们可以看到两者是相同的。</p><p id="dbbb" class="pw-post-body-paragraph ld le iq lf b lg lh jr li lj lk ju ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">总结这篇文章，我们使用了许多huggingface预训练模型中的一个，并将其加载到TensorFlow Serve上，这样我们就可以发出HTTP请求，并轻松地将这些模型扩展到生产中。我们还学习了如何使用模型记号化器向我们加载的模型发出请求，以及如何将softmax函数应用到我们的结果中以获得所需的值。在我的下一篇文章中，我将展示我们如何通过编写我们的记号赋予器来将自己从transformers库中完全抽象出来，并使用它来请求我们加载的模型。喜欢随时提出建议或改进意见，希望这篇文章能帮助你实现你的目标。</p></div></div>    
</body>
</html>