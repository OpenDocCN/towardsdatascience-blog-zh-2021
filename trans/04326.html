<html>
<head>
<title>How does a DQN Approximate Bellman’s Equation?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DQN如何逼近贝尔曼方程？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-does-a-dqn-approximate-bellmans-equation-53e591d5e33?source=collection_archive---------35-----------------------#2021-04-12">https://towardsdatascience.com/how-does-a-dqn-approximate-bellmans-equation-53e591d5e33?source=collection_archive---------35-----------------------#2021-04-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2d58" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">建立直觉</h2></div><p id="4844" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是那些可能比自己发现更难解释的事情之一。在深度Q学习中，贝尔曼方程由DQN近似的方式在直觉上并不明显。这篇文章是关于我对正在发生的事情的探索。一个简单的DQL模型，其代码可在<a class="ae lb" href="https://github.com/kamfonas/DQN-Bellman" rel="noopener ugc nofollow" target="_blank">这里</a>获得，有助于可视化DQN学习，并揭示了算法如何迭代和向固定点前进。如果感兴趣，下载模型并调整参数，以探索它们的效果和相互依赖性。</p><p id="16f3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为什么这种洞察力是有用的？因为它导致在配置与经验重放相关的超参数(如缓冲区大小、采样批量大小、目标更新频率)时做出更明智的决策。它也有助于解释意外结果或不稳定性的一些原因，特别是在训练有限数量的数据时。</p><p id="1880" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本文假设读者熟悉深度Q-Learning，并且具有经验重放和双(策略和目标)DQN实例的典型实现的工作知识。有太多关于强化学习、Q学习和深度Q学习的参考文献。DeepLizard 的这一系列<a class="ae lb" href="https://deeplizard.com/learn/video/xVkPh9E9GfE" rel="noopener ugc nofollow" target="_blank">四教程、</a><a class="ae lb" href="https://neuro.cs.ut.ee/author/tambet-matiisen/" rel="noopener ugc nofollow" target="_blank"> Tambet Matiisen </a>的文章“<a class="ae lb" href="https://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/" rel="noopener ugc nofollow" target="_blank">揭秘深度RL </a>”以及Jonathan Hui的<a class="ae lb" href="https://jonathan-hui.medium.com/rl-dqn-deep-q-network-e207751f7ae4" rel="noopener"> RL - DQN深度Q-Network </a>就是这样三个参考资料，它们为本文奠定了基础，并提供了引入该主题的补充方式。</p><p id="e424" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">DQN被训练来近似的贝尔曼方程的形式是:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/fa8697112e66c27688ea3bb5cada34d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bhfT9BV01E6WFSmgetDbmg.png"/></div></div></figure><blockquote class="lo"><p id="c82f" class="lp lq iq bd lr ls lt lu lv lw lx la dk translated">采取行动a后状态s的Q值是括号中表达式的期望值，由此从(s，a)到新状态s '和奖励r的映射从经验重放历史h中导出。括号中的表达式是该步骤的奖励与新状态s '的贴现最大Q值的总和，其中a '是最大化行动。贴现因子为<strong class="ak"> <em class="ly"> γ。</em>T13】</strong></p></blockquote><p id="1487" class="pw-post-body-paragraph kf kg iq kh b ki lz jr kk kl ma ju kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated">双DQN的实现，使用两个相同的神经网络实例:策略DQN和目标DQN。政策——DQN是用于制定每一步决策的地方，也是所有培训发生的地方。目标DQN只是策略DQN的快照的被动副本，每<strong class="kh ir"><em class="me"/></strong>步都会被替换。这是通过简单地复制其权重来完成的。设<strong class="kh ir"> <em class="me"> l </em> </strong>为跟踪这些<strong class="kh ir"> <em class="me"> τ </em> </strong>步循环的序号。在周期<strong class="kh ir"> <em class="me"> l </em> </strong>期间的目标DQN近似于在周期<strong class="kh ir"> <em class="me"> l-1 </em> </strong>期间学习的策略DQN函数。当然，这假设DQN的架构是正确的，并且能够学习接近目标Q值，我们认为这是给定的，这超出了本文的范围。</p><p id="5ca1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了直观显示目标和政策Q值随时间的变化，并展示它们如何捕捉任意大量的未来贴现回报，我们将进行两种简化:</p><ol class=""><li id="8823" class="mf mg iq kh b ki kj kl km ko mh ks mi kw mj la mk ml mm mn bi translated">我们将我们的环境限制为只有一个动作，使动作选择具有确定性。这消除了在比较不同轨迹的累积未来回报时的任何模糊性，并确保我们是在比较苹果与苹果。</li><li id="c1a5" class="mf mg iq kh b ki mo kl mp ko mq ks mr kw ms la mk ml mm mn bi translated">我们用五个随机特征(可以是任何数字)构建状态，取值从0到1，奖励总是设置为这五个值的平均值，即每一步的期望值相同。因此，累积奖励单调增加并接近一个固定点，即平均奖励乘以γ的幂的和的渐近极限:</li></ol><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/1f75ce85605b1c2ea5bce7686f0046d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:302/format:webp/1*4FpjQZpAjjjjTxdk2bLCpg.png"/></div></figure><p id="f3c3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们使用这个数学表达式来生成未来奖励的每个深度级别的理论贝尔曼方程结果，并将这些值与每个步骤中相应的目标和政策DQN生成的值进行比较。</p><p id="d561" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">考虑到这两种简化，我们可以将贝尔曼方程简化为:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/e909a9cdf4df91452ea7be34b5fdfa03.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*dssFrowSwEIvdH481ST54A.png"/></div></figure><p id="9c7c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中t表示沿着确定性轨迹的步骤序列。上面的函数是尾递归的，和原来的贝尔曼方程一样，可以简化为一次迭代。此外，如我们所见，第二项形成一个收敛于一个不动点的级数。为了说明这是如何工作的，让我们用循环序列<strong class="kh ir"> <em class="me"> l </em> </strong>来解开递归标记级数的每一级:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/569eca4d9df29c26df33a9f92547e8c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*GayyQNPpZC4O90-CtrlZ6w.png"/></div><p class="mw mx gj gh gi my mz bd b be z dk translated">未来奖励的进展</p></figure><p id="b203" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第一个公式简单地计算Q作为步骤t的奖励。第二个和每个后续公式通过增加一个未来折扣奖励的步骤使计算更加精确。显然，<strong class="kh ir"> <em class="me"> l </em> </strong>的每一次增加都对应于贝尔曼方程右侧的一个额外嵌套项:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi na"><img src="../Images/483ff1019f8c72d4c5bbe337d6d52500.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*y48T6x4iCETu0F3A6zakHg.png"/></div><p class="mw mx gj gh gi my mz bd b be z dk translated">用贝尔曼方程研究未来报酬的级数</p></figure><p id="c5a2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个Q函数阶梯对应于由目标DQN在由<strong class="kh ir"> <em class="me"> l </em> </strong>表示的周期序列中捕获的快照阶梯。最初，目标Q函数是第一个方程，其中<strong class="kh ir"> <em class="me"> l </em> </strong> <em class="me"> =0 </em>。每个新周期<strong class="kh ir"> <em class="me"> l </em> </strong>开始于将目标与策略DQN同步，随后训练策略DQN以接近贴现未来奖励的附加水平(<strong class="kh ir"> <em class="me"> l+1 </em> </strong>)。在每个周期中，DQN接近由贝尔曼方程定义的固定点。</p><p id="d4ad" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这看起来像是可以通过迭代实现的递归函数的教科书示例。不同之处在于，该循环不是简单地聚集数据，而是“递归地改进”非线性函数，即策略DQN，其模拟收敛序列，即贝尔曼方程。在每次迭代期间，DQN被训练到极限的更好的近似值，即固定点。因为我们使用迭代而不是递归，我们的过程要求我们在训练期间覆盖策略DQN的相同实例。目标DQN就像一个“变量”,它保存了在生成目标时使用的最后一次迭代的策略DQN的副本。每一次迭代都需要<strong class="kh ir"> <em class="me"> k </em> </strong>步，但是如果我们选择设置k=1，那么我们的双DQN实现将会崩溃(或者变得类似于)为单个DQN实现。</p></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><p id="0910" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们来看看所有这些与DQN架构和相关超参数相关的一些含义。但是首先，我们总结一下我们使用的符号:</p><p id="7302" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="me"> k </em> </strong>是目标更新事件之间的步数<br/> <strong class="kh ir"> <em class="me"> m </em> </strong>是一集的步数(假设没有终端状态)<br/> <strong class="kh ir"> <em class="me"> m/k </em> </strong>是一集的目标更新事件数<br/> <strong class="kh ir"> <em class="me"> l </em> </strong>是截至时间<strong class="kh ir"> <em class="me"> t </em>的目标权重更新总数<br/> <strong class="kh ir"> <em class="me"> b </em> </strong>是来自重放缓冲区的每批重放样本的经验数</strong></p><p id="5729" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"><em class="me">DQN的训练不能捕获比l更多的未来折扣奖励水平:</em></strong>【DQN】不能将比执行的目标权重更新的次数更长的未来折扣奖励范围纳入其贝尔曼方程的近似中。即使<strong class="kh ir"> <em class="me"> k </em> </strong>比成功训练策略DQN所需的时间长得多，也是如此，因为用于训练的目标被限制为比前一周期的目标多一个级别。即使每个周期的训练都很完美，目标也会被训练到<strong class="kh ir"> <em class="me"> l </em> </strong>的水平。</p><p id="18f0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"><em class="me">γ的高值需要更大数量的目标权重更新:</em> </strong>考虑选择<strong class="kh ir"><em class="me"/></strong><em class="me">= 0.95</em>需要<strong class="kh ir"><em class="me">l</em></strong><em class="me">= 45</em>次迭代来覆盖在折扣Q值中贡献10%或更多的所有项。这是因为<strong class="kh ir"><em class="me">γ</em></strong><em class="me">⁴⁵=0.10</em>。对于<strong class="kh ir"><em class="me">γ</em></strong><em class="me">= 0.8</em>只需要大约<strong class="kh ir"><em class="me">l</em></strong><em class="me">=</em>10次迭代自<strong class="kh ir"><em class="me">γ</em></strong><em class="me">⁰= 0.11</em>。这说明了为什么设置<strong class="kh ir"><em class="me">γ</em></strong><em class="me">= 1</em>或非常接近它会有问题。通常，我们希望接近渐近极限的10%以上，因此我们必须确保允许足够的目标更新来充分训练DQN。为了包括所有贡献超过1%的未来奖励，这两种情况所需的迭代次数分别为90和21。最小迭代次数由表达式<strong class="kh ir"><em class="me">log(d)/log(γ)</em></strong>给出，其中<strong class="kh ir"> <em class="me"> d </em> </strong>是最小可接受折扣，而<strong class="kh ir"> <em class="me"> γ </em> </strong>是折扣因子:</p><blockquote class="lo"><p id="31ef" class="lp lq iq bd lr ls ni nj nk nl nm la dk translated"><strong class="ak"><em class="ly">log(0.01)/log(0.95)= 90</em></strong></p><p id="1e7a" class="lp lq iq bd lr ls ni nj nk nl nm la dk translated">log(0.01)/log(0.80)= 21</p></blockquote><p id="5510" class="pw-post-body-paragraph kf kg iq kh b ki lz jr kk kl ma ju kn ko mb kq kr ks mc ku kv kw md ky kz la ij bi translated"><strong class="kh ir"> <em class="me">用于将策略DQN训练到每个新目标的示例数量是b*k: </em> </strong>可能需要调整一批中的经验数量，以便在一个训练周期中使用足够的经验。当然，训练可以持续不止一集。</p><p id="f5f3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="me">第1次迭代期间的训练对于所有级别i ≤ l连续进行:</em> </strong>学习通常比上面的讨论所表明的更流畅且更少条块化。例如，一个目标迭代的训练可能在分配的<strong class="kh ir"> <em class="me"> k </em> </strong>步骤中只部分完成。对于单个DQN的情况或者如果k = 1，这显然是这种情况。训练优化所有未来奖励等级，最高(但不超过)<strong class="kh ir"> <em class="me"> l </em> </strong>。</p><p id="42e2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">设置k=1模拟单个DQN的情况:</strong>使用单个DQN作为政策和目标，只要它收敛，将可能近似于贝尔曼方程。这个场景相当于有两个实例，并且在每一步都更新权重，即<strong class="kh ir"><em class="me">k</em></strong><em class="me">= 1</em>，从而策略DQN的训练量只受批量<strong class="kh ir"> <em class="me"> b </em> </strong>的控制。</p><p id="41a7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">避免重放缓冲区中的陈旧体验:</strong>较长的重放缓冲区可以容纳更多种类的体验和更大的批量样本。根据应用，这可能是所希望的。然而，如果缓冲区跨越多个情节，特别是如果相同的数据集范围被重复用于训练，这可能导致不稳定。原因是，从混合了新的和陈旧的经验的缓冲区中取样可能会导致不同的轨迹和不同的奖励，从而导致不一致或倒退的学习行为。这更有可能发生在有噪声的数据中。比方说，在训练的早期阶段，一个特定的状态倾向于动作1，而在训练的后期，同样的状态倾向于动作2。如果两个体验都存在于重放缓冲区中，选择较早的一个将会适得其反。避免这种情况的一种方法是让重放缓冲区安全地变大，但不要大到有采样陈旧体验的风险。</p><p id="1e25" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="me"> DQN有效性，低l和/或嘈杂的数据可以被混杂:</em> </strong> DQN架构的不足之处，或嘈杂的环境可以造成跳动的损失。不仅绘制损失图，而且分别绘制目标Q值和预测Q值也有助于分析。如果平台以与目标重量更新相对应的间隔出现，则有理由怀疑训练没有充分渗透到足够的未来折扣水平，或者更新必须更频繁，或者可能需要更大批量或更长时间的训练。如果这发生在训练过程中，请考虑在以前没有训练过的数据中形成新机制的可能性。</p><p id="6c38" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="me">对于较小规模的训练数据，考虑到γ，k，l，b，m: </em> </strong>之间错综复杂的依赖关系，<strong class="kh ir"> <em class="me"> γ </em> </strong>的值越高，说明足够的目标更新<strong class="kh ir"> <em class="me"> l </em> </strong>对于确保未来奖励折扣的足够深度越重要。请记住，正是目标更新频率扩展了未来折扣奖励的范围，因此如果您需要<strong class="kh ir"> <em class="me"> γ </em> </strong>非常接近1，您可以考虑单个DQN(如果它收敛)，或者具有较小<strong class="kh ir"><em class="me"/></strong>k的双dqn增加批量<strong class="kh ir"> <em class="me"> b </em> </strong>可以补偿较短的迭代。为了提高稳定性，尽量允许足够多的例子<strong class="kh ir"><em class="me">b</em></strong><em class="me">*</em><strong class="kh ir"><em class="me">k</em></strong>在每个目标迭代内进行训练。每个周期培训所需的示例数量也取决于数据的性质和DQN体系结构。</p></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><p id="27e5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一个具有单一动作环境的普通工作DQL有助于说明这些要点。你可以在<a class="ae lb" href="https://github.com/kamfonas/DQN-Bellman" rel="noopener ugc nofollow" target="_blank"> this GitHub </a> repo获取代码。该状态包含五个取值在0.0和1.0之间的随机特征。回报总是这五个特征的平均值。下图显示了以三种不同方式产生的随机批次经验样本的平均Q值:(1)政策DQN，(2)目标DQN，以及(3)根据前面所示的每个<strong class="kh ir"> <em class="me"> k </em> </strong>步骤的Bellman方程的扩展计算出的理论值。该情节长12000步，目标更新每1000步发生一次，并且批量样本大小为250次经历。该图清楚地显示了从Q=0.5开始，每1000步增加一个折扣项，这些值如何在理论级数后达到平稳状态。我们用贴现因子<strong class="kh ir"> <em class="me"> γ </em> </strong> <em class="me"> = 0.8。</em>奖励的期望值总是0.5，作为(0，1)中5个随机值的平均值。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/94c97ddd71dd020319231e68baeec017.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*NPC4vGXA0HDmxlj2K27b7g.png"/></div><p class="mw mx gj gh gi my mz bd b be z dk translated">k=1000，b=250，m=12000</p></figure><p id="fa89" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">大批量和长(1000步)目标更新间隔导致低方差并允许快速收敛。数列收敛到的渐近极限是<strong class="kh ir"><em class="me">r</em></strong>/(1-<strong class="kh ir"><em class="me">γ</em></strong><em class="me">)= 2.5</em>for<em class="me"/><strong class="kh ir"><em class="me">γ</em></strong><em class="me">= 0.8</em><strong class="kh ir"><em class="me">r</em></strong><em class="me">= 0.5。</em></p><p id="e1a0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过将目标更新频率加倍到2000步，如下图所示，适合该集的六个周期不会从上一次运行的前六个周期扩展未来的奖励深度，并且它们最终与目标更新频率为1000时处于相同的水平。正是级别的数量将贝尔曼方程的精确度限制在未来奖励折扣的6个级别。实际上，每一级似乎都可以通过少量的步骤被充分训练到新的水平。当然，在更复杂的环境中，可能需要更多的示例来实现充分的培训。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/b9dd07b9bca93cf8c775f8165da90824.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*R5i0Pc9SZHkcZIgLKVggkg.png"/></div><p class="mw mx gj gh gi my mz bd b be z dk translated">k=2000，b=250，m=12000</p></figure><p id="5789" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，我们将更新目标频率设置为1，并将批处理设置为1000。下图显示了理论线如何以接近垂直的方式达到渐近值，而目标值和政策值需要接近1500步才能达到该水平。这演示了如何同时训练多个深度级别，最多可达权重更新事件的数量。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/ded622e097fb6a5c4097729af7f248b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*Y59hHjgmdawCvtnhKS2Qmw.png"/></div><p class="mw mx gj gh gi my mz bd b be z dk translated">k=1，b=1000，m=12000</p></figure><p id="7ea4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们重复相同的运行，但使用较小的批量:250。请注意，方差增加，训练变得更加紧张，</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi no"><img src="../Images/9b3205e78bfd683ccad4b305d74011ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*24ssCPZrPOraaCVZ8b3wVw.png"/></div><p class="mw mx gj gh gi my mz bd b be z dk translated">k=1，b=250，m=12000</p></figure><p id="d944" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可以尝试批量大小、目标更新间隔、未来奖励折扣因子的不同组合，甚至尝试1、2或3个隐藏层的不同dqn。“readme”文件概述了代码以及如何通过配置文件更改超参数。</p></div></div>    
</body>
</html>