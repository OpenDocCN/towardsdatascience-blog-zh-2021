<html>
<head>
<title>Writing Custom CUDA Kernels with Triton</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Triton编写自定义CUDA内核</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/writing-custom-cuda-kernels-with-triton-abf6b6ad1168?source=collection_archive---------26-----------------------#2021-09-12">https://towardsdatascience.com/writing-custom-cuda-kernels-with-triton-abf6b6ad1168?source=collection_archive---------26-----------------------#2021-09-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="e1a5" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">探索Triton的实时(JIT)编译器和代码生成后端</h2></div><p id="40a1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">随着深度学习的成功和研究论文的爆炸，我们经常会发现自己处于这样一种情况:我们提出了一个新想法，但却发现它没有经过硬件加速。更具体地说，当我们提出一个新的激活函数或自我关注机制，并且必须依赖PyTorch/Tensorflow提供的功能来处理模块的向前和向后传递时。</p><p id="46c3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">PyTorch JIT是这些情况下的一种选择。但是PyTorch JIT是一个高级编译器，只能优化部分代码，不能用来写自定义CUDA内核。</p><p id="6fe2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">写CUDA内核还有一个问题。这很难做到。针对局部性和并行性优化计算非常耗时且容易出错，通常需要花费大量时间学习如何编写CUDA代码的专家。此外，GPU架构正在快速发展，如最新版本的张量核心，这意味着编写代码以利用硬件的最大性能是一个更大的挑战。</p><p id="3c34" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这就是OpenAI <a class="ae lb" href="https://github.com/openai/triton" rel="noopener ugc nofollow" target="_blank"> Triton </a>的用武之地。Triton有三个主要组成部分</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi lc"><img src="../Images/98a68e6bf213d52e08db2e569cf9973d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/format:webp/0*ltGtN0KSKTL9rNBS.png"/></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">图Triton主要组件概述。<a class="ae lb" href="http://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>(经许可)</p></figure><ol class=""><li id="7385" class="lo lp iq kh b ki kj kl km ko lq ks lr kw ls la lt lu lv lw bi translated">Triton-C:一种类似C的语言，主要面向已经熟悉CUDA的程序员。</li><li id="9ffe" class="lo lp iq kh b ki lx kl ly ko lz ks ma kw mb la lt lu lv lw bi translated">Triton-IR:一种基于LLVM的中间表示(IR)。Triton-IR程序直接从Triton-C构建而来。简而言之，LLVM提供了许多特定于硬件的优化，这意味着我们可以直接使用Nvidia的CUDA编译器(NVCC)来优化我们特定于特定硬件的代码。</li><li id="e928" class="lo lp iq kh b ki lx kl ly ko lz ks ma kw mb la lt lu lv lw bi translated">Triton-JIT:一个实时(JIT)编译器和代码生成后端，用于将Triton-IR程序编译成高效的LLVM位代码。这还包括许多独立于机器的优化，这意味着我们的工作量减少了。</li></ol><p id="0e17" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对我来说，Triton-JIT是Triton项目中最令人兴奋的部分。它允许几乎没有CUDA编程经验的程序员用Python编写高度优化的CUDA内核。在讨论Triton之前，我们需要了解CUDA程序是如何在GPU上工作的。</p><p id="b062" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有用的链接</p><ul class=""><li id="b26c" class="lo lp iq kh b ki kj kl km ko lq ks lr kw ls la mc lu lv lw bi translated"><a class="ae lb" href="http://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf" rel="noopener ugc nofollow" target="_blank"> Triton:用于平铺式神经网络计算的中间语言和编译器</a></li><li id="4356" class="lo lp iq kh b ki lx kl ly ko lz ks ma kw mb la mc lu lv lw bi translated"><a class="ae lb" href="https://openai.com/blog/triton/" rel="noopener ugc nofollow" target="_blank">Triton简介:用于神经网络的开源GPU编程</a></li><li id="f91e" class="lo lp iq kh b ki lx kl ly ko lz ks ma kw mb la mc lu lv lw bi translated"><a class="ae lb" href="https://github.com/openai/triton" rel="noopener ugc nofollow" target="_blank"> triton github </a></li><li id="690f" class="lo lp iq kh b ki lx kl ly ko lz ks ma kw mb la mc lu lv lw bi translated"><a class="ae lb" href="https://triton-lang.org/" rel="noopener ugc nofollow" target="_blank"> triton文档</a></li></ul><h1 id="0b73" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">GPU编程基础</h1><p id="a35c" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">从CPU(主机)开始。CPU可以访问RAM、存储磁盘和所有连接的外围设备。另一方面，GPU(设备)无法访问RAM或任何东西。GPU有自己的内存，称为VRAM，数据必须从CPU-&gt;GPU复制，以便GPU对其进行处理，数据必须再次从GPU-&gt;CPU复制，以便CPU将其存储在其中一个存储设备中或与连接的外围设备共享。</p><blockquote class="na nb nc"><p id="d117" class="kf kg nd kh b ki kj jr kk kl km ju kn ne kp kq kr nf kt ku kv ng kx ky kz la ij bi translated"><strong class="kh ir"> <em class="iq">注:</em> </strong> <em class="iq">这就是你要尽可能减少CPU和GPU之间数据移动的原因。要做到这一点，您必须集思广益，研究如何以块的形式加载数据，以便并行处理数据，或者在导入下一个数据项之前，以可以多次重用的方式导入数据。</em></p></blockquote><p id="941d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在CUDA中，我们在线程组<strong class="kh ir">块</strong>中启动许多<strong class="kh ir">线程</strong>，形成一个<strong class="kh ir">网格</strong>。线程<em class="nd">块</em>中的所有<em class="nd">线程</em>可以相互通信。您可以一次启动每个块1024个线程和2^32-1块。图2显示了一个这样的例子。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="ni nj di nk bf nl"><div class="gh gi nh"><img src="../Images/d9882188062fc049972b7675d0bcbc78.png" data-original-src="https://miro.medium.com/v2/resize:fit:882/format:webp/0*M5SiqcQ6ZMPMol_m.png"/></div></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">图CUDA程序的架构。<a class="ae lb" href="https://www.wikiwand.com/en/Thread_block_(CUDA_programming)" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="6550" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用<em class="nd">块</em>背后的想法是，如果你将来得到一个新的GPU，你不需要改变你的代码。因此，新的GPU可以简单地同时执行更多的<em class="nd">块</em>，而无需更改任何代码。</p><h1 id="13f0" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">CPU程序vs CUDA程序</h1><p id="cad6" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">不涉及技术细节，让我们考虑一个简单的例子，添加两个长度为3的数组。</p><p id="d1e1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在C++中，如果我们想要添加这些数组，那么我们将创建一个运行三次的for循环(假设是单线程程序)。</p><p id="8de2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是在CUDA中，我们将启动3个线程，每个线程将在一个索引处执行加法，而for循环在一个步骤中完成。实际上，会发生以下情况</p><ol class=""><li id="a5c2" class="lo lp iq kh b ki kj kl km ko lq ks lr kw ls la lt lu lv lw bi translated">将<code class="fe nm nn no np b">arr1</code>、<code class="fe nm nn no np b">arr2</code>从CPU复制到GPU。</li><li id="0af2" class="lo lp iq kh b ki lx kl ly ko lz ks ma kw mb la lt lu lv lw bi translated">创建一个大小为3的新数组(或者将加法的结果存储在<code class="fe nm nn no np b">arr1</code>中)。</li><li id="66bb" class="lo lp iq kh b ki lx kl ly ko lz ks ma kw mb la lt lu lv lw bi translated">启动3个线程进行加法运算，并将结果存储在新数组中。</li><li id="e3a7" class="lo lp iq kh b ki lx kl ly ko lz ks ma kw mb la lt lu lv lw bi translated">将结果从GPU复制到CPU。</li></ol><p id="23bc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">因为GPU有1000个内核，所以在GPU上做加法、矩阵乘法等简单的事情要比CPU快得多(前提是加速比大于CPU和GPU之间传输数据所花费的时间)。</p><h1 id="9783" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">CUDA vs Triton</h1><p id="4b0d" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">我们在上面看到了CUDA执行模型。现在我们来看看Triton和上面的模型有什么不同。</p><p id="b420" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在CUDA中，每个内核都与一个<em class="nd">线程块</em>相关联(即一组<em class="nd">线程</em>)。在Triton中，每个内核都与一个<em class="nd">线程</em>相关联。这种执行范例解决了线程间的内存同步、线程间通信的问题，同时允许自动并行化。</p><p id="a35d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，块由一个<em class="nd">范围</em>组成，即指向线程的指针块，而不是将线程存储在线程块中。有趣的是，你可以想有多少范围就有多少范围。因此，如果您的输入是2D，您可以为x轴指定Range(10 ),为y轴指定Range(5 ),总共50个线程。同样，您可以根据需要定义任意多个维度的范围。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/e247fcd59b7baaa1d7c2b62893fda0c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/format:webp/0*woWdTx4gFw2hfcgm.png"/></div><p class="lk ll gj gh gi lm ln bd b be z dk translated">图3: CUDA执行模型与Triton执行模型。<a class="ae lb" href="http://www.eecs.harvard.edu/~htk/publication/2019-mapl-tillet-kung-cox.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>(经许可)</p></figure><h1 id="15f5" class="md me iq bd mf mg mh mi mj mk ml mm mn jw mo jx mp jz mq ka mr kc ms kd mt mu bi translated">使用Triton添加两个数组</h1><p id="e5e6" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">现在我们对CUDA和Triton的工作原理有了基本的了解，我们可以编写Triton程序了。使用以下命令安装Triton</p><pre class="ld le lf lg gt nr np ns nt aw nu bi"><span id="97e1" class="nv me iq np b gy nw nx l ny nz">pip install triton</span></pre><p id="01e0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下面给出了这些步骤的摘要:</p><ol class=""><li id="b42c" class="lo lp iq kh b ki kj kl km ko lq ks lr kw ls la lt lu lv lw bi translated">定义<em class="nd">*块* </em>。我们知道<em class="nd">*块* </em>是通过指定一个范围来定义的。所以除此之外，我们只需要定义一维的范围。设它是512，我们定义它为全局<code class="fe nm nn no np b">BLOCK_SIZE=512</code>。</li><li id="f575" class="lo lp iq kh b ki lx kl ly ko lz ks ma kw mb la lt lu lv lw bi translated">512的范围实际上意味着我们启动512个线程来进行计算。</li><li id="34cf" class="lo lp iq kh b ki lx kl ly ko lz ks ma kw mb la lt lu lv lw bi translated">接下来，我们得到输入数据的索引。假设输入数组的大小为1000。因为我们定义了大小为512的块，所以我们将以大小为512的块来处理输入数组。所以第一个数据块来自<code class="fe nm nn no np b">0:512</code>，第二个数据块来自<code class="fe nm nn no np b">512:1024</code>。这是使用下面显示的代码完成的</li></ol><pre class="ld le lf lg gt nr np ns nt aw nu bi"><span id="5dd6" class="nv me iq np b gy nw nx l ny nz"><em class="nd"># Addition is 1D, so we only need to get the index of axis=0<br/></em>pid = triton.language.program_id(axis=0)</span><span id="9a98" class="nv me iq np b gy oa nx l ny nz"><em class="nd"># Below offsets are a list of pointers<br/></em>block_start = pid * BLOCK_SIZE<strong class="np ir"><br/></strong>offsets = block_start + triton.language.arange(0, BLOCK_SIZE)</span></pre><p id="7da2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">4.屏蔽以保护内存操作。在上面的例子中，输入数组的大小为<code class="fe nm nn no np b">N=1000</code>，但是偏移量来自<code class="fe nm nn no np b">512:1024</code>。因此，我们需要指定一个掩码来防止越界访问。需要为每个轴指定该掩码。</p><pre class="ld le lf lg gt nr np ns nt aw nu bi"><span id="9425" class="nv me iq np b gy nw nx l ny nz">mask = offsets &lt; N</span></pre><p id="6c6e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">5.加载数据。现在我们已经定义了偏移量和掩码，我们可以从RAM加载数据并屏蔽掉所有额外的元素。</p><pre class="ld le lf lg gt nr np ns nt aw nu bi"><span id="eea5" class="nv me iq np b gy nw nx l ny nz">def add_kernel(arr1_ptr, arr2_ptr, output_ptr, ...):<br/>    ...<br/>    arr1 = triton.language.load(arr1_ptr + offsets, mask=mask)<br/>    arr2 = triton.language.load(arr2_ptr + offsets, mask=mask)</span></pre><p id="00d9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">6.做相关操作。在这种情况下，我们只需要做加法。</p><pre class="ld le lf lg gt nr np ns nt aw nu bi"><span id="b0a8" class="nv me iq np b gy nw nx l ny nz">output = arr1 + arr2</span></pre><p id="0a47" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">7.完成计算后，将结果存储在RAM中。GPU无法访问存储，所以我们必须首先将数据移动到RAM，然后如果需要，我们可以将数据存储到磁盘。</p><pre class="ld le lf lg gt nr np ns nt aw nu bi"><span id="56db" class="nv me iq np b gy nw nx l ny nz">triton.language.store(output_ptr + offsets, output, mask=mask)</span></pre><p id="76ee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">整个内核的代码如下所示</p><pre class="ld le lf lg gt nr np ns nt aw nu bi"><span id="087a" class="nv me iq np b gy nw nx l ny nz">import triton<br/>import triton.language as tl</span><span id="7020" class="nv me iq np b gy oa nx l ny nz">BLOCK_SIZE = 512</span><span id="3fc6" class="nv me iq np b gy oa nx l ny nz">@triton.jit<br/>def add_kernel(arr1_ptr, arr2_ptr, output_ptr, N):<br/>    <em class="nd"># Step 1: Get range of axis<br/>    </em>pid = tl.program_id(axis=0)<br/>    <br/>    <em class="nd"># Step 2: Define the offsets and mask<br/>    </em>block_start = pid * BLOCK_SIZE<br/>    offsets = block_start + tl.arange(0, BLOCK_SIZE)<br/>    mask = offsets &lt; N</span><span id="2fa8" class="nv me iq np b gy oa nx l ny nz"><em class="nd">    # Step 3: Load the data from RAM<br/>    </em>arr1 = tl.load(arr1_ptr + offsets, mask=mask)<br/>    arr2 = tl.load(arr2_ptr + offsets, mask=mask)<br/>    <br/>    <em class="nd"># Step 4: Do the computation<br/>    </em>output = arr1 + arr2</span><span id="6109" class="nv me iq np b gy oa nx l ny nz"><em class="nd">    # Step 5: Store the result in RAM<br/>    </em>tl.store(output_ptr + offsets, output, mask=mask)</span></pre><p id="2685" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要使用内核，我们可以定义一个如下所示的助手函数</p><pre class="ld le lf lg gt nr np ns nt aw nu bi"><span id="5ca5" class="nv me iq np b gy nw nx l ny nz">def add(arr1: torch.Tensor, arr2: torch.Tensor):<br/>    output = torch.empty_like(arr1)<br/>    N = output.numel()</span><span id="7eb4" class="nv me iq np b gy oa nx l ny nz">    grid = lambda meta: (triton.cdiv(N, BLOCK_SIZE),)</span><span id="48f8" class="nv me iq np b gy oa nx l ny nz">    add_kernel[grid](arr1, arr2, output, N)<br/>    return output</span></pre><p id="f03b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">基本上是指定我们工作的空间。在我们的例子中，网格是1D，我们指定如何沿着网格分割数据。因此，如果输入数组的大小，那么我们希望网格如下<code class="fe nm nn no np b">[0:512], [512:1024]</code>。所以在这一步，我们基本上是指定如何分割输入数据并将其传递给内核。</p><p id="71dd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">默认情况下，<code class="fe nm nn no np b">grid</code>接受一个位置参数，我们称之为<code class="fe nm nn no np b">meta</code>。<code class="fe nm nn no np b">meta</code>的目的是提供类似<code class="fe nm nn no np b">BLOCK_SIZE</code>的信息，但是在这个例子中，我们使用了一个全局变量来定义，所以使用<code class="fe nm nn no np b">meta</code>是多余的。</p><p id="fd69" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在我们可以像调用普通python函数一样调用<code class="fe nm nn no np b">add</code>函数，如下所示(确保传递给函数的输入已经在GPU上)</p><pre class="ld le lf lg gt nr np ns nt aw nu bi"><span id="3142" class="nv me iq np b gy nw nx l ny nz">arr_size = 100_000<br/>arr1 = torch.rand(arr_size, device='cuda')<br/>arr2 = torch.rand(arr_size, device='cuda')</span><span id="13b9" class="nv me iq np b gy oa nx l ny nz">pytorch_out = arr1 + arr2<br/>triton_out = add(arr1, arr2)</span><span id="db2f" class="nv me iq np b gy oa nx l ny nz">print(torch.sum(torch.abs(pytorch_out - triton_out)))</span></pre><p id="83cf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在外</p><pre class="ld le lf lg gt nr np ns nt aw nu bi"><span id="c4e7" class="nv me iq np b gy nw nx l ny nz">&gt; python main.py<br/>tensor(0., device='cuda:0')</span></pre><h2 id="edf3" class="nv me iq bd mf ob oc dn mj od oe dp mn ko of og mp ks oh oi mr kw oj ok mt ol bi translated">添加更高的维数张量</h2><p id="db92" class="pw-post-body-paragraph kf kg iq kh b ki mv jr kk kl mw ju kn ko mx kq kr ks my ku kv kw mz ky kz la ij bi translated">我们也可以对N维张量重用相同的核。这给了我们避免为不同数量的输入维度编写多个内核的灵活性。这个想法很简单，我们在辅助函数中把张量整形为1D张量，然后整形输出张量。</p><p id="4be0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">整形操作不是耗时的操作，因为我们只是修改张量类中的步幅值。修改后的助手函数如下所示。</p><pre class="ld le lf lg gt nr np ns nt aw nu bi"><span id="2da9" class="nv me iq np b gy nw nx l ny nz">def add(arr1: torch.Tensor, arr2: torch.Tensor):<br/>    input_shape = arr1.shape<br/>    arr1 = arr1.view(-1)<br/>    arr2 = arr2.view(-1)</span><span id="7eed" class="nv me iq np b gy oa nx l ny nz">    output = torch.empty_like(arr1)<br/>    N = output.numel()</span><span id="c923" class="nv me iq np b gy oa nx l ny nz">    grid = lambda meta: (triton.cdiv(N, <strong class="np ir">BLOCK_SIZE</strong>),)</span><span id="2b1c" class="nv me iq np b gy oa nx l ny nz">    add_kernel[grid](arr1, arr2, output, N)</span><span id="8af1" class="nv me iq np b gy oa nx l ny nz">    output = output.view(input_shape)<br/>    return output</span></pre><p id="a20b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后我们像以前一样调用这个函数</p><pre class="ld le lf lg gt nr np ns nt aw nu bi"><span id="383b" class="nv me iq np b gy nw nx l ny nz">arr_size = (100, 100, 100)<br/>arr1 = torch.rand(arr_size, device='cuda')<br/>arr2 = torch.rand(arr_size, device='cuda')</span><span id="0a2d" class="nv me iq np b gy oa nx l ny nz">pytorch_out = arr1 + arr2<br/>triton_out = add(arr1, arr2)</span><span id="7689" class="nv me iq np b gy oa nx l ny nz">print(torch.sum(torch.abs(pytorch_out - triton_out)))</span></pre><p id="b071" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在外</p><pre class="ld le lf lg gt nr np ns nt aw nu bi"><span id="57cf" class="nv me iq np b gy nw nx l ny nz">&gt; python main.py<br/>tensor(0., device='cuda:0')</span></pre><p id="6713" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是一个简单的例子。但是在矩阵乘法这样的复杂例子中，如何分割数据会对性能产生巨大的影响。Triton docs提供了一个关于如何编写一个高效的矩阵乘法内核的教程<a class="ae lb" href="https://triton-lang.org/getting-started/tutorials/03-matrix-multiplication.html" rel="noopener ugc nofollow" target="_blank">,其中包含了更多的细节。</a></p><p id="95de" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这是一个简短的教程，介绍了GPU编程基础知识，并让您开始使用Triton。如果您想了解更多信息，请查看openai/github 上的Triton项目。</p><p id="52a9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae lb" href="https://twitter.com/Kkushaj" rel="noopener ugc nofollow" target="_blank"> twitter </a>，<a class="ae lb" href="https://www.linkedin.com/in/kushaj/" rel="noopener ugc nofollow" target="_blank"> linkedin </a>，<a class="ae lb" href="https://github.com/KushajveerSingh" rel="noopener ugc nofollow" target="_blank"> github </a></p></div><div class="ab cl om on hu oo" role="separator"><span class="op bw bk oq or os"/><span class="op bw bk oq or os"/><span class="op bw bk oq or"/></div><div class="ij ik il im in"><p id="ab90" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="nd">原载于2021年9月12日</em><a class="ae lb" href="https://kushajveersingh.github.io/blog/writing-custom-cuda-kernels-with-triton" rel="noopener ugc nofollow" target="_blank"><em class="nd">https://kushajveersingh . github . io</em></a><em class="nd">。</em></p></div></div>    
</body>
</html>