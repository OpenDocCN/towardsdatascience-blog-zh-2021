<html>
<head>
<title>Understanding PyTorch Loss Functions: The Maths and Algorithms (Part 2)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">理解PyTorch损失函数:数学和算法(下)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-pytorch-loss-functions-the-maths-and-algorithms-part-2-104f19346425?source=collection_archive---------21-----------------------#2021-04-08">https://towardsdatascience.com/understanding-pytorch-loss-functions-the-maths-and-algorithms-part-2-104f19346425?source=collection_archive---------21-----------------------#2021-04-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="ec49" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">PyTorch中损失函数的数学定义、算法和实现的分步指南</h2></div><p id="11ab" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> Part 1可以在这里找到</strong><a class="ae le" rel="noopener" target="_blank" href="/understanding-pytorch-loss-functions-the-maths-and-algorithms-part-1-6e439b27117e"><strong class="kk iu"/></a><strong class="kk iu">。</strong></p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/0c4448cc230ccf6b721aea7dd2e7cc6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OcRy2f34sP8LpJNO"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@antoine1003?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Antoine Dautry </a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="5a58" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是第1部分的延续，你可以在这里找到<a class="ae le" rel="noopener" target="_blank" href="/understanding-pytorch-loss-functions-the-maths-and-algorithms-part-1-6e439b27117e"/>。在这篇文章中，我们将通过定义数学公式，编码算法并在PyTorch中实现，来深入挖掘PyTorch中鲜为人知但有用的损失函数。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="5997" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">介绍</h1><p id="f13e" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">选择最佳损失函数是一项设计决策，取决于我们的计算约束(如速度和空间)、数据集中是否存在显著异常值以及我们拥有的输入/输出类型。</p><p id="6dcd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">尽管如此，在选择不同的损失函数时，我们需要问的第一个问题是我们面前的数据类型。一般来说，损失函数可以根据我们正在处理的特定任务进行灵活的分组:或者是一个<strong class="kk iu">回归</strong>或者是<strong class="kk iu">分类</strong>问题。回归处理连续的数据集，例如在给定一个国家的人口增长率、城市化、历史GDP趋势等的情况下预测其人均GDP。另一方面，分类问题涉及有限的离散类别，例如预测卫星图像是否正在经历任何降雨事件。</p><h1 id="df8a" class="mc md it bd me mf na mh mi mj nb ml mm jz nc ka mo kc nd kd mq kf ne kg ms mt bi translated">回归损失函数</h1><h2 id="7f61" class="nf md it bd me ng nh dn mi ni nj dp mm kr nk nl mo kv nm nn mq kz no np ms nq bi translated">平均绝对误差(L1损耗)—但更好……(平滑损耗)</h2><p id="acff" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">这个损失函数类似于MAE，但是更好更稳定:例如，防止爆炸梯度。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nr"><img src="../Images/b6be2c50470e0167ea9e9160b4bdc835.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uQOq3T6w2PsGlIrFwh0sVg.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">平滑1损耗</p></figure><p id="07e6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那看起来…令人畏惧！让我们慢慢分解这些。</p><p id="5956" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">第一行概括了损失函数的数学定义。给定一组预测，<em class="mz"> x </em>和它们的基本事实，<em class="mz"> y </em>，我们平均出一个损失指标，<em class="mz"> z. </em></p><p id="f61f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">那么什么是<em class="mz"> z </em>呢？它是一个基于情况的损失函数:<strong class="kk iu"> <em class="mz">如果</em> </strong>预测值和地面实况值之间的绝对差值低于一个<em class="mz">β</em>值(这是一个由用户预先确定的先验值)，我们将该差值的平方乘以0.5并除以<em class="mz">β；</em> <strong class="kk iu"> <em class="mz">否则</em> </strong>从预测值和实际值的绝对差值中减去一半<em class="mz">β</em>。</p><p id="6d22" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在Python中，这可以写成:</p><p id="8a5d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="mz">算法:smooth 1 loss</em></strong></p><pre class="lg lh li lj gt ns nt nu nv aw nw bi"><span id="85f2" class="nf md it nt b gy nx ny l nz oa">import numpy as np</span><span id="f6b3" class="nf md it nt b gy ob ny l nz oa">y_pred = np.array([0.000, 0.100, 0.200])<br/>y_true = np.array([0.000, 0.200, 0.250])</span><span id="2201" class="nf md it nt b gy ob ny l nz oa"># Defining z function <br/>def calculate_z(diff, beta=0.5):<br/>    if (diff &lt; beta).all():<br/>        return 0.5 * np.power(diff, 2) / beta<br/>    else:<br/>        return diff - 0.5 * beta</span><span id="ef90" class="nf md it nt b gy ob ny l nz oa"># Defining Smooth L1 Loss function (with beta defined to be 0.5)<br/>def smoothl1_loss(pred, true):<br/>    # Get absolute difference<br/>    differences = pred - true<br/>    abs = np.absolute(differences)</span><span id="e8d8" class="nf md it nt b gy ob ny l nz oa">    # Get the intermediate z values<br/>    z_values = np.apply_along_axis(calculate_z, 0, abs)</span><span id="e087" class="nf md it nt b gy ob ny l nz oa">    # Get the mean<br/>    smoothl1loss = z_values.mean()<br/>    return smoothl1loss</span><span id="86c1" class="nf md it nt b gy ob ny l nz oa">smoothl1loss_value = smoothl1_loss(y_pred, y_true)<br/>print ("SmoothL1Loss error is: " + str(smoothl1loss_value))</span></pre><p id="001a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在PyTorch中，它只是一个简单的函数调用…</p><p id="23f6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="mz"> PyTorch实现:smoothl loss</em></strong></p><pre class="lg lh li lj gt ns nt nu nv aw nw bi"><span id="6242" class="nf md it nt b gy nx ny l nz oa">import torch</span><span id="9e10" class="nf md it nt b gy ob ny l nz oa">smoothl1_loss = torch.nn.SmoothL1Loss(beta=0.5)</span><span id="ae69" class="nf md it nt b gy ob ny l nz oa">input = torch.randn(2, 3, requires_grad=True)<br/>target = torch.randn(2, 3)<br/>output = smoothl1_loss(input, target)</span><span id="d06d" class="nf md it nt b gy ob ny l nz oa">input<br/>#tensor([[ 0.6051,  1.8739,  0.8015],<br/>        [ 0.6713, -0.2191, -1.6360]], requires_grad=True)</span><span id="162d" class="nf md it nt b gy ob ny l nz oa">target<br/>#tensor([[ 0.5638, -0.0400,  1.0652],<br/>        [-0.7935,  1.6107, -0.3567]])</span><span id="84a9" class="nf md it nt b gy ob ny l nz oa">output<br/>tensor(0.9265, grad_fn=&lt;SmoothL1LossBackward&gt;)</span></pre><h1 id="7fc4" class="mc md it bd me mf na mh mi mj nb ml mm jz nc ka mo kc nd kd mq kf ne kg ms mt bi translated">分类损失函数</h1><p id="1cbe" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">还记得我们的<a class="ae le" rel="noopener" target="_blank" href="/understanding-pytorch-loss-functions-the-maths-and-algorithms-part-1-6e439b27117e">第一部分</a>对二元交叉熵的研究吗？我们将对它稍加改进以获得更大的数值稳定性(即性能更好)。</p><h2 id="781b" class="nf md it bd me ng nh dn mi ni nj dp mm kr nk nl mo kv nm nn mq kz no np ms nq bi translated">二元交叉熵—但更好… (BCE With Logits)</h2><p id="5217" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">这个损失函数是BCE更稳定的版本(即。你可以阅读更多关于log-sum-exp数值稳定性的技巧)，它在计算BCELoss之前结合了一个Sigmoid层。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi oc"><img src="../Images/971fd26aa2c6af1dd5763267bfa501da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*zLOeTMUQ67OrqjLp.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">二元交叉熵损失函数</p></figure><p id="f151" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">简单重述一下BCE:如果你只有两个标签(例如真或假，猫或狗，<em class="mz">等</em>)，那么二元交叉熵(BCE)是最合适的损失函数。请注意，在上面的数学定义中，当实际标签为1 (y(i) = 1)时，函数的后半部分会消失。在实际标签为0 (y(i) = 0)的情况下，等式的前半部分被丢弃。简而言之，我们只是将实际预测概率的对数乘以地面真实类。</p><p id="32a7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在的区别在于在两个地方应用了一个<em class="mz"> Sigmoid </em>函数:</p><ul class=""><li id="63d6" class="od oe it kk b kl km ko kp kr of kv og kz oh ld oi oj ok ol bi translated">log( <strong class="kk iu"> sigmoid </strong> (y_hat))和</li><li id="d276" class="od oe it kk b kl om ko on kr oo kv op kz oq ld oi oj ok ol bi translated">log(<strong class="kk iu">s形</strong> (1 — y_hat))</li></ul><p id="9ce3" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在Python中，这可以写成…</p><p id="d029" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="mz">算法:BCEWithLogits </em> </strong></p><pre class="lg lh li lj gt ns nt nu nv aw nw bi"><span id="ad09" class="nf md it nt b gy nx ny l nz oa">import numpy as np</span><span id="ed55" class="nf md it nt b gy ob ny l nz oa">y_pred = np.array([0.1880, 0.4137, 0.2285])<br/>y_true = np.array([0.0, 1.0, 0.0]) #2 labels: (0,1)</span><span id="d822" class="nf md it nt b gy ob ny l nz oa">def sigmoid(x):<br/>    return 1 / (1 + np.exp(-x))</span><span id="873e" class="nf md it nt b gy ob ny l nz oa">def BCEWithLogits(y_pred, y_true):<br/>    total_bcelogits_loss = np.sum(-y_true * np.log(sigmoid(y_pred)) - (1 - y_true) * np.log(sigmoid(1 - y_pred)))</span><span id="d9e8" class="nf md it nt b gy ob ny l nz oa">    # Get the mean BCEWithLogits loss<br/>    num_of_samples = y_pred.shape[0]<br/>    <!-- -->mean_bcelogits_loss = total_bcelogits_loss / num_of_samples<br/>    <br/>    return mean_bcelogits_loss</span><span id="cd40" class="nf md it nt b gy ob ny l nz oa">bcelogits_value = BCEWithLogits(y_pred, y_true)<br/>print ("BCEWithLogits error is: " + str(<!-- -->bcelogits_value<!-- -->))</span></pre><p id="83cc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">它的PyTorch实现也很简单。</p><p id="aae0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu"> <em class="mz"> PyTorch实现:BCEWithLogits </em> </strong></p><pre class="lg lh li lj gt ns nt nu nv aw nw bi"><span id="22d4" class="nf md it nt b gy nx ny l nz oa">import torch</span><span id="62c2" class="nf md it nt b gy ob ny l nz oa">bcelogits_loss = torch.nn.BCEWithLogitsLoss()</span><span id="150e" class="nf md it nt b gy ob ny l nz oa">input = torch.randn(3, requires_grad=True)<br/>target = torch.empty(3).random_(2)<br/>output = bcelogits_loss(input, target)</span><span id="f3b1" class="nf md it nt b gy ob ny l nz oa">input<br/>#tensor([-0.7844, -1.8377,  0.1435], requires_grad=True)</span><span id="380a" class="nf md it nt b gy ob ny l nz oa">target<br/>#tensor([1., 0., 1.])</span><span id="3dbd" class="nf md it nt b gy ob ny l nz oa">output<br/>#tensor(0.6440, grad_fn=&lt;BinaryCrossEntropyWithLogitsBackward&gt;)</span></pre><h1 id="69ed" class="mc md it bd me mf na mh mi mj nb ml mm jz nc ka mo kc nd kd mq kf ne kg ms mt bi translated">结论</h1><p id="a509" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">这就是我们对PyTorch的不太流行但更好(在数值上更稳定等)的损失函数、它们的数学定义、算法实现和PyTorch的API实践的中间接触。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="7fab" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">【https://tinyurl.com/2npw2fnz】<em class="mz">订阅我的电子邮件简讯:</em><a class="ae le" href="https://tinyurl.com/2npw2fnz" rel="noopener ugc nofollow" target="_blank"><em class="mz"/></a><em class="mz"/><strong class="kk iu"><em class="mz">我定期在那里用通俗易懂的英语和漂亮的可视化总结AI研究论文。</em>T55】</strong></p></div></div>    
</body>
</html>