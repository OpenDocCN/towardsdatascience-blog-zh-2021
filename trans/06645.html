<html>
<head>
<title>How do you apply PCA to Logistic Regression to remove Multicollinearity?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何将主成分分析应用于逻辑回归以消除多重共线性？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-do-you-apply-pca-to-logistic-regression-to-remove-multicollinearity-10b7f8e89f9b?source=collection_archive---------2-----------------------#2021-06-15">https://towardsdatascience.com/how-do-you-apply-pca-to-logistic-regression-to-remove-multicollinearity-10b7f8e89f9b?source=collection_archive---------2-----------------------#2021-06-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3e33" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">PCA在消除多重共线性中的作用</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/9363f1ab1061ac6f2944c4de222ee930.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m3t-4Rl3bQrEejeRuwpFlQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Gabriella Clare Marino 在<a class="ae ky" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="f079" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="lv">当要素(输入变量)与数据集中的一个或多个其他要素高度相关时，会出现多重共线性</em> </strong>。它会影响回归和分类模型的性能。PCA(主成分分析)利用多重共线性，将高度相关的变量组合成一组不相关的变量。因此，PCA可以有效地消除特征之间的多重共线性。</p><p id="11e1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本帖中，我们将在名为<em class="lv">乳腺癌</em>数据的分类数据集上构建一个逻辑回归模型。初始模型可以被认为是基础模型。然后，我们将对<em class="lv">乳腺癌</em>数据应用主成分分析，并再次建立逻辑回归模型。之后，我们将比较基本模型和这个模型之间的性能。最后，我们将建立一个机器学习管道，将应用逻辑回归和PCA的多个步骤结合起来。读完这篇文章后，你将能够将PCA应用于逻辑回归模型。</p><p id="7f9e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们开始吧！</p><h1 id="dfb3" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">探索<em class="mo">乳腺癌</em>数据集</h1><p id="b086" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">该数据集是Scikit-learn中的内置数据集之一。根据<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html" rel="noopener ugc nofollow" target="_blank"> Scikit-learn文档</a>，它有569个样本的30个特征。目标变量有两个类别，称为0(没有乳腺癌)和1(有乳腺癌)。由于目标变量只有2个类，因此数据集用于二进制分类。数据集中没有缺失值。所有的值都是数字。所以，不需要清洗！</p><h2 id="04fd" class="mu lx it bd ly mv mw dn mc mx my dp mg li mz na mi lm nb nc mk lq nd ne mm nf bi translated">加载数据集</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ng nh l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等到加载Python代码！</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/978da1de030c3e8c1dd39dedd3adc033.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b-pqvWPO6nJyfACo7ojrYg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><em class="mo">乳腺癌</em>数据集的一部分(图片由作者提供)</p></figure><h2 id="c655" class="mu lx it bd ly mv mw dn mc mx my dp mg li mz na mi lm nb nc mk lq nd ne mm nf bi translated">创建热图</h2><p id="11f7" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">热图可以用漂亮的彩色图来显示连续变量的相关系数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ng nh l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等到加载Python代码！</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/cae63c1d638dff2e77a70f7af7d6b24e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mcMIxjqqLU2ZKXiRFnwNzA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="a7de" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如您在热点图中看到的，数据集中的一些要素彼此高度相关。因此，存在多重共线性。</p><h1 id="62ee" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">构建逻辑回归模型(基础模型)</h1><p id="60c5" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">以下代码块基于<em class="lv">乳腺癌</em>数据构建逻辑回归模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ng nh l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等到加载Python代码！</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/42b2cd85348d6ec02dd105c7671914a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*FZ8dSjxk6x58GnH0oNaszA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">基本模型的输出(图片由作者提供)</p></figure><p id="402e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基础模型非常好。一点也不合身。它在新的未知数据上表现良好。让我们看看是否可以通过应用主成分分析来提高模型的性能。</p><h1 id="bd5f" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">对<em class="mo">乳腺癌</em>数据应用主成分分析</h1><h2 id="49f8" class="mu lx it bd ly mv mw dn mc mx my dp mg li mz na mi lm nb nc mk lq nd ne mm nf bi translated">PCA概述</h2><blockquote class="nl nm nn"><p id="d721" class="kz la lv lb b lc ld ju le lf lg jx lh no lj lk ll np ln lo lp nq lr ls lt lu im bi translated">PCA是一种线性降维技术(算法)，它将一组相关变量(p)转换成较小的k (k <p number="" of="" uncorrelated="" variables="" called="" class="lb iu"> <em class="it">主成分</em> ，同时尽可能多地保留原始数据集中的变化。在机器学习(ML)的背景下，PCA是一种用于降维的无监督机器学习算法。如果变量不是在相似的尺度上测量的，我们需要在对我们的数据应用PCA之前进行特征缩放。这是因为PCA方向对数据的规模高度敏感。PCA中最重要的部分是为给定的数据集选择最佳数量的组件。</p></p></blockquote><h2 id="cfe5" class="mu lx it bd ly mv mw dn mc mx my dp mg li mz na mi lm nb nc mk lq nd ne mm nf bi translated">为我们的数据选择最佳数量的电脑</h2><p id="37c4" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">首先，我们应用PCA，保持所有组件等于原始维数(即30 ),并观察PCA如何很好地捕捉我们的数据的方差。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ng nh l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等到加载Python代码！</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/b612e7e3c06f9aa9f94fd1ecdb576fa4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1228/format:webp/1*P7dIozW3sdiILOEz4G36Ew.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="aa0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">第一个分量单独捕获数据中约44%的可变性，第二个分量捕获数据中约19%的可变性，依此类推。前6个分量一起捕获了数据中约88.76%的可变性。我们有兴趣保留前6个组件。</p><h2 id="5ef4" class="mu lx it bd ly mv mw dn mc mx my dp mg li mz na mi lm nb nc mk lq nd ne mm nf bi translated">使用6种成分再次运行PCA</h2><p id="f4d8" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">现在，我们得到了包含6个组件的转换数据集。为此，我们需要通过设置<strong class="lb iu"> n_components=6 </strong>再次运行PCA。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ng nh l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等到加载Python代码！</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/f15c72235d8f9050113b221274b22dc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*YP88lv3zogprsFfcavQJuA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">转换数据集的一部分(作者提供的图像)</p></figure><p id="0da9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们可以使用这个转换后的数据集代替原始的<em class="lv">乳腺癌</em>数据集来建立逻辑回归模型。这是两个数据集之间的区别。</p><ul class=""><li id="eac5" class="nt nu it lb b lc ld lf lg li nv lm nw lq nx lu ny nz oa ob bi translated">原始数据集有30个要素，而转换后的数据集有6个组件。</li><li id="6143" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated">转换后的数据集仅捕获原始数据集中约88.76%的可变性。</li><li id="58fe" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated">两个数据集对应的值完全不同。</li><li id="82fc" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ny nz oa ob bi translated">原始数据集中的一些变量与一个或多个其他变量高度相关(多重共线性)。转换后的数据集中没有变量与一个或多个其他变量相关。</li></ul><h2 id="29f3" class="mu lx it bd ly mv mw dn mc mx my dp mg li mz na mi lm nb nc mk lq nd ne mm nf bi translated">创建转换数据集的热图</h2><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="45f5" class="mu lx it oi b gy om on l oo op">fig = plt.figure(figsize=(10, 8))<br/>sns.heatmap(X_pca.corr(), <br/>            annot=True)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/44c588c4d3d6bff7f45a422b99499a00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*mFw4kU76f-Ft3jIJetod6Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="637a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们看不到组件之间的任何关联。这是因为PCA已经将原始数据集中的一组相关变量转换为一组<strong class="lb iu"> <em class="lv">不相关</em> </strong>变量。</p><h2 id="e3b7" class="mu lx it bd ly mv mw dn mc mx my dp mg li mz na mi lm nb nc mk lq nd ne mm nf bi translated">对转换后的数据建立逻辑回归模型</h2><p id="2f6c" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">下面的代码块在转换后的数据集(通过应用PCA获得的数据集)上构建逻辑回归模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ng nh l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等到加载Python代码！</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/d6859ac66bc9be1367d012c707f3523c.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*wjNMyFyvOV_Xqf59Hgha6A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="0bbb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以将这个输出与我们基本模型的先前输出进行比较。测试精度提高了3%。假阳性和假阴性也减少了。然而，转换的数据集(通过应用PCA获得的数据集)仅捕获原始数据集中约88.76%的可变性。所以，<strong class="lb iu"> <em class="lv">这款车型的性能提升背后的原因是什么？</em> </strong> <em class="lv"> </em>显而易见的答案是<em class="lv"> </em> <strong class="lb iu"> <em class="lv"> PCA有效地消除了多重共线性！</em>T11】</strong></p><h1 id="35f9" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">构建机器学习管道</h1><p id="41cd" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">机器学习管道通过将应用逻辑回归的多个步骤与PCA相结合，简化了整个过程。以下是步骤:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/732d61d77fb7f685c89e1509fdbb40cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fAKmpkbhnYtxKtwss_uAjw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ng nh l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等到加载Python代码！</p></figure><p id="9589" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"><em class="lv">log _ reg _ model</em></strong>流水线通过顺序应用变压器列表和最终预测器来简化训练过程。在我们的渠道中:</p><ol class=""><li id="9983" class="nt nu it lb b lc ld lf lg li nv lm nw lq nx lu ot nz oa ob bi translated"><strong class="lb iu"> StandardScaler() </strong>是一个变压器。</li><li id="7a82" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ot nz oa ob bi translated"><strong class="lb iu"> PCA() </strong>是变压器。</li><li id="f6a8" class="nt nu it lb b lc oc lf od li oe lm of lq og lu ot nz oa ob bi translated"><strong class="lb iu"> LogisticRegression() </strong>是预测者。</li></ol><p id="67d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们可以通过一个<strong class="lb iu">训练所有的估计器。</strong>称之为合体()。</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="d3d2" class="mu lx it oi b gy om on l oo op">log_reg_model.fit(X,y)</span></pre><p id="7a06" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们已经完成了引言中承诺的工作。</p><h1 id="8be8" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">摘要</h1><p id="ac23" class="pw-post-body-paragraph kz la it lb b lc mp ju le lf mq jx lh li mr lk ll lm ms lo lp lq mt ls lt lu im bi translated">PCA对于消除多重共线性是有用的。它充当数据预处理步骤。PCA还有很多其他的用例。这只是降维技术之一。你可以通过阅读我写的下面这篇文章找到更多关于那些事情的信息:</p><div class="ou ov gp gr ow ox"><a rel="noopener follow" target="_blank" href="/11-dimensionality-reduction-techniques-you-should-know-in-2021-dcb9500d388b"><div class="oy ab fo"><div class="oz ab pa cl cj pb"><h2 class="bd iu gy z fp pc fr fs pd fu fw is bi translated">2021年你应该知道的11种降维技术</h2><div class="pe l"><h3 class="bd b gy z fp pc fr fs pd fu fw dk translated">减少数据集的大小，同时尽可能保留变化</h3></div><div class="pf l"><p class="bd b dl z fp pc fr fs pd fu fw dk translated">towardsdatascience.com</p></div></div><div class="pg l"><div class="ph l pi pj pk pg pl ks ox"/></div></div></a></div></div><div class="ab cl pm pn hx po" role="separator"><span class="pp bw bk pq pr ps"/><span class="pp bw bk pq pr ps"/><span class="pp bw bk pq pr"/></div><div class="im in io ip iq"><p id="ff73" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我的读者可以通过下面的链接注册成为会员，以获得我写的每个故事的全部信息，我将收到你的一部分会员费。</p><p id="5428" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">报名链接:</strong><a class="ae ky" href="https://rukshanpramoditha.medium.com/membership" rel="noopener">https://rukshanpramoditha.medium.com/membership</a></p><p id="d094" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">非常感谢你一直以来的支持！下一个故事再见。祝大家学习愉快！</p><p id="87c1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">特别感谢Unsplash <strong class="lb iu"> </strong>上的<strong class="lb iu">加布里埃拉·克莱尔·马里诺</strong>，她为我提供了这篇文章的封面图片。本帖提供的文字内容、代码示例、其他图片和内容链接，版权归作者所有。</p><p id="a4b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="pt pu ep" href="https://medium.com/u/f90a3bb1d400?source=post_page-----10b7f8e89f9b--------------------------------" rel="noopener" target="_blank"> <strong class="lb iu">鲁克山普拉莫迪塔</strong></a><strong class="lb iu"><br/>2021–06–15</strong></p></div></div>    
</body>
</html>