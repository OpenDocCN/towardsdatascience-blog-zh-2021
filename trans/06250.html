<html>
<head>
<title>Machine Translation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器翻译</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/machine-translation-b0f0dbcef47c?source=collection_archive---------28-----------------------#2021-06-04">https://towardsdatascience.com/machine-translation-b0f0dbcef47c?source=collection_archive---------28-----------------------#2021-06-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9b48" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">统计机器翻译——SMT和神经机器翻译——NMT:带和不带注意机制的编码器-解码器结构和最先进的转换器架构。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b75556dfbb1a1cc46158bdceeb4f5435.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kwbd_SWCSm_vC3dWWxShGg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@johnygoerend" rel="noopener ugc nofollow" target="_blank">张诗钟·戈伦德</a>在<a class="ae ky" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="a5f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">机器翻译在当今数字化和全球化的世界中发挥着至关重要的作用。它通过处理和翻译一种自然语言为另一种自然语言造福社会。随着技术的进步，不同地区使用不同的语言交流着大量的信息。这使得对机器翻译的需求在过去几十年里呈指数级增长。正因为如此，在过去的几年里，机器翻译已经成为一个活跃的研究领域。它可以分为三种不同的方法:基于规则的方法、统计方法和神经方法。在本文中，我将主要关注统计和神经方法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/f79c5345df0a99ea550b2496522d12a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h-siXYTKmAA7z1pSyhJ3Zg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1:不同的机器翻译方法及其时间表。</p></figure><h1 id="2b90" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">介绍</h1><p id="aec8" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">机器翻译是将一种语言(源语言)的句子翻译成另一种语言(目标语言)的句子的任务。它是计算语言学的子领域，旨在利用计算设备自动将文本从一种语言翻译成另一种语言。机器翻译研究始于20世纪50年代初(冷战时期)。在此期间，需要将俄语文档翻译成英语。由于俄语专家不多，翻译又很费时间，所以机器翻译是有针对性的应用。当时开发的系统大多是基于规则的，使用双语词典将俄语单词映射到其英语对应单词。尽管效果不太好，但在20世纪80年代末，它让位于基于统计的系统。在20世纪90年代，基于统计单词和短语的方法变得流行起来，这种方法几乎不需要语言信息。统计机器翻译(SMT)的核心思想是从数据中学习概率模型。在2010年代，随着深度神经网络的出现，神经机器翻译成为一个主要的研究领域。NMT是一种使用深度神经网络进行机器翻译的方法。神经架构被称为序列对序列(seq2seq)。普通的seq2seq NMT包括两个递归神经网络(RNN) [1]。NMT的研究开创了自然语言处理(NLP)的许多最新创新，研究人员发现了香草seq2seq的许多改进，其中一个主要改进是注意力机制的使用[2]。受注意力机制的启发，论文“注意力是你所需要的全部”介绍了一种称为Transformer的新颖架构，这是目前最先进的语言模型[3]。这种架构完全依赖于注意力机制，没有任何RNN，其变体如BERT已经应用于许多NLP任务，并能够实现最先进的性能。</p><h1 id="6e47" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">统计机器翻译</h1><p id="fc43" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">统计机器翻译(SMT)从数据中学习概率模型。假设如果我们从德语翻译成英语，我们想要找到最好的英语句子y，给定德语句子x。SMT将制定任务如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/1c4ba9d40a1daf1e1ed2046851e7fca5.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/format:webp/1*kCEazfZzLF8Qca0LzBqJMA.png"/></div></figure><p id="fe81" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">意思是在所有可能的y中，我们要找到最好的一个。通过使用贝叶斯规则，我们可以将上面的公式转换成下面的公式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/19eaf3b2f44b9d6c6beedb2f5dd9de95.png" data-original-src="https://miro.medium.com/v2/resize:fit:484/format:webp/1*cG1VS768xhb0FabUg_wO-Q.png"/></div></figure><p id="7941" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">P(x|y)被称为<strong class="lb iu">翻译模型</strong>,它使用并行数据来模拟单词和短语应该如何翻译。并行数据的一个例子是成对的人工翻译的德语-英语句子。P(x|y)被进一步分解成P(x，a|y ),其中a是单词对齐，即源句子x和目标句子y之间的单词级和短语级对应</p><p id="9803" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">P(y)被称为<strong class="lb iu">语言模型</strong>,它使用单语数据来模拟在一种语言中生成字符串的概率。语言模型是一个函数，它对从词汇序列中提取的字符串进行概率度量。给定长度为n的字符串y，我们可以将语言模型概率P(y)推导为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/e3247699a6a0a88565ef36cca77cb3b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*jj0NSt3j_neoltUdr_0cLQ.png"/></div></figure><p id="617a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，给定一个词的整个历史来计算它的概率是低效的，我们可以通过使用n-gram模型来近似它。在n-gram模型中，它作出马尔可夫假设，即yᵢ将只依赖于前面的n-1个单词。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/14caf52abac4800310482e0957f3d37d.png" data-original-src="https://miro.medium.com/v2/resize:fit:428/format:webp/1*Hy6iwVjsxwe-XaV2FYtlmw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">二元模型示例</p></figure><p id="189d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了计算argmax，我们可以枚举每个可能的翻译y并计算概率，然而，这在计算上是昂贵的。因此，它使用解码，一种启发式搜索算法，通过删除低概率的假设来搜索最佳翻译。这是SMT工作原理的一个简要概述。最好的SMT系统极其复杂，许多重要的细节没有在这里讨论。SMT的开发既昂贵又耗时，因为它需要大量的功能工程和人力来维护。[4]</p><h1 id="d178" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">神经机器翻译</h1><p id="7fff" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">深度神经网络已经在各种应用中实现了最先进的性能。沿着使用神经网络进行SMT的研究路线，神经机器翻译(NMT)成为主要的研究领域。它使用单个端到端神经网络架构，称为序列到序列(seq2seq ),涉及两个RNN:编码器RNN和解码器RNN。编码器RNN将使用一个编码向量对源序列进行汇总，解码器RNN将根据先前的编码向量生成目标句子。seq2seq模型是一个条件语言模型，<strong class="lb iu">直接计算P(y|x) </strong>，因为解码器RNN是通过对源句子x进行条件化来预测目标句子y的下一个单词，seq2seq模型还可以用于许多其他自然语言处理任务，如摘要、对话聊天机器人等。</p><h2 id="ab10" class="mx lx it bd ly my mz dn mc na nb dp mg li nc nd mi lm ne nf mk lq ng nh mm ni bi translated">序列间(Seq2Seq)</h2><p id="5744" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">在如图2所示的标准seq2seq中，编码器RNN(蓝色块)分析源语言中的输入句子，并使用称为隐藏状态向量的历史向量对输入序列进行编码。最后的隐藏状态或编码向量被传递给解码器RNN(红色块)作为初始隐藏状态。解码器初始隐藏状态与源语句的<eos>一起将生成一个隐藏状态，然后传递给线性层。</eos></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/7d9656d62bcab1df15f2eb7c848789d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J_McY0RSIF_DkihClUAx8Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2:NMT——一个将序列A B C D翻译成X Y Z的堆叠RNN seq2seq模型<eos>是句尾。[2]</eos></p></figure><p id="aeaa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Softmax后面的线性层将输出目标语言的整个词汇的概率分布。根据该概率分布，它将选择具有最高概率的记号作为第一个字，即X，并且它将被用作解码的第二输入。来自前一步骤的第二隐藏状态和第一生成的字X将被输入到解码器RNN的第二步骤。并且将重复相同的过程，直到它产生一个<eos>令牌。从解码器RNN生成的令牌序列将是seq2seq模型的结果。[1]</eos></p><p id="3aa3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与SMT相比，NMT的优势在于它具有更好的性能，并且需要更少的人力。但是，它的可解释性较差，很难调试，也很难控制。</p><h2 id="b7d5" class="mx lx it bd ly my mz dn mc na nb dp mg li nc nd mi lm ne nf mk lq ng nh mm ni bi translated">Seq2Seq注意</h2><p id="2073" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">如前所述，在标准seq2seq模型中，编码器RNN的最后隐藏状态被用作解码器RNN的初始状态，这意味着关于源句子的所有信息将被编码为单个向量，并且它是来自源句子的唯一信息来解码目标句子。因此，编码器RNN的最后一个隐藏状态可以成为<strong class="lb iu">信息瓶颈</strong>，因为它必须仅用一个向量来捕获关于源句子的所有信息。注意机制已经被用来解决这个问题，在翻译过程中有选择地关注源句子的部分。核心思想是，在解码器的每一步，它都使用与编码器的直接连接来获得源序列的加权关注。图3展示了如何将注意力层添加到一个普通的seq2seq模型中。在解码过程中的每个时间步长t，注意力层将导出捕获相关源序列信息的上下文向量cₜ，以帮助预测当前目标词yₜ.</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/943fc13474a15e08181efb85749c093e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*juIhmaIkbuvXc241EgePLA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3:带有注意机制的seq2seq。[2]</p></figure><p id="1354" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意力分布aₜ是通过使用当前目标隐藏状态hₜ和编码器ℎ̅ₛ.的所有隐藏状态来计算的为了获得注意力分布，它将首先计算注意力分数，然后应用Softmax将分数转换为概率分布。有三种不同的方法来计算注意力分数:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/c442d0b544ae1706c7fa04e396a4d711.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LGfQDqLXa6BVmCL2NFE-IQ.png"/></div></div></figure><p id="9b8b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过使用这个对准向量aₜ作为权重，可以将上下文向量cₜ计算为所有编码器隐藏状态的加权平均值。上下文向量或注意力输出将主要包含来自获得高注意力分数的编码器的隐藏状态的信息。该上下文向量然后将与解码器的隐藏状态连接，解码器的隐藏状态然后将选择具有最高概率的目标语言标记，如在普通seq2seq模型中一样。有时，来自前一步骤的上下文向量可用于与通常的解码器输入一起馈入解码器。</p><p id="9946" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注意机制</strong>显著地<strong class="lb iu">提高了NMT的表现</strong>，并成为NLP研究的关键数学结构。注意，目标序列中的每个单词只需要找到它与源序列中的几个单词的匹配。它以某种方式解决了RNN中的长期依赖性的限制，即目标序列中的任何单词都可以与源序列中的所有单词进行通信。此外，注意力机制提供了一些可解释性，因为网络被训练来自己学习软对齐，并且我们可以通过检查注意力分布来看到解码器正在关注什么。</p><p id="da85" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在机器翻译任务中仍然存在许多困难，例如词汇表之外的问题、训练和测试数据之间的领域不匹配、在长序列上维护上下文、标签数据不太可用的低资源语言对。</p><h2 id="c020" class="mx lx it bd ly my mz dn mc na nb dp mg li nc nd mi lm ne nf mk lq ng nh mm ni bi translated">变压器</h2><p id="61b9" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">基于具有注意力的最佳性能seq2seq模型，论文《注意力是你所需要的全部》在2017年提出了一种新的架构，称为Transformer，如图4所示。主要思想是基本上只使用注意力作为表示学习，因为在之前的seq2seq注意力模型中，证明了编码器和解码器之间的注意力在NMT中是至关重要的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/20b878ca957598b8b3eb424ffb593247.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*oqhLHHWQ0y__Wlg94PuF_A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4:变压器模型架构，左侧模块是编码器，右侧模块是解码器。[3]</p></figure><p id="bd0d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我将从《变形金刚》中用于表征学习的自我关注的主要概念开始。考虑Q，K，V是单词嵌入向量。q矩阵(查询)是序列中一个单词的向量表示，K矩阵(键)是序列中所有单词的向量表示，V矩阵(值)是序列中所有单词的向量表示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/47089a527b1eac0032137207bdb46059.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*lW5ukKgeu0GOovZ6JVdsLQ.png"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/47ff08f4c401fc460dc433f55f985e69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*yDmjEYjFvu8mh67qvjInzA.png"/></div></figure><p id="4725" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">代替使用单个注意函数，该论文提出使用“多头注意”来线性地投射具有不同可学习权重的Q、K、V h次。这提供了多个表示子空间，并允许模型关注不同的位置。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/a2782353808ad164ddf12ed1eb411dd5.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*OWtQp_u4nisiifz0J5aKVw.png"/></div></figure><p id="68e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与RNN不同，《变形金刚》中的自我关注是<strong class="lb iu">顺序不变的</strong>，这意味着它不会捕捉序列信息。因此，添加了<strong class="lb iu">“位置编码”</strong>来使用正弦函数注入每个令牌的位置信息。这种解决方案提供了确定性的方法来合并序列信息，而不增加可学习参数的数量。</p><p id="ea5d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在Transformer编码器中，在注入位置编码后，编码器的输入嵌入将首先流经多头注意层，其输出随后将馈入前馈神经网络。在Transformer解码器中，有两个注意力层，称为掩蔽多头注意力和编码器-解码器注意力。通过屏蔽未来字，屏蔽多头解码器对先前生成的输出进行自我关注。编码器-解码器注意使用来自屏蔽多头的查询；编码器输出的键和值。通常，解码器输出将传递到线性层和Softmax函数，以获得目标令牌的概率分布。注意层和前馈层之间的每个子层具有残差连接(跳过连接),其后是层归一化，以改变输入，使每层具有均值0和1。</p><p id="b0b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Transformer通过提供仅需要一次计算的成对并行化乘法交互(自关注),取代了以前的seq2seq RNN模型中使用的顺序计算。该架构是当前最先进的架构，提供了高性能和可解释性。</p><h1 id="95e3" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">评估方法</h1><p id="c940" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">评估机器翻译最常用的方法是双语评估替角(BLEU)。它将机器书面翻译与一个或几个人类书面翻译进行比较，并基于n-gram(通常是1、2、3和n-gram)精度计算相似性得分。BLEU非常有用，大多数机器翻译系统都是用BLEU来评估的。然而，它并不完美，因为有各种方法来翻译一个句子。一个好的翻译可能会得到一个糟糕的BLEU分数，因为它与人类翻译的n-gram重叠很低。</p></div><div class="ab cl nq nr hx ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="im in io ip iq"><h1 id="ebd3" class="lw lx it bd ly lz nx mb mc md ny mf mg jz nz ka mi kc oa kd mk kf ob kg mm mn bi translated">参考</h1><p id="23ae" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">[1] K. Cho等人，“使用统计机器翻译的RNN编码器-解码器学习短语表示”，em NLP 2014–2014 Conf .Empir。方法Nat。郎。过程。继续。糖膏剂，第1724–1734页，2014年。</p><p id="a29e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[2] M. T. Luong、H. Pham和C. D. Manning，“基于注意力的神经机器翻译的有效方法”，计算语言学协会，2015年。</p><p id="ee44" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[3] A. Vaswani等人，“注意力是你所需要的一切”，2017年。</p><p id="1caf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">[4] A. Garg和M. Agarwal，《机器翻译:一个文献综述》，2018年。</p></div></div>    
</body>
</html>