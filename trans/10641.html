<html>
<head>
<title>The Choice of Regularization: Ridge, Lasso and Elastic Net Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">正则化的选择:脊、套索和弹性网回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-choice-of-regularization-ridge-lasso-and-elastic-net-regression-fc696f697209?source=collection_archive---------12-----------------------#2021-10-12">https://towardsdatascience.com/the-choice-of-regularization-ridge-lasso-and-elastic-net-regression-fc696f697209?source=collection_archive---------12-----------------------#2021-10-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4bf5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">将L1、L2或L1和L2正则化应用于线性回归</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d5b75dd9964aa0dcb65e06d683696ad6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iQv1rNsVYy765BnMYDJsaQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">安德烈·亨特在<a class="ae ky" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="8e47" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">大概，你可能听过<strong class="lb iu">【山脊】</strong><strong class="lb iu">【套索】</strong><strong class="lb iu">【弹力网】</strong>这样的术语。这些只是技术术语。这些背后的基本概念是正规化。我们将很快在本帖中澄清这一点。</p><p id="d8c7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">之前，我们已经从另一个角度讨论过正则化:<a class="ae ky" rel="noopener" target="_blank" href="/how-to-mitigate-overfitting-with-regularization-befcf4e41865">用正则化</a>减轻过拟合。正则化的主要好处是减轻过度拟合。正则化模型能够很好地概括未知数据。</p><p id="25dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基本上，正则化是通过向我们试图最小化的损失(成本)函数添加另一项来限制(控制)模型学习过程的过程。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lv"><img src="../Images/8dbb74989ce79fd724213aa4a65f8492.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*GzPfxQi45IYys6PYaV1_NA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="543a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">正则项</strong>(也称为<strong class="lb iu">惩罚项</strong>)可以采取不同的形式，这将在本文中很快讨论。</p><p id="056b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">预测连续值输出的线性回归模型通过最小化其损失函数来学习其系数的最佳值。同样的事情也适用于预测离散值输出的逻辑回归模型。在这两种情况下，我们都可以在模型训练阶段应用正则化。</p><p id="12b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当我们考虑将Scikit-learn<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">LogisticRegression()</strong></a>类用于逻辑回归模型时，有一个名为<strong class="lb iu"> penalty </strong>的超参数用于选择正则化类型。</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="7a45" class="mb mc it lx b gy md me l mf mg">LogisticRegression(<strong class="lx iu">penalty</strong>='...')</span></pre><p id="adba" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于<strong class="lb iu">惩罚</strong>(规则化类型)，有4个选项可供选择。</p><ul class=""><li id="49a6" class="mh mi it lb b lc ld lf lg li mj lm mk lq ml lu mm mn mo mp bi translated"><strong class="lb iu">‘无’</strong>—不应用正则化</li><li id="bafc" class="mh mi it lb b lc mq lf mr li ms lm mt lq mu lu mm mn mo mp bi translated"><strong class="lb iu">‘l1’</strong>—应用L1正则化</li><li id="9757" class="mh mi it lb b lc mq lf mr li ms lm mt lq mu lu mm mn mo mp bi translated"><strong class="lb iu">‘l2’</strong>—应用L2正则化(默认选择)</li><li id="e568" class="mh mi it lb b lc mq lf mr li ms lm mt lq mu lu mm mn mo mp bi translated"><strong class="lb iu">“弹性网”</strong>—L1和L2正则化均适用</li></ul><p id="a3fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，当我们考虑将<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu">linear regression()</strong></a>类用于线性回归模型时，没有特定的超参数来选择正则化类型。相反，我们应该为每种类型的正则化使用3个单独的类。</p><ul class=""><li id="823e" class="mh mi it lb b lc ld lf lg li mj lm mk lq ml lu mm mn mo mp bi translated">当我们将<strong class="lb iu"> L2正则化</strong>应用于线性回归的成本函数时，它被称为<strong class="lb iu">岭</strong>回归。</li><li id="6618" class="mh mi it lb b lc mq lf mr li ms lm mt lq mu lu mm mn mo mp bi translated">当我们将<strong class="lb iu"> L1正则化</strong>应用于线性回归的代价函数时，称为<strong class="lb iu">类</strong> o回归。</li><li id="79b3" class="mh mi it lb b lc mq lf mr li ms lm mt lq mu lu mm mn mo mp bi translated">当我们同时将<strong class="lb iu">L1和L2正则化</strong>应用于线性回归的成本函数时，称为<strong class="lb iu">弹性网</strong>回归。</li></ul><p id="95a9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以上所有回归类型都属于<strong class="lb iu"> <em class="mv">正则化回归</em> </strong>的范畴。</p><p id="1b65" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们详细讨论每一种类型。</p><h1 id="560e" class="mw mc it bd mx my mz na nb nc nd ne nf jz ng ka nh kc ni kd nj kf nk kg nl nm bi translated">里脊回归</h1><p id="dd4e" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">这里，我们将L2正则化项(定义如下)应用于线性回归的成本函数:</p><p id="0c2e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">L2 =α.σ(系数的平方值)</strong></p><p id="138f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html" rel="noopener ugc nofollow" target="_blank">岭回归</a>的Scikit-learn类为:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="a4bf" class="mb mc it lx b gy md me l mf mg">Ridge(<strong class="lx iu">alpha</strong>=...)</span></pre><p id="e670" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> alpha </strong>是控制正则化强度的超参数。它必须是正浮点数。默认值为1。较大的α值意味着较强的正则化(较少过拟合，可能欠拟合！).较小的值意味着弱正则化(过度拟合)。我们想要建立一个既不过度拟合也不欠拟合数据的模型。所以，我们需要为α选择一个最佳值。为此，我们可以使用<a class="ae ky" rel="noopener" target="_blank" href="/python-implementation-of-grid-search-and-random-search-for-hyperparameter-optimization-2d6a82ebf75c">超参数调整技术</a>。</p><p id="01af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注:</strong> <code class="fe ns nt nu lx b">Ridge(<strong class="lb iu">alpha=0</strong>)</code>相当于<strong class="lb iu"> LinearRegression() </strong>类求解的正态线性回归。不建议将<code class="fe ns nt nu lx b">alpha=0</code>与岭回归一起使用。相反，你应该使用正常的线性回归。</p><h1 id="2182" class="mw mc it bd mx my mz na nb nc nd ne nf jz ng ka nh kc ni kd nj kf nk kg nl nm bi translated">套索回归</h1><p id="ab56" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">这里，我们将L1正则化项(定义如下)应用于线性回归的成本函数:</p><p id="cc0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">L1 =α.σ(系数的绝对值)</strong></p><p id="ffb6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">套索回归的Scikit-learn类是:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="4cea" class="mb mc it lx b gy md me l mf mg">Lasso(<strong class="lx iu">alpha</strong>=...)</span></pre><p id="eed3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个α及其定义与L2术语中定义的α相同。默认值为1。</p><p id="1557" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注:</strong> <code class="fe ns nt nu lx b">Lasso(<strong class="lb iu">alpha=0</strong>)</code> <strong class="lb iu"> </strong>相当于<strong class="lb iu"> LinearRegression() </strong>类求解的正态线性回归。不建议将<code class="fe ns nt nu lx b">alpha=0</code>与套索回归一起使用。相反，你应该使用正常的线性回归。</p><h1 id="68a0" class="mw mc it bd mx my mz na nb nc nd ne nf jz ng ka nh kc ni kd nj kf nk kg nl nm bi translated">弹性净回归</h1><p id="76b4" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">这里，我们同时将L1和L2正则化项应用于线性回归的成本函数。</p><p id="f53c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html" rel="noopener ugc nofollow" target="_blank">弹性网回归</a>的Scikit-learn类为:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="8dfc" class="mb mc it lx b gy md me l mf mg">ElasticNet(<strong class="lx iu">alpha</strong>=..., <strong class="lx iu">l1_ratio</strong>=...)</span></pre><p id="b089" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">超参数<strong class="lb iu"> l1_ratio </strong>定义了我们如何混合<strong class="lb iu"/>L1和L2正则化。因此，它被称为<strong class="lb iu"> <em class="mv"> ElasticNet混合参数</em> </strong>。<strong class="lb iu"> l1_ratio </strong>的可接受值范围为:</p><pre class="kj kk kl km gt lw lx ly lz aw ma bi"><span id="907d" class="mb mc it lx b gy md me l mf mg"><strong class="lx iu">0 &lt;= l1_ratio &lt;= 1</strong></span></pre><p id="774b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是可能的情况:</p><ul class=""><li id="2ad8" class="mh mi it lb b lc ld lf lg li mj lm mk lq ml lu mm mn mo mp bi translated"><code class="fe ns nt nu lx b">l1_ratio = 0</code>意味着没有L1项，只有L2正则化。</li><li id="fd1d" class="mh mi it lb b lc mq lf mr li ms lm mt lq mu lu mm mn mo mp bi translated"><code class="fe ns nt nu lx b">l1_ratio = 1</code>表示没有L2项，只有L1正则化。</li><li id="4376" class="mh mi it lb b lc mq lf mr li ms lm mt lq mu lu mm mn mo mp bi translated"><code class="fe ns nt nu lx b">0 &lt; l1_ratio &lt; 1</code>是指该法规被定义为L1和L2术语的组合。如果<code class="fe ns nt nu lx b">l1_ratio</code>接近1，这意味着L1项占优势。如果<code class="fe ns nt nu lx b">l1_ratio</code>接近0，这意味着L2项占优势。</li></ul><p id="93a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以，这就是术语<strong class="lb iu">【脊】</strong><strong class="lb iu">【套索】</strong><strong class="lb iu">【弹力网】</strong>背后的想法！</p><h1 id="8c86" class="mw mc it bd mx my mz na nb nc nd ne nf jz ng ka nh kc ni kd nj kf nk kg nl nm bi translated">摘要</h1><p id="4023" class="pw-post-body-paragraph kz la it lb b lc nn ju le lf no jx lh li np lk ll lm nq lo lp lq nr ls lt lu im bi translated">没有必要总是将正则化应用于线性回归模型。首先，您可以尝试使用<strong class="lb iu"> LogisticRegression() </strong>类，然后查看输出。如果测试RMSE的值较低，而火车RMSE的值较高，则您的回归模型过拟合。然后，您可以尝试应用每种类型的正则化并查看输出。您也可以为超参数<strong class="lb iu"> alpha </strong>和<strong class="lb iu"> l1_ratio </strong>尝试不同的有效值。最后，你会有很多模型。通过查看训练集和测试集上的RMSE，您可以选择一个好的模型。请注意，一个好的模型既不过度拟合也不欠拟合数据。它应该能够在训练数据上表现良好，并且还能够在看不见的数据(测试数据)上进行很好的概括。</p><p id="f746" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">注:</strong>除了应用正则化，还有其他方法来解决过拟合问题。你可以通过阅读我写的以下系列文章来学习它们。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><a href="https://rukshanpramoditha.medium.com/list/addressing-overfitting-868959382d1d"><div class="gh gi nv"><img src="../Images/747771b979c59ccd183309360ec134cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*QJEXVS4T7r54erq5K52jAg.png"/></div></a><p class="ku kv gj gh gi kw kx bd b be z dk translated">“解决过度拟合”的文章列表(作者截图)</p></figure></div><div class="ab cl nw nx hx ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="im in io ip iq"><p id="68fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">今天的帖子到此结束。我的读者可以通过下面的链接注册成为会员，以获得我写的每个故事的全部信息，我将收到你的一部分会员费。</p><div class="od oe gp gr of og"><a href="https://rukshanpramoditha.medium.com/membership" rel="noopener follow" target="_blank"><div class="oh ab fo"><div class="oi ab oj cl cj ok"><h2 class="bd iu gy z fp ol fr fs om fu fw is bi translated">通过我的推荐链接加入Medium</h2><div class="on l"><h3 class="bd b gy z fp ol fr fs om fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="oo l"><p class="bd b dl z fp ol fr fs om fu fw dk translated">rukshanpramoditha.medium.com</p></div></div><div class="op l"><div class="oq l or os ot op ou ks og"/></div></div></a></div><p id="02b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">非常感谢你一直以来的支持！下一个故事再见。祝大家学习愉快！</p><p id="46dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">特别要感谢Unsplash网站上的<strong class="lb iu"> Andre Hunter </strong>、<strong class="lb iu">T3，他为我提供了一张很好的封面图片。</strong></p><p id="dee1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ov ow ep" href="https://medium.com/u/f90a3bb1d400?source=post_page-----fc696f697209--------------------------------" rel="noopener" target="_blank">鲁克山·普拉莫迪塔</a><br/><strong class="lb iu">2021–10–12</strong></p></div></div>    
</body>
</html>