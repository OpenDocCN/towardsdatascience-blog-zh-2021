<html>
<head>
<title>Tips and Tricks for your BERT based applications</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于BERT的应用的技巧和诀窍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tips-and-tricks-for-your-bert-based-applications-359c6b697f8e?source=collection_archive---------5-----------------------#2021-06-23">https://towardsdatascience.com/tips-and-tricks-for-your-bert-based-applications-359c6b697f8e?source=collection_archive---------5-----------------------#2021-06-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7db7" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">等一下，我们将学习使用BERT来完成NLP任务，稍后通过应用一些技巧来增加它的趣味，这些技巧可以显著提高整体性能。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/1e0b8713fa8ec0fc1672d99e7ea90827.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*quULIoJY4Qbbo8-D"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">苏珊·Q·尹在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><blockquote class="kw kx ky"><p id="5dfe" class="kz la lb lc b ld le jr lf lg lh ju li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">本文使用的代码可以在<a class="ae kv" href="http://mohitmayank.com/a_lazy_data_science_guide/natural_language_processing/BERT.html#code" rel="noopener ugc nofollow" target="_blank">这里找到</a>。更多此类文章，请访问我的网站或查看我最新的关于数据科学的书。也可以在<a class="ae kv" href="https://www.linkedin.com/in/imohitmayank/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上联系我。</p></blockquote><h2 id="11c0" class="lw lx iq bd ly lz ma dn mb mc md dp me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">介绍</h2><p id="6559" class="pw-post-body-paragraph kz la iq lc b ld ms jr lf lg mt ju li mf mu ll lm mj mv lp lq mn mw lt lu lv ij bi translated">《变形金刚》席卷了NLP世界。最近的研究表明，变压器和基于它的其他架构(如BERT、RoBERTa、XLNET)已被用于解决大量应用——无论是情感分类、欺诈检测、神经语言翻译还是问答系统。虽然所有这些都是真的，但仍有许多谜团和困惑围绕着它。一些最常见的问题是——(1)我应该只使用<code class="fe mx my mz na b">CLS</code> token还是所有token的输出来表示句子？(2)事先微调模型会增加精度吗？。如果你已经问过自己这些问题，或者刚刚开始使用BERT，那么这篇文章正适合你。请跟随我们，首先介绍BERT模型，然后尝试通过实验解决这些问题。</p><h2 id="208f" class="lw lx iq bd ly lz ma dn mb mc md dp me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">伯特</h2><p id="210d" class="pw-post-body-paragraph kz la iq lc b ld ms jr lf lg mt ju li mf mu ll lm mj mv lp lq mn mw lt lu lv ij bi translated"><strong class="lc ir"> BERT </strong>代表<strong class="lc ir"> B </strong>方向<strong class="lc ir">E</strong>n编码器<strong class="lc ir"> R </strong>代表来自<strong class="lc ir"> T </strong>变压器。基本上，它是变形金刚的一个改型，我们只保留了编码器部分，而丢弃了解码器部分。在发布时，它在11个自然语言处理任务上获得了最先进的结果。BERT背后的主要动机是处理现有语言模型的局限性，这些模型本质上是单向的。这意味着他们只考虑从左到右进行句子级推理的文本。另一方面，BERT允许令牌在自我关注层兼顾双方。这是其高性能的主要原因之一。BERT最吸引人的特点是，对于大量的NLP任务，它超级容易使用。其想法是采用预先训练的BERT模型，然后针对特定任务对其进行微调。预训练模型以无监督的方式在大型语料库上训练，因此该模型从大型文本语料库中学习表征的一般表示。这使得以后为任何其他NLP任务进行微调变得容易，因为该模型预先训练有关于语言、语法和语义表示的大上下文。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nb"><img src="../Images/52d88570f9f54fc046eda4e69b57223c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UgiVrhKuYbjHi5ay.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">不同任务上微调BERT的插图。来源[1]</p></figure><p id="9668" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mf lk ll lm mj lo lp lq mn ls lt lu lv ij bi translated">训练伯特本身就是一个有趣的范例。原始论文提出了两种无监督的训练方法，</p><ol class=""><li id="7623" class="nc nd iq lc b ld le lg lh mf ne mj nf mn ng lv nh ni nj nk bi translated"><strong class="lc ir">屏蔽LM (MLM) </strong>:随机屏蔽一定百分比(15%)的输入标记，然后模型尝试预测这些屏蔽标记。他们为此创造了一个特殊的令牌<code class="fe mx my mz na b">[MASK]</code>。</li><li id="e1b7" class="nc nd iq lc b ld nl lg nm mf nn mj no mn np lv nh ni nj nk bi translated"><strong class="lc ir">下一个句子预测(NSP) </strong>:选择两个句子A和B，使得50%的时间B是跟随A的实际下一个句子(标记为<code class="fe mx my mz na b">IsNext</code>)，而50%的时间B是来自语料库的随机句子(标记为<code class="fe mx my mz na b">NotNext</code>)。该模型被训练来预测第二个句子是否跟随第一个句子。</li></ol><p id="7bf0" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mf lk ll lm mj lo lp lq mn ls lt lu lv ij bi translated">最后，我们应该知道的关于BERT的最后一件事是模型的输入和输出。因此，和通常的NLP模型一样，BERT将句子(当然是在标记化之后)作为输入。因为它只使用转换器的编码器部分，所以它产生两个有趣的输出，</p><ul class=""><li id="a241" class="nc nd iq lc b ld le lg lh mf ne mj nf mn ng lv nq ni nj nk bi translated"><code class="fe mx my mz na b">pooler_output</code>是<code class="fe mx my mz na b">[CLS]</code>特殊令牌的嵌入。在许多情况下，它被认为是完整句子的有效表示。</li><li id="9c4f" class="nc nd iq lc b ld nl lg nm mf nn mj no mn np lv nq ni nj nk bi translated"><code class="fe mx my mz na b">last_hidden_state</code>包含来自最后一个隐藏状态的句子中所有标记的最终嵌入。我们可以应用排列不变的方法(如——最大值、平均值或总和)来将嵌入聚合成单个句子表示。</li></ul><p id="b054" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mf lk ll lm mj lo lp lq mn ls lt lu lv ij bi translated">要了解更多的细节，我强烈推荐阅读最初的BERT论文[1]和变形金刚(注意)论文[2]。</p><h2 id="86a2" class="lw lx iq bd ly lz ma dn mb mc md dp me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">实验设置</h2><p id="8783" class="pw-post-body-paragraph kz la iq lc b ld ms jr lf lg mt ju li mf mu ll lm mj mv lp lq mn mw lt lu lv ij bi translated">理论到此为止，现在让我们从有趣的部分开始，即代码。为了回答介绍部分提出的问题，我们将训练4个基于BERT的模型，并比较准确性，以确定您可以开始使用的最佳品种。对于两党公正的分析，我们将选择一个应用程序——情感识别，并对所有模型使用相同的数据集甚至相同的训练方法。我们的实验列表如下:</p><ul class=""><li id="47a5" class="nc nd iq lc b ld le lg lh mf ne mj nf mn ng lv nq ni nj nk bi translated">实验1:无微调+带Pooler输出的BERT模型</li><li id="bb9e" class="nc nd iq lc b ld nl lg nm mf nn mj no mn np lv nq ni nj nk bi translated">实验2:没有微调+具有最后隐藏输出的BERT模型</li><li id="1042" class="nc nd iq lc b ld nl lg nm mf nn mj no mn np lv nq ni nj nk bi translated">实验3:微调+带Pooler输出的BERT模型</li><li id="a0ac" class="nc nd iq lc b ld nl lg nm mf nn mj no mn np lv nq ni nj nk bi translated">实验4:微调+具有最后隐藏输出的BERT模型</li></ul><p id="ac97" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mf lk ll lm mj lo lp lq mn ls lt lu lv ij bi translated">至于这个任务，在情感识别中，我们给了一个文本，它的情感是要被推断的。通常，这是通过创建句子的潜在表示并将其用于分类来完成的。在我们的例子中，我们将在整个过程中使用BERT模型。</p><p id="764e" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mf lk ll lm mj lo lp lq mn ls lt lu lv ij bi translated">至于数据集，我们使用的是<a class="ae kv" href="https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews" rel="noopener ugc nofollow" target="_blank"> IMDB 50k数据集</a>，因此文本是电影评论，情感要么是正面的，要么是负面的。为了更快地执行，我们随机抽样了10k条评论(每种情感类型约5k条)，并执行了其余的分析。</p><h2 id="5169" class="lw lx iq bd ly lz ma dn mb mc md dp me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">先决条件代码</h2><p id="f770" class="pw-post-body-paragraph kz la iq lc b ld ms jr lf lg mt ju li mf mu ll lm mj mv lp lq mn mw lt lu lv ij bi translated">现在，在我们开始每个实验之前，让我们讨论一下将在所有实验中使用的公共代码。所以让我们从每个项目开始的地方开始——加载数据并将其分成测试和训练。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/cdc4684e94ddcb29d571d451306ad5b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UvXtk5vDFeNVj27IDG6x1A.png"/></div></div></figure><p id="73fe" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mf lk ll lm mj lo lp lq mn ls lt lu lv ij bi translated">接下来，我们将创建一个Pytorch <code class="fe mx my mz na b">Dataset</code>类实例来加载和预处理数据。这包括创建继承了<code class="fe mx my mz na b">Dataset</code>类的<code class="fe mx my mz na b">IMDBDataset</code>类。这包括3个模块:</p><ul class=""><li id="e5b5" class="nc nd iq lc b ld le lg lh mf ne mj nf mn ng lv nq ni nj nk bi translated"><code class="fe mx my mz na b">__init__</code>:我们基本上是在这里存储数据和加载标记器。</li><li id="dc51" class="nc nd iq lc b ld nl lg nm mf nn mj no mn np lv nq ni nj nk bi translated"><code class="fe mx my mz na b">__len__ </code>:返回整个数据集的长度。这是每个时期内步长计算所需要的。</li><li id="de2f" class="nc nd iq lc b ld nl lg nm mf nn mj no mn np lv nq ni nj nk bi translated"><code class="fe mx my mz na b">__getitem__ </code>:我们获取一个数据，对其进行标记，然后将其返回。</li></ul><p id="6996" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mf lk ll lm mj lo lp lq mn ls lt lu lv ij bi translated">现在，如果你注意到，我们的<code class="fe mx my mz na b">IMDBDataset</code>类是非常通用的，因此我们可以用它来准备测试和训练数据集。这正是我们接下来要做的。最后，我们通过参数<code class="fe mx my mz na b">batch_size </code>和<code class="fe mx my mz na b">shuffle</code>将<code class="fe mx my mz na b">Dataset </code>传递给<code class="fe mx my mz na b">Dataloader </code>，以创建一个可用于训练的生成器。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/4b83e03cb87e400342b0e6335a15642f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d9l8fE5cIWLK9QL8-RHZhA.png"/></div></div></figure><p id="dc6a" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mf lk ll lm mj lo lp lq mn ls lt lu lv ij bi translated">现在我们将编码伯特模型的框架。我们将使用Pytorch lightning，它充当Pytorch之上的包装器，并提供许多高效的附加组件，最重要的是，它使编写NN代码变得超级容易:)我们从创建继承<code class="fe mx my mz na b">pl.LightningModule</code>的模型类开始。现在，这个类包含以下子模块，</p><ul class=""><li id="e350" class="nc nd iq lc b ld le lg lh mf ne mj nf mn ng lv nq ni nj nk bi translated"><code class="fe mx my mz na b">__init__</code>:构造器，我们在其中定义层、损失、度量、激活函数并加载BERT模型。请注意，我们使用的是<code class="fe mx my mz na b">bert-base-uncased</code>预训练的BERT模型(~ 1.25亿个参数！)和huggingface加载这个模型的<code class="fe mx my mz na b">transformer </code>包。</li><li id="dd11" class="nc nd iq lc b ld nl lg nm mf nn mj no mn np lv nq ni nj nk bi translated">这是我们定义定制模型逻辑的地方。首先，我们将标记化的输入句子传递给BERT模型。然后，我们希望使用BERT模型的输出(在下一节中会有更多的介绍)，来执行二进制分类。因此，我们最终应用只有2个节点的简单MLP。</li><li id="024a" class="nc nd iq lc b ld nl lg nm mf nn mj no mn np lv nq ni nj nk bi translated"><code class="fe mx my mz na b">training_step</code>:我们在这里定义培训是如何进行的。所以我们批量加载数据，并将其传递给<code class="fe mx my mz na b">forward</code>模块。我们希望在<code class="fe mx my mz na b">forward </code>步骤的输出上计算<code class="fe mx my mz na b">CrossEntropyLoss</code>。我们还定义了一些我们想要计算的额外指标。请注意，这些指标仅用于记录，因为它们仅在<code class="fe mx my mz na b">self.log</code>语句中捕获。培训仅针对<code class="fe mx my mz na b">loss</code>指标进行。</li><li id="fb03" class="nc nd iq lc b ld nl lg nm mf nn mj no mn np lv nq ni nj nk bi translated"><code class="fe mx my mz na b">validation_step</code>:与<code class="fe mx my mz na b">training_step</code>基本相同，只是修改了<code class="fe mx my mz na b">self.log</code>的名称。</li><li id="2087" class="nc nd iq lc b ld nl lg nm mf nn mj no mn np lv nq ni nj nk bi translated"><code class="fe mx my mz na b">configure_optimizers</code>:这里我们定义了我们想要用于模型训练的优化器。我们正在使用<code class="fe mx my mz na b">Adam</code>优化器。另外一点是，我们可能不想训练BERT模型的全部庞大的1.25亿个参数。因此，我们将冻结所有的BERT模型层，只训练我们作为BERT输出头添加的额外分类器层。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/5208ba4690c74820f3b3e4b725b31e88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CCciSYtHRRqf0i648HOmVw.png"/></div></div></figure><p id="40eb" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mf lk ll lm mj lo lp lq mn ls lt lu lv ij bi translated">最后，既然我们已经定义了一切，训练模型就像，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/20425900596d2f364413604dc500b6cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9WexsSOTWg8xwJsNxBM1ag.png"/></div></div></figure><p id="2c1f" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mf lk ll lm mj lo lp lq mn ls lt lu lv ij bi translated">哎呦，搞定了！让我们开始实验吧！</p><h2 id="bdce" class="lw lx iq bd ly lz ma dn mb mc md dp me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">实验1:无微调+带Pooler输出的BERT模型</h2><p id="113b" class="pw-post-body-paragraph kz la iq lc b ld ms jr lf lg mt ju li mf mu ll lm mj mv lp lq mn mw lt lu lv ij bi translated">为了使用BERT模型的pooler输出，我们可以从模型生成和返回的输出中提取相关的值。这可以通过在上面共享的训练代码的第20行使用下面的代码来完成。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/10f401f1e15c4ed7f59dd787a59dfe7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D-jUr9WysezgfN9SBaaDgA.png"/></div></div></figure><p id="0728" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mf lk ll lm mj lo lp lq mn ls lt lu lv ij bi translated">是啊，就这么简单！在训练模型时，我们在验证数据集上获得了<strong class="lc ir"> 64.6%的F1宏</strong>分数和<strong class="lc ir"> 68.4%的准确度</strong>。对于基线(第一个实验)来说还不错，对吗？让我们去下一个。</p><h2 id="166b" class="lw lx iq bd ly lz ma dn mb mc md dp me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">实验2:没有微调+具有最后隐藏输出的BERT模型</h2><p id="e578" class="pw-post-body-paragraph kz la iq lc b ld ms jr lf lg mt ju li mf mu ll lm mj mv lp lq mn mw lt lu lv ij bi translated">BERT模型也返回最后的隐藏状态输出，但有一个问题是它的形状等于<code class="fe mx my mz na b">(batch, no of tokens, token dimension)</code>。我们想要减少它，使得一批中的每个数据点只有一个向量表示，以便形状转换为<code class="fe mx my mz na b">(batch, token dimension)</code>。一个简单的答案是取所有代币的平均值。如果不是因为我们在标记化期间添加的填充(用于处理较小的句子)，这将是一个很好的解决方案，因此简单的平均会给小句的句子嵌入添加很多噪声。因此，我们需要对标记进行某种形式的加权平均，这是通过使用attention_mask来执行的，attention _ mask是一个向量，用于表示哪些标记是填充(0)，哪些不是(1)。这可以通过在第20行添加以下脚本来完成，</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/7dcf5ea7566ee8b6bfd8a03041252e62.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tfkaw9tD4Fq8pgVS_YUO8w.png"/></div></div></figure><p id="2c95" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mf lk ll lm mj lo lp lq mn ls lt lu lv ij bi translated">在训练模型时，我们在验证数据集上获得了<strong class="lc ir"> 86.7%的F1宏</strong>分数和<strong class="lc ir"> 87.5%的准确度</strong>。哇，仅仅改变一行代码，准确性就提高了20%!！</p><h2 id="0024" class="lw lx iq bd ly lz ma dn mb mc md dp me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">先决条件(2):微调BERT</h2><p id="3a85" class="pw-post-body-paragraph kz la iq lc b ld ms jr lf lg mt ju li mf mu ll lm mj mv lp lq mn mw lt lu lv ij bi translated">现在剩下的实验大部分和以前一样，但是有一个主要的变化——我们将使用微调的模型。这意味着，我们将使用我们拥有的数据集，即10k电影评论数据，并使用它来使用原始的无监督训练方法之一调整基本BERT模型。然后，我们将使用这个修改后的模型进行剩余的一组实验。我强烈推荐观看James Briggs的视频，他精彩地解释了微调过程。代码可以在这里找到<a class="ae kv" href="http://mohitmayank.com/a_lazy_data_science_guide/natural_language_processing/BERT.html#fine-tuning-the-bert-model" rel="noopener ugc nofollow" target="_blank">。</a></p><p id="f406" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mf lk ll lm mj lo lp lq mn ls lt lu lv ij bi translated">但是为了简单地讨论代码逻辑，它由以下部分组成，</p><ul class=""><li id="060b" class="nc nd iq lc b ld le lg lh mf ne mj nf mn ng lv nq ni nj nk bi translated"><strong class="lc ir">加载模型</strong>:我们加载最基本的BERT模型，即<code class="fe mx my mz na b">bert-base-uncased</code>，我们将对该模型进行微调。</li><li id="9c32" class="nc nd iq lc b ld nl lg nm mf nn mj no mn np lv nq ni nj nk bi translated"><strong class="lc ir">准备数据</strong>:我们将加载10k数据集并解析文本，以所需的格式准备它。为此，我们将首先对其进行标记化，然后随机屏蔽一定比例的标记(通过用特殊的<code class="fe mx my mz na b">[MASK]</code>标记替换其标记值)。在这种情况下，我用30%的口罩进行了测试。平时也可以玩玩这个数字，效果更好！(我会建议把搜索限制在10%到40%。如果你做了，让我知道！).接下来是通常的<code class="fe mx my mz na b">dataset </code>和<code class="fe mx my mz na b">dataloader </code>部分。</li><li id="be3f" class="nc nd iq lc b ld nl lg nm mf nn mj no mn np lv nq ni nj nk bi translated"><strong class="lc ir">训练</strong>:最后，我们对模型进行一定次数(这里是20次)的训练，并保存微调后的模型。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/c5504da8ff520aea3d777f787363f11a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qN1e00SACyyzqL3e5_WvHw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">微调BERT模型的代码</p></figure><h2 id="aed4" class="lw lx iq bd ly lz ma dn mb mc md dp me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">实验3:带有Pooler输出的微调+ BERT模型，实验4:带有最后隐藏输出的微调+ BERT模型</h2><p id="2eb5" class="pw-post-body-paragraph kz la iq lc b ld ms jr lf lg mt ju li mf mu ll lm mj mv lp lq mn mw lt lu lv ij bi translated">最后两个实验的代码保持与之前相同，即实验1和2。唯一的区别是，现在我们将使用微调模型，而不是使用基本的BERT模型。这可以通过将文件夹路径传递给前面实验代码中的<code class="fe mx my mz na b">BertModel.from_pretrained(model_path)</code>脚本来实现。</p><p id="e814" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mf lk ll lm mj lo lp lq mn ls lt lu lv ij bi translated">对于实验3，我们得到了<strong class="lc ir"> 87.5%的F1分数</strong>和<strong class="lc ir"> 88.1%的准确率</strong>。另一方面，对于实验4，我们得到了<strong class="lc ir"> 79.8%的F1分数</strong>和<strong class="lc ir"> 81.3%的准确率</strong>。</p><h2 id="32cc" class="lw lx iq bd ly lz ma dn mb mc md dp me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">结果</h2><p id="9cab" class="pw-post-body-paragraph kz la iq lc b ld ms jr lf lg mt ju li mf mu ll lm mj mv lp lq mn mw lt lu lv ij bi translated">综合每个实验的结果，我们有</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/41504ad9083ded25a9329c74a2198291.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q1Vwu5XD0g94144pHGktOw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">BERT预训练(实验1和2)和微调模型(实验3和4)的综合实验结果</p></figure><p id="7709" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mf lk ll lm mj lo lp lq mn ls lt lu lv ij bi translated">现在让我们试着回答我们在引言部分提出的问题，</p><ol class=""><li id="a4f8" class="nc nd iq lc b ld le lg lh mf ne mj nf mn ng lv nh ni nj nk bi translated"><strong class="lc ir">句子表示应该只使用</strong> <code class="fe mx my mz na b"><strong class="lc ir">CLS </strong></code> <strong class="lc ir"> token还是所有token的输出？嗯，这要看情况。从实验来看，似乎如果你正在微调模型，使用pooler输出会更好。但是如果没有微调的话，最后的隐藏状态输出要好得多。就我个人而言，我更喜欢最后一个隐藏状态输出，因为它提供了比较结果，而无需任何额外的昂贵的计算微调。</strong></li><li id="fd89" class="nc nd iq lc b ld nl lg nm mf nn mj no mn np lv nh ni nj nk bi translated"><strong class="lc ir">预先微调模型会增加准确性吗？一个明确的肯定！实验3和实验4报告了比实验1更高的分数。另一方面，Exp 2的例子非常有趣，暗示了在进行微调之前尝试所有的东西。也就是说，如果你在寻找最好的结果(即使只有1%的差距)，微调是最好的方法。因此，如果你有时间和资源(讽刺的是，通常情况下不是这样)，去微调！</strong></li></ol><h2 id="7b06" class="lw lx iq bd ly lz ma dn mb mc md dp me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">参考</h2><ol class=""><li id="85e3" class="nc nd iq lc b ld ms lg mt mf ny mj nz mn oa lv nh ni nj nk bi translated">雅各布·德夫林、张明蔚、肯顿·李和克里斯蒂娜·图塔诺娃。Bert:用于语言理解的深度双向转换器的预训练。2019.<a class="ae kv" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> arXiv:1810.04805 </a>。</li><li id="838d" class="nc nd iq lc b ld nl lg nm mf nn mj no mn np lv nh ni nj nk bi translated">Ashish Vaswani、Noam Shazeer、Niki Parmar、Jakob Uszkoreit、Llion Jones、Aidan N. Gomez、Lukasz Kaiser和Illia Polosukhin。你需要的只是关注。2017.<a class="ae kv" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> arXiv:1706.03762 </a>。</li><li id="fdb6" class="nc nd iq lc b ld nl lg nm mf nn mj no mn np lv nh ni nj nk bi translated">詹姆斯·布里格斯的<a class="ae kv" href="https://www.youtube.com/watch?v=R6hcxMMOrPE" rel="noopener ugc nofollow" target="_blank">视频</a>使用MLM微调伯特模型</li></ol></div><div class="ab cl ob oc hu od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="ij ik il im in"><p id="10f9" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mf lk ll lm mj lo lp lq mn ls lt lu lv ij bi translated">干杯。</p></div></div>    
</body>
</html>