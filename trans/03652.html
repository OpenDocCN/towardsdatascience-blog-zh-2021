<html>
<head>
<title>Multi-Class Classification With Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">变压器的多级分类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multi-class-classification-with-transformers-6cf7b59a033a?source=collection_archive---------6-----------------------#2021-03-25">https://towardsdatascience.com/multi-class-classification-with-transformers-6cf7b59a033a?source=collection_archive---------6-----------------------#2021-03-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="6f38" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用BERT进行预处理、训练和预测</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ed6ec222a162e7e1c2680b8b65f4f8f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WB3nclip0fsvxmoc9SIMAQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="d02e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi lu translated"><span class="l lv lw lx bm ly lz ma mb mc di"> T </span>转换器被描述为深度学习的第四大支柱[1]，与卷积和递归神经网络类似。</p><p id="c856" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然而，从自然语言处理的角度来看——变形金刚远不止这些。自2017年推出以来，它们已经主导了大多数NLP基准，并继续每天给人留下深刻印象。</p><p id="80c1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">问题是，变形金刚太酷了。有了像HuggingFace的变形金刚这样的库，用它们来构建不可思议的解决方案已经变得<em class="md">太容易了。</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="me mf l"/></div></figure><p id="ae86" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">那么，有什么不爱呢？令人难以置信的性能与极致的易用性。</p><p id="22bf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我们将从头到尾使用transformers构建一个多类分类模型。</p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><h1 id="90b1" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">预处理</h1><p id="9790" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">我们将使用烂番茄电影评论数据集。这里，我们给出了从0到4的五个输出标签。每个标签代表一个情感类别。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/81e4af8235f7d77c79b8887a4c29564f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jpJTK51iqLFXAVMq7WFsZQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">烂番茄数据集标记和衡量从消极到积极的情绪。</p></figure><p id="0713" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里我们将简单介绍几个预处理步骤(完整的笔记本链接在本节末尾)。</p><p id="1ada" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">总的来说，我们有六个预处理步骤:</p><ul class=""><li id="ec7f" class="nl nm it la b lb lc le lf lh nn ll no lp np lt nq nr ns nt bi translated">使用<strong class="la iu">语句Id </strong>删除“段”重复。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu mf l"/></div></figure><ul class=""><li id="c0d4" class="nl nm it la b lb lc le lf lh nn ll no lp np lt nq nr ns nt bi translated">初始化空数组以存储标记化的文本。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu mf l"/></div></figure><ul class=""><li id="ecd6" class="nl nm it la b lb lc le lf lh nn ll no lp np lt nq nr ns nt bi translated">一个热点编码的情绪。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu mf l"/></div></figure><ul class=""><li id="dd88" class="nl nm it la b lb lc le lf lh nn ll no lp np lt nq nr ns nt bi translated">使用我们的输入构建一个<code class="fe nv nw nx ny b">tf.data.Dataset</code>对象，并标记张量。然后转换成我们模型的正确格式。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu mf l"/></div></figure><ul class=""><li id="e92d" class="nl nm it la b lb lc le lf lh nn ll no lp np lt nq nr ns nt bi translated">批处理和打乱我们的数据。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu mf l"/></div></figure><ul class=""><li id="d233" class="nl nm it la b lb lc le lf lh nn ll no lp np lt nq nr ns nt bi translated">90–10分为培训和验证数据。</li></ul><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu mf l"/></div></figure><p id="6c7e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所有这些步骤结合起来创建了<a class="ae nz" href="https://github.com/jamescalam/transformers/blob/main/course/project_build_tf_sentiment_model/00_preprocessing.ipynb" rel="noopener ugc nofollow" target="_blank">这个预处理脚本</a>和<a class="ae nz" href="https://github.com/jamescalam/transformers/blob/main/course/project_build_tf_sentiment_model/01_input_pipeline.ipynb" rel="noopener ugc nofollow" target="_blank">这个输入管道脚本</a>。</p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><h1 id="2a88" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">建造和训练</h1><p id="a66f" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">现在我们有了正确格式的数据，我们可以设置模型并开始训练。</p><h2 id="875f" class="oa mo it bd mp ob oc dn mt od oe dp mx lh of og mz ll oh oi nb lp oj ok nd ol bi translated">构建模型</h2><p id="a226" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">首先，我们需要初始化预训练的BERT模型，如下所示:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu mf l"/></div></figure><p id="268b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将使用典型的<code class="fe nv nw nx ny b">tf.keras</code>层围绕BERT构建一个<em class="md">框架</em>。这个框架有几个部分:</p><ul class=""><li id="c4bb" class="nl nm it la b lb lc le lf lh nn ll no lp np lt nq nr ns nt bi translated"><strong class="la iu">两个</strong>输入层(一个用于输入id，另一个用于注意屏蔽)。</li><li id="dd21" class="nl nm it la b lb om le on lh oo ll op lp oq lt nq nr ns nt bi translated">用于减少过度拟合的可能性并提高泛化能力的后BERT丢弃层。</li><li id="a461" class="nl nm it la b lb om le on lh oo ll op lp oq lt nq nr ns nt bi translated">Max pooling layer将BERT输出的3D张量转换为2D张量。</li><li id="7d82" class="nl nm it la b lb om le on lh oo ll op lp oq lt nq nr ns nt bi translated">最终输出激活使用softmax，这样我们就可以得到我们的获奖类预测。</li></ul><p id="6d3d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所有这些都写成了:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu mf l"/></div></figure><p id="ffc7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然后，我们初始化新的<code class="fe nv nw nx ny b">model</code>，并将输入/输出层提供给初始化函数。一旦完成，我们可以冻结BERT层来加速训练(以可能的性能下降为代价)。这里，我们将注释掉图层冻结线。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu mf l"/></div></figure><p id="05e9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们冻结BERT参数的原因是它们有很多，更新这些权重会显著增加训练时间。</p><p id="7470" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因为BERT是预先训练过的，进一步训练的潜在性能提高是很小的，因此不值得额外的训练时间。</p><h2 id="9a53" class="oa mo it bd mp ob oc dn mt od oe dp mx lh of og mz ll oh oi nb lp oj ok nd ol bi translated">培养</h2><p id="50f5" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">现在我们已经建立了模型架构，我们可以初始化我们的训练参数并开始训练。</p><p id="3c8d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，我们用合适的训练参数编译模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu mf l"/></div></figure><p id="31a3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里，我们使用一个带有加权衰减的Adam优化器。对于我们的损失和准确性，我们分别使用分类交叉熵和分类准确性。</p><p id="e244" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们在这里使用分类度量是因为我们的一次性编码多类输出标签是分类的。</p><p id="1199" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们从<code class="fe nv nw nx ny b">model.fit</code>开始训练:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu mf l"/></div></figure><p id="a032" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">仅仅几个时期后，我们应该得到接近75%的准确度。最后，我们可以使用以下方法保存我们的模型:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu mf l"/></div></figure><p id="7823" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">(<a class="ae nz" href="https://github.com/jamescalam/transformers/blob/main/course/project_build_tf_sentiment_model/02_build_and_train.ipynb" rel="noopener ugc nofollow" target="_blank">全训脚本</a>)。</p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><h1 id="b4d1" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">做预测</h1><p id="2bcd" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">我们已经预处理了数据，构建、训练并保存了模型。现在，我们可以开始用它做一些预测。</p><p id="adf0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您在单个笔记本或脚本中跟随，您不需要这样做——但是正如我向您展示了如何保存模型，我们也将加载它:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu mf l"/></div></figure><h2 id="67d6" class="oa mo it bd mp ob oc dn mt od oe dp mx lh of og mz ll oh oi nb lp oj ok nd ol bi translated">准备输入数据</h2><p id="dbfd" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">我们希望从像<code class="fe nv nw nx ny b">"hello world"</code>这样简单的字符串中做出预测。为此，我们需要添加一些处理步骤，将一个简单的字符串转换成正确的格式，以便我们的模型使用和预测。</p><p id="d32e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为此，我们将使用一个名为<code class="fe nv nw nx ny b">prep_data</code>的函数。该函数将获取一个字符串，使用一个<code class="fe nv nw nx ny b">BertTokenizer</code>对其进行标记，并将这些标记张量作为一个字典返回，该字典包含<code class="fe nv nw nx ny b">'input_ids'</code>和<code class="fe nv nw nx ny b">'attention_mask'</code>键-值对:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu mf l"/></div></figure><h2 id="29c0" class="oa mo it bd mp ob oc dn mt od oe dp mx lh of og mz ll oh oi nb lp oj ok nd ol bi translated">预言；预测；预告</h2><p id="759a" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">一旦我们的输入字符串被格式化为正确的dictionary-tensors格式，我们就可以将它们传递给我们的<code class="fe nv nw nx ny b">model.predict</code>方法——该方法将返回每个输出标签的概率的Numpy数组:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu mf l"/></div></figure><p id="844d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">(<a class="ae nz" href="https://github.com/jamescalam/transformers/blob/main/course/project_build_tf_sentiment_model/03_load_and_predict.ipynb" rel="noopener ugc nofollow" target="_blank">满载和预测脚本</a>)。</p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><p id="761d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这就是我们完整的多类分类变压器模型演练，从开始到结束！</p><p id="a425" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我希望你喜欢这篇文章。如果您有任何问题或建议，请通过<a class="ae nz" href="https://twitter.com/jamescalam" rel="noopener ugc nofollow" target="_blank"> Twitter </a>或在下面的评论中告诉我。如果你对更多类似的内容感兴趣，我也会在<a class="ae nz" href="https://www.youtube.com/c/jamesbriggs" rel="noopener ugc nofollow" target="_blank"> YouTube </a>上发布。</p><p id="6811" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢阅读！</p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><h1 id="9a33" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">参考</h1><p id="234b" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">[1] J. Wang，<a class="ae nz" href="https://ark-invest.com/articles/analyst-research/transformers-comprise-the-fourth-pillar-of-deep-learning/" rel="noopener ugc nofollow" target="_blank">变形金刚构成深度学习的第四支柱</a> (2021)，方舟研究</p><p id="d783" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae nz" href="https://bit.ly/nlp-transformers" rel="noopener ugc nofollow" target="_blank">🤖带变压器的NLP课程</a></p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><p id="bf70" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="md">*所有图片均由作者提供，除非另有说明</em></p></div></div>    
</body>
</html>