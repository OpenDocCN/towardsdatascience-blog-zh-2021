<html>
<head>
<title>Skip-Gram Neural Network for Graphs</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">图的跳格神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/skip-gram-neural-network-for-graphs-83b8f308bf87?source=collection_archive---------21-----------------------#2021-07-23">https://towardsdatascience.com/skip-gram-neural-network-for-graphs-83b8f308bf87?source=collection_archive---------21-----------------------#2021-07-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="0f94" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">本文将深入探讨节点嵌入的更多细节。如果您缺乏对节点嵌入的直觉和理解，可以查看以前的这篇文章，其中讨论了节点嵌入的直觉。但是如果你准备好了。我们可以开始了。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi kl"><img src="../Images/9c2af7b1920f176320cd90fed9c97cc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ObXGGEHkqe6ihfO6"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">布拉登·科拉姆在<a class="ae lb" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><p id="7789" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在关于节点嵌入的<a class="ae lb" rel="noopener" target="_blank" href="/node-embeddings-for-beginners-554ab1625d98">一级解释</a>中，我解释了为什么我们需要嵌入，这样我们就有了向量形式的图形数据。而且，我说…</p><blockquote class="lc ld le"><p id="b9b4" class="jn jo lf jp b jq jr js jt ju jv jw jx lg jz ka kb lh kd ke kf li kh ki kj kk ij bi translated"><em class="iq">嵌入应该捕获图形拓扑、节点之间的关系和进一步的信息。我们如何理解这一点，以便有一个明确的程序？</em></p></blockquote><p id="1143" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在上一篇文章中，我建议…</p><blockquote class="lc ld le"><p id="370e" class="jn jo lf jp b jq jr js jt ju jv jw jx lg jz ka kb lh kd ke kf li kh ki kj kk ij bi translated"><em class="iq">解决这个问题的一个可能的捷径是尝试形成嵌入，使得两个节点的节点嵌入在某种意义上是相似的，如果它们碰巧在网络中具有某种相似性的话。</em></p></blockquote><p id="83c2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">好的，为了更正式地建立这一点，让我们回顾一下我们已经谈到的内容。我们说我们有或者想要一些嵌入，但是我们仅有的是来自网络的节点。因此，我们需要一个变换或函数。嵌入环境中的函数称为编码器。</p><p id="0c58" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">设<strong class="jp ir"> <em class="lf"> f </em> </strong>为从节点<strong class="jp ir"> <em class="lf"> v </em> </strong>到嵌入<strong class="jp ir"> <em class="lf"> z </em> </strong>的映射编码器，其中<strong class="jp ir"> <em class="lf"> z </em> </strong>为<strong class="jp ir"><em class="lf"/></strong>维嵌入中的向量，<strong class="jp ir"> <em class="lf"> v </em> </strong>为图<strong class="jp ir"> <em class="lf">中的顶点</em></strong></p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi lj"><img src="../Images/9351a62d182916429eda1be378437943.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*K2m3-_tbEl9iQH5XQZhEKQ.png"/></div></figure><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi lk"><img src="../Images/9e6987d742c78327f62dc0a6e1b2bbb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aM5WDVKyOdjUoSawERNOww.png"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">作者通过编码器从节点到嵌入空间的映射</p></figure><p id="796e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">从我的经验来看，这个函数<strong class="jp ir"> <em class="lf"> f </em> </strong>最初是很难掌握的。尤其是当deepwalk或node2vec的随机行走方法是您听到的第一个嵌入策略时。在这两种使用随机行走的方法中，<strong class="jp ir"> <em class="lf"> f </em> </strong>只是查找嵌入矩阵中的一行或一列，因为每个节点<strong class="jp ir"> <em class="lf"> v </em> </strong>由一行或一列表示。</p><h1 id="401d" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">下一个——相似性之谜</h1><p id="d947" class="pw-post-body-paragraph jn jo iq jp b jq mj js jt ju mk jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">在前一篇文章和本文的介绍中，我提到了应该捕捉的相似性。相似性帮助我们建立嵌入，使得嵌入反映真实世界网络的信息。让我们更正式地总结一下。</p><p id="db94" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">解码器是从两个节点<em class="lf">enc(</em><strong class="jp ir"><em class="lf">v</em></strong><em class="lf">)</em>和<em class="lf">enc(</em><strong class="jp ir"><em class="lf">v</em></strong><em class="lf">)</em>的嵌入映射到相似性得分的函数:</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi mo"><img src="../Images/0a25701fa435a3219b16ce6ec8d5aca4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lLdDCHCxJZ9CJM20M5eZAw.png"/></div></div></figure><h1 id="b67e" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">解码器</h1><p id="79d7" class="pw-post-body-paragraph jn jo iq jp b jq mj js jt ju mk jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">一种可能的解码器是表示余弦距离的点积。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/d1bde37ad233d20ff501eb5d10815a82.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/format:webp/1*Z1p1_cMjrZgEM1VIdCn6Cw.png"/></div></figure><p id="0ab2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果两个向量指向相同的方向，则点积为1；如果两个向量相互垂直，则点积为0；如果两个向量指向相反的方向，则点积为-1。</p><p id="fe09" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而，其他距离度量/解码器也允许计算相似性值。</p><p id="96f8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">当我第一次看到Jure Leskovec关于节点嵌入的演讲时，我根本没有意识到这是node2vec或deepwalk的一部分。据我所知，解码器在这两种方法的算法中都是非常隐含的，但后面会有更多介绍。并且解码器仍然允许评估两个节点表示之间的嵌入空间中的相似性值。</p><h1 id="ab01" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">网络中节点的相似性</h1><p id="0d9b" class="pw-post-body-paragraph jn jo iq jp b jq mj js jt ju mk jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">定义相似性有很多种可能。下面给出了一个可能的选择。</p><p id="549d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">相似性可能意味着两个节点…</p><ul class=""><li id="afb8" class="mq mr iq jp b jq jr ju jv jy ms kc mt kg mu kk mv mw mx my bi translated">是有联系的</li><li id="d832" class="mq mr iq jp b jq mz ju na jy nb kc nc kg nd kk mv mw mx my bi translated">共享邻居</li><li id="0f64" class="mq mr iq jp b jq mz ju na jy nb kc nc kg nd kk mv mw mx my bi translated">具有相似的“结构角色”</li><li id="9d76" class="mq mr iq jp b jq mz ju na jy nb kc nc kg nd kk mv mw mx my bi translated">在随机漫步中同时出现</li></ul><p id="5175" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们多关注一下随机漫步</p><h1 id="1b01" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">为什么是随机漫步？</h1><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi ne"><img src="../Images/3f6d13d613ca35eaf50aa845e3100a01.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8WXQw8pMmIt2VsH4"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">Andrew Gook 在<a class="ae lb" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="0ecb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是为什么随机漫步和同现有助于描述相似性呢？</p><p id="5369" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这样看，如果一个随机游走者以很高的概率从节点<strong class="jp ir"> <em class="lf"> u </em> </strong>移动到节点<strong class="jp ir"> <em class="lf"> v </em> </strong>，那么<strong class="jp ir"> <em class="lf"> u </em> </strong>和<strong class="jp ir"> <em class="lf"> v </em> </strong>在某种意义上是相似的，即它们属于网络的相同邻域或结构。</p><p id="ba36" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为什么随机漫步是个好主意？很有效率。你可以很容易地将一次随机漫步与许多步行者并行。此外，您不需要考虑可能包含数百万个节点的网络的所有可能对，但它很好地描述了邻近性。</p><h1 id="3b4d" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">但更重要的是..</h1><p id="caa2" class="pw-post-body-paragraph jn jo iq jp b jq mj js jt ju mk jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">谷歌研究人员(Mikolov等人)引入word2vec的自然语言处理工作启发了纽约石溪大学计算机科学系的研究小组尝试类似的东西。句子可以被来自随机漫步的节点序列模仿，他们提出了一种叫做deepwalk的算法。</p><blockquote class="nf"><p id="aea2" class="ng nh iq bd ni nj nk nl nm nn no kk dk translated"><em class="np">节点序列模仿跳格模型所需的句子</em></p></blockquote><p id="a221" class="pw-post-body-paragraph jn jo iq jp b jq nq js jt ju nr jw jx jy ns ka kb kc nt ke kf kg nu ki kj kk ij bi translated">也许你仍然有点怀疑为什么这应该工作。deepwalk的作者们的动机如下:</p><blockquote class="lc ld le"><p id="d198" class="jn jo lf jp b jq jr js jt ju jv jw jx lg jz ka kb lh kd ke kf li kh ki kj kk ij bi translated"><em class="iq">如果连通图的度分布遵循幂律(无标度)，我们观察到顶点在短随机行走中出现的频率也将遵循幂律分布。自然语言中的词频遵循类似的分布，语言建模技术解释了这种分布行为。为了强调这种相似性，我们在图2中展示了两种不同的幂律分布。— Perozzi等人，2014年，深度行走</em></p></blockquote><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi nv"><img src="../Images/8ed34e803cba0ac9b4c454cd397a171c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GJLkdKKsVriqIzfbGL4eAA.png"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">Deepwalk，Perrozzi等人，2014年</p></figure><p id="288c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Deep walk反过来启发了斯坦福大学的研究人员来改进这种算法，他们的结果被称为node2vec，这是他们在2014年发表deepwalk两年后发表的。</p><h1 id="1b51" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">word2vec是怎么导致deepwalk的？</h1><p id="27ab" class="pw-post-body-paragraph jn jo iq jp b jq mj js jt ju mk jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">谷歌的团队基于一种叫做skip-gram的模型成功实现了单词嵌入。他们看一篇文章，提取一个单词的上下文，这个单词基本上是句子。并且跳格模型应该基于每个单词来预测该单词的上下文。例如，哪些单词很可能出现在house旁边？门、地板、厨房等。</p><p id="08ca" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们建立了这个跳格模型，随后我们看到它如何转化为deepwalk。</p><p id="f5f8" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">首先我们需要整篇文章中的一个句子。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi nw"><img src="../Images/e8c7ba3b827a1cc86bb3b3a0fc9036cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h5ZnlW127oupQ3UDezPMSA.png"/></div></div></figure><p id="c826" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来，应用一个窗口并在文本上移动，以便创建训练样本。在下图中，我选择了大小为二的窗口。这意味着我们感兴趣的单词前面有两个单词，后面有两个单词(突出显示为深蓝色)。作为窗口一部分的单词的邻域是淡蓝色的。一个词的邻域中的词的集合也称为语料库。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi nx"><img src="../Images/9b521d90dc7f06a3c68f308a09d5565b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HI61-3pPzz8DxHN7SHw5Cg.jpeg"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">自然语言处理的跳格模型的训练样本，作者</p></figure><p id="bc25" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">现在我们有了训练样本，我们需要一个模型。让我们假设我们训练这一对(“例子”，“傻”)。</p><p id="dc61" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们的输入是一个独热编码向量，其长度等于词汇<strong class="jp ir"> <em class="lf"> V </em> </strong>的长度。值为1的条目位于我们对单词“example”进行编码的位置。然后我们将这个向量乘以一个输入权重矩阵。但是基本上，将一个矩阵乘以一个独热编码的向量会导致选择一个列或行向量。矩阵<strong class="jp ir"> <em class="lf">的这一列或一行W_{in} </em> </strong>就是我们要找的嵌入向量(用蓝色竖线表示)。</p><p id="0ca9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个向量，我们现在称之为<strong class="jp ir"> <em class="lf"> h </em> </strong>随后乘以第二个权重矩阵<strong class="jp ir"> <em class="lf"> W_{out} </em> </strong>。产品被传递到非线性激活函数中。在这种情况下，它是softmax，因为我们希望对输出单词的概率分布进行建模。在我们的例子中，输入单词是“example ”,我们计算词汇表中每个单词在“example”语料库中找到它的概率。这一过程受到监督。这意味着我们希望输入“示例”给出“愚蠢”的概率为1，因为这是我们的数据所建议的。我们在“例”的邻域发现了“傻”。</p><h1 id="aab1" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">跳跃图中的表征学习</h1><p id="0990" class="pw-post-body-paragraph jn jo iq jp b jq mj js jt ju mk jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">如果嵌入向量<strong class="jp ir"> <em class="lf"> h </em> </strong>和权重矩阵<strong class="jp ir"> <em class="lf"> W_{out} </em> </strong>不向softmax提供导致1的输入，我们需要根据它们的贡献来改变这两者。这种比例变化是通过随机梯度下降实现的。</p><blockquote class="lc ld le"><p id="bae7" class="jn jo lf jp b jq jr js jt ju jv jw jx lg jz ka kb lh kd ke kf li kh ki kj kk ij bi translated">单词“silly”的预测和真实概率= 1之间的差异导致单词“example”的嵌入向量的变化。使得“例子”更好地预测“愚蠢”。</p></blockquote><h1 id="181c" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">将这些知识转化为图表..</h1><p id="cb61" class="pw-post-body-paragraph jn jo iq jp b jq mj js jt ju mk jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">回到我们的主要兴趣图嵌入，我们可以得出某些类比。我已经指出，纽约的研究人员将一个语料库或一个句子视为随机游走序列或邻域。表格的其余部分补充了其他类比。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi ny"><img src="../Images/b7c4ee8220e150d497a94c97429fa101.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yD-2HUHsYBA0XUtvfSrIWw.jpeg"/></div></div></figure><h1 id="783b" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">行动中的漫步者</h1><p id="9ceb" class="pw-post-body-paragraph jn jo iq jp b jq mj js jt ju mk jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">这个动画展示了一个随机漫步人如何探索网络。结果，存储了一系列节点。这个序列模仿我们的句子，能够使用跳格模型。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi nz"><img src="../Images/4e993bbcdb7b15ce39044c35d89187ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*UHoDnWcf6qvrlM95McQndA.gif"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">随机漫步机</p></figure><h1 id="5805" class="ll lm iq bd ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi bi translated">Deepwalk中的方法</h1><p id="df1b" class="pw-post-body-paragraph jn jo iq jp b jq mj js jt ju mk jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">在deepwalk出版物中，他们完全遵循了这一类比，并将Google研究人员在单词嵌入领域的工作用于图形嵌入。真好！我喜欢看到不同的团队互相激励。所以，不要羞于看到其他领域的研究人员正在试图解决的问题。也许你从他们那里获得了灵感，从而实现了突破。</p><p id="6103" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们一步一步地检查算法。在嵌入的随机初始化和随机选择节点之后。我们将进行一次随机漫步。随后，我们将使用一个窗口在序列上滑动。在我们的例子中，窗口大小是2。这会生成成对的节点。</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi ny"><img src="../Images/83c40e98eb5e19e7adfb1fcc770cd5e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sem9SqNWeAeek-95HZ26Rw.jpeg"/></div></div></figure><p id="1543" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下一步，生成的节点对将被传递到skip-gram模型中。该图显示了训练一对(<em class="lf"> v_5 </em>，<em class="lf"> v_8 </em>)的示例</p><figure class="km kn ko kp gt kq gh gi paragraph-image"><div role="button" tabindex="0" class="kr ks di kt bf ku"><div class="gh gi oa"><img src="../Images/2918903985b49d384f52a6815f543edb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c8NRwQ_T3tefv35Fpx9Dfw.jpeg"/></div></div><p class="kx ky gj gh gi kz la bd b be z dk translated">按作者列出的网络跳转程序</p></figure><p id="f7f2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在我们的例子中，嵌入<strong class="jp ir"> <em class="lf"> h </em> </strong>连同权重矩阵<strong class="jp ir"> <em class="lf"> W_{out} </em> </strong>没有正确地用于预测节点<em class="lf"> v_8 </em>在节点<em class="lf"> v_5 </em>的邻域中。这就是为什么我们需要用梯度下降来适应它们，以获得更精确的值。</p><p id="8aaf" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">在许多随机行走和梯度下降迭代之后，学习过程将停止。矩阵<strong class="jp ir"> <em class="lf"> W_{in} </em> </strong>包含每个节点<em class="lf"> v_i </em>的向量<em class="lf"> h_i </em>，表示嵌入。这种嵌入可以用于下游的机器学习任务。</p></div><div class="ab cl ob oc hu od" role="separator"><span class="oe bw bk of og oh"/><span class="oe bw bk of og oh"/><span class="oe bw bk of og"/></div><div class="ij ik il im in"><h1 id="cca4" class="ll lm iq bd ln lo oi lq lr ls oj lu lv lw ok ly lz ma ol mc md me om mg mh mi bi translated">摘要</h1><p id="10f4" class="pw-post-body-paragraph jn jo iq jp b jq mj js jt ju mk jw jx jy ml ka kb kc mm ke kf kg mn ki kj kk ij bi translated">简要回顾前一篇文章，解释为什么我们依靠节点相似性来形成节点嵌入。我们看到了随机漫步是如何提供一个解决方案，用计算效率来表达节点相似性的。这是将自然语言处理中的知识应用于网络的结果。我们修改了word2vec的工作方式以及它与deepwalk的关系。</p><p id="f56e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">[1] Perozzi等人，deepwalk，2014年</p><h2 id="05c1" class="on lm iq bd ln oo op dn lr oq or dp lv jy os ot lz kc ou ov md kg ow ox mh oy bi translated">相关著作</h2><div class="oz pa gp gr pb pc"><a href="https://yvesboutellier.medium.com/node-embeddings-for-beginners-554ab1625d98" rel="noopener follow" target="_blank"><div class="pd ab fo"><div class="pe ab pf cl cj pg"><h2 class="bd ir gy z fp ph fr fs pi fu fw ip bi translated">初学者的节点嵌入</h2><div class="pj l"><h3 class="bd b gy z fp ph fr fs pi fu fw dk translated">节点嵌入在开始时可能很困难。这篇文章为你提供了直觉，这样你就可以阅读更多…</h3></div><div class="pk l"><p class="bd b dl z fp ph fr fs pi fu fw dk translated">yvesboutellier.medium.com</p></div></div><div class="pl l"><div class="pm l pn po pp pl pq kv pc"/></div></div></a></div><div class="oz pa gp gr pb pc"><a rel="noopener follow" target="_blank" href="/graph-coloring-with-networkx-88c45f09b8f4"><div class="pd ab fo"><div class="pe ab pf cl cj pg"><h2 class="bd ir gy z fp ph fr fs pi fu fw ip bi translated">网络图着色</h2><div class="pj l"><h3 class="bd b gy z fp ph fr fs pi fu fw dk translated">图着色问题的解决方案在概念上很简单，但在应用上却很强大。本教程展示了…</h3></div><div class="pk l"><p class="bd b dl z fp ph fr fs pi fu fw dk translated">towardsdatascience.com</p></div></div><div class="pl l"><div class="pr l pn po pp pl pq kv pc"/></div></div></a></div></div></div>    
</body>
</html>