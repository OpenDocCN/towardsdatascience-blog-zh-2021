<html>
<head>
<title>How to Mitigate Overfitting with Feature Selection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何通过特征选择减轻过度拟合</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-mitigate-overfitting-with-feature-selection-164897c0c3db?source=collection_archive---------34-----------------------#2021-10-04">https://towardsdatascience.com/how-to-mitigate-overfitting-with-feature-selection-164897c0c3db?source=collection_archive---------34-----------------------#2021-10-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="5e12" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">解决过度拟合问题</h2><div class=""/><div class=""><h2 id="a58d" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">解决过度拟合问题—第5部分</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/76e8ffd0bb4850817bc5f2d80004054d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ES85N855kTMjOIzd1ZbzBA.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@mockup_photos?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">在<a class="ae lh" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的实体照片</a>拍摄</p></figure><p id="0b2b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这是<strong class="lk jd">【解决过度拟合问题】</strong>系列文章的最后一部分。今天，我们从第四部分继续。上次，在第4部分的第3步中，我们在"<a class="ae lh" href="https://drive.google.com/file/d/19s5qMRjssBoohFb2NY4FFYQ3YW2eCxP4/view?usp=sharing" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd">心脏病</strong> </a>"数据集上构建了一个随机森林模型。在那里，我们使用了数据集的所有13个特征。今天，我们将通过仅考虑最重要的特性来修改该模型。</p><p id="252d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">首先，我们通过使用<strong class="lk jd"> feature_importances_ </strong>属性来可视化随机森林模型的特征重要性。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="me mf l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">随机森林模型特征重要性的可视化</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mg"><img src="../Images/9113a0c3aa7ec143ba044f810f99077e.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*KenEinMy12JiQNQ56ZpX0Q.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">(图片由作者提供)</p></figure><p id="c814" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">很明显，并不是所有的特征都对模型有贡献。有些特征对模型不重要，我们可以删除它们。</p><blockquote class="mh mi mj"><p id="48db" class="li lj mk lk b ll lm kd ln lo lp kg lq ml ls lt lu mm lw lx ly mn ma mb mc md im bi translated">当我们从模型中移除最不重要的特征时，我们降低了模型的复杂性和数据中的一些噪声。这样做将有助于进一步减轻过度拟合——按作者</p></blockquote><p id="0e01" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">上图的<strong class="lk jd"> x </strong>轴包含每个特性的特性重要性分数。我们可以为重要性分数指定一个阈值(这里，我们选择<strong class="lk jd"> 0.025 </strong>)，这样分数低于该阈值的所有特征都将被消除。为此，我们可以使用Scikit-learn<strong class="lk jd">select from model</strong>类。</p><pre class="ks kt ku kv gt mo mp mq mr aw ms bi"><span id="349e" class="mt mu it mp b gy mv mw l mx my">from sklearn.feature_selection import SelectFromModel</span><span id="9eba" class="mt mu it mp b gy mz mw l mx my">selector = SelectFromModel(rfclf, threshold=0.025)<br/>features_important = selector.fit_transform(X, y)</span></pre><p id="bce0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这将返回一个2D-Numpy数组，其中仅包含由阈值决定的重要特性的值。通过将其转换成熊猫数据帧，</p><pre class="ks kt ku kv gt mo mp mq mr aw ms bi"><span id="b68d" class="mt mu it mp b gy mv mw l mx my">pd.DataFrame(features_important).head()</span></pre><p id="efb1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们可以获得下表:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi na"><img src="../Images/1a30c32fa8f03e17f507055e484d3c3a.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*-aUoDOizGCkQ7pZA1C_Wgw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">数据帧1(图片由作者提供)</p></figure><p id="ae39" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果我们也得到原始数据帧，</p><pre class="ks kt ku kv gt mo mp mq mr aw ms bi"><span id="ee9c" class="mt mu it mp b gy mv mw l mx my">X.head()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/2b1121e7f885892abf66a78963932036.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*F5evI3YqIBqd6hdvRVG6tQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">数据帧2(图片由作者提供)</p></figure><p id="98df" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们已经移除了4个特征:<strong class="lk jd"> fbs </strong>、<strong class="lk jd"> restecg </strong>、<strong class="lk jd"> trestbps </strong>和<strong class="lk jd"> chol </strong>。这些特征对模型没有足够的贡献。将它们保留在模型中会增加模型的复杂性，并导致模型过度拟合数据。</p><p id="c1e9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">移除特征的重要性分数的顺序是:</p><pre class="ks kt ku kv gt mo mp mq mr aw ms bi"><span id="f91e" class="mt mu it mp b gy mv mw l mx my"><strong class="mp jd">fbs &lt; restecg &lt; trestbps &lt; chol</strong></span></pre><p id="e8d1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果考虑特征<strong class="lk jd">【性】</strong>，其重要性值是<strong class="lk jd">【CHOL】</strong>的两倍以上。如果我们还删除了特征<strong class="lk jd">【性别】</strong>，模型将丢失一些重要数据。这将导致模型拟合不足。这就是为什么我们选择了<strong class="lk jd"> 0.025 </strong>作为阈值来选择我们想要保留在模型中的特征。通过查看特征重要性图，可以很容易地确定阈值的值。</p><p id="359c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">最后，我们可以使用移除了特征的修改后的数据帧(数据帧1)来再次构建随机森林模型。这一次，我们只使用了9个特性。换句话说，我们可以说我们降低了数据的维度。但是，请记住，数据集中的原始值保持不变。您可以将其与本系列的第3部分<a class="ae lh" rel="noopener" target="_blank" href="/how-to-mitigate-overfitting-with-dimensionality-reduction-555b755b3d66">进行比较，在第3部分</a>中，我们讨论了一种降维方法，该方法可以找到一组新的要素，这些要素包含与原始数据集中不同的值。</p><p id="67c4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们今天讨论的特征选择方法可以应用于基于树的模型，如决策树、随机森林等。对于线性回归和逻辑回归模型，向后消除和向前选择是首选。如果你想详细了解它们，可以阅读我写的这篇文章。</p></div><div class="ab cl nc nd hx ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="im in io ip iq"><p id="7ed4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">今天的部分到此结束，也是<strong class="lk jd">“解决过度拟合问题”</strong>系列文章。我们在5篇文章中讨论了5种不同的技术来解决过度拟合的问题。作为附加内容，我将添加一些选择最佳技术的指南。</p><h1 id="4f8a" class="nj mu it bd nk nl nm nn no np nq nr ns ki nt kj nu kl nv km nw ko nx kp ny nz bi translated">选择最佳技术的一些准则</h1><p id="8ef4" class="pw-post-body-paragraph li lj it lk b ll oa kd ln lo ob kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">到目前为止，我们已经讨论了以下可用于减轻过度拟合的技术:</p><ul class=""><li id="efbd" class="of og it lk b ll lm lo lp lr oh lv oi lz oj md ok ol om on bi translated"><strong class="lk jd">交叉验证</strong></li><li id="8220" class="of og it lk b ll oo lo op lr oq lv or lz os md ok ol om on bi translated"><strong class="lk jd">正规化</strong></li><li id="9804" class="of og it lk b ll oo lo op lr oq lv or lz os md ok ol om on bi translated"><strong class="lk jd">降维</strong></li><li id="33c5" class="of og it lk b ll oo lo op lr oq lv or lz os md ok ol om on bi translated"><strong class="lk jd">创作合奏</strong></li><li id="7981" class="of og it lk b ll oo lo op lr oq lv or lz os md ok ol om on bi translated"><strong class="lk jd">功能选择</strong></li></ul><p id="fdd8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您不需要应用所有这些技术来减轻过度拟合。这里有一些指导方针可以帮助你选择最好的技术。</p><ul class=""><li id="86d0" class="of og it lk b ll lm lo lp lr oh lv oi lz oj md ok ol om on bi translated">首先，在应用任何技术之前，您需要意识到您的模型是过度拟合的。如果你得到一个较高的训练分数和相对较低的测试分数，你的模型会过度拟合数据。</li><li id="bb84" class="of og it lk b ll oo lo op lr oq lv or lz os md ok ol om on bi translated">使用<a class="ae lh" rel="noopener" target="_blank" href="/k-fold-cross-validation-explained-in-plain-english-659e33c0bc0">交叉验证</a>来评估模型总是更好，交叉验证有助于您发现大量选项来减轻过度拟合。</li><li id="9c25" class="of og it lk b ll oo lo op lr oq lv or lz os md ok ol om on bi translated">最好将交叉验证与超参数调整技术结合起来，如<a class="ae lh" rel="noopener" target="_blank" href="/python-implementation-of-grid-search-and-random-search-for-hyperparameter-optimization-2d6a82ebf75c">网格搜索或随机搜索</a>，为定义的超参数找到一组最佳值。这将有助于你减轻过度拟合。任何型号都可以进行超参数调整。例如，我们可以用它来限制决策树的增长。这个过程是在<a class="ae lh" rel="noopener" target="_blank" href="/how-to-mitigate-overfitting-with-regularization-befcf4e41865">第二部分</a>中讨论的一种正规化。</li><li id="7e51" class="of og it lk b ll oo lo op lr oq lv or lz os md ok ol om on bi translated">另一种类型的正则化是将正则化项添加到损失函数中。这适用于逻辑回归、线性回归等线性模型，也适用于XGBoost等基于树的模型。对于逻辑回归，在Scikit-learn中默认应用正则化的L2变量。L1变种也可用。对于线性回归，没有在模型本身中指定正则化的选项。然而，Scikit-learn为此提供了3个不同的选项。当我们将L1正则化应用于线性回归时，它被称为<strong class="lk jd">套索</strong>回归。当我们将L2正则化应用于线性回归时，它被称为<strong class="lk jd">岭</strong>回归。当我们同时将L1和L2变量应用于线性回归时，它被称为<strong class="lk jd"> ElasticNet </strong>回归。</li><li id="ad3a" class="of og it lk b ll oo lo op lr oq lv or lz os md ok ol om on bi translated">降维技术可以应用于几乎任何数据集。最流行的DR技术是主成分分析(PCA)。除了解决过拟合问题之外，它还有许多优点。</li><li id="22d2" class="of og it lk b ll oo lo op lr oq lv or lz os md ok ol om on bi translated">如果您想在数据中找到非线性模式，您可以使用集成技术，如随机森林和XGBoost模型，而不是使用单个决策树。</li><li id="cd38" class="of og it lk b ll oo lo op lr oq lv or lz os md ok ol om on bi translated">您也可以将两种技术结合起来，以获得更好的效果。例如，您可以创建集合模型，同时应用正则化来限制单个树的增长。您可以在选择数据集中最重要的要素时创建集合模型。</li><li id="f10a" class="of og it lk b ll oo lo op lr oq lv or lz os md ok ol om on bi translated">创建模型后，您可以识别最重要的要素并从数据集中移除不必要的要素，然后仅使用最重要的要素来重新构建模型。对于像随机森林XGBoost这样的集合模型，使用<strong class="lk jd"> feature_importances_ </strong>属性可以很容易地做到这一点。对于线性回归和逻辑回归模型，可以使用单独的类进行向后消除和向前选择。</li></ul><h1 id="cf09" class="nj mu it bd nk nl nm nn no np nq nr ns ki nt kj nu kl nv km nw ko nx kp ny nz bi translated">结论</h1><p id="b02c" class="pw-post-body-paragraph li lj it lk b ll oa kd ln lo ob kg lq lr oc lt lu lv od lx ly lz oe mb mc md im bi translated">现在，你将拥有应用5种不同技术解决过度拟合问题的实践经验。我建议您将它们应用于不同的数据集。您可以对数据集应用多种技术并查看输出，然后通过查看输出来选择最佳技术。</p><p id="a2ca" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">为了方便起见，我决定列出这个系列文章的所有部分和另一个有用的帖子。现在，您可以通过单击下图在一个位置访问所有这些内容。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><a href="https://rukshanpramoditha.medium.com/list/addressing-overfitting-868959382d1d"><div class="gh gi ot"><img src="../Images/747771b979c59ccd183309360ec134cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1352/format:webp/1*QJEXVS4T7r54erq5K52jAg.png"/></div></a><p class="ld le gj gh gi lf lg bd b be z dk translated">本系列所有帖子列表(作者截图)</p></figure></div><div class="ab cl nc nd hx ne" role="separator"><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh ni"/><span class="nf bw bk ng nh"/></div><div class="im in io ip iq"><p id="b34a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">今天的帖子到此结束。我的读者可以通过下面的链接注册成为会员，以获得我写的每个故事的全部信息，我将收到你的一部分会员费。</p><p id="7c7d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">报名链接:</strong>【https://rukshanpramoditha.medium.com/membership T2】</p><p id="43c5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">非常感谢你一直以来的支持！下一个故事再见。祝大家学习愉快！</p><p id="0564" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">特别感谢Unsplash上的<strong class="lk jd">实体照片</strong>，<strong class="lk jd"> </strong>为我提供了这篇文章的封面图片(我对图片做了一些修改:添加了一些文字并删除了一些部分)。</p><p id="155a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ou ov ep" href="https://medium.com/u/f90a3bb1d400?source=post_page-----164897c0c3db--------------------------------" rel="noopener" target="_blank">鲁克山普拉莫迪塔</a><br/><strong class="lk jd">2021–10–04</strong></p></div></div>    
</body>
</html>