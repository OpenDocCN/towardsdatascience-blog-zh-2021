<html>
<head>
<title>Neural Networks : More than Deep Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">神经网络:不仅仅是深度学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/neural-networks-more-than-deep-learning-a0c9e1629781?source=collection_archive---------23-----------------------#2021-10-08">https://towardsdatascience.com/neural-networks-more-than-deep-learning-a0c9e1629781?source=collection_archive---------23-----------------------#2021-10-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1afa" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">神经网络是多功能的，让我们看看三种类型</h2></div><p id="3efd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">神经网络是计算智能的一个分支，远远超出了深度学习。虽然许多人在寻找最深的网络，但其他人正在寻找不同类型网络的独特实现和用途。在这篇文章中，我给出了三种不同类型的神经网络的高层次概述——卷积神经网络、自组织特征映射和极限学习机。</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/e161a19574d84bfb40ae1ac681f3f74e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*yDttUTBvZMBMrWQL"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">埃伯哈德·🖐·格罗斯加斯泰格在<a class="ae mb" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="1caf" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">动机</h1><p id="17a3" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">计算智能(CI)社区每年产生的研究数量令人震惊。竞争情报的一个特殊分支是一直受欢迎的<strong class="kk iu">神经网络</strong>。神经网络之所以成为主流，是因为它们的性能和对我们能把架构做多深的着迷。如今，深度学习似乎是“人工智能”的代名词，这推动了他们的接受度。我第一次接触深度学习是在做硕士论文的时候。我试图对多维声音信号进行分类(通过记录从感兴趣的区域反射回来的声音创建的图像)。那时，AlexNet是精英；很自然地，我实现了这个卷积神经网络(CNN)来评估我的数据。令我惊讶的是，它表现得非常好。这次经历让我见识到了这些黑匣子的威力。然而，直到几年后我才知道神经网络远不止深度分类/回归算法。IEEE计算学会将这一分支定义为“使用人脑作为灵感的来源，人工神经网络(NNs)是大规模并行分布式网络，可以从示例中学习和归纳。这个研究领域包括前馈神经网络、递归神经网络、自组织神经网络、深度学习、卷积神经网络等等。”您知道神经网络领域不仅仅是深度架构吗？我很兴奋地了解了其他一些领域，我将在这篇文章中分享它们。</p><p id="7160" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">卷积神经网络</strong></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi mz"><img src="../Images/6cfa8ed04efd10d7a1bad922b79b8af5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*OIORmH7UqOhcvMFm"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">照片由<a class="ae mb" href="https://unsplash.com/@joeyc?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">乔·凯恩</a>在<a class="ae mb" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="25e0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">卷积神经网络(CNN)(可以说是最流行的神经网络类型)是主要关注图像处理的神经网络。</strong>在描述神经网络时，大多数人会给出将图像分类为猫或狗的例子。CNN正是执行这一任务的神经网络类型。为了分解CNN，让我们考虑CNN的三个不同方面— <em class="na">滤波器</em>、<em class="na">架构</em>和<em class="na">优化功能</em>。</p><p id="ee49" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">过滤器</strong> <br/>在本文中，我们将过滤器视为CNN的构建模块。每个滤波器是在训练阶段优化的一组权重。我们的目标是学习一组代表我们要分类的对象类型的权重。例如，第一层过滤器可以学习边缘，当我们进一步进入网络时，这些过滤器被组合以形成更复杂的形状。然而，这并不是真正发生的事情，因为我们不是为了学习形状而优化，而是为了准确性而优化。尽管优化形状是可能的，但是看看我们为形态学共享神经网络写的一篇论文吧。<br/> <br/> <strong class="kk iu">架构</strong> <br/>如果滤镜是构建模块，那么架构就是我们用滤镜构建出来的。架构是由其结构定义的，结构可以是深的(多层过滤器)和宽的(每层有许多过滤器)。没有完美的架构，这就是为什么有这么多的架构。我的研究领域之一是融合CNN进行图像分类[2]。我们融合了七个网络，我们发现融合网络提高了性能——表明多种架构更好。许多因素限制了哪种架构最适合您的情况。例如，您将在哪里部署您的CNN？在你的手机上？在云端？更深的网络需要更多的计算能力，因此在使用(或构建)架构时考虑这些因素至关重要。</p><p id="ac99" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">优化功能</strong> <br/>优化功能驱动CNN的训练。对于图像分类，其目标可能是正确地分类尽可能多的图像。但是，假设CNN正在尝试执行对象定位(在图像中找到一个对象)。在这种情况下，优化函数将尝试查找并放置一个框在一个对象周围(并且只在该对象周围)。我们的创造力仅限于优化函数能做什么。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi nb"><img src="../Images/302d8c0bb6bfb0636a37b588b541d58e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XjPhtSB3YkmHTIzrN4Y5sg.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">插图突出了CNN的三个组成部分。我的插画:)-Joe Caione在<a class="ae mb" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的快乐狗狗if的照片</p></figure><p id="2197" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">CNN为一系列问题的解决打开了大门。像TensorFlow和Pytorch这样的开源软件包不仅促进了CNN的使用，也促进了任何NN的使用。鉴于它们的易用性，我确实觉得我们应该鼓励负责任地使用这些工具。在一个小时之内建立一个具有定制优化功能的定制架构是可能的(我已经这样做了几次)。我们应该挑战高精度足以部署模型的观念。作为这些技术的实践者，我相信我们每天都在做得更好，但我们应该继续追求卓越——特别是当我们把很多事情留给数据的时候。</p><p id="4e2b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">应用</strong> <br/>一如既往，我给大家留下一个例子。几年前，我在一家提供碎片监测服务的公司工作。监视器通过每年分析成千上万的图像来确保正确的负载估计。因此，我使用CNN来估计每次装载中碎片的数量(回归)和类型(分类)。这项任务不需要创建自定义架构；然而，我迭代了几个现成的架构，以找到性能最好的架构。这个项目花了几周时间，因为数据需要清理。数据中有例外，需要删除以确保我们不是在有缺陷的图像上进行训练。我确实改变了这个用例的优化函数。<strong class="kk iu">准确度分数对于这个应用程序来说还不够。</strong>对客户来说，一个重要因素是，如果加载调用出错，也不会差太多。例如，如果卡车是满的，我们从来不想称它是空的。因此，我将最大绝对误差添加到优化函数中。这种更新可以最大限度地减少回归值的偏差。我们的CNN可能会得到更多错误的图像，但它可以根据它们的错误程度来创建一个界限。</p><h1 id="64bd" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">自组织特征地图</h1><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi nc"><img src="../Images/88de1e8d1f8a8104a0278d1430d04ec8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uAQpWC9CF-rkrqOC"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">Marjan Blan | @marjanblan 在<a class="ae mb" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="220e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你用谷歌搜索“神经网络”，它会找到大约162，000，000个结果。其中很少是自组织特征映射(SOFMs)。SOFMs与你今天听到的大多数神经网络相对不同——在应用和设置上都不同。SOFMs推动了神经气体等其他算法的创建，但在这篇文章中，我们将从基础开始。</p><p id="92fd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">SOFMs是一种无人监管的算法，这意味着我们没有标签(想想猫或狗)。大多数无监督算法要么执行聚类，要么执行降维。降维发生在我们对某个事物有很多维度，需要降维的时候。这是一个粗略的例子，但考虑一个立方体。它的宽度、高度和深度是定义它的一种方式。然而，如果我们将这些特征相乘，就可以计算出它的体积。我们已经从对一个对象的三个描述减少到一个，因此我们已经将它的数据维度从三个减少到一个。</p><p id="335e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不幸的是，数据并不总是如此清晰。很多时候，当我们从高维空间缩减时，我们会因为缩减到几个维度而丢失信息。同样，如果立方体是蓝色的，我们也不会用它的体积值来表示。结果，一整套算法试图降低数据的维度。在没有完美的归约工具的情况下，存在很多算法。NN社区开发了SOFMs来解决这个问题。</p><p id="8bbc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要查看SOFMs的算法分类，请查看由<a class="nd ne ep" href="https://medium.com/u/68121f8321d7?source=post_page-----a0c9e1629781--------------------------------" rel="noopener" target="_blank">阿比纳夫·拉尔汉</a>撰写的这篇文章。</p><p id="3723" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">应用</strong></p><p id="cc02" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我最喜欢使用的SOFMs是它们在二维空间中可视化高维特征向量的能力。SOFMs有两个阶段——训练和映射。在训练期间，随机生成的神经元“地图”开始训练过程。在每次迭代中，我们向映射提供训练数据，它通过将具有相似输出的神经元分组来更新/组织自身。目标是相似神经元的组/簇将自己与其他组分开。如果成功，我们将能够在二维图像中看到不同的星团。在映射期间，新样本(来自高维)被分类到它们所属的聚类中。一个非常简单的例子涉及像素颜色。像素可以用三个值来表示——R、B和B。我们可以使用SOFM算法对这些像素进行聚类。在这个例子中，我们知道颜色会聚集在一起，但是如果我们对颜色一无所知，我们会在查看地图后知道它们的相似性。下一张图是我们可以预期的一个例子。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi nb"><img src="../Images/22b1d0390f85951e3b8cf5d5b2193917.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yoA1j2xpyGwrhSIrdAD46A.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">在这个例子中，我们从许多像素值中学习特征图。我的插图:)</p></figure><p id="61dc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">无监督方法在连接到应用程序时可能具有挑战性，因为它们通常用于探索数据集以更好地理解它。在大数据时代，我们的样本可能有成千上万个特征，数据科学家无法消化这么多的信息。因此，无监督技术将变得至关重要，因为我们正在收集更多的数据，并需要快速理解这些数据。SOFMs可能是更好地洞察高维数据的关键。</p><h1 id="c20a" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">极限学习机</h1><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi nf"><img src="../Images/a0a9025a7526b40528302d0c662b1152.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*9OL7DUNNsPPunVYX"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">由<a class="ae mb" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae mb" href="https://unsplash.com/@webdonut?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">网络甜甜圈</a>拍摄的照片</p></figure><p id="755c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">极限学习机(ELMs)是一种不太为人所知的神经网络。ELMs背后的理论不同于其他神经网络的训练。记住，神经网络通常使用反向传播来学习神经网络中的所有权重。</p><p id="0499" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，在ELMs中，在训练期间只学习一组重量。各层中的权重是固定的，并随机初始化，只有最后一层的权重需要学习。当数据在网络中传播时，随机初始化的权重随机地变换数据。网络的最后一层最终变成了一个需要求解的线性方程。要使这个工具工作，一些数学技巧是必要的，这里有一个很好的描述[4]。</p><p id="f7f8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">通常，我会强调一些类型的应用程序，但是到目前为止，您可能已经看到了一百种监督分类算法。相反，这里有一个关于榆树的有趣事实。榆树周围有戏剧。在学术论文中，戏剧很少出现在这一层面，但在榆树上却出现了。学术界因植根于坚实数学的新方法而繁荣。不幸的是，榆树可能不符合这个要求。榆树收到了反弹，因为榆树的想法不是新的，和[5]的作者想确保分配信贷适当的创造者正确。他们声称榆树是其他方法，他们不需要一个新的名字——极限学习机。</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="24fa" class="mc md it bd me mf ng mh mi mj nh ml mm jz ni ka mo kc nj kd mq kf nk kg ms mt bi translated">结论</h1><p id="e220" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">总之，除了世界上典型的、主流的谷歌网络和GPT-4之外，还有许多不同类型的网络。当然，我会在该表扬的地方给予表扬，这些国家队绝对值得表扬。但是，记住神经网络的分支作为一个整体同样重要。我在这篇文章中只触及了皮毛，但是这个领域每天都在进步。我等不及要看接下来会发生什么了！</p></div><div class="ab cl le lf hx lg" role="separator"><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj lk"/><span class="lh bw bk li lj"/></div><div class="im in io ip iq"><h1 id="c9e1" class="mc md it bd me mf ng mh mi mj nh ml mm jz ni ka mo kc nj kd mq kf nk kg ms mt bi translated">参考</h1><p id="7606" class="pw-post-body-paragraph ki kj it kk b kl mu ju kn ko mv jx kq kr mw kt ku kv mx kx ky kz my lb lc ld im bi translated">[1]https://arxiv.org/pdf/1912.02259.pdf<a class="ae mb" href="https://arxiv.org/pdf/1912.02259.pdf" rel="noopener ugc nofollow" target="_blank"/></p><p id="7cbe" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[2]<a class="ae mb" href="https://www.researchgate.net/publication/326586154_Explainable_AI_for_Understanding_Decisions_and_Data-Driven_Optimization_of_the_Choquet_Integral" rel="noopener ugc nofollow" target="_blank">https://www . research gate . net/publication/326586154 _ explable _ AI _ for _ Understanding _ Decisions _ and _ Data-Driven _ Optimization _ of _ the _ Choquet _ Integral</a></p><p id="a7aa" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[3]<a class="ae mb" href="https://medium.com/@abhinavr8/self-organizing-maps-ff5853a118d4" rel="noopener">https://medium . com/@ abhinavr 8/self-organizing-maps-ff 5853 a 118d 4</a></p><p id="310d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[4]<a class="ae mb" href="https://erdem.pl/2020/05/introduction-to-extreme-learning-machines" rel="noopener ugc nofollow" target="_blank">https://erdem . pl/2020/05/极限学习机简介</a></p><p id="b55c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">[5]https://citeseerx.ist.psu.edu/viewdoc/summary?<a class="ae mb" href="https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.217.2330" rel="noopener ugc nofollow" target="_blank">doi=10.1.1.217.2330 </a></p></div></div>    
</body>
</html>