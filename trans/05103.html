<html>
<head>
<title>Feature selection in machine learning using Lasso regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于Lasso回归的机器学习特征选择</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-selection-in-machine-learning-using-lasso-regression-7809c7c2771a?source=collection_archive---------2-----------------------#2021-05-05">https://towardsdatascience.com/feature-selection-in-machine-learning-using-lasso-regression-7809c7c2771a?source=collection_archive---------2-----------------------#2021-05-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="fa09" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">监督特征选择的例子</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f0c650fbf84d1b5eed11b979ef957f31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*W2SHKeoaKWLWQ_B-"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">查尔斯·德鲁维奥在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="d663" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">作为一名数据科学家，我学到的第一件事是，特征选择是机器学习管道中最重要的步骤之一。幸运的是，一些模型可以帮助我们完成这个目标，给我们他们自己对特性重要性的解释。其中一个模型是套索回归。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="1672" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">什么是套索回归？</h1><p id="3b94" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">我已经在<a class="ae ky" href="https://www.yourdatateacher.com/2021/03/29/an-introduction-to-linear-models/" rel="noopener ugc nofollow" target="_blank">之前的博客文章</a>中谈到了Lasso回归。让我总结一下这种模型的主要特性。这是一个使用成本函数的线性模型:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/a0394c0e0e25f2330cbe1ca4b19dd802.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*JF0ATjKjLDm0lEx5lPk8mg.png"/></div></figure><p id="26d5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="na"> aj </em>是第<em class="na"> j </em>个特征的系数。最后一项称为<em class="na"> l1 </em>罚项，而<em class="na"> α </em>是一个超参数，用于调整该罚项的强度。特征的系数越高，成本函数的值越高。因此，套索回归的思想是优化成本函数，减少系数的绝对值。显然，如果这些特征之前已经被缩放，例如使用标准化或其他<a class="ae ky" href="https://www.yourdatateacher.com/2021/03/22/scaling-of-the-numerical-variables/" rel="noopener ugc nofollow" target="_blank">缩放技术</a>，这是可行的。<em class="na"> α </em>必须使用交叉验证方法找到超参数值。</p><h1 id="a954" class="mc md it bd me mf nb mh mi mj nc ml mm jz nd ka mo kc ne kd mq kf nf kg ms mt bi translated">我们如何使用它进行特征选择？</h1><p id="849e" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">Lasso回归会尝试最小化成本函数，自动选择有用的要素，丢弃无用或多余的要素。在Lasso回归中，丢弃一个要素将使其系数等于0。</p><p id="a774" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，使用Lasso回归进行要素选择的想法非常简单:我们在数据集的缩放版本上拟合Lasso回归，并且我们只考虑那些系数不为0的要素。显然，我们首先需要调整<em class="na"> α </em>超参数，以便获得正确的套索回归。</p><p id="c914" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这很简单，会让我们很容易发现有用的特征，并丢弃无用的特征。</p><p id="41ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们来看看用Python怎么做。</p><h1 id="5ab9" class="mc md it bd me mf nb mh mi mj nc ml mm jz nd ka mo kc ne kd mq kf nf kg ms mt bi translated">Python中的示例</h1><p id="a3e8" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">在本例中，我将向您展示如何使用糖尿病数据集在Python中使用Lasso进行要素选择。你可以在我的<a class="ae ky" href="https://github.com/gianlucamalato/machinelearning/blob/master/Feature_selection_using_Lasso.ipynb" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>中找到全部代码。</p><p id="0875" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，让我们导入一些库:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="162b" class="nl md it nh b gy nm nn l no np">import numpy as np<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.pipeline import Pipeline<br/>from sklearn.model_selection import train_test_split, GridSearchCV<br/>from sklearn.linear_model import Lasso</span></pre><p id="4f55" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，我们可以导入数据集和要素的名称。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="d860" class="nl md it nh b gy nm nn l no np">from sklearn.datasets import load_diabetes<br/>X,y = load_diabetes(return_X_y=True)</span><span id="fd9b" class="nl md it nh b gy nq nn l no np">features = load_diabetes()['feature_names']</span></pre><p id="f3e1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">像往常一样，我们现在可以将数据集分成训练集和测试集，并只对训练集执行所有计算。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="ed44" class="nl md it nh b gy nm nn l no np">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)</span></pre><p id="60b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们必须建立我们的模型，优化它的超参数，并在训练数据集上训练它。</p><p id="9da6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为我们的数据集需要预先进行缩放，所以我们可以利用scikit-learn中强大的Pipeline对象。我们的管道是由一个标准缩放器和套索对象本身组成的。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="cbbc" class="nl md it nh b gy nm nn l no np">pipeline = Pipeline([<br/>                     ('scaler',StandardScaler()),<br/>                     ('model',Lasso())<br/>])</span></pre><p id="7477" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们要优化Lasso回归的<em class="na"> α </em>超参数。对于本例，我们将测试0.1到10之间的几个值，步长为0.1。对于每个值，我们在5重交叉验证中计算均方误差的平均值，并选择使这种平均性能指标最小化的<em class="na"> α </em>的值。为此，我们可以使用GridSearchCV对象。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="65a3" class="nl md it nh b gy nm nn l no np">search = GridSearchCV(pipeline,<br/>                      {'model__alpha':np.arange(0.1,10,0.1)},<br/>                      cv = 5, scoring="neg_mean_squared_error",verbose=3<br/>                      )</span></pre><p id="8a26" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们使用<em class="na"> neg_mean_squared_error </em>，因为网格搜索试图最大化性能指标，所以我们添加一个负号来最小化均方误差。</p><p id="479b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在可以进行网格搜索了。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="3368" class="nl md it nh b gy nm nn l no np">search.fit(X_train,y_train)</span></pre><p id="0b65" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="na"> α </em>的最佳值为:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="9234" class="nl md it nh b gy nm nn l no np">search.best_params_</span><span id="1723" class="nl md it nh b gy nq nn l no np"># {'model__alpha': 1.2000000000000002}</span></pre><p id="6f5b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们必须得到套索回归系数的值。</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="2004" class="nl md it nh b gy nm nn l no np">coefficients = search.best_estimator_.named_steps['model'].coef_</span></pre><p id="d775" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一个特性的重要性是其系数的绝对值，所以:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="cb26" class="nl md it nh b gy nm nn l no np">importance = np.abs(coefficients)</span></pre><p id="b2f2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看它的重要性:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/233e4f6b87b7ede7770a8224b00f59a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*J-cDCGALPMsA2DP-.png"/></div></div></figure><p id="363f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们所见，有3个特征的重要性为0。这些特征已经被我们的模型丢弃了。</p><p id="9959" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在套索回归中幸存下来的特征是:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="3810" class="nl md it nh b gy nm nn l no np">np.array(features)[importance &gt; 0]</span><span id="288d" class="nl md it nh b gy nq nn l no np"># array(['age', 'sex', 'bmi', 'bp', 's1', 's3', 's5'], dtype='&lt;U3')</span></pre><p id="b146" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">而3个被丢弃的特征是:</p><pre class="kj kk kl km gt ng nh ni nj aw nk bi"><span id="135e" class="nl md it nh b gy nm nn l no np">np.array(features)[importance == 0]</span><span id="27d7" class="nl md it nh b gy nq nn l no np"># array(['s2', 's4', 's6'], dtype='&lt;U3')</span></pre><p id="f9e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过这种方式，我们根据给定的目标变量，使用适当优化的Lasso回归来获取数据集最重要特征的信息。</p><h1 id="7a85" class="mc md it bd me mf nb mh mi mj nc ml mm jz nd ka mo kc ne kd mq kf nf kg ms mt bi translated">结论</h1><p id="8ef3" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">Lasso回归具有非常强大的内置要素选择功能，可用于多种情况。然而，它也有一些缺点。例如，如果特征和目标变量之间的关系不是线性的，那么使用线性模型可能不是一个好主意。通常，适当的探索性数据分析可以帮助我们更好地理解特征和目标之间最重要的关系，使我们选择最佳模型。</p><p id="d505" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想了解更多关于Lasso回归的知识，加入我的Python 在线课程<a class="ae ky" href="https://www.yourdatateacher.com/supervised-machine-learning-in-python-online-course/" rel="noopener ugc nofollow" target="_blank">监督机器学习。</a></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="3e83" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="na">原载于2021年5月5日https://www.yourdatateacher.com</em><a class="ae ky" href="https://www.yourdatateacher.com/2021/05/05/feature-selection-in-machine-learning-using-lasso-regression/" rel="noopener ugc nofollow" target="_blank"><em class="na"/></a><em class="na">。</em></p></div></div>    
</body>
</html>