<html>
<head>
<title>A Practical Introduction to 9 Regression Algorithms</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">9种回归算法的实用介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-practical-introduction-to-9-regression-algorithms-389057f86eb9?source=collection_archive---------6-----------------------#2021-09-28">https://towardsdatascience.com/a-practical-introduction-to-9-regression-algorithms-389057f86eb9?source=collection_archive---------6-----------------------#2021-09-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0ea1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">有效使用不同回归算法的实践教程</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3fa812b37de15a4d66a80f33b2774ba9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F2f1whiAvABj6le6mrLkBg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@jcoudriet?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">杰森·库德里特</a>在<a class="ae ky" href="https://unsplash.com/s/photos/graph?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="8408" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">线性回归通常是人们学习机器学习和数据科学的第一个算法。它简单易懂，但是，由于其功能有限，它不太可能是真实世界数据的最佳选择。最常见的是，线性回归被用作基线模型来评估和比较研究中的新方法。</p><p id="58f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在处理现实问题时，您应该知道并尝试许多其他回归算法。在本文中，您将通过使用<a class="ae ky" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a>和XGBoost的实践来学习9种流行的回归算法。这篇文章的结构如下:</p><ol class=""><li id="1a5a" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">线性回归</li><li id="890f" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">多项式回归</li><li id="b6a4" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">简单向量回归</li><li id="70dd" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">决策树回归</li><li id="92d5" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">随机森林回归</li><li id="648c" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">套索回归</li><li id="96ea" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">里脊回归</li><li id="945a" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">弹性网络回归</li><li id="227f" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">XGBoost回归</li></ol><blockquote class="mj"><p id="238e" class="mk ml it bd mm mn mo mp mq mr ms lu dk translated">请查看<a class="ae ky" href="https://github.com/BindiChen/machine-learning/blob/master/traditional-machine-learning/001-regression-algorithms/regression-algorithms.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>获取源代码。更多教程可从<a class="ae ky" href="https://github.com/BindiChen/machine-learning" rel="noopener ugc nofollow" target="_blank"> Github Repo </a>获得。</p></blockquote></div><div class="ab cl mt mu hx mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="im in io ip iq"><h1 id="8011" class="na nb it bd nc nd ne nf ng nh ni nj nk jz nl ka nm kc nn kd no kf np kg nq nr bi translated">1.线性回归</h1><p id="c576" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li nu lk ll lm nv lo lp lq nw ls lt lu im bi translated"><strong class="lb iu">线性回归</strong>通常是人们学习机器学习和数据科学的第一个算法。线性回归是一种线性模型，假设输入变量(<code class="fe nx ny nz oa b">X</code>)和单个输出变量(<code class="fe nx ny nz oa b">y</code>)之间存在线性关系。一般来说，有两种情况:</p><ul class=""><li id="ec70" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ob mb mc md bi translated"><strong class="lb iu">单变量线性回归</strong>:模拟<strong class="lb iu">一个</strong> <strong class="lb iu">单输入变量</strong>(单特征变量)和<strong class="lb iu">一个单输出变量</strong>之间的关系。</li><li id="409d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ob mb mc md bi translated"><strong class="lb iu">多元线性回归</strong>(也称<strong class="lb iu">多元线性回归</strong>):模拟<strong class="lb iu">多输入变量</strong>(多特征变量)和<strong class="lb iu">单输出变量</strong>之间的关系。</li></ul><p id="b1b6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这种算法非常常见，以至于<a class="ae ky" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a>在<code class="fe nx ny nz oa b">LinearRegression()</code>中内置了这种功能。让我们创建一个<code class="fe nx ny nz oa b">LinearRegression</code>对象，并使其适合训练数据:</p><pre class="kj kk kl km gt oc oa od oe aw of bi"><span id="3561" class="og nb it oa b gy oh oi l oj ok">from sklearn.linear_model import LinearRegression</span><span id="e16f" class="og nb it oa b gy ol oi l oj ok"># Creating and Training the Model<br/>linear_regressor = <strong class="oa iu">LinearRegression()</strong><br/>linear_regressor<strong class="oa iu">.fit(X, y)</strong></span></pre><p id="8fa2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一旦训练完成，我们可以检查<code class="fe nx ny nz oa b">LinearRegression</code>在<code class="fe nx ny nz oa b">coef_</code>属性中找到的系数参数:</p><pre class="kj kk kl km gt oc oa od oe aw of bi"><span id="41c5" class="og nb it oa b gy oh oi l oj ok">linear_regressor.<strong class="oa iu">coef_</strong></span><span id="7e68" class="og nb it oa b gy ol oi l oj ok">array([[-0.15784473]])</span></pre><p id="45d7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，用这个模型为训练数据拟合一条线</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/4fa98e1d54e76fce155284a9e22130bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_SjFDhmbEFGoPYFKJNqRyw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">sci kit-学习线性回归(图片由作者提供)</p></figure><p id="218e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于线性回归的几个要点:</p><ul class=""><li id="b765" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ob mb mc md bi translated">快速且易于建模</li><li id="71f6" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ob mb mc md bi translated">当要建模的关系不是非常复杂，并且您没有大量数据时，这尤其有用。</li><li id="5763" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ob mb mc md bi translated">非常直观的理解和解读。</li><li id="4bb8" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ob mb mc md bi translated">它对异常值非常敏感。</li></ul><h1 id="f565" class="na nb it bd nc nd on nf ng nh oo nj nk jz op ka nm kc oq kd no kf or kg nq nr bi translated">2.多项式回归</h1><p id="a7b9" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li nu lk ll lm nv lo lp lq nw ls lt lu im bi translated"><strong class="lb iu">多项式回归</strong>是我们想要为非线性可分数据创建模型时最流行的选择之一。这就像线性回归，但使用变量<code class="fe nx ny nz oa b">X</code>和<code class="fe nx ny nz oa b">y</code>之间的关系来找到绘制符合数据点的曲线的最佳方式。</p><p id="5d15" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于多项式回归，一些自变量的幂大于1。例如，我们可以提出如下的二次模型:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/1f7cd7d4b8792ef399f6a68aa376e67f.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*bdYuldcKZVSYBJMPSvRIoQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">二次模型(作者图片)</p></figure><ul class=""><li id="3fdd" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ob mb mc md bi translated"><code class="fe nx ny nz oa b">β_0</code>、<code class="fe nx ny nz oa b">β_1</code>和<code class="fe nx ny nz oa b">β_2</code>是系数</li><li id="601b" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ob mb mc md bi translated"><code class="fe nx ny nz oa b">x</code>是一个变量/特征</li><li id="7272" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ob mb mc md bi translated"><code class="fe nx ny nz oa b">ε</code>是偏见</li></ul><p id="6b64" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a>在<code class="fe nx ny nz oa b">PolynomialFeatures</code>中内置了这个方法。首先，我们需要生成一个由指定次数的所有多项式特征组成的特征矩阵:</p><pre class="kj kk kl km gt oc oa od oe aw of bi"><span id="8dc1" class="og nb it oa b gy oh oi l oj ok">from sklearn.preprocessing import PolynomialFeatures</span><span id="0ba0" class="og nb it oa b gy ol oi l oj ok"># We are simply generating the matrix for a quadratic model<br/>poly_reg = PolynomialFeatures(<strong class="oa iu">degree = 2</strong>)<br/><strong class="oa iu">X_poly</strong> = <strong class="oa iu">poly_reg.fit_transform(X)</strong></span></pre><p id="f020" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，让我们创建一个<code class="fe nx ny nz oa b">LinearRegression</code>对象，并将其与我们刚刚生成的特征矩阵<code class="fe nx ny nz oa b">X_poly</code>相匹配。</p><pre class="kj kk kl km gt oc oa od oe aw of bi"><span id="0b99" class="og nb it oa b gy oh oi l oj ok"># polynomial regression model<br/>poly_reg_model = LinearRegression()<br/>poly_reg_model.fit(<strong class="oa iu">X_poly</strong>, y)</span></pre><p id="c57a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在采用该模型，并为训练数据<code class="fe nx ny nz oa b">X_plot</code>拟合一条线，如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/66c443ce1f7632d9a60746342d7ed088.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0p7-7dt-iHMJvbyL5K8v8w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">多项式回归(作者图片)</p></figure><p id="089c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于多项式回归的几个要点:</p><ul class=""><li id="b4db" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ob mb mc md bi translated">能够对非线性可分离数据建模；线性回归做不到这一点。一般来说，它更加灵活，可以模拟一些相当复杂的关系。</li><li id="3368" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ob mb mc md bi translated">完全控制特征变量的建模(设置哪个指数)。</li><li id="f3ae" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ob mb mc md bi translated">需要精心设计。需要一些数据知识，以便选择最佳指数。</li><li id="d084" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ob mb mc md bi translated">如果指数选择不当，容易过度拟合。</li></ul><h1 id="68dc" class="na nb it bd nc nd on nf ng nh oo nj nk jz op ka nm kc oq kd no kf or kg nq nr bi translated">3.支持向量回归</h1><p id="511c" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li nu lk ll lm nv lo lp lq nw ls lt lu im bi translated">支持向量机在分类问题中是众所周知的。在回归中使用SVM被称为<strong class="lb iu">支持向量回归</strong> (SVR)。<a class="ae ky" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a>在<code class="fe nx ny nz oa b">SVR()</code>中内置了这个方法。</p><p id="02c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在拟合SVR模型之前，通常的最佳实践是执行特征缩放，以便每个特征具有相似的重要性。首先，让我们用<code class="fe nx ny nz oa b">StandardScaler()</code>进行特征缩放:</p><pre class="kj kk kl km gt oc oa od oe aw of bi"><span id="9a00" class="og nb it oa b gy oh oi l oj ok">from sklearn.svm import SVR<br/>from sklearn.preprocessing import StandardScaler</span><span id="4d60" class="og nb it oa b gy ol oi l oj ok"># Performing feature scaling<br/>scaled_X = StandardScaler()<br/>scaled_y = StandardScaler()</span><span id="ea32" class="og nb it oa b gy ol oi l oj ok">scaled_X = scaled_X.fit_transform(X)<br/>scaled_y = scaled_y.fit_transform(y)</span></pre><p id="6685" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们创建一个<code class="fe nx ny nz oa b">SVR</code>对象，将<strong class="lb iu">内核</strong>设置为<code class="fe nx ny nz oa b">'rbf'</code>，将<strong class="lb iu">伽玛</strong>设置为<code class="fe nx ny nz oa b">'auto'</code>。在这之后，我们调用<code class="fe nx ny nz oa b">fit()</code>来使其符合缩放的训练数据:</p><pre class="kj kk kl km gt oc oa od oe aw of bi"><span id="c24f" class="og nb it oa b gy oh oi l oj ok">svr_regressor = SVR(<strong class="oa iu">kernel='rbf'</strong>, <strong class="oa iu">gamma='auto'</strong>)<br/>svr_regressor.fit(<strong class="oa iu">scaled_X, scaled_y</strong>.ravel())</span></pre><p id="a693" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在采用该模型，并为训练数据<code class="fe nx ny nz oa b">scaled_X</code>拟合一条线，如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/9f3737cfc6e3cbcd43393af4abd8fb5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0mZ2O-kmmrrW0dtEaAV1jA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">支持向量回归(图片由作者提供)</p></figure><p id="cff4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于支持向量回归的几个要点</p><ul class=""><li id="63a3" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ob mb mc md bi translated">它对异常值是鲁棒的，并且在高维空间中是有效的</li><li id="270e" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ob mb mc md bi translated">它具有出色的泛化能力(能够恰当地适应新的、以前看不见的数据)</li><li id="71d2" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ob mb mc md bi translated">如果特征的数量远大于样本的数量，则容易过度拟合</li></ul><h1 id="0088" class="na nb it bd nc nd on nf ng nh oo nj nk jz op ka nm kc oq kd no kf or kg nq nr bi translated">4.决策树回归</h1><p id="a707" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li nu lk ll lm nv lo lp lq nw ls lt lu im bi translated"><strong class="lb iu">决策树(DTs) </strong>是一种用于分类和回归的非参数监督学习方法[1]。目标是创建一个模型，通过学习从数据特征推断的简单决策规则来预测目标变量的值。一棵树可以被看作是一个分段常数近似。</p><p id="3e99" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">决策树回归也很常见，以至于<a class="ae ky" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a>内置了<code class="fe nx ny nz oa b">DecisionTreeRegressor</code>。无需特征缩放即可创建<code class="fe nx ny nz oa b">DecisionTreeRegressor</code>对象，如下所示:</p><pre class="kj kk kl km gt oc oa od oe aw of bi"><span id="b3a4" class="og nb it oa b gy oh oi l oj ok">from sklearn.tree import DecisionTreeRegressor</span><span id="4a3e" class="og nb it oa b gy ol oi l oj ok">tree_regressor = DecisionTreeRegressor(random_state = 0)<br/>tree_regressor.<strong class="oa iu">fit(X, y)</strong></span></pre><p id="c5aa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在采用该模型，并使其符合训练数据:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/e25e8c1af01e9f61361d0e64af950d31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-1-LZMcNfYRknLkmbAyrMw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">决策树回归(图片作者提供)</p></figure><p id="1294" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于决策树的几个要点:</p><ul class=""><li id="67c2" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ob mb mc md bi translated">易于理解和解释。树木可以被可视化。</li><li id="8718" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ob mb mc md bi translated">适用于分类值和连续值</li><li id="746f" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ob mb mc md bi translated">使用DT(即预测数据)的成本是用于训练树的数据点数量的对数</li><li id="189f" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ob mb mc md bi translated">决策树的预测既不平滑也不连续(显示为分段常数近似，如上图所示)</li></ul><h1 id="0777" class="na nb it bd nc nd on nf ng nh oo nj nk jz op ka nm kc oq kd no kf or kg nq nr bi translated">5.随机森林回归</h1><p id="dd97" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li nu lk ll lm nv lo lp lq nw ls lt lu im bi translated"><strong class="lb iu">基本上，随机森林回归</strong>与决策树回归非常相似。它是一种元估计器，可以在数据集的各种子样本上拟合许多决策树，并使用平均来提高预测准确性和控制过度拟合。</p><p id="115b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随机森林回归器在回归中的表现不一定比决策树好(虽然它通常在分类中表现更好)，因为在树构造算法的本质中存在微妙的过拟合-欠拟合权衡。</p><p id="278c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随机森林回归非常普遍，以至于<a class="ae ky" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a>将它内置于<code class="fe nx ny nz oa b">RandomForestRegressor</code>中。首先，我们需要创建一个具有指定数量估算器的<code class="fe nx ny nz oa b">RandomForestRegressor</code>对象，如下所示:</p><pre class="kj kk kl km gt oc oa od oe aw of bi"><span id="bc24" class="og nb it oa b gy oh oi l oj ok">from sklearn.ensemble import <strong class="oa iu">RandomForestRegressor</strong></span><span id="6c29" class="og nb it oa b gy ol oi l oj ok">forest_regressor = RandomForestRegressor(<br/>    <strong class="oa iu">n_estimators = 300</strong>, <br/>    random_state = 0<br/>)<br/>forest_regressor.fit(X, y.ravel())</span></pre><p id="2263" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在采用该模型，并使其符合训练数据:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/76ab3675b22b8298a6dd3564b2d03a6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jW--H0TPkQoMfqy4EoUC4A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">随机森林回归(图片作者提供)</p></figure><p id="8202" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于随机森林回归的几点:</p><ul class=""><li id="f80e" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ob mb mc md bi translated">减少决策树中的过度拟合并提高准确性</li><li id="c405" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ob mb mc md bi translated">它也适用于分类值和连续值</li><li id="c082" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ob mb mc md bi translated">需要大量的计算能力和资源，因为它适合多个决策树来组合它们的输出</li></ul><h1 id="c01a" class="na nb it bd nc nd on nf ng nh oo nj nk jz op ka nm kc oq kd no kf or kg nq nr bi translated">6.套索回归</h1><p id="00d3" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li nu lk ll lm nv lo lp lq nw ls lt lu im bi translated">套索回归是使用收缩的线性回归的变体。收缩是数据值向中心点收缩为平均值的过程。这种类型的回归非常适合显示高度多重共线性(要素之间的高度相关性)的模型。</p><p id="f761" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Scikit-learn 内置了<code class="fe nx ny nz oa b">LassoCV</code>。</p><pre class="kj kk kl km gt oc oa od oe aw of bi"><span id="48d2" class="og nb it oa b gy oh oi l oj ok">from sklearn.linear_model import LassoCV</span><span id="aac5" class="og nb it oa b gy ol oi l oj ok">lasso = LassoCV()<br/>lasso.fit(X, y.ravel())</span></pre><p id="a7b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在采用该模型，并使其符合训练数据:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/fe4bfd39aa0d796230272d2009ab9d97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IZUOZzCeS1fNsz0cDZwPjg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">拉索回归(图片由作者提供)</p></figure><p id="5dbb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于拉索回归的几点:</p><ul class=""><li id="8d73" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ob mb mc md bi translated">它最常用于消除自动化变量和选择特性。</li><li id="7f02" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ob mb mc md bi translated">它非常适合显示高度多重共线性(要素之间的高度相关性)的模型。</li><li id="2de8" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ob mb mc md bi translated">套索回归利用L1正则化</li><li id="5330" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ob mb mc md bi translated">LASSO回归被认为比Ridge更好，因为它只选择一些要素，而将其他要素的系数降低到零。</li></ul><h1 id="0441" class="na nb it bd nc nd on nf ng nh oo nj nk jz op ka nm kc oq kd no kf or kg nq nr bi translated">7.里脊回归</h1><p id="5ee3" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li nu lk ll lm nv lo lp lq nw ls lt lu im bi translated">岭回归与套索回归非常相似，因为两种技术都使用收缩。岭回归和套索回归都非常适合显示高度多重共线性(要素之间的高度相关性)的模型。它们之间的主要区别是Ridge使用L2正则化，这意味着没有一个系数会像LASSO回归中那样变为零(而是接近零)。</p><p id="c643" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a>内置了<code class="fe nx ny nz oa b">RidgeCV</code>。</p><pre class="kj kk kl km gt oc oa od oe aw of bi"><span id="c1a4" class="og nb it oa b gy oh oi l oj ok">from sklearn.linear_model import RidgeCV</span><span id="f00d" class="og nb it oa b gy ol oi l oj ok">ridge = RidgeCV()<br/>ridge.fit(X, y)</span></pre><p id="0ed5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在采用该模型，并使其符合训练数据:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/5bbbf0f8900f025b7a7d2027c501d860.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jp5tYiL7f67_DxUi7OZo1Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">岭回归(图片作者提供)</p></figure><p id="d8c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于岭回归的几点:</p><ul class=""><li id="e03a" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ob mb mc md bi translated">它非常适合显示高度多重共线性(要素之间的高度相关性)的模型。</li><li id="5163" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ob mb mc md bi translated">岭回归使用L2正则化。贡献较小的特征将具有接近零的系数。</li><li id="ee18" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ob mb mc md bi translated">由于L2正则化的性质，岭回归被认为比拉索差</li></ul><h1 id="bc6c" class="na nb it bd nc nd on nf ng nh oo nj nk jz op ka nm kc oq kd no kf or kg nq nr bi translated">8.弹性网络回归</h1><p id="a8e7" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li nu lk ll lm nv lo lp lq nw ls lt lu im bi translated"><strong class="lb iu"> ElasticNet </strong>是另一个用L1和L2正则化训练的线性回归模型。它是套索和岭回归技术的混合，因此也非常适合显示严重多重共线性(要素之间的严重相关性)的模型。</p><p id="5a6d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在套索和脊之间进行权衡的一个实际优势是，它允许弹性网在旋转下继承脊的一些稳定性[2]。</p><p id="873c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a>内置了<code class="fe nx ny nz oa b">ElasticNetCV</code>。</p><pre class="kj kk kl km gt oc oa od oe aw of bi"><span id="ed6f" class="og nb it oa b gy oh oi l oj ok">from sklearn.linear_model import ElasticNetCV<br/>elasticNet = ElasticNetCV()<br/>elasticNet.fit(X, y.ravel())</span></pre><p id="ca40" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在采用该模型，并使其符合训练数据:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/f36f38526e44485ba905a90cadc3f568.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Psr_ewbapE9ZSbKoOS5nRw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">ElasticNet回归(图片由作者提供)</p></figure><p id="78e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于ElasticNet回归的几个要点:</p><ul class=""><li id="53ff" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ob mb mc md bi translated">ElasticNet总是优于LASSO和Ridge，因为它解决了这两种算法的缺点</li><li id="7d70" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ob mb mc md bi translated">ElasticNet带来了确定最优解的两个lambda值的额外开销。</li></ul><h1 id="fb36" class="na nb it bd nc nd on nf ng nh oo nj nk jz op ka nm kc oq kd no kf or kg nq nr bi translated">9.XGBoost回归</h1><p id="01b5" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li nu lk ll lm nv lo lp lq nw ls lt lu im bi translated"><strong class="lb iu">极限梯度提升</strong> ( <strong class="lb iu"> XGBoost </strong>)是梯度提升算法的一种高效且有效的实现。<strong class="lb iu">梯度提升</strong>是指一类集成机器学习算法，可用于分类或回归问题。</p><p id="c44a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> XGBoost </strong>是一个开源库，最初由<a class="ae ky" href="https://www.linkedin.com/in/tianqi-chen-679a9856/" rel="noopener ugc nofollow" target="_blank">陈天琦</a>在他2016年题为“<a class="ae ky" href="https://arxiv.org/abs/1603.02754" rel="noopener ugc nofollow" target="_blank"> XGBoost:一个可扩展的树增强系统</a>”的论文中开发。该算法被设计成计算效率高且高效。</p><p id="cbd7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果尚未安装XGBoost库，第一步是安装它。</p><pre class="kj kk kl km gt oc oa od oe aw of bi"><span id="792e" class="og nb it oa b gy oh oi l oj ok">pip install xgboost</span></pre><p id="9f59" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">XGBoost模型可以通过创建一个<code class="fe nx ny nz oa b">XGBRegressor</code>的实例来定义:</p><pre class="kj kk kl km gt oc oa od oe aw of bi"><span id="0588" class="og nb it oa b gy oh oi l oj ok">from xgboost import XGBRegressor<br/># create an xgboost regression model<br/>model = <strong class="oa iu">XGBRegressor(<br/>    n_estimators=1000, <br/>    max_depth=7, <br/>    eta=0.1, <br/>    subsample=0.7, <br/>    colsample_bytree=0.8,<br/>)</strong></span><span id="ca29" class="og nb it oa b gy ol oi l oj ok">model.fit(X, y)</span></pre><ul class=""><li id="1e4e" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ob mb mc md bi translated"><code class="fe nx ny nz oa b"><strong class="lb iu">n_estimators</strong></code>:集合中的树的数量，经常增加直到看不到进一步的改进。</li><li id="2a99" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ob mb mc md bi translated"><code class="fe nx ny nz oa b"><strong class="lb iu">max_depth</strong></code>:每棵树的最大深度，通常取值在1到10之间。</li><li id="0dbd" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ob mb mc md bi translated"><code class="fe nx ny nz oa b"><strong class="lb iu">eta</strong></code>:用于加权每个模型的学习率，通常设置为0.3、0.1、0.01或更小的小值。</li><li id="8ce6" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ob mb mc md bi translated"><code class="fe nx ny nz oa b"><strong class="lb iu">subsample</strong></code>:每棵树使用的样本数，设置为0-1之间的值，通常为1.0表示使用所有样本。</li><li id="f66e" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ob mb mc md bi translated"><code class="fe nx ny nz oa b"><strong class="lb iu">colsample_bytree</strong></code>:每个树中使用的特征(列)的数量，设置为0到1之间的值，通常为1.0以使用所有特征。</li></ul><p id="b271" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在采用该模型，并使其符合训练数据:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/adb065b856ad5344a85c9aeec2ed2590.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aIVgal1OvWLTzvwqoPIJug.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">XGBoost回归(图片由作者提供)</p></figure><p id="62ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">关于XGBoost的几点:</p><ul class=""><li id="9604" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ob mb mc md bi translated">XGBoost在稀疏和非结构化数据上表现不佳。</li><li id="3945" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ob mb mc md bi translated">该算法被设计为计算效率高且高效，但是对于大数据集来说，训练时间仍然相当长</li><li id="d45f" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ob mb mc md bi translated">它对异常值很敏感</li></ul><h1 id="819f" class="na nb it bd nc nd on nf ng nh oo nj nk jz op ka nm kc oq kd no kf or kg nq nr bi translated">结论</h1><p id="01b7" class="pw-post-body-paragraph kz la it lb b lc ns ju le lf nt jx lh li nu lk ll lm nv lo lp lq nw ls lt lu im bi translated">在本文中，我们通过使用<a class="ae ky" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a>和XGBoost的实践，介绍了9种流行的回归算法。在您的工具箱中拥有它们是很好的，这样您就可以尝试不同的算法，并为现实世界的问题找到最佳的回归模型。</p><p id="9fe0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望你喜欢这篇文章，并学到一些新的有用的东西。</p><p id="beb7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读。请查看<a class="ae ky" href="https://github.com/BindiChen/machine-learning/blob/master/traditional-machine-learning/001-regression-algorithms/regression-algorithms.ipynb" rel="noopener ugc nofollow" target="_blank">笔记本</a>获取源代码，如果你对机器学习的实用方面感兴趣，请继续关注。更多教程可从<a class="ae ky" href="https://github.com/BindiChen/machine-learning" rel="noopener ugc nofollow" target="_blank"> Github Repo </a>获得。</p><p id="8a5e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">参考文献:</strong></p><ul class=""><li id="76a2" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ob mb mc md bi translated">[1]sci kit-学习决策树文档:<a class="ae ky" href="https://scikit-learn.org/stable/modules/tree.html#tree" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/tree.html#tree</a></li><li id="75f2" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ob mb mc md bi translated">[2] Scikit-Learn ElasticNet文档:<a class="ae ky" href="https://scikit-learn.org/stable/modules/linear_model.html#elastic-net" rel="noopener ugc nofollow" target="_blank">https://sci kit-Learn . org/stable/modules/linear _ model . html # elastic-net</a></li></ul></div></div>    
</body>
</html>