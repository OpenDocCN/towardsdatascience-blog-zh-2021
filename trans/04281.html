<html>
<head>
<title>The Basics of Face Tracking in Augmented Reality</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">增强现实中人脸跟踪的基础知识</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/anchors-and-facetracking-in-ar-for-dummies-7a297ffbf7fe?source=collection_archive---------21-----------------------#2021-04-11">https://towardsdatascience.com/anchors-and-facetracking-in-ar-for-dummies-7a297ffbf7fe?source=collection_archive---------21-----------------------#2021-04-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/8e045b3fed5f3a534fb1410d6c368ed2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zk9k9YEKUVEt2Klt"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片由Unsplash的John Noonan提供</p></figure><div class=""/><div class=""><h2 id="fce1" class="pw-subtitle-paragraph kf jh ji bd b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw dk translated">锚点、实体、混合形状、系数和表达式</h2></div><h2 id="74f5" class="kx ky ji bd kz la lb dn lc ld le dp lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">一点历史</h2><p id="5c84" class="pw-post-body-paragraph lt lu ji lv b lw lx kj ly lz ma km mb lg mc md me lk mf mg mh lo mi mj mk ml im bi translated">苹果最新的iPads和iPhones配备了激光雷达扫描仪，正如你们许多人所知，它代表光探测和测距。它是一个使用红外光从场景中收集深度信息的传感器。在激光雷达之前，我们使用无线电波来实现这一目的……无线电探测和测距(雷达)。它非常适合更短的距离，而且更加精确。激光雷达工作在更远的距离，但在雾或雨不是很准确。</p><p id="19af" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">为什么我要说激光雷达？因为它是消费设备中增强现实的下一次飞跃。对于一个四处移动并理解真实世界的机器人来说，它需要理解它的周围环境以及它相对于该空间中其他事物的位置和方向。</p><p id="672b" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">你的手机是一个没有四肢的机器人。它是一个大脑和一些为大脑提供信息的传感器，以便更好地了解它的环境。激光雷达是一种增强其视觉的传感器。</p><p id="a8a9" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">我们人类通过利用双眼之间的视角差异来获得深度感。在计算机上同时保持实时帧速率是一项极其昂贵的计算。所以现在我们不得不使用激光雷达传感器和一只眼睛/相机来凑合。</p><p id="7a14" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">我们已经取得了相当大的进步，从仅仅能够检测水平面到在带有激光雷达的消费设备上相当精确地检测深度。由于我们现在可以用AR做更多的事情，我们可以开始利用增加的功能来构建更复杂的体验。</p><p id="e42a" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">为此，我们需要框架来抽象出底层系统的复杂性，并为程序员、艺术家和设计师提供额外的功能，以便在更短的时间内构建AR体验，并减少麻烦。</p><p id="b078" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">RealityKit就是这样一个框架，它的姐妹iOS应用程序Reality Composer来自App Store，是该框架的可视化包装器，RealityKit的拖放功能的启用者，以及隐藏底层框架复杂性的迷你内容创建平台。</p><h2 id="f7d9" class="kx ky ji bd kz la lb dn lc ld le dp lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">为什么真人秀作曲家可能成为下一个“iMovie”</h2><p id="f5dd" class="pw-post-body-paragraph lt lu ji lv b lw lx kj ly lz ma km mb lg mc md me lk mf mg mh lo mi mj mk ml im bi translated">iOS系统上的每个底层操作系统特性都有相应的编程框架。例如用于绘制UI的UIKit、用于Sprite动画的SpriteKit、用于用iCloud编程的CloudKit、用于编程增强现实(AR)体验的ARKit等。</p><p id="9445" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">RealityKit框架使用ARKit框架来创建AR体验。ARKit框架处理来自IMU、视觉系统和激光雷达的数据，以在空间中定位自己，并找出周围其他事物的位置。Reality kit使用这些信息将虚拟对象带入AR空间。</p><p id="5073" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">RealityKit还使某些样板任务变得更容易，如在构建共享AR体验的同时处理网络上的连接管理。虽然这不是RealityKit的出名之处。它主要是因为提供了一个平台来编程骨骼动画、真实的阴影、照明、反射等。这些能力通过一个名为Reality Composer的应用程序带给艺术家和设计师。</p><p id="8c10" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">你会觉得这些东西在虚幻引擎，Unity，Maya，3DSMax，Houdini等里面处理的比较好。然后可以将它们作为USDZ文件导入。苹果为什么需要开发3D内容创作平台？也许他们有计划在这个市场上竞争，并在Mac上提供端到端的解决方案来创建和消费AR体验。这是我的推测。</p><p id="0306" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">Reality Composer是一个用于AR的DCC(数字内容创作)平台。它既可以在Mac上运行，也可以在iPhone和iPad上运行。我在我的笔记本电脑上使用它，我发现它有直观的动作控制。</p><p id="1e79" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">连按两次以重设世界，捏合以放大和缩小。放下一个手指，用另一个手指旋转，等等。在iPhone或iPad上，您可以直接在AR中设计3D内容。虽然现在这可能看起来像是一个噱头，但将来当AR平台支持更复杂的设计时，这一点的用处将显而易见。</p><p id="0c1c" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">我们在虚拟现实中达到了一个类似的里程碑，Epic Games允许虚幻引擎在虚拟世界中运行编辑器。你可以戴着虚拟现实耳机，用你的Vive控制器抓住东西并移动它们。</p><p id="8c7e" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">这不是设计关卡的最佳方式，因为长时间在虚拟现实头盔中工作的人在后勤方面存在固有的问题。但是有一点是肯定的——如果它能大规模工作，它将减少迭代时间。</p><p id="02e5" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">在桌面应用程序上设计一些东西，然后进行构建并在设备上部署构建，以检查您在VR中的工作，这是许多不必要的往返。在你的iPad或iPhone上设计东西并不是一种不舒服或陌生的体验。</p><p id="0c72" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">因此，iOS设备上的Reality Composer可能有很好的增长潜力，可以作为未来为AR创建内容的首选应用程序。即使您要在Mac上进行设计，Reality Composer也支持快速查看功能。</p><p id="6d85" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">从应用程序中导出场景，通过电子邮件发送给自己或保存在iCloud上，并在iPhone上打开Reality Composer场景。它会向你展示你的设计在现实中的样子。无需在XCode项目中导入场景，构建并在设备上部署它来检查您的工作。</p><p id="681e" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">你也可以在Safari中嵌入场景。Safari有AR的快速查看功能。你可以用你的iPhone去apple.com看看，在AR里浏览他们的每一件产品。使用快速查看功能，直接从Safari查看工作区中的Mac Pro或Mac Book。</p><p id="ca21" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">Reality Composer现在完全集成到XCode中。</p><p id="824e" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">对我来说，这是如何将这些部分设计成形成一个端到端的管道，为苹果生态系统中的AR创建内容的一瞥。</p><p id="5bb9" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">随着最新的传感器和框架的出现，让我们继续跟踪，看看它们是如何用于跟踪人脸的。</p><h2 id="7d2e" class="kx ky ji bd kz la lb dn lc ld le dp lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">AR中的锚和实体</h2><p id="8655" class="pw-post-body-paragraph lt lu ji lv b lw lx kj ly lz ma km mb lg mc md me lk mf mg mh lo mi mj mk ml im bi translated">锚点有助于跟踪已识别的对象。当对象被AR系统识别时，锚被放置在对象上。它记录了物体在世界空间中的方向和位置。</p><p id="10ac" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">随着相机四处移动，识别的对象相对于相机移动，AR系统试图跟踪移动，以便锚可以每帧更新。</p><p id="f6ed" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">锚由一个名为<em class="mr"> didUpdateFrame </em>的会话委托函数(回调函数的别称)访问。其中，这个<em class="mr"> updateFrame </em>函数接收一个<em class="mr"> ARFrame </em>。</p><p id="8979" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated"><em class="mr"> ARFrame </em>包含关于摄像机、锚、场景、跟踪、绘图状态、地理跟踪状态、分割(检查人)等信息。大多数AR应用程序都需要访问<em class="mr"> ARFrame </em>中的数据。它以应用程序帧速率的频率可用。</p><p id="4f46" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">任何东西都可以放在锚上。宇宙飞船、小球或图像的3D模型。</p><p id="9b52" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">一旦它们被锚定，AR跟踪系统就会跟踪它们，这样当你四处移动时，它们就可以在世界的同一个地方被维护。就像你在现实世界中四处走动时会看到一棵树一样。</p><p id="4100" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">这些放置在锚点的对象在AR中被称为<em class="mr">实体</em>。</p><p id="8077" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">当ARKit检测到一张脸时，它会在被检测为鼻子的区域的正后方放置一个锚。下图中，黄点是大概的位置。这个锚叫做<em class="mr"> ARFaceAnchor </em>。当会话在前置摄像头输入中检测到唯一的人脸时，它会自动将一个<em class="mr"> ARFaceAnchor </em>对象添加到其锚点列表中。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ms"><img src="../Images/8b67500031854114c98a5b1b57e082d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*I0UskxVVYiJyBprqkyuycw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="9de5" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">它是如何检测人脸的？面部跟踪系统是由一串记录眼睛、眼球、眉毛、下巴等运动的数字定义的。这些数字被称为系数。这些系数定义了面拓扑。然后，该拓扑用于创建一个名为<em class="mr"> ARFaceGeometry </em>对象的几何体。</p><p id="995d" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">该几何模型可用于将自定义对象“附加”到AR面上。导出系数和重建面网格几何体也比导出网格更容易，因为网格中顶点的数量远远大于系数的数量。</p><p id="3803" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">系数是浮点数，其值的范围从0.0到1.0。系数的不同值记录了不同的面部表情。代表特定表达式的这些系数的集合称为混合形状。因此这些系数被称为混合形状系数。</p><p id="fc5a" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">这些系数使得3D角色的面部动画成为可能。从系数开始，让我们看看面网格几何本身。</p><p id="5246" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">人脸网格几何是人脸跟踪系统的重要组成部分。您可以在锚点位置放置一个面罩。演员在剧院里用于表演训练的那种面具。该遮罩在Reality Composer中作为3D网格随时可用。ARKit将通过在用户移动他们的脸时跟踪底层锚来跟踪面具。</p><p id="b977" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">或者你可以使用这种几何图形作为叠加内容的基础，如化妆或酷太阳镜，就像Snapchat所做的那样。Snap使用了这些相同的原则和基本概念，只是他们没有使用ARkit。他们开发了自己的AR系统。</p><p id="6bee" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">让我们花一点时间来讨论一下跟踪，这样您就可以充分理解使这一切成为可能所需的各种底层技术。</p><h2 id="a79b" class="kx ky ji bd kz la lb dn lc ld le dp lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">AR中的跟踪</h2><p id="48d0" class="pw-post-body-paragraph lt lu ji lv b lw lx kj ly lz ma km mb lg mc md me lk mf mg mh lo mi mj mk ml im bi translated">追踪并不像听起来那么容易。当你四处移动时，你可能会在离物体几米远的地方徘徊，它可能不会在你的相机视野内。但是如果你决定转身回来，你会希望它停留在你最后看到它的地方。</p><p id="0c6e" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">这些都是计算量很大的计算，所以需要一些捷径来找到实时帧速率和准确性之间的平衡。因此，对象的位置可能会随着时间的推移而改变，您可能会发现锚点的位置与上次看到时略有不同。这种差异被称为漂移。</p><p id="bd7e" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">不同的系统有不同程度的漂移。像Hololens和Magic Leap这样的头戴式AR设备有专用的硬件和更多的资源来执行空间映射(用几何图形/三角形近似相机看到的世界)和跟踪。因此，这些设备往往比手持AR设备更准确，如iPad、iPhone和越来越好的Androids。</p><p id="0d69" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">在iOS设备上引入激光雷达是帮助空间制图的一大飞跃，因为激光雷达用精确的深度信息增强了相机数据，所以系统不需要处理来自相机馈送的数据来估计深度。</p><h2 id="3cc2" class="kx ky ji bd kz la lb dn lc ld le dp lf lg lh li lj lk ll lm ln lo lp lq lr ls bi translated">最后</h2><p id="9dd2" class="pw-post-body-paragraph lt lu ji lv b lw lx kj ly lz ma km mb lg mc md me lk mf mg mh lo mi mj mk ml im bi translated">我们讨论了激光雷达及其重要性。我们谈到了Reality Composer及其潜力，然后浅尝辄止地研究了跟踪、锚点，尤其是面部跟踪。</p><p id="fa9b" class="pw-post-body-paragraph lt lu ji lv b lw mm kj ly lz mn km mb lg mo md me lk mp mg mh lo mq mj mk ml im bi translated">希望它有助于作为进入这些有趣的话题的跳板。如果你有任何问题，请在下面留言，我会尽力回答。此外，Apple的开发人员文档是一个很好的资源，可以找到您可以自己尝试的编码示例。</p></div></div>    
</body>
</html>