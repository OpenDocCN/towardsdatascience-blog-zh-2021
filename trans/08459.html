<html>
<head>
<title>Recent Advances in Transfer Learning for Natural Language Processing</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理中迁移学习的新进展</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-should-you-leverage-transfer-learning-14d08a60f616?source=collection_archive---------19-----------------------#2021-08-04">https://towardsdatascience.com/why-should-you-leverage-transfer-learning-14d08a60f616?source=collection_archive---------19-----------------------#2021-08-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="4e6f" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="6015" class="pw-subtitle-paragraph jx iz iq bd b jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko dk translated">自然语言处理中迁移学习技术的发展综述</h2></div><blockquote class="kp kq kr"><p id="9a93" class="ks kt ku kv b kw kx kb ky kz la ke lb lc ld le lf lg lh li lj lk ll lm ln lo ij bi translated">以下是我新发布的《自然语言处理的迁移学习》一书的节选。摘要总结了一些最近的NLP模型架构依赖于迁移学习的概念。</p></blockquote><p id="1f0b" class="pw-post-body-paragraph ks kt iq kv b kw kx kb ky kz la ke lb lp ld le lf lq lh li lj lr ll lm ln lo ij bi ls translated">人工智能已经戏剧性地改变了现代社会。以前由人类完成的任务现在可以由机器来完成，速度更快，成本更低，在某些情况下效率更高。这方面的流行例子包括计算机视觉应用——涉及教计算机如何理解图像和视频——例如，用于在闭路电视摄像中检测罪犯。其他计算机视觉应用包括从患者器官的图像中检测疾病，以及从植物叶子中检测植物种类。人工智能的另一个重要分支，特别处理人类自然语言数据的分析和处理，被称为<em class="ku">自然语言处理</em> (NLP)。NLP应用的例子包括语音到文本的转录和各种语言之间的翻译，等等。在图1中，AI和NLP在维恩图中与相邻的场并列。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi mb"><img src="../Images/7684d7a0ae45e52e00151cc4d74cde1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TSoPmzQPEmfFCcXh0CnTnQ.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">图一。维恩图是自然语言处理(NLP)、人工智能(AI)、机器学习和深度学习等术语相对于彼此的可视化。也显示了符号AI。(图片作者，来自<a class="ae mr" href="https://www.manning.com/books/transfer-learning-for-natural-language-processing" rel="noopener ugc nofollow" target="_blank"> <em class="jw">《自然语言处理的迁移学习》</em> </a>)</p></figure><p id="4d1c" class="pw-post-body-paragraph ks kt iq kv b kw kx kb ky kz la ke lb lp ld le lf lq lh li lj lr ll lm ln lo ij bi translated">人工智能、机器人和自动化领域技术革命的最新体现，被一些人称为第四次工业革命的<a class="ae mr" href="https://law.unimelb.edu.au/__data/assets/pdf_file/0005/3385454/Schwab-The_Fourth_Industrial_Revolution_Klaus_S.pdf" rel="noopener ugc nofollow" target="_blank"/>，是由训练大型神经网络的算法进步、通过互联网获得的大量数据以及通过最初为个人游戏市场开发的<em class="ku">图形处理单元</em>(GPU)获得的大规模并行能力的交叉引发的。更具体地说，最近依赖于人类感知的任务自动化的快速发展，特别是计算机视觉和NLP，要求神经网络理论和实践取得这些进展。这使得输入数据和期望的输出信号之间的复杂表示能够被学习，以处理这些困难的问题。</p><p id="1205" class="pw-post-body-paragraph ks kt iq kv b kw kx kb ky kz la ke lb lp ld le lf lq lh li lj lr ll lm ln lo ij bi translated">与此同时，对人工智能在不久的将来能够完成的事情的预测已经大大超过了在实践中已经取得的成就。我们被警告一个世界末日的未来，它将抹去大多数人类工作，取代我们所有人，甚至可能对我们的生存构成威胁。自然地，自然语言处理也不排除在这种推测之外，并且它是今天人工智能中最活跃的研究领域之一。</p><p id="a79f" class="pw-post-body-paragraph ks kt iq kv b kw kx kb ky kz la ke lb lp ld le lf lq lh li lj lr ll lm ln lo ij bi translated">迁移学习旨在利用不同背景下的现有知识——无论是不同的任务、语言还是领域——来帮助解决手头的问题。它受到人类学习方式的启发，因为我们通常不会针对任何给定的问题从零开始学习，而是建立在可能相关的先验知识上。例如，当一个人已经知道如何演奏另一种乐器时，学习演奏一种乐器被认为更容易。显然，乐器越相似，例如风琴和钢琴，先前的知识就越有用，学习新乐器就越容易。然而，即使乐器大不相同，比如鼓和钢琴，一些先前的知识仍然是有用的，即使不那么有用。在这个思维实验中，部分原因可能是因为坚持一个节奏是两种乐器共有的技能。</p><p id="c12b" class="pw-post-body-paragraph ks kt iq kv b kw kx kb ky kz la ke lb lp ld le lf lq lh li lj lr ll lm ln lo ij bi translated">大型研究实验室，如Lawrence Livermore国家实验室或Sandia国家实验室，以及大型互联网公司，如谷歌和脸书，能够通过对数十亿个单词和数百万张图像训练非常深度的神经网络来学习非常大型的复杂模型。例如，谷歌的自然语言处理模型BERT】接受了来自英语维基百科(25亿个单词)和图书语料库(8亿个单词)的超过30亿个单词的预训练。同样，深度<em class="ku">卷积神经网络</em>(CNN)已经在ImageNet数据集的超过1400万张图像上进行了训练，学习到的参数已经被许多组织广泛外包。今天，从零开始训练这种模型所需的资源量对于神经网络的普通从业者来说通常是不可用的，例如在较小企业工作的NLP工程师、较小学校的学生等。这是否意味着较小的参与者无法在他们的问题上取得最先进的成果？令人欣慰的是，如果应用正确，迁移学习的概念有望减轻这种担忧。</p><h2 id="fb03" class="ms mt iq bd mu mv mw dn mx my mz dp na lp nb nc nd lq ne nf ng lr nh ni nj iw bi translated">迁移学习为什么重要？</h2><p id="c790" class="pw-post-body-paragraph ks kt iq kv b kw nk kb ky kz nl ke lb lp nm le lf lq nn li lj lr no lm ln lo ij bi translated">迁移学习使您能够将从一组任务和/或领域获得的知识调整或转移到另一组任务和/或领域。这意味着，一个用海量资源训练出来的模型——包括数据、计算能力、时间、成本等。—一旦开源，更广泛的工程社区就可以对其进行微调，并在新的环境中重新使用，而所需资源只是原始资源的一小部分。这是NLP民主化的一大步，更广泛地说，是AI民主化的一大步。图2展示了这个范例，以学习如何演奏乐器为例。从图中可以看出，不同任务/域之间的信息共享可以导致后面的或<em class="ku">下游</em>任务b实现相同性能所需的数据减少</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi np"><img src="../Images/5ab6ae9860d3bc3edbebc9e20cb56945.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A5On30ws8wfz_Ghq6V715w.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">图二。转让好处的说明。收入模式，显示在底部面板中，其中信息在为不同任务/领域训练的系统之间共享，而传统模式，显示在顶部面板中，其中训练在任务/领域之间并行发生。在迁移学习范式中，通过信息/知识共享可以减少数据和计算需求。例如，如果一个人先知道如何弹钢琴，我们期望他更容易学会打鼓。(图片作者，来自<a class="ae mr" href="https://www.manning.com/books/transfer-learning-for-natural-language-processing" rel="noopener ugc nofollow" target="_blank"> <em class="jw">《自然语言处理的迁移学习》</em> </a>，灵感来自<a class="ae mr" rel="noopener" target="_blank" href="/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a">图3本文</a>)</p></figure><h2 id="67e1" class="ms mt iq bd mu mv mw dn mx my mz dp na lp nb nc nd lq ne nf ng lr nh ni nj iw bi translated">NLP迁移学习的最新进展</h2><p id="5f84" class="pw-post-body-paragraph ks kt iq kv b kw nk kb ky kz nl ke lb lp nm le lf lq nn li lj lr no lm ln lo ij bi translated">传统上，对于任何给定的问题设置(任务、领域和语言的特定组合)，学习都是在完全监督或完全无监督的方式下从零开始进行的。早在1999年，在<em class="ku">支持向量机</em> (SVMs)的背景下，半监督学习就被认为是一种解决潜在的有限标记数据可用性的方法。对较大的未标记数据集合的初始无监督预训练步骤使得下游监督学习更容易。这种方法的变体研究如何处理潜在的噪音，即可能不正确的标签——这种方法有时被称为<em class="ku">弱监督学习</em>。然而，通常假设标记和未标记数据集具有相同的采样分布。</p><p id="042b" class="pw-post-body-paragraph ks kt iq kv b kw kx kb ky kz la ke lb lp ld le lf lq lh li lj lr ll lm ln lo ij bi translated">迁移学习放松了这些假设。迁移学习的需要在1995年被公认为是“学会学习”的需要。NeurIPS可能是机器学习领域最大的会议。本质上，它规定智能机器需要拥有终身学习的能力，将学到的知识重新用于新的任务。它后来被冠以几个不同的名称进行研究，包括<em class="ku">学会学习</em>、<em class="ku">知识转移</em>、<em class="ku">归纳偏差</em>、<em class="ku">多任务学习</em>等。在多任务学习中，算法被训练成同时在多个任务上表现良好，从而揭示可能更普遍有用的特征。然而，直到2018年左右，人们才开发出实用且可扩展的方法，在NLP中实现最难的感知问题。</p><p id="5a83" class="pw-post-body-paragraph ks kt iq kv b kw kx kb ky kz la ke lb lp ld le lf lq lh li lj lr ll lm ln lo ij bi translated">2018年见证了自然语言处理领域的一场革命。对如何最好地将文本集合表示为向量的理解发生了巨大的变化。此外，开源模型可以被微调或转移到不同的任务、语言和领域，这一点得到了广泛的认可。与此同时，几个大的互联网公司发布了更多更大的NLP模型来计算这种表示，他们还指定了定义良好的程序来微调它们。突然之间，普通的从业者，甚至是一个独立的从业者，都有能力在NLP中获得最先进的结果。它甚至被广泛称为NLP的“ImageNet时刻”，指的是2012年后计算机视觉应用的爆炸，当时一个经过GPU训练的神经网络赢得了ImageNet计算机视觉竞赛。就像最初的ImageNet moment一样，预训练模型库第一次可以用于任意NLP数据的大型子集，以及定义良好的技术，用于微调它们以适应手头的特定任务，其标注数据集的大小远远小于其他情况下所需的大小。这本书的目的是描述、阐明、评估、论证性地应用、比较和对比属于这一类别的各种技术。接下来我们简要概述这些技术。</p><p id="1338" class="pw-post-body-paragraph ks kt iq kv b kw kx kb ky kz la ke lb lp ld le lf lq lh li lj lr ll lm ln lo ij bi translated">对自然语言处理的迁移学习的早期探索集中在对计算机视觉的类比上，它已经成功地应用了十多年。一个这样的模型— <a class="ae mr" href="https://arxiv.org/pdf/1901.08456.pdf" rel="noopener ugc nofollow" target="_blank">本体建模的语义推理(SIMOn) </a> — SIMOn采用字符级CNN结合双向<em class="ku">长短期记忆网络</em> (LSTMs)进行结构化语义文本分类。它展示了NLP迁移学习方法，与计算机视觉中使用的方法直接类似。大量关于计算机视觉应用的迁移学习的知识推动了这种方法。通过该模型学习的特征被证明对于无监督的学习任务也是有用的，并且在社交媒体语言数据上工作得很好，这些数据可能有些特殊，并且与维基百科和其他基于书籍的大型数据集上的语言非常不同。</p><p id="64cc" class="pw-post-body-paragraph ks kt iq kv b kw kx kb ky kz la ke lb lp ld le lf lq lh li lj lr ll lm ln lo ij bi translated">word2vec最初表述的一个显著缺点是歧义。没有办法区分一个词的各种用法，这些用法根据上下文可能有不同的含义，即同形异义词:例如，鸭子(姿势)对鸭子(鸟)，公平(聚会)对公平(公正)。在某种意义上，原始word2vec公式通过表示单应词的这些不同含义的向量的平均向量来表示每个这样的词。<a class="ae mr" href="https://arxiv.org/abs/1802.05365" rel="noopener ugc nofollow" target="_blank"> <em class="ku">来自语言模型的嵌入</em></a>——在流行的芝麻街角色后缩写为ELMo是使用双向LSTMs开发单词的语境化嵌入的尝试。ELMo模型的高级架构图如图3所示。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi nq"><img src="../Images/2347d657ab70595126d07ae47ab8024a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DcaITVXD2WowrGzWOxTR0Q.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">图3。在文档分类示例的上下文中可视化ELMo架构。(图片作者，来自<a class="ae mr" href="https://www.manning.com/books/transfer-learning-for-natural-language-processing" rel="noopener ugc nofollow" target="_blank"> <em class="jw">【自然语言处理的迁移学习】</em> </a>)</p></figure><p id="5bd3" class="pw-post-body-paragraph ks kt iq kv b kw kx kb ky kz la ke lb lp ld le lf lq lh li lj lr ll lm ln lo ij bi translated">单词在该模型中的嵌入在很大程度上取决于其上下文，对于每个这样的上下文，相应的数字表示是不同的。ELMo通过训练来预测单词序列中的下一个单词，这是语言建模领域的一项重要任务。巨大的数据集，例如维基百科和各种书籍的数据集，在这个框架中很容易用于训练。</p><p id="f2d8" class="pw-post-body-paragraph ks kt iq kv b kw kx kb ky kz la ke lb lp ld le lf lq lh li lj lr ll lm ln lo ij bi translated"><a class="ae mr" href="https://arxiv.org/abs/1801.06146" rel="noopener ugc nofollow" target="_blank"><em class="ku"/></a>(ULM-FiT)通用语言模型微调(Universal Language Model Fine-tuning)是一种被提出来为任何特定任务微调任何基于神经网络的语言模型的方法，最初在文本分类的上下文中被演示。这种方法背后的一个关键概念是<em class="ku">区别性微调</em>，其中网络的不同层以不同的速率进行训练。OpenAI <em class="ku">生成预训练转换器</em> (GPT)修改了转换器的编码器-解码器架构，以实现NLP的可微调语言模型。这个模型架构如图4所示。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi nr"><img src="../Images/35a6807ca46157339468df58b7ebf8dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BwY3Dhh9MRJGjsNFlG7yJw.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">图4。GPT架构的高级表示，显示了堆栈解码器、输入嵌入和位置编码。来自顶部的输出可以用于文本预测/生成和分类。(图片作者，来自<a class="ae mr" href="https://www.manning.com/books/transfer-learning-for-natural-language-processing" rel="noopener ugc nofollow" target="_blank"> <em class="jw">《自然语言处理的迁移学习》</em> </a>)</p></figure><p id="07f1" class="pw-post-body-paragraph ks kt iq kv b kw kx kb ky kz la ke lb lp ld le lf lq lh li lj lr ll lm ln lo ij bi translated">GPT丢弃了编码器，保留了解码器和它们的自我关注子层。<a class="ae mr" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> <em class="ku">来自变压器</em> </a> (BERT)的双向编码器表示可以说是相反的，通过保留编码器和丢弃解码器来修改变压器架构，还依赖于<em class="ku">屏蔽</em>单词，这些单词随后需要作为训练度量来准确预测。BERT如图5所示。</p><figure class="mc md me mf gt mg gh gi paragraph-image"><div role="button" tabindex="0" class="mh mi di mj bf mk"><div class="gh gi ns"><img src="../Images/d775f42630f02e0dea035e3897ce7b9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Lq5VSiCWI0SkmVJAdKq2g.png"/></div></div><p class="mn mo gj gh gi mp mq bd b be z dk translated">图5。BERT架构的高级表示，显示了堆栈编码器、输入嵌入和位置编码。在训练期间，来自top的输出被用于下一句预测和填空屏蔽语言建模目标。(图片作者，来自<a class="ae mr" href="https://www.manning.com/books/transfer-learning-for-natural-language-processing" rel="noopener ugc nofollow" target="_blank"> <em class="jw">《自然语言处理的迁移学习》</em> </a>)</p></figure><p id="f776" class="pw-post-body-paragraph ks kt iq kv b kw kx kb ky kz la ke lb lp ld le lf lq lh li lj lr ll lm ln lo ij bi translated">这些概念在书中进行了详细的讨论，在实践中的例子问题，如垃圾邮件检测，假新闻分类，列类型分类，聊天机器人，等等。</p><p id="1546" class="pw-post-body-paragraph ks kt iq kv b kw kx kb ky kz la ke lb lp ld le lf lq lh li lj lr ll lm ln lo ij bi translated">在所有这些基于语言模型的方法中——ELMo、ULM-FiT、GPT和BERT——都表明，生成的嵌入可以针对特定的下游NLP任务进行微调，只需要相对较少的标记数据点。对语言模型的关注是有意的；假设由它们诱导的假设集通常是有用的，并且已知大规模训练的数据是容易获得的。</p><p id="9925" class="pw-post-body-paragraph ks kt iq kv b kw kx kb ky kz la ke lb lp ld le lf lq lh li lj lr ll lm ln lo ij bi translated">从那时起，在前面描述的思想的基础上开发了各种新的模型——从旨在减少BERT的大小同时实现几乎相同的性能的模型，如ALBERT和DistilBERT，到旨在处理长文档的模型，如LongFormer和BigBird。</p><p id="74ef" class="pw-post-body-paragraph ks kt iq kv b kw kx kb ky kz la ke lb lp ld le lf lq lh li lj lr ll lm ln lo ij bi translated">NLP迁移学习领域目前是一个非常活跃和令人兴奋的领域，现在是学习如何利用它的好时机！</p></div><div class="ab cl nt nu hu nv" role="separator"><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny nz"/><span class="nw bw bk nx ny"/></div><div class="ij ik il im in"><p id="8873" class="pw-post-body-paragraph ks kt iq kv b kw kx kb ky kz la ke lb lp ld le lf lq lh li lj lr ll lm ln lo ij bi translated">要了解更多，请查看Sebastian Ruder的优秀博客和拥抱脸的优秀变形金刚库。也请查看我在<a class="ae mr" href="https://github.com/azunre/transfer-learning-for-nlp" rel="noopener ugc nofollow" target="_blank"> GitHub和Kaggle </a>上的代表性代码示例报告。</p><p id="3739" class="pw-post-body-paragraph ks kt iq kv b kw kx kb ky kz la ke lb lp ld le lf lq lh li lj lr ll lm ln lo ij bi translated">《自然语言处理的迁移学习》的硬拷贝和电子书版本可从<a class="ae mr" href="https://tinyurl.com/47p8zzrv" rel="noopener ugc nofollow" target="_blank">manning.com</a>、<a class="ae mr" href="https://www.amazon.com/Transfer-Learning-Natural-Language-Processing/dp/1617297267" rel="noopener ugc nofollow" target="_blank">亚马逊</a>、<a class="ae mr" href="https://www.barnesandnoble.com/w/transfer-learning-for-natural-language-processing-paul-azunre/1137938357" rel="noopener ugc nofollow" target="_blank">巴恩斯&amp;诺布尔</a>和<a class="ae mr" href="https://www.target.com/p/transfer-learning-for-natural-language-processing-by-paul-azunre-paperback/-/A-82704702" rel="noopener ugc nofollow" target="_blank">塔吉特</a>获得。</p></div></div>    
</body>
</html>