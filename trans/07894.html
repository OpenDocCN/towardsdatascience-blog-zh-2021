<html>
<head>
<title>Interpretable K-Means: Clusters Feature Importances</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可解释的K-Means:聚类特征重要性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/interpretable-k-means-clusters-feature-importances-7e516eeb8d3c?source=collection_archive---------0-----------------------#2021-07-20">https://towardsdatascience.com/interpretable-k-means-clusters-feature-importances-7e516eeb8d3c?source=collection_archive---------0-----------------------#2021-07-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="7b38" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/model-interpretability" rel="noopener" target="_blank">模型可解释性</a></h2><div class=""/><div class=""><h2 id="39fc" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">通过提取每个聚类最重要的特征来理解K均值聚类。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/75fc1905b1ac6434444287d8c34c2a4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ig8K3Iab5E7ahJQpn_pzdA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">[图片由作者制作]聚类的特征重要性方法</p></figure><p id="ed3e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi ma translated"><span class="l mb mc md bm me mf mg mh mi di"> M </span>机器学习模型要经过许多阶段才能被认为是生产就绪的。一个关键阶段是关键时刻，在这一时刻，模型获得了科学的许可；模型评估。许多评估指标被指定用于不同的目的和问题规范，但是没有一个是完美的。因此，当选择使用哪个评估指标最适合手头的问题并作为机器学习项目的功能决策诱导值时，数据科学家有一个巨大的负担。</p><p id="6e99" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">然而，一些评估度量值可能看起来不直观，并且不能用外行的术语来解释，尤其是在评估使用内部索引的无监督聚类算法时(不存在诸如真实标签之类的外部信息)。虽然这些测量对比较基准有很大的好处，但是将它们与解释结合起来对于独立、直观的验证和与利益相关者更容易的结果交流是至关重要的。</p><h1 id="605a" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">直觉与内部验证指标</h1><p id="fd78" class="pw-post-body-paragraph le lf iq lg b lh nb ka lj lk nc kd lm ln nd lp lq lr ne lt lu lv nf lx ly lz ij bi translated">当涉及到内部验证指标时，数据科学家往往会在评估过程中失去一个焦点，这就是对模型性能及其解释的直观“人类”理解。举个反例来说明，监督分类评价指标如<a class="ae ng" href="https://en.wikipedia.org/wiki/Accuracy_and_precision" rel="noopener ugc nofollow" target="_blank">准确率</a>、<a class="ae ng" href="https://en.wikipedia.org/wiki/Precision_and_recall" rel="noopener ugc nofollow" target="_blank">精度</a>或<a class="ae ng" href="https://en.wikipedia.org/wiki/Precision_and_recall" rel="noopener ugc nofollow" target="_blank">召回</a>对外行和专家都有非常直观的解释。例如:</p><ul class=""><li id="3164" class="nh ni iq lg b lh li lk ll ln nj lr nk lv nl lz nm nn no np bi translated"><strong class="lg ja">准确性</strong> : <em class="nq">“我们已经正确地对所有样本中的这个百分比的样本进行了分类。”</em></li><li id="cf57" class="nh ni iq lg b lh nr lk ns ln nt lr nu lv nv lz nm nn no np bi translated"><strong class="lg ja">精度</strong> : <em class="nq">“我们已经正确分类了所有预测阳性中的阳性百分比。”</em></li><li id="cff4" class="nh ni iq lg b lh nr lk ns ln nt lr nu lv nv lz nm nn no np bi translated"><strong class="lg ja">回想一下</strong> : <em class="nq">“我们已经正确分类了所有实际阳性中的阳性百分比。”</em></li></ul><p id="aa3a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">另一方面，内部验证指标，如广泛使用的<a class="ae ng" href="https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient" rel="noopener ugc nofollow" target="_blank">轮廓系数</a>、<a class="ae ng" href="https://scikit-learn.org/stable/modules/clustering.html#calinski-harabasz-index" rel="noopener ugc nofollow" target="_blank">卡林斯基-哈拉巴斯指数</a>、<a class="ae ng" href="https://scikit-learn.org/stable/modules/clustering.html#davies-bouldin-index" rel="noopener ugc nofollow" target="_blank">戴维斯-波尔丁指数</a>和许多其他指标，通常是比较使用，而不是固有的和独立的性能评估。这些测量值可以分解为每个聚类的紧密程度(聚类数据点彼此之间的相似程度)、聚类的分离程度(每个聚类的数据点与其他聚类的数据点之间的差异程度)，或者两者都有。但是，这些度量并不容易解释，也不容易与您正在解决的集群问题联系起来。</p><p id="a823" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">尽管这些方法非常有价值，但是当作为一名数据科学家试图了解您的模型有多好，或者试图将您的结果深入地传达给利益相关者(其中大多数内部验证指标都没有通过ELI5(解释为I 'm 5)测试)时，获得模型的解释以及内部评估指标是至关重要的。本文将关注最流行的无监督聚类算法之一；的K-手段，并提出了两种可能的技术来提取最重要的特征，为每个集群。文章的提纲如下:</p><ol class=""><li id="0c3c" class="nh ni iq lg b lh li lk ll ln nj lr nk lv nl lz nw nn no np bi translated"><a class="ae ng" href="#1db8" rel="noopener ugc nofollow"><strong class="lg ja">K-表示商业价值</strong> </a></li><li id="36e7" class="nh ni iq lg b lh nr lk ns ln nt lr nu lv nv lz nw nn no np bi translated"><a class="ae ng" href="#cae9" rel="noopener ugc nofollow"><strong class="lg ja">K-Means如何工作</strong> </a></li><li id="f8b4" class="nh ni iq lg b lh nr lk ns ln nt lr nu lv nv lz nw nn no np bi translated"><a class="ae ng" href="#7e14" rel="noopener ugc nofollow"> <strong class="lg ja">解读技巧</strong> </a></li><li id="d598" class="nh ni iq lg b lh nr lk ns ln nt lr nu lv nv lz nw nn no np bi translated"><a class="ae ng" href="#bf5c" rel="noopener ugc nofollow"> <strong class="lg ja">实战应用</strong> </a></li></ol><p id="37cd" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">如果您已经知道K-Means是如何工作的，请跳到<a class="ae ng" href="#7e14" rel="noopener ugc nofollow"> <strong class="lg ja">解释技术</strong> </a> <strong class="lg ja"> </strong>部分，或者想访问本文的知识库并直接使用代码，请访问<a class="ae ng" href="https://github.com/YousefGh/kmeans-feature-importance" rel="noopener ugc nofollow" target="_blank">K Means-feature-importance</a>。</p><h1 id="1db8" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">k-表示商业价值</h1><p id="4d06" class="pw-post-body-paragraph le lf iq lg b lh nb ka lj lk nc kd lm ln nd lp lq lr ne lt lu lv nf lx ly lz ij bi translated">假设你正在经营一家拥有成千上万客户的企业，你想更多地了解你的客户，尽管你有多少客户。举例来说，你不可能研究每一位顾客，并专门为他们制定营销活动。然而，您知道每个客户可能具有不同的人口统计(例如，年龄)、地理(例如，地区)、心理(例如，消费习惯)和行为(例如，参与度)属性。将他们分成多个相似的组将简化从每个组最普遍的属性来理解他们是谁。解决这个问题最流行的算法，我相信你也猜到了！是K-Means；“K”将指可能的客户细分/群体/集群的数量，“手段”可以被认为是K个群体中每个群体的中心(与其群体成员最相似)的虚构人物。</p><h1 id="cae9" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">K-Means如何工作</h1><p id="19e8" class="pw-post-body-paragraph le lf iq lg b lh nb ka lj lk nc kd lm ln nd lp lq lr ne lt lu lv nf lx ly lz ij bi translated">K-Means是一种无监督聚类算法，它将相似的数据样本与不相似的数据样本分组到一个组中。准确地说，它的目标是最小化类内平方和(WCSS)，从而最大化类间平方和(BCSS)。K-Means算法有不同的实现和概念变体，但是出于本文的考虑，我们将重点关注最常见的方法，即Lloyd's算法(Naive K-Means)，它遵循一种迭代方法来寻找次优解。在开始之前，我们将准备一个简单的数据集进行解释，并看看它是什么样子的。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure><div class="kp kq kr ks gt ab cb"><figure class="nz kt oa ob oc od oe paragraph-image"><img src="../Images/c77a5156b0b1fc942f3a88ab77cc5b64.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*eyUmdfj8GQ-x-BCEnvEkgA.png"/></figure><figure class="nz kt of ob oc od oe paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><img src="../Images/a31726b6ccdcbfe0ef273d399a5b3fee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1286/format:webp/1*AWhzKTTcuFCy9IfJrp-0uA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk og di oh oi translated">“df”数据框架视图</p></figure></div><p id="660d" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们需要使用K-Means将上述数据点聚类成K个组的步骤是:</p><h2 id="da2a" class="oj mk iq bd ml ok ol dn mp om on dp mt ln oo op mv lr oq or mx lv os ot mz iw bi translated">步骤1 <strong class="ak"> —选择组/簇的初始数量(K) </strong></h2><p id="a959" class="pw-post-body-paragraph le lf iq lg b lh nb ka lj lk nc kd lm ln nd lp lq lr ne lt lu lv nf lx ly lz ij bi translated">质心代表每个聚类；分配给该聚类所有数据点的平均值。选择初始组数等同于选择初始质心数k。我们可以决定<code class="fe ou ov ow ox b">K = 3</code>并从数据集中随机选择3个不同的数据点作为初始质心(有更好的方法来选择每个质心的初始坐标)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oy"><img src="../Images/f7c2665bf562a8b2ffbd1a66a1e192cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g1u4QqeZvAta4vCyOzcOkg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">[图片由作者制作]第一簇的质心初始化</p></figure><h2 id="6df7" class="oj mk iq bd ml ok ol dn mp om on dp mt ln oo op mv lr oq or mx lv os ot mz iw bi translated">步骤2-将数据点分配给聚类质心</h2><p id="8e36" class="pw-post-body-paragraph le lf iq lg b lh nb ka lj lk nc kd lm ln nd lp lq lr ne lt lu lv nf lx ly lz ij bi translated">我们必须将数据集中的每个数据点分配到最近的质心。我们将通过以下步骤使用最常见的距离度量作为“接近”的定义:</p><p id="a10b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">1.对于每个第<code class="fe ou ov ow ox b">j</code>个聚类质心<code class="fe ou ov ow ox b">C_j</code>，以及一个具有<code class="fe ou ov ow ox b">d</code>维度的数据集的一个点<code class="fe ou ov ow ox b">p</code>，使用下面的公式计算每个质心和所有数据点之间的欧几里德距离。<em class="nq">(我对</em> <code class="fe ou ov ow ox b"><em class="nq">C_j = u_c_j (u = average) = Cluster Centroid</em></code> <em class="nq">的使用只是一种简化，因为我将列出的方程都在单个聚类级别中)。</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oz"><img src="../Images/87a497a5ff4acb6b715c769ca24f19fd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ySEur9bMOnk77zbXru-__w.png"/></div></div></figure><p id="7bfc" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">2.为了最小化WCSS，我们将每个数据点分配到其最近的质心(最相似/最小距离)。这将是WCSS最小化步骤的原因来自于一个聚类的WCSS的等式，其中<code class="fe ou ov ow ox b">p_m </code>个点被分配给聚类质心<code class="fe ou ov ow ox b">C_j</code>，其中分配给聚类质心的点的距离越短，其WCSS越低。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pa"><img src="../Images/75caf7559433803cbb4d16836e5b763d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZGAe231pgSuMQiJ1q7l-zg.png"/></div></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pb"><img src="../Images/1ae89e9ab94bbb9be4d8cf5d66e6fc75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r51w60kO8mRKBqBJKq8kkg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">[图片由作者制作]数据点分配给聚类的质心</p></figure><h2 id="0c6e" class="oj mk iq bd ml ok ol dn mp om on dp mt ln oo op mv lr oq or mx lv os ot mz iw bi translated">步骤3-更新聚类质心的位置</h2><p id="f737" class="pw-post-body-paragraph le lf iq lg b lh nb ka lj lk nc kd lm ln nd lp lq lr ne lt lu lv nf lx ly lz ij bi translated">最后一步是更新聚类质心的位置，因为不同的数据点被分配给每个聚类。我们必须通过以下方式将每个质心移动到分配给它的所有数据点的平均值:</p><ol class=""><li id="6530" class="nh ni iq lg b lh li lk ll ln nj lr nk lv nl lz nw nn no np bi translated">计算每个聚类数据点的平均值</li><li id="cd2b" class="nh ni iq lg b lh nr lk ns ln nt lr nu lv nv lz nw nn no np bi translated">将新的聚类质心设置为每个聚类的新平均值</li><li id="51e8" class="nh ni iq lg b lh nr lk ns ln nt lr nu lv nv lz nw nn no np bi translated">重复<a class="ae ng" href="#6df7" rel="noopener ugc nofollow"> <strong class="lg ja">步骤2 </strong> </a>和<a class="ae ng" href="#0c6e" rel="noopener ugc nofollow"> <strong class="lg ja">步骤3 </strong> </a>直到聚类质心(新的均值)不变</li></ol><p id="1b25" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">利用<code class="fe ou ov ow ox b">sklearn.cluster.KMean</code>；我们将得出以下结论，例如，您可以采用<code class="fe ou ov ow ox b">Sample 0</code>和<code class="fe ou ov ow ox b">Sample 1</code>特征，计算它们的平均值，然后检查它是否等于<code class="fe ou ov ow ox b">Cluster Centroid D1</code>和<code class="fe ou ov ow ox b">Cluster Centroid D2</code>:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/c7e94a0d927874d192826c637211563b.png" data-original-src="https://miro.medium.com/v2/resize:fit:990/format:webp/1*Q2y1A2uBWfh09kE4EMxWXA.png"/></div></figure><p id="653b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">下图显示了K-Means如何对数据集进行聚类，其中每个质心位于分配给它的点的平均值处:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pd"><img src="../Images/638f37ff9fb32bbeb44ed6370908f77f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W0mw4Nw3i0LOGhjLqhTEdQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">[图片由作者制作] K-Means聚类应用于' df '数据框特征1和2</p></figure><h1 id="7e14" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">解释技术</h1><h2 id="1c27" class="oj mk iq bd ml ok ol dn mp om on dp mt ln oo op mv lr oq or mx lv os ot mz iw bi translated">方法1: WCSS最小化</h2><p id="dac3" class="pw-post-body-paragraph le lf iq lg b lh nb ka lj lk nc kd lm ln nd lp lq lr ne lt lu lv nf lx ly lz ij bi translated"><em class="nq">免责声明:作者开发了这个方法。请不要犹豫引用任何参考文献，如果发现或批评的方法。</em></p><p id="9e06" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这种方法是对每个质心的次优位置的直接分析。因为K-Means的目标是最小化组内平方和，并且假设所使用的距离度量是欧几里德距离:我们可以通过找到最大绝对质心维度移动来找到负责每个组的最高WCSS量(每个数据点到其组质心的距离的平方和)最小化的维度。</p><p id="60b4" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">使用上面相同的解释示例，我们可以从<code class="fe ou ov ow ox b">sklearn.cluster.KMean</code>拟合模型访问<code class="fe ou ov ow ox b">cluster_centers_</code>；最终的簇形心位置。然后显示特征名称(尺寸<code class="fe ou ov ow ox b">d</code>):</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="b194" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">绘制群集及其位置将产生以下结果:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pe"><img src="../Images/9c545b8a9f5278aee7186ff38b50e669.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6rxIK_134nGuuW88y4_z5Q.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">[图片由作者制作]K-表示'测向'数据帧聚类以及质心位置</p></figure><p id="0f5a" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">现在，让我们取每个质心，并按维度轴对其进行排序，如果有负值的要素，请记住取绝对值。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="3572" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">之后，我们可以使用排序的权重(质心在特征平面上移动的距离)获得第一个质心的维度，并将其映射到特征名称，这将为我们提供以下假设的特征重要性:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="6c15" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">因为质心在两个维度上移动了相等的距离，所以这些特征将具有同等的重要性。让我们在第一个维度上轻推最后两个样本(属于<code class="fe ou ov ow ox b">cluster 0</code>)并重复相同的过程，然后绘制新的质心</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pf"><img src="../Images/6c15d868b54c42bd18c322bda71edb74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o_NceWQge5DNiVAH9yZQzQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">[图片由作者制作] K-Means聚类在第一维度(特征1)上微移后用于聚类0</p></figure><p id="ef5b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们现在将再次应用相同的方法并提取特征重要性。请注意，聚类0在要素1上的移动比要素2多得多，因此对WCSS最小化有更大的影响。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure><h2 id="c11a" class="oj mk iq bd ml ok ol dn mp om on dp mt ln oo op mv lr oq or mx lv os ot mz iw bi translated">方法2:从无监督到有监督</h2><p id="1207" class="pw-post-body-paragraph le lf iq lg b lh nb ka lj lk nc kd lm ln nd lp lq lr ne lt lu lv nf lx ly lz ij bi translated">这种方法是模型不可知的；不排斥K-Means，在K-Means中，我们使用易于解释的分类器(如基于树的模型)将无监督聚类问题转换为一对一监督分类问题。完成此操作的步骤如下:</p><ol class=""><li id="7062" class="nh ni iq lg b lh li lk ll ln nj lr nk lv nl lz nw nn no np bi translated">将每个群集标签更改为一对一对所有二进制标签</li><li id="ea71" class="nh ni iq lg b lh nr lk ns ln nt lr nu lv nv lz nw nn no np bi translated">训练分类器以区分每个分类和所有其他分类</li><li id="e6c1" class="nh ni iq lg b lh nr lk ns ln nt lr nu lv nv lz nw nn no np bi translated">从模型中提取特征重要性(我们将使用<code class="fe ou ov ow ox b">sklearn.ensemble.RandomForestClassifier</code></li></ol><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pg"><img src="../Images/3cea4d4e2a50eb737e68ae2e34f5075e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*usaNAD229_lQrdIBu1QOGQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">[图片由作者制作]无监督聚类问题转化为一对所有监督分类问题的转换</p></figure><p id="7e70" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在步骤1之后，让我们将<code class="fe ou ov ow ox b">cluster 0</code>标签设置为<code class="fe ou ov ow ox b">1</code>，并将所有其他集群标签设置为<code class="fe ou ov ow ox b">0</code>:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="0f54" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们已经把这个问题转化为一个二元分类问题。剩下的就是训练一个分类器，并使用其在<code class="fe ou ov ow ox b">scikit-learn</code>中实现的<code class="fe ou ov ow ox b">feature_importances_</code>方法来获得在所有聚类和目标聚类之间具有最大区分能力的特征。我们还需要将它们映射到按权重排序的特性名称。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="f9e8" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">一个重要的注意事项是，这种方法发现了两个集群之间的区别，并且根本不是目标集群所固有的。此外，还有许多其他复杂的方法可以从基于树的模型和模型不可知的方法中提取最重要的特征，您可以尝试一下。</p><h1 id="bf5c" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">现实生活应用</h1><p id="3dd7" class="pw-post-body-paragraph le lf iq lg b lh nb ka lj lk nc kd lm ln nd lp lq lr ne lt lu lv nf lx ly lz ij bi translated">我选择将解释技术应用于NLP问题，因为我们可以很容易地将特征重要性(英语单词)联系起来，这可以被认为是一种基于组的关键字提取技术，其中我们的目标是使用K-Means将相似的文档聚集在一起，然后应用上述技术。我将使用的数据集可以在这里找到<a class="ae ng" href="https://www.kaggle.com/c/learn-ai-bbc/data?select=BBC+News+Train.csv" rel="noopener ugc nofollow" target="_blank"> Kaggle BBC-News </a>，这提出了一个分类问题。我们将首先排除category列(体育、商业、政治、娱乐和技术新闻文章),稍后将其用作概念验证。</p><p id="4ba9" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">新闻类别在数据集中的分布如下:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ph"><img src="../Images/aa8434f9c6af11851cd455b28247aa1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bs8P2JQ-quc_BU7P_OuIoA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">[图片由作者制作] BBC News train.csv数据集类别分布</p></figure><p id="d17c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">由于本文不是特定于NLP(自然语言处理)的，所以我不会深入讨论任何与NLP相关的任务。因此，快速转向预处理文本以准备特征。下面的代码对单词和过滤标记(单词和数字)进行规范化，这些单词和标记不具有区分能力，因此是多余的。然后计算整个数据集中提到最高的单词并绘制它们(<em class="nq">你可以跳过代码</em>):</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pi"><img src="../Images/1c6aa103bfdc642459064adbb2b7e074.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*x03WIW5ySIar-mTBQml1EA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">[图片由作者制作BBC新闻数据集中提及率最高的词</p></figure><p id="0556" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">使用TF-IDF(文本表示技术)，我们可以将分类变量(单词)转换成数字表示。我们不需要在这里缩放特征，因为TF-IDF在它的等式内归一化特征，并且它的输出应该以它的原始形式使用。如果对具有不同单位或范围的要素的数据集应用K均值，并且这种差异与问题无关，请记住缩放要素。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="eb3e" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">最后，现在是我们最在意的一步；我将上面的方法包装在一个继承自<code class="fe ou ov ow ox b">sklearn.cluster.KMean</code>的类中，除了添加了<code class="fe ou ov ow ox b">feature_importances_ </code>属性之外，可以以同样的方式使用。在初始化时，您必须为修改后的类的<code class="fe ou ov ow ox b">ordered_feature_names</code>参数提供与<code class="fe ou ov ow ox b">fit</code>方法中的<code class="fe ou ov ow ox b">X</code>参数排序相同的特性。</p><p id="f334" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">您可以在这里找到代码<a class="ae ng" href="https://github.com/YousefGh/kmeans-feature-importance" rel="noopener ugc nofollow" target="_blank">k means-feature-importance</a>并简单地在您最喜欢的CLI中克隆它，或者简单地通过访问存储库中的Colab示例来完成:</p><pre class="kp kq kr ks gt pj ox pk pl aw pm bi"><span id="1b1f" class="oj mk iq ox b gy pn po l pp pq">git clone <a class="ae ng" href="https://github.com/YousefGh/kmeans-feature-importance.git" rel="noopener ugc nofollow" target="_blank">https://github.com/YousefGh/kmeans-feature-importance.git</a></span></pre><p id="bc38" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">然后使用<code class="fe ou ov ow ox b">k</code> =新闻数据集类别的数量来运行修改后的KMeans类，以便我们稍后可以将结果与实际类别进行比较。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure><p id="11b6" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">让我们检查K-Means是否产生了类似于新闻数据集中的类别分布的聚类分布。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pr"><img src="../Images/5a5297ddca2dc4989ca8911af5a48774.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0XDUAZb0oURGgLPF6cY3sw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">[图片由作者制作]类别和K均值聚类分布</p></figure><p id="60de" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这是一个足够接近的类别分布相似性，让我们继续下去。不要忘记通过使用不同的内部验证索引来确保K-Means已经产生了准确的结果(我不会仔细检查它们，因为这超出了范围)，并且您可能没有真正的标签，所以如果K在问题领域知识中是未知的，您将需要选择K-Means中的最佳K。</p><p id="d240" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">继续解释，我们可以像这样访问第二个集群<code class="fe ou ov ow ox b">cluster 1</code>的<code class="fe ou ov ow ox b">feature_importances_</code>(这个例子是针对WCSS最小化者的):</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nx ny l"/></div></figure><h2 id="009a" class="oj mk iq bd ml ok ol dn mp om on dp mt ln oo op mv lr oq or mx lv os ot mz iw bi translated">方法比较</h2><p id="a151" class="pw-post-body-paragraph le lf iq lg b lh nb ka lj lk nc kd lm ln nd lp lq lr ne lt lu lv nf lx ly lz ij bi translated">我们将使用<code class="fe ou ov ow ox b">KMeanInterp</code>类中的<code class="fe ou ov ow ox b">feature_importance_method</code>参数比较WCSS最小化方法和无监督到有监督问题转换方法。流程如下所示:</p><ul class=""><li id="dcc3" class="nh ni iq lg b lh li lk ll ln nj lr nk lv nl lz nm nn no np bi translated">用于与独特颜色进行比较的绘图类别分布</li><li id="d7bd" class="nh ni iq lg b lh nr lk ns ln nt lr nu lv nv lz nm nn no np bi translated">将<code class="fe ou ov ow ox b">feature_importance_method</code>参数设置为<code class="fe ou ov ow ox b">wcss_min</code>，并绘制特征重要度</li><li id="74fe" class="nh ni iq lg b lh nr lk ns ln nt lr nu lv nv lz nm nn no np bi translated">将<code class="fe ou ov ow ox b">feature_importance_method</code>参数设置为<code class="fe ou ov ow ox b">unsup2sup</code>并绘制特征重要性</li><li id="6551" class="nh ni iq lg b lh nr lk ns ln nt lr nu lv nv lz nm nn no np bi translated">使用最重要的特征推断每个聚类的类别</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ph"><img src="../Images/aa8434f9c6af11851cd455b28247aa1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Bs8P2JQ-quc_BU7P_OuIoA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">[图片由作者制作] BBC News train.csv数据集类别分布。下面将使用颜色进行比较</p></figure><p id="278b" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja"> WCSS最小化器</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ps"><img src="../Images/bf81795550ddc579a596664ae54e31f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lTbxDCYR3rR4wg1i59HssA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">[图片由作者制作]对每个聚类使用<strong class="bd pt"> WCSS极小值法</strong>的最重要特征，并直观地将每个聚类映射到实际数据集中可能的类别(每个图的右上角)</p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pu"><img src="../Images/e5d476d5ec04835461dca0024c55e2c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*leSpB8vuHTFoQEIit3PtaQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">[图片由作者制作]最重要的特征使用<strong class="bd pt"> WCSS最小化器</strong>方法对每个聚类进行分析，并直观地将每个聚类映射到实际数据集中可能的类别(右上角)</p></figure><p id="c0a2" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">无监督到有监督</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pv"><img src="../Images/cd78c31de5d715190f36acdfb945a004.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_4oTkXX-GyrYI4mebZAbAA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">[图片由作者制作]对每个聚类使用<strong class="bd pt">非监督到监督</strong>方法的最重要特征，并直观地将每个聚类映射到实际数据集中可能的类别(每个图的右上角)</p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi pw"><img src="../Images/e39bafb03c83a68329ed43ac44d1954c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OrtweypJn69dK-EDXvRncQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">[图片由作者制作]对每个聚类使用<strong class="bd pt">非监督到监督</strong>方法的最重要特征，并直观地将每个聚类映射到实际数据集中可能的类别(右上角)</p></figure><h1 id="91db" class="mj mk iq bd ml mm mn mo mp mq mr ms mt kf mu kg mv ki mw kj mx kl my km mz na bi translated">结论</h1><p id="36aa" class="pw-post-body-paragraph le lf iq lg b lh nb ka lj lk nc kd lm ln nd lp lq lr ne lt lu lv nf lx ly lz ij bi translated">当事实标签在开发阶段不可用时，聚类的可解释性变得至关重要。由于内部验证指标的性质，它不仅阻止了数据科学家对聚类有效性的直接评估，还妨碍了向利益相关者简单直观地解释聚类性能。我们提出了两种可能的方法，旨在通过提取基于聚类的特征重要性来解决这一问题，这使我们能够知道为什么K-Means算法选择了每个聚类。该方法将其自身扩展到利益相关者通信、简单直观的评估、NLP中基于聚类的关键词提取以及通用特征选择技术。</p></div><div class="ab cl px py hu pz" role="separator"><span class="qa bw bk qb qc qd"/><span class="qa bw bk qb qc qd"/><span class="qa bw bk qb qc"/></div><div class="ij ik il im in"><p id="d8f4" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><em class="nq">本文笔记本，</em> <code class="fe ou ov ow ox b"><em class="nq">KMeansInterp</em></code> <em class="nq">类，连同Colab上的直接用法示例，可在这里找到</em><a class="ae ng" href="https://github.com/YousefGh/kmeans-feature-importance" rel="noopener ugc nofollow" target="_blank"><strong class="lg ja"><em class="nq"/></strong></a><strong class="lg ja"><em class="nq">。</em> </strong> <em class="nq">快乐演绎！</em></p></div><div class="ab cl px py hu pz" role="separator"><span class="qa bw bk qb qc qd"/><span class="qa bw bk qb qc qd"/><span class="qa bw bk qb qc"/></div><div class="ij ik il im in"><p id="583c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated"><strong class="lg ja">参考文献:</strong></p><ol class=""><li id="be7a" class="nh ni iq lg b lh li lk ll ln nj lr nk lv nl lz nw nn no np bi translated">Y.刘，李，熊，高，吴，“内部聚类验证方法的理解”，2010年IEEE数据挖掘国际会议，2010年，第911-916页，doi: 10.1109/ICDM.2010.35。</li><li id="1828" class="nh ni iq lg b lh nr lk ns ln nt lr nu lv nv lz nw nn no np bi translated">惠普克里格尔。运行时评估的(黑色)艺术:我们是在比较算法还是实现？。已知信息系统52，341–378(2017年)。<a class="ae ng" href="https://doi.org/10.1007/s10115-016-1004-2" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1007/s10115-016-1004-2</a></li><li id="0143" class="nh ni iq lg b lh nr lk ns ln nt lr nu lv nv lz nw nn no np bi translated">Ng，a .，，皮赫，C. (2021)。CS221。检索于2021年7月18日，来自https://stanford.edu/~cpiech/cs221/handouts/kmeans.html<a class="ae ng" href="https://stanford.edu/~cpiech/cs221/handouts/kmeans.html" rel="noopener ugc nofollow" target="_blank"/></li><li id="68be" class="nh ni iq lg b lh nr lk ns ln nt lr nu lv nv lz nw nn no np bi translated">伊斯梅利，乌迈马&amp;勒梅尔，文森特&amp;科尼埃·霍斯，安托万。(2014).测量变量对聚类贡献的监督方法。159–166.10.1007/978–3–319–12637–1_20.</li><li id="7fb7" class="nh ni iq lg b lh nr lk ns ln nt lr nu lv nv lz nw nn no np bi translated"><a class="ae ng" href="https://scikit-learn.org/stable/modules/clustering.html" rel="noopener ugc nofollow" target="_blank"> 2.3。聚类-sci kit-learn 0 . 24 . 2文档</a>，2021</li></ol></div></div>    
</body>
</html>