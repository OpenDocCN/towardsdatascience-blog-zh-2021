<html>
<head>
<title>How close is GPT-3 to Artificial General Intelligence?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">GPT 3号离人工通用智能有多近？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-close-is-gpt-3-to-artificial-general-intelligence-cb057a8c503d?source=collection_archive---------15-----------------------#2021-06-14">https://towardsdatascience.com/how-close-is-gpt-3-to-artificial-general-intelligence-cb057a8c503d?source=collection_archive---------15-----------------------#2021-06-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><p id="a2f2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">GPT-3是仿人自然语言表现的良好开端。或许更好的类比可能是人工通用智能自然语言的“能人”<strong class="js iu">【1】</strong>。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/7339fbb5a052bebffbc9d4a9f8c83e69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UwxYBR-O-AEFtMk-fWVVmw.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">语言推理的漩涡。<a class="ae le" href="https://unsplash.com/@usgs?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">美国地质勘探局</a>在<a class="ae le" href="https://unsplash.com/s/photos/swirling?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="0795" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><em class="lf">能人</em>物种区别于更早的<em class="lf">南方古猿</em>群体，它们有稍微大一点的脑壳和更小的脸和牙齿。然而，<em class="lf">能人</em>物种保留了一些类人猿的特征[1]。</p><p id="4b48" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">最重要的是，对于我将要做的类比，<em class="lf">能人</em>物种被认为是第一个制造石器的人。</p><p id="82c2" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">在我的类比中，GPT-3代表了自然语言人工智能的能人。以下是我的推理:</p><ul class=""><li id="a48e" class="lg lh it js b jt ju jx jy kb li kf lj kj lk kn ll lm ln lo bi translated">GPT-3 <em class="lf"> </em>的大脑(参数)明显比之前的NLP型号更大；</li><li id="36b4" class="lg lh it js b jt lp jx lq kb lr kf ls kj lt kn ll lm ln lo bi translated">GPT-3使用工具，例如，转移学习和微调。</li><li id="a164" class="lg lh it js b jt lp jx lq kb lr kf ls kj lt kn ll lm ln lo bi translated">GPT-3完成其他NLP任务(制造工具),它没有接受过这方面的训练。</li><li id="e98e" class="lg lh it js b jt lp jx lq kb lr kf ls kj lt kn ll lm ln lo bi translated">GPT 3号保留了GPT 2号的大部分架构。与能人物种类似，GPT 3号保留了“一些类人猿特征”。</li></ul></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h1 id="4ec4" class="mb mc it bd md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my bi translated">关键要点:是什么让GPT-3如此特别？</h1><p id="b9a7" class="pw-post-body-paragraph jq jr it js b jt mz jv jw jx na jz ka kb nb kd ke kf nc kh ki kj nd kl km kn im bi translated">GPT-3(生成式预训练变压器-3)是OpenAI迄今为止最大规模的自然语言预测(NLP)模型(2020年6月向公众开放)。</p><ol class=""><li id="95c0" class="lg lh it js b jt ju jx jy kb li kf lj kj lk kn ne lm ln lo bi translated">GPT 3号大约有1850亿个参数。相比之下，人类大脑大约有860亿个神经元，每个神经元平均有7000个突触[2，3]；</li><li id="ff65" class="lg lh it js b jt lp jx lq kb lr kf ls kj lt kn ne lm ln lo bi translated">比较苹果和橙子，人类大脑有大约60万亿个参数，比GPT-3多300倍。注意:如果自然语言任务需要10%的人脑容量，那么人脑的参数比GPT-3多30倍。</li><li id="4bd7" class="lg lh it js b jt lp jx lq kb lr kf ls kj lt kn ne lm ln lo bi translated">据估计，GPT-3的云计算时间成本在400万美元到1200万美元之间，并且需要几个月的时间进行培训[3，7]。OpenAI没有说GPT-3的训练成本是多少，也不清楚他们是否知道20%以内。然而，该论文的作者计数是31名工作人员[3]。这意味着一年至少要增加1200万美元的员工工资和福利。</li><li id="4ae4" class="lg lh it js b jt lp jx lq kb lr kf ls kj lt kn ne lm ln lo bi translated">GPT-3在总计约5000亿个标记(一个标记近似于一个单词)的几个大型文本语料库上进行训练[3]。</li></ol><blockquote class="nf"><p id="c056" class="ng nh it bd ni nj nk nl nm nn no kn dk translated">他们已经走得够远了！他们去建了一座七层楼高的摩天大楼，大约和一栋建筑物一样高。—俄克拉荷马州堪萨斯城的歌词子集[9]</p></blockquote><blockquote class="np nq nr"><p id="f79f" class="jq jr lf js b jt ns jv jw jx nt jz ka nu nv kd ke nw nx kh ki ny nz kl km kn im bi translated">注:迪拜塔摩天大楼目前有183层高。</p></blockquote></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h1 id="f547" class="mb mc it bd md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my bi translated">GPT-3站在先前工作的肩膀上</h1><ol class=""><li id="f5c4" class="lg lh it js b jt mz jx na kb oa kf ob kj oc kn ne lm ln lo bi translated">GPT-2是一个大型的基于T2转换器T3的语言模型，拥有15亿个参数，在800万个网页的数据集上进行训练。GPT-2将GPT-1模型架构的参数扩大了约10倍，并对超过10倍的数据量进行了训练。</li><li id="ea40" class="lg lh it js b jt lp jx lq kb lr kf ls kj lt kn ne lm ln lo bi translated">GPT-2最初的研究目标是在40GB的互联网文本上进行无监督训练后预测句子中的下一个单词[4]。GPT-2可能会也可能不会最初让OpenAI的研究人员惊讶地发现，它可以执行与它训练的不同的NLP任务[6]。我毫不怀疑GPT-2为OpenAI与GPT-3一起“做大”奠定了基础。</li><li id="a103" class="lg lh it js b jt lp jx lq kb lr kf ls kj lt kn ne lm ln lo bi translated">GPT-2模型和其他SOTA(国家最先进的)自然语言处理模型仍然不能执行一个新的语言任务，从只有几个例子(称为少数镜头学习)或从简单的自然语言指令描述的任务[3]。</li><li id="ad14" class="lg lh it js b jt lp jx lq kb lr kf ls kj lt kn ne lm ln lo bi translated">GPT-3将GPT-2模型架构放大了约100倍。如果你按页数或作者数计算，这是相当可观的5倍多的努力[3，5]。</li><li id="2464" class="lg lh it js b jt lp jx lq kb lr kf ls kj lt kn ne lm ln lo bi translated">GPT-3采用了ULMFIT的关键发现。在大型文本语料库上预先训练NLP模型，然后在特定任务上进行微调，在许多NLP任务和基准上实现了实质性的收益[4]。</li><li id="16d6" class="lg lh it js b jt lp jx lq kb lr kf ls kj lt kn ne lm ln lo bi translated">GPT-3的研究人员表明<a class="ae le" href="https://chatbotslife.com/tagged/artificial-intelligence" rel="noopener ugc nofollow" target="_blank"> NLP </a>的性能(以交叉熵验证损失衡量)与NLP模型的大小(参数数量)成正比[3]。GPT-3可以说是迄今为止最好的NLP模型。</li><li id="d834" class="lg lh it js b jt lp jx lq kb lr kf ls kj lt kn ne lm ln lo bi translated">再次声明，由于这是一个至关重要的发现，研究人员通过GPT-3训练表明，扩大语言模型可以显著提高任务不可知、少数镜头的性能，有时甚至达到与以前的SOTA方法相当的水平[3]。</li><li id="eefe" class="lg lh it js b jt lp jx lq kb lr kf ls kj lt kn ne lm ln lo bi translated">GPT-3可以在没有任何神经网络梯度更新或微调的情况下应用[3]。</li><li id="34bf" class="lg lh it js b jt lp jx lq kb lr kf ls kj lt kn ne lm ln lo bi translated">GPT-3论文的结论是，17.8亿个参数模型在一些少量学习任务中不够准确[3]。</li></ol><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi od"><img src="../Images/9c69e0baceebb00b9e83e12d4ad9a32b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QrwltjgAJBQHap16_fPlJQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">研究人员训练了一系列较小的GPT-3模型，其变化范围从1.25亿个参数到1，300万个参数。零炮、单炮和少炮性能之间的精度差距随着模型参数的大小而扩大。OpenAI的研究人员坚持认为，他们的研究表明，更大的模型和更多的数据输入将是适应性更强的通用语言系统的成功之路。来源:https://arxiv.org/pdf/2005.14165.pdf</p></figure><p id="0db6" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">GPT-3是迄今为止最好的NLP模型吗，因为它是最大的？</p><p id="1d20" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><strong class="js iu"> <em class="lf">注</em></strong><em class="lf">:2021年6月12日，北京人工智能研究院(BAAI)近日</em> <a class="ae le" href="https://syncedreview.com/2021/03/23/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0/" rel="noopener ugc nofollow" target="_blank"> <em class="lf">发布了关于其“悟道”AI系统的详细信息</em> </a> <em class="lf">。据悉有</em> <a class="ae le" href="https://syncedreview.com/2021/03/23/chinas-gpt-3-baai-introduces-superscale-intelligence-model-wu-dao-1-0/" rel="noopener ugc nofollow" target="_blank"> <em class="lf"> 175万亿</em> </a> <em class="lf">参数。据报道，吴导在文本分类、情感分析、自然语言推理、阅读理解等方面超过了人类的平均水平。我提醒读者注意上述声明，直到BAAI提供其他细节，如使用的硬件，训练时间和使用的训练数据。据报道，该代码是开源的。</em></p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h1 id="7836" class="mb mc it bd md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my bi translated">GPT-3的第一个例子</h1><p id="0a58" class="pw-post-body-paragraph jq jr it js b jt mz jv jw jx na jz ka kb nb kd ke kf nc kh ki kj nd kl km kn im bi translated">GPT-3在需要即时推理或领域适应的任务上表现出色，如解读单词，在句子中使用新词，或执行3位数算术[3]。</p><p id="cdcd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">GPT-3模型生成的新闻文章样本，人类评估者很难从人类撰写的文本中区分出来。增加参数的数量增加了人类读者的难度，如下图所示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oe"><img src="../Images/32ff44bafb6ed011e336b8b1229a3da6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yGe7SIi2uT_LrwIdxS97yw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">来源:[3]<a class="ae le" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/2005.14165.pdf</a></p></figure><p id="6ec7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">GPT以华莱士·斯蒂文斯的风格创作诗歌。我的反应很奇怪，我承认，有点害怕。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi of"><img src="../Images/461653a4d6119cd325eabe9b60508473.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_LfCzzGy-hNc1T71zoAkIA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">来源:https://arxiv.org/pdf/2005.14165.pdf</p></figure><p id="0dfa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">2020年6月，OpenAI发布了它为访问不同OpenAI模型而开发的<a class="ae le" href="https://beta.openai.com/" rel="noopener ugc nofollow" target="_blank"> API </a>。该API运行预训练的GPT-3模型系列，用于各种NLP任务[3]。</p><p id="ce5d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">与通常的人工智能社区实践不同，GPT-3模型的权重不向公众公布。</p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h1 id="1611" class="mb mc it bd md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my bi translated">结论</h1><p id="2823" class="pw-post-body-paragraph jq jr it js b jt mz jv jw jx na jz ka kb nb kd ke kf nc kh ki kj nd kl km kn im bi translated">OpenAI长期以来一直声称，巨大的计算能力和强化学习是通往AGI或可以学习人类可以完成的任何任务的人工智能的必要一步[14]。</p><p id="e48a" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">人工智能2.0的创始人，如Yoshua Bengio和Yann LeCun，<a class="ae le" href="https://venturebeat.com/2020/05/02/yann-lecun-and-yoshua-bengio-self-supervised-learning-is-the-key-to-human-level-intelligence/" rel="noopener ugc nofollow" target="_blank">认为从目前的</a><a class="ae le" href="https://becominghuman.ai/" rel="noopener ugc nofollow" target="_blank">人工智能</a>技术不可能创造出AGI。他们认为我们需要自我监督学习(实际上GPT-2和GPT-3是自我监督的)和先进的基于神经生物学的进步[15]。</p><p id="4829" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">然而，人工智能1.0的父亲，人工智能的祖父，如马文·明斯基和安德鲁·麦卡锡，认为丰富的知识(数据)和常识推理专家的“社会”是通往AGI的道路[16]。</p><p id="9fd3" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">GPT-3是一个存在的证据，它证明了扩大文本(数据)的数量，扩大参数(模型大小)，以及扩大训练计算的结果在一个专家身上有更好的准确性(可怕的性能)来完成少量的<a class="ae le" href="https://chatbotslife.com/ultimate-guide-to-leveraging-nlp-machine-learning-for-you-chatbot-531ff2dd870c" rel="noopener ugc nofollow" target="_blank"> NLP </a>任务。</p><p id="1b69" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">模型的架构、模型的大小和训练计算机的数量实现了常识推理专家吗？数据和常识推理能让我们到达AGI吗？</p><h2 id="6790" class="og mc it bd md oh oi dn mh oj ok dp ml kb ol om mp kf on oo mt kj op oq mx or bi translated">对人工智能可能未来的推测</h2><blockquote class="nf"><p id="c22f" class="ng nh it bd ni nj os ot ou ov ow kn dk translated">所以，我认为人工智能研究者犯的最大错误是假设他们是聪明的。是的，和人工智能相比，他们不是。—埃隆·马斯克[12]。</p></blockquote><p id="059f" class="pw-post-body-paragraph jq jr it js b jt ns jv jw jx nt jz ka kb nv kd ke kf nx kh ki kj nz kl km kn im bi translated">60到65年前，第一台计算机装满了一个房间。60年后，一个计算机核心，大约有我的头那么大，已经放大了第一台计算机的大约10亿倍(可能更多)。</p><p id="3db8" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">假设第一台可行的量子计算机充满了整个房间。60年后，我脑袋大小的量子计算机核心将会放大到第一台量子计算机的10亿倍吗？</p><p id="02cd" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">也许吧。</p><p id="0baa" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">想象一台量子计算机，其AGI(人工通用智能)模型的规模是GPT-3参数的10亿倍，或人脑参数的约300万倍。</p><blockquote class="nf"><p id="e6e9" class="ng nh it bd ni nj os ot ou ov ow kn dk translated">“我预测在2029年，我们将通过图灵测试，”雷·库兹韦尔说[11]。</p></blockquote><p id="7351" class="pw-post-body-paragraph jq jr it js b jt ns jv jw jx nt jz ka kb nv kd ke kf nx kh ki kj nz kl km kn im bi translated">注意:GPT 3号非常接近GPT 3号通过图灵测试[13]。</p><blockquote class="nf"><p id="8ab8" class="ng nh it bd ni nj os ot ou ov ow kn dk translated">GPT 3号在某些方面给人留下了深刻的印象，但在其他方面却明显不如人类。——凯文·莱基，Just，2020。</p></blockquote><p id="150f" class="pw-post-body-paragraph jq jr it js b jt ns jv jw jx nt jz ka kb nv kd ke kf nx kh ki kj nz kl km kn im bi translated">你认为我们会做一个霍金-马斯克的噩梦或者哈文斯-库兹韦尔的梦吗？</p><p id="f9f7" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们可能两者都有，也可能都没有。</p><p id="413f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我把钱投在我们的工具制造上。我怀疑我们会改变，或者我们应该改变这种行为。</p><p id="6918" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我觉得埃隆·马斯克的NuralLink项目是在我们的工具制造上赌人工智能的未来潜力[17]。</p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h1 id="51d6" class="mb mc it bd md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt mu mv mw mx my bi translated">资源</h1><p id="73c8" class="pw-post-body-paragraph jq jr it js b jt mz jv jw jx na jz ka kb nb kd ke kf nc kh ki kj nd kl km kn im bi translated">[1] <a class="ae le" href="https://humanorigins.si.edu/evidence/human-family-tree" rel="noopener ugc nofollow" target="_blank">成为人意味着什么？</a></p><p id="ec6e" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">人类大脑的规模。</p><p id="33ff" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[3] <a class="ae le" href="https://arxiv.org/pdf/2005.14165.pdf" rel="noopener ugc nofollow" target="_blank">语言模型是很少出手的学习者</a>。</p><p id="6164" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae le" href="https://arxiv.org/pdf/1801.06146.pdf" rel="noopener ugc nofollow" target="_blank"> [4]文本分类通用语言模型微调</a>。</p><p id="853d" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[5] <a class="ae le" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf" rel="noopener ugc nofollow" target="_blank">语言模型是无人监督的多任务学习者</a>。</p><p id="4c3f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[6] <a class="ae le" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank">更好的语言模型及其含义。</a></p><p id="c83c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[7] <a class="ae le" href="https://venturebeat.com/2020/06/01/ai-machine-learning-openai-gpt-3-size-isnt-everything/" rel="noopener ugc nofollow" target="_blank"> OpenAI庞大的GPT-3模型令人印象深刻，但大小并不代表一切。</a></p><p id="4387" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated"><a class="ae le" href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="noopener ugc nofollow" target="_blank">【8】通过生成性预训练提高语言理解</a>。</p><p id="472b" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">【9】<a class="ae le" href="https://www.allmusicals.com/lyrics/oklahoma/kansascity.htm" rel="noopener ugc nofollow" target="_blank">堪萨斯城的歌词出自戏剧《俄克拉荷马》。</a></p><p id="98f1" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[10] <a class="ae le" href="https://www.skyscrapercenter.com/building/burj-khalifa/3" rel="noopener ugc nofollow" target="_blank">哈利法塔摩天大楼。凯文·莱基</a></p><p id="80ce" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">未来比你想象的更好:雷·库兹韦尔对人工智能和发展的预测。</p><p id="97d9" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[12] <a class="ae le" href="https://www.wired.com/story/elon-musk-humanity-biological-boot-loader-ai/" rel="noopener ugc nofollow" target="_blank">埃隆·马斯克:对于人工智能来说，人类是一种‘生物启动加载器’。</a></p><p id="684c" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[13] <a class="ae le" href="https://lacker.io/ai/2020/07/06/giving-gpt-3-a-turing-test.html" rel="noopener ugc nofollow" target="_blank">给GPT-3做图灵测试——凯文·拉克尔的博客</a>。</p><p id="7c13" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">【14】<a class="ae le" href="https://openai.com/blog/science-of-ai/" rel="noopener ugc nofollow" target="_blank">AI训练如何规模化。</a> — OpenAI Blob贴子。</p><p id="7c38" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[ 15] <a class="ae le" href="https://venturebeat.com/2020/05/02/yann-lecun-and-yoshua-bengio-self-supervised-learning-is-the-key-to-human-level-intelligence/" rel="noopener ugc nofollow" target="_blank"> Yann LeCun和Yoshua Bengio:自我监督学习是人类水平智能的关键。</a></p><p id="25a5" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">我们正处于人工智能的风口浪尖上吗(AGI)？</p><p id="bd1f" class="pw-post-body-paragraph jq jr it js b jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn im bi translated">[17] <a class="ae le" href="https://www.technologyreview.com/2020/08/30/1007786/elon-musks-neuralink-demo-update-neuroscience-theater/" rel="noopener ugc nofollow" target="_blank">埃隆·马斯克的Neuralink是神经科学剧院。</a></p></div></div>    
</body>
</html>