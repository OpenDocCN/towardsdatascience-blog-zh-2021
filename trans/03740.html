<html>
<head>
<title>Polynomial Regression From Scratch in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中从头开始的多项式回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/polynomial-regression-in-python-b69ab7df6105?source=collection_archive---------1-----------------------#2021-03-28">https://towardsdatascience.com/polynomial-regression-in-python-b69ab7df6105?source=collection_archive---------1-----------------------#2021-03-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bdf3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从零开始的机器学习:第4部分</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/91a758f8dc7a016418b600c58d6f513c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_EsYwlU9kUshFnY0V4CbzQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">多项式回归；作者图片</p></figure><p id="382b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我们将了解多项式回归算法，通过修改假设函数和添加我们希望添加到输入中的新要素，该算法可用于拟合非线性数据。</p><p id="591a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">多项式回归只是标准线性回归的另一个版本。</p><div class="lu lv gp gr lw lx"><a href="https://medium.com/analytics-vidhya/linear-regression-from-scratch-in-python-b6501f91c82d" rel="noopener follow" target="_blank"><div class="ly ab fo"><div class="lz ab ma cl cj mb"><h2 class="bd iu gy z fp mc fr fs md fu fw is bi translated">Python中从头开始的线性回归</h2><div class="me l"><h3 class="bd b gy z fp mc fr fs md fu fw dk translated">从零开始的机器学习:第1部分</h3></div><div class="mf l"><p class="bd b dl z fp mc fr fs md fu fw dk translated">medium.com</p></div></div><div class="mg l"><div class="mh l mi mj mk mg ml ks lx"/></div></div></a></div><p id="0fd3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，我们将重述<strong class="la iu">符号</strong>(寻找新变量<code class="fe mm mn mo mp b">degrees</code>)，然后<strong class="la iu">理解</strong>算法，最后<strong class="la iu">使用Python NumPy和Matplotlib实现</strong>它，<strong class="la iu">绘制预测</strong>并计算<strong class="la iu"> r2分数</strong>。</p><h1 id="3796" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">符号—</h1><ul class=""><li id="8e18" class="ni nj it la b lb nk le nl lh nm ll nn lp no lt np nq nr ns bi translated"><code class="fe mm mn mo mp b">n</code>→功能数量</li><li id="aef5" class="ni nj it la b lb nt le nu lh nv ll nw lp nx lt np nq nr ns bi translated"><code class="fe mm mn mo mp b">m</code>→培训实例数量</li><li id="3934" class="ni nj it la b lb nt le nu lh nv ll nw lp nx lt np nq nr ns bi translated"><code class="fe mm mn mo mp b">X</code>→形状输入数据矩阵(<code class="fe mm mn mo mp b">m</code> x <code class="fe mm mn mo mp b">n</code>)</li><li id="68ac" class="ni nj it la b lb nt le nu lh nv ll nw lp nx lt np nq nr ns bi translated"><code class="fe mm mn mo mp b">y</code>→大小为<code class="fe mm mn mo mp b">m</code>的真/目标值向量</li><li id="9127" class="ni nj it la b lb nt le nu lh nv ll nw lp nx lt np nq nr ns bi translated"><code class="fe mm mn mo mp b">x(i), y(i)</code>→以训练为例，其中<code class="fe mm mn mo mp b">x(i)</code>为n维，<code class="fe mm mn mo mp b">y(i)</code>为实数。</li><li id="3713" class="ni nj it la b lb nt le nu lh nv ll nw lp nx lt np nq nr ns bi translated"><code class="fe mm mn mo mp b">degrees</code>→一个列表。我们将<code class="fe mm mn mo mp b">X^(value)</code>特征添加到输入中，其中值是列表中的值之一。(稍后详细解释)</li><li id="e5c6" class="ni nj it la b lb nt le nu lh nv ll nw lp nx lt np nq nr ns bi translated"><code class="fe mm mn mo mp b">w</code> →形状的权重(参数)(<code class="fe mm mn mo mp b">n</code> x 1)</li><li id="decc" class="ni nj it la b lb nt le nu lh nv ll nw lp nx lt np nq nr ns bi translated"><code class="fe mm mn mo mp b">b</code> →bias(参数)，一个可以<a class="ae ny" href="https://numpy.org/doc/stable/user/basics.broadcasting.html" rel="noopener ugc nofollow" target="_blank">广播的实数</a>。</li><li id="91e3" class="ni nj it la b lb nt le nu lh nv ll nw lp nx lt np nq nr ns bi translated"><code class="fe mm mn mo mp b">y_hat</code> →假设(<code class="fe mm mn mo mp b">w</code>(权重)和<code class="fe mm mn mo mp b">X</code>的点积加上<code class="fe mm mn mo mp b">b</code>(偏差))— <code class="fe mm mn mo mp b">w.X + b</code></li><li id="ef8c" class="ni nj it la b lb nt le nu lh nv ll nw lp nx lt np nq nr ns bi translated"><strong class="la iu">损失函数</strong>→均方误差损失或均方误差损失(<code class="fe mm mn mo mp b">y_hat</code> - <code class="fe mm mn mo mp b">y</code>)，你最小化这个函数以便找到参数<code class="fe mm mn mo mp b">w</code>和<code class="fe mm mn mo mp b">b</code>。</li></ul></div><div class="ab cl nz oa hx ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="im in io ip iq"><h1 id="3402" class="mq mr it bd ms mt og mv mw mx oh mz na jz oi ka nc kc oj kd ne kf ok kg ng nh bi translated">多项式回归</h1><p id="77b0" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh ol lj lk ll om ln lo lp on lr ls lt im bi translated">让我们以下面的数据集为例来理解多项式回归，其中x轴代表输入数据<code class="fe mm mn mo mp b">X</code>，y轴代表具有1000个示例(<code class="fe mm mn mo mp b">m</code>)和1个特征(<code class="fe mm mn mo mp b">n</code>)的真值/目标值<code class="fe mm mn mo mp b">y</code>。</p><pre class="kj kk kl km gt oo mp op oq aw or bi"><span id="c935" class="os mr it mp b gy ot ou l ov ow">import numpy as np<br/>import matplotlib.pyplot as plt</span><span id="1fd8" class="os mr it mp b gy ox ou l ov ow">np.random.seed(42)<br/>X = np.random.rand(1000,1)<br/>y = 5*((X)**(2)) + np.random.rand(1000,1)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/29e34a226d025603e036eb91744d7d23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*elOHWGUTCPplpqzkPHOg2w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">随机二次数据；作者图片</p></figure><p id="c932" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们对该数据使用标准线性回归，我们将只能对数据拟合一条直线，如下图中的蓝线所示，其中假设为— <code class="fe mm mn mo mp b">w1.X + b</code>(用<code class="fe mm mn mo mp b">w1</code>替换<code class="fe mm mn mo mp b">w</code>)。</p><p id="9d80" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是，我们可以看到数据不是线性的，下面显示的红点线非常适合这些数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/6dbde636624ee0a18cd47586a5308e0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JSjKX8x3Mw4mBVHCjbaYrg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">多项式与线性回归；作者图片</p></figure><p id="52ee" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，在将模型拟合到数据时，您必须回答的一个问题是— <strong class="la iu">您希望使用什么功能？</strong>您是想对数据进行直线拟合，还是想对形式为— <code class="fe mm mn mo mp b"> b + w1.X + w2.X²</code>的假设进行拟合，因为上述数据可能看起来像是二次函数拟合。</p><p id="e025" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">或者，您可能有看起来像平方根拟合的数据，因此您可能希望您的假设像这样— <code class="fe mm mn mo mp b">b + w1.X + w2.(X)^0.5</code>或者您的数据可以是任何程度的，因此您可以选择您想要的特征并修改假设函数。</p><p id="efc7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在要实现这一点，我们所要做的就是将我们的第一个特征x1(特征1)定义为<code class="fe mm mn mo mp b">X</code>，将第二个特征x2(特征2)定义为<code class="fe mm mn mo mp b">X²</code>，或者根据您想要使用的特征，将x1定义为<code class="fe mm mn mo mp b">X</code>，将x2定义为<code class="fe mm mn mo mp b">X^0.5</code>。通过定义一个新的特征x2，即<code class="fe mm mn mo mp b">X²</code>或<code class="fe mm mn mo mp b">X^0.5</code>，我们看到线性回归中的机制适用于拟合这些类型的非线性数据。</p><blockquote class="pa pb pc"><p id="3ec0" class="ky kz pd la b lb lc ju ld le lf jx lg pe li lj lk pf lm ln lo pg lq lr ls lt im bi translated"><strong class="la iu">这里需要注意的重要一点是，我们的假设仍然是线性的，因为</strong> <code class="fe mm mn mo mp b"><strong class="la iu">X²</strong></code> <strong class="la iu">或</strong> <code class="fe mm mn mo mp b"><strong class="la iu">X^0.5</strong></code> <strong class="la iu">都只是特征。然而，我们得到了数据的非线性拟合。</strong></p></blockquote><p id="d3e1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="pd">我们所要做的就是修改输入(</em> <code class="fe mm mn mo mp b"><em class="pd">X</em></code> <em class="pd">)，也就是说，只要添加我们想要的任何程度的特征，就像我们将为我们的例子添加一个特征X。</em></p><p id="2673" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您通过添加新特征来修改输入，假设会自动得到修改，因为<code class="fe mm mn mo mp b">h(x) = w.X +b</code>，其中<code class="fe mm mn mo mp b">w</code>是大小为<code class="fe mm mn mo mp b">n</code>(特征数量)的向量。</p><p id="76a2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下图显示了前10个示例，其中我们向输入数据添加了一个新的特征X。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/5fb012fac994a19b1b687f5ad56f0214.png" data-original-src="https://miro.medium.com/v2/resize:fit:1034/format:webp/1*3C8xtU4CpdYzSLSP3l0ibA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">变换输入(X)，添加X特征；作者图片</p></figure><p id="4a5d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以添加尽可能多的功能，这将是我们已经拥有的功能的一些指数运算。</p></div><div class="ab cl nz oa hx ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="im in io ip iq"><h1 id="a0eb" class="mq mr it bd ms mt og mv mw mx oh mz na jz oi ka nc kc oj kd ne kf ok kg ng nh bi translated">该算法</h1><ul class=""><li id="5239" class="ni nj it la b lb nk le nl lh nm ll nn lp no lt np nq nr ns bi translated">根据你掌握的数据，修改线性回归的假设函数。</li><li id="f1ab" class="ni nj it la b lb nt le nu lh nv ll nw lp nx lt np nq nr ns bi translated">向输入中添加所需的新的更高阶要素。</li><li id="49b4" class="ni nj it la b lb nt le nu lh nv ll nw lp nx lt np nq nr ns bi translated">对修改后的输入进行梯度下降/小批量梯度下降，以找到参数—权重和偏差。</li></ul></div><div class="ab cl nz oa hx ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="im in io ip iq"><h1 id="0894" class="mq mr it bd ms mt og mv mw mx oh mz na jz oi ka nc kc oj kd ne kf ok kg ng nh bi translated">多项式回归在起作用</h1><h1 id="7fdd" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated"><strong class="ak">损失函数</strong></h1><p id="beda" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh ol lj lk ll om ln lo lp on lr ls lt im bi translated">让我们首先定义损失函数，也就是MSE损失函数—</p><p id="8731" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">(<code class="fe mm mn mo mp b">y_hat</code> - <code class="fe mm mn mo mp b">y</code>)其中，<code class="fe mm mn mo mp b">y_hat</code>为假设— <code class="fe mm mn mo mp b">w.X + b</code></p><pre class="kj kk kl km gt oo mp op oq aw or bi"><span id="ab84" class="os mr it mp b gy ot ou l ov ow"><strong class="mp iu">def loss(y, y_hat):</strong><br/>    <br/>    # y --&gt; true/target value.<br/>    # y_hat --&gt; hypothesis<br/>    <br/>    #Calculating loss.<br/>    <strong class="mp iu">loss = np.mean((y_hat - y)**2)<br/>    return loss</strong></span></pre><h1 id="c1d0" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">计算梯度的函数</h1><p id="d4a3" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh ol lj lk ll om ln lo lp on lr ls lt im bi translated">现在，让我们写一个函数来计算损失函数相对于<code class="fe mm mn mo mp b">w</code>和<code class="fe mm mn mo mp b">b</code>的偏导数(梯度)。</p><p id="db7e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请参见注释(#)。</p><pre class="kj kk kl km gt oo mp op oq aw or bi"><span id="9685" class="os mr it mp b gy ot ou l ov ow"># Calulating gradient of loss w.r.t parameters(weights and bias).</span><span id="029e" class="os mr it mp b gy ox ou l ov ow"><strong class="mp iu">def gradients(X, y, y_hat):</strong><br/>    <br/>    # X --&gt; Input.<br/>    # y --&gt; true/target value.<br/>    # y_hat --&gt; hypothesis<br/>    # w --&gt; weights (parameter).<br/>    # b --&gt; bias (parameter).<br/>    <br/>    # m-&gt; number of training examples.<br/>    <strong class="mp iu">m = X.shape[0]</strong><br/>    <br/>    # Gradient of loss w.r.t weights.<br/>    <strong class="mp iu">dw = (1/m)*np.dot(X.T, (y_hat - y))</strong><br/>    <br/>    # Gradient of loss w.r.t bias.<br/>    <strong class="mp iu">db = (1/m)*np.sum((y_hat - y)) </strong><br/>    <br/>    <strong class="mp iu">return dw, db</strong></span></pre><h1 id="bd9d" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">向输入数据添加要素的函数</h1><p id="a25c" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh ol lj lk ll om ln lo lp on lr ls lt im bi translated">让我们编写一个函数，向输入中添加尽可能多的特性。这里我们将定义一个名为<code class="fe mm mn mo mp b">degrees</code>的变量，它是一个python列表。</p><blockquote class="pa pb pc"><p id="cc01" class="ky kz pd la b lb lc ju ld le lf jx lg pe li lj lk pf lm ln lo pg lq lr ls lt im bi translated">无论我们将什么值传递到这个列表中，我们都会向输入添加一个新的特性，即<code class="fe mm mn mo mp b">X^(value)</code>。例如，如果我们将2和3传入列表，我们将特性<code class="fe mm mn mo mp b">X²</code>和<code class="fe mm mn mo mp b">X³</code>添加到输入中。</p></blockquote><p id="6552" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请参见注释(#)。</p><pre class="kj kk kl km gt oo mp op oq aw or bi"><span id="920d" class="os mr it mp b gy ot ou l ov ow"><strong class="mp iu">def x_transform(X, degrees):</strong><br/>    <br/>    # X --&gt; Input.<br/>    # degrees --&gt; A list, We add X^(value) feature to the input<br/>    #             where value is one of the values in the list.<br/>    <br/>    # making a copy of X.<br/>    <strong class="mp iu">t = X.copy()</strong><br/>    <br/>    # Appending columns of higher degrees to X.<br/>    <strong class="mp iu">for i in degrees:<br/>        X = np.append(X, t**i, axis=1)<br/>            <br/>    return X</strong></span></pre><h1 id="c0d1" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">培训功能</h1><p id="db5d" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh ol lj lk ll om ln lo lp on lr ls lt im bi translated">训练函数包括初始化权重和偏差以及具有小批量梯度下降的训练循环。</p><p id="7721" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请参见注释(#)。</p><pre class="kj kk kl km gt oo mp op oq aw or bi"><span id="3f41" class="os mr it mp b gy ot ou l ov ow"><strong class="mp iu">def train(X, y, bs, degrees, epochs, lr):</strong><br/>    <br/>    # X --&gt; Input.<br/>    # y --&gt; true/target value.<br/>    # bs --&gt; Batch Size.<br/>    # epochs --&gt; Number of iterations.<br/>    # degrees --&gt; A list, We add X^(value) feature to the input<br/>    #             where value is one of the values in the list.<br/>    # lr --&gt; Learning rate.<br/>    <br/>    # Adding features to input X.<br/>    <strong class="mp iu">x = x_transform(X, degrees)</strong><br/>    <br/>    # m-&gt; number of training examples<br/>    # n-&gt; number of features <br/>    <strong class="mp iu">m, n = x.shape</strong><br/>    <br/>    # Initializing weights and bias to zeros.<br/>    <strong class="mp iu">w = np.zeros((n,1))<br/>    b = 0</strong><br/>    <br/>    # Reshaping y.<br/>    <strong class="mp iu">y = y.reshape(m,1)</strong><br/>    <br/>    # Empty list to store losses.<br/>    <strong class="mp iu">losses = []</strong><br/>    <br/>    # Training loop.<br/>    <strong class="mp iu">for epoch in range(epochs):<br/>        for i in range((m-1)//bs + 1):</strong><br/>            <br/>            # Defining batches.<br/>            <strong class="mp iu">start_i = i*bs<br/>            end_i = start_i + bs<br/>            xb = x[start_i:end_i]<br/>            yb = y[start_i:end_i]</strong><br/>            <br/>            # Calculating hypothesis<br/>            <strong class="mp iu">y_hat = np.dot(xb, w) + b</strong><br/>            <br/>            # Getting the gradients of loss w.r.t parameters.<br/>            <strong class="mp iu">dw, db = gradients(xb, yb, y_hat)</strong><br/>            <br/>            # Updating the parameters.<br/>            <strong class="mp iu">w -= lr*dw<br/>            b -= lr*db</strong><br/>        <br/>        # Calculating loss and appending it in the list.<br/>        <strong class="mp iu">l = loss(y, np.dot(x, w) + b)<br/>        losses.append(l)</strong><br/>        <br/>    # returning weights, bias and losses(List).<br/>    <strong class="mp iu">return w, b, losses</strong></span></pre><h1 id="3a4d" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">预测功能</h1><p id="f704" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh ol lj lk ll om ln lo lp on lr ls lt im bi translated">请参见注释(#)。</p><pre class="kj kk kl km gt oo mp op oq aw or bi"><span id="f6da" class="os mr it mp b gy ot ou l ov ow"># Predicting function.</span><span id="2ced" class="os mr it mp b gy ox ou l ov ow"><strong class="mp iu">def predict(X, w, b, degrees):</strong><br/>    <br/>    # X --&gt; Input.<br/>    # w --&gt; weights (parameter).<br/>    # b --&gt; bias (parameter).<br/>    degrees --&gt; A list, We add X^(value) feature to the input<br/>    #             where value is one of the values in the list.<br/>    <br/>    # Adding degrees to input X.<br/>    <strong class="mp iu">x1 = x_transform(X, degrees)</strong><br/>    <br/>    # Returning predictions.<br/>    <strong class="mp iu">return np.dot(x1, w) + b</strong></span></pre><h1 id="7068" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">训练和绘制预测</h1><p id="c855" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh ol lj lk ll om ln lo lp on lr ls lt im bi translated">使用<code class="fe mm mn mo mp b">train</code>功能训练数据。</p><blockquote class="pa pb pc"><p id="e575" class="ky kz pd la b lb lc ju ld le lf jx lg pe li lj lk pf lm ln lo pg lq lr ls lt im bi translated">仅向<code class="fe mm mn mo mp b">degrees</code>列表传递2。您可以尝试传递任何其他数字，看看会发生什么。</p></blockquote><pre class="kj kk kl km gt oo mp op oq aw or bi"><span id="b3b1" class="os mr it mp b gy ot ou l ov ow">w, b, l = train(X, y, bs=100, degrees=[2], epochs=1000,<br/>                lr=0.01)</span><span id="1124" class="os mr it mp b gy ox ou l ov ow"><strong class="mp iu"># Plotting</strong><br/>fig = plt.figure(figsize=(8,6))<br/>plt.plot(X, y, 'y.')<br/>plt.plot(X, predict(X, w, b, [2]), 'r.')<br/>plt.legend(["Data", "Polynomial predictions"])<br/>plt.xlabel('X - Input')<br/>plt.ylabel('y - target / true')<br/>plt.title('Polynomial Regression')<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pi"><img src="../Images/dcf689586982c69cb0f0e91c463dd37e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0OjJCip4QcZyoxZQO3lpnw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">预测用红色表示；作者图片</p></figure><p id="a32f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">看起来不错。</p><blockquote class="pa pb pc"><p id="3a39" class="ky kz pd la b lb lc ju ld le lf jx lg pe li lj lk pf lm ln lo pg lq lr ls lt im bi translated">由于我们已经在列表<code class="fe mm mn mo mp b">losses</code>中收集了每次迭代的损失，试着绘制<code class="fe mm mn mo mp b">losses</code>对迭代(<code class="fe mm mn mo mp b">epochs</code>)的图，看看在训练时损失是否下降。</p></blockquote><h1 id="bc50" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">r2得分</h1><p id="745a" class="pw-post-body-paragraph ky kz it la b lb nk ju ld le nl jx lg lh ol lj lk ll om ln lo lp on lr ls lt im bi translated">计算我们预测的r2分数，看看我们做得有多好。</p><p id="7044" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">供参考—<a class="ae ny" href="https://www.geeksforgeeks.org/python-coefficient-of-determination-r2-score/#:~:text=Coefficient%20of%20determination%20also%20called,input%20independent%20variable(s)." rel="noopener ugc nofollow" target="_blank">R2分数是多少？</a></p><pre class="kj kk kl km gt oo mp op oq aw or bi"><span id="112e" class="os mr it mp b gy ot ou l ov ow">def r2_score(y, y_hat):<br/>    return 1 - (np.sum((np.array(y_hat)-np.array(y))**2)/<br/>                np.sum((np.array(y)-np.mean(np.array(y)))**2))</span><span id="79ea" class="os mr it mp b gy ox ou l ov ow">r2_score(y_train, predict(x_train, w, b, [2]))<br/>&gt;&gt;<strong class="mp iu">0.9541881152879292</strong></span></pre><p id="ecdc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是一个很好的r2分数。</p></div><div class="ab cl nz oa hx ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="im in io ip iq"><p id="ebf4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢阅读。对于问题、评论、顾虑，请在回复部分进行讨论。更多的ML从零开始即将推出。</p><p id="c427" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">看看从零开始的机器学习系列—</p><ul class=""><li id="c6ee" class="ni nj it la b lb lc le lf lh pj ll pk lp pl lt np nq nr ns bi translated">第1部分:<a class="ae ny" href="https://medium.com/analytics-vidhya/linear-regression-from-scratch-in-python-b6501f91c82d?source=your_stories_page-------------------------------------" rel="noopener">Python中从头开始的线性回归</a></li><li id="a431" class="ni nj it la b lb nt le nu lh nv ll nw lp nx lt np nq nr ns bi translated">第二部分:<a class="ae ny" rel="noopener" target="_blank" href="/locally-weighted-linear-regression-in-python-3d324108efbf?source=your_stories_page-------------------------------------">Python中的局部加权线性回归</a></li><li id="9a31" class="ni nj it la b lb nt le nu lh nv ll nw lp nx lt np nq nr ns bi translated">第3部分:<a class="ae ny" rel="noopener" target="_blank" href="/normal-equation-in-python-the-closed-form-solution-for-linear-regression-13df33f9ad71?source=your_stories_page-------------------------------------">使用Python的正规方程:线性回归的封闭解</a></li></ul></div></div>    
</body>
</html>