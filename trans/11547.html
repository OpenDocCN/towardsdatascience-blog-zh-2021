<html>
<head>
<title>An Overview of Deep Learning — from History to Fundamentals</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习概述——从历史到基础</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-overview-of-deep-learning-from-history-to-fundamentals-f7117b2d0d37?source=collection_archive---------4-----------------------#2021-11-15">https://towardsdatascience.com/an-overview-of-deep-learning-from-history-to-fundamentals-f7117b2d0d37?source=collection_archive---------4-----------------------#2021-11-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="9f5f" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">世贸中心遗址</h2><div class=""/><div class=""><h2 id="fd36" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">一本学习深度学习本质的剧本，重点是卷积神经网络</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/47bcf88a0dd830b9d74fbfb8c2ee6456.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*od_JVlQn0i8YFypx"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@raimondklavins?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">雷蒙·克拉文斯</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="6f78" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我最近为那些想成为数据科学家的人教授了一门关于机器学习101的迷你课程。其中一个模块是关于深度学习的。我发现许多新手对这个话题感到困惑，主要是因为它经常被教授许多复杂的内容。在这篇文章中，我的目标是描述它足够简单，但不要太简单。希望有帮助！</p><p id="0dab" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">本文分为以下四个部分:</p><ul class=""><li id="9e8b" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md mj mk ml mm bi translated">什么是神经网络？</li><li id="9e06" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">什么是深度学习？</li><li id="4ac7" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">如何搭建一个简单的深度学习架构？</li><li id="8250" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">如何训练一个深度学习模型？</li></ul></div><div class="ab cl ms mt hx mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="im in io ip iq"><h1 id="5013" class="mz na it bd nb nc nd ne nf ng nh ni nj ki nk kj nl kl nm km nn ko no kp np nq bi translated">什么是神经网络？</h1><p id="fce3" class="pw-post-body-paragraph li lj it lk b ll nr kd ln lo ns kg lq lr nt lt lu lv nu lx ly lz nv mb mc md im bi translated">神经网络是一种受生物神经网络启发的计算模型，它在人脑中处理信息。神经网络由一组按层(输入、隐藏和输出)组织的人工神经元组成。这些人工神经元由突触连接，这些突触只是加权值。</p><h2 id="c019" class="nw na it bd nb nx ny dn nf nz oa dp nj lr ob oc nl lv od oe nn lz of og np iz bi translated">—构建模块是什么？</h2><p id="d049" class="pw-post-body-paragraph li lj it lk b ll nr kd ln lo ns kg lq lr nt lt lu lv nu lx ly lz nv mb mc md im bi translated">人工神经元是具有特定数学运算的计算元件。一个神经元接受它的输入(布尔或实数),并在对输入应用预定义的操作后，通过激活函数将结果传递给其他神经元。激活函数是每个节点的一部分，它将线性函数转换为非线性函数。基本上，它决定了一个神经元是否应该放电。激活函数可以是不同的数学函数，例如Step、Sigmoid和ReLU。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/83965fa145e85a19e10325d841346248.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*V7HdNZjnipAlufVI8S2MkA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图1:生物神经元(上)和人工神经元(下)。该图像已获得Shutterstock的许可。</p></figure><p id="3bbb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">一个普通的神经元(也称为感知器神经元)接受包括偏差在内的输入，并将其乘以相关的权重，然后将总和传递给阶跃函数。</p><h2 id="d357" class="nw na it bd nb nx ny dn nf nz oa dp nj lr ob oc nl lv od oe nn lz of og np iz bi translated"><strong class="ak"> —如何训练一个神经网络？</strong></h2><p id="9559" class="pw-post-body-paragraph li lj it lk b ll nr kd ln lo ns kg lq lr nt lt lu lv nu lx ly lz nv mb mc md im bi translated">训练神经网络是指计算(或找到)网络中的权重以最小化目标函数(或损失函数)的过程。您可以将神经网络中的权重视为多项式函数(如<code class="fe oi oj ok ol b">ax^2+b*x+c</code>)中的参数，其中有两个主要区别。神经网络是具有高度和潜在非线性性能的多项式函数。在这里，<em class="om">高度</em>意味着需要时参数的数量可以超过数百万。</p><p id="7c53" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">有各种算法来训练神经网络，反向传播(backpropagation)是其中之一。简而言之，反向传播是一种在训练阶段使用梯度下降技术来计算神经网络中的权重的算法。在这种方法中，计算输出与期望值相比的误差，并成比例地(即，基于当前权重)反向传播以更新网络权重。网络权重基于学习速率迭代更新，直到收敛。</p><p id="32e2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">推荐看视频了解更多这种方法。YouTube频道有一个描述这个话题的最好的可视化工具。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="on oo l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">反向传播到底在做什么？—3蓝色1棕色</p></figure><p id="1851" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">要训练神经网络，您必须知道以下问题的答案。在开始实现一个示例项目之前，您可能不会理解它们的重要性。</p><ul class=""><li id="411a" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md mj mk ml mm bi translated">如何初始化权重？</li><li id="4f85" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">停止的标准是什么？</li><li id="afc0" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">算法不收敛怎么办？</li><li id="826d" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">增加网络的复杂性有帮助吗？</li></ul><p id="98e3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">最后一件事。</strong>如果你想了解学习率、激活函数或架构等配置参数如何影响神经网络的结果，我强烈建议查看TensorFlow创建的名为Playground的交互式演示。<strong class="lk jd">太美了，太有见地了！</strong></p><div class="op oq gp gr or os"><a href="https://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=circle&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=4,2&amp;seed=0.24016&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false" rel="noopener  ugc nofollow" target="_blank"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd jd gy z fp ox fr fs oy fu fw jc bi translated">张量流——神经网络游乐场</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">这是一种构建从数据中学习的计算机程序的技术。它非常松散地基于我们如何思考…</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">playground.tensorflow.org</p></div></div><div class="pb l"><div class="pc l pd pe pf pb pg lb os"/></div></div></a></div><h1 id="2597" class="mz na it bd nb nc ph ne nf ng pi ni nj ki pj kj nl kl pk km nn ko pl kp np nq bi translated">什么是深度学习？</h1><p id="b038" class="pw-post-body-paragraph li lj it lk b ll nr kd ln lo ns kg lq lr nt lt lu lv nu lx ly lz nv mb mc md im bi translated">深度学习是一系列基于深度神经网络(具有许多隐藏层的神经网络)的<strong class="lk jd">特殊架构</strong>的机器学习方法，这些深度神经网络可以同时进行<strong class="lk jd">特征提取</strong>和<strong class="lk jd">分类</strong>，并且只需很少的人力。这种特殊的结构比简单完全连接的神经网络中的层更高级。这些特殊的架构大多建立在一个名为“<strong class="lk jd">胶囊</strong>的概念之上。胶囊是每一层中的一组神经元，它们进行大量内部计算，并输出表示数据属性(如卷积)的压缩结果。你可以阅读更多关于卷积神经网络的内容</p><h2 id="0d40" class="nw na it bd nb nx ny dn nf nz oa dp nj lr ob oc nl lv od oe nn lz of og np iz bi translated">—一个成功的故事:AlphaGo</h2><p id="1e78" class="pw-post-body-paragraph li lj it lk b ll nr kd ln lo ns kg lq lr nt lt lu lv nu lx ly lz nv mb mc md im bi translated"><a class="ae lh" href="https://deepmind.com/research/case-studies/alphago-the-story-so-far" rel="noopener ugc nofollow" target="_blank">由<a class="ae lh" href="https://deepmind.com/" rel="noopener ugc nofollow" target="_blank"> Deepmind </a>创建的AlphaGo </a>项目是深度学习的成功案例之一。正如Deepmind所说:“AlphaGo是第一个打败围棋世界冠军的计算机程序”。首先，我来描述一下围棋为什么特别。</p><p id="3a73" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">围棋是人工智能瞄准的最具挑战性的经典游戏。为什么？主要是因为玩家可以选择的移动数量。按此<a class="ae lh" href="https://www.businessinsider.com/why-google-ai-game-go-is-harder-than-chess-2016-3" rel="noopener ugc nofollow" target="_blank">篇</a>、<strong class="lk jd">T3在前2步棋后，围棋中大概有13万步棋。这个数字是国际象棋中400种可能的走法。你可以看到Go中的搜索空间是无可争议的广阔。<strong class="lk jd">当搜索空间极其广阔时，深度学习可能是一个不错的选择。</strong></strong></p><p id="f1c9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">深度学习是瞄准围棋的正确方法的另一个原因是植根于围棋的玩法。如果你问围棋手他们是如何决定一步棋的，他们通常会告诉你感觉很对。在这些你不能定义特征的场景中，你不能使用经典的机器学习算法。<strong class="lk jd">当确定有用的特征集不可行时，深度学习可能是一个不错的选择。</strong></p><h1 id="3d03" class="mz na it bd nb nc ph ne nf ng pi ni nj ki pj kj nl kl pk km nn ko pl kp np nq bi translated">什么是卷积神经网络？</h1><p id="a164" class="pw-post-body-paragraph li lj it lk b ll nr kd ln lo ns kg lq lr nt lt lu lv nu lx ly lz nv mb mc md im bi translated">卷积神经网络(或CNN)是一类深度学习架构，通常用于分析图像，如图像分类、对象检测和视频动作识别。一般来说，卷积神经网络被设计用于任何在其结构中具有某种<strong class="lk jd">空间不变性</strong>的数据，例如人脸或语音识别。空间不变性意味着，例如，图像左上角的猫耳与图像右下角的猫耳具有相同的特征。CNN建立在下面描述的两个主要构件上。</p><ul class=""><li id="6628" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md mj mk ml mm bi translated"><strong class="lk jd">卷积</strong> —细胞神经网络是空间不变的，因为它们建立在卷积算子之上。卷积是一种数学运算，它对两个函数(信号)的乘积进行积分，其中一个信号被翻转(如果需要)。例如，在过去，卷积运算符被用于计算两个信号之间的相关性或寻找信号中的模式。该操作符对于视觉数据中的特征提取非常有效。过去几年里，计算机视觉领域取得的许多进步，部分归功于卷积神经网络。</li><li id="f784" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated"><strong class="lk jd">池化</strong>—CNN中的另一个构件是池化层。它的功能是逐渐减小数据的空间大小，以减小网络大小和算法对输入中要素精确位置的敏感度。网络规模转化为在训练阶段必须计算的权重数量。</li></ul><p id="9396" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">有时，您还需要在输入图像的边界周围添加带有<code class="fe oi oj ok ol b">pixel_intensity=0 </code>的额外像素，以增加有效尺寸。这有助于在应用卷积层后保持图像大小固定。这个过程叫做<strong class="lk jd">填充</strong>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/ddda1ab5719c72a3df18be30f8164ae0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*ld4mGO5C4Ombvq_NfFTgGw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图2:Alex net——图片获得了Shutterstock的许可。</p></figure><h1 id="2599" class="mz na it bd nb nc ph ne nf ng pi ni nj ki pj kj nl kl pk km nn ko pl kp np nq bi translated">有哪些特殊的架构？</h1><ul class=""><li id="a4d0" class="me mf it lk b ll nr lo ns lr pm lv pn lz po md mj mk ml mm bi translated"><strong class="lk jd"> AlexNet — </strong> AlexNet是卷积神经网络的成功实现，在2012年赢得了ImageNet大规模视觉识别挑战赛(<a class="ae lh" href="https://www.image-net.org/challenges/LSVRC/index.php" rel="noopener ugc nofollow" target="_blank"> ILSVRC </a>)。该架构已由Alex Krizhevsky、Ilya Sutskever、Geoffrey Hinton在NeurIPS 2012上发表(图2)。在这种架构中，输入是大小为256×256的RGB图像，这些图像被随机裁剪成大小为224×224的图像。该架构包含65万个神经元和6000万个参数。此外，在两个GTX 580 3GB GPU上训练需要5-6天。它由5个卷积层和3个全连接层组成。他们首次使用整流线性单元(ReLUs)作为激活函数。</li><li id="45a4" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated"><strong class="lk jd"> VGG16 — </strong>提高深度神经网络性能的标准方法是增加深度。VGG-16是由牛津大学视觉几何小组的人发明的。该架构有13个卷积层和3个全连接层。他们还使用ReLU激活功能作为AlexNet的传统。与AlexNet相比，该网络堆叠了更多层，并使用了更小尺寸的过滤器(2×2和3×3)。它由138M参数组成。</li></ul><div class="op oq gp gr or os"><a rel="noopener follow" target="_blank" href="/vgg-neural-networks-the-next-step-after-alexnet-3f91fa9ffe2c"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd jd gy z fp ox fr fs oy fu fw jc bi translated">VGG神经网络:AlexNet之后的下一步</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">AlexNet于2012年问世，对传统的卷积神经网络(CNN)进行了改进。接下来是VGG…</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">towardsdatascience.com</p></div></div><div class="pb l"><div class="pp l pd pe pf pb pg lb os"/></div></div></a></div><ul class=""><li id="6b85" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md mj mk ml mm bi translated"><strong class="lk jd"> ResNet50 — </strong>随着网络深度的增加，精度会饱和，然后迅速下降，这主要是因为我们无法对其进行适当的训练。微软研究院用ResNet50解决了这个问题——使用跳过(或快捷方式)连接，同时构建更深层次的模型。更深的CNN(高达152层)而不影响模型的泛化。这是一个好主意…</li></ul><h1 id="3ac0" class="mz na it bd nb nc ph ne nf ng pi ni nj ki pj kj nl kl pk km nn ko pl kp np nq bi translated">如何构建一个简单的深度学习架构</h1><h2 id="51d3" class="nw na it bd nb nx ny dn nf nz oa dp nj lr ob oc nl lv od oe nn lz of og np iz bi translated"><strong class="ak"> — Keras </strong></h2><p id="4dd6" class="pw-post-body-paragraph li lj it lk b ll nr kd ln lo ns kg lq lr nt lt lu lv nu lx ly lz nv mb mc md im bi translated">Keras是一个带有Python接口的高级神经网络库，可以在主要的科学计算框架上运行，如TensorFlow(由Google创建)或CNTK(由微软创建)。与Pytorch相比，工程师通常更喜欢提供快速开发的Keras。您可以使用下面的Kears找到如何建立一个类似VGG的卷积神经网络。要了解更多关于如何使用Keras建立深度学习模型的信息，可以查看原始的<a class="ae lh" href="https://faroit.com/keras-docs/2.0.2/getting-started/sequential-model-guide/" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><pre class="ks kt ku kv gt pq ol pr ps aw pt bi"><span id="ab66" class="nw na it ol b gy pu pv l pw px"><strong class="ol jd">from</strong> keras.models <strong class="ol jd">import</strong> Sequential<br/><strong class="ol jd">from</strong> keras.layers <strong class="ol jd">import</strong> Dense, Dropout, Flatten<br/><strong class="ol jd">from</strong> keras.layers <strong class="ol jd">import</strong> Conv2D, MaxPooling2D<br/><strong class="ol jd">from</strong> keras.optimizers <strong class="ol jd">import</strong> SGD</span><span id="44b8" class="nw na it ol b gy py pv l pw px">model = <strong class="ol jd">Sequential</strong>()<br/>model.add(<strong class="ol jd">Conv2D</strong>(32, (3, 3), <strong class="ol jd">activation</strong>='relu', <strong class="ol jd">input_shape</strong>=(100, 100, 3)))<br/>model.add(<strong class="ol jd">Conv2D</strong>(32, (3, 3), <strong class="ol jd">activation</strong>='relu'))<br/>model.add(<strong class="ol jd">MaxPooling2D</strong>(<strong class="ol jd">pool_size</strong>=(2, 2)))<br/>model.add(<strong class="ol jd">Dropout</strong>(0.25))</span><span id="55f6" class="nw na it ol b gy py pv l pw px">model.add(<strong class="ol jd">Conv2D</strong>(64, (3, 3), <strong class="ol jd">activation</strong>='relu'))<br/>model.add(<strong class="ol jd">Conv2D</strong>(64, (3, 3), <strong class="ol jd">activation</strong>='relu'))<br/>model.add(<strong class="ol jd">MaxPooling2D</strong>(<strong class="ol jd">pool_size</strong>=(2, 2)))<br/>model.add(<strong class="ol jd">Dropout</strong>(0.25))</span><span id="33ac" class="nw na it ol b gy py pv l pw px">model.add(<strong class="ol jd">Flatten</strong>())<br/>model.add(<strong class="ol jd">Dense</strong>(256, <strong class="ol jd">activation</strong>='relu'))<br/>model.add(<strong class="ol jd">Dropout</strong>(0.5))<br/>model.add(<strong class="ol jd">Dense</strong>(10, <strong class="ol jd">activation</strong>='softmax'))</span><span id="6d33" class="nw na it ol b gy py pv l pw px"><strong class="ol jd">sgd</strong> = <strong class="ol jd">SGD</strong>(<strong class="ol jd">lr</strong>=0.01, <strong class="ol jd">decay</strong>=1e-6, <strong class="ol jd">momentum</strong>=0.9, <strong class="ol jd">nesterov</strong>=True)<br/>model.compile(<strong class="ol jd">loss</strong>='categorical_crossentropy', <strong class="ol jd">optimizer</strong>=sgd)</span><span id="2cc9" class="nw na it ol b gy py pv l pw px">model.fit(x_train, y_train, batch_size=32, epochs=10)</span></pre><h2 id="00b6" class="nw na it bd nb nx ny dn nf nz oa dp nj lr ob oc nl lv od oe nn lz of og np iz bi translated"><strong class="ak"> — PyTorch </strong></h2><p id="cf77" class="pw-post-body-paragraph li lj it lk b ll nr kd ln lo ns kg lq lr nt lt lu lv nu lx ly lz nv mb mc md im bi translated">PyTorch是一个基于python的低级神经网络库，构建于脸书科学计算框架(Torch)之上。其工作流程类似于Python科学计算库(Numpy)。Pytorch具有高度可配置性，与不需要复杂架构或特殊图层操作的开发者相比，更受研究人员的欢迎。您可以在下面找到如何使用PyTorch构建卷积神经网络。要了解更多关于如何使用PyTorch构建深度学习模型的信息，可以查看原始的<a class="ae lh" href="https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><pre class="ks kt ku kv gt pq ol pr ps aw pt bi"><span id="c539" class="nw na it ol b gy pu pv l pw px"><strong class="ol jd">import</strong> torch.nn <strong class="ol jd">as</strong> nn<br/><strong class="ol jd">import</strong> torch.nn.functional <strong class="ol jd">as</strong> F<br/><br/><br/><strong class="ol jd">class</strong> <strong class="ol jd">Net(nn.Module):</strong><br/>    <strong class="ol jd">def</strong> __init__<strong class="ol jd">(</strong>self<strong class="ol jd">):</strong><br/>        super<strong class="ol jd">().</strong>__init__<strong class="ol jd">()</strong><br/>        self<strong class="ol jd">.conv1</strong> <strong class="ol jd">=</strong> <strong class="ol jd">nn.Conv2d(</strong>3<strong class="ol jd">,</strong> 6<strong class="ol jd">,</strong> 5<strong class="ol jd">)</strong><br/>        self<strong class="ol jd">.pool</strong> <strong class="ol jd">=</strong> <strong class="ol jd">nn.MaxPool2d(</strong>2<strong class="ol jd">,</strong> 2<strong class="ol jd">)</strong><br/>        self<strong class="ol jd">.conv2</strong> <strong class="ol jd">=</strong> <strong class="ol jd">nn.Conv2d(</strong>6<strong class="ol jd">,</strong> 16<strong class="ol jd">,</strong> 5<strong class="ol jd">)</strong><br/>        self<strong class="ol jd">.fc1</strong> <strong class="ol jd">=</strong> <strong class="ol jd">nn.Linear(</strong>16 <strong class="ol jd">*</strong> 5 <strong class="ol jd">*</strong> 5<strong class="ol jd">,</strong> 120<strong class="ol jd">)</strong><br/>        self<strong class="ol jd">.fc2</strong> <strong class="ol jd">=</strong> <strong class="ol jd">nn.Linear(</strong>120<strong class="ol jd">,</strong> 84<strong class="ol jd">)</strong><br/>        self<strong class="ol jd">.fc3</strong> <strong class="ol jd">=</strong> <strong class="ol jd">nn.Linear(</strong>84<strong class="ol jd">,</strong> 10<strong class="ol jd">)</strong><br/><br/>    <strong class="ol jd">def</strong> <strong class="ol jd">forward(</strong>self<strong class="ol jd">,</strong> <strong class="ol jd">x):</strong><br/>        <strong class="ol jd">x</strong> <strong class="ol jd">=</strong> self<strong class="ol jd">.pool(F.relu(</strong>self<strong class="ol jd">.conv1(x)))</strong><br/>        <strong class="ol jd">x</strong> <strong class="ol jd">=</strong> self<strong class="ol jd">.pool(F.relu(</strong>self<strong class="ol jd">.conv2(x)))</strong><br/>        <strong class="ol jd">x</strong> <strong class="ol jd">=</strong> <strong class="ol jd">torch.flatten(x,</strong> 1<strong class="ol jd">)</strong> <br/>        <strong class="ol jd">x</strong> <strong class="ol jd">=</strong> <strong class="ol jd">F.relu(</strong>self<strong class="ol jd">.fc1(x))</strong><br/>        <strong class="ol jd">x</strong> <strong class="ol jd">=</strong> <strong class="ol jd">F.relu(</strong>self<strong class="ol jd">.fc2(x))</strong><br/>        <strong class="ol jd">x</strong> <strong class="ol jd">=</strong> self<strong class="ol jd">.fc3(x)</strong><br/>        <strong class="ol jd">return</strong> <strong class="ol jd">x</strong><br/><br/><br/><strong class="ol jd">net</strong> <strong class="ol jd">=</strong> <strong class="ol jd">Net()</strong></span></pre><h1 id="1442" class="mz na it bd nb nc ph ne nf ng pi ni nj ki pj kj nl kl pk km nn ko pl kp np nq bi translated">如何训练一个深度学习模型？</h1><p id="52d4" class="pw-post-body-paragraph li lj it lk b ll nr kd ln lo ns kg lq lr nt lt lu lv nu lx ly lz nv mb mc md im bi translated">在本文中不可能解释关于训练神经网络的所有内容。在这里，我想阐明一些最重要的话题。</p><h2 id="fa7c" class="nw na it bd nb nx ny dn nf nz oa dp nj lr ob oc nl lv od oe nn lz of og np iz bi translated">—定义</h2><p id="baba" class="pw-post-body-paragraph li lj it lk b ll nr kd ln lo ns kg lq lr nt lt lu lv nu lx ly lz nv mb mc md im bi translated">如上所述，<strong class="lk jd">训练</strong>神经网络是指计算网络中的权重以最小化目标函数的过程。这是一个经典的<strong class="lk jd">优化问题</strong>，你必须<strong class="lk jd">搜索</strong>使损失函数最小化的最优权重(或参数)集。搜索方法的功效决定了训练过程的<strong class="lk jd">速度</strong>和<strong class="lk jd">结果</strong>。</p><blockquote class="pz qa qb"><p id="1559" class="li lj om lk b ll lm kd ln lo lp kg lq qc ls lt lu qd lw lx ly qe ma mb mc md im bi translated"><strong class="lk jd">思考的食粮— </strong>在机器学习算法中，我们选择一个度量(例如，准确性)来评估模型；然而，我们优化了一个不同的目标函数，并“希望”最小化它的值将改进我们关心的度量。那么，我们能做些什么来确保达到预期的要求呢？</p></blockquote><h2 id="efeb" class="nw na it bd nb nx ny dn nf nz oa dp nj lr ob oc nl lv od oe nn lz of og np iz bi translated">—实施</h2><p id="f42c" class="pw-post-body-paragraph li lj it lk b ll nr kd ln lo ns kg lq lr nt lt lu lv nu lx ly lz nv mb mc md im bi translated">一般来说，要实现一个搜索方法，必须对以下问题有答案:(1)“如何确定搜索<strong class="lk jd">方向？</strong>”以及(2)“如何确定搜索<strong class="lk jd">步骤</strong>？”。</p><p id="dc7b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如上所述，<strong class="lk jd">梯度下降</strong>技术已被用于训练神经网络以指导搜索过程。搜索方向由<strong class="lk jd">梯度算子</strong>决定，搜索步长由超参数𝝺决定，也称为学习率。简而言之，梯度下降技术中的更新机制如下:xₘ=xₙ -𝝺* <strong class="lk jd"> ∇ </strong> f(xₙ).经典的梯度下降技术不能简单地用于深度学习技术，在深度学习技术中通常存在大量的权重(参数)和大量的数据点。⛔</p><p id="ef69" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">🚀<strong class="lk jd">随机梯度下降</strong> (SGD)是梯度下降技术的一种变体，对深度学习模型更有效。与使用<strong class="lk jd">批</strong>数据计算误差相反，该方法计算误差并更新训练数据集中每个数据点的<strong class="lk jd">模型。SGD的搜索速度<strong class="lk jd">更快</strong>，并且提供更<strong class="lk jd">频繁的</strong>型号更新。另一方面，每次运行的SGD结果<strong class="lk jd">不同</strong>，与标准技术相比，它的收敛<strong class="lk jd">更慢</strong>。</strong></p><p id="6231" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">除了只使用当前步骤的梯度来指导搜索，我们可以使用过去步骤的<strong class="lk jd">梯度</strong>知道<strong class="lk jd"> </strong>最近的步骤更重要。这在优化上下文中被称为动量。因此，我们可以，例如，使用梯度步骤的指数平均值来进行更有效的搜索过程。查看下面的文章，了解更多关于<strong class="lk jd">动量</strong>的信息。</p><div class="op oq gp gr or os"><a href="https://distill.pub/2017/momentum/" rel="noopener  ugc nofollow" target="_blank"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd jd gy z fp ox fr fs oy fu fw jc bi translated">为什么动量真的有效</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">这里有一个关于动量的流行故事:梯度下降是一个人走下山坡。他沿着最陡的路径…</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">蒸馏. pub</p></div></div><div class="pb l"><div class="qf l pd pe pf pb pg lb os"/></div></div></a></div><h2 id="3ccb" class="nw na it bd nb nx ny dn nf nz oa dp nj lr ob oc nl lv od oe nn lz of og np iz bi translated">SGD不管用怎么办？</h2><p id="316e" class="pw-post-body-paragraph li lj it lk b ll nr kd ln lo ns kg lq lr nt lt lu lv nu lx ly lz nv mb mc md im bi translated">我们有其他技术来解决优化问题，如<strong class="lk jd"> AdaGrad </strong>或<strong class="lk jd"> Adam </strong>(自适应矩估计)。这些方法是梯度下降优化的变体，其自适应地改变学习速率以确保具有更有效的搜索过程。简而言之，对于搜索空间中的每个<strong class="lk jd">方向</strong>以及在<strong class="lk jd">时间</strong>中的每个时刻，学习率或搜索步长可以不同。这里可以阅读更多<a class="ae lh" href="https://ruder.io/optimizing-gradient-descent/index.html" rel="noopener ugc nofollow" target="_blank">。如果你想学习如何在现实世界中使用这些技术，可以看看Keras官方</a><a class="ae lh" href="https://faroit.com/keras-docs/2.0.2/optimizers/" rel="noopener ugc nofollow" target="_blank">文档</a>。</p><h1 id="3ff0" class="mz na it bd nb nc ph ne nf ng pi ni nj ki pj kj nl kl pk km nn ko pl kp np nq bi translated">感谢阅读！</h1><p id="5340" class="pw-post-body-paragraph li lj it lk b ll nr kd ln lo ns kg lq lr nt lt lu lv nu lx ly lz nv mb mc md im bi translated">如果你喜欢这个帖子，想支持我…</p><ul class=""><li id="72b2" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md mj mk ml mm bi translated"><em class="om">跟我上</em> <a class="ae lh" href="https://medium.com/@pedram-ataee" rel="noopener"> <em class="om">中</em> </a> <em class="om">！</em></li><li id="2ca5" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated"><em class="om">在</em> <a class="ae lh" href="https://www.amazon.com/Pedram-Ataee/e/B08D6J3WNW" rel="noopener ugc nofollow" target="_blank"> <em class="om">亚马逊</em> </a> <em class="om">上查看我的书！</em></li><li id="6029" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated"><em class="om">成为</em> <a class="ae lh" href="https://pedram-ataee.medium.com/membership" rel="noopener"> <em class="om">中的一员</em> </a> <em class="om">！</em></li><li id="9a4e" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated"><em class="om">连接上</em><a class="ae lh" href="https://www.linkedin.com/in/pedrama/" rel="noopener ugc nofollow" target="_blank"><em class="om">Linkedin</em></a><em class="om">！</em></li><li id="6b66" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated"><em class="om">关注我</em> <a class="ae lh" href="https://twitter.com/pedram_ataee" rel="noopener ugc nofollow" target="_blank"> <em class="om">推特</em> </a> <em class="om">！</em></li></ul><div class="op oq gp gr or os"><a href="https://pedram-ataee.medium.com/membership" rel="noopener follow" target="_blank"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd jd gy z fp ox fr fs oy fu fw jc bi translated">通过我的推荐链接加入Medium-Pedram Ataee博士</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">pedram-ataee.medium.com</p></div></div><div class="pb l"><div class="qg l pd pe pf pb pg lb os"/></div></div></a></div></div></div>    
</body>
</html>