<html>
<head>
<title>YOLOv4–5D: An Enhancement of YOLOv4 for Autonomous Driving</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">yolov 4–5D:yolov 4的自动驾驶增强版</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/yolov4-5d-an-enhancement-of-yolov4-for-autonomous-driving-2827a566be4a?source=collection_archive---------18-----------------------#2021-09-09">https://towardsdatascience.com/yolov4-5d-an-enhancement-of-yolov4-for-autonomous-driving-2827a566be4a?source=collection_archive---------18-----------------------#2021-09-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="27c3" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">你只看一次，但有五个刻度</h2></div><p id="1c07" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在过去的几年中，目标检测已经成为深度学习和模式识别研究中最热门的话题，并且已经成为所有计算机视觉研究人员必须知道的问题。如果你读这篇文章是因为你知道文章标题中有趣的是什么，我相信你有一些物体检测的背景，所以我想忽略解释基本的东西，如什么是物体检测和有多少种物体检测器，答案可以很容易地在数百万个来源中找到，通过在谷歌或任何搜索网站上键入非常简单的关键字。但至少，我可以从总结一系列YOLO算法开始，到目前为止，这些算法一直是对象检测的图标，也是最有吸引力的基线方法，其他方法都是基于它进行改进的。</p><p id="cf10" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2015年，Joseph Redmon和他的合著者推出了YOLO的第一个版本,该版本在实时物体检测方面取得了突破。<a class="ae lb" href="https://arxiv.org/abs/1506.02640" rel="noopener ugc nofollow" target="_blank"> YOLOv1 </a>是一个一阶段的物体检测器，与当时的两阶段方法相比，推理速度快，精度可接受。<a class="ae lb" href="https://arxiv.org/abs/1612.08242" rel="noopener ugc nofollow" target="_blank"> YOLOv2 </a>，也称为YOLO9000，是一年后提出的，通过应用锚箱的概念来提高检测精度。2016年，<a class="ae lb" href="https://arxiv.org/abs/1804.02767" rel="noopener ugc nofollow" target="_blank"> YOLOv3 </a>进行了进一步的改进，采用了新的骨干网络Darknet53，并使用特征金字塔网络(FPN)作为模型颈部，能够检测三种不同比例的对象。从下一个版本<a class="ae lb" href="https://arxiv.org/abs/2004.10934" rel="noopener ugc nofollow" target="_blank"> YOLOv4 </a>开始，Joseph宣布由于一些个人原因停止进行这个项目，并将YOLO项目的主导特权交给了Alexey Bochkovskiy，Alexey在2020年引入了YOLOv4。YOLOv4通过使用新的主干CSPDarknet53 (CSP代表跨阶段部分)，添加空间金字塔池(SPP)，路径聚合网络(PAN)，并引入镶嵌数据增强方法，提高了前任YOLOv3的性能。你可以通过<a class="ae lb" href="https://pjreddie.com/darknet/" rel="noopener ugc nofollow" target="_blank">官网</a>或者github回购<a class="ae lb" href="https://github.com/AlexeyAB/darknet" rel="noopener ugc nofollow" target="_blank">暗网</a>看一下YOLO项目。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/95c658aa94c016f909c0a31d45f500b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0YosEODplz5WqQMdDTAruw.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">YOLOV4的网络架构(图见<a class="ae lb" href="https://www.semanticscholar.org/paper/YOLOv4-5D%3A-An-Effective-and-Efficient-Object-for-Cai-Luan/0e582215eaf70ca498d1656dd6e372b3ea3e9966" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><p id="968a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">目前，YOLOv4已经是YOLO系列中最先进的型号(实际上存在一个名为YOLOv5的版本，但由于一些原因，该版本没有被确认为官方版本，这可以在这篇<a class="ae lb" href="https://blog.roboflow.com/yolov4-versus-yolov5/" rel="noopener ugc nofollow" target="_blank">文章</a>中找到)。但是YOLOv4仍然没有针对所有场景进行优化；也就是说，在具有许多小物体的场景的情况下，YOLOv4仍然在努力，并且不是真正准确的，例如，在自动驾驶场景中，在道路上存在许多小而远的物体，如行人、车辆、交通标志等。如题，这篇帖子介绍了<a class="ae lb" href="http://ieeexplore.ieee.org/document/9374990" rel="noopener ugc nofollow" target="_blank">YOLOv4–5D</a>，yolov 4针对自动驾驶场景的改进。</p><p id="046c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">yolov 4–5D的新功能:</p><ul class=""><li id="63b7" class="ls lt iq kh b ki kj kl km ko lu ks lv kw lw la lx ly lz ma bi translated">主干:CSPDarknet53_dcn</li><li id="1fff" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la lx ly lz ma bi translated">脖子:PAN++的</li><li id="7114" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la lx ly lz ma bi translated">头部:增加2个大规模图层</li><li id="018f" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la lx ly lz ma bi translated">网络修剪</li></ul><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi mg"><img src="../Images/50a240d0f76375119094ddb6a1f899e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MB2XKSNOZGOHVcV2oei-8Q.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">yolov 4–5D的网络架构(图见<a class="ae lb" href="https://www.semanticscholar.org/paper/YOLOv4-5D%3A-An-Effective-and-Efficient-Object-for-Cai-Luan/0e582215eaf70ca498d1656dd6e372b3ea3e9966" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><h1 id="d048" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated"><strong class="ak"> <em class="mz"> 1。主干:cspdarknet 53 _ DCN</em>T11】</strong></h1><p id="d1e7" class="pw-post-body-paragraph kf kg iq kh b ki na jr kk kl nb ju kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">CSPDarknet53是YOLOv4的主干，yolov 4是第一个将跨阶段部分(CSP)结构集成到主干或特征提取器中的模型。yolov 4–5D中引入的修改主干通过用可变形卷积网络(DCN)替换几层中的常规卷积来重新设计，并表示为CSPDarknet53_dcn。具体而言，为了平衡效率和效果，最后一级中的3×3卷积层被替换为DCN。DCN的显著特点是它使用一个可学习的偏移值来描述目标特征的方向，这样，感受野就不局限于一个固定的范围，而是可以灵活地适应目标几何形状的变化。此外，DCN只是轻微地影响了模型中的参数数量。具有上述特征的DCN被整合到yolov 4-5D的骨架中，形成CSPDarknet53_dcn。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/423984ab8ef24b4e094af072e7d28e2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1196/format:webp/1*lJyBMHG1nPEahMjE9mbFcQ.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">可变形卷积(图见<a class="ae lb" href="https://www.semanticscholar.org/paper/YOLOv4-5D%3A-An-Effective-and-Efficient-Object-for-Cai-Luan/0e582215eaf70ca498d1656dd6e372b3ea3e9966" rel="noopener ugc nofollow" target="_blank">文件</a></p></figure><h1 id="463a" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">2.脖子:PAN++的</h1><p id="5464" class="pw-post-body-paragraph kf kg iq kh b ki na jr kk kl nb ju kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">与YOLOv4不同，yolov 4使用PAN作为模型颈部的一部分(与SPP一起)。在yolov 4–5D中，使用PAN++并将其设计为特征融合模块。PAN++被应用来平衡主干中低层特征的语义信息和高层特征的位置信息。整个网络设计为输出5种不同尺度的检测，有利于小目标检测。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/0cbab028904bff88ca3540502ac73a7f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*vy5_Haa89XJC1B4CY0Wk-w.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">PAN ++ in yolov 4–5D(图改编自<a class="ae lb" href="https://www.semanticscholar.org/paper/YOLOv4-5D%3A-An-Effective-and-Efficient-Object-for-Cai-Luan/0e582215eaf70ca498d1656dd6e372b3ea3e9966" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><h1 id="bc9f" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">3.头部:增加2个大规模图层</h1><p id="73f7" class="pw-post-body-paragraph kf kg iq kh b ki na jr kk kl nb ju kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">如上所述，再增加2个大尺度探测层的目的是增强探测小物体的能力。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi nh"><img src="../Images/97b104f7b6b86875f2f15881afd9cb6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oxUG559KrGoSCpl-Kt6BKw.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">yolov 4–5D增加了两个大规模的更好的小物体检测(红框中)(图改编自<a class="ae lb" href="https://www.semanticscholar.org/paper/YOLOv4-5D%3A-An-Effective-and-Efficient-Object-for-Cai-Luan/0e582215eaf70ca498d1656dd6e372b3ea3e9966" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><h1 id="befc" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">4.网络修剪</h1><p id="2d3d" class="pw-post-body-paragraph kf kg iq kh b ki na jr kk kl nb ju kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">批量标准化中的稀疏比例因子的概念被用于通道修剪，以修剪yolov 4-5D的主干。因为比例参数γ是一个可学习的因子，它能够表示通道的重要性。设置一个小的剪枝阈值，一般为0.1。哪个通道的γ低于0.1将被修剪。</p><h1 id="3948" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">5.结果</h1><p id="a2f8" class="pw-post-body-paragraph kf kg iq kh b ki na jr kk kl nb ju kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">下表显示了YOLOv4–5D与yolov 4的性能对比:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi ni"><img src="../Images/6951d306eb8a76a31b9cb428b57a6ec2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EyRJkaZVucsg2Ogggkg_fw.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">在BDD和KITTY数据集上，yolov 4–5D vs yolov 4(<a class="ae lb" href="https://www.semanticscholar.org/paper/YOLOv4-5D%3A-An-Effective-and-Efficient-Object-for-Cai-Luan/0e582215eaf70ca498d1656dd6e372b3ea3e9966" rel="noopener ugc nofollow" target="_blank">论文</a>中的表格)</p></figure><p id="4b5b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">YOLOv4–5D将yolov 4的性能提高了一个显著的差距。在BDD数据集上，IoU 0.5的整体mAP从65.90%提高到70.13%，提高了4.23%。在KITTY数据集上，YOLOv4-5D以87.02%的mAP产生了更高的检测性能，与原始yolov 4的85.34% mAP相比，差距为1.68%。</p><p id="7a85" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">下表显示了yolov 4–5D与其他先进方法的进一步性能比较:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/47a03836d43ff7e281dd8fc5fc5ca0b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1320/format:webp/1*qhpw2gvthZeInebvmKzj0g.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">yolov 4–5D与BDD验证数据的其他方法(在<a class="ae lb" href="https://www.semanticscholar.org/paper/YOLOv4-5D%3A-An-Effective-and-Efficient-Object-for-Cai-Luan/0e582215eaf70ca498d1656dd6e372b3ea3e9966" rel="noopener ugc nofollow" target="_blank">论文</a>中的表格)</p></figure><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi nk"><img src="../Images/28e8f707d7981a189d618b6ec2e882e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*55Y_Fn2Nn2LT-elKYh7iFA.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">yolov 4–5D与其他方法对KITTY验证数据的比较(见<a class="ae lb" href="https://www.semanticscholar.org/paper/YOLOv4-5D%3A-An-Effective-and-Efficient-Object-for-Cai-Luan/0e582215eaf70ca498d1656dd6e372b3ea3e9966" rel="noopener ugc nofollow" target="_blank">论文</a>中的表格)</p></figure><p id="21f8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，通过模型剪枝，yolov 4–5D的推理速度在保持准确性的同时显著提高了31.3%。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi nl"><img src="../Images/9cc58f07fe6ca1f03a86f58fa28bc6a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QVp7F4Wd4kjANWcphy4ehg.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">修剪过的yolov 4–5D性能(在<a class="ae lb" href="https://www.semanticscholar.org/paper/YOLOv4-5D%3A-An-Effective-and-Efficient-Object-for-Cai-Luan/0e582215eaf70ca498d1656dd6e372b3ea3e9966" rel="noopener ugc nofollow" target="_blank">论文</a>中的表格)</p></figure><h1 id="91e0" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">结论</h1><p id="3489" class="pw-post-body-paragraph kf kg iq kh b ki na jr kk kl nb ju kn ko nc kq kr ks nd ku kv kw ne ky kz la ij bi translated">在这篇文章中，我介绍了YOLOv4-5D，这是yolov 4的一个改进，用于自动驾驶场景中的物体检测。YOLOv4–5D表现出比yolov 4更高的性能，在BDD数据集上mAP提高了4.23%，在KITTY数据集上mAP提高了1.68%。此外，yolov 4–5D的修剪版本在保持相同精度的情况下，仅用98.1MB的内存进一步提高了31.3%的推理速度。</p><p id="ed6b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">欢迎读者访问我的脸书粉丝页，这是关于机器学习的分享:<a class="ae lb" href="https://www.facebook.com/diveintomachinelearning" rel="noopener ugc nofollow" target="_blank">投入机器学习</a>。我的另一篇关于用Darknet和Tensorflow-Keras执行YOLOv4物体检测的帖子也可以在<a class="ae lb" rel="noopener" target="_blank" href="/darkeras-execute-yolov3-yolov4-object-detection-on-keras-with-darknet-pre-trained-weights-5e8428b959e2">这里</a>找到。</p><p id="54ac" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">感谢您抽出时间！</p></div></div>    
</body>
</html>