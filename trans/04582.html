<html>
<head>
<title>Maximum Likelihood vs. Bayesian Estimation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">最大似然与贝叶斯估计</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/maximum-likelihood-vs-bayesian-estimation-dd2eb4dfda8a?source=collection_archive---------2-----------------------#2021-04-20">https://towardsdatascience.com/maximum-likelihood-vs-bayesian-estimation-dd2eb4dfda8a?source=collection_archive---------2-----------------------#2021-04-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="3892" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="04ce" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">参数估计方法的比较</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/75cfa4d19be3d4dda33b9b603fa752c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hMZD63Wqs9pZzuNf"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">由<a class="ae le" href="https://unsplash.com/@kazuend?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">和</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="3977" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">机器学习的核心是模型。我们如何表示数据？我们可以用什么方法将数据分组以进行比较？我们的数据来自什么分布或模型？这些问题(以及更多的问题)驱动着数据处理，但后者是参数估计的基础。</p><p id="5c33" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">最大似然估计(MLE)和贝叶斯估计(Bayesian estimation)可能是最广泛使用的两种参数估计方法，通过这两种方法，给定一些数据，我们能够估计产生这些数据的模型。为什么这很重要？在现实世界中收集的数据几乎不能代表全部人口(想象一下我们需要收集多少数据！)，因此，通过从观察到的样本总体估计分布参数，我们可以深入了解看不见的数据。</p><p id="a375" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">作为本文的先决条件，首先理解微积分和概率论中的概念是很重要的，包括联合概率和条件概率、随机变量和概率密度函数。</p><p id="5a01" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">参数估计处理近似分布的<em class="mb">参数</em>，这意味着分布的类型通常是预先假定的，这决定了您将要估计的未知参数是什么(泊松的λ，高斯的μ和σ)。我在本文中使用的例子将是高斯。</p><blockquote class="mc md me"><p id="1322" class="lf lg mb lh b li lj ka lk ll lm kd ln mf lp lq lr mg lt lu lv mh lx ly lz ma ij bi translated">样本问题:假设您想知道森林中树的高度分布，作为对树木健康的纵向生态研究的一部分，但您今年唯一可用的数据是一名徒步旅行者记录的15棵树的样本。您希望回答的问题是:“我们可以用什么分布来模拟整个森林的树高？"</p></blockquote><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/da07758851523038d34a8b6dd14f1abb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*0YmMqZ0Pb1Cfqkn_PJuOHw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">观察数据(15个样本)的直方图，以及可能产生该直方图的4个高斯曲线示例。图片作者。</p></figure><h2 id="ed54" class="mj mk iq bd ml mm mn dn mo mp mq dp mr lo ms mt mu ls mv mw mx lw my mz na iw bi translated">符号的快速注释</h2><ul class=""><li id="8111" class="nb nc iq lh b li nd ll ne lo nf ls ng lw nh ma ni nj nk nl bi translated">θ是未知变量，在我们的高斯情况下，θ = (μ，σ)</li><li id="fb9a" class="nb nc iq lh b li nm ll nn lo no ls np lw nq ma ni nj nk nl bi translated"><em class="mb"> D </em>为所有观测数据，其中<em class="mb"> D = (x_1，x_2，…，x_n) </em></li></ul><h1 id="c108" class="nr mk iq bd ml ns nt nu mo nv nw nx mr kf ny kg mu ki nz kj mx kl oa km na ob bi translated">似然函数</h1><p id="64d4" class="pw-post-body-paragraph lf lg iq lh b li nd ka lk ll ne kd ln lo oc lq lr ls od lu lv lw oe ly lz ma ij bi translated">最大似然估计和贝叶斯估计的(几乎唯一的)共同点是它们依赖于所见数据(在我们的例子中，15个样本)的<strong class="lh ja">可能性</strong>。可能性描述了每个可能的参数值产生我们观察到的数据的可能性，由下式给出:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi of"><img src="../Images/f7d5892d6fc83d6b3591a9fe9cffc46b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vrH0FYL3XGNtfVcE2vn4Yg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">似然函数。图片作者。</p></figure><p id="b417" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">由于奇妙的<a class="ae le" href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables" rel="noopener ugc nofollow" target="_blank">独立同分布假设</a>，所有的数据样本都被认为是独立的，因此我们能够放弃混乱的条件概率。</p><p id="5f0d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们回到我们的问题上。这需要知道我们15个样本的值，我们未知参数(μ，σ)的每个组合产生这组数据的概率是多少？通过使用<a class="ae le" href="https://en.wikipedia.org/wiki/Gaussian_function" rel="noopener ugc nofollow" target="_blank">高斯分布函数</a>，我们的似然函数是:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi og"><img src="../Images/8a0f31955ecdd95d3718e0b9cfec2369.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8hz_u2T6qnX_DhR7Lh_0Zg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">μ，σ上的似然函数。图片作者。</p></figure><h1 id="14a6" class="nr mk iq bd ml ns nt nu mo nv nw nx mr kf ny kg mu ki nz kj mx kl oa km na ob bi translated">最大似然估计</h1><p id="5fe5" class="pw-post-body-paragraph lf lg iq lh b li nd ka lk ll ne kd ln lo oc lq lr ls od lu lv lw oe ly lz ma ij bi translated">太棒了。现在你知道了似然函数，计算最大似然解就<em class="mb">真的很容易</em>。名字里就有。为了得到我们的估计参数(𝜃̂)，我们所要做的就是找到产生最大似然函数的参数。换句话说,(μ，σ)的什么组合给了我们上图中似然函数顶部最亮的黄点？</p><p id="4780" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了找到这个值，我们需要应用一点微积分和导数:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oh"><img src="../Images/b6e656022966b7b27d7e5b0acfc1062f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FHmPEUaBGXFbEXck369XsQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">(有问题的)𝜃̂.计算图片作者。</p></figure><p id="62a0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">你可能已经注意到了，我们遇到了一个问题。对产品进行衍生会变得非常复杂，我们希望避免这种情况。幸运的是，我们有办法解决这个问题:使用对数似然函数。回想一下:( 1)乘积的对数是对数的总和，以及(2)取任何函数的对数可能会改变数值，但不会改变该函数最大值出现的<em class="mb">,因此会给出相同的解。</em></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oi"><img src="../Images/ebe0f81bffd2369b2728e82d085fc53f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0O-bsGKbGyFlf2GQXJcXRg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">使用对数似然法正确计算𝜃̂。图片作者。</p></figure><p id="31ea" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">事实证明，对于一个高斯随机变量，最大似然解就是观测数据的均值和方差。因此，对于我们的问题，模拟树高分布的MLE解是μ=152.62和σ =11.27的高斯分布。</p><h1 id="958e" class="nr mk iq bd ml ns nt nu mo nv nw nx mr kf ny kg mu ki nz kj mx kl oa km na ob bi translated">贝叶斯估计</h1><blockquote class="mc md me"><p id="2514" class="lf lg mb lh b li lj ka lk ll lm kd ln mf lp lq lr mg lt lu lv mh lx ly lz ma ij bi translated">好消息！为了帮助您搜索该森林的树高分布，您的同事设法进入数据档案，挖掘出该森林过去10年的树高平均值。有了这些信息，您现在可以另外使用贝叶斯估计来解决这个问题。</p></blockquote><p id="8097" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">贝叶斯估计背后的中心思想是，在我们看到任何数据之前，我们已经有一些关于它来自的分布的先验知识。这种先验知识通常来自经验或过去的实验。然而，在进入这种方法的本质之前，掌握贝叶斯定理的概念是至关重要的。</p><h2 id="0d76" class="mj mk iq bd ml mm mn dn mo mp mq dp mr lo ms mt mu ls mv mw mx lw my mz na iw bi translated">贝叶斯定理</h2><p id="216b" class="pw-post-body-paragraph lf lg iq lh b li nd ka lk ll ne kd ln lo oc lq lr ls od lu lv lw oe ly lz ma ij bi translated">希望你知道，或者至少听说过概率环境中的贝叶斯定理，我们希望找到一个事件以另一个事件为条件的概率。在这里，我希望以一种能洞察贝叶斯参数估计和先验重要性的方式来构建它。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oj"><img src="../Images/0f77df497a00d5324983f6b3a48e18a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8a7JVOwbEY_xoVr0luyFYw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">贝叶斯定理。图片作者。</p></figure><p id="975d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了说明这个等式，考虑事件<em class="mb"> A =“今天早些时候下雨”</em>和事件<em class="mb"> B =“草地是湿的”</em>的例子，我们希望计算<em class="mb"> P(A|B) </em>，假定草地是湿的，早些时候下雨的概率。为此，我们必须计算<em class="mb"> P(B|A) </em>、<em class="mb"> P(B) </em>、<em class="mb"> P(A) </em>。条件概率<em class="mb"> P(B|A) </em>表示假定下雨，草地是湿的概率。换句话说，假设下雨，草地潮湿的可能性是<strong class="lh ja"><em class="mb"/></strong>。</p><p id="fa4d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="mb"> P(A) </em>的值被称为<strong class="lh ja"> <em class="mb">先验</em> </strong>:不管草是否潮湿(在知道草的状态之前)下雨的概率。这种先验知识很关键，因为它决定了我们对可能性的加权程度。如果我们在不经常下雨的地方，我们会更倾向于将湿草归因于雨水以外的东西，如露水或洒水器，这是由低的<em class="mb"> P(A) </em>值捕获的。然而，如果我们在一个经常下雨的地方，湿草更有可能是雨水的副产品，高的P(A)会反映这一点。</p><p id="cf02" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">剩下的就是<em class="mb"> P(B) </em>，也被称为<strong class="lh ja"> <em class="mb">证据</em> </strong>:草地潮湿的概率，这一事件充当了下雨事实的证据。这个值的一个重要属性是，它是最终概率的一个归一化常数，正如你将很快在贝叶斯估计中看到的，我们用一个归一化因子代替传统的“证据”</p></div><div class="ab cl ok ol hu om" role="separator"><span class="on bw bk oo op oq"/><span class="on bw bk oo op oq"/><span class="on bw bk oo op"/></div><div class="ij ik il im in"><p id="74be" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">用于贝叶斯估计的等式与贝叶斯定理具有相同的形式，主要区别在于我们现在使用模型和概率密度函数(pdf)来代替数字概率。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi or"><img src="../Images/f3a1689fa895eb3d6d85d76366fa3109.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wI0s2dEdQ1xdStsnOQVybw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">贝叶斯估计。图片作者。</p></figure><p id="783e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">请注意，首先，似然性等同于MLE中使用的似然性，其次，贝叶斯定理中通常使用的证据(在这种情况下将转化为<em class="mb"> P(D) </em>)被分子的积分所取代。这是因为(1) <em class="mb"> P(D) </em>实际计算起来极其困难，(2) <em class="mb"> P(D) </em>不依赖于θ，这才是我们真正关心的，(3)它作为归一化因子的可用性可以替代积分值，保证后验分布的积分为1。</p><p id="55aa" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">回想一下，为了求解MLE中的参数，我们采用对数似然函数的argmax来获得(μ，σ)的数值解。在贝叶斯估计中，我们改为计算参数空间上的分布，称为<strong class="lh ja"> <em class="mb">后验pdf </em> </strong>，表示为<em class="mb"> p(θ|D) </em>。这种分布表示在考虑了观察到的数据和先验知识之后，我们有多强烈地相信每个参数值是产生我们的数据的参数值。</p><p id="423c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">先验<em class="mb"> p(θ) </em>也是分布，通常与后验分布同类型。我不会深入细节，但当先验分布与后验分布匹配时，它被称为<a class="ae le" href="https://en.wikipedia.org/wiki/Conjugate_prior" rel="noopener ugc nofollow" target="_blank">共轭先验</a>，并带来许多计算上的好处。我们的例子将使用共轭先验。</p><p id="f8db" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们再一次回到树高的问题上来。除了徒步旅行者记录的15棵树之外，我们现在有了过去10年的树高的方法。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi os"><img src="../Images/5be9662c2a29e250ec3d73c9ef96e7d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*fF2g1q-NQa7KfoytuiDMJg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">过去10年的树高。图片作者。</p></figure><p id="974c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">假设今年的树高应落入所有前一年的分布中，我们的先验分布为高斯分布，μ=159.2，σ =9.3。</p><p id="3c5c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">剩下的就是计算我们的后验pdf。对于这个计算，我假设固定的σ = σ _MLE = 11.27。在现实中，我们不会以这种方式解决贝叶斯估计，但不同维度的高斯相乘，如我们的似然和先验，是极其复杂的，我相信在这种情况下简化计算足以理解这一过程，并且更容易可视化。如果你想要更多关于如何执行完整计算的资源，请查看<a class="ae le" href="https://stats.stackexchange.com/questions/474064/how-to-multiply-a-likelihood-by-a-prior" rel="noopener ugc nofollow" target="_blank">这些</a> <a class="ae le" href="https://people.eecs.berkeley.edu/~jordan/courses/281A-fall02/lectures/lecture9.ps" rel="noopener ugc nofollow" target="_blank">两个</a>链接。</p><p id="e88c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">将单变量似然和先验相乘，然后将结果归一化，我们最终得到μ=155.85和σ =7.05的后验高斯分布。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/20a8c257ccd969e313d671ebb3e0d475.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/format:webp/1*rP6v9jssJrm_leW3WHnguA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">似然、先验和后验分布。作者图片</p></figure><p id="5405" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这就是贝叶斯估计的结果。如您所见，后验分布考虑了先验和可能性，以找到两者之间的中间点。根据新的观测数据，当前的后验概率成为新的先验概率，并且用新数据给出的可能性计算新的后验概率。</p><h1 id="202a" class="nr mk iq bd ml ns nt nu mo nv nw nx mr kf ny kg mu ki nz kj mx kl oa km na ob bi translated">预言</h1><p id="a5f1" class="pw-post-body-paragraph lf lg iq lh b li nd ka lk ll ne kd ln lo oc lq lr ls od lu lv lw oe ly lz ma ij bi translated">我们有模型来描述我们的数据，那么我们能用它们做什么呢？这些模型最重要的用途是对看不见的未来数据进行预测，这基本上告诉我们一个观察结果有多大可能来自这个分布。我不会明确地为我们的例子进行计算，但是如果你想自己做的话，公式在下面。</p><h2 id="3380" class="mj mk iq bd ml mm mn dn mo mp mq dp mr lo ms mt mu ls mv mw mx lw my mz na iw bi translated">最大似然预测</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ou"><img src="../Images/7a3cf2124b55db4057740335516b3af3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cfoEQSTET0XhNFUC3zPE6w.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">MLE预测。图片作者。</p></figure><p id="3c88" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">最大似然预测利用密度函数中潜在变量的预测来计算概率。例如，在高斯情况下，我们使用(μ，σ)的最大似然解来计算预测。</p><h2 id="adb3" class="mj mk iq bd ml mm mn dn mo mp mq dp mr lo ms mt mu ls mv mw mx lw my mz na iw bi translated">贝叶斯预测</h2><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ov"><img src="../Images/3b11e91190d7f0e089f212bed31209c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ACck6JAKFX1lNjaBeFL_bA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">贝叶斯预测。图片作者。</p></figure><p id="11a7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">正如您可能猜到的那样，贝叶斯预测稍微复杂一些，它使用后验分布和随机变量θ的分布来产生新样本的预测。</p><h1 id="0065" class="nr mk iq bd ml ns nt nu mo nv nw nx mr kf ny kg mu ki nz kj mx kl oa km na ob bi translated">结束语</h1><h2 id="7e74" class="mj mk iq bd ml mm mn dn mo mp mq dp mr lo ms mt mu ls mv mw mx lw my mz na iw bi translated">什么时候用MLE？贝叶斯估计？</h2><p id="a5ed" class="pw-post-body-paragraph lf lg iq lh b li nd ka lk ll ne kd ln lo oc lq lr ls od lu lv lw oe ly lz ma ij bi translated">我们已经看到了两种参数估计方法之间的计算差异，现在一个自然的问题是:什么时候我应该使用一种而不是另一种？虽然在选择方法时没有硬性规定，但我希望你可以使用以下问题作为粗略的指导方针，引导你朝着正确的方向前进:</p><ul class=""><li id="8d7a" class="nb nc iq lh b li lj ll lm lo ow ls ox lw oy ma ni nj nk nl bi translated">您正在处理多少数据？</li></ul><p id="309c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">最大似然估计只依赖于<em class="mb">观察到的</em>数据的结果，众所周知，当数据最少时，它很容易出现偏差。考虑一个实验，你掷一枚硬币三次，每次都是正面朝上。虽然你知道一枚公平的硬币有50%的机会正面朝上，但最大似然估计告诉你<em class="mb"> P(正面)= </em> 1，而<em class="mb"> P(反面)= </em> 0。在观测数据稀疏的情况下，贝叶斯估计结合了先验知识，例如知道一个公平的硬币是50/50，可以帮助获得更准确的模型。</p><ul class=""><li id="a810" class="nb nc iq lh b li lj ll lm lo ow ls ox lw oy ma ni nj nk nl bi translated">你对你的问题有可靠的先验知识吗？</li></ul><p id="8f10" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">正如我刚才提到的，在某些情况下，先前的信念可以让你的模型受益。然而，不可靠的先验可能导致高度<a class="ae le" rel="noopener" target="_blank" href="/the-truth-about-bayesian-priors-and-overfitting-84e24d3a1153">偏差模型</a>的滑坡，需要大量的观察数据来补救。确保如果你使用先验知识，它们被很好地定义，并且<strong class="lh ja"> <em class="mb">包含与你试图解决的问题相关的洞察力</em> </strong>。如果你不确定你的病史的可靠性，最大似然法可能是一个更好的选择，尤其是如果你有足够多的数据。</p><ul class=""><li id="3169" class="nb nc iq lh b li lj ll lm lo ow ls ox lw oy ma ni nj nk nl bi translated">您的计算资源有限吗？</li></ul><p id="2f3b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">本文中一个反复出现的主题是贝叶斯计算比最大似然估计更复杂。凭借现代的计算能力，这种差异可能无关紧要，但是如果您发现自己受到资源的限制，MLE可能是您的最佳选择。</p><h2 id="b5ec" class="mj mk iq bd ml mm mn dn mo mp mq dp mr lo ms mt mu ls mv mw mx lw my mz na iw bi translated">一些有趣的属性</h2><p id="291a" class="pw-post-body-paragraph lf lg iq lh b li nd ka lk ll ne kd ln lo oc lq lr ls od lu lv lw oe ly lz ma ij bi translated">最后，已经讨论了MLE和贝叶斯估计之间的许多差异，我只想提供这两种方法之间的一些有趣的联系。</p><ol class=""><li id="1c11" class="nb nc iq lh b li lj ll lm lo ow ls ox lw oy ma oz nj nk nl bi translated">如果贝叶斯先验在所有值上是一致的(“非信息先验”)，贝叶斯预测将非常相似，如果<a class="ae le" href="https://www.researchgate.net/post/Can-anyone-explain-why-in-some-cases-Bayesian-estimation-gives-the-same-result-as-maximum-likelihood-estimation" rel="noopener ugc nofollow" target="_blank">不等于</a>，则MLE预测。</li><li id="fe61" class="nb nc iq lh b li nm ll nn lo no ls np lw nq ma oz nj nk nl bi translated">如果贝叶斯先验是明确定义的，并且在所有点上都不为零，那么，随着观测数据量接近无穷大，MLE和贝叶斯预测将<a class="ae le" href="http://www.ccs.neu.edu/home/rjw/csg220/lectures/MLE-vs-Bayes.pdf" rel="noopener ugc nofollow" target="_blank">收敛到相同的值</a>。</li></ol></div><div class="ab cl ok ol hu om" role="separator"><span class="on bw bk oo op oq"/><span class="on bw bk oo op oq"/><span class="on bw bk oo op"/></div><div class="ij ik il im in"><p id="d0ee" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">仅此而已！如果你读到这里，感谢你的阅读。感谢任何反馈！</p><ul class=""><li id="0a43" class="nb nc iq lh b li lj ll lm lo ow ls ox lw oy ma ni nj nk nl bi translated"><a class="ae le" href="https://github.com/luluricketts/misc_medium/blob/main/mle_bayes/MLE-Bayes.ipynb" rel="noopener ugc nofollow" target="_blank">链接到代码</a></li></ul></div></div>    
</body>
</html>