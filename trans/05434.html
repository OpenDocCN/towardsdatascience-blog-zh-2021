<html>
<head>
<title>A Simple Outline of Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">强化学习的简单概述</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-simple-outline-of-reinforcement-learning-4c20ddc497c9?source=collection_archive---------33-----------------------#2021-05-13">https://towardsdatascience.com/a-simple-outline-of-reinforcement-learning-4c20ddc497c9?source=collection_archive---------33-----------------------#2021-05-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="77dc" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">让我们来一次人工智能和控制相遇的旅行</h2></div><blockquote class="kf kg kh"><p id="a2cb" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">“我们想要的是一台能够从经验中学习的机器。”艾伦·图灵，1947年</p></blockquote><p id="a59f" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">你知道机器(或计算机)是如何在像国际象棋和围棋(DeepMind的AlphaGo、AlphaZero和MuZero)这样的复杂游戏中超越人类的表现，或者在没有人类干预的情况下驾驶汽车的吗？答案隐藏在它们的编程方式中。这种机器被编程为最大化某些目标，这些目标是由人类明确定义的。这种方法被称为强化学习，完全模仿人类和动物在世界中学习行为。</p><p id="abbb" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">在这篇文章中，我的目的是解释什么是强化学习和基础知识，而不是太多的细节。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi li"><img src="../Images/55983fa156c9c9ed822c9686697c2937.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*-JN_NSxktVfZWY_a"/></div></div><p class="lu lv gj gh gi lw lx bd b be z dk translated"><a class="ae ly" href="https://unsplash.com/@ninjason?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">梁杰森</a>在<a class="ae ly" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="2338" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">是什么让强化学习如此特别？</h1><p id="3a78" class="pw-post-body-paragraph ki kj iq kl b km mr jr ko kp ms ju kr lf mt ku kv lg mu ky kz lh mv lc ld le ij bi translated">今天，机器学习由3个主要范例概括。除了有监督学习(SL)和无监督学习(UL)，强化学习(RL)形成了第三种也是最后一种。虽然它是机器学习的一个子领域，但它有控制理论和博弈论的缺点。</p><p id="4fda" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">我们可以把智能代理想象成一种功能，当输入(或观察)时，期望它给出有用的输出(或动作)。学习只是越来越接近给出一个正确的(或有用的)输出。</p><p id="e493" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">在二语习得中，智能体被输入和相应的正确输出所反馈，以使它学会给看不见的输入合理的输出。</p><p id="d343" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">UL只给智能体输入信息，并让智能体学会给出一些输出信息，从而优化一个预定义的目标，如紧密度或熵之类的度量。</p><p id="5685" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">当代理需要通过在动态环境中反复试验来学习行为时，RL的问题就出现了。这就是人类和动物如何学习他们的行为，如说话和运动。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi mw"><img src="../Images/840b3d278333d98b1260aa8c539cb3f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JVqOSrN7hYEoyILdofwY0g.png"/></div></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">学习类型的图解，照片由<a class="ae ly" href="https://developer.ibm.com/articles/cc-models-machine-learning/" rel="noopener ugc nofollow" target="_blank"> IBM </a>拍摄</p></figure><p id="6250" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">不像上面所说的，学习方法不需要严格区分。如果从更大的角度来看，它们都是基于某些目标的优化。近年来，许多成功的人工智能应用是这三种范式的混合，如自监督学习、逆强化学习等。最近在ML上的成功隐藏在定义适当的目标，而不是选择3种学习类型中的一种。</p><p id="a2fc" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">但是，RL仍然是唯一的，因为它处理的是动态环境，数据不是事先给定的。因此，与SL和UL不同，RL代理应该:</p><ul class=""><li id="8716" class="mx my iq kl b km kn kp kq lf mz lg na lh nb le nc nd ne nf bi translated">收集数据本身，</li><li id="4058" class="mx my iq kl b km ng kp nh lf ni lg nj lh nk le nc nd ne nf bi translated">探索环境(学习如何收集有用的数据)，</li><li id="461b" class="mx my iq kl b km ng kp nh lf ni lg nj lh nk le nc nd ne nf bi translated">利用其知识(学习如何考虑未来状态，学习最佳策略)，</li><li id="30c2" class="mx my iq kl b km ng kp nh lf ni lg nj lh nk le nc nd ne nf bi translated">辨别后果的原因(过去的哪些行为导致了当前的情况)，</li><li id="0040" class="mx my iq kl b km ng kp nh lf ni lg nj lh nk le nc nd ne nf bi translated">在探索过程中保持自身安全(对于机械应用)。</li></ul><h2 id="e7b6" class="nl ma iq bd mb nm nn dn mf no np dp mj lf nq nr ml lg ns nt mn lh nu nv mp nw bi translated">勘探开发困境</h2><p id="395f" class="pw-post-body-paragraph ki kj iq kl b km mr jr ko kp ms ju kr lf mt ku kv lg mu ky kz lh mv lc ld le ij bi translated">如上所述，代理应该<em class="kk">利用</em>它的知识来获得最优策略。然而，如果没有足够的环境知识，agent可能会陷入局部最优解。因此，特工应该同时<em class="kk">探索</em>环境。这就出现了勘探-开发的困境，这是RL的重要组成部分。许多算法有各种技术来平衡它们。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi nx"><img src="../Images/a61ea0315b32284b1545cc778925b75b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UW7kUxeh1sbeELzwhZ7U8A.png"/></div></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">代理人应该在哪里吃饭？，照片由<a class="ae ly" href="http://ai.berkeley.edu/slides/Lecture%2011%20--%20Reinforcement%20Learning%20II/SP14%20CS188%20Lecture%2011%20--%20Reinforcement%20Learning%20II.pptx" rel="noopener ugc nofollow" target="_blank">T5】加州大学伯克利分校CS188 AI课程T7】</a></p></figure><p id="f38f" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">现在，让我们进入技术细节。</p><h1 id="fc7e" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">顺序决策</h1><p id="88c1" class="pw-post-body-paragraph ki kj iq kl b km mr jr ko kp ms ju kr lf mt ku kv lg mu ky kz lh mv lc ld le ij bi translated">顺序决策或离散控制过程是在离散时间的每个时间步做出决策，考虑环境的动态性。</p><p id="a7e9" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">在时间t，我们代理人在sₜ，受到rₜ奖励，得到oₜ观察，并根据其政策<strong class="kl ir"> π </strong>采取行动aₜ。作为行动的结果，sₜ₊₁发生状态转变，新观察oₜ₊₁以回报rₜ₊₁.</p><p id="0c76" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">期望观察值代表代理的状态。如果状态可以使用即时观察形成，观察直接用作状态(sₜ=oₜ),这被称为马尔可夫决策过程。如果不能，则称之为部分可观测马尔可夫决策过程，因为即时观测不能完全告知主体状态。然而，我们现在专注于MDP。</p><h2 id="c081" class="nl ma iq bd mb nm nn dn mf no np dp mj lf nq nr ml lg ns nt mn lh nu nv mp nw bi translated">马尔可夫决策过程(MDP)</h2><p id="5818" class="pw-post-body-paragraph ki kj iq kl b km mr jr ko kp ms ju kr lf mt ku kv lg mu ky kz lh mv lc ld le ij bi translated">MDP由以下部分组成:</p><ul class=""><li id="6f9a" class="mx my iq kl b km kn kp kq lf mz lg na lh nb le nc nd ne nf bi translated">状态空间S，作为所有可能状态的集合。</li><li id="ac27" class="mx my iq kl b km ng kp nh lf ni lg nj lh nk le nc nd ne nf bi translated">动作空间A，作为所有可能动作的集合。</li><li id="3641" class="mx my iq kl b km ng kp nh lf ni lg nj lh nk le nc nd ne nf bi translated">模型函数T(s'|s，a)，作为状态转移概率。</li><li id="9512" class="mx my iq kl b km ng kp nh lf ni lg nj lh nk le nc nd ne nf bi translated">奖励函数R(s)，作为从状态、动作、下一个状态元组到奖励的奖励映射。</li><li id="24fe" class="mx my iq kl b km ng kp nh lf ni lg nj lh nk le nc nd ne nf bi translated">贴现因子γ ∈ [0，1]，一个实数，决定未来奖励对控制目标的重要性。</li></ul><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/b41297334dce6d7606f4aa7d5a81ed04.png" data-original-src="https://miro.medium.com/v2/resize:fit:1324/format:webp/1*zvBfCEKABTBRmujyefi3vg.png"/></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">马尔可夫决策过程，作者图片</p></figure><h2 id="c985" class="nl ma iq bd mb nm nn dn mf no np dp mj lf nq nr ml lg ns nt mn lh nu nv mp nw bi translated">RL的建筑单元</h2><ul class=""><li id="3703" class="mx my iq kl b km mr kp ms lf oa lg ob lh oc le nc nd ne nf bi translated">策略函数π(a|s):取决于状态的行动的概率函数，指示在特定情况下如何行动。</li><li id="5453" class="mx my iq kl b km ng kp nh lf ni lg nj lh nk le nc nd ne nf bi translated">回报G:未来奖励在时间上的累积总和，按折现因子γ缩放。它被定义为:</li></ul><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi od"><img src="../Images/bcb41ba231e88ad02ac7be0d1ed7eb97.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/1*SUMgV6sFBU7BBGV4YLC3BQ.gif"/></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">返回值</p></figure><ul class=""><li id="dea7" class="mx my iq kl b km kn kp kq lf mz lg na lh nb le nc nd ne nf bi translated">价值函数V(s|π):状态s下遵循策略π时的期望收益，定义为；</li></ul><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/f9eabd4f66223fef054363df344eb7e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/1*sCf4aZq5Ji55kVpCzeZr0g.gif"/></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">价值函数</p></figure><ul class=""><li id="4ed9" class="mx my iq kl b km kn kp kq lf mz lg na lh nb le nc nd ne nf bi translated">动作值函数Q(s，a|π):遵循策略π时的期望收益，除了状态s第一步的动作a，定义为；</li></ul><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi of"><img src="../Images/ac015081977ac284396fdb602a3873ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:506/1*kg2Genq6u-foByG6pY_m2A.gif"/></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">动作值函数，又名Q函数</p></figure><h2 id="9812" class="nl ma iq bd mb nm nn dn mf no np dp mj lf nq nr ml lg ns nt mn lh nu nv mp nw bi translated">贝尔曼方程</h2><p id="f6d4" class="pw-post-body-paragraph ki kj iq kl b km mr jr ko kp ms ju kr lf mt ku kv lg mu ky kz lh mv lc ld le ij bi translated">RL的整体目标是最大化价值函数V，以获得最优策略π*。为此，价值函数必须满足以下贝尔曼方程。贝尔曼方程告诉我们，最优策略必须以隐含的方式使所有可能状态的平均价值函数(未来的累积报酬)最大化。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi og"><img src="../Images/5be70342a253a02b0c3e68b28a6ca66b.png" data-original-src="https://miro.medium.com/v2/resize:fit:828/1*Du5YIc1Mq-N1OH6QR_N7EQ.gif"/></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">贝尔曼方程</p></figure><p id="916e" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">你可能想知道为什么我们需要Q函数。它与政策有着直接的关系。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/7bc4d34c42415f7163bb149a1e6c72e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:322/1*GL_TQLzZnO6u-ILXMMzeCA.gif"/></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">Q函数的策略推导</p></figure><p id="878b" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">注意，两者都是相互依赖的。那么，为什么我们需要另一个定义呢？通过观察环境和行为的结果，代理可以学习当前策略的Q函数。然后，代理可以使用这个等式来改进它的策略。它允许代理在学习Q函数的同时，通过学习来改进策略。</p><h2 id="2eea" class="nl ma iq bd mb nm nn dn mf no np dp mj lf nq nr ml lg ns nt mn lh nu nv mp nw bi translated">无模型强化学习</h2><p id="66d7" class="pw-post-body-paragraph ki kj iq kl b km mr jr ko kp ms ju kr lf mt ku kv lg mu ky kz lh mv lc ld le ij bi translated">无模型RL纯粹基于经验，没有模型和奖励函数。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/417c57e0eebf0a27594fe914ee02d2ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1334/format:webp/1*xY9ydZ_QCM-gJ9DzFq8ATw.png"/></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">照片由<a class="ae ly" href="http://ai.berkeley.edu/slides/Lecture%2010%20--%20Reinforcement%20Learning%20I/SP14%20CS188%20Lecture%2010%20--%20Reinforcement%20Learning%20I.pptx" rel="noopener ugc nofollow" target="_blank"> <em class="ny"> UC Berkeley CS188 AI课程</em> </a></p></figure><h2 id="cf7e" class="nl ma iq bd mb nm nn dn mf no np dp mj lf nq nr ml lg ns nt mn lh nu nv mp nw bi translated">蒙特卡罗方法</h2><p id="de3b" class="pw-post-body-paragraph ki kj iq kl b km mr jr ko kp ms ju kr lf mt ku kv lg mu ky kz lh mv lc ld le ij bi translated">蒙特卡罗方法使用统计抽样来逼近Q函数。为了使用这种方法，必须等到模拟结束，因为每个状态都使用将来的累积奖励总和。</p><p id="f53c" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">一旦sₜ被访问，并采取行动aₜ，回报Gₜ是计算从其定义使用即时和未来奖励等待结束的一集。蒙特卡罗方法旨在最小化所有可能样本的Q(sₜ,aₜ和目标值Gₜ之间的差距。</p><p id="4a8a" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">利用预定的学习速率α，Q函数被更新为:</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/c7d2652055c64f2537063b7163ba12ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/1*cUzL09PsnTqTf1IP3Hlvog.gif"/></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">蒙特卡洛Q更新</p></figure><h2 id="9d38" class="nl ma iq bd mb nm nn dn mf no np dp mj lf nq nr ml lg ns nt mn lh nu nv mp nw bi translated">时间差分法</h2><p id="1733" class="pw-post-body-paragraph ki kj iq kl b km mr jr ko kp ms ju kr lf mt ku kv lg mu ky kz lh mv lc ld le ij bi translated">目标返回的时间差分(TD)方法bootstrap Q函数估计。这允许代理从每个奖励更新Q函数。</p><p id="023f" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">主要的TD方法是SARSA和Q学习。</p><ul class=""><li id="b216" class="mx my iq kl b km kn kp kq lf mz lg na lh nb le nc nd ne nf bi translated">SARSA最小化Q(sₜ,aₜ和目标值rₜ₊₁+γ Q(sₜ₊₁,aₜ₊₁之间的差距，目标值是收益的自举估计。因为还需要下一个动作，所以它被命名为SARSA，表示状态、动作、奖励、状态、动作序列。学习率为α时，Q更新为:</li></ul><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/818a6606093d9862d38eacde51d31735.png" data-original-src="https://miro.medium.com/v2/resize:fit:742/1*RE_RjVLpv2zHYzRHFjf8eQ.gif"/></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">SARSA Q更新</p></figure><ul class=""><li id="30e1" class="mx my iq kl b km kn kp kq lf mz lg na lh nb le nc nd ne nf bi translated">q学习最小化差距Q(sₜ,aₜ)和目标值rₜ₊₁+γ最大Q(sₜ₊₁)，这也是一个收益的自举估计。与SARSA不同，它不需要下一个动作，并假设在下一个状态采取最佳动作。学习率为α时，Q更新为:</li></ul><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/7cde47522b6ac393eb5e4dca9f8ed670.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/1*HQR9PvHi0AmSj4mzLm_xVw.gif"/></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">Q学习Q更新</p></figure><h1 id="ecb8" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">基于模型的强化学习</h1><p id="fd60" class="pw-post-body-paragraph ki kj iq kl b km mr jr ko kp ms ju kr lf mt ku kv lg mu ky kz lh mv lc ld le ij bi translated">在基于模型的强化学习中，模型和奖励函数要么预先给定，要么通过SL方法学习。</p><p id="ba57" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">基于模型的RL依赖于学习模型的质量。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi om"><img src="../Images/037682fd484be8bb9caa8725394450af.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7xdSHhAPhwkWoSWzKsUUJA.png"/></div></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">照片由<a class="ae ly" href="http://ai.berkeley.edu/slides/Lecture%2010%20--%20Reinforcement%20Learning%20I/SP14%20CS188%20Lecture%2010%20--%20Reinforcement%20Learning%20I.pptx" rel="noopener ugc nofollow" target="_blank"> <em class="ny"> UC Berkeley CS188 AI课程</em> </a></p></figure><h2 id="8e4d" class="nl ma iq bd mb nm nn dn mf no np dp mj lf nq nr ml lg ns nt mn lh nu nv mp nw bi translated">动态规划</h2><p id="3d47" class="pw-post-body-paragraph ki kj iq kl b km mr jr ko kp ms ju kr lf mt ku kv lg mu ky kz lh mv lc ld le ij bi translated">动态编程不需要采样。它是在给定模型和报酬函数的情况下解析求解最优策略。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div class="gh gi on"><img src="../Images/8e8a512d8d47e2fd5bdb309df8b87511.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/1*tmPF5iVIjMKmRVJwUUajeg.gif"/></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">动态编程Q更新</p></figure><h2 id="64d1" class="nl ma iq bd mb nm nn dn mf no np dp mj lf nq nr ml lg ns nt mn lh nu nv mp nw bi translated">基于模拟的搜索</h2><p id="fcfd" class="pw-post-body-paragraph ki kj iq kl b km mr jr ko kp ms ju kr lf mt ku kv lg mu ky kz lh mv lc ld le ij bi translated">有时，状态和动作空间太大。这使得动态编程不适用。在这种情况下，会生成随机模拟路径。其余的是通常的无模型RL，可以使用蒙特卡罗或时间差分方法。这种方法唯一优点是代理从它想要的任何状态开始模拟。</p><p id="dde4" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">蒙特卡洛法、时间差分法和动态规划法之间的比较如下所示。</p><figure class="lj lk ll lm gt ln gh gi paragraph-image"><div role="button" tabindex="0" class="lo lp di lq bf lr"><div class="gh gi oo"><img src="../Images/9825cf1da2fa855f394a9023b7819ced.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TlHmZLmY0MvwEwoBBapJTA.png"/></div></div><p class="lu lv gj gh gi lw lx bd b be z dk translated">三种主要方法的备份图，照片由<a class="ae ly" href="https://www.davidsilver.uk/wp-content/uploads/2020/03/MC-TD.pdf" rel="noopener ugc nofollow" target="_blank">大卫·西尔弗的RL课程，第4讲</a>拍摄</p></figure><h1 id="139c" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">结论</h1><p id="0aa5" class="pw-post-body-paragraph ki kj iq kl b km mr jr ko kp ms ju kr lf mt ku kv lg mu ky kz lh mv lc ld le ij bi translated">在大多数应用中，RL是通过称为深度强化学习的深度学习来结合的。最近RL的成功与神经网络有关。然而，由于样本学习效率低和安全限制，RL难以适应真实世界的场景。RL的未来取决于我们对人类学习方式的掌握。</p><p id="5cfe" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">总的来说，RL不过是在动态环境中学习。有许多RL算法和方法，但我试图给出核心定义和算法，让初学者有一个关于它的想法，保持定义尽可能简单。我希望你喜欢！</p><p id="ae8f" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr lf kt ku kv lg kx ky kz lh lb lc ld le ij bi translated">作为临时演员，我把OpenAI的捉迷藏模拟留给RL。</p><figure class="lj lk ll lm gt ln"><div class="bz fp l di"><div class="op oq l"/></div></figure></div></div>    
</body>
</html>