<html>
<head>
<title>Short-Text Topic Modelling: LDA vs GSDMM</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">短文本主题建模:LDA与GSM</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/short-text-topic-modelling-lda-vs-gsdmm-20f1db742e14?source=collection_archive---------2-----------------------#2021-06-17">https://towardsdatascience.com/short-text-topic-modelling-lda-vs-gsdmm-20f1db742e14?source=collection_archive---------2-----------------------#2021-06-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="851c" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用阿拉伯推特数据对短文本文档的两种自然语言处理主题建模方法的比较分析</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/183476367af5c8654bc3114de836a9bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ReQDtRA_6SjwCL0ewEWp3g.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过iStock向作者许可的图像</p></figure><p id="058e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我对应用于短文本文档的两种主题建模方法进行了比较分析，例如推特:<strong class="la iu">潜在狄利克雷分配</strong> (LDA)和<strong class="la iu"> Gibbs抽样狄利克雷多项式混合</strong>(GSM)。我解释了算法中的主要差异，提供了它们如何在幕后操作的直观性，解释了每种算法的预处理要求，并评估了它们在聚类不同数量的短文本文档时的比较性能。</p><p id="bed6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">这篇文章是</em> <a class="ae lv" rel="noopener" target="_blank" href="/identifying-arabic-language-political-misinformation-on-twitter-3a614c489c00"> <em class="lu">一个更大的项目</em> </a> <em class="lu">的副产品，在这个项目中，我使用了主题建模和聚类来从3600万条阿拉伯语推特中识别政治错误信息内容。</em></p><p id="860a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae lv" href="https://www.linkedin.com/in/richard-pelgrim/" rel="noopener ugc nofollow" target="_blank">在领英上跟我来</a>获取常规数据科学内容。</p><h1 id="4629" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">差异</h1><p id="bbfa" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">很可能您在登录本页之前就已经听说过LDA了。这是目前最流行的主题建模算法。然而，GSM并不享有同样的地位。这是一种鲜为人知的主题建模算法，因此也没有得到广泛的实施。有点像贾斯汀比伯和他同母异父的兄弟杰克森。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/f0de59d49743fd781e18f24767eb36f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/1*Pu0LqeBXQXTEs5OCGGEy9A.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图像通过<a class="ae lv" href="https://giphy.com/gifs/teamcoco-conan-obrien-3oFzm733B3O1nHHQek" rel="noopener ugc nofollow" target="_blank">giphy.com</a></p></figure><p id="b1b6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">撇开流行明星评级不谈，这两种方法的主要区别在于，LDA假设每个文档(在我们的案例中是推特)都由多个主题组成，并计算每个主题对文档的贡献。另一方面，GSM专门针对检测较小文档中的主题，并假设<strong class="la iu">每个文档只有一个主题。</strong>这就是为什么<a class="ae lv" rel="noopener" target="_blank" href="/a-unique-approach-to-short-text-clustering-part-1-algorithmic-theory-4d4fad0882e1">有人认为</a>更适合推特这样的微博内容的话题建模。</p><p id="a1db" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这两种方法之间的另一个区别是，LDA要求预先设置主题的数量，而GSM只要求一个上限(max。主题的数量)并将从那里工作以从数据推断主题的数量。</p><h1 id="575e" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">LDA是如何工作的</h1><h2 id="ac0e" class="mu lx it bd ly mv mw dn mc mx my dp mg lh mz na mi ll nb nc mk lp nd ne mm nf bi translated">直觉</h2><p id="764e" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated"><a class="ae lv" rel="noopener" target="_blank" href="/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158#:~:text=Topic%20modelling%20refers%20to%20the,describes%20a%20set%20of%20documents.&amp;text=And%20the%20goal%20of%20LDA,captured%20by%20those%20imaginary%20topics.">这篇文章</a>提供了一个关于LDA如何在幕后工作的很好的介绍性解释(如果你愿意的话，还会探究这一切背后的数学)。简而言之，LDA假设<strong class="la iu">每个文档由主题分布组成，每个主题又由单词分布组成</strong>。既然我们已经有了我们的文档(tweets)和我们的文字，那么LDA算法的任务就是构建连接两者的隐藏(“潜在”)层:主题。</p><p id="931d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因为LDA假设每个文档有多个主题，所以模型将返回每个主题对文档的百分比贡献的概率分布，例如<strong class="la iu"> 0.3 *主题_1，0.7 *主题_2 </strong>。这意味着30%的文档包含属于主题1的单词，剩余的70%包含属于主题2的单词。反过来，主题被定义为单词簇，这些单词簇也具有百分比贡献。所以Topic_1可能由0.3 *面粉、0.2 *黄油、0.2 *奶油、0.1 *苹果、0.1 *樱桃、0.1 *黑莓组成……这意味着Topic_1很可能是不同种类的水果派食谱的集合。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ng nh l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由瑞切尔·亨特通过<a class="ae lv" href="https://giphy.com/gifs/foxadhd-artists-on-tumblr-rachael-hunt-ZGvR8iThPE5Xi" rel="noopener ugc nofollow" target="_blank">giphy.com</a>拍摄</p></figure><p id="840f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一旦建立了LDA模型，您就可以选择提取最主要的主题(即，具有最大百分比贡献的主题)或者将您的文档建模为主题的分布，这取决于您的用例。</p><h2 id="8707" class="mu lx it bd ly mv mw dn mc mx my dp mg lh mz na mi ll nb nc mk lp nd ne mm nf bi translated">为LD惊人的成功设置您的数据</h2><p id="6d63" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">下面是使用<strong class="la iu"> gensim </strong> python库建立LDA模型所需的预处理的分步脚本。请注意，我过滤掉了字典中的一些极端情况(经常出现或很少出现的单词，因此可能会扭曲数据)，并且我使用LDA的<strong class="la iu">多核</strong>实现来允许本地并行处理，以加快模型构建。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="abba" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我的机器上，当对615万条推文建模时，每个LDA实例需要大约1小时来完成。这意味着，根据数据的大小，可以相对容易地运行LDA模型的多个实例，以查看哪些数量的主题和其他超参数在捕获数据中的一致主题方面做得最好。</p><p id="4577" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了让您了解默认情况下和LDA模型的输出，下面是我的Jupyter笔记本的截图:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/9dd09339861491a278b9c66534f97ae4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ncuH31015ETD35cRQM0nAQ.png"/></div></div></figure><p id="f4e6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如你所看到的，每个主题都被打印成单词和它们对主题的贡献百分比的“等式”。这让我们对每个主题的内容有了很好的第一印象。</p><h2 id="e145" class="mu lx it bd ly mv mw dn mc mx my dp mg lh mz na mi ll nb nc mk lp nd ne mm nf bi translated">想象你惊人的结果</h2><p id="dca9" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">使用流行库的好处之一是，肯定会有一些很酷的其他库与您创建的输出很好地融合在一起。一个例子是pyLDAvis库，它出色地完成了LDA的粗略输出(见上图),并将其融入到magic sauce中，以创建输出的交互式HTML可视化报告。</p><p id="f474" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如你在下面的quick-n-dirty截屏中看到的，pyLDAvis报告在左侧的主题间距离图上绘制了主题(在我们的例子中是5个，参见上面脚本中的<strong class="la iu"> num_topics </strong>)。在右侧，报告列出了与每个主题最相关的前30个单词。你会发现一旦你选择了右边的一个单词，左边的圆圈就会移动。选择一个单词将显示该单词出现在哪个主题(一个或多个)中。圆圈将根据所选单词对特定主题的贡献百分比而增大。这确实有助于我们理解潜在主题层的内容和架构。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nj nh l"/></div></figure><h1 id="29a0" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">GSDMM的工作原理</h1><h2 id="cb99" class="mu lx it bd ly mv mw dn mc mx my dp mg lh mz na mi ll nb nc mk lp nd ne mm nf bi translated">直觉</h2><p id="2db9" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">虽然LDA在较大的文本(&gt; 50个单词左右)上表现很好，但在尝试对较短文本的主题进行建模时，它的性能往往会下降，原因很明显，一个简短的文本(如一条推文或一个论坛问题的标题)可能只涉及一个主题。</p><p id="14e0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">进入GSDMM:一个替代的主题建模算法，正是从这个假设出发。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/8f20a3cba8c63ef17dc37070ea5c980a.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*9XhLNdUYy3I_X-Qy45easA.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自giphy.com</p></figure><p id="a186" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了理解GSDMM是如何工作的，理解一个类似的概率过程是有帮助的，这个过程通常被称为<strong class="la iu">电影组过程</strong>。我将从Matyas Amrouche  *的这篇文章中复制一个简单的叙述式解释:</p><blockquote class="nl nm nn"><p id="203a" class="ky kz lu la b lb lc ju ld le lf jx lg no li lj lk np lm ln lo nq lq lr ls lt im bi translated">“想象一下，一群学生在一家餐馆里，随机坐在K张桌子旁。他们都被要求在纸上写下他们最喜欢的电影(但必须是一个简短的名单)。目标是以这样一种方式将他们聚集在一起，使得同一组中的学生分享相同的电影兴趣。为此，学生必须根据以下两条规则逐一选择新的桌子:</p><p id="6b9b" class="ky kz lu la b lb lc ju ld le lf jx lg no li lj lk np lm ln lo nq lq lr ls lt im bi translated">规则一:选择学生多的桌子。这个规则提高了完整性，所有对同一部电影感兴趣的学生都被分配到同一个桌子上。</p><p id="bb07" class="ky kz lu la b lb lc ju ld le lf jx lg no li lj lk np lm ln lo nq lq lr ls lt im bi translated">规则2:选择一张桌子，让学生分享相似电影的兴趣。这条规则旨在增加同质性，我们只希望成员们在同一张桌子上分享对同一部电影的兴趣。</p><p id="1e51" class="ky kz lu la b lb lc ju ld le lf jx lg no li lj lk np lm ln lo nq lq lr ls lt im bi translated">重复这个过程后，我们预计一些表格会消失，另一些会变得更大，最终会有一群学生符合他们对电影的兴趣。这就是GSDMM算法所做的事情！"</p></blockquote><h2 id="40c9" class="mu lx it bd ly mv mw dn mc mx my dp mg lh mz na mi ll nb nc mk lp nd ne mm nf bi translated">为gsdm的巨大成功设置您的数据</h2><p id="fce4" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">正如我前面提到的，GSDMM在Python库中的实现远不如它流行的表亲LDA。在尝试了一些不同的实现之后，我选定了这个Python包(T0 ),您可以使用以下命令直接从Github repo安装它:</p><p id="fad7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu"> pip安装git+https://github . com/rwalk/GSD mm . git</strong></p><p id="7edb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">下面是一个将数据预处理成所需格式的脚本:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="8e84" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">注意，GSDMM只需要两个输入变量:numpy数组形式的文档和字典/词汇表的长度。我们还创建了一个<strong class="la iu"> bow_corpus </strong>，因为稍后计算主题连贯性时它会派上用场。</p><p id="9673" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">拟合模型将生成如下所示的输出；您的模型在第一次迭代中被初始化为您指定的最大主题数(<strong class="la iu"> K </strong>)。然后，它总共运行<strong class="la iu"> n_iters </strong>次迭代，并试图将文档混洗到少于K个簇中。</p><p id="a0bb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我的机器上，<strong class="la iu">只用了17分钟就完成了10万条推文的子集</strong>(来自原始的615万条数据集)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/eed940ae3dd5e75170c243dc7a903d8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*B9HjdbK1dKGn_55wAWJsvQ.png"/></div></figure><p id="7a1f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们有了我们的GSDMM模型，我们可以使用下面的代码片段检查主题，该代码片段改编自<a class="ae lv" href="https://www.kaggle.com/ptfrwrd/topic-modeling-guide-gsdm-lda-lsi#GSDMM-for-the-topic-modeling:" rel="noopener ugc nofollow" target="_blank"> this Kaggle notebook </a>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="78fa" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这将生成一个集群总数的列表，其中包含前20个(在本例中)最常出现的单词，以及前20个单词在主题中出现的次数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/f9559ca388e73c7eea7b71c42048ba7e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xQZFi2KIVX0KCKSw9-WMfQ.png"/></div></div></figure><p id="b2d9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">据我所知，不幸的是，没有办法像显示LDA输出那样显示GSDMM输出。相反，我所做的是使用主题中的单词及其计数，格式化为python字典，为每个主题生成一个单词云。请参见下面的示例代码和输出。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ng nh l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nt"><img src="../Images/061dad6696e00487f9580c0594cbc5c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oE3YGy9BKBF5zCmFkQgsbQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">我的GSDMM主题群之一的输出示例</p></figure><h1 id="150e" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">比较性能</h1><p id="3607" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">现在我们知道了如何设置两种算法来输出我们的文档作为主题，是时候看看比伯兄弟中谁真的“比其他人漂亮得多”。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nj nh l"/></div></figure><p id="9deb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了比较这两种主题建模方法的性能，我选择关注3个关键参数:</p><ul class=""><li id="bf3a" class="nu nv it la b lb lc le lf lh nw ll nx lp ny lt nz oa ob oc bi translated">运行时间</li><li id="9f08" class="nu nv it la b lb od le oe lh of ll og lp oh lt nz oa ob oc bi translated">建模主题的连贯性和一致性</li><li id="3989" class="nu nv it la b lb od le oe lh of ll og lp oh lt nz oa ob oc bi translated">话题连贯性得分</li></ul><h2 id="3c2b" class="mu lx it bd ly mv mw dn mc mx my dp mg lh mz na mi ll nb nc mk lp nd ne mm nf bi translated">关于评估主题模型的一个注记</h2><p id="a81a" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">主题建模不是一门精确的科学。语境——就我而言，还有翻译——构成了一个可能含义的复杂网络，人眼通常比计算机更容易梳理出来(至少在经过适当训练之前)。这就是为什么我主要依靠对主题模型的视觉检查来评估每个模型的质量。</p><p id="b900" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将使用主题连贯性的客观测量作为额外的检查来验证这些目测检查。虽然主题连贯性可以给你的评估表现带来一种(诱人的)客观性错觉，但我发现用<a class="ae lv" href="https://stackoverflow.com/questions/54762690/what-is-the-meaning-of-coherence-score-0-4-is-it-good-or-bad" rel="noopener ugc nofollow" target="_blank">Stack Overflow thread</a>的一些发人深省的怀疑来平衡这一点很有帮助:</p><ul class=""><li id="7b58" class="nu nv it la b lb lc le lf lh nw ll nx lp ny lt nz oa ob oc bi translated">0.3是不好的</li><li id="1b17" class="nu nv it la b lb od le oe lh of ll og lp oh lt nz oa ob oc bi translated">0.4很低</li><li id="65f7" class="nu nv it la b lb od le oe lh of ll og lp oh lt nz oa ob oc bi translated">0.55没问题</li><li id="f660" class="nu nv it la b lb od le oe lh of ll og lp oh lt nz oa ob oc bi translated">0.65可能是最好的结果了</li><li id="7698" class="nu nv it la b lb od le oe lh of ll og lp oh lt nz oa ob oc bi translated">0.7就不错了</li><li id="43cf" class="nu nv it la b lb od le oe lh of ll og lp oh lt nz oa ob oc bi translated">0.8不太可能</li><li id="8bbf" class="nu nv it la b lb od le oe lh of ll og lp oh lt nz oa ob oc bi translated">0.9很可能是错的</li></ul><h2 id="8420" class="mu lx it bd ly mv mw dn mc mx my dp mg lh mz na mi ll nb nc mk lp nd ne mm nf bi translated">运行时间</h2><p id="f00f" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">您可能已经注意到，在上面对GSDMM的描述中，我在一个100K文档的子集上运行了这个模型(大约是我的总数据集的1.67%)。如果你一直在阅读，脑海里不停地萦绕着一个问题，关于我为什么要那样做……答案就在这里！</p><p id="ecd4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">GSDMM无法处理我总共615万条推文的数据集。至少在48小时的运行时间内没有。).</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/340699dfdfbf9d9c3692ffd85345e48b.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/1*x3BvUUpqFuxIL5yBFXhqUg.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自giphy.com<a class="ae lv" href="https://giphy.com/gifs/the-hunger-games-time-catching-fire-aAXtF4kGYeZFe" rel="noopener ugc nofollow" target="_blank"/></p></figure><p id="d701" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这意味着，就运行时间而言，LDA远远优于GSDMM(它在大约1小时内处理了完整的6M数据集)。不仅如此，出于所有实际目的，我们可以说GSDMM不能处理非常大的数据集，至少不能一次处理完。</p><p id="2445" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这对LDA来说是一次重大胜利。</p><p id="f6a6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="lu">注意:截至2021年6月，GSDMM还不能被并行化，尽管</em> <a class="ae lv" href="https://ojs.aaai.org/index.php/AAAI/article/view/4325/4203" rel="noopener ugc nofollow" target="_blank"> <em class="lu">本文</em> </a> <em class="lu">似乎暗示有一个正在开发的版本可以。</em></p><h2 id="5b56" class="mu lx it bd ly mv mw dn mc mx my dp mg lh mz na mi ll nb nc mk lp nd ne mm nf bi translated">主题建模质量:目视检查</h2><p id="7950" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">然而，实际考虑和可行性只是故事的一面。因为当我们在短文本文档上查看主题建模输出的质量时，我毫不怀疑GSDMM比LDA做得更好。在我构建的所有9次迭代中，GSDMM能够从我的项目的其余部分中一致地提取政治推文，而LDA的输出在每次迭代中变化很大，总体上更加混乱。</p><p id="720a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">GSDMM大获全胜。</p><p id="ec4c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi">1–1.</p><h2 id="361f" class="mu lx it bd ly mv mw dn mc mx my dp mg lh mz na mi ll nb nc mk lp nd ne mm nf bi translated">主题建模质量:连贯性得分</h2><p id="c829" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">话题连贯得分是一个客观的衡量标准，它植根于语言学的分布假设:意思相似的词往往出现在相似的语境中。如果所有或大部分单词密切相关，则认为主题是连贯的。更多细节见<a class="ae lv" href="http://www.saf21.eu/wp-content/uploads/2017/09/5004a165.pdf" rel="noopener ugc nofollow" target="_blank">这篇科学文章</a>。</p><p id="e19d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以使用gensim库中的<strong class="la iu"> CoherenceModel </strong>实例轻松计算主题一致性分数。对LDA来说，很简单:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="eea2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">GSDMM的实现需要多做一点工作，因为我们首先必须将主题中的单词作为列表列表(变量<strong class="la iu">主题</strong>)获取，然后将其输入gensim CoherenceModel。注意，这是我们使用之前创建的<strong class="la iu"> bow_corpus </strong>变量的地方。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="c0a2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在我的例子中，计算主题连贯性分数证实了我的眼球观察所观察到的:就其输出主题的连贯性和一致性而言，GSDMM远远优于LDA。</p><h1 id="af34" class="lw lx it bd ly lz ma mb mc md me mf mg jz mh ka mi kc mj kd mk kf ml kg mm mn bi translated">最终裁决</h1><p id="3cab" class="pw-post-body-paragraph ky kz it la b lb mo ju ld le mp jx lg lh mq lj lk ll mr ln lo lp ms lr ls lt im bi translated">那么…哪个比伯兄弟今晚会和我们一起回家？</p><p id="4763" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">嗯，在数据科学和机器学习的世界里，这完全取决于你的醉酒程度，我指的是你的用例。你想要一个速度快、对最终结果不那么模糊的比伯，还是一个愿意花时间(而不是以一种友好、自由的方式)一次处理一小部分数据的比伯？</p><p id="aae9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">总结一下，这里有一个表格，列出了每种方法的利与弊:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/70c279fd74e8d34258a96e17c4fb1802.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CPBCYXP8h8bmZM5CFDyOAQ.png"/></div></div></figure><p id="c106" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你和我一样，你当然会想要两个世界最好的东西。两个比一个好，是我的人生格言。具体来说，对于我的项目，我想要由GSDMM提供的细粒度、连贯的主题输出，但是我有一个最后期限，真的没有时间等48个小时以上。</p><p id="9a7d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以我建了一个变通办法。我将615万条推文的完整数据集输入LDA模型，作为对数据的第一次粗略传递。然后，我删除了那些被分配到我绝对确定与我感兴趣的话题(政治内容)无关的话题的推文。这使得推文数量减少到35万条(约17%)。然后，我将这个粗略过滤的子集输入到一个GSDMM模型中，得到了我正在寻找的高质量主题建模输出。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/6be6b3b820cb0ef592a7e089714e7fb3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bxyB1U3ItKw3sp-YDGxrmQ.png"/></div></div></figure><p id="b71f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">关于我如何做到这一点的完整实现代码，请点击这里查看我的项目的Jupyter笔记本。</p><p id="b876" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以也许根本不用选。</p></div><div class="ab cl ok ol hx om" role="separator"><span class="on bw bk oo op oq"/><span class="on bw bk oo op oq"/><span class="on bw bk oo op"/></div><div class="im in io ip iq"><p id="003c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">谢谢你能走到这一步！为了表达我的感激之情，我将留给你最后一个令人痛心的流行参考:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="or nh l"/></div></figure><p id="db79" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">点击它…我谅你也不敢！</p></div><div class="ab cl ok ol hx om" role="separator"><span class="on bw bk oo op oq"/><span class="on bw bk oo op oq"/><span class="on bw bk oo op"/></div><div class="im in io ip iq"><p id="ffd8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢您的阅读！<a class="ae lv" href="https://www.linkedin.com/in/richard-pelgrim/" rel="noopener ugc nofollow" target="_blank">在LinkedIn </a>上关注我，了解常规的数据科学和NLP更新和技巧。</p><p id="fbc1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">请考虑成为支持写作社区的媒体成员:</p><div class="os ot gp gr ou ov"><a href="https://richardpelgrim.medium.com/membership" rel="noopener follow" target="_blank"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd iu gy z fp pa fr fs pb fu fw is bi translated">使用此推荐链接加入媒体</h2><div class="pc l"><h3 class="bd b gy z fp pa fr fs pb fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="pd l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">richardpelgrim.medium.com</p></div></div><div class="pe l"><div class="pf l pg ph pi pe pj ks ov"/></div></div></a></div></div></div>    
</body>
</html>