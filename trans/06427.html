<html>
<head>
<title>Anomaly Detection in Manufacturing, Part 2: Building a Variational Autoencoder</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">制造业中的异常检测，第2部分:构建可变自动编码器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/anomaly-detection-in-manufacturing-part-2-building-a-variational-autoencoder-248abce07349?source=collection_archive---------10-----------------------#2021-06-09">https://towardsdatascience.com/anomaly-detection-in-manufacturing-part-2-building-a-variational-autoencoder-248abce07349?source=collection_archive---------10-----------------------#2021-06-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="8bb4" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/manf-ds-python" rel="noopener" target="_blank">用PYTHON制造数据科学</a></h2><div class=""/><div class=""><h2 id="aeab" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">机器故障？使用可变自动编码器来检测和防止它们</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/63ed80ee981a5ec31c69035a6e7bb7be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tjnpS7QQkeat7VDrKoF-VA.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@smudgern6?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">丹尼尔·史密斯</a>在<a class="ae le" href="https://unsplash.com/@smudgern6?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="624f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在<a class="ae le" rel="noopener" target="_blank" href="/anomaly-detection-in-manufacturing-part-1-an-introduction-8c29f70fc68b">之前的文章</a>(本系列的第1部分)中，我们讨论了如何使用自动编码器进行异常检测。我们还探索了<a class="ae le" href="https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/#milling" rel="noopener ugc nofollow" target="_blank">加州大学伯克利分校铣削数据集</a>。接下来，我们将使用自动编码器的变体——变分自动编码器(VAE)——对铣削数据集进行异常检测。</p><p id="0fe7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这篇文章中，我们将看到VAE与传统自动编码器的相似和不同之处。然后，我们将实现一个VAE，并在铣削数据上训练它。在下一篇第3部分的<a class="ae le" rel="noopener" target="_blank" href="/anomaly-detection-in-manufacturing-part-3-visualize-the-results-a2afb5f61d2f">中，我们将检查VAE的异常检测性能。</a></p><h1 id="b648" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">变分自动编码器</h1><p id="43e2" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">variable auto encoder于2013年推出，目前广泛用于机器学习应用。[1]VAE不同于传统的自动编码器，因为VAE既是概率性的又是生成性的。那是什么意思？VAE生成的输出部分是随机的(即使在训练之后),也可以生成与被训练数据相似的新数据。</p><p id="1aa5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">VAE在线有很好的解释——我会带你去Alfredo Canziani的深度学习课程(下面的视频来自YouTube)。不管怎样，我试图解释一下。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="my mz l"/></div></figure><p id="d332" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在高层次上，VAE具有与传统自动编码器相似的结构。然而，编码器学习不同的编码；即，VAE学习均值编码、<strong class="lh ja"> <em class="na"> </em> </strong>和标准差编码、<strong class="lh ja"/>。然后，VAE从高斯分布中随机采样，具有由编码器产生的相同的平均值和标准偏差，以生成潜在变量<strong class="lh ja"> <em class="na"> z </em> </strong>。这些潜在变量被“解码”以重建输入。</p><p id="eb10" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">下图展示了如何使用VAE重建信号。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nb"><img src="../Images/d9d1ffc7bdf4a63d535b71f93e53e27f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c2ECyMArKilDJ5N6dAnZbw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">一种变化的自动编码器架构(上图)，以及一个通过VAE的数据样本示例(下图)。数据在编码器中被压缩以产生平均值和标准偏差编码。然后，通过添加高斯噪声，从均值和标准差编码中创建编码。解码器使用编码(或潜在变量)来重建输入。(图片由作者提供，灵感来自<a class="ae le" href="https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646/ref=sr_1_1?dchild=1&amp;keywords=Tensorflow+geron&amp;qid=1622670016&amp;sr=8-1" rel="noopener ugc nofollow" target="_blank">奥雷连·杰龙</a>)</p></figure><p id="d59c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在训练期间，VAE努力最小化其重建损失(在我们的情况下，我们使用二进制交叉熵)，同时，使用潜在损失强制高斯结构。这种结构是通过Kullback-Leibler (KL)散度实现的，在最初的VAE论文中对损失进行了详细的推导。[1]潜在损失如下:*</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nc"><img src="../Images/86a1da4540d3652d97b4e726e65b3eff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OS2Stn55_eFsQaXhBSqeFQ.png"/></div></div></figure><p id="6229" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">其中<em class="na"> K </em>是潜在变量的数量，而<em class="na"> β </em>是由Higgens等人[2]引入的可调超参数</p><p id="59a2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">VAE学习嵌入在编码中的因子，这些因子可用于生成新数据。作为这些因素的一个例子，VAE可以被训练来识别图像中的形状。一个因素可能编码了形状有多尖的信息，而另一个因素可能看它有多圆。然而，在VAE中，这些因素经常在编码中纠缠在一起(潜在变量)。</p><p id="2d0b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">将超参数β<em class="na">(β)</em>调整为大于1的值，可以使因子“解开”,使得每个编码一次仅代表一个因子。因此，可以获得模型的更大的可解释性。具有可调β的VAE有时被称为解纠缠-变分-自动编码器，或者简称为<em class="na"> β </em> -VAE。为了简单起见，我们仍然称<em class="na"> β </em> -VAE为VAE。</p><h1 id="feb4" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">数据准备</h1><p id="29d0" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">在继续之前，我们需要准备数据。最终，我们将使用VAE来检测“异常”工具状况，这对应于工具磨损或失效的时间。但是首先我们需要标记数据。</p><p id="20db" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如上一篇文章所示，每次切削都有一个相关的侧面磨损量，<em class="na"> VB </em>，在切削结束时测量。我们将根据刀具的磨损程度将每个切削标记为健康、退化或失败——这些是刀具健康类别。以下是模式:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/933c85c09d2d0fde3274319e32067664.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/format:webp/1*joCU4vjH1KitMA0y5RpHuA.png"/></div></figure><p id="75c5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我创建了一个数据准备类，它获取原始的Matlab文件，一个带标签的CSV(每个切口都标有相关的侧面磨损)，并输出训练/验证/和测试数据。然而，我想在课上强调一个重要的功能。也就是<code class="fe ne nf ng nh b">create_tensor</code>函数。</p><blockquote class="ni nj nk"><p id="8088" class="lf lg na lh b li lj ka lk ll lm kd ln nl lp lq lr nm lt lu lv nn lx ly lz ma ij bi translated">注意:为了简洁，我不会覆盖所有的代码——按照<a class="ae le" href="https://colab.research.google.com/github/tvhahn/Manufacturing-Data-Science-with-Python/blob/master/Metal%20Machining/1.B_building-vae.ipynb" rel="noopener ugc nofollow" target="_blank"> Colab笔记本</a>中的步骤，训练一些模型。</p></blockquote><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="no mz l"/></div></figure><p id="e16f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><code class="fe ne nf ng nh b">create_tensor</code>函数获取一个单独的片段，将其分解成块，并放入一个数组中。它使用一个固定大小的窗口(<code class="fe ne nf ng nh b">window_size</code>变量)将切割信号分割成块，然后沿着信号“滑动”窗口。窗口“滑动”预定的量，由<code class="fe ne nf ng nh b">stride</code>变量设置。</p><p id="859f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们将采用165个切割中的每一个(记住，原始167个切割中的两个被破坏)，并应用64的窗口大小和64的步幅(窗口之间没有重叠)。</p><p id="9fbe" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我目视检查了每个切割，并选择了“稳定”切割区域出现的时间，通常是信号开始收集后的五秒钟左右，信号收集结束前的几秒钟。该信息存储在“<a class="ae le" href="https://github.com/tvhahn/ml-tool-wear/blob/master/data/processed/labels_with_tool_class.csv" rel="noopener ugc nofollow" target="_blank">labels _ with _ tool _ class . CSV</a>”文件中。</p><p id="a9ec" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">使用<code class="fe ne nf ng nh b">data_prep.py </code>(参见<a class="ae le" href="https://github.com/tvhahn/ml-tool-wear" rel="noopener ugc nofollow" target="_blank"> github repo </a>)和一些Python魔术，我们可以创建训练/验证/测试数据集。脚本看起来是这样的:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="no mz l"/></div></figure><p id="93f5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">数据的最终分布如下所示。注意数据有多不平衡(即相对较少的“失败”样本)？这是制造/工业数据中的常见问题，也是使用自监督方法的另一个原因。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi np"><img src="../Images/a9fa9e8de1c2deeb552292ed65393b80.png" data-original-src="https://miro.medium.com/v2/resize:fit:824/format:webp/1*tcBTWACWaQqkS9ujTLBgPA.png"/></div></figure><p id="a511" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">对于异常检测，通常只对“正常”数据训练自动编码器。我们将做同样的事情，在健康数据(0类)上训练我们的VAE。但是，检查异常检测的性能将使用所有数据来完成。换句话说，我们将在“瘦”数据集上训练我们的VAE，但在“完整”数据集上测试。</p><h1 id="4382" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">构建模型</h1><p id="656d" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">我们现在明白了什么是变分自动编码器，以及数据是如何准备的。是时候建造了！</p><p id="ed08" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们的VAEs将由卷积层、批量标准化层和最大池层组成。下图显示了我们的一个VAE模型可能的样子。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nq"><img src="../Images/98fabf5143ac540909d9517867f5797c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uq2yvd0RVBR_jPLA7l-TpA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">VAE使用的示例模型架构。编码器的输入是铣削数据样本，对于(64，6)的输入形状，窗口大小为64。有3个卷积层，滤波器大小为17，编码大小为18。(图片由作者提供)</p></figure><p id="d3ff" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我不会讨论这个模型的所有细节。但是，这里有一些要点:</p><ul class=""><li id="d30c" class="nr ns iq lh b li lj ll lm lo nt ls nu lw nv ma nw nx ny nz bi translated">我使用时间卷积网络作为卷积层的基础。实现来自Philippe Remy——感谢Philippe！你可以在这里找到他的github回购<a class="ae le" href="https://github.com/philipperemy/keras-tcn" rel="noopener ugc nofollow" target="_blank">。</a></li><li id="5c8b" class="nr ns iq lh b li oa ll ob lo oc ls od lw oe ma nw nx ny nz bi translated">Aurélien Geron的书<a class="ae le" href="https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646/ref=sr_1_1?dchild=1&amp;keywords=Aur%C3%A9lien+Geron&amp;qid=1614346360&amp;sr=8-1" rel="noopener ugc nofollow" target="_blank">“用Scikit-Learn、Keras和TensorFlow实践机器学习”</a>很棒。特别是，他关于VAEs的部分非常有用，我在这里使用了他的一些方法。有一本Jupyter的笔记本，摘自他的书中关于他的github的章节，非常有用。谢谢奥雷连恩！[3]</li><li id="674f" class="nr ns iq lh b li oa ll ob lo oc ls od lw oe ma nw nx ny nz bi translated">正如Geron所建议的，我使用了四舍五入的精确度来衡量模型在训练过程中的表现。</li></ul><p id="8249" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">模型函数大概是这样的:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="no mz l"/></div></figure><h1 id="3e37" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">训练模型</h1><p id="e3c3" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">是时候开始训练一些模特了。为了选择超参数，我们将使用随机搜索。</p><p id="d413" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为什么要随机搜索？嗯，它实现起来相当简单，而且与网格搜索相比，已经显示出产生良好的结果。[4] Scikit-learn有一些实现随机搜索的好方法——我们将使用<code class="fe ne nf ng nh b"><a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ParameterSampler.html" rel="noopener ugc nofollow" target="_blank">ParameterSampler</a></code>方法。</p><p id="1b19" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们将训练一堆不同的阀门，都有不同的参数。在每个VAE都经过训练(训练以最小化重建损失)并保存模型后，我们将浏览VAE模型，并查看它在异常检测中的表现(我们将在第3部分中讨论)。下面是随机搜索训练过程的示意图:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi of"><img src="../Images/b279b9b39f8cd3e44220683ce4abc233.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1irVpVBfaSLinQRH8UgcCA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">随机搜索训练过程有三个步骤。首先，随机选择超参数。其次，用这些参数训练VAE。第三，检查经过训练的VAE的异常检测性能。(图片由作者提供)</p></figure><p id="368c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在实践中，当我运行这个实验时，我在Google Colab上训练了大约1000个VAE模型(免费GPU耶！).在训练完所有1000个模型后，我将它们转移到我的本地计算机上，使用一个功能不太强大的GPU，然后检查这些模型的异常检测性能。Colab GPUs的持续使用是有限的，因此以这种方式最大限度地利用它们上的GPU是有意义的。</p><p id="c134" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">您可以在<a class="ae le" href="https://colab.research.google.com/github/tvhahn/Manufacturing-Data-Science-with-Python/blob/master/Metal%20Machining/1.B_building-vae.ipynb" rel="noopener ugc nofollow" target="_blank"> Colab笔记本</a>中查看完整的训练循环。试着训练一些模特！</p><h1 id="0698" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated"><strong class="ak">结论</strong></h1><p id="e85c" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">在这篇文章中，我们了解了VAE与传统自动编码器的相似和不同之处。然后，我们准备铣削数据，为超参数选择创建随机搜索，并开始训练模型。</p><p id="0248" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在<a class="ae le" rel="noopener" target="_blank" href="/anomaly-detection-in-manufacturing-part-3-visualize-the-results-a2afb5f61d2f">的下一篇文章</a>，第3部分中，我们将评估经过训练的vae，看看它们是如何用于异常检测的。我们将使用精度-召回曲线来询问模型性能。最后，我们将创建一些漂亮的图形来可视化结果(我的最爱！).</p><h1 id="fafe" class="mb mc iq bd md me mf mg mh mi mj mk ml kf mm kg mn ki mo kj mp kl mq km mr ms bi translated">参考</h1><p id="63de" class="pw-post-body-paragraph lf lg iq lh b li mt ka lk ll mu kd ln lo mv lq lr ls mw lu lv lw mx ly lz ma ij bi translated">[1]金马博士和韦林博士(2013年)。<a class="ae le" href="https://arxiv.org/abs/1312.6114" rel="noopener ugc nofollow" target="_blank">自动编码变分贝叶斯。</a> <em class="na"> arXiv预印本arXiv:1312.6114 </em>。</p><p id="5fa9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[2]希金斯、伊琳娜等人(2016)。"<a class="ae le" href="https://openreview.net/forum?id=Sy2fzU9gl" rel="noopener ugc nofollow" target="_blank"> beta-vae:用约束变分框架学习基本视觉概念。</a>”</p><p id="b7cf" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[3]盖伦，奥雷连恩。(2019).<a class="ae le" href="https://www.amazon.com/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646/ref=sr_1_1?dchild=1&amp;keywords=Aur%C3%A9lien+Geron&amp;qid=1614346360&amp;sr=8-1" rel="noopener ugc nofollow" target="_blank">使用Scikit-Learn、Keras和TensorFlow进行机器实践学习:构建智能系统的概念、工具和技术。</a>奥莱利传媒。</p><p id="b9a7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[4]j .伯格斯特拉和y .本吉奥(2012年)。<a class="ae le" href="https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a" rel="noopener ugc nofollow" target="_blank">超参数优化的随机搜索。</a> <em class="na">机器学习研究杂志</em>，<em class="na"> 13 </em> (2)。</p></div><div class="ab cl og oh hu oi" role="separator"><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol"/></div><div class="ij ik il im in"><p id="ece2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="na">本文原载于tvhahn.com</em><a class="ae le" href="https://www.tvhahn.com/posts/building-vae/" rel="noopener ugc nofollow" target="_blank"><em class="na"/></a><em class="na">。此外，这项工作是对发表在IJHM </em>  <em class="na">的</em> <a class="ae le" href="https://www.researchgate.net/publication/350842309_Self-supervised_learning_for_tool_wear_monitoring_with_a_disentangled-variational-autoencoder" rel="noopener ugc nofollow" target="_blank"> <em class="na">研究的补充。官方的GitHub回购是</em> </a><a class="ae le" href="https://github.com/tvhahn/ml-tool-wear" rel="noopener ugc nofollow" target="_blank"> <em class="na">这里的</em> </a> <em class="na">。</em></p><p id="be9d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="na">除非另有说明，本帖及其内容由作者授权于</em><a class="ae le" href="https://creativecommons.org/licenses/by-sa/4.0/" rel="noopener ugc nofollow" target="_blank"><em class="na">CC BY-SA 4.0</em></a><em class="na">。</em></p><p id="c282" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">* <em class="na">更正:在最初的文章中，我在潜在损失函数中有一个错误(尽管代码很好)。我已经对它进行了修改，以匹配Geron中的内容。此外，记谱法与阿尔弗雷多·坎齐亚尼不同。请参考他的视频——非常好！</em></p></div></div>    
</body>
</html>