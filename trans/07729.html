<html>
<head>
<title>Backpropagation from Scratch: How Neural Networks Really Work</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从头开始反向传播:神经网络如何真正工作</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/backpropagation-from-scratch-how-neural-networks-really-work-36ee4af202bf?source=collection_archive---------5-----------------------#2021-07-15">https://towardsdatascience.com/backpropagation-from-scratch-how-neural-networks-really-work-36ee4af202bf?source=collection_archive---------5-----------------------#2021-07-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><p id="1302" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">神经网络究竟是如何工作的？我将向您展示一个完整的示例，它是用Python从头开始编写的，包含您完全理解该过程所需的所有数学知识。</p><p id="ff24" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我也会用简单的英语解释一切。你可以跟着读，只读课文，仍然能得到大意。但是要自己从头开始重新实现一切，你必须理解数学和代码。</p><p id="be15" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我看过几篇这样的文章，但在大多数情况下，它们是不完整的。我想给这个话题一个完整的治疗:所有的数学+工作代码，包括电池。</p><p id="ada1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">你必须掌握线性代数、微积分和统计学，才能完全理解数学部分:矩阵乘法、点积、偏导数、链式法则、正态分布、概率、标准差等。</p><h1 id="64cd" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">我们试图解决的问题</h1><p id="ffce" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">我们有一大组图像，每个都是28 x 28像素，包含手绘数字。我们需要编写一个应用程序，可以查看图像，并识别那里画的数字。换句话说，如果应用程序看到这张图片…</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi lo"><img src="../Images/1b76fe2856f74253618e763f31abe623.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*pialfqZwoEIzn7xguV3_hQ.png"/></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">(图片由作者提供)</p></figure><p id="a1d3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">…然后它应该打印出这个…</p><pre class="lp lq lr ls gt ma mb mc md aw me bi"><span id="ee5f" class="mf km iq mb b gy mg mh l mi mj">[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]</span></pre><p id="39cb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">…这意味着“我看到了数字4”:这里有10个插槽，第一个对应数字0，第二个对应数字1，第三个对应数字2，依此类推；除了对应于编号4的时隙具有值1之外，所有时隙都具有值0。这意味着应用程序已经“识别”了数字4。</p><p id="e284" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">或者如果它看到这个图像…</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi lo"><img src="../Images/ce64f527167279fdf8a704d82550b21d.png" data-original-src="https://miro.medium.com/v2/resize:fit:502/format:webp/1*FB3OIsT0YCrjmHQVsxrCVw.png"/></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">(图片由作者提供)</p></figure><p id="a411" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">…然后它应该打印出这个…</p><pre class="lp lq lr ls gt ma mb mc md aw me bi"><span id="afc8" class="mf km iq mb b gy mg mh l mi mj">[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]</span></pre><p id="ed93" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">…意思是“我看到第二个”。</p><p id="9981" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这被称为<strong class="jp ir">图像识别</strong>，因为我们正在识别图像。它也被称为<strong class="jp ir">分类</strong>，因为我们有一堆输入(数字的图像)需要被分类(归类)到10个不同的类别(它们代表的数字种类，从0到9)。</p><p id="0a71" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这些图像实际上就是充满数字的矩阵。矩阵中的每个像素都是不同的数字。我们可以使用28 x 28的正方形像素矩阵，或者我们可以简单地将每个矩阵展开成一串784个数字——软件实际上并不关心形状，只要我们保持一致。</p><p id="066c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">神经网络应该很好地用于图像分类。让我们试着从头开始构建一个。</p><h1 id="6c78" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">什么是神经元？</h1><p id="2ae6" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">神经元是神经网络的基本构件。它有几个输入Iᵢ和一个输出o</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi mk"><img src="../Images/f09732162b45af6f71d5ca0e7065e9be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*CpUz13vnupBy_aYO1kXgjQ.png"/></div></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">(图片由作者提供)</p></figure><p id="59f7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">内部和Z就是所有输入I的加权和(乘以它们的权重W):</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/435ae75622752aec85072e760163f92d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*EHPsZXEvgO1cHypzMdun4g.png"/></div></figure><p id="6328" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">通常，Z的公式末尾会添加一个恒定偏置项B(因此Z通常以“… + B”结尾)。在这篇文章中<strong class="jp ir">我们将忽略偏见</strong>；网络性能会稍微差一些，但数学会更简单。</p><p id="591b" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">z的值可以在很宽的范围内，但是对于O，我们希望将值保持在一个合理的区间内，比如[0，1]，这类似于概率，只是更容易处理。你还需要一些非线性，否则输出将总是所有输入的线性集合，这肯定不是你想要的。</p><p id="34ca" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">所以我们需要使用一个函数将Z转换成O，比如<strong class="jp ir"> sigmoid </strong>，它的输出总是在这个范围内。我们说乙状结肠是神经元的<strong class="jp ir">激活功能</strong>。我们可以使用许多不同的激活函数A(Z ),但sigmoid是一个很好的基本例子，所以我们在这里使用它。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/2f1a432d38300b6626c92618311f8c43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*BagXbuh41qyXwaXZFrDskA.png"/></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">(图片由作者提供)</p></figure><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/82c4a6f019314cf371fca0bea6bc6bab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*ozk5YlJY3wtfaamzEex88g.png"/></div></figure><p id="5437" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">稍后，我们还需要sigmoid的导数，所以我们把它放在这里:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/44e79594bb847b7bc7bfd42070c87909.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*CFjyYtqNT6TQgLXQfxDu-w.png"/></div></figure><p id="a3ba" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">乙状结肠的Python代码:</p><pre class="lp lq lr ls gt ma mb mc md aw me bi"><span id="edfb" class="mf km iq mb b gy mg mh l mi mj">def sigmoid(x, derivative = False):<br/>    if derivative:<br/>        return np.exp(-x) / ((np.exp(-x) + 1) ** 2)<br/>    else:<br/>        return 1 / (1 + np.exp(-x))</span></pre><h1 id="3bb7" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">什么是神经网络？</h1><p id="8bd6" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">神经网络通常由几层组成，每层包含多个神经元:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/572e513c87077ab0048dcff35f83998c.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*IAnczBdHgvR3dxlr2dvMwA.png"/></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">(图片由作者提供)</p></figure><p id="cce5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为所有连接编写所有方程，根据一些输入计算最终输出可能会非常无聊。一个网络可以有很多连接，这意味着一大堆方程。</p><p id="c58f" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">然而，<strong class="jp ir">矩阵符号</strong>可以用一个简单得多的矩阵方程代替一个方程组。</p><p id="a8de" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于每一层l，所有神经元的内部和Z是前一层输出的加权和。每层的输出仅仅是应用于该层所有内部和的激活函数。层l的矩阵形式:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/7f47ae76196013dbf4ea2b79f9533296.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*p35nnQ9sfd4oeljX4HTkGw.png"/></div></figure><p id="b3ca" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们把这些方程推广到整个网络。</p><p id="06ad" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们将使用X符号来表示<strong class="jp ir">输入</strong>(我们正在识别的手写图像)。所以，让我们用矩阵符号来写上面所有的内容，所有的层，所有的值。</p><p id="7283" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Oₙ的最终方程式有点不同(最后一层得到了特殊处理)，我们将在下面讨论<strong class="jp ir"> softmax </strong>时再回到它。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/0b86cb28c4eca5a9e40ecaad13912e91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*Li3pV4N-QSa0UpzmqNT1hQ.png"/></div></figure><p id="9d05" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">矩阵用点积相乘。</p><p id="1a02" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">每个Wₗ矩阵的形状为矩形；一边的大小是当前层神经元的数量；另一边的大小是上一层神经元的数量；<strong class="jp ir">权值总数是前一层神经元数乘以当前层神经元数的乘积</strong>。这就是点积起作用的“魔力”。</p><h1 id="ca0b" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">初始化网络</h1><p id="8033" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">当我们创建网络时，我们不知道我们需要什么样的权重。我们可以简单地<strong class="jp ir">给权重</strong>分配随机值。例如，我们可以从标准正态分布中抽取随机值。</p><p id="a9be" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">为了减少权重可能得到的非常大的值的数量，我们将缩小与权重数量成比例的分布的标准偏差。更多的权重=更窄的分布(更接近0)。这将避免在计算z时必须处理非常大的数字。</p><p id="95e9" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">神经元的数量有点难以计算。输入和输出已经确定:如果图像中有784个像素，则需要784个输入。如果你要识别10个类别，你需要10个输出。对于其他层，它变得更加复杂；一般来说，神经元的数量从输入到输出逐层逐渐“减少”是个好主意。</p><p id="17ea" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">参见下面的代码。整个网络(“模型”)都在称为“模型”的变量中。它有4层。输入层的大小与一个输入样本数据的大小相同(784)。输出的大小与一个输出样本数据(10)的大小相同。其他两层具有中等数量的神经元(128和64)。记住，我们只在模型中存储权重W。</p><pre class="lp lq lr ls gt ma mb mc md aw me bi"><span id="b150" class="mf km iq mb b gy mg mh l mi mj">def nn_init(lsize):<br/>    # human-readable names for clarity<br/>    input_layer  = lsize[0]<br/>    hidden_1     = lsize[1]<br/>    hidden_2     = lsize[2]<br/>    output_layer = lsize[3]<br/>    <br/>    # narrowing down the standard deviation by layer size, with np.sqrt()<br/>    # large layers have tighter initial values<br/>    nnet = {<br/>        'w0': np.random.randn(hidden_1, input_layer) * np.sqrt(1. / hidden_1),<br/>        'w1': np.random.randn(hidden_2, hidden_1) * np.sqrt(1. / hidden_2),<br/>        'w2': np.random.randn(output_layer, hidden_2) * np.sqrt(1. / output_layer)<br/>    }<br/>    return nnet</span><span id="545e" class="mf km iq mb b gy ms mh l mi mj">layer_sizes = [x_train[0].shape[0], 128, 64, y_train[0].shape[0]]<br/>model = nn_init(layer_sizes)</span></pre><h1 id="9a61" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">输出</h1><p id="1ec4" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">我们正在做分类，所以网络有K个输出，每一个输出代表我们正在识别的一类物体。在这种情况下，K = 10，因为对象是数字，并且有10个类别(0，1，2，…，9)。</p><p id="6a5d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">我们可以对每个输出使用sigmoid函数，就像我们对网络中所有其他神经元所做的那样。那么每个输出将具有0和1之间的值，并且最高值(最接近1)获胜。但是我们可以更进一步:为了使输出看起来像实际的概率，让我们确保它们的总和总是1。每个输出在0和1之间，它们的总和需要正好是1，因为对象在总共K个类中的任何一个类中的总概率总是1(我们从不向网络显示“外来”对象，至少在训练期间不显示，这是重要的部分)。</p><p id="0375" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这可以通过<strong class="jp ir"> softmax </strong>函数来完成。对于每个输出Oᵢ，我们如下计算其值:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/d64f1f6031219dda8d1dd3d957d322b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*j1WkBiqGlNPrirY5K-rgEQ.png"/></div></figure><p id="8f06" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面的Python函数计算softmax，但是对指数进行了移位以避免计算非常大的值。结果是一样的，函数看起来有点复杂，但是在实践中表现得更好。</p><pre class="lp lq lr ls gt ma mb mc md aw me bi"><span id="200b" class="mf km iq mb b gy mg mh l mi mj">def softmax(x, derivative = False):<br/>    # for stability, we shift values down so max = 0<br/>    # <a class="ae mt" href="https://cs231n.github.io/linear-classify/#softmax" rel="noopener ugc nofollow" target="_blank">https://cs231n.github.io/linear-classify/#softmax</a><br/>    exp_shifted = np.exp(x - x.max())<br/>    if derivative:<br/>        return exp_shifted / np.sum(exp_shifted, axis = 0) * (1 - exp_shifted / np.sum(exp_shifted, axis = 0))<br/>    else:<br/>        return exp_shifted / np.sum(exp_shifted, axis = 0)</span></pre><h1 id="13df" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">正向传播</h1><p id="9e34" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">好了，我们有了网络，我们在输入端应用一个样本值(一个数字的图像)，我们如何计算它的输出呢？我们逐层进行，将输入/先前输出乘以权重，应用激活函数(sigmoid ),等等，直到我们到达输出，在那里应用softmax。让我们再次展示方程式:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/0b86cb28c4eca5a9e40ecaad13912e91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*Li3pV4N-QSa0UpzmqNT1hQ.png"/></div></figure><p id="8304" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">仅此而已。<strong class="jp ir">一个训练有素的网络就是这样运作的</strong>。这是一系列的线性代数运算，逐层进行，最终根据输入和网络权重计算输出。</p><p id="67ab" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">下面的Python函数就是这样做的。它假设模型(所有W矩阵)是一个全局变量(为了代码简单，这是学校的东西，而不是生产代码)，它将一个输入样本X作为参数，并返回一个包含一堆矩阵的字典，每个矩阵包含网络中每一层的O和Z值。</p><p id="e6a3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">字典中最后一个矩阵是网络输出。这就是我们想要的结果。</p><pre class="lp lq lr ls gt ma mb mc md aw me bi"><span id="3eef" class="mf km iq mb b gy mg mh l mi mj">def forward_pass(x):<br/>    # the model is a global var, used here read-only<br/>    <br/>    # NN state: internal sums, neuron outputs<br/>    nn_state = {}<br/>    <br/>    # "output zero" is the output from receptors = input to first layer in the NN<br/>    # these are activations for the input layer<br/>    nn_state['o0'] = x<br/>    <br/>    # from input layer to hidden layer 1<br/>    # weighted sum of all activations, then sigmoid<br/>    nn_state['z1'] = np.dot(model['w0'], nn_state['o0'])<br/>    nn_state['o1'] = sigmoid(nn_state['z1'])<br/>    <br/>    # from hidden 1 to hidden 2<br/>    nn_state['z2'] = np.dot(model['w1'], nn_state['o1'])<br/>    nn_state['o2'] = sigmoid(nn_state['z2'])<br/>    <br/>    # from hidden 2 to output<br/>    nn_state['z3'] = np.dot(model['w2'], nn_state['o2'])<br/>    nn_state['o3'] = softmax(nn_state['z3'])<br/>    <br/>    return nn_state</span></pre><h1 id="9852" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">训练网络</h1><p id="ba8c" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">但是如何训练一个网络呢？如果我们从随机权重开始，并应用输入x(一个图像)，输出Oₙ将是无意义的，它不会接近“真正的”输出y</p><p id="d086" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">训练的中心思想是，我们必须调整权重w，直到网络Oₙ的输出接近y(“理想”值)。高层次的训练算法可能如下所示:</p><ul class=""><li id="0323" class="mu mv iq jp b jq jr ju jv jy mw kc mx kg my kk mz na nb nc bi translated">用随机权重初始化网络</li><li id="a3b0" class="mu mv iq jp b jq nd ju ne jy nf kc ng kg nh kk mz na nb nc bi translated">应用输入样本X(图像)</li><li id="9ae2" class="mu mv iq jp b jq nd ju ne jy nf kc ng kg nh kk mz na nb nc bi translated">进行正向传播并计算输出Oₙ</li><li id="ccfa" class="mu mv iq jp b jq nd ju ne jy nf kc ng kg nh kk mz na nb nc bi translated">将输出Oₙ与您想要获得的实际值y进行比较</li><li id="2b7a" class="mu mv iq jp b jq nd ju ne jy nf kc ng kg nh kk mz na nb nc bi translated">调整最后一层的权重，使输出变得稍微“更好”(误差减少)</li><li id="7d22" class="mu mv iq jp b jq nd ju ne jy nf kc ng kg nh kk mz na nb nc bi translated">逐层返回并调整权重，以减少误差(这是困难的部分)</li><li id="d831" class="mu mv iq jp b jq nd ju ne jy nf kc ng kg nh kk mz na nb nc bi translated">应用下一个输入样本X(另一个图像)</li><li id="bdd6" class="mu mv iq jp b jq nd ju ne jy nf kc ng kg nh kk mz na nb nc bi translated">使用许多不同对的输入X和输出Y样本，重复上述所有操作数千次，直到网络表现足够好</li></ul><p id="972a" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">困难的部分是弄清楚如何调整权重，从输出到输入，从而减少误差。这被称为<strong class="jp ir">反向传播</strong>。但是要实现这一点，我们首先需要创建一些工具。</p><h1 id="a0d2" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">成本函数</h1><p id="7653" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">假设我们有网络输出Oₙ，并将其与来自训练数据的“完美”输出y进行比较。Oₙ离y有多近，我们如何定义“近”？如果我们有一个值随着Oₙ变得越来越“不同”于y而增加的函数，我们可以称之为<strong class="jp ir">成本函数</strong>(因为错误是昂贵的)，我们可以用它来降低成本。</p><p id="107c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于这个分类问题，<strong class="jp ir">交叉熵</strong>可以起到代价的作用(在其他情况下使用其他函数)。假设我们有K个输出，它是所有输出的和:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/45ae690a07b216aebe6a48871634aeb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*EpRW3e8HQREejKvEqXDPyg.png"/></div></figure><p id="b24e" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">其中，Oᵢ是网络上的各种输出，Yᵢ是每个输出的相应训练值。</p><p id="1362" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个公式有效的原因是<strong class="jp ir"> Yᵢ可以是0也可以是1 </strong>。如果在Y样本中有10个值，那么其中9个值将是0(对象X不属于这些类别中的任何一个)，其中一个值将是1(对象X属于该类别)。那么总和中的一项或另一项变成0，另一项只取决于Oᵢ.</p><p id="77d3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果O和Y都是1维向量(确实如此)，那么在使用点积的矩阵符号中，上面的等式变成:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/93b081a2f26376f850108aa3feb74060.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*Sd-i4O6VKK6V6pfD8SxmLg.png"/></div></figure><p id="d0d6" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对数是按元素应用的，差1-y也是如此。</p><p id="e5aa" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">Numpy库可以进行矩阵运算，因此Python的cost函数是对最后一个公式的直接直译:</p><pre class="lp lq lr ls gt ma mb mc md aw me bi"><span id="b837" class="mf km iq mb b gy mg mh l mi mj">def part_cost(o, y):<br/>    c = np.dot(y, np.log(o)) + np.dot((1 - y), np.log(1 - o))<br/>    return -c</span></pre><p id="b146" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们记住这个函数，我们稍后将使用它来评估网络的性能。</p><h1 id="016d" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">最小化成本</h1><p id="2bc0" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">这是争论的重点:给定从输出O和训练值Y计算的成本C，我们想要改变权重W，使得C减少(通过使O更类似于Y)。</p><p id="aca1" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">如果我们可以计算C关于W的导数(我们称之为δ)，然后在导数平面的向下方向调整W，那么C会减少。如果导数是正的，那么我们减少W(因为最小值朝向更小的Ws)，反之亦然。这项技术的另一个名字是<strong class="jp ir">梯度下降</strong>。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/b591474c4ff923f3a956d169269556fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*2iPH_DcR3A1zoFDR--BJew.png"/></div></figure><p id="3b07" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">换句话说:想想W空间中的成本函数C。忘了X或者别的什么吧。只想到C(W)。<strong class="jp ir">你需要在W空间中穿行C，直到你在W空间中偶然发现某个最小值</strong>。这就是“训练”的含义:找到使c最小的最佳W值集。</p><p id="7ecb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">接下来的并不是严格的证明。这更像是一种数学直觉。如果你仔细看这些方程，你会注意到我们滥用数学的几个地方(例如，混淆微分和实际的有限变化)。在实践中，它会成功的——神经网络对错误有很强的抵抗力。</p><p id="66db" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们将δ定义为成本C w r t的偏导数，内部和z，对于层l，这写为:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/bcaa90b61d50018d8cdcc67952311509.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*2ZXjTytIA31Ixa1X8Nlz8g.png"/></div></figure><p id="a03d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">对于前一层l-1，我们通过应用链式法则获得:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/8d2ee2c0b9ce26950e0fbed849c0cde1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*QyPSciFUginQMZjvMaJLxA.png"/></div></figure><p id="f6bb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是我们从Z的定义中知道这是真的:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/056665aeba60036783ac035491af0999.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*I8DfrONoaX4U4_bQNkBrqQ.png"/></div></figure><p id="c233" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">将最后一个公式代入前一个公式，我们得到:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/6133df9bfd703806d5d63a27c3e10a68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*edXCad3DL9OVdKP1MS5Pgw.png"/></div></figure><p id="2857" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这个公式是递归的。我们所需要的是最后一层的δₗ，我们可以从c和Zₗ计算出来，这是我们从正向传播中知道的。一旦我们有了δₗ <strong class="jp ir">，我们就可以一层一层地回去，一个接一个地计算所有层的δ值。</strong></p><p id="8ae3" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">更重要的是，现在我们还可以逐层返回，计算成本函数c相对于每层l的权重Wₗ的导数δₗ。<strong class="jp ir">这是我们使反向传播工作真正需要的。我们现在越来越接近了。</strong></p><p id="5d6c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">让我们使用链式法则重新计算每层的δ:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/ed54175b1987c36e270f1ff1c33167ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*UQ_F7UmM8y1uC4uQWkay2w.png"/></div></figure><p id="cf79" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">换句话说:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/fcecc32270cdb49c02fb3427b4d84f66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*A1OcYnRSQaZxgi3WSyN3AQ.png"/></div></figure><p id="3860" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">但是我们有来自正向传播的所有Oₗ值(它们是每一层的输出)，并且我们已经计算了上面所有的δₗ值。这意味着我们可以计算所有图层的δₗ，从输出开始，通过网络返回。</p><p id="a93d" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">一旦我们有了所有大的增量，我们就可以用它们来更新权重。在实践中，更新将通过学习速率因子λ进行加权，学习速率因子λ通常是一个小值(0.001)，因此我们不会对网络进行突然的大的改变。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/ec32f97455e80af4d935f87feb15d386.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*R7dWN3abIxkt1DHkuVKzIQ.png"/></div></figure><p id="c028" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">同样，如果δ(C . W . r . t . W .的导数)为正，这意味着在较小W值的方向上将有较小C值。所以我们需要减少W来使C减少。反之亦然。</p><p id="62f0" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">λ可能很难思考。不要停留在“单位”或类似的东西上。只需选择一个λ值，使W的变化足够小——小步前进。不管δ发生了什么，在乘以λ之后，结果应该比w小，这就是全部。这足以让算法工作。</p><h1 id="f345" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">反向传播和训练</h1><p id="4d59" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">对于每个训练样本，通过反向传播修改网络权重W以最小化成本的工作方式如下:</p><ul class=""><li id="4cc5" class="mu mv iq jp b jq jr ju jv jy mw kc mx kg my kk mz na nb nc bi translated">将训练样本X(图像)应用于输入</li><li id="e129" class="mu mv iq jp b jq nd ju ne jy nf kc ng kg nh kk mz na nb nc bi translated">进行正向传播，计算所有图层的所有Z和O(输出)值</li><li id="9fa1" class="mu mv iq jp b jq nd ju ne jy nf kc ng kg nh kk mz na nb nc bi translated">递归(向后)计算所有层的所有δ矩阵</li><li id="1404" class="mu mv iq jp b jq nd ju ne jy nf kc ng kg nh kk mz na nb nc bi translated">从δ矩阵计算所有δ矩阵</li><li id="795b" class="mu mv iq jp b jq nd ju ne jy nf kc ng kg nh kk mz na nb nc bi translated">更新权重</li></ul><p id="c5bb" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是一个完整的反向传播步骤。请参见下面的Python代码:</p><pre class="lp lq lr ls gt ma mb mc md aw me bi"><span id="d958" class="mf km iq mb b gy mg mh l mi mj">def backward_pass(x, y):<br/>    # do the forward pass, register the state of the network<br/>    nn_state = forward_pass(x)<br/>    <br/>    # small deltas: derivatives of the error w.r.t. z<br/>    nn_state['d3'] = nn_state['o3'] - y<br/>    nn_state['d2'] = np.dot(nn_state['d3'], model['w2']) * softmax(nn_state['z2'], derivative = True)<br/>    nn_state['d1'] = np.dot(nn_state['d2'], model['w1']) * sigmoid(nn_state['z1'], derivative = True)<br/>    <br/>    # large deltas: adjustments to weights<br/>    nn_state['D2'] = np.outer(nn_state['d3'], nn_state['o2'])<br/>    nn_state['D1'] = np.outer(nn_state['d2'], nn_state['o1'])<br/>    nn_state['D0'] = np.outer(nn_state['d1'], nn_state['o0'])<br/>    <br/>    return nn_state</span></pre><p id="3ea5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">backward_pass()函数将图像X和训练响应Y作为参数。该模型是一个全局变量。它返回包含所有Z、O、D和D矩阵的nn_state字典。这里d代表δ。D矩阵是在反向投影期间计算的权重调整项δ。</p><p id="26d2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">参见下面的完整培训代码。“时期”是代码需要检查整个训练数据的次数；良好的训练需要多次通过；我们做5个纪元。定义了一个训练率t_rate，用于权重更新——它实际上是我们方程中的λ(抱歉改变了符号)。</p><p id="f9f2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">您可以在每个输入样本X之后更新权重(随机梯度下降)，或者您可以累积整个时期的W变化(累积δ矩阵)并在最后应用所有内容(批量梯度下降)，或者您可以在两者之间做些事情。这里我们使用随机下降。</p><p id="a106" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">该代码还评估成本(与完美答案的偏差)和准确性(分类正确的次数)。</p><p id="54ca" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">最后，模型(W矩阵)被保存到磁盘上。</p><pre class="lp lq lr ls gt ma mb mc md aw me bi"><span id="2d04" class="mf km iq mb b gy mg mh l mi mj">epochs = 5<br/>t_rate = 0.001</span><span id="9065" class="mf km iq mb b gy ms mh l mi mj"># train<br/>print('################### training ####################')<br/>for e in range(epochs):<br/>    print('epoch:', e)<br/>    <br/>    samples = x_train.shape[0]<br/>    cost = 0<br/>    hit_count = 0<br/>    for i in tqdm(range(samples)):<br/>        m_state = backward_pass(x_train[i], y_train[i])<br/>        # add partial cost<br/>        cost += part_cost(m_state['o3'], y_train[i])<br/>        <br/>        # stochastic gradient descent<br/>        # update weights<br/>        model['w0'] -= t_rate * m_state['D0']<br/>        model['w1'] -= t_rate * m_state['D1']<br/>        model['w2'] -= t_rate * m_state['D2']<br/>        <br/>        if np.argmax(m_state['o3']) == np.argmax(y_train[i]):<br/>            # successful detection<br/>            hit_count += 1</span><span id="d45d" class="mf km iq mb b gy ms mh l mi mj"># performance evaluation<br/>    cost = cost / samples<br/>    accuracy = hit_count / samples<br/>    print('cost:', cost, 'accuracy:', accuracy)</span><span id="3ed6" class="mf km iq mb b gy ms mh l mi mj"># save the model<br/>with open('model.pickle', 'wb') as f:<br/>    pickle.dump(model, f)</span></pre><h1 id="b62d" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">评估网络性能</h1><p id="530f" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">那么网络做得有多好呢？我们用训练数据x_train和y_train对它进行了训练。但是我们已经为测试留出了数据:x_test和y_test。</p><p id="5784" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">只保留一些数据用于测试总是一个好主意。这是网络在训练中从未见过的数据。因此，我们保证性能评估将是公平的。大约20%是留出测试数据的合适大小(假设您有大量的数据——数千个样本或更多)。</p><p id="539c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">评估只是检查测试数据，计算成本和准确性，并打印出结果。</p><pre class="lp lq lr ls gt ma mb mc md aw me bi"><span id="c541" class="mf km iq mb b gy mg mh l mi mj"># test<br/>print('################### testing ####################')</span><span id="16f1" class="mf km iq mb b gy ms mh l mi mj"># load the model<br/>if os.path.isfile('model.pickle'):<br/>    with open('model.pickle', 'rb') as f:<br/>        model = pickle.load(f)</span><span id="dd60" class="mf km iq mb b gy ms mh l mi mj"># run the whole test data<br/>samples = x_test.shape[0]<br/>cost = 0<br/>hit_count = 0<br/>for i in tqdm(range(samples)):<br/>    m_state = forward_pass(x_test[i])<br/>    cost += part_cost(m_state['o3'], y_test[i])<br/>    if np.argmax(m_state['o3']) == np.argmax(y_test[i]):<br/>        hit_count += 1</span><span id="62ef" class="mf km iq mb b gy ms mh l mi mj"># evaluate performance<br/>cost = cost / samples<br/>accuracy = hit_count / samples<br/>print('cost:', cost, 'accuracy:', accuracy)</span></pre><p id="4b7c" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">因为我们不对神经元使用偏置，网络的性能将相当适中:大约60%的准确率。它肯定可以做得更好。但这是一个学习示例，不是生产代码。</p><p id="9b42" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这是训练中的网络性能，请注意它是如何随着每个时期而变得更好的:</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/a19294eaf607f1aa5fbe66dbc3f8efd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*f-5Jf99vlyoooH4EmaMvzA.png"/></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">(图片由作者提供)</p></figure><p id="19b2" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">参见下面测试中的性能。它实际上比在训练中要好一点，这不是典型的，但训练算法的行为在某种程度上有点随机。</p><figure class="lp lq lr ls gt lt gh gi paragraph-image"><div role="button" tabindex="0" class="ml mm di mn bf mo"><div class="gh gi mp"><img src="../Images/dbea26fbff47a35bab5b03267f69a120.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*ryyDNEgGlIbXHaRuWL6tmw.png"/></div></div><p class="lw lx gj gh gi ly lz bd b be z dk translated">(图片由作者提供)</p></figure><p id="e6a5" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">这里有一个Jupyter笔记本，上面有本文使用的全部代码:</p><p id="6ffd" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated"><a class="ae mt" href="https://github.com/FlorinAndrei/misc/blob/master/nn_backprop.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/FlorinAndrei/misc/blob/master/nn _ back prop . ipynb</a></p><p id="0d98" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它下载了MNIST_784数据集，这是一组带有手写数字的图像，总共约有70，000张图像。它使用Tensorflow库中的函数来准备训练/测试分割(这完全是多余的，但是非常方便)。</p><p id="c3df" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">它训练和测试网络。另外，它生成了本文中使用的一些图像。</p><h1 id="2d90" class="kl km iq bd kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li bi translated">笔记</h1><p id="6493" class="pw-post-body-paragraph jn jo iq jp b jq lj js jt ju lk jw jx jy ll ka kb kc lm ke kf kg ln ki kj kk ij bi translated">所有的图片和代码都是由本文作者创作的。</p><p id="f2b7" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">几年前，我在Coursera上参加了吴恩达的机器学习课程，如果你想理解这种算法的数学复杂性，这非常有用。</p><div class="nj nk gp gr nl nm"><a href="https://www.coursera.org/learn/machine-learning" rel="noopener  ugc nofollow" target="_blank"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd ir gy z fp nr fr fs ns fu fw ip bi translated">机器学习</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">4，250，577已注册讲师:吴恩达字幕:英语、阿拉伯语+11更多机器学习</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">www.coursera.org</p></div></div><div class="nv l"><div class="nw l nx ny nz nv oa lu nm"/></div></div></a></div><p id="29ee" class="pw-post-body-paragraph jn jo iq jp b jq jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh ki kj kk ij bi translated">感谢阅读！</p></div></div>    
</body>
</html>