<html>
<head>
<title>Regression in the face of messy outliers? Try Huber regressor</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">面对杂乱的离群值回归？试试Huber回归器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/regression-in-the-face-of-messy-outliers-try-huber-regressor-3a54ddc12516?source=collection_archive---------10-----------------------#2021-11-04">https://towardsdatascience.com/regression-in-the-face-of-messy-outliers-try-huber-regressor-3a54ddc12516?source=collection_archive---------10-----------------------#2021-11-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b46e" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">数据中的异常值无处不在，它们会搞乱你的回归问题。尝试哈伯回归器来解决这个问题。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3f97817a503ad6595130bb151c34675f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X8YMmvAynEf7nbF5LzHvig.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:作者创作</p></figure><h1 id="9c52" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">有什么问题？</h1><p id="8dbf" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">假设您有一个包含两个要素<em class="mm"> X </em> 1和<em class="mm"> X </em> 2的数据集，您正在对其执行线性回归。然而，数据集中引入了一些噪声/异常值。</p><p id="1d7e" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">比方说，<strong class="ls iu"><em class="mm">y</em>-值异常值与它们应有的值相比特别低</strong>。那看起来像什么？</p><p id="0346" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这是你得到的数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/d743e84c8eaa386ebcc7400d2d2090ba.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DoE_1nKPvRFcavUG5j2K0g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:作者创作</p></figure><p id="72ae" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">然而，如果你真的考虑一下<em class="mm"> X </em> - <em class="mm"> Y </em>数据的斜率，那些<em class="mm"> X </em>值的预期Y值应该要高得多。类似于下面的内容，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/892e43a56eedc253de0872b538e01409.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6he8cMUlfTBPXnz4NoMD4g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:作者创作</p></figure><p id="8036" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这些是明显的异常值，您可以运行一个简单的探索性数据分析(EDA ),在构建回归模型之前从数据集中捕捉并丢弃它们。</p><p id="5b16" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">但是你不能指望捕捉到所有尺度和所有维度上的异常值<strong class="ls iu"><em class="mm"/></strong>。具有100或1000个维度(特征)的数据集的可视化具有足够的挑战性，以手动检查图并发现异常值。</p><p id="19fa" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">对异常值<em class="mm">稳健的回归算法</em>听起来像是对那些讨厌的坏数据点的一个好赌注。胡伯回归器就是我们将在本文中讨论的一个工具。</p><h1 id="b021" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">胡伯回归</h1><p id="4657" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在统计学中，Huber损失是一种特殊的损失函数(由瑞士数学家Peter Jost Huber  于1964年首次引入),广泛用于稳健回归问题——存在异常值的情况会降低基于误差的最小平方损失的性能和准确性。</p><div class="mv mw gp gr mx my"><a rel="noopener follow" target="_blank" href="/where-did-the-least-square-come-from-3f1abc7f7caf"><div class="mz ab fo"><div class="na ab nb cl cj nc"><h2 class="bd iu gy z fp nd fr fs ne fu fw is bi translated">最小二乘法从何而来？</h2><div class="nf l"><h3 class="bd b gy z fp nd fr fs ne fu fw dk translated">在机器学习面试中，如果被问及最小二乘损失的数学基础，你会怎么回答…</h3></div><div class="ng l"><p class="bd b dl z fp nd fr fs ne fu fw dk translated">towardsdatascience.com</p></div></div><div class="nh l"><div class="ni l nj nk nl nh nm ks my"/></div></div></a></div><h2 id="f5a2" class="nn kz it bd la no np dn le nq nr dp li lz ns nt lk md nu nv lm mh nw nx lo ny bi translated">技术细节</h2><p id="3e51" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">损失由下式给出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/da484518c3b4bad70b8e34152fb6ee5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*ZdCzdOtHoWOMbxNzDF_lXg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:<a class="ae mu" href="https://en.wikipedia.org/wiki/Huber_loss" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="0969" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们可以看到，只有当残差的绝对值小于某个固定参数时，损失才是通常残差的平方(<em class="mm"> y </em> — <em class="mm"> f </em> ( <em class="mm"> x </em>))。这个参数的选择和调整对于获得一个好的估计器是很重要的。当残差大于该参数时，损耗是残差绝对值和Huber参数的函数。</p><p id="b7d1" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">现在，您可能还记得基础统计学中的<strong class="ls iu">平方损失来自均值附近的无偏估计量，而绝对差损失来自中位数附近的无偏估计量</strong>。中位数比平均数对异常值更稳健。</p><p id="25dd" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu"> Huber损失是这两种类型的平衡折衷</strong>。它对异常值是稳健的，但也不会完全忽略它们。当然，可以使用自由参数进行调整。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/8fcbe28cbadb5d048519cddefd24d204.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HDb-puoKCJcR0WtxRT1jWg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:作者创作</p></figure><h2 id="90c6" class="nn kz it bd la no np dn le nq nr dp li lz ns nt lk md nu nv lm mh nw nx lo ny bi translated">Python演示</h2><p id="a5f1" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">演示笔记本在我的Github repo中。</p><p id="f861" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们创建了合成数据，并用以下代码添加了一些噪声异常值，</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="3308" class="nn kz it oc b gy og oh l oi oj">import numpy as np<br/>from sklearn.datasets import make_regression</span><span id="906f" class="nn kz it oc b gy ok oh l oi oj">rng = np.random.RandomState(0)<br/>X, y, coef = make_regression(<br/>    n_samples=200, n_features=2, noise=4.0, coef=True, random_state=0)</span><span id="0fc0" class="nn kz it oc b gy ok oh l oi oj"><strong class="oc iu"># The first four data points are outlier</strong><br/>X[:4] = rng.uniform(10, 20, (4, 2))<br/>y[:4] = rng.uniform(10, 20, 4)</span></pre><p id="4410" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">现在，你所要做的就是调用Scikit-learn的内置<code class="fe ol om on oc b">HuberRegressor</code>估算器并拟合数据。为了比较，我们也调用了标准的<code class="fe ol om on oc b">LinearRegression</code>方法。</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="9ee4" class="nn kz it oc b gy og oh l oi oj">from sklearn.linear_model import HuberRegressor, LinearRegression</span><span id="5f0b" class="nn kz it oc b gy ok oh l oi oj">huber = HuberRegressor().fit(X, y)<br/>linear = LinearRegression().fit(X, y)</span></pre><p id="f6ab" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">现在，我们知道前4个数据点是异常值。所以，如果我们试图用第一个数据点预测y值，我们会得到这样的结果，</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="2e40" class="nn kz it oc b gy og oh l oi oj">linear.predict(X[:1,])</span><span id="c8e1" class="nn kz it oc b gy ok oh l oi oj">&gt;&gt; array([87.38004436])</span></pre><p id="bae6" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">正如所料，线性回归预测是<em class="mm"> y </em>的低值。为什么？因为线性拟合(基于最小平方损失)因其巨大的杠杆作用而偏向异常值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/52c5343eadf367780f7b7ed413423fcf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9Jq5TEcdfFCjaL7uA_8gOQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:作者创作</p></figure><p id="721c" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">然而，Huber估计量预测了更合理(高)的值，</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="c4cf" class="nn kz it oc b gy og oh l oi oj">huber.predict(X[:1,])</span><span id="4933" class="nn kz it oc b gy ok oh l oi oj">&gt;&gt; array([806.72000092])</span></pre><p id="dd67" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">为了进一步证明Huber估计量的稳健性，我们可以使用估计的系数并绘制最佳拟合线，</p><pre class="kj kk kl km gt ob oc od oe aw of bi"><span id="7455" class="nn kz it oc b gy og oh l oi oj">huber_y1 = np.arange(-2.5,20,0.01)*huber.coef_[0] + \<br/>            np.arange(-2.5,20,0.01)*huber.coef_[1] + \<br/>            huber.intercept_</span><span id="b62b" class="nn kz it oc b gy ok oh l oi oj">plt.figure(dpi=120)<br/>plt.scatter(X[:,0],y)<br/>plt.plot(np.arange(-2.5,20,0.01), <br/>         huber_y1,<br/>         color='red',linestyle='--')<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/d5fc669c6f47c3fd28b1359440b089dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DBwGphqlz1TokehmFPBlBQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来源:作者创作</p></figure><h2 id="5da0" class="nn kz it bd la no np dn le nq nr dp li lz ns nt lk md nu nv lm mh nw nx lo ny bi translated">泰尔-森估计量</h2><p id="2c4f" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">虽然我们不在本文中讨论它，但鼓励读者检查<a class="ae mu" href="https://en.wikipedia.org/wiki/Theil%E2%80%93Sen_estimator" rel="noopener ugc nofollow" target="_blank"> Theil-Sen估计器</a>，这是另一种稳健的线性回归技术，并且对异常值高度不敏感。</p><p id="3e98" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">正如所料，Scikit-learn也为这个估计器提供了一个内置方法:<a class="ae mu" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.TheilSenRegressor.html#sklearn.linear_model.TheilSenRegressor" rel="noopener ugc nofollow" target="_blank"> <strong class="ls iu"> Scikit-learn Theil-Sen估计器</strong> </a>。</p><p id="9bb5" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">对于具有大量特征的多元线性回归，这是所有稳健估计量中非常有效和快速的回归算法。</p><h1 id="fda1" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">摘要</h1><p id="2874" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在这篇简短的文章中，我们讨论了数据集中存在异常值时的线性回归估计量问题。我们用一个简单的例子证明，基于传统最小二乘损失函数的线性估计可能会预测出完全错误的值，因为它们倾向于离群值。</p><p id="89ba" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们讨论了几个稳健估计量，并详细演示了休伯回归。非参数统计在许多地方使用这些稳健的回归技术，尤其是当数据预计会特别嘈杂时。</p><p id="2bc3" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">数据科学专业的学生和专业人士也应该具备这些强大的回归方法的工作知识，以便在存在异常值的情况下自动对大型数据集进行建模。</p></div><div class="ab cl oq or hx os" role="separator"><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov ow"/><span class="ot bw bk ou ov"/></div><div class="im in io ip iq"><p id="0c3b" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">喜欢这篇文章吗？成为 <a class="ae mu" href="https://medium.com/@tirthajyoti/membership" rel="noopener"> <strong class="ls iu"> <em class="mm">中等会员</em> </strong> </a> <em class="mm">继续</em> <strong class="ls iu"> <em class="mm">无限制学习</em> </strong> <em class="mm">。如果您使用下面的链接，</em> <strong class="ls iu"> <em class="mm">，我将收取您的一部分会员费，而不会对您产生额外费用</em> </strong> <em class="mm">。</em></p><div class="mv mw gp gr mx my"><a href="https://medium.com/@tirthajyoti/membership" rel="noopener follow" target="_blank"><div class="mz ab fo"><div class="na ab nb cl cj nc"><h2 class="bd iu gy z fp nd fr fs ne fu fw is bi translated">通过我的推荐链接加入媒体</h2><div class="nf l"><h3 class="bd b gy z fp nd fr fs ne fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="ng l"><p class="bd b dl z fp nd fr fs ne fu fw dk translated">medium.com</p></div></div><div class="nh l"><div class="ox l nj nk nl nh nm ks my"/></div></div></a></div></div></div>    
</body>
</html>