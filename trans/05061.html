<html>
<head>
<title>How to adapt a multilingual T5 model for a single language</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何为单一语言调整多语言T5模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-adapt-a-multilingual-t5-model-for-a-single-language-b9f94f3d9c90?source=collection_archive---------12-----------------------#2021-05-04">https://towardsdatascience.com/how-to-adapt-a-multilingual-t5-model-for-a-single-language-b9f94f3d9c90?source=collection_archive---------12-----------------------#2021-05-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="ddd6" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="f29a" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">仅为您的语言的标记加载嵌入，以减少模型大小</h2></div><p id="7332" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><a class="ae lk" href="https://arxiv.org/abs/1910.10683" rel="noopener ugc nofollow" target="_blank"> T5 </a>是谷歌的一个编码器-解码器转换器，曾经是几个NLU和NLG问题的SOTA，现在仍然是seq2seq任务(如文本摘要)的基础。第一个T5型号是仅用于英语的<a class="ae lk" href="https://huggingface.co/t5-base" rel="noopener ugc nofollow" target="_blank"/>，然后是大规模的<a class="ae lk" href="https://huggingface.co/google/mt5-base" rel="noopener ugc nofollow" target="_blank">多语言版本</a>。这个模型涵盖了101种语言，规模确实很大。</p><p id="5fc4" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这篇文章展示了如何通过修剪冗余的嵌入从多语言模型中提取出单一语言模型。这将参数数量减少了两倍以上，而质量没有显著损失。对于俄语，我们的结果是<a class="ae lk" href="https://huggingface.co/cointegrated/rut5-base" rel="noopener ugc nofollow" target="_blank">，但是你可以用mT5的101种语言中的任何一种来尝试。</a></p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="lr ls di lt bf lu"><div class="gh gi ll"><img src="../Images/c2757da14164143914343c499e10fe2a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*59IfFHhcDXtd0hsc1IfTUw.png"/></div></div><p class="lx ly gj gh gi lz ma bd b be z dk translated">三分之二的MT5参数是嵌入的，我们可以去掉不用的。图片由作者提供。</p></figure><h2 id="4298" class="mb mc iq bd md me mf dn mg mh mi dp mj kx mk ml mm lb mn mo mp lf mq mr ms iw bi translated">选择词汇</h2><p id="cde7" class="pw-post-body-paragraph ko kp iq kq b kr mt ka kt ku mu kd kw kx mv kz la lb mw ld le lf mx lh li lj ij bi translated">这个想法类似于论文<a class="ae lk" href="https://arxiv.org/abs/2010.05609" rel="noopener ugc nofollow" target="_blank">Load What your Need:多语言BERT </a>的一个小版本。我们使用原始记号赋予器来处理俄语语料库，统计不同记号的频率，并且仅保留足够频繁使用的记号，修剪所有其他记号。</p><p id="a030" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们还在模型中保留了少量的英语标记，使其成为双语的。我们需要这一点来使模型能够将知识从英语转移到俄语下游任务，还因为英语单词和短语经常出现在现代俄语文本中。</p><p id="2a7e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们首先加载现有的多语言模型。</p><pre class="lm ln lo lp gt my mz na nb aw nc bi"><span id="d480" class="mb mc iq mz b gy nd ne l nf ng">import torch<br/>from transformers import T5ForConditionalGeneration, T5Tokenizer<br/>tokenizer = T5Tokenizer.from_pretrained("google/mt5-base")<br/>model = T5ForConditionalGeneration.from_pretrained('google/mt5-base')</span></pre><p id="533d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">该模型主要由嵌入组成:其33%的参数是输入嵌入(在其编码器和解码器之间共享)，33%是输出嵌入。</p><pre class="lm ln lo lp gt my mz na nb aw nc bi"><span id="a515" class="mb mc iq mz b gy nd ne l nf ng">def msize(m):<br/>    return sum(p.numel() for p in m.parameters())</span><span id="c785" class="mb mc iq mz b gy nh ne l nf ng">print(msize(model.shared) / msize(model))   # 0.3298<br/>print(msize(model.lm_head) / msize(model))  # 0.3298</span></pre><p id="2303" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了估计不同标记的频率，我们从<a class="ae lk" href="https://wortschatz.uni-leipzig.de/en/download/Russian" rel="noopener ugc nofollow" target="_blank">莱比锡语料库</a>中提取了一个俄语和一个英语句子语料库。我们使用这两种语言是因为我们希望我们的模型最终是双语的。</p><pre class="lm ln lo lp gt my mz na nb aw nc bi"><span id="8461" class="mb mc iq mz b gy nd ne l nf ng">import pandas as pd<br/>import csv<br/>from collections import Counter<br/>from tqdm.auto import tqdm, trange</span><span id="a420" class="mb mc iq mz b gy nh ne l nf ng">df_ru = pd.read_csv('rus-ru_web-public_2019_1M-sentences.txt', sep='\t', header=None, quoting=csv.QUOTE_NONE)<br/>df_ru.columns = ['idx', 'text']<br/>cnt_ru = Counter()<br/>for text in tqdm(df_ru.text):<br/>    cnt_ru.update(tokenizer.encode(text))<br/>print(len(cnt_ru), len(cnt_ru)/tokenizer.vocab_size)  <br/># 58438 0.2336</span></pre><p id="a13f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在统计了俄语语料库中的标记后，我们发现只有23%的模型词汇被使用。此外，前20K个标记构成了俄语语料库的99%以上。对于英语来说，统计数据是相似的。</p><pre class="lm ln lo lp gt my mz na nb aw nc bi"><span id="426b" class="mb mc iq mz b gy nd ne l nf ng">for top in 10_000, 20_000, 30_000:<br/>    print(top, sum(v for k, v in cnt_ru.most_common(top)) / sum(cnt_ru.values()))<br/># 10000 0.9645<br/># 20000 0.9940<br/># 30000 0.9982</span></pre><p id="0c22" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们决定使用下列词汇:</p><ul class=""><li id="002b" class="ni nj iq kq b kr ks ku kv kx nk lb nl lf nm lj nn no np nq bi translated">原始令牌化器的1K个顶级令牌(以防万一)</li><li id="5992" class="ni nj iq kq b kr nr ku ns kx nt lb nu lf nv lj nn no np nq bi translated">英语词汇中的顶级10K</li><li id="5bc7" class="ni nj iq kq b kr nr ku ns kx nt lb nu lf nv lj nn no np nq bi translated">俄语词汇前20K名</li><li id="41d3" class="ni nj iq kq b kr nr ku ns kx nt lb nu lf nv lj nn no np nq bi translated">T5使用的100个特殊令牌</li></ul><p id="be5c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这给了我们30K令牌的词汇表，是多语言版本中250K令牌的12%。</p><pre class="lm ln lo lp gt my mz na nb aw nc bi"><span id="cf6f" class="mb mc iq mz b gy nd ne l nf ng">new_tokens = set(range(1000))<br/>for i, (k, v) in enumerate(cnt_en.most_common(10_000)):<br/>    if k not in new_tokens:<br/>        new_tokens.add(k)<br/>for i, (k, v) in enumerate(cnt_ru.most_common(25_000)):<br/>    if len(new_tokens) == 29_900:<br/>        print(i, 'Russan tokens are included')<br/>        break<br/>    if k not in new_tokens:<br/>        new_tokens.add(k)</span><span id="d5b4" class="mb mc iq mz b gy nh ne l nf ng">for t in range(tokenizer.vocab_size - 100, tokenizer.vocab_size):<br/>    new_tokens.add(t)</span><span id="8a3d" class="mb mc iq mz b gy nh ne l nf ng">print(len(new_tokens))<br/>kept_ids = sorted(new_tokens)</span></pre><h2 id="82e6" class="mb mc iq bd md me mf dn mg mh mi dp mj kx mk ml mm lb mn mo mp lf mq mr ms iw bi translated">更新模型</h2><p id="62ba" class="pw-post-body-paragraph ko kp iq kq b kr mt ka kt ku mu kd kw kx mv kz la lb mw ld le lf mx lh li lj ij bi translated">更新神经网络很容易:只需替换其输入和输出嵌入的参数。这将模型大小减少了58%(从2.2GB减少到0.9GB)。</p><pre class="lm ln lo lp gt my mz na nb aw nc bi"><span id="5851" class="mb mc iq mz b gy nd ne l nf ng">new_size = len(kept_ids)<br/>new_emb = torch.nn.Embedding(new_size, model.shared.embedding_dim)<br/>new_head = torch.nn.Linear(in_features=model.lm_head.in_features, out_features=new_size, bias=False)<br/>for new_id, old_id in enumerate(kept_ids):<br/>    new_emb.weight.data[new_id] = model.shared.weight.data[old_id]<br/>    new_head.weight.data[new_id] = model.lm_head.weight.data[old_id]<br/>model.shared.weight = new_emb.weight<br/>model.lm_head.weight = new_head.weight</span><span id="5b67" class="mb mc iq mz b gy nh ne l nf ng">model.config.__dict__['vocab_size'] = new_size<br/>model.config.__dict__['_name_or_path'] = 'cointegrated/rut5-base'</span></pre><p id="d615" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">更新记号赋予器出人意料地更加棘手。T5使用Sentencepiece tokenizer，用C实现，对Python是不透明的。幸运的是，我们可以下载它的模型，并使用它的Protobuf表示将其部署到Python中。</p><pre class="lm ln lo lp gt my mz na nb aw nc bi"><span id="095d" class="mb mc iq mz b gy nd ne l nf ng">! wget <a class="ae lk" href="https://raw.githubusercontent.com/google/sentencepiece/master/src/sentencepiece_model.proto" rel="noopener ugc nofollow" target="_blank">https://raw.githubusercontent.com/google/sentencepiece/master/src/sentencepiece_model.proto</a><br/>! protoc --python_out=. sentencepiece_model.proto</span><span id="e22a" class="mb mc iq mz b gy nh ne l nf ng">import sentencepiece_model_pb2 as spmp<br/>smp = tokenizer.sp_model.serialized_model_proto()<br/>m = spmp.ModelProto()<br/>m.ParseFromString(smp)</span><span id="63cb" class="mb mc iq mz b gy nh ne l nf ng">print('the loaded model has pieces:', len(m.pieces))<br/>new_pieces = [m.pieces[idx] for idx in kept_ids]<br/>print('the new pieces:', len(new_pieces))</span><span id="1771" class="mb mc iq mz b gy nh ne l nf ng"># replace the content of the first 30K pieces<br/>for i, p in enumerate(new_pieces):<br/>    m.pieces[i].piece = p.piece<br/>    m.pieces[i].score = p.score<br/>    m.pieces[i].type = p.type</span><span id="202e" class="mb mc iq mz b gy nh ne l nf ng"># drop the remaining pieces<br/>n = len(new_pieces)<br/>for i in trange(len(m.pieces) - n):<br/>    m.pieces.pop(len(m.pieces) - 1)</span><span id="77e7" class="mb mc iq mz b gy nh ne l nf ng">print(len(m.pieces))<br/>with open('new_sp.model', 'wb') as f:<br/>    f.write(m.SerializeToString())</span><span id="f011" class="mb mc iq mz b gy nh ne l nf ng">new_tokenizer = T5Tokenizer('new_sp.model', extra_ids=0)</span></pre><p id="4fe2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">现在我们可以保存新的模型和新的记号赋予器。</p><pre class="lm ln lo lp gt my mz na nb aw nc bi"><span id="ebac" class="mb mc iq mz b gy nd ne l nf ng">new_tokenizer.save_pretrained('rut5-base')<br/>model.save_pretrained('rut5-base')</span></pre><p id="f0e8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">到目前为止，创建模型的所有代码都可以在Github 的<a class="ae lk" href="https://gist.github.com/avidale/44cd35bfcdaf8bedf51d97c468cc8001" rel="noopener ugc nofollow" target="_blank">上获得。俄罗斯T5型号在</a><a class="ae lk" href="https://huggingface.co/cointegrated/rut5-base" rel="noopener ugc nofollow" target="_blank">的Huggingface仓库</a>有售。</p><p id="b365" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">坦率地说，这个模型本身是非常无用的，因为mT5只在预测缺失单词的无人监督的任务上受过训练。然而，这个模型可以针对许多其他任务进行微调:文本摘要、翻译、对话响应生成、释义等。在下一篇文章中，我们将展示如何进行这样的微调。订阅敬请关注！</p></div><div class="ab cl nw nx hu ny" role="separator"><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob oc"/><span class="nz bw bk oa ob"/></div><div class="ij ik il im in"><p id="a62e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这篇文章是由大卫·戴尔(<a class="ae lk" href="https://daviddale.ru/en" rel="noopener ugc nofollow" target="_blank">https://daviddale.ru/en</a>)写的，他是NLP的研究科学家和聊天机器人的开发者。</p></div></div>    
</body>
</html>