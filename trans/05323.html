<html>
<head>
<title>Powerful Feature Selection with Recursive Feature Elimination (RFE) of Sklearn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Sklearn的递归特征消除(RFE)功能强大的特征选择</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/powerful-feature-selection-with-recursive-feature-elimination-rfe-of-sklearn-23efb2cdb54e?source=collection_archive---------2-----------------------#2021-05-11">https://towardsdatascience.com/powerful-feature-selection-with-recursive-feature-elimination-rfe-of-sklearn-23efb2cdb54e?source=collection_archive---------2-----------------------#2021-05-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="338d" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">即使删除93个功能，也能获得相同的模型性能</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/519b0f6a47f750a0539e0ea96cbab0d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GRB6-yRZbUuFHfsHVtbkuA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd ky">照片由</strong> <a class="ae kz" href="https://unsplash.com/@victoriano?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> <strong class="bd ky">维多利亚·伊斯基耶多</strong></a><strong class="bd ky"/><a class="ae kz" href="https://unsplash.com/s/photos/selection?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"><strong class="bd ky">Unsplash</strong></a></p></figure><p id="f220" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">基本的特征选择方法主要是关于特征的单个属性以及它们如何相互作用。<a class="ae kz" rel="noopener" target="_blank" href="/how-to-use-variance-thresholding-for-robust-feature-selection-a4503f2b5c3f?source=your_stories_page-------------------------------------"> <em class="lw">方差阈值处理</em> </a>和<a class="ae kz" rel="noopener" target="_blank" href="/how-to-use-pairwise-correlation-for-robust-feature-selection-20a60ef7d10?source=your_stories_page-------------------------------------"> <em class="lw">成对特征选择</em> </a>是基于方差和它们之间的相关性去除不必要特征的几个例子。然而，更实用的方法是根据特性对特定模型性能的影响来选择特性。Sklearn提供的一种这样的技术是递归特征消除(RFE)。它通过逐个移除特征来降低模型的复杂性，直到留下最佳数量的特征。</p><p id="2558" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">由于其灵活性和易用性，它是最流行的特征选择算法之一。该算法可以环绕任何模型，并且它可以产生提供最高性能的最佳可能特征集。通过完成本教程，您将学习如何在Sklearn中使用它的实现。</p><div class="lx ly gp gr lz ma"><a href="https://ibexorigin.medium.com/membership" rel="noopener follow" target="_blank"><div class="mb ab fo"><div class="mc ab md cl cj me"><h2 class="bd iu gy z fp mf fr fs mg fu fw is bi translated">通过我的推荐链接加入Medium-BEXGBoost</h2><div class="mh l"><h3 class="bd b gy z fp mf fr fs mg fu fw dk translated">获得独家访问我的所有⚡premium⚡内容和所有媒体没有限制。支持我的工作，给我买一个…</h3></div><div class="mi l"><p class="bd b dl z fp mf fr fs mg fu fw dk translated">ibexorigin.medium.com</p></div></div><div class="mj l"><div class="mk l ml mm mn mj mo ks ma"/></div></div></a></div><p id="8976" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">获得由强大的AI-Alpha信号选择和总结的最佳和最新的ML和AI论文:</p><div class="lx ly gp gr lz ma"><a href="https://alphasignal.ai/?referrer=Bex" rel="noopener  ugc nofollow" target="_blank"><div class="mb ab fo"><div class="mc ab md cl cj me"><h2 class="bd iu gy z fp mf fr fs mg fu fw is bi translated">阿尔法信号|机器学习的极品。艾总结的。</h2><div class="mh l"><h3 class="bd b gy z fp mf fr fs mg fu fw dk translated">留在循环中，不用花无数时间浏览下一个突破；我们的算法识别…</h3></div><div class="mi l"><p class="bd b dl z fp mf fr fs mg fu fw dk translated">alphasignal.ai</p></div></div><div class="mj l"><div class="mp l ml mm mn mj mo ks ma"/></div></div></a></div></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h2 id="3d96" class="mx my it bd mz na nb dn nc nd ne dp nf lj ng nh ni ln nj nk nl lr nm nn no np bi translated">递归特征消除背后的思想</h2><p id="2dff" class="pw-post-body-paragraph la lb it lc b ld nq ju lf lg nr jx li lj ns ll lm ln nt lp lq lr nu lt lu lv im bi translated">考虑安苏尔男性数据集的这个子集:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl nv"><img src="../Images/ca73576cecbbdae72b2145079ae26f58.png" data-original-src="https://miro.medium.com/v2/format:webp/1*12W3UEzDgF4Tn0ODkfJwhw.png"/></div></figure><p id="c608" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">它记录了6000多名美国陆军人员的100多种不同类型的身体测量数据。我们的目标是使用尽可能少的特征来预测以磅为单位的重量。(数据集中有93个数字要素)</p><p id="42bd" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">让我们用随机森林回归器建立一个基本性能。我们将首先构建特性和目标数组，并将它们分成训练集和测试集。然后，我们将拟合估计量，并使用R平方对其性能进行评分:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="1d78" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们实现了0.948的出色R平方。我们可以使用所有98个特性来完成这项工作，这比我们可能需要的要多得多。所有Sklearn估计器都有特殊的属性，显示特征权重(或系数)，或者以<code class="fe ny nz oa ob b">coef_</code>或<code class="fe ny nz oa ob b">.feature_importances_</code>给出。让我们看看随机森林回归模型的计算系数:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nw nx l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl nv"><img src="../Images/015441fa2f522bd05004cebad6de584a.png" data-original-src="https://miro.medium.com/v2/format:webp/1*SpLAm4urEEMuip4s4KhPCw.png"/></div></figure><p id="eb25" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">要降低模型的复杂性，请始终从移除权重接近0的要素开始。由于所有权重都要乘以特征值，因此这样小的权重对整体预测的贡献非常小。看上面的权重，可以看到很多权重接近0。</p><p id="8cbb" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们可以设置一个低阈值，并基于它过滤掉特征。但我们必须记住，即使删除一个单一的特征也会迫使其他系数发生变化。因此，我们必须一步一步地消除它们，通过对拟合的模型系数进行排序，留下权重最低的特征。手动为98个特性做这件事会很麻烦，但是谢天谢地Sklearn为我们提供了递归特性消除功能— <a class="ae kz" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html" rel="noopener ugc nofollow" target="_blank"> RFE类</a>来完成这项任务。</p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h2 id="ea8c" class="mx my it bd mz na nb dn nc nd ne dp nf lj ng nh ni ln nj nk nl lr nm nn no np bi translated">Sklearn递归特征消除类</h2><p id="fa65" class="pw-post-body-paragraph la lb it lc b ld nq ju lf lg nr jx li lj ns ll lm ln nt lp lq lr nu lt lu lv im bi translated">RFE是一个转换估计器，这意味着它遵循熟悉的Sklearn的拟合/转换模式。由于其易于配置的特性和稳健的性能，它是一种流行的算法。顾名思义，它根据我们在每次迭代中选择的模型给出的权重，一次删除一个特性。</p><p id="8f33" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">下面，您将看到一个使用上述随机森林回归模型的RFE示例:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="3b30" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">拟合估计器后，它有一个<code class="fe ny nz oa ob b">.support_</code>属性，为丢弃的要素提供一个带有假值的布尔掩膜。我们可以用它来划分数据子集:</p><pre class="kj kk kl km gt oc ob od oe aw of bi"><span id="7b66" class="mx my it ob b gy og oh l oi oj">X_train.loc[:, rfe.support_]</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="ab gu cl nv"><img src="../Images/b7f907d1ba58d4c041b87a813406fe1f.png" data-original-src="https://miro.medium.com/v2/format:webp/1*j1KL37HSDEN4xXrz669ciw.png"/></div></figure><p id="eba9" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">或者你可以直接调用<code class="fe ny nz oa ob b">.transform()</code>来获得一个新的<code class="fe ny nz oa ob b">numpy</code>数组和相关的特性。让我们使用这个较小的子集再次测试随机森林回归器:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="ffc1" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">即使在删除了将近90个特性之后，我们还是得到了同样的分数，这令人印象深刻！</p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h2 id="283b" class="mx my it bd mz na nb dn nc nd ne dp nf lj ng nh ni ln nj nk nl lr nm nn no np bi translated">RFE性能考虑因素</h2><p id="e2b2" class="pw-post-body-paragraph la lb it lc b ld nq ju lf lg nr jx li lj ns ll lm ln nt lp lq lr nu lt lu lv im bi translated">由于RFE每次丢弃一个特征时都会在整个数据集上训练给定的模型，因此对于像我们这样具有许多特征的大型数据集来说，计算时间会很长。为了控制这种行为，RFE提供了<code class="fe ny nz oa ob b">step</code>参数，让我们在每次迭代中丢弃任意数量的特性，而不是一个:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nw nx l"/></div></figure></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h2 id="5404" class="mx my it bd mz na nb dn nc nd ne dp nf lj ng nh ni ln nj nk nl lr nm nn no np bi translated">选择要自动保留的特征数量</h2><p id="06cd" class="pw-post-body-paragraph la lb it lc b ld nq ju lf lg nr jx li lj ns ll lm ln nt lp lq lr nu lt lu lv im bi translated">RFE最重要的超参数是<em class="lw">估计器</em>和<em class="lw"> n_features_to_select </em>。在最后一个例子中，我们任意选择了10个特性，并希望得到最好的结果。然而，由于RFE可以包装任何模型，我们必须根据它们的性能来选择相关特征的数量。</p><p id="660d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">为了实现这一点，Sklearn提供了一个类似的<code class="fe ny nz oa ob b">RFECV</code>类，它通过交叉验证实现递归特性消除，并自动找到要保留的最佳特性数量。下面是一个在简单的线性回归中使用RFECV的例子。我们将选择线性回归，因为我们可以猜测人体测量值之间存在线性相关性。此外，结合交叉验证，随机森林回归将变得更加计算昂贵:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="1e3e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我为<code class="fe ny nz oa ob b">cv</code>和<code class="fe ny nz oa ob b">scoring</code>参数提供了默认值。一个新的超参数是<code class="fe ny nz oa ob b">min_features_to_select</code>——你可以从名字中猜出它是做什么的。让我们看看估计器计算出要保留多少个特征:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="22ea" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><code class="fe ny nz oa ob b">RFECV</code>告诉我们只保留98个中的5个。让我们只在这5个上训练模型，并观察它的性能:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nw nx l"/></div></figure><p id="eadc" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">即使去掉了93个特性，我们仍然得到了0.956的高分。</p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h2 id="cae1" class="mx my it bd mz na nb dn nc nd ne dp nf lj ng nh ni ln nj nk nl lr nm nn no np bi translated">摘要</h2><p id="87e1" class="pw-post-body-paragraph la lb it lc b ld nq ju lf lg nr jx li lj ns ll lm ln nt lp lq lr nu lt lu lv im bi translated">通过阅读本教程，您学习了:</p><ul class=""><li id="a0ba" class="ok ol it lc b ld le lg lh lj om ln on lr oo lv op oq or os bi translated">递归特征消除背后的思想</li><li id="f490" class="ok ol it lc b ld ot lg ou lj ov ln ow lr ox lv op oq or os bi translated">如何使用Sklearn RFE类实现算法</li><li id="fb8b" class="ok ol it lc b ld ot lg ou lj ov ln ow lr ox lv op oq or os bi translated">如何决定使用RFECV类自动保留的特性数量</li></ul><p id="3506" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">如果你想更深入地了解算法，你可以阅读这篇<a class="ae kz" href="https://machinelearningmastery.com/rfe-feature-selection-in-python/" rel="noopener ugc nofollow" target="_blank">帖子</a>。</p></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h2 id="e6c4" class="mx my it bd mz na nb dn nc nd ne dp nf lj ng nh ni ln nj nk nl lr nm nn no np bi translated">关于特性选择的进一步阅读:</h2><ul class=""><li id="bb37" class="ok ol it lc b ld nq lg nr lj oy ln oz lr pa lv op oq or os bi translated"><a class="ae kz" rel="noopener" target="_blank" href="/how-to-use-variance-thresholding-for-robust-feature-selection-a4503f2b5c3f?source=your_stories_page-------------------------------------">如何使用方差阈值进行鲁棒特征选择</a></li><li id="f92b" class="ok ol it lc b ld ot lg ou lj ov ln ow lr ox lv op oq or os bi translated"><a class="ae kz" rel="noopener" target="_blank" href="/how-to-use-pairwise-correlation-for-robust-feature-selection-20a60ef7d10?source=your_stories_page-------------------------------------">如何使用成对相关性进行稳健的特征选择</a></li><li id="2f11" class="ok ol it lc b ld ot lg ou lj ov ln ow lr ox lv op oq or os bi translated"><a class="ae kz" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html" rel="noopener ugc nofollow" target="_blank">递归特征消除(RFE) Sklearn文档</a></li><li id="d1f0" class="ok ol it lc b ld ot lg ou lj ov ln ow lr ox lv op oq or os bi translated"><a class="ae kz" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html" rel="noopener ugc nofollow" target="_blank"> RFECV Sklearn文档</a></li></ul></div><div class="ab cl mq mr hx ms" role="separator"><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv mw"/><span class="mt bw bk mu mv"/></div><div class="im in io ip iq"><h2 id="6058" class="mx my it bd mz na nb dn nc nd ne dp nf lj ng nh ni ln nj nk nl lr nm nn no np bi translated">您可能也会感兴趣:</h2><ul class=""><li id="bbd7" class="ok ol it lc b ld nq lg nr lj oy ln oz lr pa lv op oq or os bi translated">【HalvingGridSearch使超参数调谐速度提高了11倍</li><li id="6017" class="ok ol it lc b ld ot lg ou lj ov ln ow lr ox lv op oq or os bi translated"><a class="ae kz" href="https://towardsdev.com/intro-to-object-oriented-programming-for-data-scientists-9308e6b726a2?source=your_stories_page-------------------------------------" rel="noopener ugc nofollow" target="_blank">面向数据科学家的面向对象编程介绍</a></li></ul></div></div>    
</body>
</html>