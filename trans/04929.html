<html>
<head>
<title>Phasic Policy Gradient (PPG) Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">阶段性政策梯度(PPG)第二部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/phasic-policy-gradient-ppg-part-2-c93afeaf37d4?source=collection_archive---------15-----------------------#2021-04-29">https://towardsdatascience.com/phasic-policy-gradient-ppg-part-2-c93afeaf37d4?source=collection_archive---------15-----------------------#2021-04-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b77f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">PyTorch中的实现</h2></div><p id="6033" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">如果你只是想要一个代码的链接，</strong> <a class="ae lb" href="https://github.com/09tangriro/ppg" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">这里就是</strong> </a> <strong class="kh ir">。尽情享受吧！</strong></p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/e76c21d2785d7406c41332e256e40ef0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*f2RlMFaxqQoCggas"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">照片由<a class="ae lb" href="https://unsplash.com/@florianolv?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Florian Olivo </a>在<a class="ae lb" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="498e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本系列的第一部分中，我们探究了PPG背后的理论。这一次，我们将采用一种更实用的方法，在LunarLander-v2环境上测试之前，使用PyTorch实现该算法！！</p><p id="7c2a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">提醒一下，算法本身如下图1所示:</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi ls"><img src="../Images/25b75f23bb2e20807709e0c6e799ccbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3hUhXQShyjZIQlXeUteEYw.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">图1: PPG算法[1]</p></figure><h1 id="4e32" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">把密码给我！</h1><p id="5b6a" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">这篇文章的目的是作为一个演示，如果你想要更多的功能，请留意<a class="ae lb" href="https://github.com/09tangriro/stable-baselines3-contrib" rel="noopener ugc nofollow" target="_blank">这个链接</a>，我在这里将算法添加到<a class="ae lb" href="https://github.com/Stable-Baselines-Team/stable-baselines3-contrib" rel="noopener ugc nofollow" target="_blank">稳定基线3 Contrib </a>开源库！</p><h2 id="2bfc" class="mq lu iq bd lv mr ms dn lz mt mu dp md ko mv mw mf ks mx my mh kw mz na mj nb bi translated">缓冲</h2><p id="9e43" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">我们需要两个缓冲器。一个用于训练策略阶段并跟踪代理在每个时间步经历的展开，另一个用于训练辅助阶段，它需要存储在每个时间步遍历的值目标和状态。代码1中定义了必要的存储器参数。注意，AuxMemory包含一个old_values条目，尽管图1中的算法没有指定这一点。这样做是为了允许更新函数中的值裁剪，以便进行更稳定的训练。</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nc nd l"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">代码1:记忆类型</p></figure><p id="9b24" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在代码2中，我们使用了集合库中的deque数据结构，所以我们不需要担心自己直接实现复杂的缓冲区！我们将把AuxMemory元组添加到aux_memories缓冲区，把Memory元组添加到memories缓冲区。</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nc nd l"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">代码2:缓冲区</p></figure><h2 id="da58" class="mq lu iq bd lv mr ms dn lz mt mu dp md ko mv mw mf ks mx my mh kw mz na mj nb bi translated">网络</h2><p id="d1bf" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">接下来，让我们定义我们将使用的神经网络。为了保持简单，我们坚持使用多层感知器(MLPs ),但也可以随意摆弄其他更有趣的网络！记住PPG的要点是有两个不相交的网络，所以我们定义了两个网络类，一个用于演员，一个用于评论家。</p><p id="783e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">代码3中显示了参与者。注意，我们定义了两个头；actor_head将输出用于策略分布的预测策略参数，而value_head将输出预测辅助值。</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nc nd l"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">代码3:演员神经网络</p></figure><p id="bdc1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">代码4中显示了批评家。这甚至比演员没有额外的头，只是一个香草MLP网络更简单！</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nc nd l"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">代码4:评论家神经网络</p></figure><h2 id="205c" class="mq lu iq bd lv mr ms dn lz mt mu dp md ko mv mw mf ks mx my mh kw mz na mj nb bi translated">培养</h2><p id="ad8b" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">有两个培训阶段，让我们从代码5所示的策略阶段开始。</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nc nd l"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">代码5:策略阶段</p></figure><p id="de2b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">clipped_value_loss如代码6所示，可用于促进更稳定的训练:</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nc nd l"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">代码6:限幅值损失函数</p></figure><p id="2216" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">代码7说明了辅助阶段。希望现在我们在辅助存储器中存储旧值预测的原因更加明显，因为这里的值损失使用代码6中的clipped_value_loss。</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nc nd l"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">代码7:辅助阶段</p></figure><h2 id="05bf" class="mq lu iq bd lv mr ms dn lz mt mu dp md ko mv mw mf ks mx my mh kw mz na mj nb bi translated">主循环</h2><p id="7b8f" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">现在，我们只需要把所有东西放在一起。代码8显示了主循环。<strong class="kh ir">关键线路是从105到112 </strong>，它们规定了培训时间表。</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="nc nd l"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">代码8:主PPG环路</p></figure><p id="0164" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">需要注意的是，这是为具有明确分布策略的代理而设计的，所以<strong class="kh ir">连续代理在这里不兼容！</strong></p><h1 id="5c5c" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">结果</h1><p id="f91f" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">我们使用OpenAI gym的LunarLander-v2环境来测试我们的代理，在下面的7500场比赛后检查结果:</p><figure class="ld le lf lg gt lh"><div class="bz fp l di"><div class="ne nd l"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">特工经过7500场训练！</p></figure><p id="e9bf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">很明显，代理在开始时出了一点问题，但能够自我纠正，再多一点培训，它可能会做得更好！</p></div><div class="ab cl nf ng hu nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="ij ik il im in"><p id="91e8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您觉得这篇文章有用，请考虑:</p><ul class=""><li id="e86a" class="nm nn iq kh b ki kj kl km ko no ks np kw nq la nr ns nt nu bi translated">跟踪我🙌</li><li id="0082" class="nm nn iq kh b ki nv kl nw ko nx ks ny kw nz la nr ns nt nu bi translated"><a class="ae lb" href="https://medium.com/subscribe/@rohan.tangri" rel="noopener"> <strong class="kh ir">订阅我的邮件通知</strong> </a>永不错过上传📧</li><li id="75a9" class="nm nn iq kh b ki nv kl nw ko nx ks ny kw nz la nr ns nt nu bi translated">使用我的媒介<a class="ae lb" href="https://medium.com/@rohan.tangri/membership" rel="noopener"> <strong class="kh ir">推荐链接</strong> </a> <strong class="kh ir"> </strong>直接支持我，并获得无限量的优质文章🤗</li><li id="7538" class="nm nn iq kh b ki nv kl nw ko nx ks ny kw nz la nr ns nt nu bi translated">使用我的<a class="ae lb" href="https://magic.freetrade.io/join/rohan/1095e108" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">自由贸易链接</strong> </a>获得价值3- 200英镑的免费份额🤑</li></ul><h1 id="dae9" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">参考</h1><p id="3e08" class="pw-post-body-paragraph kf kg iq kh b ki ml jr kk kl mm ju kn ko mn kq kr ks mo ku kv kw mp ky kz la ij bi translated">[1]卡尔·科布、雅各布·希尔顿、奥列格·克里莫夫、约翰·舒尔曼<a class="ae lb" href="https://arxiv.org/abs/2009.04416" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/2009.04416</a>提出的阶段性政策梯度</p></div></div>    
</body>
</html>