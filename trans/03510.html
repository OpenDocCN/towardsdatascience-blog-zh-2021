<html>
<head>
<title>Framework for prototyping of machine learning algorithms (Python)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习算法原型框架(Python)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/framework-for-prototyping-of-machine-learning-algorithms-a3bd5f6f3a22?source=collection_archive---------10-----------------------#2021-03-21">https://towardsdatascience.com/framework-for-prototyping-of-machine-learning-algorithms-a3bd5f6f3a22?source=collection_archive---------10-----------------------#2021-03-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d2b6" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">利用Python中的scipy开发损失函数的经典机器学习算法原型的通用框架</h2></div><p id="3676" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">简介</strong></p><p id="a99d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">阅读科学文章和尝试新的有前途的算法是许多数据科学家的重点。不幸的是，这些算法位于小的存储库中，并且通常包含几个错误，因此它们很难使用，并且您将花费相当多的时间来调试(或者解决依赖冲突)。幸运的是，这些算法通常可以简化并使用损失函数来表示，在本文中，我将使用主成分分析(PCA)和支持向量机(SVM)作为主要示例，但它适用于更多技术。</p><p id="cd0c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这种方法有多种优势:</p><ul class=""><li id="d4b6" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la lg lh li lj bi translated">了解机器学习算法的底层行为。</li><li id="57bd" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">避免由于可用实现中的某些默认设置而导致的错误。</li><li id="5236" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">易于针对数据的某些特性进行定制(以防违反常见假设)。</li><li id="3a74" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">不要浪费时间为你的问题寻找一个好的算法实现。</li><li id="3bfa" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">通过调整损失函数(如Huber或Hampel)，容易为异常值建立鲁棒性。</li><li id="df49" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">大多数算法假设误差项呈正态分布，然而，这并不总是有效的。通过调整损失函数，这些误差项可以归一化。</li></ul><p id="51b1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">可能的缺点:</p><ul class=""><li id="1606" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la lg lh li lj bi translated">性能/速度</li><li id="9b6a" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">数值不稳定性</li></ul></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="449a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">主成分分析的损失函数表示</strong></p><p id="e159" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">主成分分析(PCA)可以被表示为寻找最大化方差的投影，第一主成分可以被获得为:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/bf292360eb1c12439a9e81651b613d62.png" data-original-src="https://miro.medium.com/v2/resize:fit:362/format:webp/1*6DwWGfuFHAU5H_O-UOphcw.png"/></div></figure><p id="f5cb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中w是第一主成分(p×1)的加载向量，p是数据的维数，x_k是样本k(p×1)，总共有N个样本。更多信息可以在“支持向量机:方法和应用”(J. Suykens)中找到。</p><p id="6852" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在当前形式中，如果w变得无限大，则该损失函数最大，因此范数通常被限制为1(归一化):</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi me"><img src="../Images/448893dbbd6f7606abebd53f41d83033.png" data-original-src="https://miro.medium.com/v2/resize:fit:148/format:webp/1*v_9-srZkd-xlGYYmliEpYw.png"/></div></figure><p id="709f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这可以翻译成Python代码，如下所示:</p><figure class="lx ly lz ma gt mb"><div class="bz fp l di"><div class="mf mg l"/></div></figure><p id="dd56" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这导致了一个约束优化问题，我们可以使用scipy.optimize轻松地解决它，它支持许多不同的求解器，具体取决于您的需求，并且已经根据您的输入选择了一个合适的求解器。</p><p id="7656" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">将生成一些随机高斯数据(维数p=2 ),并且将使用来自sklearn.decomposition的PCA实现对其进行基准测试。</p><figure class="lx ly lz ma gt mb"><div class="bz fp l di"><div class="mf mg l"/></div></figure><p id="4dfc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">PC1使用sk learn:[-0.42388046 -0.90571814]<br/>PC1使用损失函数:[-0.42388046-0.90571814]</p><p id="56d8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第一个主成分的加载对于两个实现是完全相同的，这表明该框架仅使用几行代码就可以强大地表示算法。</p><p id="f769" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们将在i7–8750h CPU(2.2 GHz，12个内核)上对两种实现的速度性能进行基准测试，以获得第一个主成分。scikit-learn实现使用LAPACK库(Fortran)解决了奇异值分解。我们将对X数据集中的多个样本进行比较。条形图中显示的时间包括创建实例和定义方法。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/86826f4e6c7d76c908fff384659d5230.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*1BJY9ezMoF3fLixWx4aEpA.png"/></div></figure><p id="6212" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这种情况下，损失函数的实现也明显快于用于计算第一主分量的奇异值分解。scikit-learn实现赶上了大样本量，但这只是因为它开始对数据进行随机下采样，因此<strong class="kh ir">损失函数实现更快、更准确</strong>，而<strong class="kh ir">在该基准测试中只有几行代码</strong>。</p></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="fcb1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">SVM的损失函数表示</strong></p><p id="dda3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">支持向量机可以在原始空间和对偶空间中求解(如果使用的核可以在原始空间中表示)。为了简单起见，这里实现的SVM将在原始空间中求解。有关原始和对偶空间的更多背景信息，请参见“支持向量机:方法和应用”(J. Suykens)。</p><p id="5220" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">原始空间中线性SVM的损失函数被定义为:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/9a409b064a757dc4c06dc88334313780.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*qQ2E1TNJF6jGNrngIRvJuw.png"/></div></figure><p id="3581" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有以下限制:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/c4eed91c2b0a9ee302d3d8aadd2ec7b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:414/format:webp/1*IH1VSZ2Mfgi_Wt4OEyVZSg.png"/></div></figure><p id="c861" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中w是我们想要获得的权重向量(p x 1)，gamma是一个超参数，它指定了二阶正则化和误分类数之间的理想折衷(如果gamma很大，将会有更多的过拟合)。x_k是样本k(p×1)，总共有N个样本。ζ_ k是样本k的松弛变量，样本k被错误分类得越多(离判定边界越远)，该变量就越大。y_k是样本k的标签，可以是正(+1)，也可以是负(-1)。b是决策边界的偏移量。</p><p id="d7d3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">将其转换为Python代码会产生以下类(铰链损耗的平方):</p><figure class="lx ly lz ma gt mb"><div class="bz fp l di"><div class="mf mg l"/></div></figure><p id="44c6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在将基于损失函数的这个类与sklearn实现进行比较(就像我们对PCA所做的那样)。这会产生以下python代码:</p><figure class="lx ly lz ma gt mb"><div class="bz fp l di"><div class="mf mg l"/></div></figure><p id="5010" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">重量sklearn:[0.349575-0.17398369]<br/>重量损失:[ 0.34916208 -0.17402776]</p><p id="e089" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">权重之间有微小的差异，但是如果我们画出决策边界，我们可以看到几乎是相同的。</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/716f156da221180e65f1136355098742.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*os3Co3qCPvIkfFTmtsIaIQ.png"/></div></figure><p id="5b0e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，性能将进行基准测试，类似于我们对PCA所做的，两种实现都允许运行，直到收敛(迭代次数没有截止)。这导致:</p><figure class="lx ly lz ma gt mb gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/6aebf5f3755fb4182e30417fb662f86a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*CG_OL5xJ8oxfexZFzuhYcg.png"/></div></figure><p id="6434" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对于1000个样本，定制实现大约慢10倍，对于50000个样本，这降低到大约慢3倍。总的来说，这对于一个两行长的原型来说是不错的。请注意，约束是以非线性方式定义的，而它们实际上可以写成线性约束。改进这一点也将改善定制应用程序的计时。</p></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="7cb5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">结论</strong></p><p id="2fb6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">通过编写一个优化损失函数的小类来原型化机器学习算法是快速、准确和容易的。这样做将增加你对问题的了解，从而允许你改进算法。如果原型满足您的需求，但是数值稳定性或速度对于您的应用程序来说不够，那么建议您在论文中搜索快速实现，或者查看一些git库。</p></div><div class="ab cl lp lq hu lr" role="separator"><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu lv"/><span class="ls bw bk lt lu"/></div><div class="ij ik il im in"><p id="bf3f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">参考文献</strong></p><p id="5fe5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mk">JAK·苏肯斯，T·范·盖斯特尔，J·德·布拉班特，B·德·穆尔，J·范德瓦勒。(2002).“最小二乘支持向量机:方法和应用”。世界科学</em></p><p id="6d90" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mk">n . Halko、p . g . Martins son和j . a . Tropp(2011年)。《寻找具有随机性的结构:构造近似矩阵分解的概率算法》。暹罗评论，53(2)，217–288。</em></p><p id="a1c0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="mk"> Martinsson，P. G .，Rokhlin，v .和Tygert，M. (2011年)。“矩阵分解的随机算法”。应用和计算谐波分析，30(1)，47–68。</em></p></div></div>    
</body>
</html>