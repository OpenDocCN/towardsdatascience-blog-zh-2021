<html>
<head>
<title>A Comparison of Synthetic vs Human Labeled Dataset to Train a UNet Segmentation Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">训练UNet分割模型的合成与人工标记数据集的比较</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-comparison-of-synthetic-vs-human-labeled-dataset-to-train-a-unet-segmentation-model-10588f8d9c12?source=collection_archive---------37-----------------------#2021-04-28">https://towardsdatascience.com/a-comparison-of-synthetic-vs-human-labeled-dataset-to-train-a-unet-segmentation-model-10588f8d9c12?source=collection_archive---------37-----------------------#2021-04-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="461b" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/making-sense-of-big-data" rel="noopener" target="_blank">理解大数据</a></h2><div class=""/><p id="4f27" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">作者:<a class="ae ku" href="https://www.linkedin.com/in/aaronsoellinger/" rel="noopener ugc nofollow" target="_blank">伦索林格</a> &amp; <a class="ae ku" href="https://www.linkedin.com/in/willkunz/" rel="noopener ugc nofollow" target="_blank">威尔昆兹</a>@<a class="ae ku" href="https://wallshots.co/" rel="noopener ugc nofollow" target="_blank">WallShots.co</a></p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div class="gh gi kv"><img src="../Images/bd811d7ae9c78a5200d1d40e099b7035.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*FIi65rkAZi8elUrl9_g16Q.jpeg"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">将合成训练数据与传统训练数据进行比较(图片由作者提供)</p></figure><p id="c5f0" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">手动标记数据既昂贵又繁琐。一种紧急的方法是使用合成数据生成来大规模减少标记足够大的数据集以进行分割所必需的提升。在本文中，我们使用一个合成数据集和一个传统标记的小型数据集创建了一个基准模型。在我们的例子中，没有现成的训练数据集，所以我们必须创建它。</p><p id="2702" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我们正在构建的模型的目标是识别图像中的任何图片框架。具体来说，我们希望识别包含图片的图片帧区域(不是填充或帧本身)。本文描述了我们的解决方案架构，以及我们如何使用它来比较用两种不同方法创建的基线模型，以形成训练数据集。首先，我们生成一个简单的合成数据集，它由彩色矩形组成。我们将其与另一种方法进行比较，即所谓的“传统方法”，在这种方法中，使用我们的标记工具(Label Studio)找到并标记“野生的”。</p><p id="19aa" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">我们针对这个问题的栈是Label Studio，用Python Fast.ai训练环境进行标注。在本文中，实验基于带有<code class="fe lh li lj lk b">resnet34</code>主干的unet架构。</p><p id="c827" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">最后，我们将展示使用不同方法训练的两个基线模型的比较，以开发训练数据集。我们希望这篇文章对那些实现细分模型的人来说是一个有趣的案例研究。</p><p id="262c" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">对于传统的标签任务，我们使用Label Studio。我们贴标设置的详细描述可在此处找到:</p><p id="5b27" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated"><a class="ae ku" rel="noopener" target="_blank" href="/development-of-a-benchmark-dataset-with-an-interface-to-the-fastai-dataloader-using-label-studio-d3aa3c26661f">https://towardsdatascience . com/development-of-a-benchmark-dataset-with-a-interface-to-the-fastai-data loader-using-label-studio-d3aa 3c 26661 f</a></p></div><div class="ab cl ll lm hu ln" role="separator"><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq lr"/><span class="lo bw bk lp lq"/></div><div class="ij ik il im in"><h1 id="447b" class="ls lt iq bd lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp bi translated">数据集</h1><h2 id="c2f7" class="mq lt iq bd lu mr ms dn ly mt mu dp mc kh mv mw mg kl mx my mk kp mz na mo iw bi translated">合成数据集</h2><p id="233a" class="pw-post-body-paragraph jw jx iq jy b jz nb kb kc kd nc kf kg kh nd kj kk kl ne kn ko kp nf kr ks kt ij bi translated">处理1:在具有以下特征的2000幅合成生成的图像上进行训练:</p><ul class=""><li id="a718" class="ng nh iq jy b jz ka kd ke kh ni kl nj kp nk kt nl nm nn no bi translated">单一矩形，即放置在背景“场景”上的“框架”</li><li id="3df2" class="ng nh iq jy b jz np kd nq kh nr kl ns kp nt kt nl nm nn no bi translated">在界限内随机调整大小</li><li id="0b7b" class="ng nh iq jy b jz np kd nq kh nr kl ns kp nt kt nl nm nn no bi translated">在界限内随机着色</li><li id="47ef" class="ng nh iq jy b jz np kd nq kh nr kl ns kp nt kt nl nm nn no bi translated">随机放置在固定大小(224x224)的背景矩形边界内</li></ul><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="gh gi nu"><img src="../Images/858277658e4bcb6231183c2aed6ad6c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q-EUvYmoUK5m9KvZ6E5pvA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">可视化小批量合成数据示例(图片由作者提供)</p></figure><pre class="kw kx ky kz gt nz lk oa ob aw oc bi"><span id="ae88" class="mq lt iq lk b gy od oe l of og">## Primary synthetic data generator method, amount of desired data can be adjusted ##<br/>def _random_select_valid_tl(scene:MaskedImg, frame_template:dict, mask:MaskedImg):<br/>    """<br/>    TODO: support multiple frames, currently could have overlapping frames<br/>    """<br/>    sx,sy,sz = scene.img.shape<br/>    vertices = np.array(frame_template['label_data'])<br/>    fwidth = abs(<br/>        max(vertices[:,0]) -<br/>        min(vertices[:,0])<br/>    )<br/>    flength = abs(<br/>        max(vertices[:,1]) - <br/>        min(vertices[:,1])<br/>    )<br/>    tlx,tly = (<br/>        np.random.randint(0, sx-fwidth-1),<br/>        np.random.randint(0, sy-flength-1)<br/>    )<br/>    return tlx,tly</span><span id="2a29" class="mq lt iq lk b gy oh oe l of og">def add_frame_to_scene(scene:MaskedImg, frame_template:dict, mask:MaskedImg, plot:bool):<br/>    """<br/>    In:<br/>        scene: np.array, mutable, frame gets written on top<br/>        frame: np.array, list of vertices<br/>        <br/>        <br/>    """<br/>    # adjust the frame coords<br/>    tlx,tly = _random_select_valid_tl(<br/>        scene=scene, frame_template=frame_template, mask=mask<br/>    )<br/>    frame = np.array(frame_template['label_data']).copy()<br/>    frame[:,0] = frame[:,0] + tlx<br/>    frame[:,1] = frame[:,1] + tly<br/>    <br/>    # create the "filled scene"<br/>    vertices_to_region(<br/>        mask_i=scene.img, # gets muted<br/>        label_data=frame.tolist(),<br/>        label_num=frame_template['color'],<br/>        plot=plot<br/>    )<br/>    <br/>    # update the mask to reflect the frame added to the scene<br/>    vertices_to_region(<br/>        mask_i=mask.img, # gets muted<br/>        label_data=frame.tolist(),<br/>        label_num=frame_template['label_num'],<br/>        plot=plot<br/>    )<br/>    # plt.imshow(mask.img)<br/>    # plt.show()</span><span id="33d7" class="mq lt iq lk b gy oh oe l of og">def select_random_color(color_list:list):<br/>    sc = color_list[np.random.randint(0,len(color_list))]<br/>    return color_list[<br/>        np.random.randint(0,len([x for x in color_list if x != sc]))<br/>    ]</span><span id="600c" class="mq lt iq lk b gy oh oe l of og">def generator(scene_shapes, frame_templates, color_list, masks_path, scenes_path, plot=False):<br/>    errors = 0<br/>    for i in range(len(scene_shapes)):<br/>        scene_shape = scene_shapes[i]<br/>        frame_template = frame_templates[i]<br/>        <br/>        # instantiate the scene<br/>        scene = MaskedImg()<br/>        scene.load_from_parameters(<br/>            shape=scene_shape,<br/>            value=select_random_color(color_list),<br/>            value_space='rgb'<br/>        )</span><span id="d150" class="mq lt iq lk b gy oh oe l of og"># instantiate the mask<br/>        mask = MaskedImg()<br/>        mask.load_from_parameters(<br/>            shape=scene_shape[:2],<br/>            value=0,<br/>            value_space='int'<br/>        )<br/>        try:<br/>            add_frame_to_scene(<br/>                scene=scene, <br/>                frame_template=frame_template, <br/>                mask=mask,<br/>                plot=plot<br/>            )<br/>        except:<br/>            errors += 1<br/>            continue<br/>            <br/>        # plt.imshow(scene.img)<br/>        # plt.show()<br/>        # plt.imshow(mask.img)<br/>        # plt.show()<br/>        <br/>        maskfp = f'{str(masks_path)}/{i}.tif'<br/>        scenefp = f'{str(scenes_path)}/{i}.jpeg'<br/>        mask.save(fp=maskfp)<br/>        scene.save(fp=scenefp)<br/>        <br/>    print('Finished with {} errors.'.format(errors))</span><span id="42ff" class="mq lt iq lk b gy oh oe l of og">exp_id = 0<br/>scenes_path = Path("/ws/data/wallshots-framefinder/{}/scenes".format(exp_id))<br/>masks_path = Path("/ws/data/wallshots-framefinder/{}/masks".format(exp_id))<br/>scenes_path.mkdir(exist_ok=True, parents=True)<br/>masks_path.mkdir(exist_ok=True, parents=True)</span><span id="ba71" class="mq lt iq lk b gy oh oe l of og"><br/>n = 2000<br/>color_list = [<br/>    (0, 0, 255), (102, 255, 51), (204, 153, 0), (255, 51, 204), <br/>    (51, 102, 153), (255, 0, 0), (0, 255, 0), (255, 255, 0), (0, 255, 255),<br/>    (128, 0, 255), (204, 102, 0), (153, 0, 51), (255, 102, 153),<br/>    (102, 255, 153), (204, 255, 153), (255, 255, 204), (51, 51, 0), <br/>    (126, 153,64), (230, 30, 120), (50, 23, 200)]</span><span id="4d0c" class="mq lt iq lk b gy oh oe l of og">scene_shapes = [(224,224) for i in range(n)]<br/>lws = [(np.random.randint(10,100),np.random.randint(10,100)) for i in range(n)]<br/>frame_templates = [{<br/>        'label_data': np.array([<br/>            (0,0), (0,lw[1]), (lw[0],lw[1]), (lw[0],0)<br/>        ]),<br/>        'color': select_random_color(color_list),<br/>        'label_num': 1<br/>    } for lw in lws]</span><span id="0525" class="mq lt iq lk b gy oh oe l of og">generator(<br/>    scene_shapes=scene_shapes, <br/>    frame_templates=frame_templates, <br/>    color_list=color_list,<br/>    scenes_path=scenes_path,<br/>    masks_path=masks_path,<br/>    plot=False<br/>)</span></pre><h2 id="a027" class="mq lt iq bd lu mr ms dn ly mt mu dp mc kh mv mw mg kl mx my mk kp mz na mo iw bi translated">手动标记的数据集</h2><p id="7aec" class="pw-post-body-paragraph jw jx iq jy b jz nb kb kc kd nc kf kg kh nd kj kk kl ne kn ko kp nf kr ks kt ij bi translated">从互联网上选择10张图片，并使用我们的标签测试平台进行标签。我们再次使用Label Studio完成传统的标记任务，建立了一个新的Label Studio项目，这样我们就不会混淆我们为基准目的标记的图像和我们为训练目的创建的标签。</p><ul class=""><li id="48d7" class="ng nh iq jy b jz ka kd ke kh ni kl nj kp nk kt nl nm nn no bi translated">10张基础图片取自“野外”资源，如谷歌搜索。</li><li id="47dd" class="ng nh iq jy b jz np kd nq kh nr kl ns kp nt kt nl nm nn no bi translated">在没有显著微调的情况下，在“第一遍”中手动标记。</li><li id="c6aa" class="ng nh iq jy b jz np kd nq kh nr kl ns kp nt kt nl nm nn no bi translated">图像包含真实世界或真实世界人造场景中的图片框架</li></ul><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="gh gi oi"><img src="../Images/482b4506dc45126bf60d7f009a1f6e87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M8ekyZbYXAw2hzOxAIU4_g.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">Label Studio设置的屏幕截图(图片由作者提供)</p></figure><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="gh gi oj"><img src="../Images/d29e84a4caafcdaca3ec1041ad02fc88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Toep7CGYTGvu08_U2hph0Q.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">可视化小批量的传统标签示例(图片由作者提供)</p></figure><h2 id="8fd6" class="mq lt iq bd lu mr ms dn ly mt mu dp mc kh mv mw mg kl mx my mk kp mz na mo iw bi translated">基准数据集</h2><p id="fe1e" class="pw-post-body-paragraph jw jx iq jy b jz nb kb kc kd nc kf kg kh nd kj kk kl ne kn ko kp nf kr ks kt ij bi translated">我们通过仔细选择“来自野外”的示例图像并使用label studio分段功能对其进行标记，创建了一个基准数据集。参见我们之前的一篇文章，该文章对此进行了深入描述:<a class="ae ku" rel="noopener" target="_blank" href="/development-of-a-benchmark-dataset-with-an-interface-to-the-fastai-dataloader-using-label-studio-d3aa3c26661f">https://towardsdatascience . com/development-of-a-benchmark-dataset-with-a-interface-to-the-fastai-data loader-using-label-studio-d3aa 3c 26661 f</a></p><ul class=""><li id="2a98" class="ng nh iq jy b jz ka kd ke kh ni kl nj kp nk kt nl nm nn no bi translated">8基本图像取自“野外”来源，如谷歌搜索。</li><li id="4f41" class="ng nh iq jy b jz np kd nq kh nr kl ns kp nt kt nl nm nn no bi translated">手动标记，并进行重大微调。</li><li id="6d6b" class="ng nh iq jy b jz np kd nq kh nr kl ns kp nt kt nl nm nn no bi translated">图像包含真实世界或真实世界人造场景中的图片框架。</li><li id="9224" class="ng nh iq jy b jz np kd nq kh nr kl ns kp nt kt nl nm nn no bi translated">图像选择绊倒模型。例如，条纹墙、障碍物、景深、图像噪声。</li></ul><figure class="kw kx ky kz gt la gh gi paragraph-image"><div role="button" tabindex="0" class="nv nw di nx bf ny"><div class="gh gi ok"><img src="../Images/5f06c5d9ab91badfacbc683d09d7e6e5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7ddSsFSxGD2hvTBPuxvCOQ.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">可视化基准数据集(作者提供的图片)</p></figure><h1 id="3990" class="ls lt iq bd lu lv ol lx ly lz om mb mc md on mf mg mh oo mj mk ml op mn mo mp bi translated">实验</h1><h2 id="6b1f" class="mq lt iq bd lu mr ms dn ly mt mu dp mc kh mv mw mg kl mx my mk kp mz na mo iw bi translated">测量结果</h2><p id="5c66" class="pw-post-body-paragraph jw jx iq jy b jz nb kb kc kd nc kf kg kh nd kj kk kl ne kn ko kp nf kr ks kt ij bi translated">每个实验中创建的模型都根据独立的基准数据集进行评估，该数据集旨在准确地表示模型的真实环境。单独的基准数据集将是实验结果的唯一真实来源。我们之所以有一个单独的基准数据集和自己的数据管道，是因为基准数据集的目的需要明确，并反映在创建它的底层流程中。这里定义了一个基准数据集来注入导致模型失效的例子。与此相反，训练数据集旨在使模型尽可能好。我们认为这可能是一个重要的区别，它保证了单独维护基准数据集的额外开销。</p><h2 id="2458" class="mq lt iq bd lu mr ms dn ly mt mu dp mc kh mv mw mg kl mx my mk kp mz na mo iw bi translated">模型</h2><p id="f531" class="pw-post-body-paragraph jw jx iq jy b jz nb kb kc kd nc kf kg kh nd kj kk kl ne kn ko kp nf kr ks kt ij bi translated">这里，我们比较两个不同实验的结果。这些模型将非常相似，但不同之处在于它们训练的训练数据集。在每种情况下，模型的实例化都是相似的，使用fastai <code class="fe lh li lj lk b">unet_learner</code>需要一个分段数据加载器。我们能够对所有数据集使用相同的目录结构，这简化了数据加载器的创建，因为它们可以共享相同的代码。该结构如下所示:</p><pre class="kw kx ky kz gt nz lk oa ob aw oc bi"><span id="cf69" class="mq lt iq lk b gy od oe l of og">benchmark/<br/>├── masks<br/>│   ├── 1.tif<br/>│   ├── 2.tif<br/>│   ├── 3.tif<br/>│   ├── 4.tif<br/>│   ├── 5.tif<br/>│   ├── 6.tif<br/>│   ├── 8.tif<br/>│   └── 9.tif<br/>└── scenes<br/>    ├── 1.jpg<br/>    ├── 2.jpg<br/>    ├── 3.jpg<br/>    ├── 4.jpg<br/>    ├── 5.jpg<br/>    ├── 6.jpg<br/>    ├── 8.jpg<br/>    └── 9.jpg</span></pre><p id="17e0" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">数据加载器实例化如下，其中<code class="fe lh li lj lk b">saveto</code>是文件夹位置(例如在<code class="fe lh li lj lk b">benchmark/</code>之上):</p><pre class="kw kx ky kz gt nz lk oa ob aw oc bi"><span id="7a43" class="mq lt iq lk b gy od oe l of og">size = 224<br/>imgs_saveto = saveto/'scenes'<br/>dls = SegmentationDataLoaders.from_label_func(<br/>    imgs_saveto, <br/>    bs=6,<br/>    fnames=[<br/>        name <br/>        for name in imgs_saveto.iterdir()<br/>        if not name.is_dir()<br/>    ],<br/>    label_func=<br/>        lambda x: <br/>        str(x).replace('scenes', 'masks').replace('jpg', 'tif'),<br/>    item_tfms=[Resize((size,size))],<br/>    batch_tfms=[<br/>        Normalize.from_stats(*imagenet_stats)<br/>    ],<br/>    valid_pct=0.00<br/>)</span></pre><p id="0416" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">在传统创建的训练示例的情况下，<code class="fe lh li lj lk b">valid_pct</code>参数为0.0，因为我们想要利用所有10个图像，因为图像很少。每一个都是珍贵的…</p><h2 id="884b" class="mq lt iq bd lu mr ms dn ly mt mu dp mc kh mv mw mg kl mx my mk kp mz na mo iw bi translated">培养</h2><p id="3192" class="pw-post-body-paragraph jw jx iq jy b jz nb kb kc kd nc kf kg kh nd kj kk kl ne kn ko kp nf kr ks kt ij bi translated">我们通过绘制覆盖在来自基准数据集的输入图像上的预测掩膜，对两个模型进行了并排比较。为此，我们对用合成数据训练的模型运行了以下训练程序:</p><pre class="kw kx ky kz gt nz lk oa ob aw oc bi"><span id="7347" class="mq lt iq lk b gy od oe l of og">learn = unet_learner(<br/>    dls, <br/>    resnet34,<br/>    cbs=WandbCallback(),<br/>    n_out=2,<br/>    path=Path('.')<br/>)</span><span id="f03b" class="mq lt iq lk b gy oh oe l of og">learn.fit_one_cycle(10, slice(1e-2, 1e-3))<br/>learn.save('model1-a-0')<br/>learn.fit_one_cycle(10, slice(1e-2, 1e-3))<br/>learn.fit_one_cycle(10, slice(1e-5, 1e-7))<br/>learn.fit_one_cycle(10, slice(1e-6, 1e-7))<br/>learn.save('model1-a-1')</span></pre><p id="1eea" class="pw-post-body-paragraph jw jx iq jy b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ij bi translated">对于每个历元，合成数据集比传统标记的数据集花费更长的时间。我们使用的训练策略只是给定数据的一个合理的起点。对于传统标记的数据集，我们运行以下程序:</p><pre class="kw kx ky kz gt nz lk oa ob aw oc bi"><span id="0df3" class="mq lt iq lk b gy od oe l of og">learn = unet_learner(<br/>    dls,<br/>    resnet34,<br/>    cbs=WandbCallback(), <br/>    n_out=2, <br/>    path=Path('.')<br/>)</span><span id="fbca" class="mq lt iq lk b gy oh oe l of og">learn.fit_one_cycle(20, slice(1e-2, 1e-3))<br/>learn.fit_one_cycle(20, slice(1e-2, 1e-3))<br/>learn.fit_one_cycle(20, slice(1e-2, 1e-3))<br/>learn.fit_one_cycle(20, slice(1e-2, 1e-3))<br/>learn.fit_one_cycle(20, slice(1e-2, 1e-3))<br/>learn.fit_one_cycle(20, slice(1e-5, 1e-7))<br/>learn.fit_one_cycle(20, slice(1e-6, 1e-7))<br/>learn.save('model2')</span></pre><h2 id="6fb0" class="mq lt iq bd lu mr ms dn ly mt mu dp mc kh mv mw mg kl mx my mk kp mz na mo iw bi translated">定性评价</h2><p id="34b1" class="pw-post-body-paragraph jw jx iq jy b jz nb kb kc kd nc kf kg kh nd kj kk kl ne kn ko kp nf kr ks kt ij bi translated">在下图中，我们将调整了大小的输入上的预测掩码叠加到使用上面粘贴的策略训练的每个模型上。在左侧，图像通过仅使用合成数据创建的模型运行，而右侧是来自我们手动标记的数据集的模型。</p><figure class="kw kx ky kz gt la gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/1129ce2cce68bf6122281573da007d82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*Iw91cYExFPP4jFqkvL2b7w.jpeg"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">用2000个简单合成生成的图像(左)与10个手动标记的图像(右)训练的模型的比较(图像由作者提供)</p></figure><h2 id="925a" class="mq lt iq bd lu mr ms dn ly mt mu dp mc kh mv mw mg kl mx my mk kp mz na mo iw bi translated">结论</h2><p id="e225" class="pw-post-body-paragraph jw jx iq jy b jz nb kb kc kd nc kf kg kh nd kj kk kl ne kn ko kp nf kr ks kt ij bi translated">这个练习的目的是对下一次投资做出决定。我们应该花费更多的资源来标记数据，还是应该进一步开发代码以使合成数据集更加真实？我们对传统上标记为训练数据集的10图像的有效性印象最深。因此，前进的道路是添加数据扩充，以更好地最大化来自传统数据集的每个训练样本。我们直观地看到，虽然很有希望，但使合成标签代码更真实的前期投资是很高的。因此，我们决定以后再来。有价值的方法是传统和合成标记方法的结合，这将使我们能够生成大量有点真实的标记图像，而无需在标记或虚拟现实开发方面的巨大投资。</p></div></div>    
</body>
</html>