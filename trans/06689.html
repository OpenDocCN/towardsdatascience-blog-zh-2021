<html>
<head>
<title>5 Explainable Machine Learning Models You Should Understand</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你应该了解的5个可解释的机器学习模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/explainable-ai-9a9af94931ff?source=collection_archive---------5-----------------------#2021-06-16">https://towardsdatascience.com/explainable-ai-9a9af94931ff?source=collection_archive---------5-----------------------#2021-06-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3c51" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">当简单的方法奏效时，为什么要使用复杂的模型？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4e727647022ae2ad8d9a424c58c9d681.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*1VYs2MKkUd2TAK0b"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">尼克·莫里森在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="3dea" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="570a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">众所周知，机器学习在我们的日常生活中无处不在。从亚马逊上的<strong class="lt iu">产品推荐</strong>，定向广告，看什么的<strong class="lt iu">建议</strong>，到搞笑的<strong class="lt iu"> Instagram </strong> <strong class="lt iu">滤镜</strong>。</p><p id="4a4c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果这些出了问题，<strong class="lt iu">大概不会毁了你的人生</strong>。也许你不会得到完美的自拍，或者也许公司将不得不在广告上花更多的钱。</p><p id="6a5a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">执法中的面部识别怎么样？贷款或抵押申请？无人驾驶车辆？</strong></p><p id="5cac" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在这些<strong class="lt iu">高风险</strong>的应用中，我们不能盲目的进去。我们需要能够剖析我们的模型，我们需要能够在模型接近生产系统之前理解和解释我们的模型。</p><blockquote class="ms"><p id="2d53" class="mt mu it bd mv mw mx my mz na nb mm dk translated">当我们对人们做出可能对他们的生活产生负面影响的决策时，如抵押贷款或信用评分，可解释的机器学习是必不可少的。</p></blockquote><p id="77f9" class="pw-post-body-paragraph lr ls it lt b lu nc ju lw lx nd jx lz ma ne mc md me nf mg mh mi ng mk ml mm im bi translated">使用可解释的模型还允许更有效的调试，以及更好地理解模型中的<strong class="lt iu">公平性、隐私</strong>、<strong class="lt iu">因果关系</strong>和更多<strong class="lt iu">信任</strong>。</p><h1 id="7dd3" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">目录</h1><ul class=""><li id="9a52" class="nh ni it lt b lu lv lx ly ma nj me nk mi nl mm nm nn no np bi translated"><a class="ae ky" href="#b822" rel="noopener ugc nofollow">可解释的类型</a></li><li id="3601" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nm nn no np bi translated"><a class="ae ky" href="#13ad" rel="noopener ugc nofollow">可解释模型</a> <br/> - <a class="ae ky" href="#d2c9" rel="noopener ugc nofollow">广义线性模型</a> <br/> - <a class="ae ky" href="#3896" rel="noopener ugc nofollow">决策树</a> <br/> - <a class="ae ky" href="#26f0" rel="noopener ugc nofollow">广义可加模型</a> <br/> - <a class="ae ky" href="#3500" rel="noopener ugc nofollow">单调梯度推进</a>-<a class="ae ky" href="#8e7f" rel="noopener ugc nofollow">TabNet</a></li><li id="8256" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nm nn no np bi translated"><a class="ae ky" href="#f8b3" rel="noopener ugc nofollow">比较</a></li><li id="e7d6" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nm nn no np bi translated"><a class="ae ky" href="#9379" rel="noopener ugc nofollow">结论</a></li></ul><h1 id="b822" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">可解释的类型</h1><ul class=""><li id="448c" class="nh ni it lt b lu lv lx ly ma nj me nk mi nl mm nm nn no np bi translated"><strong class="lt iu"> Post-hoc: </strong>这是指我们在预测完成后或模型训练完成后解释模型。这很棒，因为这些方法允许我们解释高度复杂的模型。然而，这些方法可能被愚弄，在特定条件下有缺陷，并且需要额外的复杂性来产生解释。常见的例子是SHAP和莱姆Python包。</li><li id="2e3c" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nm nn no np bi translated"><strong class="lt iu">固有:</strong>有些模型可以开箱即用，无需额外的模型或库。这些通常更简单，在某些情况下可能预测能力更低——尽管<a class="ae ky" href="https://arxiv.org/pdf/2004.14545.pdf" rel="noopener ugc nofollow" target="_blank">一些研究人员</a>认为情况并不总是如此！</li></ul><p id="2c67" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">本文将涵盖内在可解释的模型。</strong></p><h1 id="13ad" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">内在可解释的模型</h1><p id="4aab" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><em class="nv">利普顿，2016</em>(<a class="ae ky" href="https://arxiv.org/abs/1606.03490" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1606.03490</a>)使用3个标准定义模型可解释性:</p><ol class=""><li id="ce4b" class="nh ni it lt b lu mn lx mo ma nw me nx mi ny mm nz nn no np bi translated"><strong class="lt iu">模拟能力</strong>:人类能在“合理的”时间内完成模型的步骤吗？</li><li id="1b43" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nz nn no np bi translated"><strong class="lt iu">可分解性</strong>:模型的各个方面，包括它的特征、参数、权重，都可以分解吗？</li><li id="45b4" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nz nn no np bi translated"><strong class="lt iu">算法透明性</strong>:我们能否理解模型将如何对看不见的数据做出反应。</li></ol><p id="297c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">您可能已经知道符合这个标准的模型；<em class="nv">决策树</em>和<em class="nv">逻辑回归</em>。<strong class="lt iu">只要模型没有使用太多的特征</strong>，两者都满足所有3个标准。</p><blockquote class="ms"><p id="e059" class="mt mu it bd mv mw mx my mz na nb mm dk translated">即使你使用一个可解释的模型，使用太多的特性或者高度工程化的特性也会降低你的模型的可解释性。我们也需要保持我们的数据和预处理是可解释的。</p></blockquote><p id="ce81" class="pw-post-body-paragraph lr ls it lt b lu nc ju lw lx nd jx lz ma ne mc md me nf mg mh mi ng mk ml mm im bi translated">然而，有几个鲜为人知的可解释模型非常适合放在您的工具箱中。这些方法比决策树和逻辑回归更有预测能力，同时仍然保持一定程度的可解释性，允许你在下一个项目中平衡可解释性和准确性。</p><h2 id="d2c9" class="oa la it bd lb ob oc dn lf od oe dp lj ma of og ll me oh oi ln mi oj ok lp ol bi translated">广义线性模型</h2><p id="9052" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这是什么？它是如何工作的？</p><p id="bacc" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">一个<strong class="lt iu"> GLM </strong>实际上只是一种谈论<strong class="lt iu">线性</strong>或<strong class="lt iu">逻辑</strong> <strong class="lt iu">回归</strong>的花哨说法。这里的关键概念是，还有其他的线性模型使这些更加灵活。</p><p id="549a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">GLM有三个组成部分:</p><ol class=""><li id="0690" class="nh ni it lt b lu mn lx mo ma nw me nx mi ny mm nz nn no np bi translated"><strong class="lt iu">线性预测器</strong>:这只是回归方程——变量和一些预测变量的线性组合。</li></ol><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/b6593f5e9c0651fc1b2b5f99e0e13b79.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/format:webp/0*YmczWcYawsCrvrR3.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">线性回归方程。图片作者。</p></figure><p id="4338" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">2.<strong class="lt iu">链接功能</strong>:将变量的线性组合链接到概率分布。在线性回归中，这只是<strong class="lt iu">恒等式</strong>的链接函数。</p><p id="5bdd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">3.<strong class="lt iu">概率分布</strong>:我们的y变量就是这样产生的。在线性回归中，这是一个<strong class="lt iu">正态</strong>分布。</p><p id="107d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">通过改变这些，我们可以得到不同的模型。使用具有<strong class="lt iu">伯努利</strong>分布的<strong class="lt iu"> Logit </strong>链接函数，我们得到了一个逻辑回归。</p><p id="1a26" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">一个不太为人所知的版本是泊松回归，它使用泊松分布。这假设我们的变量线性组合与<em class="nv"> y </em>的<strong class="lt iu">对数</strong>相关。</p><p id="8ebd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">要了解更多关于逻辑回归的知识，请点击这里查看我的文章:</p><div class="on oo gp gr op oq"><a rel="noopener follow" target="_blank" href="/simple-logistic-regression-a9735ed23abd"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd iu gy z fp ov fr fs ow fu fw is bi translated">清理逻辑回归</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">非数学家指南</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">towardsdatascience.com</p></div></div><div class="oz l"><div class="pa l pb pc pd oz pe ks oq"/></div></div></a></div><p id="555b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">为什么可以解释？</strong></p><ul class=""><li id="46fd" class="nh ni it lt b lu mn lx mo ma nw me nx mi ny mm nm nn no np bi translated">因为这只是简单的数学</li></ul><p id="a2fb" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">下图显示了一个简单的一元线性回归方程。我们知道，对于x的任何值，我们都可以用这条线计算出y，我们的目标。我们也知道这条线是如何计算的(通过最小化误差)。</p><p id="61de" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们可以把它放大到我们想要的变量，数学仍然成立。我们只需将各项相加就能产生一个结果。</p><p id="a536" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">逻辑回归或泊松回归更复杂，但核心概念是正确的。我们对变量的线性组合求和。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pf"><img src="../Images/fbda5d56cd373b911cd251ea6db8643b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nZ_AFbQhA0ikPzle2mIVtQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">简单的单变量线性回归。图片作者。</p></figure><ul class=""><li id="9c84" class="nh ni it lt b lu mn lx mo ma nw me nx mi ny mm nm nn no np bi translated"><strong class="lt iu">系数意味着什么</strong></li></ul><p id="e5ae" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在线性回归中，我们的系数是根据目标变量给出的。这使我们能够做出如下声明:</p><blockquote class="pg ph pi"><p id="912b" class="lr ls nv lt b lu mn ju lw lx mo jx lz pj mp mc md pk mq mg mh pl mr mk ml mm im bi translated">增加5000万平方英尺将使我们的房价增加1000英镑</p></blockquote><p id="9d8a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在逻辑回归中，这些是以对数概率表示的，我们可以将其转换成概率。</p><p id="ddcf" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">事实上，这些系数可以转换成人类可以理解的陈述，真正增加了线性模型的解释能力。</p><ul class=""><li id="a70d" class="nh ni it lt b lu mn lx mo ma nw me nx mi ny mm nm nn no np bi translated"><strong class="lt iu">交互必须明确编程</strong></li></ul><p id="b633" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">基于树的模型将计算出变量之间的相互作用，这会增加复杂性。许多可解释性方法没有很好地涵盖交互，并且很难解决。例如</p><blockquote class="pg ph pi"><p id="4d39" class="lr ls nv lt b lu mn ju lw lx mo jx lz pj mp mc md pk mq mg mh pl mr mk ml mm im bi translated">每增加50平方米，我们的房价就会增加1000英镑，直到我们达到200平方米，每50平方米*卧室数量，我们的房价就会增加1000英镑。</p></blockquote><p id="5e1f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这种复杂关系只有在线性模型中才能找到，如果它在特征工程期间在<strong class="lt iu">中被明确地</strong>计算和编程。</p><p id="32d6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">实施</strong></p><ul class=""><li id="00b6" class="nh ni it lt b lu mn lx mo ma nw me nx mi ny mm nm nn no np bi translated">scikit-learn提供了一个GLMs的实现</li><li id="098d" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nm nn no np bi translated"><a class="ae ky" href="https://www.statsmodels.org/stable/glm.html" rel="noopener ugc nofollow" target="_blank"> Statsmodels也提供了一个很好的实现</a></li></ul><h2 id="3896" class="oa la it bd lb ob oc dn lf od oe dp lj ma of og ll me oh oi ln mi oj ok lp ol bi translated">决策树</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/f95127d15e24a9ca9f95af0c857c4a14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1256/format:webp/1*sLDAdAfy9AywX1tGDozelw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用于水果分类的决策树示例。图片作者。</p></figure><p id="87dc" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这是什么？它是如何工作的？</p><p id="0563" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">大多数人应该在一生中的某个时候见过决策树！算法版本使用一些简单的数学来生成“最优”决策树，即最好地分割我们的数据的树。</p><p id="9aa8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">要了解它是如何工作的，请看这里:</p><div class="on oo gp gr op oq"><a rel="noopener follow" target="_blank" href="/decision-trees-ac091793070"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd iu gy z fp ov fr fs ow fu fw is bi translated">决策树的非技术性指南</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">现代的、基于树的模型的构建模块…</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">towardsdatascience.com</p></div></div><div class="oz l"><div class="pn l pb pc pd oz pe ks oq"/></div></div></a></div><p id="3c28" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">为什么可以解释？</strong></p><p id="ebe5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">决策树的伟大之处在于，我们可以真正地<strong class="lt iu">提取整棵树</strong>，并理解为什么模型会对数据集中的任何样本做出预测。</p><p id="8f3f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">一旦树木变得非常大(<em class="nv"> max_depth </em> = 7+)，由于树叶数量的指数增长，人类就很难跟踪它们。然而，在这一点上，我们仍然可以编写一些基本代码来突出显示我们的数据达到其预测的路径，以及测试模型将如何对看不见的数据做出反应。</p><p id="4390" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我发现这是最容易解释的模型之一，因为几乎没有数学知识，而且这个概念在世界其他地方都可以找到。</p><p id="88bd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">实现</strong></p><p id="5fef" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">像往常一样，<a class="ae ky" href="https://scikit-learn.org/stable/modules/tree.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>将是我对这个模型的首选。</p><p id="8bd3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">要绘制树，有许多不同的选项，<a class="ae ky" href="https://mljar.com/blog/visualize-decision-tree/" rel="noopener ugc nofollow" target="_blank">这里有一个很好的列表</a>。</p><h2 id="26f0" class="oa la it bd lb ob oc dn lf od oe dp lj ma of og ll me oh oi ln mi oj ok lp ol bi translated">广义可加模型</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi po"><img src="../Images/680ac544eaed7f533ce9bce55c478284.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/format:webp/0*kIU_iH_UvqoNJIAG.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">GAMs vs GLMs。图片作者。</p></figure><p id="b38d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">它是什么，如何工作？</strong></p><p id="48a1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">广义加性模型(GAMs)是GLM模型的扩展，去掉了一个主要的限制；<strong class="lt iu">我们现在可以对数据中的非线性关系进行建模。</strong></p><p id="f15f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">gam通过使用一系列被称为<strong class="lt iu">样条</strong>的复杂函数来估计每个变量来完成这些工作。我们仍然对我们的变量求和，但是<strong class="lt iu">样条</strong>意味着变量和目标值可以具有非线性关系。</p><p id="778e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">要了解更多关于GAMs的信息，请点击这里:</p><div class="on oo gp gr op oq"><a rel="noopener follow" target="_blank" href="/generalised-additive-models-6dfbedf1350a"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd iu gy z fp ov fr fs ow fu fw is bi translated">什么是广义加性模型？</h2><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">towardsdatascience.com</p></div></div><div class="oz l"><div class="pp l pb pc pd oz pe ks oq"/></div></div></a></div><p id="3ccd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">为什么可以解释？</strong></p><p id="f7e4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">毫无疑问，GAMs比逻辑回归或线性回归更难解释。这个模型要复杂得多，其背后的数学原理也是如此。然而，当你考虑到它们的灵活性时，它们仍然保持一定程度的可解释性，这是一个很好的权衡。</p><ul class=""><li id="f575" class="nh ni it lt b lu mn lx mo ma nw me nx mi ny mm nm nn no np bi translated"><strong class="lt iu">非线性变量的组合？</strong></li></ul><p id="7ee4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">目标变量仍然是所有其他变量的<strong class="lt iu">和一些权重</strong>，我们现在有一个复杂的函数来模拟每个变量。我们仍然可以提取和可视化每个变量的函数，大多数GAM包使用部分依赖图来对所有特性进行提取和可视化。</p><p id="2729" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">交互必须手动编程</strong>，这限制了复杂性。我们还可以大致了解模型在看不见的数据上的表现，因为我们知道每个特征的样条函数。</p><p id="9dfb" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">实施</strong></p><p id="c854" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">根据我的研究，R 中的<a class="ae ky" href="https://cran.r-project.org/web/packages/mgcv/mgcv.pdf" rel="noopener ugc nofollow" target="_blank"> <em class="nv"> mgcv </em>包似乎是最适合GAMs的。但是，我更喜欢Python两个最佳选项是</a><a class="ae ky" href="https://www.statsmodels.org/stable/gam.html" rel="noopener ugc nofollow" target="_blank"> Statsmodels </a>和<a class="ae ky" href="https://pygam.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank"> PyGAM </a>。</p><p id="2d84" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">微软研究院已经开源了他们的<a class="ae ky" href="https://github.com/interpretml/interpret" rel="noopener ugc nofollow" target="_blank"> InterpretML包</a>，其中包括他们的可解释增强机器，他们称之为GAM 2.0，因为它使用具有自动交互术语和梯度增强的GAM来保持可解释性，提高性能，并减少数据科学家深入模型的需要。</p><h2 id="3500" class="oa la it bd lb ob oc dn lf od oe dp lj ma of og ll me oh oi ln mi oj ok lp ol bi translated">单调梯度增强</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pq"><img src="../Images/ffd43c90b413e6a5195809e3b940e925.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*muyYyVbbOTsHWrnUaAeMsA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">单调与非单调关系。图片作者。</p></figure><p id="e946" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">它是什么，如何工作？</strong></p><p id="beb8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">梯度推进模型被认为是表格数据的最佳模型，但由于推进的性质，它们是不可解释的。这些模型可以使用数百棵不同权重的单棵树。他们也倾向于自己计算出交互项，而我们对这些交互项几乎没有透明度。通常使用SHAP或石灰来提高这些模型的可解释性。</p><p id="3c7e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">单调关系是指目标和特征具有线性关系，例如:</p><ul class=""><li id="8d8c" class="nh ni it lt b lu mn lx mo ma nw me nx mi ny mm nm nn no np bi translated">你的身体质量指数<strong class="lt iu">增加</strong>，你患心脏病的风险<strong class="lt iu">增加</strong>。</li><li id="a7e7" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nm nn no np bi translated">你的信用评分<strong class="lt iu">降低</strong>，你获得贷款的可能性<strong class="lt iu">降低</strong>。</li><li id="6d12" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nm nn no np bi translated">降雨量<strong class="lt iu">增加</strong>自行车租赁数量<strong class="lt iu">减少</strong>。</li></ul><p id="5b7c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">线性模型是完全单调的，但是由于梯度增强包括相互作用并且可以模拟非线性关系，<strong class="lt iu">它们通常不产生单调关系。</strong></p><p id="78da" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">XGBoost、LightGBM和Catboost都有一个简单的<strong class="lt iu">超参数</strong>，强制变量具有正或负的单调关系。</p><p id="4799" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">为什么可以解释？</strong></p><p id="4bbc" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">使用单调关系意味着我们可以使用上述语句来解释我们的模型。它导致模型满足算法透明性标准，因为这种关系是固定的。我们还可以将一些现实世界的知识构建到模型中，使其更容易被商务人士理解，从而为生产带来更多变化。</p><p id="e791" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">实现</strong></p><ul class=""><li id="9798" class="nh ni it lt b lu mn lx mo ma nw me nx mi ny mm nm nn no np bi translated"><strong class="lt iu"> XGBoost </strong></li></ul><p id="0c3d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在XGBoost中，我们将<em class="nv"> monotone_constraints </em>参数指定为一个字符串元组(语音标记内的括号),在我们的数据集中每个特征有一个数字，因此“(1，0，-1)”表示特征1、2和3。1为正单调关系，-1为负，0为无关系。</p><pre class="kj kk kl km gt pr ps pt pu aw pv bi"><span id="d937" class="oa la it ps b gy pw px l py pz">import xgboost as xgb</span><span id="6a5b" class="oa la it ps b gy qa px l py pz">params = {'monotone_constraints':'(1,0,-1)'}<br/><br/>model <strong class="ps iu">=</strong> xgb<strong class="ps iu">.</strong>train(params, <br/>                  X_train,<br/>                  num_boost_round <strong class="ps iu">=</strong> 1000,<br/>                  early_stopping_rounds <strong class="ps iu">=</strong> 10)</span></pre><ul class=""><li id="a387" class="nh ni it lt b lu mn lx mo ma nw me nx mi ny mm nm nn no np bi translated"><strong class="lt iu"> LightGBM </strong></li></ul><p id="7df6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">LightGBM大体上与XGBoost相同，但是我们需要以列表而不是字符串/元组的形式传递我们的特性。LightGBM还为方法提供了一个额外的参数。使用这种方法，我们可以选择模型试图坚持约束的力度。</p><p id="cfd3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><a class="ae ky" href="https://lightgbm.readthedocs.io/en/latest/Parameters.html#monotone_constraints_method" rel="noopener ugc nofollow" target="_blank">文件规定</a>:</p><blockquote class="pg ph pi"><p id="ed66" class="lr ls nv lt b lu mn ju lw lx mo jx lz pj mp mc md pk mq mg mh pl mr mk ml mm im bi translated"><code class="fe qb qc qd ps b">basic</code>，最基本的单调约束法。它根本不会降低库的速度，但会过度限制预测</p><p id="3dd3" class="lr ls nv lt b lu mn ju lw lx mo jx lz pj mp mc md pk mq mg mh pl mr mk ml mm im bi translated"><code class="fe qb qc qd ps b">intermediate</code>，一个<a class="ae ky" href="https://hal.archives-ouvertes.fr/hal-02862802/document" rel="noopener ugc nofollow" target="_blank">更高级的方法</a>，可能会稍微减慢库的速度。但是，这种方法比基本方法限制少得多，应该可以显著改善结果</p><p id="e33e" class="lr ls nv lt b lu mn ju lw lx mo jx lz pj mp mc md pk mq mg mh pl mr mk ml mm im bi translated"><code class="fe qb qc qd ps b">advanced</code>，一个<a class="ae ky" href="https://hal.archives-ouvertes.fr/hal-02862802/document" rel="noopener ugc nofollow" target="_blank">更高级的方法</a>，可能会拖慢库。然而，这种方法甚至比中间方法更少约束，并且应该再次显著改善结果</p></blockquote><pre class="kj kk kl km gt pr ps pt pu aw pv bi"><span id="4780" class="oa la it ps b gy pw px l py pz">import lightgbm as lgb</span><span id="96d6" class="oa la it ps b gy qa px l py pz">params = {'monotone_constraints': [-1, 0, 1],<br/>          '<!-- -->monotone_constraints_method<!-- -->':'basic'}<br/><br/>model = lgb.train(params, <br/>                  X_train, <br/>                  num_round = 1000, <br/>                  early_stopping_rounds = 10)</span></pre><ul class=""><li id="e40f" class="nh ni it lt b lu mn lx mo ma nw me nx mi ny mm nm nn no np bi translated"><strong class="lt iu"> Catboost </strong></li></ul><p id="31df" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">Catboost与其他的非常相似，但提供了更多的灵活性，因为我们可以将约束作为数组传递，使用切片并显式命名一个特性。</p><p id="83bb" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这个参数叫做<code class="fe qb qc qd ps b">monotone_constraints</code>，你可以在这里查看<a class="ae ky" href="https://catboost.ai/docs/search/?query=monotone+constraints" rel="noopener ugc nofollow" target="_blank">的Catboost文档。</a></p><h2 id="8e7f" class="oa la it bd lb ob oc dn lf od oe dp lj ma of og ll me oh oi ln mi oj ok lp ol bi translated">TabNet</h2><p id="56cf" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这是什么？它是如何工作的？</p><p id="6e96" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><a class="ae ky" href="https://arxiv.org/abs/1908.07442" rel="noopener ugc nofollow" target="_blank"> TabNet </a>由谷歌大脑研究人员于2019年发表。传统上，当处理表格数据时，神经网络方法在梯度提升方面没有显著改进。然而，Tabnet能够<strong class="lt iu">在各种基准测试中超越领先的基于树的模型</strong>。它比增强树模型更容易解释，因为它有<strong class="lt iu">内置的解释能力</strong>。也可以不用任何特征预处理使用<strong class="lt iu">。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qe"><img src="../Images/de007dbdc5ec51d6fdde08d83b04d6d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_2Ysp7B2AiM8ocSSUZRNRA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">TabNet模型架构。图片作者。灵感来自<a class="ae ky" href="https://arxiv.org/pdf/1908.07442.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1908.07442.pdf</a>。</p></figure><p id="c0ed" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">我在TabNet上的文章更详细地介绍了这个模型，请看这里:</strong></p><div class="on oo gp gr op oq"><a rel="noopener follow" target="_blank" href="/tabnet-e1b979907694"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd iu gy z fp ov fr fs ow fu fw is bi translated">TabNet:梯度推进的终结？</h2><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">towardsdatascience.com</p></div></div><div class="oz l"><div class="qf l pb pc pd oz pe ks oq"/></div></div></a></div><p id="a265" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">为什么可以解释？</strong></p><p id="82da" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">TabNet使用顺序注意机制来选择最重要的特征，这影响了掩盖最不重要特征的“面具”。我们可以使用该掩膜的权重来了解哪些特征比其他特征使用得更频繁，这实质上允许我们了解模型使用哪些特征来进行预测。</p><p id="7c81" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">要素选择是在数据集的行级别执行的，这意味着我们实际上可以探索为单个预测选择了哪些要素。掩模的数量是模型的超参数</p><p id="14d7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">实施</strong></p><p id="d8d3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">使用<strong class="lt iu"> TabNet </strong>的最佳方式是使用<a class="ae ky" href="https://github.com/dreamquark-ai/tabnet" rel="noopener ugc nofollow" target="_blank"> Dreamquark的PyTorch实现</a>。它使用scikit-learn风格的包装器，并且与GPU兼容。Dreamquark还提供了一些非常棒的笔记本，它们完美地展示了如何实现TabNet，同时也验证了原作者关于模型在某些基准上的准确性的声明。</p><p id="5916" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">分类</strong></p><div class="on oo gp gr op oq"><a href="https://github.com/dreamquark-ai/tabnet/blob/develop/census_example.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd iu gy z fp ov fr fs ow fu fw is bi translated">dreamquark-ai/tabnet</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">PyTorch实现TabNet论文:https://arxiv.org/pdf/1908.07442.pdf—dream quark-ai/TabNet</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">github.com</p></div></div><div class="oz l"><div class="qg l pb pc pd oz pe ks oq"/></div></div></a></div><p id="f1ae" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">回归</strong></p><div class="on oo gp gr op oq"><a href="https://github.com/dreamquark-ai/tabnet/blob/develop/regression_example.ipynb" rel="noopener  ugc nofollow" target="_blank"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd iu gy z fp ov fr fs ow fu fw is bi translated">dreamquark-ai/tabnet</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">PyTorch实现TabNet论文:https://arxiv.org/pdf/1908.07442.pdf—dream quark-ai/TabNet</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">github.com</p></div></div><div class="oz l"><div class="qh l pb pc pd oz pe ks oq"/></div></div></a></div><h1 id="f8b3" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">比较模型</h1><p id="ec68" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">让我们回到利普顿的3个标准，并应用于每个模型。提醒一下，标准是…</p><ol class=""><li id="c4aa" class="nh ni it lt b lu mn lx mo ma nw me nx mi ny mm nz nn no np bi translated">模拟能力:人类能在“合理的”时间内完成模型的步骤吗？</li><li id="41df" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nz nn no np bi translated"><strong class="lt iu">可分解性</strong>:模型的各个方面，包括它的特征、参数、权重都可以分解吗？</li><li id="8274" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nz nn no np bi translated"><strong class="lt iu">算法透明性</strong>:我们能否理解模型将如何对看不见的数据做出反应。</li></ol><p id="5683" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们还要考虑<strong class="lt iu">局部可解释性</strong>；根据模型使用了哪些特征以及在何种程度上使用了每个特征来做出决策，模型可以在多大程度上做出单个预测。</p><p id="e24b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我在下面的图表中总结了这一点，根据每个标准对每个模型进行了低、中或高评分。这不是一门精确的科学，但是你可以考虑每个分数与线性回归的关系。</p><p id="27ab" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">记住，任何特性工程都可能完全打乱这些分数。</strong>从神经网络中创建复杂的交互项、数学变换或特征可能会提高准确性，但肯定会降低可解释性。更多的特性也会降低可解释性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qi"><img src="../Images/cc02f547af6170bf2cbaa7527d6906b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CtPl8yU8rUZtvBjxV-zuCA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">根据每个模型的可解释性标准评分。图片作者。</p></figure><h1 id="9379" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="bc88" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">围绕模型可解释性的驱动力和讨论只会增加。随着人工智能被用于语言建模、面部识别和无人驾驶汽车，拥有能够在决策背后进行推理的模型比以往任何时候都更加重要。</p><p id="048e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">在你的下一个项目中尝试这些可解释的模型中的一个，让我知道结果如何。</strong></p><h2 id="0eb4" class="oa la it bd lb ob oc dn lf od oe dp lj ma of og ll me oh oi ln mi oj ok lp ol bi translated">了解更多信息</h2><div class="on oo gp gr op oq"><a rel="noopener follow" target="_blank" href="/mlops-at-home-part1-4c60db29d4a2"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd iu gy z fp ov fr fs ow fu fw is bi translated">在家创建一个现代的、开源的MLOps堆栈</h2><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">towardsdatascience.com</p></div></div><div class="oz l"><div class="qj l pb pc pd oz pe ks oq"/></div></div></a></div><h2 id="f48c" class="oa la it bd lb ob oc dn lf od oe dp lj ma of og ll me oh oi ln mi oj ok lp ol bi translated">将我的内容直接发送到您的收件箱！</h2><div class="on oo gp gr op oq"><a href="https://adamsh.substack.com/p/coming-soon" rel="noopener  ugc nofollow" target="_blank"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd iu gy z fp ov fr fs ow fu fw is bi translated">Data Siens:机器学习技巧、诀窍和教程的资源。</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">欢迎来到亚当的数据西恩斯。我是一名数据科学家，有5年的分析经验。我喜欢谈论任何事情…</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">adamsh.substack.com</p></div></div><div class="oz l"><div class="qk l pb pc pd oz pe ks oq"/></div></div></a></div></div></div>    
</body>
</html>