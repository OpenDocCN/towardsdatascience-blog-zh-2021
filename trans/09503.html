<html>
<head>
<title>Deep Q learning is no rocket science</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度Q学习不是火箭科学</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-q-learning-is-no-rocket-science-e34912f1864?source=collection_archive---------18-----------------------#2021-09-03">https://towardsdatascience.com/deep-q-learning-is-no-rocket-science-e34912f1864?source=collection_archive---------18-----------------------#2021-09-03</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1563" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用pytorch解释和编码的深度Q和双Q学习</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/e40884ae78ce92bc26216bff1cb7c55f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*SJqQ3gJxMVW92m0X-bcbdQ.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(作者GIF)</p></figure><h1 id="db49" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">情境中的深度Q学习</h1><p id="4ca2" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">q学习是强化学习社区中早已存在的一种方法。然而，最近通过使用<a class="ae mi" rel="noopener" target="_blank" href="/backpropagation-in-neural-networks-6561e1268da8">神经网络</a>与Q学习相结合，在该领域取得了巨大进展。这就是所谓的深度Q学习的诞生。这种方法的全部潜力在2013年被看到，当时谷歌向世界展示了他们的DQN代理商在玩雅达利突破。对我来说，这是我第一次接触这个领域，我立刻对它产生了兴趣。</p><p id="ca6c" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">其他强化学习方法有<a class="ae mi" rel="noopener" target="_blank" href="/snake-with-policy-gradients-deep-reinforcement-learning-5e6e921db054">策略梯度法</a>和行动者批评法。行动者批评方法是Q学习和政策梯度方法的混合。</p><h1 id="78fe" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">一般强化学习</h1><p id="8039" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">在强化学习中，我们有一个环境，在这个环境中，一个代理正在做动作。环境然后返回一个奖励和一个新的状态。代理人得到的报酬也取决于他实际所处的状态。所以他/她应该学会根据实际情况调整行动。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi mo"><img src="../Images/bff92c4c49309d0dfbdf79a6ae0a2c22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W8A2YMGl3GkLtcIG6XebLA.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">作者图片</p></figure><p id="5150" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">我们要用的环境是CartPole-v0环境，作者是OpenAI。我们可以在python中轻松使用它。这是一个非常简单的环境，对于大多数深度强化学习算法来说，求解起来也非常简单。因此，每当我写一个新的强化学习脚本时，我都会用它来测试我是否正确地实现了所有的东西。</p><h1 id="044e" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">南极环境</h1><p id="1e76" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">我们从导入和测试openai gym开始。如果您还没有安装它，您可以通过键入:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="f1a2" class="my kv it mu b gy mz na l nb nc">pip3 install gym</span></pre><p id="2a9d" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">然后，我们编写一个脚本来测试我们的健身房环境。我们让代理执行随机的动作，只是为了看看是否一切正常。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nd ne l"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(作者代码)</p></figure><p id="1d96" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">代理的目标是尽可能长时间地平衡操纵杆。如果棍子在两个方向偏离垂直方向15度，它就失败了，这一集就结束了。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/e0c5e0a1d18291bda25c436f28382667.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*dO8JKz5f0GM8LRl0BhEEPw.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(图片由作者提供)</p></figure><p id="a0dd" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">代理为实现其目标可以采取的<strong class="lo iu">行动</strong>有:</p><ul class=""><li id="c772" class="nf ng it lo b lp mj ls mk lv nh lz ni md nj mh nk nl nm nn bi translated">向左移动(0)</li><li id="41d2" class="nf ng it lo b lp no ls np lv nq lz nr md ns mh nk nl nm nn bi translated">向右移动(1)</li></ul><p id="8016" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><strong class="lo iu">奖励</strong>以如下方式给出:</p><ul class=""><li id="0235" class="nf ng it lo b lp mj ls mk lv nh lz ni md nj mh nk nl nm nn bi translated">每走一步+1，棍子是直立的</li></ul><p id="7389" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><strong class="lo iu">状态/观察值</strong>是4个值的列表:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/ea9514766dda0384d8a2f313d014fc38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1290/format:webp/1*eOzzbBeHClpA8eXsGlMUIQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(图片由作者提供)</p></figure><h1 id="abb1" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">深度Q学习</h1><h2 id="8059" class="my kv it bd kw nu nv dn la nw nx dp le lv ny nz lg lz oa ob li md oc od lk oe bi translated">蜕变能</h2><p id="8df0" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">考虑下面的场景。我们的鸡饿了。它可以执行两个操作。它可以吃也可以写。因为它饿了，更好的选择是吃东西，这确实会给它更多的奖励。但是奖励是在行动完成后给予的，所以它怎么会知道。在这种情况下，动作值(或Q值)可以帮助我们。这个Q值取决于状态和可能的动作。给定状态，Q(s，a)然后返回执行动作的值。所以鸡应该执行q值最高的动作！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi of"><img src="../Images/4327f41a283a7a7371fbf2e018bdd74f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LDcQ0OU9Z147US3TT3QYFg.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(图片由作者提供)</p></figure><p id="9f8c" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">Q值由下式定义:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/f7e40aacd505759eef79d877c18ce624.png" data-original-src="https://miro.medium.com/v2/resize:fit:1124/format:webp/1*xDgQ0sXNExa3YoYkkGFECQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(图片由作者提供)</p></figure><p id="4e1f" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">这个等式简单地意味着，对于一个给定的状态，一个行为的价值是这个行为执行后的直接回报加上下一个状态的(贴现的)最高Q值。伽马(𝛾)实际上是贴现因子，它解释了未来回报的不确定性。𝛾通常大于0.9，小于1。低gamma导致代理更关注眼前的回报，而高gamma导致代理更关注未来的高回报。</p><h2 id="99b8" class="my kv it bd kw nu nv dn la nw nx dp le lv ny nz lg lz oa ob li md oc od lk oe bi translated">时间差异学习</h2><p id="3456" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">我们已经看到了Q值的精确定义，我也告诉过你，我们将使用神经网络来逼近它。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi oh"><img src="../Images/25c23acc26e94782673098b86805f999.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G-Jr8b9gXGCkrEG01n5Diw.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(图片由作者提供)</p></figure><p id="0cda" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">因此，我们希望使网络的近似Q值尽可能接近数学定义。为了实现这个目标，我们使用神经网络的能力来最小化损失函数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/ed841f00946e46f494e1c69cbe0e5c78.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/format:webp/1*jJ-v2JKLx2eZEqd8qXmGOA.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(图片由作者提供)</p></figure><p id="1625" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">我们把实际时间步长的Q值称为“预测”，把包括下一个状态目标的Q值的项称为“预测”。</p><p id="a85d" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">我们使用的损失函数就是均方误差损失。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi oj"><img src="../Images/36c6e12896e385052c073e922a008d0a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BX70Y7VAFMHvxf7S2gyyxA.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(图片由作者提供)</p></figure><p id="af66" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">利用时间差学习，我们可以在每个时间步训练我们的代理。</p><h2 id="f6fb" class="my kv it bd kw nu nv dn la nw nx dp le lv ny nz lg lz oa ob li md oc od lk oe bi translated">勘探开发的困境</h2><p id="dacc" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">如果我们让我们的代理贪婪地利用它认为具有最高Q值的策略，很可能存在更好的策略。代理人将永远无法探索这一策略，因此永远看不到与之相关的高回报。所以在开始的时候，我们让自己做随机的行为，随着时间的推移，减少它采取随机行为的可能性。这个过程被称为𝜖贪婪行动选择。</p><h2 id="5188" class="my kv it bd kw nu nv dn la nw nx dp le lv ny nz lg lz oa ob li md oc od lk oe bi translated">体验回放</h2><p id="1f16" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">深度Q学习不是特别好，除非我们通过添加经验回放来增强它。为此，我们建立了一个记忆，把所有的状态、行为和奖励都储存在里面。训练时，我们从记忆中检索随机的一批，并在其上进行训练。这意味着代理很可能没有接受过上一个时间步骤的培训。</p><p id="9baf" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">下面的代码实现了一个ReplayBuffer类，它负责存储和分配随机批次的内存。对于我们存储的每个时间步长:(state，action，reward，next_state，done)。done是一个结束标志，在一集期间为0，在一集结束时为1。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nd ne l"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(作者代码)</p></figure><h2 id="eb13" class="my kv it bd kw nu nv dn la nw nx dp le lv ny nz lg lz oa ob li md oc od lk oe bi translated">整个算法和代码</h2><p id="9f15" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">我决定向您展示主文件的python代码，而不是伪代码，因为如果您了解python，这更容易理解。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nd ne l"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(作者代码)</p></figure><p id="fab5" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">我们可以调整许多超参数，我们将它们传递给代理的构造函数:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="c68c" class="my kv it mu b gy mz na l nb nc">agent = DQAgent(learning_rate=0.00025, gamma=0.90, batch_size=32, <br/>                     state_len=len(env.reset()), <br/>                     n_actions = env.action_space.n,<br/>                     mem_size = 1000000,<br/>                     min_memory_for_training=1000,<br/>                      epsilon=1,epsilon_dec=0.99,<br/>                     epsilon_min = 0.02)</span></pre><p id="59f5" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><strong class="lo iu">学习率:</strong>神经网络的学习率。高值会使代理学习得更快，但也会导致在复杂环境中的次优行为。</p><p id="cd6e" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><strong class="lo iu">伽马:</strong> 𝛾通常在0.9以上，小于1。低gamma导致代理更关注眼前的回报，而高gamma导致代理更关注未来的高回报。</p><p id="6433" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><strong class="lo iu">批次大小:</strong>给神经网络执行小批次梯度下降的时间步数。高批次数量稳定了学习，但也可能导致停滞。</p><p id="5b0c" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><strong class="lo iu">训练的最小记忆:</strong>学习过程开始前的步数。一个合适的数字可以防止网络过度适应前几个训练样本。</p><p id="31f6" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><strong class="lo iu">ε:</strong>ε的起始值。1表示代理在开始时只执行随机操作(探索), 0表示它不会开始执行随机操作(利用)。</p><p id="369e" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><strong class="lo iu">Epsilon dec:</strong>Epsilon在每个时间步长后乘以的系数，以减少它。应该大于0.99小于1。</p><p id="4e35" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><strong class="lo iu">ε最小值:</strong>ε的最小值。如果ε低于该值，它将不会减小。任何时候都要有一点点的探索，让代理不断学习，这一点很重要。</p><p id="aca7" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">现在我们代理的全部代码是:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nd ne l"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(作者代码)</p></figure><p id="af88" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">让我们来讨论一下这段代码:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="0111" class="my kv it mu b gy mz na l nb nc">target = rewards_batch + torch.mul(self.gamma* self.q(new_states_batch).max(axis = 1).values, (1 - dones_batch))</span></pre><p id="7b76" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">它计算TD目标。假设我们的批量是5。</p><p id="bee3" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">那么计算可能如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi ok"><img src="../Images/a0d89e40253a2f6e2aeac71fd0c7f531.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*16cq38fJlaP3CHgVfgBTDQ.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(图片由作者提供)</p></figure><p id="d264" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">我们做max运算，因为我们想看Q值最高的动作。</p><p id="74f9" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">让我们看看下面的代码行:</p><pre class="kj kk kl km gt mt mu mv mw aw mx bi"><span id="7dd8" class="my kv it mu b gy mz na l nb nc">prediction = self.q.forward(states_batch).gather(1,actions_batch.unsqueeze(1)).squeeze(1)</span></pre><p id="c7d0" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">它计算预测。批量大小为5时，可能如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi ol"><img src="../Images/089eae86491cecb3e3d0f0783557ceec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8rFb0PLzKb5s_TCR-SJQ3w.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(图片由作者提供)</p></figure><p id="f0b6" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">我们从代理实际采取的行动中选择q值。</p><h2 id="c6b6" class="my kv it bd kw nu nv dn la nw nx dp le lv ny nz lg lz oa ob li md oc od lk oe bi translated">测试</h2><p id="47b7" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">现在是时候实际测试我们的算法了。我们用我们的CartPole-v0环境测试它。</p><p id="ebc5" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">在100集之后，代理人设法击败了环境，并且几乎总是获得最高分。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/440bd30c5072d6e0579590e6ecef111b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*Uzdg3yH6u73WwyiYi5pjHw.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(图片由作者提供)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/ce887a001c1af6959d8c110fab608669.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*FPC8VuWSF9h9sP9oKYFQqg.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(图片由作者提供)</p></figure><h1 id="18ae" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">双Q学习</h1><p id="2b75" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">为了进一步改进我们的深度Q学习算法，我们可以选择具有单独的预测值和目标值。这也叫双Q学习。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi on"><img src="../Images/51e15cc9bb6245600b61b21420b96e33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TmLgNdZYI1S3tj0PG1IE6w.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(图片由作者提供)</p></figure><p id="9d40" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">这些动作将通过预测网络进行预测。反向传播只发生在预测网络中。预测网络的参数每隔几次迭代就被复制到目标网络。目标网络的参数保持“冻结”多久也是一个超参数。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="mp mq di mr bf ms"><div class="gh gi oo"><img src="../Images/4b1589d63fa872147cf4752a32198ec8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oK5KnZSPki8wUlUxAjd3NQ.png"/></div></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(图片由作者提供)</p></figure><p id="ecb2" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">我们冻结了目标网络，因为这样预测和目标的相关性就降低了。这有助于学习过程</p><h2 id="c6d4" class="my kv it bd kw nu nv dn la nw nx dp le lv ny nz lg lz oa ob li md oc od lk oe bi translated">测试</h2><p id="2420" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">现在我们有一点乐趣，用openai环境“LunarLander-v2”测试我们的算法。你可以在下面找到我用过的超参数列表。</p><p id="e16d" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><strong class="lo iu">学习率:</strong> 0.001</p><p id="61f7" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><strong class="lo iu">伽玛:</strong> 0.99</p><p id="dcfa" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><strong class="lo iu">批量:</strong> 64</p><p id="0ca5" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><strong class="lo iu">训练最小内存:</strong> 1000000</p><p id="12b7" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><strong class="lo iu">ε:</strong>1</p><p id="181f" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><strong class="lo iu">εdec:</strong>0.995</p><p id="0fe1" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><strong class="lo iu">ε最小值:</strong> 0.02</p><p id="c7f0" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated"><strong class="lo iu">冻结_迭代:</strong> 6</p><p id="1004" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">以下GIF拍摄于729集。代理人在着陆方面做得很好。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/e40884ae78ce92bc26216bff1cb7c55f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*SJqQ3gJxMVW92m0X-bcbdQ.gif"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(作者GIF)</p></figure><p id="4a00" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">学习曲线是这样的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/3c21942b410a34331ef19a364eff9d4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*oJMLfB_OwPz5OBearAmP5A.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(图片由作者提供)</p></figure><p id="e3df" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">请注意，它被20集移动平均线平滑。</p><h2 id="600e" class="my kv it bd kw nu nv dn la nw nx dp le lv ny nz lg lz oa ob li md oc od lk oe bi translated">密码</h2><p id="e73c" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">主文件:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nd ne l"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(作者代码)</p></figure><p id="4dbe" class="pw-post-body-paragraph lm ln it lo b lp mj ju lr ls mk jx lu lv ml lx ly lz mm mb mc md mn mf mg mh im bi translated">代理的代码:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nd ne l"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">(作者代码)</p></figure><h1 id="6ed9" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">作者相关文章</h1><div class="op oq gp gr or os"><a rel="noopener follow" target="_blank" href="/snake-with-policy-gradients-deep-reinforcement-learning-5e6e921db054"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd iu gy z fp ox fr fs oy fu fw is bi translated">具有策略梯度的Snake深度强化学习</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">策略梯度深度强化学习在蛇游戏中的应用</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">towardsdatascience.com</p></div></div><div class="pb l"><div class="pc l pd pe pf pb pg ko os"/></div></div></a></div><div class="op oq gp gr or os"><a rel="noopener follow" target="_blank" href="/backpropagation-in-neural-networks-6561e1268da8"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd iu gy z fp ox fr fs oy fu fw is bi translated">神经网络中的反向传播</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">从零开始的神经网络，包括数学和python代码</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">towardsdatascience.com</p></div></div><div class="pb l"><div class="ph l pd pe pf pb pg ko os"/></div></div></a></div><div class="op oq gp gr or os"><a rel="noopener follow" target="_blank" href="/how-you-can-use-gpt-j-9c4299dd8526"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd iu gy z fp ox fr fs oy fu fw is bi translated">如何使用GPT J</h2><div class="oz l"><h3 class="bd b gy z fp ox fr fs oy fu fw dk translated">GPT J解释了3种简单的方法，你可以如何访问它</h3></div><div class="pa l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">towardsdatascience.com</p></div></div><div class="pb l"><div class="pi l pd pe pf pb pg ko os"/></div></div></a></div><h2 id="da3f" class="my kv it bd kw nu nv dn la nw nx dp le lv ny nz lg lz oa ob li md oc od lk oe bi translated">想联系支持我？</h2><p id="f5c5" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">领英<br/><a class="ae mi" href="https://www.linkedin.com/in/vincent-m%C3%BCller-6b3542214/" rel="noopener ugc nofollow" target="_blank">https://www.linkedin.com/in/vincent-m%C3%BCller-6b3542214/</a><br/>脸书<br/><a class="ae mi" href="https://www.facebook.com/profile.php?id=100072095823739" rel="noopener ugc nofollow" target="_blank">https://www.facebook.com/profile.php?id=100072095823739</a><br/>推特<br/><a class="ae mi" href="https://twitter.com/Vincent02770108" rel="noopener ugc nofollow" target="_blank">https://twitter.com/Vincent02770108</a><br/>中等<br/><a class="ae mi" href="https://medium.com/@Vincent.Mueller" rel="noopener">https://medium.com/@Vincent.Mueller</a><br/>成为中等会员并支持我(你的部分会费直接归我)<br/><a class="ae mi" href="https://medium.com/@Vincent.Mueller/membership" rel="noopener">https://medium.com/@Vincent.Mueller/membership</a></p></div></div>    
</body>
</html>