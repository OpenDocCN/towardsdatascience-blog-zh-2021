<html>
<head>
<title>Autoencoder network optimization for dimensionality reduction</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用于降维的自动编码器网络优化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/autoencoder-network-optimization-for-dimensionality-reduction-67922ccc6889?source=collection_archive---------21-----------------------#2021-04-19">https://towardsdatascience.com/autoencoder-network-optimization-for-dimensionality-reduction-67922ccc6889?source=collection_archive---------21-----------------------#2021-04-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f367" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何优化隐藏层数和大小？</h2></div><p id="b593" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在<a class="ae le" rel="noopener" target="_blank" href="/dimensionality-reduction-with-autoencoders-versus-pca-f47666f80743">之前的文章</a>中，我们看到了如何通过使用线性激活函数和“mse”作为损失度量，利用自动编码器网络(AE)模拟PCA降维。</p><p id="7aaf" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在一个合成数据集上，我们比较了基于分类分数和潜在变量的降维性能。我们看到了如何通过修改网络(添加更多层，堆叠自动编码器)或最终允许激活函数为非线性来改善结果。然而，这些变化是任意的。在这篇文章中，我想进行同样的基本分析，但这一次使用一种随机搜索的方法来寻找最佳的网络结构。我们将通过使用方便的KerasRegressor包装器来实现这一点</p><pre class="lf lg lh li gt lj lk ll lm aw ln bi"><span id="ce96" class="lo lp it lk b gy lq lr l ls lt">from tensorflow.keras.wrappers.scikit_learn import KerasRegressor</span></pre><p id="b4af" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">与前一篇文章不同的是，我们将从一个新的数据集开始:我们将使用一个严格非线性的数据集，它由来自sk learn . datasets:make _ friendman 2的4个变量组成。为了增加维度，我们也将使用sklearn的多项式特征(order=2，interactions_only=True)。这可能会稍微减轻主成分分析的工作，但我们会将11个变量的系统减少到3个。</p><p id="e116" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在第一部分中，我们将使用浅网络的性能作为起点，就像我们在上一篇文章中所做的那样。在第二部分，我们将优化参数。第一个尝试是优化隐藏层的数量和形状(最多4层)。</p><figure class="lf lg lh li gt lv gh gi paragraph-image"><div class="gh gi lu"><img src="../Images/ceaff1596ef2491d75246ed99337c19f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/format:webp/1*erRQ_J315VSt-PZ7nxiiXA.jpeg"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">自动编码器网络结构。图片作者。</p></figure></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h1 id="84dd" class="mj lp it bd mk ml mm mn mo mp mq mr ms jz mt ka mu kc mv kd mw kf mx kg my mz bi translated">基线性能</h1><p id="a81c" class="pw-post-body-paragraph ki kj it kk b kl na ju kn ko nb jx kq kr nc kt ku kv nd kx ky kz ne lb lc ld im bi translated">为了理解优化效果如何，我们将比较自动编码器和PCA(主成分分析)的维数减少。<br/>我们建立了包含4个特征的数据集。我们引入了多项式特征来增加维数，然后我们对所有这些特征应用标准缩放。最后，我们将把数据集的维数减少到3。</p><p id="ae82" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">主成分分析解释了该数据集约77%的方差。</p><figure class="lf lg lh li gt lv gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/4877db5dc57c753e8eaffebea29fd804.png" data-original-src="https://miro.medium.com/v2/resize:fit:1312/format:webp/1*TxWaJYOf95qmSif0rdff2g.png"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">PCA和编码器前3个组件。颜色显示要预测的值。</p></figure><p id="fb13" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用具有<em class="ng">最大深度3 </em>的<em class="ng"> RandomForestRegressor </em>，我们在具有原始4个特征的数据集上获得了0.92的回归分数。在具有多项式和缩放要素的数据集上为0.98。<br/>使用具有3个成分的PCA，我们得到0.67的回归分数。使用简单的线性自动编码器，1级a分数约为0.7。使用简单的堆叠AE:10–10–3，我们可以达到0.77。</p><p id="7ffb" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从这些数据中我们已经了解到，构成结果y的非线性特征强烈地影响着潜在变量的预测能力。自动编码器已经比PCA执行得更好，我们仍然使用线性激活函数。</p><figure class="lf lg lh li gt lv gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/dc1284e1eae90c82ed31f62c9e95c2fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*uuiHa0yHEnKnzh7mfh7Xzw.png"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">对不同结构的回归结果进行评分。</p></figure><h1 id="b959" class="mj lp it bd mk ml ni mn mo mp nj mr ms jz nk ka mu kc nl kd mw kf nm kg my mz bi translated">优化层数</h1><p id="da55" class="pw-post-body-paragraph ki kj it kk b kl na ju kn ko nb jx kq kr nc kt ku kv nd kx ky kz ne lb lc ld im bi translated">我们现在要优化堆栈编码器。我们通过导入一个新的库来实现:KerasRegressor。它将允许我们使用scikit-learn中的RandomizedSearchCV。</p><pre class="lf lg lh li gt lj lk ll lm aw ln bi"><span id="aad6" class="lo lp it lk b gy lq lr l ls lt">from tensorflow.keras.wrappers.scikit_learn import KerasRegressor</span></pre><p id="0995" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要生成层的组合，有几种方法。直截了当的方法是用简单的列表理解一些条件句。最终我们可以决定定义一个函数，它的形状与我们数据的输入形状相关联。</p><pre class="lf lg lh li gt lj lk ll lm aw ln bi"><span id="ae02" class="lo lp it lk b gy lq lr l ls lt">output_dim = 3<br/>sizes = [10,10,5,5,0]<br/>shapes = [(i,j,k,q,output_dim) for i in sizes for j in sizes for k in sizes for q in sizes if (i&gt;=j&gt;=2*k&gt;=output_dim)]</span></pre><p id="9712" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">以下是从前面的列表理解中输出的一些形状:正如你在代码中注意到的，我们确保最终的维度总是我们的潜在空间维度。</p><pre class="lf lg lh li gt lj lk ll lm aw ln bi"><span id="85ae" class="lo lp it lk b gy lq lr l ls lt">(10, 10, 5, 10, 3),<br/> (10, 10, 5, 10, 3),<br/> (10, 10, 5, 5, 3),<br/> (10, 10, 5, 5, 3),<br/> (10, 10, 5, 0, 3),<br/> (10, 10, 5, 10, 3),<br/> (10, 10, 5, 10, 3),<br/> (10, 10, 5, 5, 3),<br/> (10, 10, 5, 5, 3),<br/> (10, 10, 5, 0, 3),<br/> (10, 10, 5, 10, 3),<br/> (10, 10, 5, 10, 3), ... ]</span></pre><p id="c90b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">层结构现在变成如下图所示:</p><figure class="lf lg lh li gt lv gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/942168b45de3be4c2688f2b31ed9d340.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*F6SvTusidPokCdXzegjO_g.jpeg"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">将被优化的堆叠编码器结构。图片作者。</p></figure></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><p id="1b5b" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们定义一个函数，它将使用我们刚刚定义的图层大小作为参数。我们将像上一篇文章一样构建编码器，但这次我们也将解码器的重量与编码器的重量联系起来。这样，我们将减少可训练参数，搜索将会更快。最后，它将连接两个模型，就像我们之前在堆栈自动编码器中所做的那样。</p><figure class="lf lg lh li gt lv"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="f486" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">既然我们已经定义了构建自动编码器的函数，我们就定义了参数搜索网格，并利用RandomizedSearchCV功能来执行搜索。</p><p id="aad9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，我们正在寻找最能减少重建损失的网络结构。我们不是在寻找最佳的网络参数来改善我们的回归得分问题！</p><figure class="lf lg lh li gt lv"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="681f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">经过一番搜索，最佳结果似乎是(8，8，5，3)层网络。尽管如此，分数都是可比的。看起来至少有一些额外的层会有所帮助，但不会太复杂。</p><figure class="lf lg lh li gt lv gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/3f0ee84b5cb0676d6425dea78c2118b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*hWXsgI_tY6H4rVO2Tmex9Q.png"/></div><p class="ly lz gj gh gi ma mb bd b be z dk translated">不同网络的平均测试分数。</p></figure><p id="8a0a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">结果报告如下。左边是到目前为止我们考虑的所有结果。右边是从0.6开始的(奇怪的)柱状图。即使在这种规模下，搜索的好处也不明显。</p><div class="lf lg lh li gt ab cb"><figure class="nr lv ns nt nu nv nw paragraph-image"><img src="../Images/84272867d4886444de2a2310ece90ffb.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*fcDnz6GC7966n3JBsCrEUA.png"/></figure><figure class="nr lv nx nt nu nv nw paragraph-image"><img src="../Images/1300adb9037d9103abeedf02f6530e4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*SWheQqDnp6Rk9qNuChf6uA.png"/></figure></div><h1 id="77c5" class="mj lp it bd mk ml ni mn mo mp nj mr ms jz nk ka mu kc nl kd mw kf nm kg my mz bi translated">结论</h1><p id="1be4" class="pw-post-body-paragraph ki kj it kk b kl na ju kn ko nb jx kq kr nc kt ku kv nd kx ky kz ne lb lc ld im bi translated">在这篇短文中，我们看到了如何构建一个用于降维的自动编码器，并将其与上一篇文章中的PCA等标准算法进行比较，但使用的是强非线性数据集。在这种情况下，我们做了一个回归问题。</p><p id="7df8" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们试图通过使用多项式特征来帮助PCA更好地执行，但是与原始数据集结果相比，信息损失仍然很大。一个简单的欠完整自动编码器已经开始比PCA表现得更好，增加层数和将解码器的权重与编码器的权重联系起来会使它甚至更好。</p><p id="2449" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们在scikit-learn RandomizedSearchCV和GridSearchCV中使用了Keras序列模型的包装器。在这个数据集上没有观察到更大的改进。很可能，由于原始维度只有11，搜索效率并不高。</p><h1 id="f133" class="mj lp it bd mk ml ni mn mo mp nj mr ms jz nk ka mu kc nl kd mw kf nm kg my mz bi translated">文献学</h1><p id="5baa" class="pw-post-body-paragraph ki kj it kk b kl na ju kn ko nb jx kq kr nc kt ku kv nd kx ky kz ne lb lc ld im bi translated">[1] A. Géron，用Scikit-Learn进行机器学习的实践，Keras &amp; TensorFlow(第二版)。真的吗？<br/> [2] A. Gulli，A. Kapoor，S. Pal，深度学习用TensorFlow 2和Keras。(第二版)。打包。<br/>【3】<a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_friedman1.html#sklearn.datasets.make_friedman1" rel="noopener ugc nofollow" target="_blank">sk learn . datasets . make _ Friedman 1—scikit-learn 0 . 24 . 1文档(scikit-learn.org)</a><br/>【4】<a class="ae le" rel="noopener" target="_blank" href="/dimensionality-reduction-with-autoencoders-versus-pca-f47666f80743">用自动编码器降维</a></p></div></div>    
</body>
</html>