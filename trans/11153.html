<html>
<head>
<title>Leveraging N-grams to Extract Context From Text</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">利用N元语法从文本中提取上下文</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/leveraging-n-grams-to-extract-context-from-text-bdc576b47049?source=collection_archive---------10-----------------------#2021-11-01">https://towardsdatascience.com/leveraging-n-grams-to-extract-context-from-text-bdc576b47049?source=collection_archive---------10-----------------------#2021-11-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="a4f7" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">简单介绍一个基本的NLP概念</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/d9349a8f530f1279fa3afd4f80a4767a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t7lx34vYoJ8lAeNU6un7_g.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://www.pexels.com/@agk42?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">亚历山大·奈特</a>从<a class="ae kv" href="https://www.pexels.com/photo/high-angle-photo-of-robot-2599244/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">派克斯</a>拍摄</p></figure><p id="c27b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">人们在学习处理文本数据时，难免会碰到n元文法。它们通常在让机器理解给定文本的上下文方面起着关键作用。</p><p id="23e1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，这个术语在无数的数据科学项目中被提出。</p><p id="3bf4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，不要将n元语法视为可以简单掩盖的行话，重要的是花时间了解这个概念的来龙去脉，因为它将作为理解更高级的自然语言处理工具和技术的基础。</p><p id="785b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">那么，什么是n-gram呢？</p><p id="4c24" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">简单来说，N元文法是指N个单词或字符的序列。</p><h1 id="14da" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">例子</h1><p id="6614" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">我们来考虑一下这句话:“我住在纽约”。</p><p id="d628" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一个单字模型(n=1)将此文本存储在一个单词的标记中:</p><p id="0519" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">【“我”、“住”、“在”、“新”、“纽约”】</p><p id="6846" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">二元模型(n=2)将该文本存储在两个单词的标记中:</p><p id="a17b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[“我住”，“住在”，“在纽约”，“纽约”]</p><p id="3632" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这个场景中，城市“New York”不会被识别为具有unigram的实体，因为每个标记只存储一个单词。另一方面，二元模型将单词“New”和“York”连接起来，并允许机器将“New York”识别为单个实体，从而从文本中提取上下文。</p><h1 id="ba93" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">应用程序</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mp"><img src="../Images/184662860a4bfe192214f73d61d5c5d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oWiRWtwKsaR9zHeBiRZX0g.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://www.pexels.com/@romanp?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">罗曼·波霍雷基</a>从<a class="ae kv" href="https://www.pexels.com/photo/person-sitting-inside-car-with-black-android-smartphone-turned-on-230554/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank">派克斯</a>拍摄</p></figure><p id="c1fc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">n元语法在自然语言处理中无处不在。</p><p id="25c4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">想想你的信使或搜索引擎中的文本建议功能，你已经学会想当然。想想垃圾邮件检测器或仇恨言论检测器，它们让你的社交媒体体验更加愉快。这些特性都依赖于n-grams来实现它们所熟知的可靠性。</p><p id="67bb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">也就是说，没有一个特定的n-gram模型胜过所有其他模型。在NLP模型中利用n-grams的最佳方式只能通过实验来确定。</p><h1 id="5d15" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">个案研究</h1><p id="8505" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">让我们做一个案例研究来展示n-grams对模型性能的影响。</p><p id="3863" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这里，我们将使用一个包含视频游戏评论(此处<a class="ae kv" href="https://www.kaggle.com/jp797498e/twitter-entity-sentiment-analysis" rel="noopener ugc nofollow" target="_blank">可访问</a>)及其相应情感得分的数据集进行一个简单的情感分析。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mq mr l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">正在加载数据集</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/a06b11b30fffa9d3be0188f795368dfd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*-tNrLltHF-U6L842-jkBwQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代码输出(图片由作者提供)</p></figure><p id="b081" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在任何预测建模之前，首先清理数据。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mq mr l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">数据预处理</p></figure><p id="62eb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于这个案例研究，在用于训练机器学习分类器之前，文本将通过sklearn模块中的<a class="ae kv" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html" rel="noopener ugc nofollow" target="_blank">计数矢量器</a>对象转换为一袋单词。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mq mr l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">一袋单字</p></figure><p id="fa7d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意:“ngram_range”参数指的是将包含在单词包中的文本的n元语法的范围。n-gram范围(1，1)意味着单词包将只包括一元词。</p><p id="a3f6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们看看一个朴素贝叶斯模型是如何用n元语法范围(1，1)预测评论的情绪的。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mq mr l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用Unigrams训练模型</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/5ef1f2f8ea96e2a990e83c8c176f7a89.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*DUxvR2IRXjOjMltIGu1fvg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代码输出(图片由作者提供)</p></figure><p id="f238" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">朴素贝叶斯分类器的f1值为0.752。</p><p id="2ccb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一般来说，单词袋模型是一种非常简单的单词矢量化方法，并且具有一定的局限性。主要是，单词在收集时没有排序，所以文本中的许多上下文丢失了。</p><p id="5fb2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这种情况下，使用n-gram可以通过建立某种顺序来保存上下文，从而帮助解决这个问题。</p><p id="6855" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们看看在将二元模型(n=2)添加到输入要素后，模型的表现如何。这可以通过将“ngrams_range”参数更改为(1，2)来实现。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mq mr l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">用单字和双字训练模型</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/757436d52b37821d7aeb6ccf98b13c06.png" data-original-src="https://miro.medium.com/v2/resize:fit:442/format:webp/1*UA7P4R1PjuUT4w1zg-qUXQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代码输出(图片由作者提供)</p></figure><p id="f6c5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在纳入二元模型后，该模型获得了更高的f-1分数。这可以归因于当机器输入两个单词的序列而不仅仅是单个单词时，它获得了更大的上下文。</p><p id="a20b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">话虽如此，当涉及到n-grams时，越多不一定越好。在某些情况下，拥有太多的特征会导致模型不太理想。</p><p id="82bd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以通过展示该模型预测不同n元语法范围的给定文本的情感有多准确来证明这一点。</p><p id="2979" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里，用训练集构建了具有不同n元语法范围的10个模型，并用测试集进行了评估。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mq mr l"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">训练具有10个N-gram范围的模型</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/6b49ca23b472ae12bd9e4a3a500dc881.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*5WZJWhd3QRb3YeHujrlhaw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代码输出(图片由作者提供)</p></figure><p id="7344" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">基于这些结果，该模型在n元语法范围为(1，5)时表现最佳。这意味着使用从一元到五元的n元模型训练模型有助于获得最佳结果，但更大的n元模型只会导致更稀疏的输入特征，这会影响模型的性能。</p><h1 id="ef29" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结论</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mv"><img src="../Images/23ae83587790241b8d160421a89a9cb2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iEGV0LbyKJ-xY4rBoSIi-A.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由来自<a class="ae kv" href="https://www.pexels.com/photo/black-and-white-laptop-2740956/?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> Pexels </a>的<a class="ae kv" href="https://www.pexels.com/@prateekkatyal?utm_content=attributionCopyText&amp;utm_medium=referral&amp;utm_source=pexels" rel="noopener ugc nofollow" target="_blank"> Prateek Katyal </a>拍摄</p></figure><p id="b0bc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">n元语法是自然语言处理中一个简单但重要的概念。考虑到需要从文本中提取洞察力的各种应用，n元文法无疑将在许多机器学习项目中发挥重要作用。</p><p id="0f6f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这就是为什么深入理解n元语法及其对模型性能的影响至关重要。利用这个工具只会提高您的模型的能力。</p><p id="2db8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我祝你在机器学习的努力中好运！</p></div></div>    
</body>
</html>