<html>
<head>
<title>Explainable AI: An illuminator in the field of black-box machine learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可解释的人工智能:黑盒机器学习领域的一个亮点</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/explainable-ai-an-illuminator-in-the-field-of-black-box-machine-learning-62d805d54a7a?source=collection_archive---------18-----------------------#2021-11-29">https://towardsdatascience.com/explainable-ai-an-illuminator-in-the-field-of-black-box-machine-learning-62d805d54a7a?source=collection_archive---------18-----------------------#2021-11-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9593" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">不同的机器学习可解释性工具如何用于解释的概述</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/956795d11b9adad48e4f7bc29b6af5d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LmkfehIFDf9BywZcusL_9Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">威廉·戴尼奥在<a class="ae kv" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="00d8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">随着机器学习领域的发展，许多新的复杂机器学习模型被广泛应用于各个领域。特别是，随着深度学习的进步，数据正被用于做出一些关键决策。但有时，即使是人工智能专家也很难解释所谓的“黑箱模型”所做的某些预测。当涉及到医疗保健、自动驾驶汽车等高风险领域时，了解这些信息变得非常重要</p><ol class=""><li id="cf8b" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">我们的模型学习是什么？</li><li id="ee49" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">模型的哪些部分负责做出某些预测？</li><li id="e17c" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">模型是否稳健？</li></ol><p id="84ad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">不同的模型可解释性技术有助于回答这些问题。在本文中，我分享了一些常用的可解释性工具的概述。</p><p id="633f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mg">让我们让可解释性工具对所有人都是可解释的😀。</em></p><p id="de39" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在开始之前，最好先了解一些本文中会用到几次的术语。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/285e6793fd8120103a127b30d306f7e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*cgSDzjTNcNEjhOdDbGT5BA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="2c37" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">解释特定预测的最佳方式是，是否有任何可解释的机器学习技术可用于决策。可解释的ML技术的一些例子是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mi"><img src="../Images/44abf15d25beee0f1ae75a5c92a816cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fAkQap-33MiwbJ5patzekA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">一些可解释的ML技术的例子[ <a class="ae kv" href="https://christophm.github.io/interpretable-ml-book/simple.html" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="fc7d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">广义线性模型(GLM)家族的所有成员都非常容易解释。在本文中，我不会深入研究可解释的ML算法。要知道GLM是如何工作的，去看看我的另一篇<a class="ae kv" rel="noopener" target="_blank" href="/what-is-so-general-about-generalized-linear-model-15dde9be2640">文章。</a></p><div class="mj mk gp gr ml mm"><a rel="noopener follow" target="_blank" href="/what-is-so-general-about-generalized-linear-model-15dde9be2640"><div class="mn ab fo"><div class="mo ab mp cl cj mq"><h2 class="bd ir gy z fp mr fr fs ms fu fw ip bi translated">广义线性模型有什么好概括的？</h2><div class="mt l"><h3 class="bd b gy z fp mr fr fs ms fu fw dk translated">线性和非线性数据回归模型的一般形式</h3></div><div class="mu l"><p class="bd b dl z fp mr fr fs ms fu fw dk translated">towardsdatascience.com</p></div></div><div class="mv l"><div class="mw l mx my mz mv na kp mm"/></div></div></a></div></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><p id="4ce1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">排列特征重要性:</strong></p><p id="60e8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">特征重要性为每个特征给出一个分数，它告诉我们这些特征中的哪些对我们的模型是重要的，这些特征中的哪些在驱动我们的模型预测中起着至关重要的作用。存在一些特定于模型的特征重要性度量，例如对于GLMs，由特征的标准偏差缩放的参数系数作为特征重要性。类似地，对于基于树的模型，节点中分裂的杂质减少给出了特征重要性的度量。</p><p id="6189" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是置换特征重要性是模型不可知的特征重要性度量之一，其中我们通过置换其值来计算特征的重要性。</p><p id="4906" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果预测误差增加，则我们可以说该特定特征是重要的。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/74a9aca462bef06c6ec413c72935300d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wkEuTGyA5YahbkqFg_x_FQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">排列特征重要性示例[图片由作者提供]</p></figure><p id="3a94" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">特征重要性计算如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/c4bb9a87591e6a88b415380f5d02b044.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-Z7bHDFrJvnbG7hpjxz3SA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用排列特征重要性的特征重要性[图片由作者提供]</p></figure><p id="9bc1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果Fᵢⱼ的值大于1，则可以认为某个要素是重要的。一个特性的值的排列可以用不同的方式完成，特别是n！因此，置换特征重要性给出了作为输出的置信区间。</p></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><p id="2fa1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">部分相关图(PDP): </strong></p><p id="7449" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">部分相关图是一种全局方法，它考虑了所有数据实例，并给出了预测因子/特征和结果变量之间的全局关系。</p><p id="afe3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">PDP计算一个/两个特征对结果的边际影响。它没有捕捉到特征之间的相互作用及其对预测结果的影响。</p><p id="f535" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了绘制PDP，我们有两组功能:</p><ul class=""><li id="cd07" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr nk ly lz ma bi translated">我们要为其绘制PDP的功能</li><li id="5417" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr nk ly lz ma bi translated">机器学习模型中使用的其他特征。</li></ul><p id="6d2c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为数字特征绘制PDP的机制与为分类特征绘制PDP的机制略有不同。</p><p id="cedd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了绘制数字特征X₁的PDP，</p><ol class=""><li id="2283" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">在原始数据上拟合机器学习模型<strong class="ky ir"> f </strong>。</li><li id="7aa8" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">为了获得特定数量的PDP，比如说X₁₁的特征X₁，通过将所有数据实例的值改变为X₁₁来创建人工数据。</li><li id="31f4" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">使用已经拟合的机器学习模型对每个数据实例进行预测。</li><li id="3969" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">要素X₁的特定X₁₁量的PDP值将是步骤3中完成的所有预测的平均值。</li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/281de8f00c94f70d30bb46e7533e0d77.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hzmUjKCt0bmRlWD_DGULow.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">特性特定值的PDP图片由作者提供]</p></figure><p id="ef43" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">5.遵循上述过程来覆盖X₁的整个区间，并给出X₁的PDP图。</p><p id="a0b1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">类似地，为了绘制分类特征的PDP，通过用任何一个可能的类别改变特征的值来创建人工数据，并且对该特征的所有类别继续该过程。数字和分类特征的PDP分别如下所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/5c48a8f0fefcad84e84cced30c2bdf21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ea1slBP-Ge9XC6ALsubcqQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">数字特性和分类特性的PDP[<a class="ae kv" href="https://christophm.github.io/interpretable-ml-book/pdp.html" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="316a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以，我们已经学会了如何绘制PDP。但是，通过观察这些图，我们能对特性的重要性说些什么呢？</p><p id="2abb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">主要的想法是，平板PDP告诉我们，功能并不那么重要。</p><p id="7268" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mg">PDP的变化越多，特性就越重要。</em></p><p id="50e8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于数字特征，特征重要性的值通过以下公式计算，该公式基本上是该特征的PDP值的标准偏差。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/44c4be76c17bb229360f68834a5970cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BGWsLI_jtiw1ysKEjmesKw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">数字特征的特征重要性[ <a class="ae kv" href="https://christophm.github.io/interpretable-ml-book/pdp.html" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="2973" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">类似地，对于分类特征，重要性分数通过以下方式计算:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/3ea680b57d7b4a9f821f423e3899e965.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6uUKzexerDqCASNm4kDKoA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">分类特征的特征重要性[ <a class="ae kv" href="https://christophm.github.io/interpretable-ml-book/pdp.html" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><p id="ee77" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">什么是代孕模型？</strong></p><p id="81d5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">代理模型是一种可解释的模型，其训练方式使得它能够以最大的解释近似底层黑盒模型的预测。它使用黑盒模型做出的预测作为输入，并尝试拟合一个可以近似黑盒模型的可解释模型(在上一节中讨论过)。</p><ul class=""><li id="7d63" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr nk ly lz ma bi translated">如果R值很高，我们可以说代理模型是底层黑盒模型的近似。我们可以通过以下方式计算R平方值:</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/3e58b921a425cfb79eda6c7b8fab92ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*EGiuoX7jc8vRLmXRax5tZw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">代理模型的r平方值[图片由作者提供]</p></figure><ul class=""><li id="379f" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr nk ly lz ma bi translated">训练代理模型是一种模型不可知的方法，因为它不需要关于底层模型的任何信息，所需要的只是每个数据实例的特征值和底层模型的预测。</li><li id="0981" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr nk ly lz ma bi translated">全局代理模型是整个黑盒模型的近似。</li><li id="420a" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr nk ly lz ma bi translated">局部代理模型用于通过底层黑盒模型来近似特定实例的预测。</li></ul></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><p id="de40" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> LIME(本地可解释模型不可知解释):</strong></p><p id="4431" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">LIME使用局部代理模型来对单个预测进行具体解释，而不是关注整个模型的解释。</p><p id="2f20" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">LIME的假设是，我们可以任意多次探索黑盒模型。此外，LIME认为任何算法都是黑盒(即使我们将LIME应用于线性回归，它也会假设线性回归是黑盒)。</p><p id="780d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们的目标是知道为什么黑箱模型做出了某种预测。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/21dacaac274fcd8d77f28defbc79ad76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eRbt7tB87C016Mkw81QFKg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">用于解释的感兴趣的数据点[图片由作者提供]</p></figure><p id="e4f6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从上面的图片中，我们可以看到存在一个复杂的决策边界，这是两个类别的分类。但是，我们想知道为什么模型为突出显示的数据实例做出某个决定，哪些特性对做出这样的决定更负责。</p><p id="1f64" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为此，我们通过扰动来自原始数据集的样本并由黑盒对这些扰动样本进行预测来创建新数据集。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/455f84e6685190dbd6905f438319051c.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*BsO48VDq5FIQtblkSoNsCQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">为LIME创建的新数据集[图片由作者提供]</p></figure><p id="63f5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">创建的扰动样本根据它们与感兴趣的数据点的接近程度进行加权。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/aa1bc2819f61250ae636742b8a8acbef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VqIKFqD8gvEnhoENHM7vuQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">感兴趣的数据点附近的加权样本[图片由作者提供]</p></figure><p id="5427" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从上图中我们可以看到，靠近感兴趣实例的样本被赋予较高的权重(圆圈越大，权重越大)，而远离感兴趣点的样本被赋予较低的权重。</p><p id="a039" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这个新的数据集上训练本地代理模型。这个本地代理模型应该是上面列出的可解释模型家族中的任何模型。</p><p id="9ace" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于数据实例(x)的解释，我们想要最小化损失L，该损失L通过保持g的复杂度低(对于线性回归较少特征；对于决策树，树的深度更小)。这是g的复杂性和f的近似性之间的权衡。这种权衡被称为<strong class="ky ir">保真度-可解释性权衡</strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/ea8c1641fa3529f3e9d3fe2e109eff30.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5xZk-1i7rFo_T4mMnBT4eA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">石灰的优化功能[图片由作者提供]</p></figure><p id="d776" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中G是可解释模型族。</p><p id="0c86" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在训练时，记住给予扰动样本的局部可解释模型权重。</p><p id="00e8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mg">我们正在x的邻域中搜索复杂模型的最佳近似模型</em></p><p id="57bd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">指数平滑核用于定义x的邻域:较小的核宽度意味着样本必须非常接近感兴趣的点才能影响局部拟合的模型。我不会说太多细节。更多细节你可以查看这本<a class="ae kv" href="https://christophm.github.io/interpretable-ml-book/" rel="noopener ugc nofollow" target="_blank">书</a>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/03e0a9c5845f90ef2938476566839987.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*boh-wJu62Y1y72CneTxkow.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用局部代理模型解释感兴趣的数据点[图片由作者提供]</p></figure><p id="84ee" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从上面的图像中，你可以看到一个局部可解释的线性模型被拟合。一般这类线性模型本质上是稀疏的(<a class="ae kv" rel="noopener" target="_blank" href="/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b">套索回归</a>由<a class="ns nt ep" href="https://medium.com/u/9a3c3c477239?source=post_page-----62d805d54a7a--------------------------------" rel="noopener" target="_blank"> Saptashwa Bhattacharyya </a>精彩解释)。通过解释这个局部模型，我们可以对复杂模型对感兴趣点所做的预测做出某些解释。</p><p id="004b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们已经了解石灰的工作原理，但这还不够。让我们看看如何解释一个LIME输出。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/f6317cb4892e4608a6dfba443e046a47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jQesiVuhMW7ANFnsW7NbPQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">特定数据点的石灰输出[ <a class="ae kv" href="https://github.com/marcotcr/lime" rel="noopener ugc nofollow" target="_blank">源</a></p></figure><p id="710b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设我们想要解释一个表格数据的特定数据实例的石灰输出，该表格数据有两个类别“可食用”和“有毒”。在这里，我们希望了解特定数据实例的不同特征的值如何影响预测。橙色高亮支持有毒，蓝色支持食用。局部拟合模型预测该数据实例有毒，概率得分为1。上图中间部分给出的权重是局部拟合模型的参数。这里我们可以看到“气味=恶臭”这个值是增加被归类为有毒的机会的最重要的值，而“鳃大小=宽”是唯一减少被归类为有毒的机会的值。</p><p id="ada9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要了解LIME如何处理图像和文本数据，请查看此<a class="ae kv" href="https://github.com/marcotcr/lime" rel="noopener ugc nofollow" target="_blank">源</a>。</p></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><p id="3e93" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> SHAP(沙普利附加解释):</strong></p><p id="f011" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">沙普利加法解释或SHAP是基于博弈论中沙普利值的概念。SHAP背后的主要思想是知道每个单独的特征对做出某个预测有多大的贡献。这可能很难计算单个的贡献，因为在特征之间可能存在相互作用。</p><p id="fefb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">SHAP是既可用于本地解释又可用于全球解释的度量之一。</p><p id="4f73" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mg">所以，我们理解为什么沙普利和为什么解释但为什么加法？？</em></p><p id="a69f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们看看。</p><p id="2231" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Shapley值使特定数据点的输出不同于基础模型的基线输出。假设φᵢⱼ是特征j的iᵗʰ数据实例的沙普利值。这些值或正或负地使输出不同于基线输出。因为Shapley值对基线输出的影响的增加，所以它是“相加的”。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/293cc8c33f5f2ebb1af86cebdba98818.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bEldWjZC_cTpfvSHkJXObw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">SHAP解说[图片由作者提供]</p></figure><p id="6662" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们看看这些φᵢⱼ是如何计算的。</p><p id="a089" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">iᵗʰ数据实例的jᵗʰ要素的沙普利值φᵢⱼ按以下方式计算</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/f524b555cc7a5b790b1debbe32d630cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3f0G4jLEw7Sww4liG2LP7w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">iᵗʰ数据实例的jᵗʰ特征的沙普利值计算公式[图片由作者提供]</p></figure><p id="31e2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在哪里，</p><p id="5c07" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">s是特征的子集。</p><p id="b146" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">f是底层黑盒模型。</p><p id="7661" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">f(SU{j})是具有感兴趣特征的子集S的黑盒模型的预测。</p><p id="d3b6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">f(S)是没有感兴趣特征的子集S的黑盒模型的预测。</p><p id="7ce9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设我们总共有p个特征。因此，每个S都是p-1个特征的子集。</p><p id="cfda" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如，假设我们要计算φ₁₁，那么s就是绿色高亮显示的特征值的子集。在这里，我们的目标是了解特性的特定值是如何影响输出的。我们考虑不同的组合来捕捉特征之间的交互作用。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/5c9266fc33a182b635ba71824b710d73.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gdlnaVs_Tp_dwzg-k7saSQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">用于计算Shapley值的子集选择[图片由作者提供]</p></figure><p id="7b65" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据子集s中存在多少特征来给出加权项。其背后的直觉是，如果子集s中已经包括了许多特征，则对添加jᵗʰ特征的贡献给予更大的加权。这表明即使已经包括了许多其他特征，jᵗʰ特征也会对预测产生强烈的改变。</p><p id="f50e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们看看如何解释特定数据点的SHAP输出。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/713b4a204762c50e636881e8fd4c4e89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pW_NocaFgTL9vg8V_oDKkQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">特定数据点的SHAP输出[ <a class="ae kv" href="https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="fad2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于这个特定的例子，底层黑盒模型的基线输出是22.841，这个特定数据点的输出是16.178。红色高亮显示的条表示有助于输出向基线输出移动的特征值，蓝色条表示有助于向相反方向移动的特征值。</p><p id="39a3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要知道SHAP如何帮助解释全球，我们必须知道SHAP功能的重要性和SHAP依赖图。</p><p id="68c6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> SHAP特征重要性:</strong></p><p id="6f2d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mg"> </em> jᵗʰ <em class="mg"> </em>特征的沙普利值按以下方式计算。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/d6271fdc4173992c8aee74bc4ecb66ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/1*66b-gHxrxJLjknCQlclJ1g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用Shapley值计算全局特征重要性的公式[图片由作者提供]</p></figure><p id="20fb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">特征重要性值越高，该特征越重要。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nz"><img src="../Images/b0050571a16553f50378a0e083490160.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y9YXl4vOBIl2Hr6v5zNp2g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">全球特征重要性图[ <a class="ae kv" href="https://christophm.github.io/interpretable-ml-book/shap.html" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="857a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于这个特殊的例子，使用激素避孕药的年数是最重要的特征，x轴上的值代表这些特征对产出的平均影响的绝对量。</p><p id="a197" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> SHAP依赖情节:</strong></p><p id="927f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">SHAP依赖图可以被认为是PDP的替代方案。使用SHAP依赖图，我们还可以推断出特征之间的相互作用对模型预测的影响，这对于PDP是做不到的。它基本上是通过绘制x轴上的特征值和y轴上相应的Shapley值来创建的。</p><p id="4e14" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于jᵗʰ特征形状依赖图是通过绘图创建的</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/befa045f5ce6eb23630871b5af1f4a1b.png" data-original-src="https://miro.medium.com/v2/resize:fit:480/format:webp/1*2d8VyFwfkp0GVCCd3rOUMg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">绘制SHAP依赖图的数据[图片由作者提供]</p></figure><p id="31f7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们来看看它的样子，以及如何解读它。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oa"><img src="../Images/d3c25a0c3194f45bae8c0010df368427.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ppwsICZFSvmMZomXz5Eqvw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">SHAP依赖情节[ <a class="ae kv" href="https://slundberg.github.io/shap/notebooks/plots/dependence_plot.html" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="dd81" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于这个例子，当年龄在20到40岁之间时，这在模型预测中起着至关重要的作用，在此之后，年龄对模型输出的影响相当稳定。年龄与另一个特征----教育---- Num的交互影响也可以从该图中得到解释，因为它显示了在接近20岁时，较少的教育比高等教育对产出的影响更大。</p><p id="514f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="mg">注:</em>为了更好地理解这些方法并尽可能清晰地解释结果，本文中使用的图是从不同来源收集的。</p></div><div class="ab cl nb nc hu nd" role="separator"><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng nh"/><span class="ne bw bk nf ng"/></div><div class="ij ik il im in"><p id="fa25" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">结论</strong></p><p id="e7c8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文讨论的所有可解释性工具中，SHAP是最常用的，因为它的粒度以及它在本地和全局解释中的实现。虽然与其他技术相比，它需要一点额外的时间。因此，在使用任何可解释性工具之前，知道解释的目的是什么是非常重要的。</p><p id="5828" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">如果你喜欢这篇文章请点击推荐。那太不可思议了。</strong></p><p id="8286" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">关注我在<a class="ae kv" href="https://medium.com/@ayan.kundu09" rel="noopener"> Medium </a>和<a class="ae kv" href="https://www.linkedin.com/in/ayan-kundu-a86293149/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上的博客帖子和更新。</p><p id="8e80" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">参考文献</strong></p><ol class=""><li id="46ee" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">C.莫尔纳尔。<em class="mg">可解释的机器学习</em>。<a class="ae kv" href="https://christophm.github.io/interpretable-ml-book/" rel="noopener ugc nofollow" target="_blank">https://christophm.github.io/interpretable-ml-book/</a></li><li id="bae4" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">https://shap.readthedocs.io/en/latest/index.html SHAP文件<a class="ae kv" href="https://shap.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank"/></li><li id="611f" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated">米（meter的缩写））t .里贝罗，s .辛格，c .盖斯特林。“我为什么要相信你？”解释任何分类器的预测。<a class="ae kv" href="https://arxiv.org/pdf/1602.04938v3.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1602.04938v3.pdf</a></li></ol><p id="7a29" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们把它包起来。非常感谢你的阅读和快乐学习。T25】</p></div></div>    
</body>
</html>