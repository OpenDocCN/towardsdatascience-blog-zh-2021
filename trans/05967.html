<html>
<head>
<title>What’s the Maximum ROC You Can Achieve? (No, the Answer Is Not 1)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你能达到的最大ROC是多少？(不，答案不是1)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/whats-the-maximum-roc-you-can-achieve-no-the-answer-is-not-1-da11b66c0f28?source=collection_archive---------23-----------------------#2021-05-28">https://towardsdatascience.com/whats-the-maximum-roc-you-can-achieve-no-the-answer-is-not-1-da11b66c0f28?source=collection_archive---------23-----------------------#2021-05-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="3257" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">无论是准确度、精度，还是ROC下面积，大概你的预测模型都无法达到完美(甚至无法接近)。这是(统计上的)原因。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/62325b41483ec4fedd51a28df069194b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_8FYb-hVyCw065ov_tvkQg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">[图片由作者提供]</p></figure><p id="fd03" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">人们普遍认为预测模型的准确性完全取决于两个因素:</p><ul class=""><li id="93a6" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">你的<strong class="la iu">数据</strong>有多完整整洁；</li><li id="709c" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">你的<strong class="la iu">分类器</strong>有多强大。</li></ul><p id="675d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当然，这两个方面是最基本的。但是还有第三种，经常被遗忘:</p><ul class=""><li id="7782" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">你正在解决的<strong class="la iu">问题</strong>有多难？</li></ul><p id="f716" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">直觉上，很明显不是所有的问题都一样难。例如，对MNIST数字进行分类并不像预测明天的股票市场那样困难。换句话说，<strong class="la iu">每个问题都有一个固有的“硬度”</strong>。这对数据科学家有着重要的意义，因为实际上，它意味着:</p><blockquote class="mi"><p id="2240" class="mj mk it bd ml mm mn mo mp mq mr lt dk translated">在给定任务上可以实现的最大ROC不一定是100%,但是可以低得多，这取决于问题的难度。</p></blockquote><p id="725c" class="pw-post-body-paragraph ky kz it la b lb ms ju ld le mt jx lg lh mu lj lk ll mv ln lo lp mw lr ls lt im bi translated">这只是常识，但在某个时候，我开始问自己:</p><blockquote class="mi"><p id="bf9f" class="mj mk it bd ml mm mn mo mp mq mr lt dk translated"><strong class="ak">分类任务的难度有没有量化的衡量标准</strong>？</p></blockquote><p id="bb15" class="pw-post-body-paragraph ky kz it la b lb ms ju ld le mt jx lg lh mu lj lk ll mv ln lo lp mw lr ls lt im bi translated">我发现这个问题发人深省，因为它触及了一个有趣话题的本质:<strong class="la iu">机器学习的边界，以及——我要说——一般学习的边界</strong>。这篇文章介绍了我是如何回答这个问题的。</p><h1 id="6674" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">1.让我们摇摆吧</h1><p id="0530" class="pw-post-body-paragraph ky kz it la b lb np ju ld le nq jx lg lh nr lj lk ll ns ln lo lp nt lr ls lt im bi translated">想想任何分类任务，无论是预测保险欺诈、分类垃圾邮件还是推荐电影。</p><p id="6612" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">假设我们能够为该任务生成所有可能的预测模型(这个假设的模型集被称为“模型空间”)。对于它们中的每一个，我们也可以计算其在ROC曲线下的<a class="ae nu" href="https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc" rel="noopener ugc nofollow" target="_blank">面积(从现在开始简称为“ROC”)。</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/b0bf8feb3f56b4923ac1dd3a959bde3c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*4FR6_z5dCgSSy6zr6lMv6g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">模型空间。[图片由作者提供]</p></figure><p id="c1ce" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">正如我们所知，ROC是衡量模型性能的一个标准。因此，如果我们采用模型空间中具有最高ROC的模型，那么这就是正在讨论的任务的<strong class="la iu">最佳可能模型</strong>。</p><p id="84d9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">事实上，</p><blockquote class="mi"><p id="bd0a" class="mj mk it bd ml mm mn mo mp mq mr lt dk translated">最佳可能模型的ROC是任务“难度”的度量。</p></blockquote><p id="e648" class="pw-post-body-paragraph ky kz it la b lb ms ju ld le mt jx lg lh mu lj lk ll mv ln lo lp mw lr ls lt im bi translated">事实上，根据定义，最好的模型是不可战胜的。因此，<strong class="la iu">最佳可能模型的ROC越高，问题越简单；最佳可能模型的ROC越低，问题越难</strong>。</p><p id="3d45" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所以，我们所有的工作归结为找到某种方法来定义最佳可能模型的特征。要做到这一点，让我们从基础开始。</p><h1 id="f92f" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">2.最好的模型</h1><p id="4b26" class="pw-post-body-paragraph ky kz it la b lb np ju ld le nq jx lg lh nr lj lk ll ns ln lo lp nt lr ls lt im bi translated">任何分类任务的结构都可以概括如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/14814482b097f235a6d41092ee9a3847.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R1pAwg_vbt426i-wcjoTKw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一个二元分类问题的结构。[图片由作者提供]</p></figure><p id="25f9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个想法是，目标变量是一些“隐藏的”(或不可观察的)概率的实现。<strong class="la iu">预测模型的目的是对这些潜在的概率进行估计(尽可能准确)</strong>。</p><p id="d27b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是隐藏的概率和观察到的结果之间有什么联系呢？如果所有样本都是独立的，生成观察结果就像为每个样本投掷一枚(装载的)硬币，硬币的装载与样本的隐藏概率成比例。在Python中，投掷一枚装满硬币的等价体是<code class="fe nx ny nz oa b">numpy.random.choice</code>，所以生成过程看起来是这样的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/19a713fdf6a69afbc519200831743cdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nZC7DqsJhn44NIt59CNzQA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据生成过程的样子。[图片由作者提供]</p></figure><p id="c164" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">隐藏概率的随机性是不可分散的。这意味着<strong class="la iu">没有模型能比隐藏概率</strong>做得更好。换句话说，</p><blockquote class="mi"><p id="2a4f" class="mj mk it bd ml mm mn mo mp mq mr lt dk translated">最好的可能模型是能够正确猜测所有隐藏概率的模型。</p></blockquote><p id="4a6a" class="pw-post-body-paragraph ky kz it la b lb ms ju ld le mt jx lg lh mu lj lk ll mv ln lo lp mw lr ls lt im bi translated">我们终于有了我们一直在寻找的最佳可能模型的定义！由于我们最初的直觉是计算最佳可能模型的ROC，这相当于计算隐藏概率的ROC。</p><p id="8df0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">综合我们发现的一切，我们可以得出这样的结论:</p><blockquote class="mi"><p id="ed75" class="mj mk it bd ml mm mn mo mp mq mr lt dk translated">在预测问题上可能达到的最高ROC是产生目标变量的概率的ROC。</p></blockquote><p id="bca4" class="pw-post-body-paragraph ky kz it la b lb ms ju ld le mt jx lg lh mu lj lk ll mv ln lo lp mw lr ls lt im bi translated">根据这个定义，现在很容易计算出我们上面看到的玩具问题的最高可实现ROC:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/9c0e15be003b60b5b06ffed91eb990b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9XQyf-KJFbyy1Be32wYsmw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用Python计算ROC分数。[图片由作者提供]</p></figure><p id="eb7d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">只有77%，离100%还差得很远！</p><p id="17cf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">除了这个小例子，我们能不能用这个洞见说点更普遍的？为了回答这个问题，我们需要后退一步，尝试在更一般的背景下理解隐藏概率的特征。</p><h1 id="eb02" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">3.隐藏概率的拼图</h1><p id="9104" class="pw-post-body-paragraph ky kz it la b lb np ju ld le nq jx lg lh nr lj lk ll ns ln lo lp nt lr ls lt im bi translated">考虑到隐藏概率是不可观测的，我们对它们了解多少？</p><p id="75ea" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">至少，<strong class="la iu">我们知道他们的意思</strong>。事实上，假设<strong class="la iu">与目标变量</strong>的均值重合是合理的。例如，如果您正在尝试对垃圾邮件进行分类，并且您观察到收到的邮件中有10%是垃圾邮件，您可以得出结论，潜在隐藏概率的平均值也是10%。</p><p id="ed5d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">我们还可以说一下它们的方差</strong>，至少是上下界。方差的下限当然是零。关于上界，我们可以用<a class="ae nu" href="https://en.wikipedia.org/wiki/Bhatia%E2%80%93Davis_inequality" rel="noopener ugc nofollow" target="_blank"> Bathia-Davis不等式</a>，它说明方差的上界是<code class="fe nx ny nz oa b">(mean — min)*(max — mean)</code>。由此，我们可以得出结论:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/05a7cc649c8c1142a0f9b6f290505d13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gbMW2CRZUOULbrmTQfEGCA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">我们所知道的隐藏概率的均值和方差，其中<strong class="bd oe"> y </strong>代表目标变量。[图片由作者提供]</p></figure><p id="1726" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但这还不是全部。我们也可以假设隐藏概率是贝塔分布的。事实上，<a class="ae nu" href="https://en.wikipedia.org/wiki/Beta_distribution" rel="noopener ugc nofollow" target="_blank">贝塔分布</a>是描述概率的最佳选择，因为:</p><ul class=""><li id="bde5" class="lu lv it la b lb lc le lf lh lw ll lx lp ly lt lz ma mb mc bi translated">它仅在区间[0，1]上定义(因此，它主要用于表示百分比和比例，例如在贝叶斯推理中)；</li><li id="b5e0" class="lu lv it la b lb md le me lh mf ll mg lp mh lt lz ma mb mc bi translated">它足够通用，可以表示许多不同的“形状”。</li></ul><p id="dd35" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">贝塔分布由两个正参数决定，称为<em class="of"> α </em>和<em class="of"> β </em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/ea06699167b992621db1cd1a561328ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n-kF7DmT83P0kHPwUzgiKw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">贝塔分布的概率密度函数，基于<em class="oh"> α </em>和<em class="oh"> β </em>的不同值【来源:<a class="ae nu" href="https://en.wikipedia.org/wiki/Beta_distribution" rel="noopener ugc nofollow" target="_blank">维基百科</a></p></figure><p id="fef8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是即使我们假设隐藏概率遵循贝塔分布，我们如何选择<em class="of"> α </em>和<em class="of"> β </em>？</p><p id="df6c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">贝塔分布的一个方便的特性是均值和方差都可以很容易地从<em class="of"> α </em>和<em class="of"> β: </em>中获得</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/03648edc1962714619e22e3b00cf0a3d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iq8BTnkw2pbXfryWu0dn-w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作为参数函数的贝塔均值和方差。[图片由作者提供]</p></figure><p id="4da8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">然而，在我们的例子中，我们宁愿做相反的事情，即从均值和方差中得到<em class="of"> α </em>和<em class="of"> β </em>。为了做到这一点，将上面的两个方程表示为均值和方差的函数就足够了。经过一些简单的代数运算，结果是这样的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/0f37610afeb79dab4b65da3847abc764.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0hclBzGLIc3Dubq2cTYPPw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作为均值和方差函数的β参数。[图片由作者提供]</p></figure><p id="0124" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">将我们看到的所有关系放在一起，我们可以编写一个Python函数，提供对应于任何(合法的)均值和方差对的<em class="of"> α </em>和<em class="of"> β </em>:</p><pre class="kj kk kl km gt oj oa ok ol aw om bi"><span id="6aee" class="on my it oa b gy oo op l oq or"><strong class="oa iu">def </strong>get_beta_params(mean, var):</span><span id="d3d5" class="on my it oa b gy os op l oq or">    <strong class="oa iu">assert </strong>0 &lt; mean &lt; 1, 'mean must be in ]0 and 1['<br/>    <strong class="oa iu">assert </strong>0 &lt; var &lt; mean * (1 - mean), 'var must be in ]0, mean * (1 - mean)['</span><span id="97bd" class="on my it oa b gy os op l oq or">    alpha = ((1 - mean) / var - 1 / mean) * mean ** 2<br/>    beta = alpha * (1 / mean - 1)</span><span id="5372" class="on my it oa b gy os op l oq or">    <strong class="oa iu">return </strong>alpha, beta</span></pre><p id="c4df" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该函数允许我们绘制具有给定均值(假设为0.1)和不同方差水平的贝塔分布的pdf(注意，在这种情况下，方差的上限将是0.1 * 9 = . 09)。</p><pre class="kj kk kl km gt oj oa ok ol aw om bi"><span id="ef9e" class="on my it oa b gy oo op l oq or"><strong class="oa iu">from </strong>scipy.stats <strong class="oa iu">import </strong>beta <strong class="oa iu">as </strong>beta_distrib<br/><strong class="oa iu">import </strong>numpy <strong class="oa iu">as </strong>np</span><span id="ff3f" class="on my it oa b gy os op l oq or">mean = .1</span><span id="9555" class="on my it oa b gy os op l oq or"><strong class="oa iu">for </strong>variance <strong class="oa iu">in </strong>[.000001, .0005, .001, .002, .005, .01, .04, .089999]:<br/>  alpha, beta = get_beta_params(mean, variance)<br/>  pdf = beta_distrib(alpha, beta).pdf(np.linspace(0, 1, 100))</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ot"><img src="../Images/b9709888550116b6755fff0e9fae8f9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g7jqBOKvTWl3qp3DYxvquw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">均值= .10且方差不同的贝塔分布的概率密度函数。注意:最小理论方差是0，而最大理论方差是0.09[图片由作者提供]</p></figure><p id="da9e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">除了绘制概率密度函数，一旦我们知道了<em class="of"> α </em>和<em class="of"> β </em>，我们可以通过函数<code class="fe nx ny nz oa b">np.random.beta(alpha, beta, size)</code>抽取一个随机样本，并绘制各自的直方图:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ou"><img src="../Images/33c0dec9a05e99e14bc69ee4ac7b0690.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*z6AcAuvjGHeDbdFUDxACZQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">直方图:均值= .1和不同方差的贝塔分布。[图片由作者提供]</p></figure><h1 id="b3d7" class="mx my it bd mz na nb nc nd ne nf ng nh jz ni ka nj kc nk kd nl kf nm kg nn no bi translated">4.寻找ROC的上界</h1><p id="28ff" class="pw-post-body-paragraph ky kz it la b lb np ju ld le nq jx lg lh nr lj lk ll ns ln lo lp nt lr ls lt im bi translated">既然我们已经学会了如何模拟尽可能多的Beta分布(对于给定的均值，从最小方差到最大方差)，我们终于可以回到我们最初的目的了。</p><p id="9a44" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">事实上，从每个分布中，我们可以抽取一个概率样本，用于生成目标变量。比较概率和目标值计算出的ROC正是我们最初寻找的度量。</p><p id="99d2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是一个流程图:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/4e6e26d120b4f673e8a7c06a1d8f2afa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jYCDo9Ixjamu7OKAXLSQ0w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">完整的过程:隐藏概率是从具有指定参数的Beta中提取的，然后概率被用于生成目标变量，最后ROC分数是根据隐藏概率和观察结果计算的。[图片由作者提供]</p></figure><p id="9adc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在Python中，该过程如下所示:</p><pre class="kj kk kl km gt oj oa ok ol aw om bi"><span id="d703" class="on my it oa b gy oo op l oq or"><strong class="oa iu">import </strong>pandas <strong class="oa iu">as </strong>pd<br/><strong class="oa iu">import </strong>numpy <strong class="oa iu">as </strong>np<br/><strong class="oa iu">from </strong>sklearn.metrics <strong class="oa iu">import </strong>roc_auc_score</span><span id="6a49" class="on my it oa b gy os op l oq or">mean = .1<br/>var_min = 0<br/>var_max = mean * (1 - mean)</span><span id="09f6" class="on my it oa b gy os op l oq or"><strong class="oa iu">for </strong>var <strong class="oa iu">in </strong>np.linspace(var_min + 1e-10, var_max - 1e-10, 100):<br/>  alpha, beta = get_beta_params(mean, var)<br/>  proba = pd.Series(np.random.beta(alpha, beta, 100000))    <br/>  y = proba.apply(<strong class="oa iu">lambda </strong>p: np.random.choice([0, 1], p = [1 - p, p]))<br/>  roc_auc_score(y, proba)</span></pre><p id="a86d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们看一些例子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/8df460237cf301fd11edfe318ce6ad18.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_j-m1gPXur_IQvA09UwuEg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">贝塔分布及其ROC的一些例子。[图片由作者提供]</p></figure><p id="8bf6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是6个不同的测试版，平均值为0.1。但是我们可以生成无穷多个，从最小方差到最大方差。所以让我们取100个平均值相同、方差递增的贝塔。然后，让我们在<em class="of"> x </em>轴上绘制最大方差的百分比，并在<em class="of"> y </em>轴上绘制各自的ROC。这是我们得到的结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ox"><img src="../Images/05c3090e7d038189051256bb86d98b23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RxIei6PBj-p8le98iJ9QbA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">贝塔分布的方差与其相关ROC的关系。[图片由作者提供]</p></figure><p id="2f62" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">是不是很迷人？有一个明显的模式:</p><blockquote class="mi"><p id="d207" class="mj mk it bd ml mm mn mo mp mq mr lt dk translated">隐藏概率的方差越低，问题越“难”(即可实现的ROC越低)。</p></blockquote><p id="6db9" class="pw-post-body-paragraph ky kz it la b lb ms ju ld le mt jx lg lh mu lj lk ll mv ln lo lp mw lr ls lt im bi translated">毕竟，这很直观。当方差为0时，意味着所有样本都具有相同的隐藏概率:不可能对它们进行排序，因此即使是最好的可能模型也不会比随机模型好。这意味着ROC = .5。</p><p id="1702" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">相反，当方差达到最大值时，这个过程实际上没有随机性:最好的可能模型可以完美地区分积极因素和消极因素。这意味着ROC = 1。</p><p id="cc57" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><strong class="la iu">最有可能的是，所有现实世界的分类任务都介于这两个极端之间。</strong></p><p id="1c32" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当然，我们所看到的不仅适用于ROC，也适用于任何性能指标。出于好奇，让我们也画出F1的分数，平均精度和准确度:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/191598253e9c8f8942739c15fac737db.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y5i7pJDZpRjsak3cfT2CBg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">贝塔分布的方差与其相关指标(ROC、F1、平均精度、准确度)之间的关系。[图片由作者提供]</p></figure><p id="8963" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这次旅行让我们发现了为什么有些问题看起来如此“困难”。我们已经证明，对可以达到的最大ROC有统计限制:如果生成概率的分布具有低方差，即使是最好的可能模型也不能在物理上达到高ROC分数。</p><p id="469e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是对我们最初猜测的更严谨的解释。您实现的ROC不仅取决于您的数据有多好或您的模型有多强大，还取决于您正在解决的问题有多困难。</p></div><div class="ab cl oz pa hx pb" role="separator"><span class="pc bw bk pd pe pf"/><span class="pc bw bk pd pe pf"/><span class="pc bw bk pd pe"/></div><div class="im in io ip iq"><p id="66bc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢您的阅读！我希望这篇文章对你有用。</p><p id="de93" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我感谢反馈和建设性的批评。如果你想谈论这篇文章或其他相关话题，你可以发短信给我<a class="ae nu" href="https://www.linkedin.com/in/samuelemazzanti/" rel="noopener ugc nofollow" target="_blank">我的Linkedin联系人</a>。</p></div></div>    
</body>
</html>