<html>
<head>
<title>Introduction to TensorFlow Probability</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">张量流概率简介</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-tensorflow-probability-6d5871586c0e?source=collection_archive---------3-----------------------#2021-10-30">https://towardsdatascience.com/introduction-to-tensorflow-probability-6d5871586c0e?source=collection_archive---------3-----------------------#2021-10-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="051b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用张量流概率建立贝叶斯神经网络不应错过的关键模块</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/427cce920aab86ad690362083f31580b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5GORsPCQZhB67eCWMF4s7g.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://blog.cyda.hk/" rel="noopener ugc nofollow" target="_blank"> cyda </a></p></figure><h1 id="7ff6" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">目标</h1><p id="7ff9" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在前面的文章中，我们讨论了贝叶斯神经网络(BNN)的概念及其背后的数学理论。对于那些刚到BNN的人，确保你已经检查了下面的链接。</p><div class="mn mo gp gr mp mq"><a rel="noopener follow" target="_blank" href="/why-you-should-use-bayesian-neural-network-aaf76732c150"><div class="mr ab fo"><div class="ms ab mt cl cj mu"><h2 class="bd iu gy z fp mv fr fs mw fu fw is bi translated">为什么你应该使用贝叶斯神经网络？</h2><div class="mx l"><h3 class="bd b gy z fp mv fr fs mw fu fw dk translated">贝叶斯神经网络解释了模型中的不确定性，并提供了权重和权重的分布</h3></div><div class="my l"><p class="bd b dl z fp mv fr fs mw fu fw dk translated">towardsdatascience.com</p></div></div><div class="mz l"><div class="na l nb nc nd mz ne ks mq"/></div></div></a></div><div class="mn mo gp gr mp mq"><a rel="noopener follow" target="_blank" href="/8-terms-you-should-know-about-bayesian-neural-network-467a16266ea0"><div class="mr ab fo"><div class="ms ab mt cl cj mu"><h2 class="bd iu gy z fp mv fr fs mw fu fw is bi translated">关于贝叶斯神经网络你应该知道的8个术语</h2><div class="mx l"><h3 class="bd b gy z fp mv fr fs mw fu fw dk translated">先验，后验，贝叶斯定理，负对数似然，KL散度，替代，变分…</h3></div><div class="my l"><p class="bd b dl z fp mv fr fs mw fu fw dk translated">towardsdatascience.com</p></div></div><div class="mz l"><div class="nf l nb nc nd mz ne ks mq"/></div></div></a></div><p id="e2b4" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated">今天，我们将探索使用张量流概率来实现BNN模型的概率规划。从本文中，您将学习如何使用张量流概率来</p><ol class=""><li id="76ef" class="nl nm it lt b lu ng lx nh ma nn me no mi np mm nq nr ns nt bi translated">构建不同的<strong class="lt iu">发行版</strong>并从中取样。</li><li id="f2c3" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated">使用<strong class="lt iu">双射函数</strong>转换数据。</li><li id="4479" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated">将<strong class="lt iu">概率层</strong>与Keras结合起来构建BNN模型。</li><li id="aecb" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated">推论并说明不同类型的<strong class="lt iu">不确定性</strong>。</li></ol></div><div class="ab cl nz oa hx ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="im in io ip iq"><h1 id="ff88" class="kz la it bd lb lc og le lf lg oh li lj jz oi ka ll kc oj kd ln kf ok kg lp lq bi translated">什么是张量流概率？</h1><p id="7d80" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">TensorFlow Probability (TFP)是TensorFlow中用于概率推理和统计分析的库。它提供了概率方法与深度网络的集成，使用自动微分的基于梯度的推理，以及通过硬件加速(GPU)和分布式计算对大型数据集和模型的可扩展性。事实上，TFP是一个综合的工具，由许多模块组成，包括概率层、变分推理、MCMC、Nelder-Mead、BFGS等。但我不会深入所有模块，只会挑选和演示其中的一部分。如果你想知道更多的细节，请查看下面的技术文档。</p><div class="mn mo gp gr mp mq"><a href="https://www.tensorflow.org/probability" rel="noopener  ugc nofollow" target="_blank"><div class="mr ab fo"><div class="ms ab mt cl cj mu"><h2 class="bd iu gy z fp mv fr fs mw fu fw is bi translated">张量流概率</h2><div class="mx l"><h3 class="bd b gy z fp mv fr fs mw fu fw dk translated">TensorFlow Probability (TFP)是一个基于TensorFlow构建的Python库，它可以很容易地组合概率模型…</h3></div><div class="my l"><p class="bd b dl z fp mv fr fs mw fu fw dk translated">www.tensorflow.org</p></div></div><div class="mz l"><div class="ol l nb nc nd mz ne ks mq"/></div></div></a></div></div><div class="ab cl nz oa hx ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="im in io ip iq"><h1 id="fc48" class="kz la it bd lb lc og le lf lg oh li lj jz oi ka ll kc oj kd ln kf ok kg lp lq bi translated">分布</h1><p id="de5c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">作为概率编程包，TFP支持的关键模块之一是不同种类的统计分布。老实说，我不得不说TFP做得非常好，它覆盖了相当多的发行版(包括很多我不知道是什么的发行版，哈哈)。经过统计，我发现截至2021年10月，已经提供了117个发行版。我不打算在这里列出他们的名字。更多细节，可以查看他们的api文档<a class="ae ky" href="https://www.tensorflow.org/probability/api_docs/python/tfp/distributions" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><p id="9a39" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated">今天，我想和大家分享一些我认为有用的功能。在下面，我将使用二项式作为说明。</p><pre class="kj kk kl km gt om on oo op aw oq bi"><span id="c7b1" class="or la it on b gy os ot l ou ov">import tensorflow_probability as tfp<br/>tfd = tfp.distributions</span><span id="e43c" class="or la it on b gy ow ot l ou ov">n = 10000<br/>p = 0.3<br/>binomial_dist = tfd.Binomial(total_count=n, probs=p)</span></pre><blockquote class="ox oy oz"><p id="6939" class="lr ls pa lt b lu ng ju lw lx nh jx lz pb ni mc md pc nj mg mh pd nk mk ml mm im bi translated"><strong class="lt iu"> 1。样本数据</strong></p></blockquote><p id="d934" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated">假设我们想从刚刚创建的二项分布中抽取5个样本。您只需使用<strong class="lt iu"> <em class="pa">样本</em> </strong>方法，并指定您想要抽取的样本数量。</p><pre class="kj kk kl km gt om on oo op aw oq bi"><span id="c392" class="or la it on b gy os ot l ou ov">binomial_dist.sample(5)</span></pre><p id="cbc0" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated"><em class="pa"> &lt; tf。张量:shape=(5，)，dtype=float32，numpy = array(<br/>【3026。, 3032., 2975., 2864., 2993.]，dtype=float32) &gt; </em></p><blockquote class="ox oy oz"><p id="078d" class="lr ls pa lt b lu ng ju lw lx nh jx lz pb ni mc md pc nj mg mh pd nk mk ml mm im bi translated"><strong class="lt iu"> 2。汇总统计数据</strong></p></blockquote><p id="1a39" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated">另一个很酷的特性是获取汇总统计数据。对于像二项式这样的简单分布，我们可以很容易地通过下式得出统计数据</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/9a1b20b98aed3835fb8aef36b0b48bfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:678/format:webp/1*1cZTU8MDpzN6-qi6k1q5lA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://blog.cyda.hk/" rel="noopener ugc nofollow" target="_blank"> cyda </a>拍摄</p></figure><p id="0d38" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated">然而，对于一些复杂的分布，计算统计数据可能并不容易。但是现在你可以简单地利用这些工具。</p><pre class="kj kk kl km gt om on oo op aw oq bi"><span id="0bfa" class="or la it on b gy os ot l ou ov">mu = binomial_dist.mean()<br/>std = binomial_dist.stddev()<br/>print('Mean:', mu.numpy(), ', Standard Deviation:', std.numpy())</span></pre><p id="4000" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated"><em class="pa">均值:3000.0，标准差:45.825756 </em></p><blockquote class="ox oy oz"><p id="acfa" class="lr ls pa lt b lu ng ju lw lx nh jx lz pb ni mc md pc nj mg mh pd nk mk ml mm im bi translated">3.<strong class="lt iu">概率密度函数(PDF) </strong></p></blockquote><p id="d12b" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated">要找到给定的<em class="pa"> x </em>的PDF，我们可以使用<strong class="lt iu"> <em class="pa"> prob </em> </strong>的方法。</p><pre class="kj kk kl km gt om on oo op aw oq bi"><span id="eb7c" class="or la it on b gy os ot l ou ov">x = 3050<br/>pdf = binomial_dist.prob(x)<br/>print('PDF:', pdf.numpy())</span></pre><p id="4d62" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated"><em class="pa"> PDF: 0.004784924 </em></p><pre class="kj kk kl km gt om on oo op aw oq bi"><span id="720c" class="or la it on b gy os ot l ou ov">import matplotlib.pyplot as plt<br/>fig = plt.figure(figsize = (10, 6))<br/>x_min = int(mu-3*std)<br/>x_max = int(mu+3*std)<br/>pdf_list = [binomial_dist.prob(x) for x in range(x_min, x_max)]<br/>plt.plot(range(x_min, x_max), pdf_list)<br/>plt.title('Probability Density Function')<br/>plt.ylabel('$pdf$')<br/>plt.xlabel('$x$')<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/4a0cc80bf2d87d083baf995cdc0dbda9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*kgBX7oZ6OXDkhgUEyAyYgA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">cyda 拍摄的照片</p></figure><blockquote class="ox oy oz"><p id="1392" class="lr ls pa lt b lu ng ju lw lx nh jx lz pb ni mc md pc nj mg mh pd nk mk ml mm im bi translated"><strong class="lt iu"> 4。累积密度函数</strong></p></blockquote><p id="f535" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated">要找到给定<em class="pa"> x </em>的CDF，我们可以使用方法<strong class="lt iu"> <em class="pa"> cdf </em> </strong>。</p><pre class="kj kk kl km gt om on oo op aw oq bi"><span id="762a" class="or la it on b gy os ot l ou ov">x = 3050<br/>cdf = binomial_dist.cdf(x)<br/>print('CDF:', cdf.numpy())</span></pre><p id="5af3" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated"><em class="pa"> CDF: 0.865279 </em></p><pre class="kj kk kl km gt om on oo op aw oq bi"><span id="719f" class="or la it on b gy os ot l ou ov">fig = plt.figure(figsize = (10, 6))<br/>x_min = int(mu-3*std)<br/>x_max = int(mu+3*std)<br/>cdf_list = [binomial_dist.cdf(x) for x in range(x_min, x_max)]<br/>plt.plot(range(x_min, x_max), cdf_list)<br/>plt.title('Cumulative Density Function')<br/>plt.ylabel('$cdf$')<br/>plt.xlabel('$x$')<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/f944e42cb3e6876bfd22d4ffb077a524.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*_5RF5qbXTpQ7XVmFG9VLQQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">cyda 拍摄的照片</p></figure><blockquote class="ox oy oz"><p id="1b96" class="lr ls pa lt b lu ng ju lw lx nh jx lz pb ni mc md pc nj mg mh pd nk mk ml mm im bi translated"><strong class="lt iu"> 5。对数似然</strong></p></blockquote><p id="0e54" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated">我想介绍的最后一个方法是<strong class="lt iu"> <em class="pa"> log_prob </em> </strong>。这用于计算对数似然。根据前两篇文章，我想每个人都会意识到这有多重要，因为我们总是用它作为损失函数。因此，要找到对数似然，我们只需调用</p><pre class="kj kk kl km gt om on oo op aw oq bi"><span id="fe2b" class="or la it on b gy os ot l ou ov">x = 3050<br/>l = binomial_dist.log_prob(x)<br/>print('Log-likelihood:', l.numpy())</span></pre><p id="cb19" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated"><em class="pa">对数可能性:-5.342285 </em></p><pre class="kj kk kl km gt om on oo op aw oq bi"><span id="68e5" class="or la it on b gy os ot l ou ov">fig = plt.figure(figsize = (10, 6))<br/>x_min = int(mu-3*std)<br/>x_max = int(mu+3*std)<br/>l_list = [binomial_dist.log_prob(x) for x in range(x_min, x_max)]<br/>plt.plot([j for j in range(x_min, x_max)], l_list)<br/>plt.title('Log-Likelihood')<br/>plt.ylabel('$l$')<br/>plt.xlabel('$x$')<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/0534c251ee9f45bfb41f250d893c9048.png" data-original-src="https://miro.medium.com/v2/resize:fit:1220/format:webp/1*080xoXXiDbTsqjM1Q2WMkg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">cyda 拍摄的照片</p></figure></div><div class="ab cl nz oa hx ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="im in io ip iq"><h1 id="7360" class="kz la it bd lb lc og le lf lg oh li lj jz oi ka ll kc oj kd ln kf ok kg lp lq bi translated">双喷射器</h1><p id="6177" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">双射体是由张量流命名的术语，基本上是指双射变换。根据定义，双射变换是两个集合的元素之间的函数，其中一个集合的每个元素恰好与另一个集合的一个元素配对，而另一个集合的每个元素恰好与第一个集合的一个元素配对。</p><p id="3b67" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated">对我来说，我会把双对象看作是现成的数据转换函数。在这里你可以找到很多常用的函数，比如<strong class="lt iu"><em class="pa">Log</em></strong><strong class="lt iu"><em class="pa">Exp</em></strong><strong class="lt iu"><em class="pa">Sigmoid</em></strong><strong class="lt iu"><em class="pa">Tanh</em></strong><strong class="lt iu"><em class="pa">soft plus</em></strong><strong class="lt iu"><em class="pa">soft sign</em></strong>等等。</p><p id="4799" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated">您可以简单地用下面的代码调用bijector。</p><pre class="kj kk kl km gt om on oo op aw oq bi"><span id="bfec" class="or la it on b gy os ot l ou ov">tfb = tfp.bijectors<br/>exp = tfb.Exp()</span></pre><p id="6a67" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated">要应用转换，只需将值传递给对象。</p><pre class="kj kk kl km gt om on oo op aw oq bi"><span id="d508" class="or la it on b gy os ot l ou ov">import numpy as np<br/>x = np.linspace(-3, 3, 100)<br/>y = exp(x)</span><span id="abdf" class="or la it on b gy ow ot l ou ov">fig = plt.figure(figsize = (10, 6))<br/>plt.plot(x, y)<br/>plt.title('Exponential Transform')<br/>plt.ylabel('$exp(x)$')<br/>plt.xlabel('$x$')<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/a21bc9b14cc875e30d0d57bd1810b1f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*7k0uLBJzv3XqBA0kJb_w2A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">cyda 拍摄的照片</p></figure><p id="7c97" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated">我想分享一个小技巧，有一个名为<strong class="lt iu"> <em class="pa"> Chain、</em> </strong>的双射器，用于应用一系列双射器。例如，如果我们想将<em class="pa"> x </em>传递给<strong class="lt iu"> <em class="pa"> softplus </em> </strong>函数，然后传递给<strong class="lt iu"> <em class="pa"> exp </em> </strong>函数。我们可以这样写</p><pre class="kj kk kl km gt om on oo op aw oq bi"><span id="c526" class="or la it on b gy os ot l ou ov">exp = tfb.Exp()<br/>softplus = tfb.Softplus()<br/>chain = tfb.Chain([exp, softplus])</span></pre><p id="3f62" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated">这样做相当于</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pj"><img src="../Images/de34269a90821307f0ca041fea91c48c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jD6J3llbD48XPyRUMw9cBg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">cyda 拍摄的照片</p></figure><pre class="kj kk kl km gt om on oo op aw oq bi"><span id="3353" class="or la it on b gy os ot l ou ov">x = np.linspace(-3, 3, 100)<br/>y_chain = chain(x)<br/>y_exp = exp(x)</span><span id="f0bc" class="or la it on b gy ow ot l ou ov">fig = plt.figure(figsize = (10, 6))<br/>plt.plot(x, y_chain, label = 'Chain')<br/>plt.plot(x, y_exp, label = 'Exp')<br/>plt.title('Exponential Transform')<br/>plt.ylabel('$exp(x)$')<br/>plt.xlabel('$x$')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/8dbaf835e50084f56ba97289b705f9e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1218/format:webp/1*3D5ftF_U4qT7vPRF8tR2Pg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://blog.cyda.hk/" rel="noopener ugc nofollow" target="_blank"> cyda </a>拍摄</p></figure></div><div class="ab cl nz oa hx ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="im in io ip iq"><h1 id="e6fe" class="kz la it bd lb lc og le lf lg oh li lj jz oi ka ll kc oj kd ln kf ok kg lp lq bi translated">层</h1><p id="1e99" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu"> <em class="pa"> tfp.layers </em> </strong>模块通过将原来的层替换为概率层，为开发人员建立了一个用户友好的界面，方便他们将模型从标准神经网络切换到贝叶斯神经网络。在下面，我将列出一些我经常使用的层作为参考。</p><ul class=""><li id="20e9" class="nl nm it lt b lu ng lx nh ma nn me no mi np mm pl nr ns nt bi translated"><strong class="lt iu">认知的不确定性</strong></li><li id="0e5c" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm pl nr ns nt bi translated"><strong class="lt iu">独立正态</strong> : <strong class="lt iu"> </strong>随机不确定性</li><li id="b77b" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm pl nr ns nt bi translated"><strong class="lt iu">分布λ</strong>:随机不确定性</li></ul><h2 id="7139" class="or la it bd lb pm pn dn lf po pp dp lj ma pq pr ll me ps pt ln mi pu pv lp pw bi translated"><strong class="ak"> <em class="px"> 1。创建数据集</em> </strong></h2><p id="ab64" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">因此，为了训练BNN模型，首先，我们必须创建数据集。</p><pre class="kj kk kl km gt om on oo op aw oq bi"><span id="70af" class="or la it on b gy os ot l ou ov">def create_dataset(n, x_range, slope=2, intercept=10, noise=0.5):<br/>    x_uniform_dist = tfd.Uniform(low=x_range[0], high=x_range[1])<br/>    x = x_uniform_dist.sample(n).numpy().reshape(-1, 1)<br/>    y_true = slope*x+intercept<br/>    eps_uniform_dist = tfd.Normal(loc=0, scale=1)<br/>    eps = eps_uniform_dist.sample(n).numpy().reshape(-1, 1)*noise*x<br/>    y = y_true + eps<br/>    return x, y, y_true</span><span id="d6c2" class="or la it on b gy ow ot l ou ov">n_train = 5000<br/>n_val = 1000<br/>n_test = 5000<br/>x_range = [-10, 10]<br/>x_train, y_train, y_true = create_dataset(n_train, x_range)<br/>x_val, y_val, _ = create_dataset(n_val, x_range)<br/>x_test = np.linspace(x_range[0], x_range[1], n_test).reshape(-1, 1)</span></pre><p id="11f8" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated">样本数据实际上是一个线性拟合，带有一个异质方差以及<em class="pa"> x </em>的值。为了更好地可视化，您可以使用以下代码进行绘图。</p><pre class="kj kk kl km gt om on oo op aw oq bi"><span id="1086" class="or la it on b gy os ot l ou ov">def plot_dataset(x_train, y_train, x_val, y_val, y_true, title):<br/>    fig = plt.figure(figsize = (10, 8))<br/>    plt.scatter(x_train, y_train, marker='+', label='Train')<br/>    plt.scatter(x_val, y_val, marker='+', color='r', label='Val')<br/>    plt.plot(x_train, y_true, color='k', label='Truth')<br/>    plt.title(title)<br/>    plt.xlabel('$x$')<br/>    plt.ylabel('$y$')<br/>    plt.legend()<br/>    plt.show()</span><span id="2534" class="or la it on b gy ow ot l ou ov">plot_dataset(x_train, y_train, x_val, y_val, y_true, 'Dataset')</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi py"><img src="../Images/835cb5eb97509f5ddb4f8b6d932d4d6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*0mVb_ITGbIXcPXDSFaQPlQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">cyda 拍摄的照片</p></figure><h2 id="9ac7" class="or la it bd lb pm pn dn lf po pp dp lj ma pq pr ll me ps pt ln mi pu pv lp pw bi translated">2.标准神经网络</h2><p id="6df8" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在训练BNN之前，我想制作一个标准的神经网络作为比较的基线。</p><pre class="kj kk kl km gt om on oo op aw oq bi"><span id="f99b" class="or la it on b gy os ot l ou ov">import tensorflow as tf<br/>tfkl = tf.keras.layers</span><span id="97a7" class="or la it on b gy ow ot l ou ov"># Model Architecture<br/>model = tf.keras.Sequential([<br/>    tfkl.Dense(1, input_shape = (1,))<br/>])</span><span id="0b50" class="or la it on b gy ow ot l ou ov"># Model Configuration<br/>model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01), loss=tf.keras.losses.MeanSquaredError())</span><span id="628e" class="or la it on b gy ow ot l ou ov"># Early Stopping Callback<br/>callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30, min_delta=0, mode='auto', baseline=None, restore_best_weights=True)</span><span id="8d76" class="or la it on b gy ow ot l ou ov"># Model Fitting<br/>history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=1000, verbose=False, shuffle=True, callbacks=[callback], batch_size = 100)</span></pre><p id="4dcc" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated">正如你可能观察到的，简单模型只设置了一个<strong class="lt iu"> <em class="pa">密集</em> </strong>隐藏层，为了检查模型的执行情况，我们可以使用<strong class="lt iu"> <em class="pa">预测</em> </strong>方法。</p><pre class="kj kk kl km gt om on oo op aw oq bi"><span id="caea" class="or la it on b gy os ot l ou ov">y_pred = model.predict(x_test)<br/>fig = plt.figure(figsize = (10, 8))<br/>plt.scatter(x_train, y_train, marker='+', label='Train')<br/>plt.plot(x_train, y_true, color='k', label='Truth')<br/>plt.plot(x_test, y_pred, color='r', label='Predict')<br/>plt.legend()<br/>plt.title('Standard Neural Network')<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi py"><img src="../Images/3b10bca19de0dd47b3ca70cbfa7b49a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*vpNGayq8KZqmIebTw5g_TA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">cyda 拍摄的照片</p></figure><p id="275b" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated">看起来，预测与预期的真实线性线相匹配。然而，使用SNN不能告诉预测的不确定性。</p><h2 id="f34f" class="or la it bd lb pm pn dn lf po pp dp lj ma pq pr ll me ps pt ln mi pu pv lp pw bi translated">3.贝叶斯神经网络</h2><p id="ec56" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">所以就到了主菜。让我们一步一步地讨论代码。</p><p id="3725" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated"><strong class="lt iu"> 3.1模型架构</strong></p><pre class="kj kk kl km gt om on oo op aw oq bi"><span id="db79" class="or la it on b gy os ot l ou ov">tfpl = tfp.layers</span><span id="de24" class="or la it on b gy ow ot l ou ov">model = tf.keras.Sequential([<br/>    tfkl.Dense(2, input_shape = (1,)),<br/>    tfpl.DistributionLambda(lambda t: tfd.Normal(loc=t[..., :1], scale=1e-3+tf.math.abs(t[...,1:])))<br/>])</span></pre><p id="6f44" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated">你可能会注意到，我们将有一个<strong class="lt iu"> <em class="pa">密集</em> </strong>层输出两个神经元。你能猜出这两个参数是干什么用的吗？它们是<strong class="lt iu">均值</strong>和<strong class="lt iu">标准差</strong>，将被传递给我们在<strong class="lt iu"> <em class="pa">分布层</em> </strong>中指定的正态分布。</p><p id="1ae6" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated"><strong class="lt iu"> 3.2车型配置</strong></p><pre class="kj kk kl km gt om on oo op aw oq bi"><span id="e061" class="or la it on b gy os ot l ou ov">negloglik = lambda y_true, y_pred: -y_pred.log_prob(y_true)<br/>model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01), loss=negloglik)</span></pre><p id="4484" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated">根据我们在以前文章中的讨论，我们将对BNN使用负对数似然，而不是MSE。如果你不知道我在说什么，我强烈建议你回到前两篇文章，先熟悉一下概念和理论。</p><p id="9d3d" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated"><strong class="lt iu"> 3.3训练模型</strong></p><pre class="kj kk kl km gt om on oo op aw oq bi"><span id="7bbd" class="or la it on b gy os ot l ou ov"># Early Stopping Callback<br/>callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=300, min_delta=0, mode='auto', baseline=None, restore_best_weights=True)</span><span id="d017" class="or la it on b gy ow ot l ou ov"># Model Fitting<br/>history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=10000, verbose=False, shuffle=True, callbacks=[callback], batch_size = 100)</span></pre><p id="135c" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated">这里没有什么特别的，基本上一切都和SNN的场景相似，所以我将跳过这一部分。</p><p id="0620" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated"><strong class="lt iu"> 3.4预测</strong></p><p id="cdcc" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated">今天最有价值的代码就在这里。</p><pre class="kj kk kl km gt om on oo op aw oq bi"><span id="df5b" class="or la it on b gy os ot l ou ov"># Summary Statistics<br/>y_pred_mean = model(x_test).mean()<br/>y_pred_std = model(x_test).stddev()</span></pre><p id="f1a5" class="pw-post-body-paragraph lr ls it lt b lu ng ju lw lx nh jx lz ma ni mc md me nj mg mh mi nk mk ml mm im bi translated">通过调用<strong class="lt iu"> <em class="pa"> mean </em> </strong>和<strong class="lt iu"> <em class="pa"> stddev </em> </strong>函数，现在可以告诉你预测的不确定性水平。</p><pre class="kj kk kl km gt om on oo op aw oq bi"><span id="7c82" class="or la it on b gy os ot l ou ov">fig = plt.figure(figsize = (10, 8))<br/>plt.scatter(x_train, y_train, marker='+', label='Train')<br/>plt.plot(x_train, y_true, color='k', label='Truth')<br/>plt.plot(x_test, y_pred_mean, color='r', label='Predicted Mean')<br/>plt.fill_between(x_test.ravel(), np.array(y_pred_mean+1*y_pred_std).ravel(), np.array(y_pred_mean-1*y_pred_std).ravel(), color='C1', alpha=0.5, label='Aleatoric Uncertainty (1SD)')<br/>plt.fill_between(x_test.ravel(), np.array(y_pred_mean+2*y_pred_std).ravel(), np.array(y_pred_mean-2*y_pred_std).ravel(), color='C1', alpha=0.4, label='Aleatoric Uncertainty (2SD)')<br/>plt.fill_between(x_test.ravel(), np.array(y_pred_mean+3*y_pred_std).ravel(), np.array(y_pred_mean-3*y_pred_std).ravel(), color='C1', alpha=0.3, label='Aleatoric Uncertainty (3SD)')<br/>plt.title('Bayesian Neural Network')<br/>plt.xlabel('$x$')<br/>plt.ylabel('$y$')<br/>plt.legend()<br/>plt.show()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi py"><img src="../Images/b02f0781a755cc84998452a75c9d49f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1232/format:webp/1*K8h9qKIDs5dFYopnxYyfQw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">cyda 拍摄的照片</p></figure></div><div class="ab cl nz oa hx ob" role="separator"><span class="oc bw bk od oe of"/><span class="oc bw bk od oe of"/><span class="oc bw bk od oe"/></div><div class="im in io ip iq"><h1 id="9037" class="kz la it bd lb lc og le lf lg oh li lj jz oi ka ll kc oj kd ln kf ok kg lp lq bi translated">结论</h1><p id="a1b8" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">看，你现在能够区分模型输出的任意不确定性。等等，那么认知的不确定性在哪里？我把这部分留给你们去进一步探索。提示是将<strong class="lt iu"> <em class="pa">密集</em> </strong>层替换为<strong class="lt iu"> <em class="pa">密集</em> </strong>层。去试试吧。=)</p></div></div>    
</body>
</html>