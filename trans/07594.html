<html>
<head>
<title>Explainable AI (XAI) with SHAP -Multi-Class Classification Problem</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">可解释的人工智能(XAI)与SHAP-多类分类问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/explainable-ai-xai-with-shap-multi-class-classification-problem-64dd30f97cea?source=collection_archive---------3-----------------------#2021-07-12">https://towardsdatascience.com/explainable-ai-xai-with-shap-multi-class-classification-problem-64dd30f97cea?source=collection_archive---------3-----------------------#2021-07-12</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9230" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">多类分类问题的SHAP XAI分析实用指南</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/bde95bb7a4da4cf89c5204351d5a0e85.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3RhiJAGTUW79TAaIzaPVUw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片由<a class="ae kv" href="https://www.pexels.com/photo/abstract-art-circle-clockwork-414579/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a> Sager来自<a class="ae kv" href="https://pixabay.com/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=2755908" rel="noopener ugc nofollow" target="_blank"> Pixabay </a></p></figure><p id="9133" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">模型可解释性成为机器学习管道的基本部分。将机器学习模型作为“黑盒”不再是一个选项。幸运的是，有一些分析工具，如lime、ExplainerDashboard、Shapash、Dalex等等，正在迅速发展，变得越来越流行。在之前的帖子中，我们解释了如何使用<a class="ae kv" rel="noopener" target="_blank" href="/explainable-ai-xai-with-shap-regression-problem-b2d63fdca670"> SHAP解决回归问题</a>。本指南提供了一个实际示例，说明如何使用和解释开源python包SHAP来进行多类分类问题中的XAI分析，并使用它来改进模型。</p><p id="fa32" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Lundberg和Lee (2016)的SHAP(Shapley Additive explaints)是一种解释个体预测的方法，基于博弈理论上的最优Shapley值<a class="ae kv" href="https://christophm.github.io/interpretable-ml-book/shap.html#shap-feature-importance" rel="noopener ugc nofollow" target="_blank">【1】</a>。计算Shapley值以获得特征贡献在计算上是昂贵的。有两种方法可以近似SHAP值以提高计算效率:KernelSHAP、TreeSHAP(仅适用于基于树的模型)。</p><p id="f0ba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">SHAP提供基于Shapley值集合的全球和局部解释方法。在本指南中，我们将使用Kaggle数据集<a class="ae kv" href="https://www.kaggle.com/tunguz/internet-firewall-data-set" rel="noopener ugc nofollow" target="_blank">【2】</a>中的互联网防火墙数据集示例，展示多类分类问题的一些SHAP输出图。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="5367" class="lx ly iq lt b gy lz ma l mb mc"># load the csv file as a data frame<br/>df = pd.read_csv('log2.csv')<br/>y = df.Action.copy()<br/>X = df.drop('Action',axis=1)</span></pre><p id="a8a0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">像往常一样创建模型并进行拟合。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="56fd" class="lx ly iq lt b gy lz ma l mb mc">X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, random_state=0)</span><span id="a5e4" class="lx ly iq lt b gy md ma l mb mc">cls = RandomForestClassifier(max_depth=2, random_state=0)<br/>cls.fit(X_train, y_train)</span></pre><p id="e019" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，为了对模型有一个基本的印象，我推荐查看特性重要性和混淆矩阵。为了理解我们在特征重要性方面所处的位置，我使用了scikit-learn来计算每棵树内的杂质减少量<a class="ae kv" href="https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html" rel="noopener ugc nofollow" target="_blank">【3】</a>。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="6a76" class="lx ly iq lt b gy lz ma l mb mc">֫importances = cls.feature_importances_<br/>indices = np.argsort(importances)<br/>features = df.columns<br/>plt.title('Feature Importances')<br/>plt.barh(range(len(indices)), importances[indices], color='g', align='center')<br/>plt.yticks(range(len(indices)), [features[i] for i in indices])<br/>plt.xlabel('Relative Importance')<br/>plt.show()</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi me"><img src="../Images/1f5eca833efa254b52adfb8149e438f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CwheetYqfYVJvX3wfqPY3g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="cc77" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">稍后，我们可以将这些结果与通过Shapley值计算的特征重要性进行比较。</p><h1 id="7076" class="mf ly iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">混淆矩阵</h1><p id="319c" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf my lh li lj mz ll lm ln na lp lq lr ij bi translated">混淆矩阵是一种可视化模型性能的方法。更重要的是，我们可以很容易地看到模型失败的确切位置。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="85c0" class="lx ly iq lt b gy lz ma l mb mc">class_names = ['drop', 'allow', 'deny', 'reset-both']<br/>disp = plot_confusion_matrix(cls, X_test, y_test, display_labels=class_names, cmap=plt.cm.Blues, xticks_rotation='vertical')</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/1ad6ca0b3d8bcb3a0e346f265be97b5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*3MVgNOI3b31vBcQloU76_A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="8704" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">模型无法从类reset-both中检测到任何实例。原因是一个不平衡的数据集，提供了很少数量的重置示例，这两个类都可以从中学习。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="2b87" class="lx ly iq lt b gy lz ma l mb mc">y.value_counts()<br/>allow         37640<br/>deny          14987<br/>drop          12851<br/>reset-both       54<br/>Name: Action, dtype: int64</span></pre><h1 id="6794" class="mf ly iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">SHAP汇总图</h1><p id="06ba" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf my lh li lj mz ll lm ln na lp lq lr ij bi translated">模型输出的SHAP值解释了要素如何影响模型的输出。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="b99e" class="lx ly iq lt b gy lz ma l mb mc"># compute SHAP values<br/>explainer = shap.TreeExplainer(cls)<br/>shap_values = explainer.shap_values(X)</span></pre><p id="bf40" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们可以绘制相关的图来帮助我们分析模型。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="0669" class="lx ly iq lt b gy lz ma l mb mc">shap.summary_plot(shap_values, X.values, plot_type="bar", class_names= class_names, feature_names = X.columns)</span></pre><p id="9139" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在此图中，叠加了要素对类的影响，以创建要素重要性图。因此，如果您创建要素是为了将某个特定的类与其他类区分开来，那么这就是您可以看到它的图。换句话说，多类分类的摘要图可以向您显示机器从特征中学习到了什么。</p><p id="4527" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在下面的例子中，我们可以看到类drop很少使用pkts_sent、Source Port和Bytes Sent特性。我们还可以看到，类allow和deny同等地使用相同的特性。这就是它们之间的混淆程度相对较高的原因。为了更好地区分允许类和拒绝类，需要生成专门用于这些类的新功能。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nc"><img src="../Images/d58da469488a7c13e6b36e76bce52d36.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CKzqmnK6s5e5MiA76mlApw.png"/></div></div></figure><p id="c6f8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">还可以看到特定类的summary_plot。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="f11d" class="lx ly iq lt b gy lz ma l mb mc">shap.summary_plot(shap_values[1], X.values, feature_names = X.columns)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nd"><img src="../Images/a4ade451e8361e2c70bd8773537d463d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kY96juxJWuwFZMRxdfso6g.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="b730" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">摘要图结合了特征重要性和特征效果。摘要图上的每个点都是一个特征和一个实例的Shapley值。y轴上的位置由特征决定，x轴上的位置由Shapley值决定。可以看到，最不重要的特性pkts_sent具有较低的Shapley值。颜色代表从低到高的特性值。重叠点在y轴方向上抖动，因此我们可以了解每个要素的Shapley值的分布情况。这些功能根据其重要性进行排序。</p><p id="6efa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在汇总图中，我们看到了特性值和对预测的影响之间关系的初步迹象。但是为了看到这种关系的确切形式，我们必须看看SHAP依赖图。</p><h1 id="b03a" class="mf ly iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">SHAP依赖图</h1><p id="d3a1" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf my lh li lj mz ll lm ln na lp lq lr ij bi translated">部分依赖图(短PDP或PD图)显示了一个或两个特征对机器学习模型的预测结果的边际效应(j . h . Friedman 2001<a class="ae kv" href="https://statweb.stanford.edu/~jhf/ftp/trebst.pdf" rel="noopener ugc nofollow" target="_blank">【3】</a>)。部分相关性图可以显示目标和特征之间的关系是线性的、单调的还是更复杂的。</p><p id="ef9f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">部分相关图是一种全局方法:该方法考虑所有实例，并给出一个关于特征与预测结果的全局关系的陈述。PDP假设第一特征与第二特征不相关。如果违反了这一假设，为部分相关图计算的平均值将包括非常不可能甚至不可能的数据点。</p><p id="f188" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">依赖图是一种散点图，显示单个功能对模型所做预测的影响。在这个例子中，当每个住所的平均房间数高于6时，房产价值显著增加。</p><ul class=""><li id="1ba0" class="ne nf iq ky b kz la lc ld lf ng lj nh ln ni lr nj nk nl nm bi translated">每个点都是数据集中的一个预测(行)。</li><li id="b42e" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated">x轴是数据集中的实际值。</li><li id="7abb" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nj nk nl nm bi translated">y轴是该要素的SHAP值，表示知道该要素的值会在多大程度上改变该样本预测的模型输出。</li></ul><p id="2108" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该颜色对应于可能与我们正在绘制的特征有交互作用的第二个特征(默认情况下，该第二个特征被自动选择)。如果另一个特征和我们正在绘制的特征之间存在交互作用，它将显示为明显的垂直着色图案。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="f541" class="lx ly iq lt b gy lz ma l mb mc">֫# If we pass a numpy array instead of a data frame then we<br/># need pass the feature names in separately<br/>shap.dependence_plot(0, shap_values[0], X.values, feature_names=X.columns)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/51b661f96b0044436d8c07043579d1cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*93-oLNFWhHiYQ3XeSHzpKw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="fe68" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上面的示例中，我们可以看到功能(源端口和NAT源端口)之间交互的清晰垂直彩色图案。</p><h1 id="ff84" class="mf ly iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">SHAP力图</h1><p id="8d48" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf my lh li lj mz ll lm ln na lp lq lr ij bi translated">力图给了我们单一模型预测的可解释性。在该图中，我们可以看到特征如何影响特定观察的模型预测。用于错误分析或对特定案例的深入理解非常方便。</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="772f" class="lx ly iq lt b gy lz ma l mb mc">i=8<br/>shap.force_plot(explainer.expected_value[0], shap_values[0][i], X.values[i], feature_names = X.columns)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/715108e30ec6f8ab92eb0670778eaa2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5MO8whWJF99IfBOvQSuW3Q.png"/></div></div></figure><p id="9079" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从这个情节我们可以看出:</p><ol class=""><li id="84a1" class="ne nf iq ky b kz la lc ld lf ng lj nh ln ni lr nu nk nl nm bi translated">模型预测_proba值:0.79</li><li id="6657" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nu nk nl nm bi translated">基本值:这是在我们不知道当前实例的任何特性的情况下预测的值。基本值是模型输出在训练数据集上的平均值(代码中的explainer.expected_value)。在本例中，基值= 0.5749</li><li id="b4fc" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nu nk nl nm bi translated">绘图箭头上的数字是该实例的特征值。运行时间(秒)=5，数据包= 1</li><li id="902b" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nu nk nl nm bi translated">红色表示将模型得分推高的要素，蓝色表示将得分推低的要素。</li><li id="be9f" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nu nk nl nm bi translated">箭头越大，要素对输出的影响越大。影响的减少或增加量可以在x轴上看到。</li><li id="8a47" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nu nk nl nm bi translated">5秒的运行时间增加了该类的属性allow。包1，减少属性值。</li></ol><h1 id="d5ea" class="mf ly iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">SHAP瀑布图</h1><p id="29b5" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf my lh li lj mz ll lm ln na lp lq lr ij bi translated">瀑布图是单实例预测的另一个局部分析图。让我们以8号实例为例:</p><pre class="kg kh ki kj gt ls lt lu lv aw lw bi"><span id="d85c" class="lx ly iq lt b gy lz ma l mb mc">row = 8<br/>shap.waterfall_plot(shap.Explanation(values=shap_values[0][row], <br/>                                              base_values=explainer.expected_value[0], data=X_test.iloc[row],  <br/>                                         feature_names=X_test.columns.tolist()))</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/5e9a9ad7a419656a5cd4404ba038ea72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KLiM5GK_lvIok-GGLRZqaQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><ol class=""><li id="adc1" class="ne nf iq ky b kz la lc ld lf ng lj nh ln ni lr nu nk nl nm bi translated"><em class="nw"> f(x) </em>为模型predict_proba值:0.79。</li><li id="2cb8" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nu nk nl nm bi translated"><em class="nw"> E[f(x)]是</em>基值= 0.5749。</li><li id="e613" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nu nk nl nm bi translated">左侧是特征值，箭头表示特征对预测的贡献。</li><li id="28f6" class="ne nf iq ky b kz nn lc no lf np lj nq ln nr lr nu nk nl nm bi translated">每一行显示了每个特征的正(红色)或负(蓝色)贡献如何将值从背景数据集上的预期模型输出移动到本次预测的模型输出<a class="ae kv" href="https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/plots/waterfall.html" rel="noopener ugc nofollow" target="_blank">【2】</a>。</li></ol><h1 id="b306" class="mf ly iq bd mg mh mi mj mk ml mm mn mo jw mp jx mq jz mr ka ms kc mt kd mu mv bi translated">摘要</h1><p id="c4e2" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf my lh li lj mz ll lm ln na lp lq lr ij bi translated">SHAP框架已被证明是机器学习模型解释领域的一个重要进步。SHAP结合了几种现有的方法，创造了一种直观的，理论上合理的方法来解释任何模型的预测。SHAP值量化了特征对预测<a class="ae kv" rel="noopener" target="_blank" href="/introducing-shap-decision-plots-52ed3b4a1cba">【6】</a>的影响的大小和方向(正或负)。我相信XAI分析与SHAP和其他工具应该是机器学习管道的一个组成部分。本帖的代码可以在<a class="ae kv" href="https://github.com/Iditc/Posts-on-Medium/blob/main/Explainable%20AI/Explainable%20AI%20(XAI)%20with%20SHAP_MultiClass%20Classification%20Problem.ipynb" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p></div></div>    
</body>
</html>