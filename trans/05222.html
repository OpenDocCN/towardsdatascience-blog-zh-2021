<html>
<head>
<title>Perceptual Losses for Deep Image Restoration</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度图像恢复的感知损失</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/perceptual-losses-for-image-restoration-dd3c9de4113?source=collection_archive---------4-----------------------#2021-05-08">https://towardsdatascience.com/perceptual-losses-for-image-restoration-dd3c9de4113?source=collection_archive---------4-----------------------#2021-05-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="d300" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">从均方误差到GANs——什么是好的感知损失函数？</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/19794e249b61186bb9839329dd0de3f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bYTxbAKGKhaHoAoxOgFJTQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片由作者提供。</p></figure><p id="ae2c" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">神经网络的第一次应用让我忍不住惊呼“哇！”是关于风格转变的开创性论文。这项工作使用卷积神经网络(CNN)将风格从一幅图像转移到另一幅图像。这是第一次，我们可以在普通的图片上添加很酷的艺术滤镜——将任何照片变成Vang Gough的画，或者添加莫奈的笔触！</p><p id="8f69" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们现在已经发展到把一幅图像的特征转移到另一幅图像的更复杂的问题。例如，人类肖像的情感/风格转移，或者从一个视频到另一个视频的运动转移。现实主义的水平和质量所取得的成果也有天空火箭！然而，在这份第一风格的转印纸中提出的一个元素以这样或那样的形式持续存在——<em class="ls">知觉丧失</em>。</p><p id="dfc4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="ls">感知损失是损失函数中的一个术语，它鼓励自然的和在感知上令人愉快的结果。</em></p><p id="fd72" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这篇文章中，我将谈论不同种类的感知损失。然而，由于它们的应用在复杂程度上有所不同，因此它们的使用也有细微差别，我将集中讨论它们可以应用的最简单的问题之一——图像恢复。这里，目标是从失真的对应物中恢复高质量的图像，该图像可能已经被噪声、欠采样、模糊、压缩等破坏。</p><p id="9645" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">本文部分基于<a class="ae lr" href="https://arxiv.org/abs/2103.14616" rel="noopener ugc nofollow" target="_blank">我们最近的工作</a>，该工作探索了对于图像恢复任务，如单幅图像超分辨率、去噪和JPEG伪像消除，什么是好的损失函数的问题。作为这项工作的结果，我们还提出了一个新的损失，专门为图像恢复定制。点击查看<a class="ae lr" href="https://github.com/gfxdisp/mdf" rel="noopener ugc nofollow" target="_blank">代码！</a></p><p id="e923" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">下面，我先说说正在解决的问题。然后，我讨论了各种感知损失函数，并比较他们的表现。</p><h1 id="42ec" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">图像恢复</h1><p id="d718" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">如果图像占用很少的空间，却保持高质量，这不是很好吗？这是研究压缩算法的工程师和研究人员向自己提出的问题。然而，这并不是一项简单的任务——压缩算法经常会引入伪像。</p><div class="kg kh ki kj gt ab cb"><figure class="mq kk mr ms mt mu mv paragraph-image"><img src="../Images/41759106db5f4b77981506d8d6cfaa96.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*--jSJWW1IAAaC_6XeUbDFg.jpeg"/></figure><figure class="mq kk mr ms mt mu mv paragraph-image"><img src="../Images/74415e013369f975947e51f8addab855.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*cxIfZG60j5AbjBml_A_aDw.jpeg"/></figure><figure class="mq kk mr ms mt mu mv paragraph-image"><img src="../Images/43d769c1bcf462c4042a2c70db75e3db.png" data-original-src="https://miro.medium.com/v2/resize:fit:642/format:webp/1*o1pmrxu9Dn9Fw9PH66Noog.jpeg"/><p class="kr ks gj gh gi kt ku bd b be z dk mw di mx my translated">从左至右:地面真相，JPEG压缩图像，重建消除人工制品。来自<a class="ae lr" href="https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/" rel="noopener ugc nofollow" target="_blank"> BSD </a>数据集的图像。</p></figure></div><p id="8cb3" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">用旧诺基亚手机拍摄的图像分辨率如何？我们能否将它们超分辨率提高16倍，从而在现代高分辨率显示器上获得愉悦的观看体验？</p><div class="kg kh ki kj gt ab cb"><figure class="mq kk mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/df8cc0447de1a999b1eb5341b0e2425a.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*uzce8ANigvGigbhbBhmoRA.jpeg"/></div></figure><figure class="mq kk mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/9dfb6685631a8a89c115fe889e47842a.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*PuEsejROqp0cQuB3jRUxvw.jpeg"/></div></figure><figure class="mq kk mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/d9a3c5ce8a87c6268a6d43bd86540758.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*fAwXjWyL1XKvgKoav8CDyw.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk mw di mx my translated">从左至右:原始图像、降采样图像和4x超分辨率图像。来自<a class="ae lr" href="https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/" rel="noopener ugc nofollow" target="_blank"> BSD </a>数据集的图像。</p></figure></div><p id="88f7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在弱光条件下拍摄的图像怎么样？通过相机管道的图像引入了噪声。我们能把它从照片上去掉吗？</p><div class="kg kh ki kj gt ab cb"><figure class="mq kk mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/93ec4122a284618f42cb81055b2ce4ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*-s7_7UyaqejkLjo32GGeBg.jpeg"/></div></figure><figure class="mq kk mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/9232a207ee09281e6823e059e189f0cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*sngDzYY6EdiujyGe0sRwCA.jpeg"/></div></figure><figure class="mq kk mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/9f2b13afd23abe360056c6e165434167.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*mffaHa7904jY71gzBCkcNg.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk mw di mx my translated">从左至右:原始图像、噪声图像和清洁图像。图片来自<a class="ae lr" href="https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/" rel="noopener ugc nofollow" target="_blank"> BSD </a>数据集。</p></figure></div><p id="38ac" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这些问题的答案是肯定的！深度学习方法取得了特别的成功。卷积神经网络(CNN)专门针对图像，特别是我在以前的<a class="ae lr" rel="noopener" target="_blank" href="/cnn-cheat-sheet-the-essential-summary-for-a-quick-start-58820a14d3b4">文章</a>中详细谈到的图像，经常被用于这项任务。</p><h2 id="14f4" class="mz lu iq bd lv na nb dn lz nc nd dp md le ne nf mf li ng nh mh lm ni nj mj nk bi translated">为什么我们需要一个感知损失函数？</h2><p id="4edb" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">影响图像恢复方法性能的因素之一是定义优化目标的损失函数。</p><p id="32e0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在图像恢复的情况下，目标是将受损图像恢复到<em class="ls">视觉上</em>与原始的未失真图像相匹配。因此，我们需要设计符合这一目标的损失。</p><p id="fc4b" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在解决这个问题时，我们承认开发完美恢复目标图像的方法可能是不可能的，因为重建问题本质上是不适定的，即对于任何失真的图像，都可能有多个感觉上令人满意的似乎合理的解决方案。</p><h1 id="3294" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">感知损失</h1><p id="b534" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">下面，我将损失函数分类为手工制作的损失，这些损失依赖于现有的度量；特征方面的损失，其中使用深度学习模型提取图像统计；和分布损失，其中损失将解决方案推向自然图像的流形。</p><h2 id="a037" class="mz lu iq bd lv na nb dn lz nc nd dp md le ne nf mf li ng nh mh lm ni nj mj nk bi translated">手工制作的损失</h2><p id="1894" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">直觉上，感知损失应该随着感知质量的增加而减少。因此，设计良好的感知损失函数的最初尝试着眼于提取简单的图像统计，并将它们用作损失函数中的分量。</p><p id="06dc" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，并不是所有的统计数据都是好的。例如，考虑一个标准损失项——L2。优化图像恢复方法以最小化平均每像素平方差会导致图像模糊。</p><p id="2ce9" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">问题在于当损失接近零时的形状。误差越接近零，梯度越小，这意味着与地面真实值的小偏差(这对锐度很重要)不会受到太多的惩罚。</p><p id="4185" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这是一个众所周知的问题很长一段时间与L1被用来作为一个更好的替代图像恢复。L1具有恒定的梯度，这意味着随着损耗接近零，梯度不会减小，从而产生看起来更清晰的图像。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/6f894ad87559da09debdfc53f48a5f6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nThsXS0pybKouKldAHE8vQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">用L2和L1损失训练超分辨率方法(EDSR)的结果。来自<a class="ae lr" href="https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/bsds/" rel="noopener ugc nofollow" target="_blank"> BSD </a>数据集的图像。</p></figure><p id="1a33" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae lr" href="https://arxiv.org/pdf/1511.08861.pdf" rel="noopener ugc nofollow" target="_blank">赵等人。艾尔。</a>使用L2、L1、SSIM和MS-SSIM(后两者是客观的图像质量度量)作为损失函数，研究了图像超分辨率、去噪和去马赛克算法产生的图像的视觉质量。由L1损失和MS-SSIM损失的组合训练的算法产生的图像达到了由客观质量度量测量的最佳质量。紧随这一结果的是单独使用的L1损失。</p><p id="90c8" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae lr" href="https://arxiv.org/abs/2005.01338" rel="noopener ugc nofollow" target="_blank">后期作品</a>比较了几种图像重建方法中用作损失函数的图像质量指标。他们发现，许多流行的图像质量评估指标不具有可以保证良好重建结果的属性。</p><p id="0380" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">缺点<br/> </strong>尽管L1能产生更清晰的图像，但它不会根据像素对感知质量的重要性来衡量像素。人类视觉系统被调整为聚焦于特定的、显著的区域，例如面部。因此，良好的感知损失也应该考虑到这一点。</p><h2 id="5dfe" class="mz lu iq bd lv na nb dn lz nc nd dp md le ne nf mf li ng nh mh lm ni nj mj nk bi translated">特征损失</h2><p id="b60b" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">一类新的损失函数，最近获得了显著的普及，采用神经网络作为特征提取器。最常见的是，损失被计算为经过训练的图像分类网络(例如<a class="ae lr" href="https://www.robots.ox.ac.uk/~vgg/publications/2015/Simonyan15/" rel="noopener ugc nofollow" target="_blank"> VGG网络</a>)的隐藏层的激活之间的L2距离。</p><p id="19ad" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">由于隐藏层中的池化，实现损失函数的网络通常不是双射的，这意味着对函数的不同输入可能导致相同的潜在表示。因此，基于特征的损失通常与正则化项(如L2或L1范数)结合使用，并且需要仔细调整每个损失分量的权重。</p><p id="bfa7" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><a class="ae lr" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank">最早的工作</a>使用从VGG网络提取的参考和测试图像的特征之间的L2范数作为损失函数来训练风格转移和超分辨率算法。在这里，VGG网络在<a class="ae lr" href="https://www.image-net.org" rel="noopener ugc nofollow" target="_blank"> ImageNet数据集</a>上被训练。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/82706d3704f500bd41c03f090c667ca0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nGxIUPAw5YxLXOq95Ex2bg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片由<a class="ae lr" href="https://arxiv.org/abs/1603.08155" rel="noopener ugc nofollow" target="_blank">约翰逊</a></p></figure><p id="3fb1" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">使用L2范数进行特征比较有些武断。后来的工作已经开发了替代方案来比较提取的表征。因此，这是一种专门针对风格转换的<a class="ae lr" href="https://arxiv.org/abs/1803.02077" rel="noopener ugc nofollow" target="_blank">语境损失</a>。</p><p id="71ca" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">缺点<br/> </strong>基于特征的损失有多个缺点——它们计算量大，需要正则化和超参数调整，涉及在不相关的任务上训练的大型网络，因此图像恢复任务的训练过程非常占用存储器。</p><h2 id="5af1" class="mz lu iq bd lv na nb dn lz nc nd dp md le ne nf mf li ng nh mh lm ni nj mj nk bi translated">分布损耗(GAN)</h2><p id="2068" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">由于许多图像恢复算法本质上是不适定的，例如，由超分辨率或去噪算法产生的图像可能具有可接受的感知质量，而不是精确地匹配地面实况，所以图像重建算法可以被优化以产生在自然图像流形上的图像，该图像受到与地面实况分布的相似性的约束。为了确保第一个要求得到满足，许多作品都依赖于<a class="ae lr" href="https://arxiv.org/abs/1406.2661" rel="noopener ugc nofollow" target="_blank">生成对抗网络(GAN)的</a>。</p><p id="62ec" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这样的设置中，图像生成算法具有几个损失项:被训练来区分生成图像和自然图像的鉴别器，以及一个或几个限制生成器网络产生接近地面真实的图像的损失项。</p><p id="15f6" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">典型的约束条件是L1规范，使训练正规化，如<a class="ae lr" href="https://arxiv.org/abs/1607.07539" rel="noopener ugc nofollow" target="_blank">叶</a>和<a class="ae lr" href="https://arxiv.org/abs/1611.07004" rel="noopener ugc nofollow" target="_blank">伊索拉</a>所示，可以产生相当好的效果。<a class="ae lr" href="https://arxiv.org/abs/1602.02644" rel="noopener ugc nofollow" target="_blank"> Dosovitsky </a>和<a class="ae lr" href="https://arxiv.org/abs/1609.04802" rel="noopener ugc nofollow" target="_blank"> Ledig </a>使用基于特征的VGG损失。其他一些作品使用了手工制作和特征损失相结合的方法。<a class="ae lr" href="https://arxiv.org/abs/2005.07502" rel="noopener ugc nofollow" target="_blank"> Tej </a>和<a class="ae lr" href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w31/Jo_Investigating_Loss_Functions_for_Extreme_Super-Resolution_CVPRW_2020_paper.pdf" rel="noopener ugc nofollow" target="_blank"> Jo </a>引入了基于鉴别器特征损失的正则化。</p><p id="5614" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><strong class="kx ir">缺点<br/> </strong> GANs确保生成的图像位于自然图像流形上，但当单独使用时，可能会导致图像与输入有很大不同，需要多个损失项和仔细的微调。此外，由于其优化问题的不稳定性，gan的训练也具有挑战性。</p><h2 id="638d" class="mz lu iq bd lv na nb dn lz nc nd dp md le ne nf mf li ng nh mh lm ni nj mj nk bi translated"><a class="ae lr" href="https://arxiv.org/abs/2103.14616" rel="noopener ugc nofollow" target="_blank">我们的工作</a></h2><p id="651c" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">在我们的工作中，我们观察到单个自然图像足以训练一个轻量级的特征提取器，该特征提取器在单个图像的超分辨率、去噪和JPEG伪影去除方面优于最先进的损失函数。</p><p id="9c65" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们提出了一种新的多尺度鉴别特征(MDF)损失，包括一系列鉴别器，训练以惩罚由生成器引入的错误。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/43ce63384cb33035425ccd84f6168a72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*deO49ZvBXXc_pS2zZFEYug.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">多尺度鉴别器。图片由作者提供。</p></figure><p id="8b00" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我们的损失函数的基础基于以下命题:</p><p id="5061" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="ls">命题1: </em>用作损耗特征提取器的网络应被训练成对发电机的恢复误差敏感。这使得特征空间更适合于在特定任务的训练期间惩罚失真。</p><p id="729e" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated"><em class="ls">命题2: </em>学习自然图像流形，这是通常归因于鉴别器的任务，是更困难的任务，并且与特征方式损失函数不太相关。</p><p id="a735" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了验证这两个命题，我们设计了一个新的特征损失。特征空间包括这组鉴别器网络的中间激活。鉴别器网络被训练为<a class="ae lr" href="https://arxiv.org/abs/1905.01164" rel="noopener ugc nofollow" target="_blank">单图像GAN </a>，其从种子图像中移除特定任务的失真(阶段1)。我们用y表示种子图像，以将其与用于学习恢复任务的用x表示的训练图像区分开。所提出的损失函数以多尺度方式训练，使得它对多尺度下的相关失真敏感。种子图像可以具有与训练图像不同的大小，可以描绘不同类型的场景，或者可以是合成图像。用于特定任务生成器的生成图像和测试图像的训练鉴别器的中间特征之间的L2范数被用作损失。</p><h2 id="d160" class="mz lu iq bd lv na nb dn lz nc nd dp md le ne nf mf li ng nh mh lm ni nj mj nk bi translated">比较性能</h2><p id="edbf" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">视觉内容的最终消费者是人类观察者。因此，如果有一组观察者来判断针对各种损失函数优化的图像重建算法所产生的图像质量，那就太好了。</p><p id="1fb0" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在我们的工作中，我们在亚马逊Mechanical Turk众包平台上进行感知实验。为了测试的最佳灵敏度，我们使用了完全设计<a class="ae lr" rel="noopener" target="_blank" href="/dataset-fusion-sushi-age-and-image-quality-and-what-the-hell-do-they-have-in-common-814e8dae7cf7">成对比较方案</a>。</p><p id="d770" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在我们的实验中，我们比较了四种图像恢复应用的损失函数:使用<a class="ae lr" href="https://arxiv.org/pdf/1609.04802.pdf" rel="noopener ugc nofollow" target="_blank"> SR-ResNet </a>的单幅图像超分辨率，使用<a class="ae lr" href="https://arxiv.org/abs/1707.02921" rel="noopener ugc nofollow" target="_blank"> EDSR </a>的单幅图像超分辨率，去噪和JPEG伪像去除。</p><p id="0eb4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">对于每个应用程序，我们都进行了两两比较实验，汇总了收集到的比较结果，并使用<a class="ae lr" href="https://arxiv.org/abs/1712.03686" rel="noopener ugc nofollow" target="_blank">这种方法</a>对结果进行了显著差异(JND)(瑟斯顿)衡量。缩放的结果用JND单位表示质量差异。一个JND单位意味着75%的人会(从一对方法中)选择一种方法。缩放的结果显示了我们的方法相对于其他损失函数的持续改进。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/b561d5de4302b840279e37131b2c8b46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_WNNmvENzYEb46ebnS2WHw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来自亚马逊MTurk的主观实验，以JND为单位(越高越好)。误差线表示95%的置信区间。LPIPS和VGG损失与L2损失一起使用。图片由作者提供。</p></figure><p id="a580" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">为了获得更深入的了解，下面我们将结果可视化为感知失真权衡，其在x轴上示出了失真(PSNR ),在y轴上示出了JND质量值(反向标度)。所有应用的结果清楚地表明，MDF损耗导致最低的失真和最高的感知质量。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/f1125539178e1116f6a6adc77753e67f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o8LCp9n5ZShQpTIBXkGx6A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">测试损失的感知失真权衡。轴已经颠倒，因此最低的失真显示在左边，最高的感知质量显示在底部。图片由作者提供。</p></figure><p id="b08d" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">上图还显示，传统的<a class="ae lr" rel="noopener" target="_blank" href="/deep-image-quality-assessment-30ad71641fac">客观图像质量评估指标</a>，如PSNR或NIQE，在预测图像的感知质量时可能不可靠。他们也没有抓住感知差异的实际意义；我们不知道0.5 dB的改善是否会被普通观察者所接受。</p><h2 id="903f" class="mz lu iq bd lv na nb dn lz nc nd dp md le ne nf mf li ng nh mh lm ni nj mj nk bi translated">定性结果</h2><div class="kg kh ki kj gt ab cb"><figure class="mq kk mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/18805ccd76b224570b297ee3ba200ec5.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*W_fion1pMLjrV_z4Qo9e5w.jpeg"/></div></figure><figure class="mq kk mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/f07af2b2800c87c8a4d3b96afcd04ecf.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*6PyPDVQ8MxBhz9SKpta5Vg.jpeg"/></div></figure><figure class="mq kk mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/4b294119e2a00bfdbc149c61cd304300.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*G0AyRY7yumbdNVaDlY-_Cw.jpeg"/></div></figure></div><div class="ab cb"><figure class="mq kk mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/361e0c4b346ddd6a522ea13b9ddd6141.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*c6K_NrusH4rJZxvH-7DwGA.jpeg"/></div></figure><figure class="mq kk mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/d1e189857d31f89fc2fee485ccefefca.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*u-rDiqR9CCYSBVef1ndSeA.jpeg"/></div></figure><figure class="mq kk mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/78d5df0624ef95071804069c372d5254.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*KyxTbr6UUQdzCO31adjr4w.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk mw di mx my translated">使用EDSR获得4倍单幅图像超分辨率的结果。从左上顺时针方向:原件、L2、L1、VGG+L1、MDF、MS-SSIM+1。来自BSD数据集的图像。</p></figure></div><div class="ab cb"><figure class="mq kk mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/925644b70413398ae7900452dcddfc11.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*EroNDiWhaFp9jsuVVQV6Og.jpeg"/></div></figure><figure class="mq kk mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/b50910053eb4307c8833b5031586c9a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*yzNnPVOXC3h4Tcklu_0llg.jpeg"/></div></figure><figure class="mq kk mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/f7532b4d8a69430bb2ab3ff0b6fcae04.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*p2Ct3F6EVkxqoclVchEINg.jpeg"/></div></figure></div><div class="ab cb"><figure class="mq kk mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/06c48a7631dd971153cb85a990b5348b.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*R69IjLSwo59P3sHNCuzaYw.jpeg"/></div></figure><figure class="mq kk mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/ccf2c657c5395686d1d3188f5c9fef37.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*V4-0Z0Z1HIgCtaLALrd9jw.jpeg"/></div></figure><figure class="mq kk mr ms mt mu mv paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/ab1bb54c924289f76ba24af34cf9ff90.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*YlC8yUOwAvaoYQB1j486Yg.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk mw di mx my translated">使用SR-ResNet获得4倍单幅图像超分辨率的结果。从左上顺时针方向:原件、L2、L1、VGG+L1、MDF、MS-SSIM+1。来自BSD数据集的图像。</p></figure></div><p id="ae74" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">想了解更多关于JPEG伪像去除和去噪的结果，请查看我们的<a class="ae lr" href="https://www.cl.cam.ac.uk/research/rainbow/projects/mdf/" rel="noopener ugc nofollow" target="_blank">项目页面</a>！</p><h1 id="8a5d" class="lt lu iq bd lv lw lx ly lz ma mb mc md jw me jx mf jz mg ka mh kc mi kd mj mk bi translated">摘要</h1><p id="37a5" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">设计有效的感知损失函数是具有挑战性的。尤其困难的是从人类观察者那里获得反馈，以判断图像生成方法产生的结果的质量——这既昂贵又耗时。</p><p id="4fff" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">然而，关于损失函数的多个发现已经被提出:好的感知损失不需要预测感知的图像质量，并且好的质量度量可能不会产生好的损失函数；应该小心选择用于比较特征方面损失的深度表示的度量；特定任务损失往往比一般损失表现更好。</p><p id="56cb" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">更多细节请看<a class="ae lr" href="https://arxiv.org/abs/2103.14616" rel="noopener ugc nofollow" target="_blank">我们最近的作品</a>，关于图像恢复的损失函数，记住我们这里也有<a class="ae lr" href="https://github.com/gfxdisp/mdf" rel="noopener ugc nofollow" target="_blank">代码</a>！</p><p id="bed4" class="pw-post-body-paragraph kv kw iq kx b ky kz jr la lb lc ju ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">如果你喜欢这篇文章，请与朋友分享！要阅读更多关于机器学习和图像处理的主题，请点击订阅！</p><h2 id="e70f" class="mz lu iq bd lv na nb dn lz nc nd dp md le ne nf mf li ng nh mh lm ni nj mj nk bi translated">喜欢作者？保持联系！</h2><p id="983b" class="pw-post-body-paragraph kv kw iq kx b ky ml jr la lb mm ju ld le mn lg lh li mo lk ll lm mp lo lp lq ij bi translated">我错过了什么吗？不要犹豫，直接在<a class="ae lr" href="https://www.linkedin.com/in/aliakseimikhailiuk/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或<a class="ae lr" href="https://twitter.com/mikhailiuka" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上给我留言、评论或发消息吧！</p><div class="nq nr gp gr ns nt"><a rel="noopener follow" target="_blank" href="/deep-image-quality-assessment-30ad71641fac"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd ir gy z fp ny fr fs nz fu fw ip bi translated">深度图像质量评估</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">深入研究全参考图像质量评估。从主观画质实验到深层客观…</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">towardsdatascience.com</p></div></div><div class="oc l"><div class="od l oe of og oc oh kp nt"/></div></div></a></div><div class="nq nr gp gr ns nt"><a rel="noopener follow" target="_blank" href="/cnn-cheat-sheet-the-essential-summary-for-a-quick-start-58820a14d3b4"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd ir gy z fp ny fr fs nz fu fw ip bi translated">卷积神经网络——概要</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">如何使用、何时使用以及提高性能的有用技巧</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">towardsdatascience.com</p></div></div><div class="oc l"><div class="oi l oe of og oc oh kp nt"/></div></div></a></div><div class="nq nr gp gr ns nt"><a rel="noopener follow" target="_blank" href="/deep-video-inpainting-756e60ddcaaf"><div class="nu ab fo"><div class="nv ab nw cl cj nx"><h2 class="bd ir gy z fp ny fr fs nz fu fw ip bi translated">深度视频修复</h2><div class="oa l"><h3 class="bd b gy z fp ny fr fs nz fu fw dk translated">用深度神经网络去除视频中不需要的对象。问题设置和最先进的审查。</h3></div><div class="ob l"><p class="bd b dl z fp ny fr fs nz fu fw dk translated">towardsdatascience.com</p></div></div><div class="oc l"><div class="oj l oe of og oc oh kp nt"/></div></div></a></div></div></div>    
</body>
</html>