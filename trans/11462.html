<html>
<head>
<title>Interpreting an LSTM through LIME</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">透过莱姆解读LSTM</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/interpreting-an-lstm-through-lime-e294e6ed3a03?source=collection_archive---------6-----------------------#2021-11-11">https://towardsdatascience.com/interpreting-an-lstm-through-lime-e294e6ed3a03?source=collection_archive---------6-----------------------#2021-11-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7e48" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">了解如何通过LIME解释Keras LSTM，并深入到文本分类器的LIME库的内部工作</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/dde66a160f7fb9d9f2fce5b80b8b7a83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4y85TxW9eyKIwmBf"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">格伦·卡丽在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><h1 id="1205" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">概述</h1><ul class=""><li id="bea6" class="lo lp iq lq b lr ls lt lu lv lw lx ly lz ma mb mc md me mf bi translated">我决定写这篇博客，因为我在那里找不到很多教如何在Keras模型中使用LIME的教程。<strong class="lq ir">当然，许多文章关注基于Sci-kit learn的模型，但是使用LIME和Keras需要我们编写一个额外的函数，正如我们将看到的。</strong></li><li id="7ef3" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb mc md me mf bi translated">其次，我想解释一下LIME是如何适应文本分类问题的，这是我在阅读该库的文本解释器代码时才理解的。</li></ul><p id="05dc" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">现在来看内容，这是我将要讲述的内容</p><ol class=""><li id="397a" class="lo lp iq lq b lr mn lt mp lv na lx nb lz nc mb nd md me mf bi translated">为什么我们需要解释模型</li><li id="31e5" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb nd md me mf bi translated">什么是石灰</li><li id="f44d" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb nd md me mf bi translated">LIME如何为文本工作</li><li id="4004" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb nd md me mf bi translated">为分类问题构造LSTM</li><li id="712f" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb nd md me mf bi translated">通过奇特的视觉解释来解读莱姆的LSTM</li></ol><h1 id="60c3" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">为什么我们需要解释我们的模型？</h1><p id="826c" class="pw-post-body-paragraph ml mm iq lq b lr ls jr mo lt lu ju mq lv ne ms mt lx nf mv mw lz ng my mz mb ij bi translated">有时，特别是在医疗保健和金融等关键领域，我们希望知道模型预测背后的推理。当误差范围太窄时，我们希望确保我们的模型像人类一样思考并做出逻辑决策。仅仅得到正确的答案是不够的，我们想知道我们的模型是如何找到答案的。假设我们的分类器正在预测情感，在这种情况下，我们希望确定预测是基于指示情感的词，如“快乐”和“可怕”的词，而不是不相关的词，如某人的名字，这些词对情感没有贡献。更重要的是，解释我们的模型可以帮助减少偏见，例如，我们可以看到我们的模型是否偏向某人的种族甚至性别。帮助我们解释模型的算法属于<strong class="lq ir">可解释人工智能(XAI)的大范畴。</strong></p><h1 id="0b32" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">石灰背后的直觉</h1><p id="2c9d" class="pw-post-body-paragraph ml mm iq lq b lr ls jr mo lt lu ju mq lv ne ms mt lx nf mv mw lz ng my mz mb ij bi translated">首先，我们根据可解释性对模型进行分类。像<strong class="lq ir">线性回归或(小)决策树</strong>这样的模型很容易被人类理解。然而，像<strong class="lq ir">神经网络和LSTMs </strong>这样的模型具有成千上万的权重和许多层，使得人类很难解释它们。</p><p id="cf40" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">下面是LIME解决这个问题的方法。下面的图表是来自原来的<a class="ae kv" href="https://arxiv.org/pdf/1602.04938.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lq ir">石灰</strong> </a>纸。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/d18723da6c971f72fef6c12919d46e69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1012/format:webp/0*9mKjkVrlNSk98mWr.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">石灰是如何工作的，正如最初的石灰论文中提到的[1]</p></figure><p id="0cdb" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">考虑上面是一个二元分类问题(红类和蓝类)</p><ol class=""><li id="d584" class="lo lp iq lq b lr mn lt mp lv na lx nb lz nc mb nd md me mf bi translated">弯曲的红色和蓝色区域表示我们原始模型<strong class="lq ir">(我们称之为黑盒模型)</strong>的决策空间</li><li id="4b15" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb nd md me mf bi translated">假设我们想要解释我们的模型关于放大实例<strong class="lq ir">(用红色加号表示)</strong>的决定。首先，我们在这个红色实例中及其周围创建样本。</li><li id="d673" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb nd md me mf bi translated">现在我们<strong class="lq ir">根据它们与我们想要解释的实例</strong>的接近程度对它们进行加权，然后<strong class="lq ir">使用我们的黑盒模型</strong>为这些实例生成预测。</li><li id="f50a" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb nd md me mf bi translated">现在我们有了新的本地合成数据和标签，我们在此数据上训练一个<strong class="lq ir">可解释的</strong> (LIME默认使用岭回归)模型。在训练时，我们更重视接近我们想要解释的实例的数据点</li><li id="cda3" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb nd md me mf bi translated">嘣！我们现在可以观察训练模型的<strong class="lq ir">权重，以获得关于影响黑盒模型预测的特征(及其值)的见解。</strong></li></ol><h1 id="09d5" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">LIME是如何处理文本数据的？</h1><ol class=""><li id="d3e1" class="lo lp iq lq b lr ls lt lu lv lw lx ly lz ma mb nd md me mf bi translated">给定一个句子，我们首先为这个句子构造一个<a class="ae kv" href="https://en.wikipedia.org/wiki/Bag-of-words_model" rel="noopener ugc nofollow" target="_blank"> <strong class="lq ir">单词包</strong> </a> (BoW)表示。</li><li id="702c" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb nd md me mf bi translated">现在，该库从原始句子中随机选择单词，并操纵该句子，以新的单词组合/顺序生成5000个句子。</li><li id="6ddd" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb nd md me mf bi translated">使用<a class="ae kv" href="https://en.wikipedia.org/wiki/Cosine_similarity" rel="noopener ugc nofollow" target="_blank"> <strong class="lq ir">余弦距离</strong> </a>对这些样本按照它们与原始句子的相似程度进行加权。</li><li id="5779" class="lo lp iq lq b lr mg lt mh lv mi lx mj lz mk mb nd md me mf bi translated">现在我们有了新的矢量化句子样本，并且知道了它们的接近度，LIME遵循上一节提到的相同过程。</li></ol><h1 id="ffb7" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">用石灰解读LSTM</h1><h2 id="d946" class="nj kx iq bd ky nk nl dn lc nm nn dp lg lv no np li lx nq nr lk lz ns nt lm nu bi translated">数据集</h2><p id="6d1b" class="pw-post-body-paragraph ml mm iq lq b lr ls jr mo lt lu ju mq lv ne ms mt lx nf mv mw lz ng my mz mb ij bi translated">我们将致力于来自<a class="ae kv" href="https://www.kaggle.com/sripaadsrinivasan/yelp-coffee-reviews" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>的Yelp咖啡评论数据集。我对数据进行了预处理和清理，并使其适应二元分类任务。你可以在这里<a class="ae kv" href="https://jovian.ai/rajbsangani/lime-medium" rel="noopener ugc nofollow" target="_blank"> <strong class="lq ir">查看全部代码。</strong>T25】</a></p><p id="b5ed" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">这是预处理和清理后数据集的样子</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nv nw l"/></div></figure><h2 id="32a5" class="nj kx iq bd ky nk nl dn lc nm nn dp lg lv no np li lx nq nr lk lz ns nt lm nu bi translated">模型</h2><p id="1511" class="pw-post-body-paragraph ml mm iq lq b lr ls jr mo lt lu ju mq lv ne ms mt lx nf mv mw lz ng my mz mb ij bi translated">我使用了一个LSTM模型，它有100维的隐藏状态，前面有一个32维的嵌入层。你可以在这里看到模型摘要。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nv nw l"/></div></figure><h2 id="d278" class="nj kx iq bd ky nk nl dn lc nm nn dp lg lv no np li lx nq nr lk lz ns nt lm nu bi translated">培训和结果</h2><p id="5afc" class="pw-post-body-paragraph ml mm iq lq b lr ls jr mo lt lu ju mq lv ne ms mt lx nf mv mw lz ng my mz mb ij bi translated">在仅仅训练了2个时期的模型之后，我们在训练数据上实现了非常高的准确度。此外，我们在测试数据上实现了类似的准确性，并获得了非常好的F1分数，这向我们表明，高准确性不仅仅是数据不平衡的结果。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nv nw l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nv nw l"/></div></figure><h1 id="c904" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">使用石灰文本解释器解释模型</h1><p id="4516" class="pw-post-body-paragraph ml mm iq lq b lr ls jr mo lt lu ju mq lv ne ms mt lx nf mv mw lz ng my mz mb ij bi translated">首先<code class="fe nx ny nz oa b">pip install lime</code></p><p id="3af3" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">现在使用我们的类标签实例化文本解释器。</p><p id="b0a4" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated"><strong class="lq ir">对于最重要的部分，由于我们的Keras模型不像sci-kit learn模型那样实现predict_proba函数，我们需要手动创建一个</strong>。这是你怎么做的。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nv nw l"/></div></figure><p id="729b" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">只需使用预处理和标记化步骤，并使用模型的预测返回一个维度数组(输入大小X num__target_classes [在我们的例子中为2])</p><h1 id="5a58" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">结果</h1><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="nv nw l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ob"><img src="../Images/68ecabec4557dd0ac6ca7cd614cbaae7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zQS5eeDNVMp9IIbAghFnHg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">对给定句子的解释(图片由作者提供)</p></figure><p id="d315" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">首先，我们注意到<strong class="lq ir">的实际评分是2/5，这不是一个很好的评价。</strong> <strong class="lq ir">我们的模型正确地将带有负面情绪的输出分类为0.82 </strong>。现在，当我们看到消极和积极的词语时，我们可以看到像meh和pricing这样的<strong class="lq ir">词语有助于消极情绪</strong>，而像nice、vegan和fancy这样的词语有助于积极情绪。您在这里看到的权重(针对每个突出显示的单词)是使用我们的本地(可解释)模型计算的。</p><h1 id="effc" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">结论</h1><p id="ab1b" class="pw-post-body-paragraph ml mm iq lq b lr ls jr mo lt lu ju mq lv ne ms mt lx nf mv mw lz ng my mz mb ij bi translated">我们看到了如何通过自定义的predict_proba函数使用LIME解释Keras模型。然而，请注意，酸橙并非没有缺点，正如我们可以看到的,<strong class="lq ir"> little </strong>被误归类为阳性，而<strong class="lq ir"> food </strong>被归类为阳性，而它根本不应该被突出显示。(至少在我看来)。LIME仍然强大到足以解释像这样的简单问题，甚至可以用来生成全局解释<strong class="lq ir">(而不是单独解释每个实例)</strong>。</p><p id="c367" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">查看我的<a class="ae kv" href="https://github.com/rajlm10" rel="noopener ugc nofollow" target="_blank"> <strong class="lq ir"> GitHub </strong> </a>其他一些项目。可以联系我<a class="ae kv" href="https://rajsangani.me/" rel="noopener ugc nofollow" target="_blank"> <strong class="lq ir"> <em class="nh">这里</em> </strong> </a> <strong class="lq ir"> <em class="nh">。</em> </strong>感谢您的宝贵时间！</p><p id="3773" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">我将把它留给另一篇文章。如果你喜欢这个，这里还有一些！</p><div class="oc od gp gr oe of"><a rel="noopener follow" target="_blank" href="/locality-sensitive-hashing-in-nlp-1fb3d4a7ba9f"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd ir gy z fp ok fr fs ol fu fw ip bi translated">自然语言处理中的位置敏感哈希算法</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">关于如何通过区分位置来减少搜索空间以加快文档检索的实践教程…</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">towardsdatascience.com</p></div></div><div class="oo l"><div class="op l oq or os oo ot kp of"/></div></div></a></div><div class="oc od gp gr oe of"><a rel="noopener follow" target="_blank" href="/dealing-with-features-that-have-high-cardinality-1c9212d7ff1b"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd ir gy z fp ok fr fs ol fu fw ip bi translated">处理具有高基数的要素</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">一个简单的实用程序，我用来处理具有许多唯一值的分类特征</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">towardsdatascience.com</p></div></div><div class="oo l"><div class="ou l oq or os oo ot kp of"/></div></div></a></div><div class="oc od gp gr oe of"><a rel="noopener follow" target="_blank" href="/regex-essential-for-nlp-ee0336ef988d"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd ir gy z fp ok fr fs ol fu fw ip bi translated">正则表达式对NLP至关重要</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">理解各种正则表达式，并将其应用于自然语言中经常遇到的情况…</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">towardsdatascience.com</p></div></div><div class="oo l"><div class="ov l oq or os oo ot kp of"/></div></div></a></div><div class="oc od gp gr oe of"><a rel="noopener follow" target="_blank" href="/powerful-text-augmentation-using-nlpaug-5851099b4e97"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd ir gy z fp ok fr fs ol fu fw ip bi translated">使用NLPAUG的强大文本增强！</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">通过文本增强技术处理NLP分类问题中的类别不平衡</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">towardsdatascience.com</p></div></div><div class="oo l"><div class="ow l oq or os oo ot kp of"/></div></div></a></div><h1 id="f6fa" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">参考</h1><p id="d8a7" class="pw-post-body-paragraph ml mm iq lq b lr ls jr mo lt lu ju mq lv ne ms mt lx nf mv mw lz ng my mz mb ij bi translated">[1]里贝罗，M. T .，辛格，s .，&amp; Guestrin，C. (2016年8月)。“我为什么要相信你？”解释任何分类器的预测。《第22届ACM SIGKDD知识发现和数据挖掘国际会议论文集》(第1135-1144页)。</p><p id="c83f" class="pw-post-body-paragraph ml mm iq lq b lr mn jr mo lt mp ju mq lv mr ms mt lx mu mv mw lz mx my mz mb ij bi translated">石灰码:<a class="ae kv" href="https://github.com/marcotcr/lime" rel="noopener ugc nofollow" target="_blank">https://github.com/marcotcr/lime</a></p></div></div>    
</body>
</html>