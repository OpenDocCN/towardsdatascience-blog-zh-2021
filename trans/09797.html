<html>
<head>
<title>How to Build a WordPiece Tokenizer For BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何为BERT构建一个词块标记器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-build-a-wordpiece-tokenizer-for-bert-f505d97dddbb?source=collection_archive---------1-----------------------#2021-09-14">https://towardsdatascience.com/how-to-build-a-wordpiece-tokenizer-for-bert-f505d97dddbb?source=collection_archive---------1-----------------------#2021-09-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="86ba" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="532e" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">从头开始构建BertTokenizer的简单指南</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/c1e32a7c4f815a70d7252d4969dc6cff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XnnzX9IwCq_J7wf7_csiag.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="7d23" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi md translated">对于许多更具体的用例来说，从头构建一个转换器模型通常是唯一的选择。尽管BERT和其他transformer模型已经针对许多语言和领域进行了预训练，但它们并没有涵盖所有内容。</p><p id="9890" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">通常，这些不太常见的用例会从有人来构建特定的transformer模型中获益最多。它可能是一种不常用的语言或者一个不太懂技术的领域。</p><p id="22b1" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">BERT是一系列基于语言的机器学习中最受欢迎的转换器——从情感分析到问答。BERT已经实现了跨越许多边界和行业的多样化创新。</p><p id="771a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">对于许多人来说，设计新的BERT模型的第一步是记号赋予器。在这篇文章中，我们将看看BERT使用的WordPiece tokenizer看看我们如何从头开始构建自己的WordPiece tokenizer。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="mn mo l"/></div></figure></div><div class="ab cl mp mq hx mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="im in io ip iq"><h1 id="36f6" class="mw mx it bd my mz na nb nc nd ne nf ng ki nh kj ni kl nj km nk ko nl kp nm nn bi translated">文字片</h1><p id="c9af" class="pw-post-body-paragraph lh li it lj b lk no kd lm ln np kg lp lq nq ls lt lu nr lw lx ly ns ma mb mc im bi translated">BERT使用所谓的<strong class="lj jd">单词块</strong>记号赋予器。它的工作原理是将单词拆分成完整的形式(例如，一个单词成为一个标记)，或者拆分成<em class="mm">单词片段</em>——其中一个单词可以拆分成多个标记。</p><p id="2582" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">一个有用的例子是我们有多种形式的单词。例如:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nt mo l"/></div></figure><p id="a87a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">通过将单词拆分成单词块，我们已经识别出单词<code class="fe nu nv nw nx b">"surfboard"</code>和<code class="fe nu nv nw nx b">"snowboard"</code>通过单词块<code class="fe nu nv nw nx b">"##board"</code>共享含义，我们甚至没有对我们的令牌进行编码，也没有通过BERT以任何方式对它们进行处理。</p><p id="fb65" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">使用单词片段允许BERT轻松识别相关单词，因为它们<em class="mm">通常</em>共享一些相同的输入标记，然后这些标记被输入到BERT的第一层。</p><p id="bc72" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><em class="mm">作为旁注，还有许多其他的transformer记号赋予器——比如SentencePiece或者流行的</em> <a class="ae ny" rel="noopener" target="_blank" href="/transformers-from-scratch-creating-a-tokenizer-7d7418adb403"> <em class="mm">字节级字节对编码(BPE)记号赋予器</em> </a> <em class="mm">。它们各有利弊，但是最初的BERT使用的是单词块标记器。</em></p></div><div class="ab cl mp mq hx mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="im in io ip iq"><h1 id="247f" class="mw mx it bd my mz na nb nc nd ne nf ng ki nh kj ni kl nj km nk ko nl kp nm nn bi translated">构建标记器</h1><p id="8ecd" class="pw-post-body-paragraph lh li it lj b lk no kd lm ln np kg lp lq nq ls lt lu nr lw lx ly ns ma mb mc im bi translated">当构建一个新的标记器时，我们需要大量的非结构化语言数据。我的首选是OSCAR语料库——一个巨大的多语言数据集，涵盖了166种不同的语言。</p><p id="59e0" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然而，那里有许多数据集。HuggingFace的<code class="fe nu nv nw nx b">datasets</code>库也提供了对其中大部分内容的便捷访问。我们可以看到使用Python有多少:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nt mo l"/></div></figure><p id="1bd4" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">一个很酷的<strong class="lj jd"> 1306 </strong>数据集。其中许多也非常庞大——OSCAR本身被分成166种语言，OSCAR的许多“部分”包含万亿字节的数据。</p><p id="9c8f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们可以用HF的<code class="fe nu nv nw nx b">datasets</code>下载奥斯卡意大利语语料库。但是，我们应该小心，因为完整的数据集包含11.3亿个样本。总共约69GB的数据。HF允许我们使用<code class="fe nu nv nw nx b">split</code>参数指定我们只想要完整数据集的<em class="mm">部分</em>。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nt mo l"/></div></figure><p id="f822" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在我们的<code class="fe nu nv nw nx b">split</code>参数中，我们已经指定我们想要来自<code class="fe nu nv nw nx b">train</code>数据集的第一个<code class="fe nu nv nw nx b">2000000</code>样本(大多数数据集被组织成<code class="fe nu nv nw nx b">train</code>、<code class="fe nu nv nw nx b">validation</code>和<code class="fe nu nv nw nx b">test</code>集合)。尽管这仍然会下载完整的<code class="fe nu nv nw nx b">train</code>集——它将被缓存在本地以备将来使用。</p><p id="14a4" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们可以通过将<code class="fe nu nv nw nx b">streaming=True</code>参数添加到<code class="fe nu nv nw nx b">load_dataset</code>来避免下载和缓存整个数据集——在这种情况下<code class="fe nu nv nw nx b">split</code>必须设置为<code class="fe nu nv nw nx b">"train"</code>(没有<code class="fe nu nv nw nx b">[:2000000]</code>)。</p><h2 id="0015" class="nz mx it bd my oa ob dn nc oc od dp ng lq oe of ni lu og oh nk ly oi oj nm iz bi translated">数据格式编排</h2><p id="0a86" class="pw-post-body-paragraph lh li it lj b lk no kd lm ln np kg lp lq nq ls lt lu nr lw lx ly ns ma mb mc im bi translated">下载完数据后，我们必须将其重新格式化为简单的明文文件，每个样本之间用一个换行符隔开。将每个样本存储在一个文件中会创建一个巨大的文本文件。所以，我们把它们分成许多份。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nt mo l"/></div></figure><h2 id="6877" class="nz mx it bd my oa ob dn nc oc od dp ng lq oe of ni lu og oh nk ly oi oj nm iz bi translated">培养</h2><p id="60d2" class="pw-post-body-paragraph lh li it lj b lk no kd lm ln np kg lp lq nq ls lt lu nr lw lx ly ns ma mb mc im bi translated">一旦我们保存了所有简单的、换行符分隔的明文文件——我们继续训练我们的记号赋予器！</p><p id="9efc" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们首先使用<code class="fe nu nv nw nx b">pathlib</code>创建一个包含所有明文文件的列表。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nt mo l"/></div></figure><p id="8716" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然后，我们初始化并训练分词器。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nt mo l"/></div></figure><p id="a087" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这里有几个重要的参数需要注意，在初始化期间，我们有:</p><ul class=""><li id="f792" class="ok ol it lj b lk ll ln lo lq om lu on ly oo mc op oq or os bi translated"><code class="fe nu nv nw nx b">clean_text</code> —通过删除控制字符并用空格替换所有空白来清除文本。</li><li id="3f40" class="ok ol it lj b lk ot ln ou lq ov lu ow ly ox mc op oq or os bi translated"><code class="fe nu nv nw nx b">handle_chinese_chars</code> —标记器是否包括汉字周围的空格(如果在数据集中找到)。</li><li id="190f" class="ok ol it lj b lk ot ln ou lq ov lu ow ly ox mc op oq or os bi translated"><code class="fe nu nv nw nx b">stripe_accents</code> —我们是否删除重音，何时<code class="fe nu nv nw nx b">True</code>这会使é → e、ⅳ→o等。</li><li id="ee8d" class="ok ol it lj b lk ot ln ou lq ov lu ow ly ox mc op oq or os bi translated"><code class="fe nu nv nw nx b">lowercase</code> —如果<code class="fe nu nv nw nx b">True</code>标记器将大写和小写字符视为相等；A == a，B == b，等等。</li></ul><p id="843f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在培训期间，我们使用:</p><ul class=""><li id="3838" class="ok ol it lj b lk ll ln lo lq om lu on ly oo mc op oq or os bi translated"><code class="fe nu nv nw nx b">vocab_size</code> —我们的记号赋予器中记号的数量。在稍后的文本标记化过程中，未知单词将被分配一个不理想的<code class="fe nu nv nw nx b">[UNK]</code>标记。我们应该尽可能减少这种情况。</li><li id="6231" class="ok ol it lj b lk ot ln ou lq ov lu ow ly ox mc op oq or os bi translated"><code class="fe nu nv nw nx b">min_frequency</code> —一对令牌被合并的最小频率。</li><li id="f926" class="ok ol it lj b lk ot ln ou lq ov lu ow ly ox mc op oq or os bi translated"><code class="fe nu nv nw nx b">special_tokens</code>—BERT使用的特殊令牌列表。</li><li id="9208" class="ok ol it lj b lk ot ln ou lq ov lu ow ly ox mc op oq or os bi translated"><code class="fe nu nv nw nx b">limit_alphabet</code> —不同字符的最大数量。</li><li id="afee" class="ok ol it lj b lk ot ln ou lq ov lu ow ly ox mc op oq or os bi translated"><code class="fe nu nv nw nx b">workpieces_prefix</code> —添加到单词的<em class="mm">片段</em>的前缀(就像我们前面例子中的<code class="fe nu nv nw nx b"><strong class="lj jd">##</strong>board</code>)。</li></ul><p id="6e8c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在我们完成训练后，剩下的就是保存我们闪亮的新标记器。我们使用<code class="fe nu nv nw nx b">save_model</code>方法来实现这一点——指定一个目录来保存我们的标记器和标记器名称:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nt mo l"/></div></figure><p id="6c76" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">就这样，我们构建并保存了我们的BERT tokenizer。在我们的记号赋予器目录中<em class="mm">应该</em>找到一个文件— <code class="fe nu nv nw nx b">vocab.txt</code>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oy"><img src="../Images/2c7c969305b62e764d8c342b30b4eec0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NSCvbuBWLmpLd3cT_H0KLQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd oz"> vocab.txt </strong>文件的屏幕截图——我们新的令牌化器文本到令牌ID的映射。</p></figure><p id="f3d5" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在标记化过程中,<code class="fe nu nv nw nx b">vocab.txt</code>用于将文本映射到标记，然后根据<code class="fe nu nv nw nx b">vocab.txt</code>中标记的行号将标记映射到标记id——这些id然后被输入到BERT中！</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pa"><img src="../Images/d18b901ed8815ca04b9d564da1a7da16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1EpZpf1cSj9E2FabLQ2t7A.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><strong class="bd oz"> vocab.txt </strong>的一小部分，显示令牌及其令牌id(如行号)。</p></figure></div><div class="ab cl mp mq hx mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="im in io ip iq"><h1 id="5c5c" class="mw mx it bd my mz na nb nc nd ne nf ng ki nh kj ni kl nj km nk ko nl kp nm nn bi translated">符号化</h1><p id="660b" class="pw-post-body-paragraph lh li it lj b lk no kd lm ln np kg lp lq nq ls lt lu nr lw lx ly ns ma mb mc im bi translated">现在我们有了记号赋予器；我们可以继续使用<code class="fe nu nv nw nx b">from_pretrained</code>加载它，就像我们使用任何其他标记器一样，我们必须指定保存标记器的本地目录。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nt mo l"/></div></figure><p id="cdc3" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们也像往常一样标记:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nt mo l"/></div></figure><p id="8cdf" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这里我们返回大多数BERT任务需要的三个张量，<code class="fe nu nv nw nx b">input_ids</code>、<code class="fe nu nv nw nx b">token_type_ids</code>和<code class="fe nu nv nw nx b">attention_mask</code>。我们可以看到由<code class="fe nu nv nw nx b">2</code>代表的初始<code class="fe nu nv nw nx b">[CLS]</code>令牌和由<code class="fe nu nv nw nx b">3</code>代表的最终<code class="fe nu nv nw nx b">[SEP]</code>令牌。</p><p id="a8a2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">由于我们的<code class="fe nu nv nw nx b">vocab.txt</code>文件包含我们的令牌和令牌id的映射(例如，行号)——我们可以通过将我们的<code class="fe nu nv nw nx b">input_ids</code>令牌id与<code class="fe nu nv nw nx b">vocab.txt</code>中的行对齐来访问令牌:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nt mo l"/></div></figure><p id="8e87" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们试试另一个——如果你在意大利，这是一个不错的选择:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nt mo l"/></div></figure><p id="41fe" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">最后，让我们对将要拆分成多个单词片段的内容进行标记:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nt mo l"/></div></figure><p id="0b05" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这就是我们构建和应用意大利Bert tokenizer所需的一切！</p></div><div class="ab cl mp mq hx mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="im in io ip iq"><p id="49ed" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这就是本文的全部内容，涵盖了为BERT定制的单词块标记器的构建过程。</p><p id="594a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我希望你喜欢它！如果你有任何问题，请通过<a class="ae ny" href="https://twitter.com/jamescalam" rel="noopener ugc nofollow" target="_blank"> Twitter </a>或在下面的评论中告诉我。如果你想要更多这样的内容，我也会在<a class="ae ny" href="https://www.youtube.com/c/jamesbriggs" rel="noopener ugc nofollow" target="_blank"> YouTube </a>上发布。</p><p id="e1dd" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">感谢阅读！</p></div><div class="ab cl mp mq hx mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="im in io ip iq"><p id="d89a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><a class="ae ny" href="https://bit.ly/nlp-transformers" rel="noopener ugc nofollow" target="_blank">🤖《变形金刚》NLP课程70%的折扣</a></p></div><div class="ab cl mp mq hx mr" role="separator"><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu mv"/><span class="ms bw bk mt mu"/></div><div class="im in io ip iq"><h1 id="b4a1" class="mw mx it bd my mz na nb nc nd ne nf ng ki nh kj ni kl nj km nk ko nl kp nm nn bi translated">参考</h1><p id="448e" class="pw-post-body-paragraph lh li it lj b lk no kd lm ln np kg lp lq nq ls lt lu nr lw lx ly ns ma mb mc im bi translated"><a class="ae ny" href="https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.pre_tokenizers" rel="noopener ugc nofollow" target="_blank"> HuggingFace Tokenizers文档</a></p></div></div>    
</body>
</html>