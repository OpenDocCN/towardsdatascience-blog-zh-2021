<html>
<head>
<title>Which model should you choose? BigBird, ConvBERT, Longformer…</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你应该选择哪种型号？大鸟，康伯特，朗福尔曼…</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/advancing-over-bert-bigbird-convbert-dynabert-bca78a45629c?source=collection_archive---------11-----------------------#2021-05-18">https://towardsdatascience.com/advancing-over-bert-bigbird-convbert-dynabert-bca78a45629c?source=collection_archive---------11-----------------------#2021-05-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="cff3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">超过BERT的进展..</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/229f1d6c7377067bd76859e5342609cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FPRp1F1dtzB7m-84rxskyQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图像<a class="ae ky" href="https://unsplash.com/photos/wb85DEFXeRg" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="9c4c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di"> B </span> ERT及其变体也在2021年席卷了NLP领域。在NeurIPS2020和ICLR2020中分别有超过20篇关于BERT或Transformers的文章，并且这一趋势还在继续！在本帖中，我们将分析哪些最新的进步是必须的，以及它们何时有用。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><blockquote class="ml mm mn"><p id="9bf4" class="kz la mo lb b lc ld ju le lf lg jx lh mp lj lk ll mq ln lo lp mr lr ls lt lu im bi translated">对BERT的最初改进要么增加了数据，要么提高了计算能力，从而超越了它。然而，最近，从StructBERT和ALBERT开始，模型在概念和架构上已经超越了BERT。</p></blockquote><p id="bc4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di"> S </span> <strong class="lb iu"> tructBERT </strong>:又名爱丽丝，将<strong class="lb iu">语言结构</strong>融入预训练过程。具体来说，它引入了一个新的目标函数，称为WSO(词结构目标)。WSO旨在预测句子中单词的顺序。这是通过屏蔽15%的记号来实现的，类似于BERT，但是随后训练以预测混洗的15%记号的正确位置。结果，模型开始学习单词的排序！他们还引入了SSO(句子结构目标)，这是BERT的下一句预测(NSP任务)的修改版本，用于预测下一句和上一句，再次提高了模型的排序能力。这是在使用<strong class="lb iu">可比数据和计算能力的同时<strong class="lb iu">优于BERT </strong>的第一种方法。</strong></p><p id="9a78" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> ALBERT: </strong>是相对于BERT的第一个<strong class="lb iu">架构改进</strong>，使用18倍小的模型在5个NLU任务上胜过BERT。尺寸<strong class="lb iu">的减小是通过两个进步实现的。首先，遵循共享参数以减少自由变量的思想，ALBERT在跨层注意力地图中引入了<strong class="lb iu">参数共享</strong>，减少了大约7000万个冗余参数。其次，ALBERT对巨大的令牌嵌入矩阵执行<strong class="lb iu">低维投影</strong>，减少了多达2000万个附加参数。由于过度拟合的机会有限，ALBERT还消除了丢失现象，从而降低了内存消耗。</strong></p><p id="6954" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，这两种方法都只能得出一个近似的解决方案，因此与BERT相比，ALBERT的性能会降低1–2%。为了避免这一点，ALBERT引入了<strong class="lb iu">句子顺序预测</strong>(瞧！)<strong class="lb iu"> </strong>任务类似于StructBERT，增加了比BERT多10倍的数据(我们以前也见过！).由于尺寸减小，但数据和近似设置增加，在消耗相当数量的计算资源时，ALBERT的性能优于BERT。</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><blockquote class="ml mm mn"><p id="3662" class="kz la mo lb b lc ld ju le lf lg jx lh mp lj lk ll mq ln lo lp mr lr ls lt lu im bi translated">网络宽度/深度适配？是啊！也许比蒸馏还要好！</p><p id="d634" class="kz la mo lb b lc ld ju le lf lg jx lh mp lj lk ll mq ln lo lp mr lr ls lt lu im bi translated">——谷歌，微软，华为。</p></blockquote><p id="065e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di"> D </span> <strong class="lb iu"> ynaBERT </strong>:取得显著进步。由华为推出的DynaBERT是一款宽度和深度自适应模型，旨在降低计算成本。虽然这个想法对深度学习来说并不新鲜，但结果是有影响的。DynaBERT几乎总是比DistilBERT的计算成本低2-3倍，但却能获得相当的精度！</p><p id="6fca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从技术上来说，DynaBERT通过解耦来并行化多个注意力头和层的计算。这使得调整模型的宽度变得很容易，只需要调整头部和层数就可以了！然后，DynaBERT继续通过在训练期间计算重要性分数来自动识别和移除最不重要的头/层。使用臭名昭著的学生-教师方法来训练可变深度模型。因此，DynaBERT的性能至少可以与相同或更小型号的BERT相媲美，而且它的性能确实优于DistilBERT。(<a class="ae ky" href="https://proceedings.neurips.cc/paper/2020/file/6f5216f8d89b086c18298e043bfe48ed-Paper.pdf" rel="noopener ugc nofollow" target="_blank">条</a>)</p><p id="5bf9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">漏斗变压器</strong>:在类似的一行，漏斗变压器，确实如其名。它汇集输入，就像自动编码器逐渐将隐藏状态压缩成更短的序列。压缩跨三个块进行，其中每个块都是一组大小相同的层。从一个块到下一个块，隐藏单元的数量(序列长度)通过平滑的池层减少。这消除了BERT层中众所周知的冗余，最终性能当然比BERT更好！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/c323e0e725bfdd960ee1724f9df03cc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vPOUtsUjn0eGcY8nbQVDmA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">漏斗变压器，图片来自<a class="ae ky" href="https://proceedings.neurips.cc/paper/2020/file/2cd2915e69546904e4e5d4a2ac9e1652-Paper.pdf" rel="noopener ugc nofollow" target="_blank">文章</a></p></figure><p id="ded6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">微软在NeurIPS 2020上的另一项<a class="ae ky" href="https://proceedings.neurips.cc/paper/2020/file/a1140a3d0df1c81e24ae954d935e8926-Paper.pdf" rel="noopener ugc nofollow" target="_blank">工作</a>通过简单的网络大小调整，在相同或更高的精度下，将BERT的训练速度提高了2.5倍。有趣的是，他们提出I)在多头之前移动层范数，以允许更高的学习速率。ii)帮助绕过子层的选通机制iii)层丢弃时间表。概念上类似于宽度/深度适配！</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><blockquote class="ml mm mn"><p id="2708" class="kz la mo lb b lc ld ju le lf lg jx lh mp lj lk ll mq ln lo lp mr lr ls lt lu im bi translated">一系列的努力引入了新颖的关注层，使变压器更具成本效益。这些方法减少了注意矩阵中的条目。</p><p id="1047" class="kz la mo lb b lc ld ju le lf lg jx lh mp lj lk ll mq ln lo lp mr lr ls lt lu im bi translated">其中最突出的是:</p></blockquote><p id="ff1d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated"><span class="l lw lx ly bm lz ma mb mc md di"> B </span>直觉是，对角线上的注意力是局部注意力，两边是远距离注意力(称为全局注意力)，随机位置有助于更好地逼近矩阵。不错——对！</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/cb418c8393fae409f215b34f75e6de58.png" data-original-src="https://miro.medium.com/v2/resize:fit:498/format:webp/1*75CX8VspTdvGHiUksu5phg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://arxiv.org/pdf/2007.14062.pdf" rel="noopener ugc nofollow" target="_blank">文章</a></p></figure><p id="9096" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，BigBird在BERT的基础上实现了5–10%的准确性改进，并显示在4096长度的序列上，这是BERT的8倍长的序列。实现这一点的一个挑战是在GPU/TPU上计算稀疏运算的能力。这通过将稀疏表示转换成块并计算这些块中的关注度以及再次重构回矩阵来解决。对于序列长度，该模型以O(L)进行缩放。注意:早些时候Google也引入了一个<a class="ae ky" href="https://arxiv.org/pdf/2004.08483.pdf" rel="noopener ugc nofollow" target="_blank"> ETC </a>变体，如果你有一个捕捉单词之间语义关系的图表，这个变体会很有用。</p><p id="9779" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> Longformer </strong>:由艾伦研究所(Allen Institute)计算矩阵内部对角线、边和边的投影中的注意力。该模型在长达20K令牌的序列上进行训练和评估，并且标度为O(L)！因此能够执行文档级任务。</p><p id="c7f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="mo"> Longformer和BigBird的性能相当接近，BigBird的性能提高了1–3 %,但是它使用的计算能力是Longformer的16倍！</em></p><p id="73d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">重整器:</strong>用LSH注意力代替<strong class="lb iu"> </strong>变压器的二次点积注意力。简而言之，LSH(局部敏感散列)注意力使用基于散列的记号分组来执行本地化的点积，这允许点积矩阵中高值的良好质量近似。重要的是，这意味着重整器只需要O(L log(L))而不是O(L)复杂度，从而产生显著的加速。模型显示在30K标记大小的序列上，对于更大的序列长度，速度提高了5-10倍。然而，与BigBird和Longformer不同，对于较小的序列，成本效率优势是最小的。</p><p id="9a1c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> ConvBERT: </strong>使用跨度级动态卷积改进BERT。它使用卷积核来捕获单词之间的局部相似性，然后将它们纳入自我注意，以创建混合注意块。由于受到新的关注，ConvBERT模型的性能提高了5–6 %,计算成本效率提高了5倍。(<a class="ae ky" href="https://proceedings.neurips.cc/paper/2020/file/96da2f590cd7246bbde0051047b0d6f7-Paper.pdf" rel="noopener ugc nofollow" target="_blank">条</a>)</p></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><h1 id="7410" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">那么，用哪一个呢？</h1><p id="a0ed" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">是的，我们现在有了比伯特更受欢迎的型号。</p><blockquote class="ml mm mn"><p id="91ec" class="kz la mo lb b lc ld ju le lf lg jx lh mp lj lk ll mq ln lo lp mr lr ls lt lu im bi translated"><strong class="lb iu">长文</strong>:大鸟胜，可比回落:Longformer</p><p id="ad87" class="kz la mo lb b lc ld ju le lf lg jx lh mp lj lk ll mq ln lo lp mr lr ls lt lu im bi translated"><strong class="lb iu">更小的网络/速度</strong> : DynaBERT、ConvBERT</p><p id="0d7d" class="kz la mo lb b lc ld ju le lf lg jx lh mp lj lk ll mq ln lo lp mr lr ls lt lu im bi translated"><strong class="lb iu">多语言</strong>:如果计算机不是问题，XLM-罗伯塔，否则就是姆伯特</p></blockquote></div><div class="ab cl me mf hx mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="im in io ip iq"><p id="f2db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是BERT技术进步的第2部分。参见第一部分的这篇博文。</p><p id="dfe0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以通过@ LinkedIn联系到作者。</p></div></div>    
</body>
</html>