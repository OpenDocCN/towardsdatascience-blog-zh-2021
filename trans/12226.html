<html>
<head>
<title>Implement Policy Iteration in Python — A Minimal Working Example</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Python 实现策略迭代——一个最小的工作示例</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implement-policy-iteration-in-python-a-minimal-working-example-6bf6cc156ca9?source=collection_archive---------2-----------------------#2021-12-12">https://towardsdatascience.com/implement-policy-iteration-in-python-a-minimal-working-example-6bf6cc156ca9?source=collection_archive---------2-----------------------#2021-12-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="c169" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解这个经典的动态规划算法，以优化解决马尔可夫决策过程模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c3da89b36ecabf958b0ecdc43332be23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*QwsjZuZp9S7QUxbf"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@homajob?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">斯科特·格雷厄姆</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="1439" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">几天前我写了一篇关于<a class="ae ky" rel="noopener" target="_blank" href="/implement-value-iteration-in-python-a-minimal-working-example-f638907f3437">价值迭代</a>(理查德·贝尔曼，1957)，今天是政策迭代的时候了(罗纳德·霍华德，1960)。<strong class="li iu">策略迭代</strong>是求解马尔可夫决策过程模型的精确算法，保证找到最优策略。与值迭代相比，一个好处是有一个明确的停止标准——一旦策略稳定，它就是可证明的最优策略。然而，对于具有许多状态的问题，它通常具有较高的计算负担。</p><p id="8ce1" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">策略迭代是许多现代强化学习算法的基础，更具体地说是策略近似算法的<a class="ae ky" rel="noopener" target="_blank" href="/the-four-policy-classes-of-reinforcement-learning-38185daa6c8a">类。事实上，bertsekis(1996)将其解释为一个行动者-批评家模型，将政策更新与价值函数相结合。在讨论最接近的策略优化和自然策略梯度之前，先掌握这个基本算法是有意义的。</a></p><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/the-four-policy-classes-of-reinforcement-learning-38185daa6c8a"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">强化学习的四个策略类别</h2><div class="mm l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mn l"><div class="mo l mp mq mr mn ms ks mf"/></div></div></a></div><h1 id="26bb" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">策略迭代</h1><p id="a07b" class="pw-post-body-paragraph lg lh it li b lj nl ju ll lm nm jx lo lp nn lr ls lt no lv lw lx np lz ma mb im bi translated">与价值迭代相比，策略迭代更加广泛。萨顿和巴尔托(2019)将其分为三个步骤。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/8b10a9456f875f08836709d83f710d87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*f1RCCOtKKlhYYS1RA5Xl9g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">政策迭代算法[来源:萨顿&amp;巴尔托(公开发表)，2019]</p></figure><p id="0331" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">步骤 1 — <strong class="li iu">初始化</strong> —设置(可能任意)值函数和策略。<strong class="li iu">值函数</strong>给出了处于每个状态的感知值，<strong class="li iu">策略</strong>是返回任何给定状态的动作的规则。</p><p id="1982" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在步骤 2 — <strong class="li iu">策略评估</strong> —以非常类似于值迭代的方式确定每个状态的值。然而，我们不是通过最大化所有操作来确定值，而是简单地使用当前策略来计算值。简而言之，我们将行动的价值<code class="fe nr ns nt nu b">a=π(s)</code>(直接回报<code class="fe nr ns nt nu b">r</code> +下游价值<code class="fe nr ns nt nu b">V(s’)</code>)乘以转移概率<code class="fe nr ns nt nu b">p</code>。更多细节——比如误差容限<code class="fe nr ns nt nu b">θ</code>——请查看关于价值迭代的文章。</p><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/implement-value-iteration-in-python-a-minimal-working-example-f638907f3437"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">用 Python 实现值迭代——一个最小的工作示例</h2><div class="nv l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">掌握简单和经典的动态规划算法，寻找马尔可夫决策过程的最优解…</h3></div><div class="mm l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mn l"><div class="nw l mp mq mr mn ms ks mf"/></div></div></a></div><p id="d361" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">步骤 3 — <strong class="li iu">策略改进</strong> —寻求使用主流价值函数来改进策略。对于每个状态，它验证当前策略建议的行动是否确实是价值最大化行动。如果没有，则更新策略并重复步骤 2。该算法在这两个步骤之间交替，直到策略保持稳定。此时，达到了最优策略，算法终止。</p><p id="295c" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">对于价值迭代和策略迭代之间的直接比较，请在这里并排查看它们。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nx"><img src="../Images/04f6a33a9ee4580ad940bec4a1672eae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VqOXOqYxpwRTXGDJGjOgLg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">策略迭代(左)和价值迭代(右)的比较。关键区别在于策略迭代分离了评估和策略更新。[改编自萨顿&amp;巴尔托出版社(公开发行)，2019 年]</p></figure><h1 id="0ce6" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">一个最小的工作示例</h1><p id="48d5" class="pw-post-body-paragraph lg lh it li b lj nl ju ll lm nm jx lo lp nn lr ls lt no lv lw lx np lz ma mb im bi translated">编码示例的时间到了。我喜欢用非常简单的问题来演示算法，正如下面的简要概述所证明的那样。</p><h1 id="c860" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">问题是</h1><p id="c5ea" class="pw-post-body-paragraph lg lh it li b lj nl ju ll lm nm jx lo lp nn lr ls lt no lv lw lx np lz ma mb im bi translated">感兴趣的问题是一维世界(一排瓷砖)。在中间，有一个瓷砖作为终止状态。落在这块瓷砖上产生+10 的奖励，在瓷砖间走动每移动一次花费-1。代理可以决定向左或向右移动，但最终有 10%的时间走错了方向。有了直接回报、预期下游回报和转移概率，它就具备了 MDP 的基本要素。</p><h1 id="5315" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">该算法</h1><p id="5a16" class="pw-post-body-paragraph lg lh it li b lj nl ju ll lm nm jx lo lp nn lr ls lt no lv lw lx np lz ma mb im bi translated">Python 算法与前面展示的数学过程没有太大的不同。注意，最大迭代次数是为了限制计算量而进行的简化。特别是，如果两个或多个策略执行得同样好，算法可能会在它们之间无休止地循环。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ny nz l"/></div></figure><h1 id="bacb" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">一些实验</h1><p id="ba6e" class="pw-post-body-paragraph lg lh it li b lj nl ju ll lm nm jx lo lp nn lr ls lt no lv lw lx np lz ma mb im bi translated">是时候做点实验了。我将提供一些计算示例以及对结果的一些思考。</p><p id="3f51" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们设置所有的值函数<code class="fe nr ns nt nu b">V(s)=0</code>，并在<code class="fe nr ns nt nu b">π=[0,0,0,0,0]</code>初始化策略(即总是向左移动)。</p><p id="5a07" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">然后，我们转向政策评估。重申:在这一步中，我们根据<strong class="li iu">当前策略</strong> <code class="fe nr ns nt nu b">π</code>确定状态值。策略评估是一个迭代步骤，重复直到所有值都低于错误阈值<code class="fe nr ns nt nu b">θ</code>。在这种情况下(状态 0，迭代 1)，初始估计值<code class="fe nr ns nt nu b">V(0)</code>是 0。由于直接奖励是-1，误差<code class="fe nr ns nt nu b">Δ</code>也是 1，我们需要重复。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/77cf5cd8887c45ef568b223a073ec1dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zH-Pqbtf4UfI_3Us3g9Dmg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">第一步政策评估。在策略评估过程中，我们根据现行策略计算每个状态的值。重复评估步骤，直到值在预定的误差范围内。</p></figure><p id="5ba4" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">达到正确的价值观需要一些时间。如果我们取<code class="fe nr ns nt nu b">θ= 1e-10</code>，对于所有状态，我们需要在<code class="fe nr ns nt nu b">Δ&lt;θ</code>之前进行不少于 185 次迭代。我们获得<code class="fe nr ns nt nu b">V=[-8.62, -7.08, 10.00, 7.61, 5.68]</code>。</p><p id="bd4b" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">现在，我们继续讨论政策改进。我们测试我们的初始策略(“始终向左”)是否确实是最优的— <em class="ob">剧透警告:</em>根据我们刚刚确定的值，它不是最优的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/3294705f9c7b903e602c1f94ab052d79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Yq-DfRN1TjruFbvEttGo6g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">状态 1 的第一个政策改进步骤。给定之前确定的状态值，我们验证当前策略是否是最佳的，或者不同的操作会产生更好的结果。如果我们可以通过采取不同的措施来改进当前策略，我们就更新策略并返回到策略评估。</p></figure><p id="f302" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">显然，在最左边的瓷砖上，向右比向左好。因此，不符合停止标准。我们更新<code class="fe nr ns nt nu b">π</code>并返回到策略评估步骤来重新计算值。</p><p id="9b04" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">最终，我们应该到达一个政策不再改变的点。在这种情况下，它只是一个循环。更新策略后，我们在<em class="ob">策略评估</em>期间计算新的状态值(16 次迭代),并且在随后的策略改进步骤中不进行任何策略更改。算法已经终止，产生了<code class="fe nr ns nt nu b">π=[1,1,0,0,0]</code>。</p><p id="ef5d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">除此之外，其实没什么好说的。在<code class="fe nr ns nt nu b">θ</code>和步骤 2 和步骤 3 之间的循环数之间有一个计算上的权衡，但是对于这个特殊的问题，它几乎无关紧要。</p><p id="0e7a" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">虽然在许多方面与值迭代相似，但注意策略迭代返回一个<em class="ob">策略</em>，而值迭代返回一组<em class="ob">值函数</em>。当转向近似(强化学习)算法时，这种区别变得相关。你可以有一个好的政策，而不需要很好的价值函数(或者甚至不知道它们)，你也可以有准确的价值函数，而不需要对政策本身有明确的理解。</p><h1 id="aff0" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">最后的话</h1><p id="05f5" class="pw-post-body-paragraph lg lh it li b lj nl ju ll lm nm jx lo lp nn lr ls lt no lv lw lx np lz ma mb im bi translated">与值迭代一样，策略迭代是一种基本算法，许多近似算法都是从它派生出来的。在真正进入强化学习的世界之前，确保理解策略迭代。</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="a802" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><em class="ob">您可能感兴趣的一些更简单的工作示例:</em></p><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/a-minimal-working-example-for-deep-q-learning-in-tensorflow-2-0-e0ca8a944d5e"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">TensorFlow 2.0 中深度 Q 学习的最小工作示例</h2><div class="nv l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">一个多臂土匪的例子来训练一个 Q 网络。使用 TensorFlow，更新过程只需要几行代码</h3></div><div class="mm l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mn l"><div class="od l mp mq mr mn ms ks mf"/></div></div></a></div><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/a-minimal-working-example-for-continuous-policy-gradients-in-tensorflow-2-0-d3413ec38c6b"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">TensorFlow 2.0 中连续策略梯度的最小工作示例</h2><div class="nv l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">一个简单的训练高斯演员网络的例子。定义自定义损失函数并应用梯度胶带…</h3></div><div class="mm l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mn l"><div class="oe l mp mq mr mn ms ks mf"/></div></div></a></div><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/a-minimal-working-example-for-discrete-policy-gradients-in-tensorflow-2-0-d6a0d6b1a6d7"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">TensorFlow 2.0 中离散策略梯度的最小工作示例</h2><div class="nv l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">一个训练离散演员网络的多兵种土匪例子。在梯度胶带功能的帮助下…</h3></div><div class="mm l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mn l"><div class="of l mp mq mr mn ms ks mf"/></div></div></a></div><h1 id="de49" class="mt mu it bd mv mw mx my mz na nb nc nd jz ne ka nf kc ng kd nh kf ni kg nj nk bi translated">参考</h1><p id="750a" class="pw-post-body-paragraph lg lh it li b lj nl ju ll lm nm jx lo lp nn lr ls lt no lv lw lx np lz ma mb im bi translated">贝尔曼河(1957 年)。马尔可夫决策过程。<em class="ob">数学与力学学报</em>，<em class="ob"> 6 </em> (5)，679–684。</p><p id="98ae" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">Bertsekas 博士和 Tsitsiklis，J. N. (1996 年)。<em class="ob">神经动态规划</em>。雅典娜科技公司。</p><p id="df6f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">霍华德，R. A. (1960)。动态规划和马尔可夫过程。</p><p id="fb68" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">萨顿和巴尔托(2019 年)。<em class="ob">强化学习:简介</em>。麻省理工出版社。</p></div></div>    
</body>
</html>