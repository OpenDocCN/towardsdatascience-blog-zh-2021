<html>
<head>
<title>Use this clustering method if you have many outliers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如果有许多异常值，请使用这种聚类方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/use-this-clustering-method-if-you-have-many-outliers-5c99b4cd380d?source=collection_archive---------12-----------------------#2021-08-06">https://towardsdatascience.com/use-this-clustering-method-if-you-have-many-outliers-5c99b4cd380d?source=collection_archive---------12-----------------------#2021-08-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1a0a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">稳健结果的k-中位数变化</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/af4a7b886491ef2df7d995ebc0db5f55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YT2GcSxwTiUXg0Bw"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@fabioha?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">法比奥</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="009e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果您曾经想要将数据分组，您可能已经尝试过著名的k-means算法。由于它非常简单，所以被广泛使用，但它的简单性也带来了一些缺点。其中之一是它对异常值的<br/>敏感性，因为它使用经典的欧几里德距离作为相异度<br/>度量。</p><p id="714d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">不幸的是，现实世界中的数据集经常带有许多异常值，您可能无法在数据清理阶段将其完全删除。如果你遇到过这个问题，我想给你介绍一下k-medians算法。通过使用中值而不是平均值，并使用更健壮的相异度度量，它对异常值不太敏感。</p><p id="19e8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本文中，我将向您展示以下内容:</p><ul class=""><li id="7960" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><a class="ae kv" href="http://31f1" rel="noopener ugc nofollow" target="_blank"> k线中线直觉</a></li><li id="e0e5" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><a class="ae kv" href="http://448b" rel="noopener ugc nofollow" target="_blank">在R中从头实现</a></li></ul><p id="2971" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">像往常一样，你也可以在我的<a class="ae kv" href="https://github.com/MSHelm/algorithms-from-scratch" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到这篇文章的所有代码。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="31f1" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">k-中位数直觉</h1><p id="66ea" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">k-medians试图通过选择<br/>不同的相异度度量来减轻k-means对异常值的敏感性。我们通常使用绝对差而不是欧几里德距离，这也称为L1范数或曼哈顿或出租车距离(因为您可以使用它来计算出租车到达矩形网格中的目标所需的转弯次数)。<br/>这对异常值不太敏感，因为这些异常值只与<br/>它们到中心的实际距离有关，而不是距离的平方，因为<br/>是欧几里德距离的情况:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/7219f67d2540ff44c4d96d7196af13ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9juPK8SUki8krbOo9IPssQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">两点p和q之间的曼哈顿度量。对于每个维度，计算两点之间的绝对差值并求和。这也被称为L1规范。图片作者。</p></figure><p id="99c2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是，如果更合适的话，我们也可以使用其他指标，例如Kullback-Leibler散度来比较分布。</p><p id="9ac0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了更加可靠，我们还选择了中间值而不是中心的平均值。所以最后我们需要优化以下问题:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/26323c5fd731f25a1bc36328ded4ad1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*guwwqh_U66tBQzSvtlDaqg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">k-中位数优化问题的数学表述。尝试找到一组聚类C，使每个点到其所属聚类中心Ci的绝对差最小。图片作者。</p></figure><p id="84e5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">k-中值的方法与k-均值非常相似，也是Llodyd的<br/>算法。简单总结一下:</p><pre class="kg kh ki kj gt nm nn no np aw nq bi"><span id="7b84" class="nr mo iq nn b gy ns nt l nu nv">Input parameter k (number of clusters) and n_iter (number of iterations)<br/>Randomly initialize k objects in the data set as centers<br/>Do n_iter times:<br/>  Assign each object to its closest center<br/>   Calculate the new centers</span></pre><p id="dbb9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以在我关于k-means的文章中找到关于劳埃德算法的更详细的解释:</p><div class="nw nx gp gr ny nz"><a rel="noopener follow" target="_blank" href="/a-deep-dive-into-k-means-f9a1ef2490f8"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd ir gy z fp oe fr fs of fu fw ip bi translated">对k-means的深入探究</h2><div class="og l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">towardsdatascience.com</p></div></div><div class="oh l"><div class="oi l oj ok ol oh om kp nz"/></div></div></a></div></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="448b" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">在R中从头开始实现</h1><p id="fd32" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">如果我们看看编程实现，我们会发现它不像k-means那样无处不在。例如在R中，stats包中没有k-medians <br/>函数。所以让我们自己编码:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="on oo l"/></div></figure></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="c737" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">在虹膜数据集上进行测试</h1><p id="72ad" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">接下来，让我们看看我们的函数如何在公共虹膜数据集上执行。我们将<br/>将它与基本的R k-means实现进行比较，看看它们可能有什么不同:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi op"><img src="../Images/ff1ffea435026df5840b82f4772fc722.png" data-original-src="https://miro.medium.com/v2/resize:fit:1276/format:webp/1*om08jFP5_C4pxH9JGJ1ogA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">k-中间值与真实值和k-均值的比较。两种聚类算法都倾向于找到正确的聚类，只需要观察最小的距离。</p></figure><p id="5259" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于这个数据集，我们只观察到k-中位数和k-均值之间的微小差异，但它也不包含太多的离群值。</p></div><div class="ab cl mg mh hu mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="ij ik il im in"><h1 id="c217" class="mn mo iq bd mp mq mr ms mt mu mv mw mx jw my jx mz jz na ka nb kc nc kd nd ne bi translated">摘要</h1><p id="831b" class="pw-post-body-paragraph kw kx iq ky b kz nf jr lb lc ng ju le lf nh lh li lj ni ll lm ln nj lp lq lr ij bi translated">如你所见，它与k-means非常相似，我们只是使用了不同的<br/>距离函数，并使用了中值。</p><p id="fba5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">关于k-中位数的一个常见误解是，作为中心返回的中位数总是数据集中的实际点。很容易看出这不是真的。考虑以下由5个点组成的示例集群:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/6a89d118d358147b6ae37af0ba093ab4.png" data-original-src="https://miro.medium.com/v2/resize:fit:326/format:webp/1*nLLLF_SEG5QQurVNqLDjfA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">为什么聚类的中心不需要成为k-中位数的实际数据点的示例。考虑由上述5个点组成的集群。因为中位数是在k-中位数中为每个维度单独计算的，所以中位数将是x = 3，y = 3。但是在数据集中不存在点(3，3)。</p></figure><p id="c3ce" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因为中位数是在k-中位数中为每个维度单独计算的，所以中位数将是x = 3，y = 3。但是在数据集中不存在点(3，3)。</p><p id="add5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，当然您可以将k-medians与改进的<br/>初始化结合在一起，比如kmeans++，使其更加健壮。你可以在我的文章中找到关于如何做的细节:</p><div class="nw nx gp gr ny nz"><a rel="noopener follow" target="_blank" href="/try-this-simple-trick-to-improve-your-clustering-b2d5d502039b"><div class="oa ab fo"><div class="ob ab oc cl cj od"><h2 class="bd ir gy z fp oe fr fs of fu fw ip bi translated">尝试这个简单的技巧来改善你的聚类</h2><div class="or l"><h3 class="bd b gy z fp oe fr fs of fu fw dk translated">开始初始化的k-means++算法</h3></div><div class="og l"><p class="bd b dl z fp oe fr fs of fu fw dk translated">towardsdatascience.com</p></div></div><div class="oh l"><div class="os l oj ok ol oh om kp nz"/></div></div></a></div><p id="2933" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">下次见，届时我们将讨论k-means的最高级变体，围绕medoids进行划分，结束这个迷你系列。</p></div></div>    
</body>
</html>