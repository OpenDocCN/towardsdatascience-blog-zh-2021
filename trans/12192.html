<html>
<head>
<title>How to Collect a Reddit Dataset</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何收集Reddit数据集</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-collect-a-reddit-dataset-c369de539114?source=collection_archive---------4-----------------------#2021-12-10">https://towardsdatascience.com/how-to-collect-a-reddit-dataset-c369de539114?source=collection_archive---------4-----------------------#2021-12-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="7398" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">在Python中使用PSAW和PRAW</h2></div><p id="aca4" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">Reddit是一个社交媒体平台，由子论坛或子编辑组成，每个子论坛专注于一个给定的主题。一些公共子数据可能是有趣数据的深井，随时可供探索！然而，即使想到如何收集这些数据，尤其是大量的数据，也是令人望而生畏的。</p><p id="d122" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在这篇文章中，我们将使用Python开发一个工具，从任何(公共)子编辑中收集公共可用的Reddit帖子，包括他们的评论帖子，按发布年份组织。</p><p id="b0f9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们将详细介绍必要的<strong class="kk iu">设置</strong>和<strong class="kk iu">记录功能</strong>，完整的<strong class="kk iu">主算法</strong>和逐块的<strong class="kk iu">，最后是<strong class="kk iu">收集数据的示例</strong>和<strong class="kk iu">收集和探索的想法</strong>。</strong></p><h1 id="d201" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">设置</h1><p id="e2df" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">我们的工具将基于<a class="ae le" href="https://pypi.org/project/psaw/" rel="noopener ugc nofollow" target="_blank"> PSAW </a>和<a class="ae le" href="https://praw.readthedocs.io/en/stable/" rel="noopener ugc nofollow" target="_blank"> PRAW </a>。PSAW的用法相当简单。然而，为了使用PRAW (Reddit的API)，您需要设置您的授权令牌，以便Reddit知道您的应用程序。您可以按照<a class="ae le" href="https://github.com/reddit-archive/reddit/wiki/OAuth2" rel="noopener ugc nofollow" target="_blank">这个指南</a>快速获得您的令牌，并<strong class="kk iu">替换</strong>下面代码中的相关信息，具体来说，就是您的客户端ID和密码，以及您的reddit用户名和密码。</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="22d5" class="ml lg it mh b gy mm mn l mo mp">import praw<br/>from psaw import PushshiftAPI</span><span id="14cf" class="ml lg it mh b gy mq mn l mo mp"># to use PSAW<br/>api = PushshiftAPI()</span><span id="e156" class="ml lg it mh b gy mq mn l mo mp"># to use PRAW<br/>reddit = praw.Reddit(<br/>    client_id = "YOUR_CLIENT_ID_HERE",<br/>    client_secret = "YOUR_CLIENT_SECRET_HERE",<br/>    username = "YOUR_USERNAME_HERE",<br/>    password = "YOUR_PASSWORD_HERE",<br/>    user_agent = "my agent"<br/>)</span></pre><p id="c566" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们还必须定义子编辑列表(可以只有一个！)和年份范围，即从哪一年开始和哪一年结束将检索帖子。在这种情况下，我们将从2020年初到2021年底(到今天为止)发布的<a class="ae le" href="https://www.reddit.com/r/shortscarystories/" rel="noopener ugc nofollow" target="_blank">r/shortscarystorys subreddit</a>中提取帖子。最后，我们必须设置存储数据的目录。</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="26b4" class="ml lg it mh b gy mm mn l mo mp">subreddits = ['shortscarystories']<br/>start_year = 2020<br/>end_year = 2021</span><span id="b807" class="ml lg it mh b gy mq mn l mo mp"># directory on which to store the data<br/>basecorpus = './my-dataset/'</span></pre><h1 id="0cfc" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">日志记录功能</h1><p id="9970" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">日志功能让您深入了解正在运行的进程。当这个过程很长，你想知道它是如何进行的时候，这些尤其重要。在我们的例子中，我们将简单地编写一个函数，将算法刚刚采取的动作，以及采取该动作需要多长时间打印到屏幕上。如果我们最终提取了很多很多的数据，并希望确保程序不只是停留在某个地方，这将是非常有见地的。这个函数可以很容易地修改为记录到一个文件，而不是打印到屏幕上:这就是为什么我把它封装到一个函数中，而不是把打印直接内置到算法中。</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="f0c8" class="ml lg it mh b gy mm mn l mo mp">import time</span><span id="15d3" class="ml lg it mh b gy mq mn l mo mp">def log_action(action):<br/>    print(action)<br/>    return</span></pre><h1 id="3999" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">主算法</h1><p id="c2ab" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">主要思想非常简单:在<start_year>和<end_year>之间的每一年，创建一个目录，并且为<subreddits>中的每个子编辑创建一个目录来存储该子编辑在这一年中的帖子。</subreddits></end_year></start_year></p><p id="b5b9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是完整的代码，然后我们将一次一个块地检查它。</p><figure class="mc md me mf gt mr"><div class="bz fp l di"><div class="ms mt l"/></div></figure><h1 id="344b" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated">详述算法</h1><h2 id="b8ba" class="ml lg it bd lh mu mv dn ll mw mx dp lp kr my mz lr kv na nb lt kz nc nd lv ne bi translated"><strong class="ak"> <em class="nf"> ###第1块# #</em># T8】</strong></h2><p id="5e63" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">简单地说，对于从<start_year>到<end_year>之间的每一年，创建一个目录并定义开始和结束时间戳(从当年开始直到下一年开始)。这些时间戳作为输入提供给PSAW请求，指定从中检索数据的时间窗口。</end_year></start_year></p><h2 id="9d5b" class="ml lg it bd lh mu mv dn ll mw mx dp lp kr my mz lr kv na nb lt kz nc nd lv ne bi translated"><strong class="ak"> <em class="nf"> ###第二块# #</em># #第十四块】</strong></h2><p id="7f86" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">对于<subreddits>中的每一个子编辑，开始为日志记录计算提取时间，在当年的目录中创建相应的目录，并定义CSV文件的路径，在该文件中存储相应的要提取的帖子。</subreddits></p><h2 id="52e7" class="ml lg it bd lh mu mv dn ll mw mx dp lp kr my mz lr kv na nb lt kz nc nd lv ne bi translated"><strong class="ak"> <em class="nf"> ###第3块# #</em># #第19块】</strong></h2><p id="0261" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">定义存储每个帖子的Python字典(存储在内存中，稍后写入之前定义的CSV文件路径)。我们对存储帖子的ID、URL、标题、分数、评论数、创建时间戳和帖子的实际正文(selftext)感兴趣。可以提取其他指标——使用本指南找出您确切需要的指标。</p><h2 id="0b6a" class="ml lg it bd lh mu mv dn ll mw mx dp lp kr my mz lr kv na nb lt kz nc nd lv ne bi translated"><strong class="ak"> <em class="nf"> ###块4 # #</em># # T25】</strong></h2><p id="7ba4" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">PSAW允许我们创建一个迭代器来遍历一组特定的文章(也称为提交)。我们用之前设置的参数来指定这个集合:开始和结束时间戳、特定的子编辑以及要检索的帖子数量。在这个例子中，我将其限制在100篇帖子，但是Python关键字<strong class="kk iu"> None </strong>可以用于<strong class="kk iu">在指定的时间框架</strong>内浏览所有帖子。此外，PSAW没有给我们每篇文章想要的所有信息，所以<strong class="kk iu">我们只使用它来检索文章的ID </strong>，然后<strong class="kk iu">在PRAW使用该ID来获取剩余的信息</strong>。</p><h2 id="80d9" class="ml lg it bd lh mu mv dn ll mw mx dp lp kr my mz lr kv na nb lt kz nc nd lv ne bi translated"><strong class="ak"><em class="nf"># # # 5 # # #</em># T11】</strong></h2><p id="d68c" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">对于我们在前面的块中定义的迭代器中的每个post(或submission ),获取它的ID并使用它从PRAW获取完整的post数据，并将它附加到我们在块3中定义的Python字典中。</p><h2 id="6149" class="ml lg it bd lh mu mv dn ll mw mx dp lp kr my mz lr kv na nb lt kz nc nd lv ne bi translated"><strong class="ak"> <em class="nf"> ###块6 ### </em> </strong></h2><p id="add3" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">每个帖子都有一个相关的评论线程，我们也希望收集这些评论线程，以便将来进行可能的分析。我们希望将这篇文章的整个评论存储在一个单独的CSV文件中，我们现在为该文件定义适当的文件路径。我们还定义了Python字典来存储这篇文章的评论线程，特别是每个评论的ID、父ID、链接ID，最重要的是它的正文。</p><h2 id="8a78" class="ml lg it bd lh mu mv dn ll mw mx dp lp kr my mz lr kv na nb lt kz nc nd lv ne bi translated"><strong class="ak"> <em class="nf"> ###块7 # #</em># #块19】</strong></h2><p id="4914" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">现在，对于当前帖子的扁平化评论线程中的每个评论(它最初是一个树结构，我们显式地将其扁平化，但总是可以基于评论父ID检索原始树结构)，我们检索适当的数据并存储在我们在前面的块中定义的Python字典中。一旦我们检查了当前帖子的所有评论并存储在字典中，我们就可以将它保存到CSV文件中(该路径是我们在前面的块中定义的)。我们使用Pandas轻松地转换成正确的格式并写入文件。</p><h2 id="10ce" class="ml lg it bd lh mu mv dn ll mw mx dp lp kr my mz lr kv na nb lt kz nc nd lv ne bi translated"><strong class="ak"><em class="nf"># # # 8 # # #</em># # 23】</strong></h2><p id="0480" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">最后，一旦我们遍历了迭代器中的所有post(即，在块4中定义的指定时间范围内)并存储在字典中，我们可以将它保存到CSV文件中(我们在块2中定义了该路径)。同样，我们使用Pandas轻松地转换成正确的格式并写入文件。</p><h2 id="9987" class="ml lg it bd lh mu mv dn ll mw mx dp lp kr my mz lr kv na nb lt kz nc nd lv ne bi translated">记录输出</h2><p id="6fb7" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">这是上面代码中示例的日志记录输出:</p><pre class="mc md me mf gt mg mh mi mj aw mk bi"><span id="47eb" class="ml lg it mh b gy mm mn l mo mp">[Year] 2020<br/>	[Subreddit] shortscarystories<br/>		[Info] Found submissions: 100<br/>		[Info] Elapsed time:  26.14s<br/>[Year] 2021<br/>	[Subreddit] shortscarystories<br/>		[Info] Found submissions: 100<br/>		[Info] Elapsed time:  27.96s</span></pre><h1 id="7f74" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated"><strong class="ak">收集的数据示例</strong></h1><figure class="mc md me mf gt mr gh gi paragraph-image"><div role="button" tabindex="0" class="nh ni di nj bf nk"><div class="gh gi ng"><img src="../Images/cd745ae5c97767fd7e38c8bd1321a35d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4CD0IvFbU6Mi8jIc4r7xVQ.png"/></div></div><p class="nn no gj gh gi np nq bd b be z dk translated">前3篇收集自2020年r/shortscarystories的帖子(2020-shortscarystories-submissions . CSV)。图片作者。</p></figure><p id="addd" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">需要注意的一个重要方面是，Reddit的原始数据并不完美。有些帖子的内容可能已经被删除，但帖子仍然存在，在“随意杀人”条目中可以看到，帖子的正文是“[已删除]”。不是盲目地收集所有数据，而是可以改变块5以忽略文本主体为“[已删除]”、“[已移除]”或空的所有帖子。这也可以在稍后的数据预处理阶段执行。</p><h1 id="5080" class="lf lg it bd lh li lj lk ll lm ln lo lp jz lq ka lr kc ls kd lt kf lu kg lv lw bi translated"><strong class="ak">收集和探索的想法</strong></h1><p id="e767" class="pw-post-body-paragraph ki kj it kk b kl lx ju kn ko ly jx kq kr lz kt ku kv ma kx ky kz mb lb lc ld im bi translated">最后，我在这里留下我写这篇文章时的一些想法，这些想法可以产生有趣的挑战和结果。</p><ol class=""><li id="fe00" class="nr ns it kk b kl km ko kp kr nt kv nu kz nv ld nw nx ny nz bi translated">利用最近来自https://www.reddit.com/r/recipes/<a class="ae le" href="https://www.reddit.com/r/recipes/" rel="noopener ugc nofollow" target="_blank">的帖子，人们可以收集一个食谱图片、配料和说明的数据集。有了这些数据，人们可以开发出这样的系统:基于成分列表的配方建议器、基于成分列表的配方创建器、基于配方图像的配方指令合成器等等。</a></li><li id="7f2c" class="nr ns it kk b kl oa ko ob kr oc kv od kz oe ld nw nx ny nz bi translated">利用来自<a class="ae le" href="https://www.reddit.com/r/shortscarystories/" rel="noopener ugc nofollow" target="_blank">https://www.reddit.com/r/shortscarystories/</a>帖子的数据，开发如下系统:给定标题作为提示的恐怖短篇故事生成器，基于故事标题和文本的评分/评论预测，用于观众统计预测，等等。</li><li id="daa0" class="nr ns it kk b kl oa ko ob kr oc kv od kz oe ld nw nx ny nz bi translated">使用来自<a class="ae le" href="https://www.reddit.com/r/AskReddit/" rel="noopener ugc nofollow" target="_blank">https://www.reddit.com/r/AskReddit/</a>帖子的数据(帖子是一个问题，评论是可能的答案)，开发一个oracle系统，它对每一个可以想象到的问题都有一个答案。</li></ol></div></div>    
</body>
</html>