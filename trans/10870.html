<html>
<head>
<title>Cliff-Walking Problem With The Discrete Policy Gradient Algorithm</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于离散策略梯度算法的悬崖行走问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/cliff-walking-problem-with-the-discrete-policy-gradient-algorithm-59d1900d80d8?source=collection_archive---------19-----------------------#2021-10-21">https://towardsdatascience.com/cliff-walking-problem-with-the-discrete-policy-gradient-algorithm-59d1900d80d8?source=collection_archive---------19-----------------------#2021-10-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="013b" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用Python实现了一个完整的增强算法。手动执行这些步骤来说明该过程的内部工作原理。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a2a4125ae056bba4ede4cbabf3de5333.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*ll4M5yh1mslZoea3"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">詹妮弗·卡佩尔在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="c854" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">教科书上的悬崖行走问题继续引人入胜。虽然非常简单，但它阐明了强化学习算法的许多有趣的方面。在处理了一些基于价值的实现(这里是SARSA和Q-learning<a class="ae ky" rel="noopener" target="_blank" href="/walking-off-the-cliff-with-off-policy-reinforcement-learning-7fdbcdfe31ff"/>，这里是深度Q-learning<a class="ae ky" rel="noopener" target="_blank" href="/deep-q-learning-for-the-cliff-walking-problem-b54835409046"/>)之后，现在是时候转向基于策略的实现了。尽管现在有很多库，我们还是用Python手动实现了这个过程，来演示<strong class="li iu">离散策略梯度算法</strong>背后的机制。最后，我们与SARSA进行了面对面的比较。</p><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/policy-gradients-in-reinforcement-learning-explained-ecec7df94245"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">强化学习中的策略梯度解释</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">了解所有关于基于似然比的政策梯度算法(加强):直觉，推导，和…</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="mp l mq mr ms mo mt ks mf"/></div></div></a></div><h1 id="84f6" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">悬崖行走问题</h1><p id="906b" class="pw-post-body-paragraph lg lh it li b lj nm ju ll lm nn jx lo lp no lr ls lt np lv lw lx nq lz ma mb im bi translated">先把问题勾勒出来。我们的世界是一个相当全面的网格，除了应该避免的致命悬崖。代理从悬崖的左手边开始，目标(代理不知道)在右手边等待。目标是找到到达目标的最短路径而不坠入悬崖，这直接结束了这一集。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/5f4d13cbe56bfec804201001f9a70090.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M79MspX4MckRX3wqKCgAxA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">悬崖世界[图片由作者提供]</p></figure><p id="c42f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">存在各种强化学习解决方案。本文主要讨论策略梯度算法。简单地说，策略告诉代理在给定状态下做什么——比计算和利用对应于每个动作的值函数更直接。在此设置中，您可能倾向于将策略视为一个表格，其中列出了48个图块中每个图块的最佳操作。</p><p id="73cb" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">不过这有点复杂，因为这种方法依赖于随机策略。策略<strong class="li iu">不是简单地告诉我们最好的行动，而是提供了我们应该采取每个行动的概率</strong>。不涉及太多细节，随机政策通常在未知环境中是有意义的，并为探索提供了一种内置机制。不良行为的概率应该趋近于零。</p><h1 id="93b6" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">离散策略— Softmax函数</h1><p id="7ece" class="pw-post-body-paragraph lg lh it li b lj nm ju ll lm nn jx lo lp no lr ls lt np lv lw lx nq lz ma mb im bi translated">是时候正式制定政策了。我们使用一个<em class="ns"> softmax </em>策略，它有效地将任何一系列真实值映射到一系列概率。作为输入，它需要一个特征向量和一个相应权重的向量。</p><p id="880f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">作为特征向量<em class="ns"> ϕ(s,a，</em>我们定义了一个48⋅4向量，每四个元素代表任何给定瓷砖上的移动——本质上是一个查找表(或者可以使用<a class="ae ky" rel="noopener" target="_blank" href="/what-are-post-decision-states-and-what-do-they-want-from-us-9e02105b7f40">后决策状态</a>方法，只需要48个特征，但丢失了细节)。该向量是<strong class="li iu">一位热编码的</strong>，这意味着我们有一个具有191个0和1个1的向量，对应于状态<em class="ns"> s </em>中的动作<em class="ns"> a </em>。如前所述，<em class="ns"> θ </em>是一个权重向量，每个<em class="ns"> (s，a) </em>对一个。<em class="ns"> ϕ(s,a) </em>和<em class="ns"> θ </em>的点积捕捉了位于特定瓷砖上的值，与众所周知的q值相当。</p><p id="f1ad" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">现在，我们可以将softmax策略形式化如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/eb765063f0a79c2fef127c8298b3974f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*FYMA_h6fBakOiwzb3HOOYQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Softmax政策。</p></figure><p id="6337" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><em class="ns"> ϕ(s,a)^⊤ θ越大，</em>a<em class="ns">采取行动的概率越高。用正常的语言来说:我们很有可能采取措施，让我们更接近目标。但是在我们知道这些步骤之前，我们首先需要了解政策。</em></p><h1 id="9cc0" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">离散政策梯度</h1><p id="2c77" class="pw-post-body-paragraph lg lh it li b lj nm ju ll lm nn jx lo lp no lr ls lt np lv lw lx nq lz ma mb im bi translated">定义了策略本身之后，让我们来看看如何更新它。这就是政策梯度定理的用武之地。为了保持文章的范围，我们将自己限制在权重更新过程中:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/bca9b03603dad75f5243f9987b9601d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*17i0-OJ49z1wHnXUggcZ1g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">策略梯度方法的更新功能</p></figure><p id="84ef" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这里的<em class="ns"> α </em>简单来说就是学习率。奖励函数<em class="ns"> G_t </em>也很简单:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/239d768d0a635a897daaa401a26db760.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cG5qnJlznuFA1NG2dEmOsw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">累积奖励函数</p></figure><p id="2df6" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">主要的挑战在于讨厌的<em class="ns"> ∇_θ对数(π_θ(a|s)) — </em>也称为<em class="ns">得分函数— </em>我们如何将它转化为可实施的规则？从数学上来说，这是相当多的工作(详细推导<a class="ae ky" rel="noopener" target="_blank" href="/policy-gradients-in-reinforcement-learning-explained-ecec7df94245">这里</a>)，所以让我们直接跳到最终结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/b70ae9df8b972c3abedcdd328eb49f50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UXfGBm3UlLVAb9HMnKMqqw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">离散政策梯度的得分函数。得分函数是所选动作的特征向量减去所有动作的加权特征向量。</p></figure><p id="c653" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在教科书中，最下面的方程通常是作为最终结果出现的，但实际上我发现其他方程更有用。</p><p id="5b12" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">举个例子吧。处理一个48元素的向量有点不方便，所以假设我们有一个只有4个元素的特征向量。首先，我们的一键向量<em class="ns"> ϕ(s,a) </em>可能看起来像<code class="fe nx ny nz oa b">[0 1 0 0]</code>——代表我们采取的行动。其次，我们计算我们的<strong class="li iu">期望</strong>，它本质上是一个<strong class="li iu">加权特征向量</strong>。假设我们有一个概率向量<code class="fe nx ny nz oa b">[0.2 0.5 0.1 0.2]</code>，每个动作将我们引向一个不同的瓦片(状态)。所有的动作都是一次性编码的，所以期望可以写成</p><p id="f187" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><code class="fe nx ny nz oa b">weighted_phi: [1 0 0 0]*0.2 + [0 1 0 0]*0.5 + [0 0 1 0]*0.1 + [0 0 0 1]*0.2 = [0.2 0.5 0.1 0.2]</code></p><p id="75d6" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">对，那和概率向量本身完全一样。事情并不总是这么简单——特征向量不一定是一个热门向量——但是你会明白的。现在，我们只需要从特征向量中减去期望值就可以得到</p><p id="726a" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><code class="fe nx ny nz oa b">score function: [0 1 0 0] - [0.2 0.5 0.1 0.2] = [-0.2 0.5 -0.1 -0.2]</code></p><p id="0624" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">概括地说，我们采用了与我们选择的动作相对应的特征向量，然后<strong class="li iu">减去所有动作的加权特征向量</strong>，从而获得该动作的得分。</p><p id="fb22" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">为了执行权重更新<em class="ns">δθ</em>，我们将得分函数与奖励<em class="ns"> G_t </em>和学习率<em class="ns"> α </em>相乘。例如:</p><p id="9781" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><code class="fe nx ny nz oa b">Weight update: Δθ = α * score_function * G_t = 0.01 *[-0.2 0.5 -0.1 -0.2]*10 = [-0.02 0.05 -0.01 -0.02]</code></p><p id="9107" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">直觉上，其中的含义是清楚的。如果我们把一个积极的分数(行动2)乘以一个积极的回报，这个概率会增加，我们会在未来更频繁地采取行动。更微妙:<strong class="li iu">奖励信号越强，更新越大</strong>。渐渐地，概率在最坏的情况下会收敛到局部最优，在最好的情况下会收敛到全局最优。</p><h1 id="d396" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">Python实现</h1><p id="9713" class="pw-post-body-paragraph lg lh it li b lj nm ju ll lm nn jx lo lp no lr ls lt np lv lw lx nq lz ma mb im bi translated">回到我们的悬崖漫步问题。最初，我们不知道什么行为是好的，因此相等地初始化概率是有意义的(简单地将所有的<em class="ns"> θ </em>设置为0)。我们的代理人会在一段时间内磕磕绊绊，但最终应该击中目标。这给了倒数第二块瓷砖一个强烈的奖励信号(增加向下移动的概率)，从那里开始就是连锁反应。</p><p id="22ad" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">请注意，尽管该方法是基于政策而不是基于价值的，但<strong class="li iu">奖励轨迹</strong>隐含地保留了整个过程中的核心角色。更新只发生在一集的结尾，所以如果我们走了20步，我们执行20次更新，追溯完整的轨迹(即蒙特卡罗学习)。</p><p id="5e4a" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">完整的实现可以在我的<a class="ae ky" href="https://github.com/woutervanheeswijk/cliff_walking_public" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到，在这里我将重点放在最相关的片段上(为了可读性做了一些清理)。</p><p id="407c" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">首先，让我们定义策略:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="6b45" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">相应的softmax可以表示如下。实际的实现通常涉及缩放—指数可能会变得非常大。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="0d81" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">使用相应的折扣，为存储的轨迹中的每个决策时刻<em class="ns"> t </em>计算奖励函数<em class="ns"> G_t </em>。通常，您还需要在这里执行一些奖励标准化。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="5532" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">最后，评分函数<em class="ns"> ∇_θ log(π_θ(a|s)) </em>，需要执行更新<em class="ns">δθ=α⋅g_t⋅∇_θlog(π_θ(a | s))。</em>提醒一下，我们只是简单的从实际的特征向量(选择的动作)中减去加权的特征向量(所有动作)。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ob oc l"/></div></figure><p id="3eb6" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这就是我们所需要的。现在让我们运行它，使用SARSA(另一个基于策略的方法，但是基于值)进行比较。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/d19e74ae60660297e8df2c653cf657d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*CzoqMrppVENtc_uFYIjGvQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">10，000集的步骤数，使用策略梯度和SARSA进行培训。SARSA更快地收敛到更好的解，并且经历更少的噪声。[图片由作者提供]</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/23640e59cd463681e589eef5dbd5b2cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:804/format:webp/1*AjUhfulFQnBVQc3FUzYQwg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">政策梯度和SARSA的培训路径。由于更高的探索，梯度剂走了更长的弯路。两者都不走最短路径，都是带有嵌入式探索的策略方法。[图片由作者提供]</p></figure><p id="465e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">显而易见，政策梯度更新比SARSA更新要混乱得多。这部分是随机策略所固有的(我们的SARSA代理只探索了5%的时间，梯度方法探索了更多的时间)-即使在10，000次迭代之后，代理也要绕着悬崖走一大段路，以对抗随机移动的<strong class="li iu">不利影响</strong>。</p><p id="72b8" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">此外，SARSA使用时间差异，而政策梯度使用完全回报轨迹——后者具有<strong class="li iu">高得多的方差</strong>。因此，我们也需要较低的学习速率<em class="ns"> α </em>来获得稳定的结果。</p><p id="231b" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">虽然政策梯度法在这里没有得到好评，但这并不意味着被驳回。像任何算法一样，对某些问题来说，这是一个很好的选择，但对其他问题来说，这是一个很差的选择。这些结果只是作为一个典型的强化行为的例子，可悲的是，它包含了高度的可变性和向局部最优的收敛。一如既往，适当的调整，奖励正常化，功能设计等。对提高性能大有帮助。</p><p id="b581" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">但是现在，让我们庆幸我们又一次勇敢地面对了悬崖。</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="56a9" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><em class="ns">完整的代码，包括其他针对悬崖行走问题的RL解决方案，可以在我的GitHub上找到:</em></p><div class="mc md gp gr me mf"><a href="https://github.com/woutervanheeswijk/cliff_walking_public" rel="noopener  ugc nofollow" target="_blank"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">GitHub—woutervanheeswijk/Cliff _ walking _ public:悬崖行走强化学习示例，带…</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">此时您不能执行该操作。您已使用另一个标签页或窗口登录。您已在另一个选项卡中注销，或者…</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">github.com</p></div></div><div class="mo l"><div class="of l mq mr ms mo mt ks mf"/></div></div></a></div><h1 id="77e7" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">外卖食品</h1><ul class=""><li id="78ed" class="og oh it li b lj nm lm nn lp oi lt oj lx ok mb ol om on oo bi translated">为了解决悬崖行走问题，我们可以采用离散策略，使用softmax策略选择动作。</li><li id="f702" class="og oh it li b lj op lm oq lp or lt os lx ot mb ol om on oo bi translated">更新依赖于得分函数，该函数是对应于所选动作的特征向量<em class="ns"> ϕ(s,a) </em>减去所有动作的加权特征向量<em class="ns"> ϕ(s,⋅) </em>。</li><li id="d861" class="og oh it li b lj op lm oq lp or lt os lx ot mb ol om on oo bi translated">逐渐地，策略收敛到每个状态的最佳动作的高概率(在这种情况下是平铺)。</li><li id="2b19" class="og oh it li b lj op lm oq lp or lt os lx ot mb ol om on oo bi translated">由于奖励轨迹的高可变性和高探索性，在这种基本实现中，收敛比基于价值的方法慢。</li></ul><p id="a147" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><em class="ns">Q-learning和SARSA的实现:</em></p><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/walking-off-the-cliff-with-off-policy-reinforcement-learning-7fdbcdfe31ff"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">用非策略强化学习走下悬崖</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">政策外强化学习和政策内强化学习的深入比较</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="ou l mq mr ms mo mt ks mf"/></div></div></a></div><p id="c29f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><em class="ns">深度Q学习的实现:</em></p><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/deep-q-learning-for-the-cliff-walking-problem-b54835409046"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">悬崖行走问题的深度Q学习</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">一个完整的Python实现，用TensorFlow 2.0导航悬崖。</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="ov l mq mr ms mo mt ks mf"/></div></div></a></div><p id="6d5b" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><em class="ns">首先在离散政策梯度的基础上迎头赶上？查看我的文章最小工作实例和理论推导:</em></p><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/a-minimal-working-example-for-discrete-policy-gradients-in-tensorflow-2-0-d6a0d6b1a6d7"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">TensorFlow 2.0中离散策略梯度的最小工作示例</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">一个训练离散演员网络的多兵种土匪例子。在梯度胶带功能的帮助下…</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="ow l mq mr ms mo mt ks mf"/></div></div></a></div><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/policy-gradients-in-reinforcement-learning-explained-ecec7df94245"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">强化学习中的策略梯度解释</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">了解所有关于基于似然比的政策梯度算法(加强):直觉，推导，和…</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="mp l mq mr ms mo mt ks mf"/></div></div></a></div><h1 id="770e" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">参考</h1><p id="7e23" class="pw-post-body-paragraph lg lh it li b lj nm ju ll lm nn jx lo lp no lr ls lt np lv lw lx nq lz ma mb im bi translated">Benderski，E. (2016年)。<em class="ns">soft max函数及其导数。</em><a class="ae ky" href="https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/" rel="noopener ugc nofollow" target="_blank">https://Eli . the green place . net/2016/the-soft max-function-and-its-derivative/</a></p><p id="fa0b" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">西格尔，E. (2029)。<em class="ns">政策梯度。</em><a class="ae ky" href="https://siegel.work/blog/PolicyGradient/" rel="noopener ugc nofollow" target="_blank">https://siegel.work/blog/PolicyGradient/</a></p><p id="0c08" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">西尔弗博士(2020)。第七讲:政策梯度<a class="ae ky" href="https://www.davidsilver.uk/wp-content/uploads/2020/03/pg.pdf" rel="noopener ugc nofollow" target="_blank">https://www . David silver . uk/WP-content/uploads/2020/03/pg . pdf</a></p><p id="19a6" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">萨顿和巴尔托(2018年)。强化学习:介绍。麻省理工出版社。</p><p id="e0d4" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">威廉姆斯，R. J. (1992)。<em class="ns">连接主义强化学习的简单统计梯度跟踪算法。机器学习，8(3–4):229–256。</em></p><p id="24da" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">于，鄂(2017)。<em class="ns">用Python从零开始的策略渐变。</em><a class="ae ky" href="http://quant.am/cs/2017/08/07/policy-gradients/" rel="noopener ugc nofollow" target="_blank">http://quant.am/cs/2017/08/07/policy-gradients/</a></p></div></div>    
</body>
</html>