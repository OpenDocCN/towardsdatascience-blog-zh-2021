<html>
<head>
<title>Feature Selection: How to Throw Away 95% of Your Features and Get 95% Accuracy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">特征选择:如何丢弃95%的特征并获得95%的准确率</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feature-selection-how-to-throw-away-95-of-your-data-and-get-95-accuracy-ad41ca016877?source=collection_archive---------5-----------------------#2021-04-17">https://towardsdatascience.com/feature-selection-how-to-throw-away-95-of-your-data-and-get-95-accuracy-ad41ca016877?source=collection_archive---------5-----------------------#2021-04-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="50d5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">特征选择是数据管道中的一个基本步骤。一个例子？在MNIST数据集上，只需要40个像素(总共784个像素)就可以获得95%以上的准确率(99% ROC)。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/996fe01c1500d573f0480ec102de49c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ly-AED-X1FW4DA_yG5QYsw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">[作者图]</p></figure><h1 id="e3a5" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">1.为什么选择功能？</h1><p id="4657" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">你能认出这些手写的数字吗？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mm"><img src="../Images/3689adf613ac937fa2eec5924baed90a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gijWFadBK0Prx5BJq54Avw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一些MNIST数字后，删除75%的图像。[作者图]</p></figure><p id="05f6" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">你可能很容易就能分别识别0、3和8。</p><p id="a693" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如果是这样的话，即使只显示了原始图像的25%,而剩余的75%被红色像素覆盖，你也能够正确地对它们进行分类。这是一个简单的任务，因为“相关的”像素是可见的，而只有“不相关的”和“多余的”像素被隐藏。</p><p id="39be" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这是“特征选择”的一个简单例子。</p><p id="0cb0" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">特征选择是在机器学习中执行的过程，在将数据馈送到预测模型之前，移除一些特征。如果数据是表格的形式，这仅仅意味着从表格中删除许多列。</p><blockquote class="ms"><p id="3d70" class="mt mu it bd mv mw mx my mz na nb ml dk translated">为什么要费心选择功能呢？我就不能把所有的数据都扔进我的预测模型里，让他来干脏活吗？</p></blockquote><p id="4bc3" class="pw-post-body-paragraph lq lr it ls b lt nc ju lv lw nd jx ly lz ne mb mc md nf mf mg mh ng mj mk ml im bi translated">实际上，有几个原因可以让您进行特征选择:</p><ul class=""><li id="d123" class="nh ni it ls b lt mn lw mo lz nj md nk mh nl ml nm nn no np bi translated"><strong class="ls iu">记忆</strong>。大数据占用大空间。删除要素意味着需要更少的内存来处理数据。有时还存在外部约束(例如，Google的AutoML允许您使用不超过1000列的<a class="ae nq" href="https://cloud.google.com/automl-tables/docs/quotas#:~:text=AutoML%20Tables%20enforces%20the%20following,Between%202%20and%201%2C000%20columns." rel="noopener ugc nofollow" target="_blank"/>。因此，如果您有超过1000列，您将被迫只保留其中的一部分)。</li><li id="0930" class="nh ni it ls b lt nr lw ns lz nt md nu mh nv ml nm nn no np bi translated"><strong class="ls iu">时间</strong>。用更少的数据训练一个模型可以节省你很多时间。</li><li id="37a1" class="nh ni it ls b lt nr lw ns lz nt md nu mh nv ml nm nn no np bi translated"><strong class="ls iu">精度</strong>。少即是多:这也适用于机器学习。包括冗余或不相关的特征意味着包括不必要的噪声。通常，根据较少数据训练的模型表现更好。</li><li id="976f" class="nh ni it ls b lt nr lw ns lz nt md nu mh nv ml nm nn no np bi translated"><strong class="ls iu">可解释性</strong>。更小的模型也意味着更易解释的模型。想象一下，如果你不得不解释一个基于成千上万不同因素的模型:这是不可行的。</li><li id="0a05" class="nh ni it ls b lt nr lw ns lz nt md nu mh nv ml nm nn no np bi translated"><strong class="ls iu">调试</strong>。较小的型号更容易维护和排除故障。</li></ul><h1 id="8b65" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">2.从数据开始</h1><p id="d5a2" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在本文中，我们将比较一些特征选择的方法。我们的操场数据集将是世界闻名的“MNIST”。</p><p id="d13f" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">MNIST是由70，000张手写数字的黑白图像组成的数据集。每幅图像都是28 x 28 (= 784)像素。每个像素被编码为从1(白色)到255(黑色)的整数:该值越高，颜色越深。按照惯例，60，000幅图像用作训练集，10，000幅图像用作测试集。</p><p id="77db" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">可以通过Keras命令将数据导入Python(我们还将重塑数据集，使其成为二维表格):</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="6c35" class="ob kz it nx b gy oc od l oe of">from keras.datasets import mnist</span><span id="8e46" class="ob kz it nx b gy og od l oe of">(X_train, y_train), (X_test, y_test) = mnist.load_data()</span><span id="0041" class="ob kz it nx b gy og od l oe of">X_train = X_train.reshape(60000, 28 * 28)<br/>X_test = X_test.reshape(10000, 28 * 28)</span></pre><p id="270b" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">例如，让我们打印出第8行的值:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="6d92" class="ob kz it nx b gy oc od l oe of">print(X_train[7, :])</span></pre><p id="4bdf" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这是结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/73872d2e044dc458093493fcf81c1036.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aozS3um4f6eaEVgmx3boUA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">MNIST训练数据集的第8幅图像的所有784个像素。[作者图]</p></figure><p id="8c6d" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">通过Matplotlib，我们还可以显示相应的图像:</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="a882" class="ob kz it nx b gy oc od l oe of">import matplotlib.pyplot as plt</span><span id="4851" class="ob kz it nx b gy og od l oe of">plt.imshow(X_train[7, :].reshape(28, 28), cmap = 'binary', vmin = 0, vmax = 255)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/7274a7d6358980522274f61add36ca45.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dAhK-qSOK-_yVvTG_w-fKg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">MNIST训练数据集的第8幅图像。</p></figure><h1 id="f5f0" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">3.特征选择之战</h1><p id="0c18" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们的任务是选择少量的列(即像素)，这些列在输入预测模型时足以达到良好的精度水平。</p><p id="86d4" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">有许多可能的策略和算法来执行特征选择。在本文中，我们将测试其中的6个:</p><p id="87ae" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu"> 3.1 F统计量</strong></p><p id="b7d6" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">f检验是<a class="ae nq" href="https://en.wikipedia.org/wiki/F-test" rel="noopener ugc nofollow" target="_blank">方差分析f检验</a>的结果。该检验计算为比率:<em class="oj">组间变异性</em> / <em class="oj">组内变异性</em>，其中组为目标类。</p><p id="abd2" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">其思想是，当组(0类图像、1类图像、…、9类图像)之间的可变性高而同一组内的可变性低时，像素是相关的。</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="f4a5" class="ob kz it nx b gy oc od l oe of">from sklearn.feature_selection import f_classif</span><span id="1264" class="ob kz it nx b gy og od l oe of">f = f_classif(X_train, y_train)[0]</span></pre><p id="811c" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu"> 3.2相互信息</strong></p><p id="2a45" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">互信息是两个变量之间相互依赖的度量。</p><p id="6d5f" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">由于MI的公式需要知道每个变量的概率分布(通常我们不知道分布)，因此<a class="ae nq" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html" rel="noopener ugc nofollow" target="_blank"> <em class="oj"> scikit-learn </em>实现</a>采用了基于<em class="oj"> k </em>最近邻距离的非参数近似。</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="b870" class="ob kz it nx b gy oc od l oe of">from sklearn.feature_selection import mutual_info_classif</span><span id="d4e2" class="ob kz it nx b gy og od l oe of">mi = mutual_info_classif(X_train, y_train)</span></pre><p id="28e8" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu"> 3.3逻辑回归</strong></p><p id="0c0e" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如果目标变量是分类的(如我们的情况)，可以对数据进行逻辑回归。然后，这些特征的相对重要性可以被用来从最相关到最不相关对它们进行排序。</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="caaf" class="ob kz it nx b gy oc od l oe of">from sklearn.linear_model import LogisticRegression</span><span id="bbe5" class="ob kz it nx b gy og od l oe of">logreg = LogisticRegression().fit(X_train, y_train)</span></pre><p id="8a77" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu"> 3.4灯GBM </strong></p><p id="f146" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">任何预测模型都可以做到这一点。比如LightGBM。</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="8d6e" class="ob kz it nx b gy oc od l oe of">from lightgbm import LGBMClassifier</span><span id="897b" class="ob kz it nx b gy og od l oe of">lgbm = LGBMClassifier(<br/>    objective = 'multiclass',<br/>    metric = 'multi_logloss',<br/>    importance_type = 'gain'<br/>).fit(X_train, y_train)</span></pre><p id="b16c" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">3.5博鲁塔</p><p id="0872" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">Boruta是2010年作为r的一个包设计的一个优雅的算法。Boruta的目的是告诉每个特征是否与目标变量有某种关系。所以，<strong class="ls iu">Boruta的输出更多的是每个特性的是/否，而不是特性的排序</strong>。</p><p id="1c8e" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">(如果你很好奇想知道更多关于Boruta的功能，我写了一篇关于它的帖子:<a class="ae nq" rel="noopener" target="_blank" href="/boruta-explained-the-way-i-wish-someone-explained-it-to-me-4489d70e154a"> Boruta准确地解释了你希望别人如何向你解释</a>)。</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="4028" class="ob kz it nx b gy oc od l oe of">from boruta import BorutaPy<br/>from sklearn.ensemble import RandomForestClassifier</span><span id="6c87" class="ob kz it nx b gy og od l oe of">boruta = BorutaPy(<br/>    estimator = RandomForestClassifier(max_depth = 5), <br/>    n_estimators = 'auto', <br/>    max_iter = 100<br/>).fit(X_train, y_train)</span></pre><p id="8a53" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated"><strong class="ls iu"> 3.6 MRMR </strong></p><p id="2a3b" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">MRMR(代表“最大相关性最小冗余”)是在2005年设计的用于特征选择的算法。MRMR背后的想法是<strong class="ls iu">识别与目标变量高度相关且彼此冗余度较小的特征子集</strong>。</p><p id="0bec" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">(如果你很想知道更多关于MRMR的运作，我写了一篇关于它的帖子:<a class="ae nq" rel="noopener" target="_blank" href="/mrmr-explained-exactly-how-you-wished-someone-explained-to-you-9cf4ed27458b"> MRMR准确地解释了你希望别人如何向你解释</a>)。</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="de83" class="ob kz it nx b gy oc od l oe of">from mrmr import mrmr_classif</span><span id="9815" class="ob kz it nx b gy og od l oe of">mrmr = mrmr_classif(X_train, y_train)</span></pre><p id="1854" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">所有这些算法都提供了特征的“排序”(除了Boruta，它有一个是/否的结果)。我们来看看根据算法的不同，排名是如何变化的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/b55c3b1dfc1e05a39bb42b19bdbd5ebd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jOPQDmlNrxtQE78E5-S_XQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">MNIST。根据不同的算法，像素按其相关性排序。[作者图]</p></figure><h1 id="2c95" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">4.哪种方法效果更好？</h1><p id="d7ac" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">此时，一个自然的问题是:</p><blockquote class="ms"><p id="7d0d" class="mt mu it bd mv mw mx my mz na nb ml dk translated">特征选择应该选择什么方法？</p></blockquote><p id="ae63" class="pw-post-body-paragraph lq lr it ls b lt nc ju lv lw nd jx ly lz ne mb mc md nf mf mg mh ng mj mk ml im bi translated">在数据科学中，最好的选择是测试不同的方法，看看哪种方法能更好地处理数据。因此，让我们在MNIST身上试试吧。</p><p id="4d49" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我们将采用5种方法提供的特征排名(因为Boruta不提供排名)，并查看在前<em class="oj"> K </em>个特征上训练预测模型时可以达到什么精度(对于最高达40的<em class="oj"> K </em>)。</p><pre class="kj kk kl km gt nw nx ny nz aw oa bi"><span id="53a4" class="ob kz it nx b gy oc od l oe of">import pandas as pd<br/>from catboost import CatBoostClassifier<br/>from sklearn.metrics import accuracy_score</span><span id="0f1f" class="ob kz it nx b gy og od l oe of">algos = ['f', 'mi', 'logreg', 'lightgbm', 'mrmr']<br/>ks = [1, 2, 5, 10, 15, 20, 30, 40]<br/>accuracy = pd.DataFrame(index = ks, columns = algos)</span><span id="de4b" class="ob kz it nx b gy og od l oe of">for algo in algos:<br/>  <br/>  for nfeats in ks:</span><span id="54ea" class="ob kz it nx b gy og od l oe of">    feats = ranking[algo][:n_feats]<br/>    <br/>    clf = CatBoostClassifier().fit(<br/>      X_train[:, feats], y_train,<br/>      eval_set = (X_test[:, feats], y_test),<br/>      early_stopping_rounds = 20<br/>    )<br/>                <br/>    accuracy.loc[k, algo] = accuracy_score(<br/>      y_true = y_test, y_pred = clf.predict(X_test[:, cols])))</span></pre><p id="bc5d" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这些是结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/ad46ca9d39b8bd4857d6c7f4bf447a97.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B6SfXl2x33WOn9GfpGSMLQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在前K个特征上训练的预测模型的准确性，K高达40。[作者图]</p></figure><p id="8a03" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">在这种情况下，<strong class="ls iu"> MRMR胜过了其他算法</strong>。</p><p id="825b" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">如图所示，在MRMR识别的最相关的40个像素上训练的预测器在测试图像上达到95.54%的准确度！</p><p id="6734" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">这令人印象深刻，尤其是考虑到:</p><ul class=""><li id="aeb4" class="nh ni it ls b lt mn lw mo lz nj md nk mh nl ml nm nn no np bi translated">40像素只是整个图像(由28 x 28 = 784个像素组成)的5%；</li><li id="1679" class="nh ni it ls b lt nr lw ns lz nt md nu mh nv ml nm nn no np bi translated">我们使用了一个预测模型(CatBoost ),没有进一步的调整，因此这个性能可能还可以进一步提高。</li></ul><p id="82c8" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">因此，在MNIST的情况下，<strong class="ls iu">我们可以丢弃95%的数据，但仍然可以获得95%以上的准确性(这相当于ROC下的面积为99.85%！)</strong>。</p><p id="45f2" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">即使MNIST是一个“简单”的数据集，主要的要点对于大多数真实世界的数据集都是有效的。通常，仅使用一小部分特征就可以实现高水平的准确性。有效的功能选择允许您构建在内存、时间、准确性、可解释性和调试简易性方面更高效的数据管道。</p></div><div class="ab cl om on hx oo" role="separator"><span class="op bw bk oq or os"/><span class="op bw bk oq or os"/><span class="op bw bk oq or"/></div><div class="im in io ip iq"><p id="839a" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">感谢您的阅读！我希望这篇文章对你有用。</p><p id="d6fa" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">本文中显示的结果完全可以通过这段代码重现:<a class="ae nq" href="https://github.com/smazzanti/mrmr/blob/main/notebooks/mnist.ipynb" rel="noopener ugc nofollow" target="_blank">https://github . com/smazzanti/mrmr/blob/main/notebooks/mnist . ipynb</a>。</p><p id="adb4" class="pw-post-body-paragraph lq lr it ls b lt mn ju lv lw mo jx ly lz mp mb mc md mq mf mg mh mr mj mk ml im bi translated">我感谢反馈和建设性的批评。如果你想谈论这篇文章或其他相关话题，可以发短信到<a class="ae nq" href="https://www.linkedin.com/in/samuelemazzanti/" rel="noopener ugc nofollow" target="_blank">我的Linkedin联系人</a>给我。</p></div></div>    
</body>
</html>