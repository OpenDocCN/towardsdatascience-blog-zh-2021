<html>
<head>
<title>An Introduction to Kernel Methods</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">内核方法介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-introduction-to-kernel-methods-9c16fc8fefd2?source=collection_archive---------25-----------------------#2021-04-09">https://towardsdatascience.com/an-introduction-to-kernel-methods-9c16fc8fefd2?source=collection_archive---------25-----------------------#2021-04-09</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="f7a2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从核密度估计到非参数回归。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/e64f1ce9b8997e1161f68e7e47db1dda.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5BFX3zSlwwIpDw9h"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">斯科特·韦伯在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="497c" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="2857" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">统计</strong>和<strong class="lt iu">机器学习</strong>中的许多技术都源自“从某种概率分布中抽取数据”的概念。<strong class="lt iu">概率分布</strong>本身是数据点可能性(或者频率，如果你愿意的话)的模型，即观察到某个值、结果等的可能性有多大。基于这种想法的方法(或算法)被称为<strong class="lt iu">参数化</strong>，因为它们是由底层分布(如高斯分布)的参数指定的。</p><p id="4520" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然而，假设的可能性模型可能不真实；“真实的”分布可能是偏斜的，具有不同成分的多模态，等等。一般来说，这对于分析数据的科学家来说是未知的。为了克服参数模型的局限性，一类基于所谓的<a class="ae ky" href="https://en.wikipedia.org/wiki/Kernel_(statistics)" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">内核</strong> </a>的<strong class="lt iu">非参数</strong>方法被并行开发。这篇文章旨在展示内核背后的基本思想以及它们在统计学和机器学习中的应用。</p><h1 id="e62f" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">核密度估计</h1><p id="4388" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">当估计某个变量<em class="ms"> x </em>的概率分布时，会出现第一个问题。为了获得分布的第一印象，绘制了具有不同<strong class="lt iu">仓宽</strong>的三个<strong class="lt iu">直方图</strong>(图1)。</p><div class="kj kk kl km gt ab cb"><figure class="mt kn mu mv mw mx my paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/09aff3c3e78c2a449856b6ea0f71064b.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*h_DVMTo0nMDz_-9YtREgNQ.png"/></div></figure><figure class="mt kn mu mv mw mx my paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/3d363aa820399c0543106b50754d676e.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*yCkqTMnC3Epb5O6iCsBm9g.png"/></div></figure><figure class="mt kn mu mv mw mx my paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/82b4fe6065be5f9d9ddc231aefb4ac28.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*bAyEaJ0nhgM8d3UgV2moRQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk mz di na nb translated"><strong class="bd nc">图1: </strong>不同仓宽下x的经验分布。(来源:作者)</p></figure></div><p id="2260" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">根据仓宽的不同，关于底层分布的结论完全不同；它可以是均匀的，也可以是具有三种或更多模式的多模式。因此，不清楚使用哪个概率分布来估计相应的参数(如均值和方差)。注意，每一次观察都将<em class="ms"> ρ </em>的一些“量”添加到它所在的仓中。因此，上面的图可以通过在<em class="ms"> x </em>的不同水平的个人贡献的总和来构建。假设样本是独立同分布的。因此，我们所能做的就是将概率分布表示为数据点<em class="ms"> xᵢ </em>周围的“小密度”的叠加，即所谓的<a class="ae ky" href="https://en.wikipedia.org/wiki/Kernel_density_estimation" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">核密度</strong> </a>(通过观察量归一化)。因此，<em class="ms">p(x)= 1/nσk(x，xᵢ) </em>，以<em class="ms"> K(x，y) </em>为内核。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nd ne l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">核密度函数。</p></figure><p id="3254" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">一个可能的内核是<a class="ae ky" href="https://en.wikipedia.org/wiki/Radial_basis_function_kernel" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">高斯内核</strong></a>；高斯核的公式如下所示。它本质上是一个以xᵢ为中心的概率分布。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nd ne l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">高斯核函数。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nf"><img src="../Images/1017679fefae8e7a341f6b5f6fc0a90c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H2AFUdKjGEzDIW4bbbVdjg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nc">图2: </strong>高斯核对于<em class="ng"> γ </em>有不同的取值。(来源:作者)</p></figure><p id="5273" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">高斯核由一个<strong class="lt iu">超参数</strong> <em class="ms"> γ </em>(图2)进行参数化，该参数作为方差并确定点<em class="ms"> xᵢ </em>周围的密度“浓度”。还有其他可用的核(均匀核、抛物线核、余弦核等)。)，但是现在使用这个就足够了。</p><p id="de17" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">形式上，内核是一个具有两个输入<em class="ms"> x </em>和<em class="ms"> y </em>的非负函数。对内核<em class="ms"> K(x，y) </em>的要求是</p><ul class=""><li id="e2f4" class="nh ni it lt b lu mn lx mo ma nj me nk mi nl mm nm nn no np bi translated">它是对称的，即<em class="ms"> K(x，y) = K(y，x) </em>，</li><li id="b20d" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nm nn no np bi translated">半正定的，即具有成对评估的有限矩阵应该没有负特征值，</li><li id="dacf" class="nh ni it lt b lu nq lx nr ma ns me nt mi nu mm nm nn no np bi translated">它应该整合成一个。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/463d5bfd0f82fd3f85b7858c332bd797.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4XyE86sUO38rpO6qlKujhw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nc">图3: </strong>不同<em class="ng"> γ值的概率密度。</em>(来源:作者)</p></figure><p id="9d54" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">出现的一个问题是<em class="ms"> γ </em>的选择，其值必须是固定的。根据<em class="ms"> γ </em>的值，密度可能看起来非常不同(图3)，就像图1中的面元宽度一样。对于太大的值，模型过于简单，重要信息丢失(拟合不足)，而对于太小的值，情况正好相反(拟合过度)。这里的主要问题是“小”样本量，因为通常估计量是<a class="ae ky" href="https://en.wikipedia.org/wiki/Consistency_(statistics)" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">一致的</strong> </a>并且收敛于无限多个样本的极限中的真实密度。虽然有选择<em class="ms"> γ </em>的经验公式，但最简单的方法是进行<a class="ae ky" href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">交叉验证</strong> </a>寻找其最优值。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nd ne l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">核密度估计的交叉验证。</p></figure><p id="5426" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">更准确地说，使验证集中的数据概率最大化的值<em class="ms"> γ </em>被认为是最优的。图4显示了作为<em class="ms"> γ </em>的函数的负对数似然性，以及具有最佳参数的密度，这似乎非常符合。</p><div class="kj kk kl km gt ab cb"><figure class="mt kn nw mv mw mx my paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/87e44d4db13ea205dcf8c4ca8f5400b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*aCoPFcekbJWrITP_94Xa8A.png"/></div></figure><figure class="mt kn nx mv mw mx my paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/9d6fabdfcb3ce7fd4ef10ed10a611079.png" data-original-src="https://miro.medium.com/v2/resize:fit:994/format:webp/1*fEr9p7guomVhld1Gh-KW3g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk ny di nz nb translated"><strong class="bd nc">图4: </strong>负对数似然和核密度，以及<em class="ng"> γ的最佳值。</em>(来源:作者)</p></figure></div><h1 id="43db" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">非参数回归</h1><p id="b73d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">因为回归和分类问题可以从概率分布中导出，所以核也可以用于执行回归和分类。一条回归线是条件分布<em class="ms"> p(y|x) </em>的期望<em class="ms"> E </em>，即<em class="ms">f(x)= E【p(y | x)】</em>。而且认为<em class="ms"> p(y|x) = p(x，y)/p(x) </em>。于是，<em class="ms"> E[p(y|x)] = </em> <em class="ms"> ∫ y[p(x，y)/p(x)]dy </em>。</p><p id="f167" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这产生了<em class="ms"> f(x) </em>的以下表示，称为<a class="ae ky" href="https://en.wikipedia.org/wiki/Kernel_regression" rel="noopener ugc nofollow" target="_blank"> Nadaraya-Watson核估计器</a>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nd ne l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">非参数回归函数。</p></figure><p id="2a37" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">预测可以被视为加权平均，而权重是训练数据中的样本到新点的相对位置。因此，核充当相似性度量。此外，出现与上述相同的问题，因为超参数<em class="ms"> γ </em>必须被“调整”。这一次，要优化的度量将是<a class="ae ky" href="https://en.wikipedia.org/wiki/Mean_squared_error" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">均方误差</strong> </a>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nd ne l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">回归问题的交叉验证。</p></figure><p id="4f47" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="ms"> γ </em>的最佳值是最小化均方误差的值。在图5中，最左边的图显示，对于高值，函数过于简单(欠拟合)，而对于低值，函数变得有些嘈杂(过拟合)。最佳值似乎在0.01和0.001之间，最右边的图显示了通过交叉验证发现的该问题的最佳回归线。注意，均方误差的最小值对应于数据中不可约的误差(噪声)。</p><div class="kj kk kl km gt ab cb"><figure class="mt kn oa mv mw mx my paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/16eba627a9237da597ab2e1071df41c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*pxDM-f-NSRwrM0Ykr8_yvQ.png"/></div></figure><figure class="mt kn ob mv mw mx my paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/3d692d87a344bf4bf671b6a24a84254f.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*KKCPXBv2W4y4hBxiv5oniA.png"/></div></figure><figure class="mt kn oa mv mw mx my paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><img src="../Images/6d698b91d6f087bb26c06614c8843455.png" data-original-src="https://miro.medium.com/v2/resize:fit:668/format:webp/1*Njfd6iSwb3D9aMoyWlDEQA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk mz di na nb translated"><strong class="bd nc">图5: </strong>非参数回归。(来源:作者)</p></figure></div><h1 id="2c07" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">内核作为内积</h1><p id="f499" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">除了作为一种表达概率分布的方式，内核还可以以另一种方式进行解释，这在对高斯内核进行<strong class="lt iu">泰勒级数展开</strong>时变得清晰(图6)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/421b9a6ff0621ecc6dee5792faceb421.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MFPcmWtmcXs4sKMYBqWtBw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nc">图6: </strong>高斯核的泰勒级数展开。(来源:作者)</p></figure><p id="beba" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">展开表明，核<em class="ms"> K(x，y) </em>实际上是两个向量<em class="ms"> x </em>、<em class="ms"> y </em>的多项式<strong class="lt iu">基展开</strong>φ的<a class="ae ky" href="https://en.wikipedia.org/wiki/Inner_product_space" rel="noopener ugc nofollow" target="_blank"/>&lt;(<em class="ms">x</em>)、φ(<em class="ms">y</em>)&gt;(具有无限阶)。因此，原始数据被隐式地映射到由多项式的新基向量构建的新空间，即<strong class="lt iu">特征图</strong> — <strong class="lt iu"> </strong>，在<strong class="lt iu">支持向量机</strong>中被用作所谓的<strong class="lt iu">核技巧</strong>。</p><h1 id="083a" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="ec6d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">总之，核方法是估计函数的另一种简单的方法。然而，这种非参数技术也有一些缺点。例如，计算从训练转移到评估(<a class="ae ky" href="https://en.wikipedia.org/wiki/Lazy_learning" rel="noopener ugc nofollow" target="_blank">懒惰学习</a>)，因为训练几乎是免费的(除了只有一个超参数估计)。另一方面，他们不能在训练数据之外进行外推，这在图5中最右边的图中是可见的；对于<em class="ms"> x </em>的高值，曲线变平，这直观上看起来是“错误的”。最后，潜在的机制没有被发现，也就是说，模型不是真正可解释的，除非应用一些模型检查技术。对于参数模型，参数的大小表示变量的相关性。尽管如此，如果唯一的目标是预测，核方法无疑是经典方法的一种替代方法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/f8bdc172de07fceab2e49ba973b5172e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*82-1Lk-n3WQB8W3x"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">斯蒂芬·菲利普斯-Hostreviews.co.uk在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure></div></div>    
</body>
</html>