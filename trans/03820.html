<html>
<head>
<title>Four Deep Learning Papers to Read in April 2021</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">2021年4月要读的四篇深度学习论文</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/four-deep-learning-papers-to-read-in-april-2021-77f6b0e42b9b?source=collection_archive---------6-----------------------#2021-03-30">https://towardsdatascience.com/four-deep-learning-papers-to-read-in-april-2021-77f6b0e42b9b?source=collection_archive---------6-----------------------#2021-03-30</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="a776" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="181f" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">从元梯度到时钟值，神经网络的全局工作空间理论和训练稳定性的边缘</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/67234c631898a78be28164059c2c142d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DkJ2zpszjSsMiUantuEuNQ.png"/></div></div></figure><p id="078b" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">欢迎来到四月版的【T2:机器学习拼贴】系列，在这里我提供了不同深度学习研究流的概述。那么这个系列是关于什么的呢？简单地说，我为我最近最喜欢的一篇论文起草了一张幻灯片的视觉摘要。每一周。在月末，所有的视觉拼贴画都被收集在一篇总结性的博客文章中。因此，我希望给你一个视觉和直观的深入了解一些最酷的趋势。所以，废话不多说:这里是我在2021年3月读过的四篇最喜欢的论文，以及为什么我认为它们对深度学习的未来很重要。</p><h2 id="2f3a" class="ma mb it bd mc md me dn mf mg mh dp mi lm mj mk ml lq mm mn mo lu mp mq mr iz bi translated">“通过元学习子目标发现选项”</h2><p id="e4bc" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated"><em class="mx">作者:Veeriah等人(2021) </em>📝<a class="ae lz" href="https://arxiv.org/abs/2102.06741" rel="noopener ugc nofollow" target="_blank">论文</a></p><p id="84a7" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">一段话总结:</strong>电机控制是一个极具挑战性的问题。我们人类非常擅长这一点，因为我们在多个扩展的时间尺度上进行规划:我们不是绘制出每一个肌肉动作，而是在抽象的层面上进行推理，并执行一系列精细的动作。分层强化学习(HRL)旨在借助所谓的时间抽象来模仿这种方法。简单地说，时态抽象就是一个在一段时间内执行的机动程序。选项(<a class="ae lz" href="https://www.sciencedirect.com/science/article/pii/S0004370299000521" rel="noopener ugc nofollow" target="_blank">萨顿等人，1999 </a>)是这种时间抽象的一种特殊类型。它们由一个子策略和一个相应的终止条件组成。选项策略由更高级别的管理器调用并执行，直到终止条件停止它。HRL的一个关键问题是如何自动推断出有用的选项，这些选项可以在多项任务中转换？Veeriah等人(2021)提出通过<a class="ae lz" rel="noopener" target="_blank" href="/meta-policy-gradients-a-survey-78dc742d395d">元梯度</a>学习选项参数化。在外环中，元梯度通过优化更新步骤过程传播高阶梯度来优化超参数，这取决于它们(以可微分的方式)。在本文中，优化的超参数被选择为指定选项的神经网络。所提出的称为MODAC的多寿命元梯度方法能够发现有用的选项，这些选项转移到新的设置，其中仅允许管理器策略被重新训练。因此，元梯度方法可以从任务分布中提取有意义的规律。他们在一个标准的四个房间的问题上测试他们的方法，然后将其扩展到更具挑战性的DeepMind实验室领域。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/a16ac3a5dbaad35c9d8328997283db94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*08MhrjK4rm-g9Dvh5jQK7g.png"/></div></div><p class="my mz gj gh gi na nb bd b be z dk translated">ML-Collage [9/52]:数字改编自Veeriah等人(2021) |📝<a class="ae lz" href="https://arxiv.org/abs/2102.06741" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><h2 id="4ff6" class="ma mb it bd mc md me dn mf mg mh dp mi lm mj mk ml lq mm mn mo lu mp mq mr iz bi translated"><strong class="ak">“发条变分自动编码器”</strong></h2><p id="ce14" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated"><em class="mx">作者:Saxena等人(2021) </em>📝<a class="ae lz" href="https://arxiv.org/abs/2102.09532" rel="noopener ugc nofollow" target="_blank">论文</a></p><p id="16f1" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><strong class="lf jd">一段话总结:</strong>递归生成模型在生成长序列的清晰图像和捕捉视频中的长期依赖关系方面存在困难。Saxena等人(2021)的时钟变分自动编码器(CW-VAE)旨在通过扩展<a class="ae lz" href="https://arxiv.org/pdf/1811.04551" rel="noopener ugc nofollow" target="_blank">递归状态空间模型</a>(RSSM；Hafner et al. 2019)，这是一类复发性VAEs。CW-VAEs的核心是通过引入以不同固定时钟速度变化的潜在时间层次来扩展这些潜在动态模型。顶层以较慢的速率适应，并调节较低层的生成过程。等级越低，速度越快。在最低层，该模型通过转置CNN的上采样输出生成的图像。使用证据下限(ELBO)目标对整个递归VAE架构进行端到端训练。作者证明，这种动态潜在变量的时间抽象层次优于许多基线，这些基线不包含潜在层次或所有级别以相同的速度跳动。我特别喜欢cool ablation研究，它旨在提取存储在不同级别的内容信息。通过切断流入顶层的输入信号，作者能够证明顶层向较低层提供全局非特定信息。最后，他们还表明，潜在的动态能够适应预先条件序列输入的速度:高频序列导致更多的信息被快速的低水平潜在变量捕获。总之，作用于不同时间尺度的机制的层次结构不仅对于强化学习非常有用，而且对于生成模型也非常有用。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nc"><img src="../Images/210dee13679b7b1adcb8a2a5ead6ce24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xp-ZAMVLGnaBMXCXH2DvAQ.png"/></div></div><p class="my mz gj gh gi na nb bd b be z dk translated">ML-Collage [10/52]:数字改编自Saxena等人(2021) |📝<a class="ae lz" href="https://arxiv.org/abs/2102.09532" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><h2 id="9025" class="ma mb it bd mc md me dn mf mg mh dp mi lm mj mk ml lq mm mn mo lu mp mq mr iz bi translated"><strong class="ak">“通过共享的全局工作空间协调神经模块”</strong></h2><p id="11b2" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated"><em class="mx">作者:Goyal等人(2021) </em> |📝<a class="ae lz" href="https://arxiv.org/abs/2103.01197" rel="noopener ugc nofollow" target="_blank">论文</a></p><p id="ba32" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">一段话总结:最突出的意识理论之一是<a class="ae lz" href="https://en.wikipedia.org/wiki/Global_workspace_theory" rel="noopener ugc nofollow" target="_blank">全球工作空间理论</a>。它提出了一个简单的认知架构，其中经过处理的感官知觉被投射到一个共享的工作空间，也称为“黑板”。来自不同来源的信息被选择性地写入这个工作空间，并被下意识地处理。这个处理阶段集成了不同的模态并丢弃了不相关的特征。之后，转换后的信息被传播到与意识过程相关的其他大脑区域。Goyal等人(2021)从这种高级意识的神经科学理论中获得灵感，并概述了一种将工作空间与注意力机制相结合的计算框架，以促进学习到的神经模块之间的协调。更具体地说，作者提出了一个低维瓶颈(也称为共享工作空间)来促进专家模块的同步。不同的神经网络(例如变压器或不同的LSTMs)必须竞争写入“瓶颈”工作区。然后，基于软或硬注意机制更新输出表示。关键思想是带宽限制有助于独立但整合的机制的协调学习。使用一组详尽的实验，作者表明，所提出的机制促进了模块之间的专业化，并有助于稳定它们的端到端训练。此外，工作空间的低维特性减少了专家之间昂贵的成对注意力交互。因此，它不仅有益于训练，而且有益于推理。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nd"><img src="../Images/ab59a966786c11b07c4eda0f6ee76216.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vjYs_0ceRtY7Ls1FygkyjQ.png"/></div></div><p class="my mz gj gh gi na nb bd b be z dk translated">ML-Collage [11/52]:图片改编自Goyal等人的作品(2021) |📝<a class="ae lz" href="https://arxiv.org/abs/2103.01197" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure><h2 id="321d" class="ma mb it bd mc md me dn mf mg mh dp mi lm mj mk ml lq mm mn mo lu mp mq mr iz bi translated"><strong class="ak">“神经网络上的梯度下降通常发生在稳定性的边缘”</strong></h2><p id="94a5" class="pw-post-body-paragraph ld le it lf b lg ms kd li lj mt kg ll lm mu lo lp lq mv ls lt lu mw lw lx ly im bi translated"><em class="mx">作者:科恩等人(2021) </em>📝<a class="ae lz" href="http://arxiv.org/abs/2103.00065" rel="noopener ugc nofollow" target="_blank">论文</a></p><p id="22fe" class="pw-post-body-paragraph ld le it lf b lg lh kd li lj lk kg ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">一段话总结:深度学习中最迷人但仍未得到充分解释的观察结果之一是，我们似乎能够仅使用随机梯度下降等简单算法来有效优化数十亿个参数。但是我们对学习动力和趋同行为到底了解多少呢？Cohen等人(2021年)后退一步，研究批次由整个数据集组成时梯度下降的特殊情况。作者表明，这种全批次梯度下降版本在一个非常特殊的制度。也就是说，处于“稳定的边缘”。这个边缘是关于什么的？神经网络训练有两个阶段:在最初的第一阶段，训练损失Hessian(也称为锐度)的最大特征值逐渐增加，直到它达到2/学习速率。在这个阶段，训练损失单调下降。一旦这个特征值达到2/学习率，就达到了“稳定边缘”。之后，梯度下降抑制了清晰度的进一步增长。相反，它徘徊在2/学习率阈值之上。在短时间内，训练损失不再表现为单调，而是波动的。但是在更长的时间尺度上，梯度下降仍然可以减少损失。作者通过多项任务和不同架构(包括标准CNN和变压器)验证了这一经验观察。由此产生的发现质疑了关于梯度下降的传统优化智慧的许多方面:梯度下降如何抑制清晰度的持续增长？这对学习费率表意味着什么？我们真的需要让它们随着时间退火吗？好的科学工作开启了许多有趣的未来研究问题，这项工作无疑属于这一范畴。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ne"><img src="../Images/df8381591257ec0e7b05532e87ecd56b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sSAk09bH8e4d1jG6PDilRQ.png"/></div></div><p class="my mz gj gh gi na nb bd b be z dk translated">ML-Collage [12/52]:数字改编自Cohen等人(2021) |📝<a class="ae lz" href="http://arxiv.org/abs/2103.00065" rel="noopener ugc nofollow" target="_blank">论文</a></p></figure></div><div class="ab cl nf ng hx nh" role="separator"><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk nl"/><span class="ni bw bk nj nk"/></div><div class="im in io ip iq"><blockquote class="nm nn no"><p id="3ccc" class="ld le mx lf b lg lh kd li lj lk kg ll np ln lo lp nq lr ls lt nr lv lw lx ly im bi translated">这是这个月的。让我知道你最喜欢的论文是什么。如果你想获得一些每周ML拼贴输入，查看Twitter上的标签<a class="ae lz" href="https://twitter.com/hashtag/mlcollage" rel="noopener ugc nofollow" target="_blank"># ML collage</a>。你也可以看看我上一篇博文中的拼贴画:</p></blockquote><div class="ns nt gp gr nu nv"><a rel="noopener follow" target="_blank" href="/four-deep-learning-papers-to-read-in-march-2021-9d933f52aafa"><div class="nw ab fo"><div class="nx ab ny cl cj nz"><h2 class="bd jd gy z fp oa fr fs ob fu fw jc bi translated">2021年3月要读的四篇深度学习论文</h2><div class="oc l"><h3 class="bd b gy z fp oa fr fs ob fu fw dk translated">从合成梯度到胶囊网络，网络训练守恒定律&amp;多主体生成模型</h3></div><div class="od l"><p class="bd b dl z fp oa fr fs ob fu fw dk translated">towardsdatascience.com</p></div></div><div class="oe l"><div class="of l og oh oi oe oj lb nv"/></div></div></a></div><blockquote class="nm nn no"><p id="2d13" class="ld le mx lf b lg lh kd li lj lk kg ll np ln lo lp nq lr ls lt nr lv lw lx ly im bi translated">最后，如果你想了解更多关于RL和其他领域的元梯度的潜力，你可以查看我以前的调查博客。我也有机会在<a class="ae lz" href="https://youtu.be/hfaZwgk_iS0" rel="noopener ugc nofollow" target="_blank">最近的ML Street Talk播客插曲</a>中采访了主要的元渐变作者之一——来自DeepMind的汤姆·萨哈维:</p></blockquote><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="ok ol l"/></div></figure></div></div>    
</body>
</html>