<html>
<head>
<title>Hyperparameter Tuning in Lasso and Ridge Regressions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Lasso和Ridge回归中的超参数调整</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hyperparameter-tuning-in-lasso-and-ridge-regressions-70a4b158ae6d?source=collection_archive---------4-----------------------#2021-05-16">https://towardsdatascience.com/hyperparameter-tuning-in-lasso-and-ridge-regressions-70a4b158ae6d?source=collection_archive---------4-----------------------#2021-05-16</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4028" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">我用来优化正则化参数的方法。scikit-learn的Python指南。</h2></div><div class="ki kj kk kl gt ab cb"><figure class="km kn ko kp kq kr ks paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><img src="../Images/305d69245dfe1b31742b9fc225230486.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*p5n_8kbEbtG3cAzSMBLEbA.png"/></div></figure><figure class="km kn ko kp kq kr ks paragraph-image"><div role="button" tabindex="0" class="kt ku di kv bf kw"><img src="../Images/c2ef05bf8cb7bfc2b44a569e17cab3ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*pzAKfvpJ1WDKVR4gzFtoPQ.png"/></div><p class="kz la gj gh gi lb lc bd b be z dk ld di le lf translated">作者图片</p></figure></div><p id="1371" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">在这篇文章中，我们首先来看看套索和岭回归时的一些常见错误，然后我将描述我通常采取的调整超参数的步骤。代码是用Python写的，我们主要依靠<a class="ae mc" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>。本指南主要关注套索的例子，但基本理论与山脊非常相似。</p><p id="5617" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">起初，我并没有真正意识到需要另一个关于这个主题的指南——毕竟这是一个非常基本的概念。然而，当我最近想证实一些事情时，我意识到，很多指南要么非常学术性，要么太简单，要么就是完全错误。一个非常常见的混淆来源是，在sklearn中，总是有十几种不同的方法来计算同一个东西。</p><p id="5cfb" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">所以，事不宜迟，这里是我对这个话题的2美分。</p><h1 id="58c7" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">快速理论背景</h1><p id="bdfd" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">Lasso和Ridge都是正则化方法，它们旨在通过引入惩罚因子来正则化复杂模型。它们在减少过度拟合、处理多重共线性或自动特征工程方面非常出色。这可能看起来违反直觉，但通过使模型更努力地解释训练数据，我们可以更好地理解底层结构，从而更好地概括和更好地拟合测试数据。</p><h2 id="8683" class="na me it bd mf nb nc dn mj nd ne dp mn lp nf ng mp lt nh ni mr lx nj nk mt nl bi translated">线性回归</h2><p id="685f" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">根据<a class="ae mc" href="https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression" rel="noopener ugc nofollow" target="_blank"> sklearn </a>的公式，这是在<a class="ae mc" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression" rel="noopener ugc nofollow" target="_blank">线性回归</a>模型中最小化的表达式，即所谓的<a class="ae mc" href="https://en.wikipedia.org/wiki/Ordinary_least_squares" rel="noopener ugc nofollow" target="_blank">普通最小二乘法</a>:</p><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/44b8c38791e751be7a2d2c61aea76a0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:290/format:webp/1*-b--A8N2U17J_-UILinO7Q.png"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated">线性回归公式</p></figure><p id="452d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">其中<em class="nn"> X </em>矩阵为自变量，<em class="nn"> w </em>为权重系数，<em class="nn"> y </em>为因变量。</p><h2 id="cd9f" class="na me it bd mf nb nc dn mj nd ne dp mn lp nf ng mp lt nh ni mr lx nj nk mt nl bi translated">山脉</h2><p id="483f" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated"><a class="ae mc" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge" rel="noopener ugc nofollow" target="_blank">岭</a>回归采用该表达式，并在系数平方的末尾添加一个惩罚因子:</p><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/b083bb591e408219f3b28e5d5b9d6fbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:430/format:webp/1*nxfnR1JfaWlWXWUuXkJsIw.png"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated">岭公式</p></figure><p id="d36e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这里，α是正则化参数，这是我们要优化的。该模型惩罚较大的系数，并试图更均匀地分配权重。通俗地说，这就是山脊模型的作用:</p><blockquote class="np nq nr"><p id="875f" class="lg lh nn li b lj lk ju ll lm ln jx lo ns lq lr ls nt lu lv lw nu ly lz ma mb im bi translated">X1，我们看到你做得很好，如果不是因为处罚因素，我们会给你很大的压力。然而，X2只是比你稍微差一点，如果我们在你们两个之间分配权重，我们会得到更低的惩罚，因此总得分会更高。</p></blockquote><h2 id="32cb" class="na me it bd mf nb nc dn mj nd ne dp mn lp nf ng mp lt nh ni mr lx nj nk mt nl bi translated">套索</h2><p id="a265" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated"><a class="ae mc" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso" rel="noopener ugc nofollow" target="_blank"> Lasso </a>做了类似的事情，但是使用权重的绝对值之和(l1范数)作为惩罚。</p><p id="a6d9" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><em class="nn">注意:在sklearn公式中还有一个</em> <code class="fe nv nw nx ny b"><em class="nn">n_samples</em></code> <em class="nn">，它是观测值的个数，对于同一个X和y，它应该是不变的，我没有找到为什么会有那个的解释，也许是为了比较不同的模型，如果你有更好的想法，请告诉我。</em></p><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/3eff7bd645d573a7aadac97625dc11a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:590/format:webp/1*CXnXA7u8VJ84rg5pbOrDZw.png"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated">套索公式</p></figure><p id="d026" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">Lasso将开始降低不太重要的变量的系数，也有可能将系数降低到0。通俗地说:</p><blockquote class="np nq nr"><p id="ca9f" class="lg lh nn li b lj lk ju ll lm ln jx lo ns lq lr ls nt lu lv lw nu ly lz ma mb im bi translated">X1，你对总分的最小贡献被记录下来。然而，根据最新的处罚分数，我们将不得不让你退出回归。把你留在身边不值得。</p></blockquote><h2 id="715f" class="na me it bd mf nb nc dn mj nd ne dp mn lp nf ng mp lt nh ni mr lx nj nk mt nl bi translated">弹性网</h2><p id="6ac7" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">值得注意的是，你还可以用一个<a class="ae mc" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet" rel="noopener ugc nofollow" target="_blank">弹性网</a>将两种惩罚结合在同一个模型中。你需要优化两个超参数。在本指南中，我们不打算讨论这个选项。</p><h1 id="a405" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">使用的库</h1><p id="bed9" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">如果您想遵循代码，这里列出了您需要的所有库:</p><pre class="ki kj kk kl gt oa ny ob oc aw od bi"><span id="bb08" class="na me it ny b gy oe of l og oh"><strong class="ny iu">import</strong> pandas <strong class="ny iu">as</strong> pd<br/><strong class="ny iu">import</strong> numpy <strong class="ny iu">as</strong> np<br/><strong class="ny iu">import</strong> matplotlib.pyplot <strong class="ny iu">as</strong> plt<br/><strong class="ny iu">import</strong> seaborn <strong class="ny iu">as</strong> sns<br/><strong class="ny iu">from</strong> sklearn.metrics <strong class="ny iu">import</strong> \<br/>    r2_score, get_scorer<br/><strong class="ny iu">from</strong> sklearn.linear_model <strong class="ny iu">import</strong> \<br/>    Lasso, Ridge, LassoCV,LinearRegression<br/><strong class="ny iu">from</strong> sklearn.preprocessing <strong class="ny iu">import</strong> \<br/>    StandardScaler, PolynomialFeatures<br/><strong class="ny iu">from</strong> sklearn.model_selection <strong class="ny iu">import \<br/>   </strong> KFold, RepeatedKFold, GridSearchCV, \<br/>    cross_validate, train_test_split</span></pre><h1 id="5fea" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">一般提示</h1><p id="2aa7" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">在这一节中，我们将讨论一些通用技巧和常见错误，以避免正则化回归。示例使用了波士顿住房数据，您可以从<a class="ae mc" href="https://www.kaggle.com/prasadperera/the-boston-housing-dataset?select=housing.csv" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>下载。</p><p id="ed8c" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">处理数据:</p><pre class="ki kj kk kl gt oa ny ob oc aw od bi"><span id="ac74" class="na me it ny b gy oe of l og oh">column_names = \<br/>    ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE',\<br/>     'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']<br/>data = pd.read_csv("../datasets/housing.csv", \<br/>    header=None, delimiter=r"\s+", names=column_names)<br/>y = data['MEDV']<br/>X = data.drop(['MEDV'], axis = 1)</span></pre><h2 id="d438" class="na me it bd mf nb nc dn mj nd ne dp mn lp nf ng mp lt nh ni mr lx nj nk mt nl bi translated">提示1:调整独立变量</h2><p id="ee7c" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">正如标题所暗示的:这绝对是调整你的变量以进行正则化回归的必要条件。(正如我们所知，像缩放这样的线性变换对普通线性回归的预测没有影响。)如果你仔细看看这些公式，你就会很清楚为什么你必须为正则化回归进行缩放:如果你的一个变量恰好在一个非常小的范围内，它的系数将会很大，因此，它将会由于惩罚而受到更多的惩罚。反之亦然，一个大范围的变量将得到小的系数，受惩罚的影响较小。套索和山脊都是如此。</p><p id="e1e9" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">假设你做了如下的事情。</p><p id="c72f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">(再次说明，这个例子是没有缩放的，<strong class="li iu">不会产生正确的结果，不要这样做。</strong>另外，请注意，除了缩放之外，还有其他问题，我们将很快返回。)</p><pre class="ki kj kk kl gt oa ny ob oc aw od bi"><span id="f30d" class="na me it ny b gy oe of l og oh"># don't copy!!!</span><span id="f4a9" class="na me it ny b gy oi of l og oh">cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)<br/>lasso_alphas = np.linspace(0, 0.2, 21)</span><span id="8fbc" class="na me it ny b gy oi of l og oh">lasso = Lasso()<br/>grid = dict()<br/>grid['alpha'] = lasso_alphas<br/>gscv = GridSearchCV( \<br/>    lasso, grid, scoring='neg_mean_absolute_error', \<br/>    cv=cv, n_jobs=-1)<br/>results = gscv.fit(X, y)</span><span id="c66b" class="na me it ny b gy oi of l og oh">print('MAE: %.5f' % results.best_score_)<br/>print('Config: %s' % results.best_params_)</span></pre><p id="0a5a" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这将是结果:</p><pre class="ki kj kk kl gt oa ny ob oc aw od bi"><span id="a761" class="na me it ny b gy oe of l og oh">MAE: -3.37896<br/>Config: {'alpha': 0.01}</span></pre><p id="64df" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">然而，如果你事先调整你的<code class="fe nv nw nx ny b">X</code>变量，你通常会得到一个更好的分数。为了缩放，我们可以使用sklearn的<a class="ae mc" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html" rel="noopener ugc nofollow" target="_blank"> StandardScaler </a>。这种方法将变量集中在0附近，使标准偏差等于1。</p><pre class="ki kj kk kl gt oa ny ob oc aw od bi"><span id="6837" class="na me it ny b gy oe of l og oh">sc = StandardScaler()</span><span id="7795" class="na me it ny b gy oi of l og oh">X_scaled = sc.fit_transform(X)<br/>X_scaled = pd.DataFrame(data = X_scaled, columns = X.columns)</span></pre><p id="d168" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">如果我们在上面的代码块中用<code class="fe nv nw nx ny b">X_scaled</code>替换<code class="fe nv nw nx ny b">X</code>，我们会得到:</p><pre class="ki kj kk kl gt oa ny ob oc aw od bi"><span id="7435" class="na me it ny b gy oe of l og oh">MAE: -3.35080<br/>Config: {'alpha': 0.08}</span></pre><p id="5b10" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">是的，没有太大的改善，但这是由于我们稍后将看到的一些因素。最重要的是，波士顿住房数据是线性回归的一个非常好的定制玩具例子，所以我们不能提高预测那么多。</p><blockquote class="np nq nr"><p id="f86e" class="lg lh nn li b lj lk ju ll lm ln jx lo ns lq lr ls nt lu lv lw nu ly lz ma mb im bi translated"><strong class="li iu">总结</strong>:正则化前使用StandardScaler对自变量进行缩放。无需调整因变量。</p></blockquote><h2 id="83af" class="na me it bd mf nb nc dn mj nd ne dp mn lp nf ng mp lt nh ni mr lx nj nk mt nl bi translated">提示№2。:当Alpha等于零时…</h2><p id="a443" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">如果在“套索”和“山脊”中为alpha参数选择0，则基本上是在拟合线性回归，因为在公式的OLS部分没有应用惩罚。</p><p id="a6f4" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">由于计算的复杂性，sklearn文档实际上不鼓励使用alpha = 0参数运行这些模型。我还没有遇到过它引起任何计算问题的情况，它总是给出与线性回归模型相同的结果。</p><blockquote class="np nq nr"><p id="4f2f" class="lg lh nn li b lj lk ju ll lm ln jx lo ns lq lr ls nt lu lv lw nu ly lz ma mb im bi translated"><strong class="li iu">总结</strong>:挑alpha = 0没有意义，那就是简单的线性回归。</p></blockquote><h2 id="3831" class="na me it bd mf nb nc dn mj nd ne dp mn lp nf ng mp lt nh ni mr lx nj nk mt nl bi translated">提示№3:扫完一次不要停下来</h2><p id="c26b" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">在上面的例子中，我们遍历了一组alphass，尝试了所有的alpha，并选择了得分最高的一个。然而，就像通常使用<a class="ae mc" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" rel="noopener ugc nofollow" target="_blank"> GridSearchCV </a>时一样，建议进行多次扫描。找到alpha值最高的区域，然后进行更细致的检查。</p><p id="c1ee" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">根据我的经验，尤其是Lasso，选择最低的非零参数是一个常见的错误，而实际上最佳参数是一个小得多的数字。请看后半部分的例子。</p><p id="1166" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><em class="nn">注:当然，我们永远不会用网格搜索法找到实际的最优数，但我们可以足够接近。</em></p><p id="1546" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">你也可以看到结果。这是未缩放版本的样子:</p><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/e0981ad1e2b6011f6cfec2be43c83f75.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*AB7ib5II3EI1nJKI7KfymQ.png"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated">住房数据，MAE，未按比例</p></figure><p id="d657" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">对于每个alpha，GridSearchCV都符合一个模型，我们选择了验证数据得分最高的alpha(例如，<a class="ae mc" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedKFold.html" rel="noopener ugc nofollow" target="_blank">重复文件夹</a>中测试折叠的平均得分)。在这个例子中，你可以看到在0和0.01之间可能没有一个疯狂的尖峰。当然，这仍然是不正确的，因为我们没有扩展。</p><p id="71d0" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这是缩放版本的图:</p><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/4f721f6ccd4650f9c9e8e04d24647ab5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*rTxnBNIOeMcgNF5Qq4IN2Q.png"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated">住房数据，MAE，按比例</p></figure><p id="80bd" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">再一次，它看起来很好，在0.07和0.09之间可能没有什么奇怪的事情发生。</p><blockquote class="np nq nr"><p id="0a62" class="lg lh nn li b lj lk ju ll lm ln jx lo ns lq lr ls nt lu lv lw nu ly lz ma mb im bi translated"><strong class="li iu">总结</strong>:情节是你的朋友，观察阿尔法曲线。确保你选择的阿尔法在一个好的“曲线”区域。</p></blockquote><h2 id="9e04" class="na me it bd mf nb nc dn mj nd ne dp mn lp nf ng mp lt nh ni mr lx nj nk mt nl bi translated">提示№4:仔细考虑你的评分方法</h2><p id="0af9" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">你可能会尝试用不同的方法来检查你的结果。如前所述，sklearn通常有一堆不同的方法来计算同一个东西。例如，有一种<a class="ae mc" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html" rel="noopener ugc nofollow" target="_blank"> LassoCV </a>方法将Lasso和GridSearchCV合二为一。</p><p id="5fbd" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">您可以尝试这样的方法来获得最佳alpha(在示例中不再使用未缩放的版本):</p><pre class="ki kj kk kl gt oa ny ob oc aw od bi"><span id="40ca" class="na me it ny b gy oe of l og oh">lasso = LassoCV(alphas=lasso_alphas, cv=cv, n_jobs=-1)<br/>lasso.fit(X_scaled, y)<br/>print('alpha: %.2f' % lasso.alpha_)</span></pre><p id="8489" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这将返回:</p><pre class="ki kj kk kl gt oa ny ob oc aw od bi"><span id="a2b6" class="na me it ny b gy oe of l og oh">alpha: 0.03</span></pre><p id="2a5a" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">等等，这不是上面那个0.08的阿尔法值吗？是的。差异的原因是什么？LassoCV使用R分数，您不能更改它，而在前面，我们在GridSearchCV对象中指定了MAE(嗯，减去MAE，但这只是为了最大化并保持一致)。这是我警告过你不要复制的代码:</p><pre class="ki kj kk kl gt oa ny ob oc aw od bi"><span id="62a6" class="na me it ny b gy oe of l og oh">scoring='neg_mean_absolute_error'</span></pre><p id="e460" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">看到了吧，问题是，sklearn有几十种评分方法，看看<a class="ae mc" href="https://scikit-learn.org/stable/modules/model_evaluation.html" rel="noopener ugc nofollow" target="_blank">列表</a>。你当然可以选择比如说<a class="ae mc" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.max_error.html#sklearn.metrics.max_error" rel="noopener ugc nofollow" target="_blank"> max_error </a>来衡量你的模型的性能。但是，该模型针对平方差进行了优化。我认为使用任何从平方差得到的东西都更加一致。毕竟LassoCV用的是R，所以也许那是个征兆？</p><p id="8848" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这整个<em class="nn">“在一个基础上优化，然后在另一个基础上比较性能”</em>实际上在上面的图中非常明显。注意绿线在一段时间内是如何增加的。那是训练分数。正常情况下，在我们施加惩罚因子后，它应该不会表现得更好。</p><p id="af7e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">通常，这是你会看到的曲线形状。训练数据得分立即下降，验证数据得分上升一段时间，然后下降:</p><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/4c3b7eca68e26692c5dd4705b11347c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*1_OlE42bYF4L6452vfR6cw.png"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated">住房数据，R，按比例</p></figure><blockquote class="np nq nr"><p id="0025" class="lg lh nn li b lj lk ju ll lm ln jx lo ns lq lr ls nt lu lv lw nu ly lz ma mb im bi translated"><strong class="li iu">总结</strong>:使用R或其他基于方差的模型作为回归的主要评分。</p></blockquote><h1 id="c089" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">我的方法</h1><p id="781f" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">在本节中，我将向大家介绍我用来准备数据和拟合正则化回归的方法。</p><h2 id="1dcf" class="na me it bd mf nb nc dn mj nd ne dp mn lp nf ng mp lt nh ni mr lx nj nk mt nl bi translated">准备数据</h2><p id="8adb" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">在得到<em class="nn"> X </em>和<em class="nn"> y </em>之前，我不会详述数据。我使用这个<a class="ae mc" href="https://www.countyhealthrankings.org/explore-health-rankings/rankings-data-documentation" rel="noopener ugc nofollow" target="_blank">美国县国民健康排名</a>数据集合中的一个版本来生成下面的结果，但是对于这个例子来说这真的无关紧要。</p><p id="1e77" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">所以，假设你有一个漂亮干净的<em class="nn"> X </em>和<em class="nn"> y </em>，下一步是留出一个测试数据集，使用方便的<a class="ae mc" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" rel="noopener ugc nofollow" target="_blank"> train_test_split </a>。如果您想使结果可重复，选择任意数字作为<code class="fe nv nw nx ny b">my_random_state</code>。</p><pre class="ki kj kk kl gt oa ny ob oc aw od bi"><span id="c285" class="na me it ny b gy oe of l og oh">X_train , X_test, y_train, y_test = train_test_split(<br/>    X, y, test_size=1000, random_state=my_random_state)</span></pre><p id="5e01" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">下一步是包含<a class="ae mc" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html" rel="noopener ugc nofollow" target="_blank">多项式特征</a>。我们将结果保存在<code class="fe nv nw nx ny b">poly</code>对象中，这很重要，我们稍后会用到它。</p><pre class="ki kj kk kl gt oa ny ob oc aw od bi"><span id="7b8b" class="na me it ny b gy oe of l og oh">poly = PolynomialFeatures(<br/>    degree = 2, include_bias = False, interaction_only = False)</span></pre><p id="9669" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这将产生变量的所有二次多项式组合。需要注意的是，我们将<code class="fe nv nw nx ny b">include_bias</code>设置为<code class="fe nv nw nx ny b">False</code>。这是因为我们不需要截距列，回归模型本身将包含一个截距列。</p><p id="3686" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这就是我们如何转换和重命名<em class="nn"> X </em>的方法。它假设您将<em class="nn"> X </em>保存在pandas数据帧中，并且需要做一些调整来保持列名可用。如果你不想要名字，你只需要第一行。</p><pre class="ki kj kk kl gt oa ny ob oc aw od bi"><span id="6a9e" class="na me it ny b gy oe of l og oh">X_train_poly = poly.fit_transform(X_train)<br/>polynomial_column_names = \<br/>    poly.get_feature_names(input_features = X_train.columns)<br/>X_train_poly = \<br/>    pd.DataFrame(data = X_train_poly, <br/>        columns = polynomial_column_names )</span><span id="b1a5" class="na me it ny b gy oi of l og oh">X_train_poly.columns = X_train_poly.columns.str.replace(' ', '_')<br/>X_train_poly.columns = X_train_poly.columns.str.replace('^', '_')</span></pre><p id="e3ff" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">完成这一步后，下一步就是扩展。既然我们引入了多项式，这一点就更重要了，幅值将是无标度的。</p><pre class="ki kj kk kl gt oa ny ob oc aw od bi"><span id="b591" class="na me it ny b gy oe of l og oh">sc = StandardScaler()</span><span id="1332" class="na me it ny b gy oi of l og oh">X_train_poly_scaled = sc.fit_transform(X_train_poly)<br/>X_train_poly_scaled = pd.DataFrame( \<br/>        data = X_train_poly_scaled, columns = X_train_poly.columns)</span></pre><p id="06ba" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">棘手的部分来了。如果我们想要使用测试数据集，我们需要应用相同的步骤。</p><p id="b685" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">但是，我们不需要再次调整对象。嗯，和<code class="fe nv nw nx ny b">poly</code>没关系，但是对于<code class="fe nv nw nx ny b">sc</code>，我们想保留我们以前拟合<code class="fe nv nw nx ny b">X_train_poly</code>的方法。是的，这意味着测试数据不会完全标准化，这没关系。所以不用<code class="fe nv nw nx ny b">fit_transform</code>，我们用<code class="fe nv nw nx ny b">transform</code>。</p><pre class="ki kj kk kl gt oa ny ob oc aw od bi"><span id="a649" class="na me it ny b gy oe of l og oh">X_test_poly = poly.transform(X_test)<br/>X_test_poly_scaled = sc.transform(X_test_poly)</span></pre><h2 id="3057" class="na me it bd mf nb nc dn mj nd ne dp mn lp nf ng mp lt nh ni mr lx nj nk mt nl bi translated">功能</h2><p id="2abf" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">您可能想知道如何生成我们上面使用的图。我使用了两个函数，基于上面列出的库。第一个函数绘制一个图:</p><figure class="ki kj kk kl gt kn"><div class="bz fp l di"><div class="ok ol l"/></div></figure><p id="741a" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">第二个基本上是一个网格搜索，有一些额外的东西:它也运行测试分数，当然保存情节。</p><figure class="ki kj kk kl gt kn"><div class="bz fp l di"><div class="ok ol l"/></div></figure><p id="7c28" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我不想在这里赘述，我认为这是不言自明的，我们稍后会看到如何调用它的例子。</p><p id="f7b8" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">有一件事，我认为很酷:sklearn有一个<a class="ae mc" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.get_scorer.html" rel="noopener ugc nofollow" target="_blank"> get_scorer </a>函数，它基于它的sklearn字符串代码返回一个scorer对象。例如:</p><pre class="ki kj kk kl gt oa ny ob oc aw od bi"><span id="fb8d" class="na me it ny b gy oe of l og oh">scorer = get_scorer('r2')<br/>scorer(model, X_test, y_test)</span></pre><p id="1e02" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">现在我们有了一个额外的方法来计算同样的事情。</p><h2 id="a5a8" class="na me it bd mf nb nc dn mj nd ne dp mn lp nf ng mp lt nh ni mr lx nj nk mt nl bi translated">微调</h2><p id="cabc" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">一旦这个过程像这样建立起来，我们需要做的就是运行不同alpha数组的函数。</p><p id="9e36" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这个过程中有趣的一点是，我们还绘制了测试分数:</p><ol class=""><li id="81c5" class="om on it li b lj lk lm ln lp oo lt op lx oq mb or os ot ou bi translated">取训练数据集和一个alpha</li><li id="eb73" class="om on it li b lj ov lm ow lp ox lt oy lx oz mb or os ot ou bi translated">做交叉验证，保存训练和验证分数；</li><li id="2fe1" class="om on it li b lj ov lm ow lp ox lt oy lx oz mb or os ot ou bi translated">假设这是我们选择的alpha，并在整个训练数据上拟合一个没有交叉验证的模型；</li><li id="adbb" class="om on it li b lj ov lm ow lp ox lt oy lx oz mb or os ot ou bi translated">计算该模型将在测试数据上获得的分数，并保存测试分数。</li></ol><p id="7f2f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这不是你在“现实生活”中会做的事情(除非你做Kaggle比赛)，因为现在有可能优化你的测试数据集。我们在此仅包括它来说明模型性能。红线是不同阿尔法的测试分数。</p><p id="dfb0" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">我们还需要一个交叉验证对象，这里没有一个好的答案，这是一个选项:</p><pre class="ki kj kk kl gt oa ny ob oc aw od bi"><span id="708d" class="na me it ny b gy oe of l og oh">cv = KFold(n_splits=5, shuffle=True, random_state=my_random_state)</span></pre><p id="d2d0" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">为了说明我对多步参数搜索的重要性的观点，假设我们想要检查这些alphas:</p><pre class="ki kj kk kl gt oa ny ob oc aw od bi"><span id="e40a" class="na me it ny b gy oe of l og oh">lasso_alphas = np.linspace(0, 0.02, 11)</span></pre><p id="a984" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">运行该函数后:</p><pre class="ki kj kk kl gt oa ny ob oc aw od bi"><span id="1718" class="na me it ny b gy oe of l og oh">chosen_alpha, max_validation_score, test_score_at_chosen_alpha = \<br/>    regmodel_param_test(<br/>        lasso_alphas, X_train_poly_scaled, y_train, <br/>        cv, scoring = 'r2', model_name = 'LASSO', <br/>        X_test = X_test_poly_scaled, y_test = y_test, <br/>        draw_plot = True, filename = 'lasso_wide_search')</span><span id="af3a" class="na me it ny b gy oi of l og oh">print("Chosen alpha: %.5f" % \<br/>    chosen_alpha)<br/>print("Validation score: %.5f" % \<br/>    max_validation_score)<br/>print("Test score at chosen alpha: %.5f" % \<br/>    test_score_at_chosen_alpha)</span></pre><p id="6a8b" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">结果是:</p><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/b6d3e46505c7ea001fb13fa9f7274b59.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*3geRBW5ANPxiea1Wfn3qoA.png"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated">套索广泛搜索</p></figure><pre class="ki kj kk kl gt oa ny ob oc aw od bi"><span id="4bf4" class="na me it ny b gy oe of l og oh">Chosen alpha: 0.00200<br/>Validation score: 0.82310<br/>Test score at chosen alpha: 0.80673</span></pre><p id="896b" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这是否意味着我们找到了最优α？你可能会看着图，看到漂亮的尖钉，决定这是足够高的。不完全是。如果我们在更精细的层次上运行它:</p><pre class="ki kj kk kl gt oa ny ob oc aw od bi"><span id="80ba" class="na me it ny b gy oe of l og oh">lasso_alphas = np.linspace(0, 0.002, 11)</span></pre><p id="c8d3" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">这是结果，请注意0.02，最右边的点是我们在前面的图表中的峰值:</p><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/82dbc23c825c5bc5b4e59acc872532da.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*p5n_8kbEbtG3cAzSMBLEbA.png"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated">套索窄搜索</p></figure><pre class="ki kj kk kl gt oa ny ob oc aw od bi"><span id="2219" class="na me it ny b gy oe of l og oh">Chosen alpha: 0.00060<br/>Validation score: 0.83483<br/>Test score at chosen alpha: 0.82326</span></pre><p id="db39" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">如果我们早一点停止，我们会选择一个导致总体测试R低2%的alpha，我认为这是相当重要的。</p><h2 id="3fc7" class="na me it bd mf nb nc dn mj nd ne dp mn lp nf ng mp lt nh ni mr lx nj nk mt nl bi translated">在我忘记之前…山脊！</h2><p id="fe8b" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">对了，帖子的标题里有脊，除了理论介绍，我们还没讨论。</p><p id="dbfe" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">原因很简单:它的工作方式和Lasso完全一样，你可能只是想选择不同的alpha参数，然后在<code class="fe nv nw nx ny b">model_name</code>参数中传递‘Ridge’。Ridge也有同样的问题(我不包括我们搜索alphas范围来检查的部分):</p><figure class="ki kj kk kl gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/7bb3149fc5790ba9f817a343fac535b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*pzAKfvpJ1WDKVR4gzFtoPQ.png"/></div><p class="kz la gj gh gi lb lc bd b be z dk translated">岭窄搜索</p></figure><p id="3a66" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">你会注意到，我们基于蓝线选择的点似乎不再是红线的最佳点。是的，这是真的，但我认为这是一个巧合，它在套索模型中表现得如此之好。</p><h1 id="bec3" class="md me it bd mf mg mh mi mj mk ml mm mn jz mo ka mp kc mq kd mr kf ms kg mt mu bi translated">摘要</h1><p id="5543" class="pw-post-body-paragraph lg lh it li b lj mv ju ll lm mw jx lo lp mx lr ls lt my lv lw lx mz lz ma mb im bi translated">所以你有它，这就是我如何为套索和山脊做超参数调整。我希望你觉得有帮助，要点再说一遍:</p><ul class=""><li id="a05f" class="om on it li b lj lk lm ln lp oo lt op lx oq mb pa os ot ou bi translated">记得缩放你的变量；</li><li id="5876" class="om on it li b lj ov lm ow lp ox lt oy lx oz mb pa os ot ou bi translated">α= 0只是线性回归；</li><li id="b142" class="om on it li b lj ov lm ow lp ox lt oy lx oz mb pa os ot ou bi translated">当搜索最佳参数时，执行多个步骤；</li><li id="7e43" class="om on it li b lj ov lm ow lp ox lt oy lx oz mb pa os ot ou bi translated">使用基于平方差的分数来衡量绩效。</li></ul></div></div>    
</body>
</html>