<html>
<head>
<title>Iterative Pruning Methods for Artificial Neural Networks in Julia</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Julia中人工神经网络的迭代剪枝方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/iterative-pruning-methods-for-artificial-neural-networks-in-julia-c605f547a485?source=collection_archive---------10-----------------------#2021-09-11">https://towardsdatascience.com/iterative-pruning-methods-for-artificial-neural-networks-in-julia-c605f547a485?source=collection_archive---------10-----------------------#2021-09-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/eef5c2a0a95f6a6197d8fdb447732b21.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MLIjvdpkCmEuW8gk"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">摩根·豪斯尔在<a class="ae jd" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h2 id="d677" class="je jf jg bd b dl jh ji jj jk jl jm dk jn translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想与理论</a></h2><div class=""/><div class=""><h2 id="dbc2" class="pw-subtitle-paragraph km jp jg bd b kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld dk translated">基于剪枝的压缩技术综述</h2></div><p id="c0a4" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">近年来，深度学习模型在实时嵌入式应用中变得更加流行。特别是，这些模型已经成为从自然语言处理到计算机视觉等几个领域的基础。计算能力的增加有利于面对<strong class="lg jq">实际挑战，这些挑战已经通过采用广泛的神经网络</strong>得到解决；随着网络越来越深入，模型大小也增加，<strong class="lg jq">引入了对指定输出没有贡献的冗余参数</strong>。最近，研究人员专注于如何通过修剪多余的参数来减少存储和计算能力而不损害性能的不同方法。</p><p id="e970" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">本文的参考实现语言是Julia [1]，这是一种最近开发的面向科学计算的编程语言。特别是，我们将扩展Flux库[2，3]，通常用于机器学习。</p><h2 id="b657" class="ma mb jg bd mc md me dn mf mg mh dp mi ln mj mk ml lr mm mn mo lv mp mq mr jm bi translated">为什么修剪？</h2><p id="e092" class="pw-post-body-paragraph le lf jg lg b lh ms kq lj lk mt kt lm ln mu lp lq lr mv lt lu lv mw lx ly lz ij bi translated"><strong class="lg jq">自80年代末开始使用的剪枝算法</strong>【4，5】<strong class="lg jq">包括从网络中删除单个权重或整个神经元，以减少冗余，同时不牺牲准确性</strong>。为了在通过示例训练的系统中获得良好的泛化能力，应该使用适合数据的最小系统[6]。经验证据表明，训练一个大型网络，然后压缩它，比从头开始训练一个较小的网络更容易[7]。</p><figure class="my mz na nb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mx"><img src="../Images/6d98df495e7aa3bf1d6394174b8f5b39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_fxwqHZuyCvEyb8zQMSAbQ.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">(<em class="nc">作者图片</em>)</p></figure><p id="d8cd" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这种方法的问题在于，通过<strong class="lg jq">去除太多的边缘，可能会丢失所学的内容</strong>。主要的困难在于找到最佳的模型尺寸。因此，我们可以将剪枝视为一个神经网络架构搜索[8]问题，目标是<strong class="lg jq">为所考虑的任务</strong>找到最佳网络。修剪方法的各种实现主要在四个方面不同:</p><ul class=""><li id="bc81" class="nd ne jg lg b lh li lk ll ln nf lr ng lv nh lz ni nj nk nl bi translated"><strong class="lg jq">结构</strong>:结构化剪枝包括消除网络中的整个神经元，并允许使用更小的矩阵来加速计算。另一方面，非结构化修剪更灵活，因为它允许消除单个连接，但不能有效地加速神经网络的计算。</li></ul><figure class="my mz na nb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nm"><img src="../Images/81c335ce14bfa7740c7f0a42cdffc659.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DOAff7VansDw0toJQCNzag.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">(<em class="nc">图片作者</em>)</p></figure><ul class=""><li id="10ef" class="nd ne jg lg b lh li lk ll ln nf lr ng lv nh lz ni nj nk nl bi translated"><strong class="lg jq">显著性</strong>:可以被视为与每个边缘相关联的重要性，根据所采用的策略来确定是否应该消除它</li><li id="809d" class="nd ne jg lg b lh nn lk no ln np lr nq lv nr lz ni nj nk nl bi translated"><strong class="lg jq">调度</strong>:决定每一步要淘汰的网络比例。例如，一次性修剪包括在一个步骤中消除网络的期望百分比。其他可能的策略是在每一步消除网络的恒定部分，或者采用更复杂的调度功能。</li><li id="3f29" class="nd ne jg lg b lh nn lk no ln np lr nq lv nr lz ni nj nk nl bi translated"><strong class="lg jq">微调</strong>:在修剪阶段之后，准确度通常会下降，因此重新训练网络以稳定它可能是有用的。有不同的可能方法来处理微调。在边缘消除之后，可以使用修剪步骤之前具有的相同权重(没有被消除)来训练，或者使用在某个先前阶段具有的边缘的权重，或者从开始重新初始化剩余的边缘</li></ul><h2 id="cc15" class="ma mb jg bd mc md me dn mf mg mh dp mi ln mj mk ml lr mm mn mo lv mp mq mr jm bi translated">数量修剪</h2><p id="ccc9" class="pw-post-body-paragraph le lf jg lg b lh ms kq lj lk mt kt lm ln mu lp lq lr mv lt lu lv mw lx ly lz ij bi translated">这是最简单的剪枝算法。在共同训练阶段之后，去除具有较低显著性的连接。链接的显著性仅由其权重的绝对值给出。幅度修剪有两种主要的变体。一旦所需百分比的连接被固定，<strong class="lg jq">逐层幅度修剪</strong>将从每层移除该百分比的边，而<strong class="lg jq">全局幅度修剪</strong>将从整个网络移除一个百分比，而不区分层。更复杂的分层剪枝技术引入了灵敏度分析阶段，允许为每层指定不同的稀疏度阈值，以便在不太敏感的层中消除更多的边(即那些权重的消除对网络结果影响较小的边)。首先，<strong class="lg jq">总是需要一个训练阶段，以便了解哪些是网络最重要的连接</strong>，那些具有最高绝对值的连接。<strong class="lg jq">在消除</strong>显著性较低的连接后，<strong class="lg jq">网络性能将会下降</strong>。</p><figure class="my mz na nb gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ns"><img src="../Images/3c0ca6732506e830fb2f44fbbc78d273.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c7HkvnTCTHu6FnxkK5nxJQ.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">(<em class="nc">图片作者</em>)</p></figure><p id="bf44" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">由于这个原因，从最后获得的权重开始重新训练网络而不从头重新初始化参数是必要的。修剪参数的百分比越高，精度下降越多，这就是为什么修剪通常迭代地进行，一次消除一小部分网络。这意味着需要一个时间表，在这个时间表中，我们指定要删除的网络部分，分多少步，每一步要删除多少个连接。</p><h2 id="f41f" class="ma mb jg bd mc md me dn mf mg mh dp mi ln mj mk ml lr mm mn mo lv mp mq mr jm bi translated">镗孔预处理步骤</h2><pre class="my mz na nb gt nt nu nv nw aw nx bi"><span id="4a9a" class="ma mb jg nu b gy ny nz l oa ob"><strong class="nu jq">using</strong> Flux: onehotbatch , onecold<br/><strong class="nu jq">using </strong>MLDatasets</span><span id="9455" class="ma mb jg nu b gy oc nz l oa ob">train_x , train_y = CIFAR10.traindata()<br/>test_x , test_y = CIFAR10.testdata()</span><span id="ed64" class="ma mb jg nu b gy oc nz l oa ob">X = Flux.flatten(train_x)<br/>Y = onehotbatch(train_y , 0:9)</span><span id="ae55" class="ma mb jg nu b gy oc nz l oa ob">test_X = Flux.flatten(test_x)<br/>test_Y = onehotbatch(test_y , 0:9)</span><span id="5b91" class="ma mb jg nu b gy oc nz l oa ob">data = Flux.Data.DataLoader ((X,Y),batchsize = 128, shuffle=true)<br/>test_data = Flux.Data.DataLoader ((test_X ,test_Y), batchsize = 128)<br/></span></pre><h2 id="339d" class="ma mb jg bd mc md me dn mf mg mh dp mi ln mj mk ml lr mm mn mo lv mp mq mr jm bi translated">快速实现</h2><p id="3fee" class="pw-post-body-paragraph le lf jg lg b lh ms kq lj lk mt kt lm ln mu lp lq lr mv lt lu lv mw lx ly lz ij bi translated">修剪通常通过将权重设置为零并在随后的训练中冻结它们来实现。我的实现使用了一种基于元素的操作，将权重矩阵乘以二进制剪枝掩码。</p><figure class="my mz na nb gt is gh gi paragraph-image"><div class="gh gi od"><img src="../Images/8342e7f38d7afe05f10371fb7139f094.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*RU5U9fToR2B9Qfv-xfRmyg.png"/></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">(<em class="nc">图片作者</em>)</p></figure><p id="0437" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">第一个矩阵代表神经网络层的权重，而第二个矩阵是<strong class="lg jq">掩码，它将某个阈值</strong>下的所有值设置为零，在本例中为0.5</p><p id="5f64" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">在Flux中，简单的致密层由两个基本部分定义。首先，一个struct包含三个字段:权重、偏置和激活函数。</p><pre class="my mz na nb gt nt nu nv nw aw nx bi"><span id="d5cc" class="ma mb jg nu b gy ny nz l oa ob"><strong class="nu jq">struct</strong> Dense{F, M &lt;: AbstractMatrix , B}<br/>  weight :: M<br/>  bias::B<br/>  sigma::F<br/>end</span></pre><p id="7849" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">第二个关键部分是表示向前一步计算的函数，如下所示:</p><pre class="my mz na nb gt nt nu nv nw aw nx bi"><span id="4e6a" class="ma mb jg nu b gy ny nz l oa ob"><strong class="nu jq">function </strong>(a::Dense)(x:: AbstractVecOrMat)<br/>  W, b, sigma = a.weight , a.bias , a.sigma<br/>  <strong class="nu jq">return</strong> sigma.(W*x .+ b)<br/>end</span></pre><p id="8209" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我开发的实现扩展了Flux提供的实现，如前所述，添加了带有矩阵掩码的Hadamard乘积。然后，层PrunableDense被定义为一种结构，该结构通过添加用于位矩阵的字段来重用密集层:</p><pre class="my mz na nb gt nt nu nv nw aw nx bi"><span id="f71d" class="ma mb jg nu b gy ny nz l oa ob"><strong class="nu jq">struct</strong> PrunableDense<br/>  dense :: Dense<br/>  mask:: BitMatrix<br/>end</span></pre><p id="b07b" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">其次，我们重新定义了前向阶跃函数，以包括掩模矩阵的哈达玛乘积:</p><pre class="my mz na nb gt nt nu nv nw aw nx bi"><span id="44f9" class="ma mb jg nu b gy ny nz l oa ob"><strong class="nu jq">function</strong> (a:: PrunableDense)(x:: AbstractVecOrMat)<br/>  W, b,sigma , M = a.dense.W, a.dense.b, a.dense.sigma , a.mask<br/>  <strong class="nu jq">return</strong> sigma .((W.*M)*x .+ b)<br/>end</span></pre><p id="648d" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">现在你可以用你想要的方式使用这些可修剪的密集层，然后减少你的神经网络的大小！</p></div><div class="ab cl oe of hu og" role="separator"><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj ok"/><span class="oh bw bk oi oj"/></div><div class="ij ik il im in"><h2 id="ad0e" class="ma mb jg bd mc md me dn mf mg mh dp mi ln mj mk ml lr mm mn mo lv mp mq mr jm bi translated">文献学</h2><p id="45ac" class="pw-post-body-paragraph le lf jg lg b lh ms kq lj lk mt kt lm ln mu lp lq lr mv lt lu lv mw lx ly lz ij bi translated">[1]杰夫·贝赞森、艾伦·埃德尔曼、斯特凡·卡尔平斯基和维尔拉·B·沙阿。朱莉娅:一种新的数值计算方法。暹罗评论，59(1):65–98，2017。</p><p id="663e" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[2]迈克尔·英尼斯、埃利奥特·萨巴、基诺·菲舍尔、达里亚·甘地、马尔科·康采托鲁迪洛索、尼图·玛利亚·乔伊、泰詹·卡尔马里、阿维克·帕尔和维尔拉·沙阿。带有通量的时尚造型。更正，abs/1811.01457，2018。</p><p id="9a0f" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[3]迈克·英尼斯。通量:优雅的机器学习与朱莉娅。开源软件杂志，2018。</p><p id="d510" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[4]史蒂文·雅诺夫斯基。《神经网络中的剪枝与剪枝》,物理评论A，39:6600–6603，1989年6月</p><p id="4705" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[5]戴维斯·布拉洛克、何塞·哈维尔·冈萨雷斯·奥尔蒂斯、乔纳森·弗兰克尔和约翰·古塔格。神经网络剪枝是什么状态？arXiv预印本:2003.03033，2020。</p><p id="3a76" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">6 Anselm Blumer、Andrzej Ehrenfeucht、David Haussler和Manfred K Warmuth。奥卡姆剃刀。信息处理通讯，24(6):377–380，1987。</p><p id="780d" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">7拉塞尔·里德。剪枝算法-综述。IEEE神经网络汇刊，4(5):740–747，1993。</p><p id="10d2" class="pw-post-body-paragraph le lf jg lg b lh li kq lj lk ll kt lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">[8]，孙明杰，周廷辉，，和特雷弗·达雷尔。重新思考网络修剪的价值，2019。</p></div></div>    
</body>
</html>