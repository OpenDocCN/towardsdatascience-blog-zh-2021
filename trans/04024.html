<html>
<head>
<title>Text Processing in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的文本处理</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/text-processing-in-python-29e86ea4114c?source=collection_archive---------4-----------------------#2021-04-05">https://towardsdatascience.com/text-processing-in-python-29e86ea4114c?source=collection_archive---------4-----------------------#2021-04-05</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><h2 id="4507" class="is it iu bd b dl iv iw ix iy iz ja dk jb translated" aria-label="kicker paragraph">面向所有人的Python文本处理指南。</h2><div class=""/><div class=""><h2 id="d793" class="pw-subtitle-paragraph ka jd iu bd b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr dk translated">使用NLTK和spaCy的文本处理示例</h2></div><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ks"><img src="../Images/e0c3a0e742553044bf9567a5c408a623.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UGi_3qjtzaWl5FmO2yHO2Q.jpeg"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">布雷特·乔丹在<a class="ae li" href="https://unsplash.com/s/photos/computer-process?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="1b9e" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">互联网连接了世界，而脸书、Twitter和Reddit等社交媒体为人们表达对某个话题的看法和感受提供了平台。然后，智能手机的普及直接增加了这些平台的使用率。例如，有96%或22.4亿脸书活跃用户通过智能手机和平板电脑使用脸书[1]。</p><p id="0493" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">社交媒体使用的增加增加了文本数据的大小，并促进了自然语言处理(NLP)中的学习或研究，例如，信息检索和情感分析。很多时候，待分析的文档或文本文件非常庞大，包含大量噪声，直接使用原始文本进行分析是不适用的。因此，文本处理对于为建模和分析提供清晰的输入是必不可少的。</p></div><div class="ab cl mf mg hy mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="in io ip iq ir"><p id="ea55" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">文本处理包含两个主要阶段，即标记化和规范化[2]。<strong class="ll je">记号化</strong>是将一个较长的文本串分割成较小的片段或记号的过程[3]。<strong class="ll je">规范化</strong>指将数字转换为对应的单词，去除标点符号，将所有文本转换为相同的大小写，去除停用词，去除噪音，去除词条和词干。</p><ul class=""><li id="245f" class="mm mn iu ll b lm ln lp lq ls mo lw mp ma mq me mr ms mt mu bi translated">词干-删除词缀(后缀、前缀、中缀、抑扬)，例如，连读</li><li id="04d7" class="mm mn iu ll b lm mv lp mw ls mx lw my ma mz me mr ms mt mu bi translated">引理化——根据单词的引理获取标准形式。例如，更好的到好的[4]</li></ul><p id="4de0" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">在本文中，我将演示用<strong class="ll je"> Python进行文本处理。</strong></p><p id="e24a" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">以下是从丹尼尔·茹拉夫斯基和詹姆斯·h·马丁的《语音和语言处理》一书中摘录的一段话[6]。</p><blockquote class="na nb nc"><p id="7271" class="lj lk nd ll b lm ln ke lo lp lq kh lr ne lt lu lv nf lx ly lz ng mb mc md me in bi translated">“赋予计算机处理人类语言能力的想法和计算机本身的想法一样古老。这本书是关于这个令人兴奋的想法的实现和含义。我们介绍了一个充满活力的跨学科领域，其许多方面有许多对应的名称，如语音和语言处理、人类语言技术、自然语言处理、计算语言学以及语音识别和合成。这个新领域的目标是让计算机执行涉及人类语言的有用任务，如实现人机交流、改善人与人之间的交流，或者只是对文本或语音进行有用的处理。”</p></blockquote><p id="3e50" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这一段将在下面的文本处理示例中使用。</p></div><div class="ab cl mf mg hy mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="in io ip iq ir"><h1 id="793f" class="nh ni iu bd nj nk nl nm nn no np nq nr kj ns kk nt km nu kn nv kp nw kq nx ny bi translated">Python中的文本处理</h1><p id="d4b3" class="pw-post-body-paragraph lj lk iu ll b lm nz ke lo lp oa kh lr ls ob lu lv lw oc ly lz ma od mc md me in bi translated">对于Python中的文本处理，演示中将使用两个自然语言处理(NLP)库，即NLTK(自然语言工具包)和spaCy。之所以选择这两个库而不是其他文本处理库，如Gensim和Transformer，是因为NLTK和spaCy是最受欢迎的库，对自然语言处理(NLP)初学者来说非常友好。</p><p id="5c2b" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">对于NLTK和spaCy，首先需要将文本保存为变量。</p><pre class="kt ku kv kw gu oe of og oh aw oi bi"><span id="a28e" class="oj ni iu of b gz ok ol l om on">text = """The idea of giving computers the ability to process human language is as old as the idea of computers themselves. This book is about the implementation and implications of that exciting idea. We introduce a vibrant interdisciplinary field with many names corresponding to its many facets, names like speech and language processing, human language technology, natural language processing, computational linguistics, and speech recognition and synthesis. The goal of this new field is to get computers to perform useful tasks involving human language, tasks like enabling human-machine communication, improving human-human communication, or simply doing useful processing of text or speech."""</span></pre><h2 id="72cd" class="oj ni iu bd nj oo op dn nn oq or dp nr ls os ot nt lw ou ov nv ma ow ox nx ja bi translated">使用NLTK进行文本处理</h2><ol class=""><li id="6c42" class="mm mn iu ll b lm nz lp oa ls oy lw oz ma pa me pb ms mt mu bi translated">导入所有需要的库</li></ol><pre class="kt ku kv kw gu oe of og oh aw oi bi"><span id="04b1" class="oj ni iu of b gz ok ol l om on">import re<br/>import pandas as pd<br/>import nltk<br/>from nltk.tokenize import WordPunctTokenizer<br/>nltk.download(’stopwords’)<br/>from nltk.corpus import stopwords<br/># needed for nltk.pos_tag function nltk.download(’averaged_perceptron_tagger’)<br/>nltk.download(’wordnet’)<br/>from nltk.stem import WordNetLemmatizer</span></pre><ol class=""><li id="86cf" class="mm mn iu ll b lm ln lp lq ls mo lw mp ma mq me pb ms mt mu bi translated"><strong class="ll je">标记化</strong></li></ol><p id="fab8" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">使用tokenizer将句子分成一系列单词(记号)。</p><pre class="kt ku kv kw gu oe of og oh aw oi bi"><span id="b6b2" class="oj ni iu of b gz ok ol l om on">word_punct_token = WordPunctTokenizer().tokenize(text)</span></pre><p id="7e06" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">除了上面使用的<code class="fe pc pd pe of b">WordPunctTokenizer</code>，NLTK库中还有几个记号赋予器模块。比如<code class="fe pc pd pe of b">word_tokenize</code>和<code class="fe pc pd pe of b">RegexpTokenizer</code>。<code class="fe pc pd pe of b">RegexpTokenizer</code>能够通过设置<code class="fe pc pd pe of b">RegexpTokenizer(‘\w+|\$[\d\.]+|\S+’)</code>将9.99美元这样的货币分离为一个单独的令牌。所有提到的记号赋予器将以列表形式返回记号。NLTK还有一个名为<code class="fe pc pd pe of b">sent_tokenize </code>的模块，它能够将段落分成句子列表。</p><p id="97f1" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><strong class="ll je"> 2。标准化</strong></p><p id="2f5c" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">下面的脚本删除了不是单词的标记，例如，符号和数字，以及只包含少于两个字母或只包含辅音的标记。这个脚本在这个例子中可能没有用，但是在处理大量文本数据时非常有用，它有助于清除大量干扰。每当我处理文本数据时，我总是喜欢包含它。</p><pre class="kt ku kv kw gu oe of og oh aw oi bi"><span id="f644" class="oj ni iu of b gz ok ol l om on">clean_token=[]<br/>for token in word_punct_token:<br/>    token = token.lower()<br/>    <em class="nd"># remove any value that are not alphabetical</em><br/>    new_token = re.sub(r'[^a-zA-Z]+', '', token) <br/>    <em class="nd"># remove empty value and single character value</em><br/>    if new_token != "" and len(new_token) &gt;= 2: <br/>        vowels=len([v for v in new_token if v in "aeiou"])<br/>        if vowels != 0: # remove line that only contains consonants<br/>            clean_token.append(new_token)</span></pre><p id="0a9f" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><strong class="ll je"> 2a。移除停用字词</strong></p><p id="4dc2" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">停用词指的是没有太多含义的词，如介词。NLTK和spaCy在库中有不同数量的停用词，但是NLTK和spaCy都允许我们添加任何我们认为必要的词。例如，当我们处理电子邮件时，我们可以添加<code class="fe pc pd pe of b">gmail</code>、<code class="fe pc pd pe of b">com</code>、<code class="fe pc pd pe of b">outlook </code>作为停用词。</p><pre class="kt ku kv kw gu oe of og oh aw oi bi"><span id="7ae9" class="oj ni iu of b gz ok ol l om on"># Get the list of stop words<br/>stop_words = stopwords.words('english')<br/># add new stopwords to the list<br/>stop_words.extend(["could","though","would","also","many",'much'])<br/>print(stop_words)<br/># Remove the stopwords from the list of tokens<br/>tokens = [x for x in clean_token if x not in stop_words]</span></pre><p id="5c0d" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><strong class="ll je"> 2b。词性标注(POS Tag) </strong></p><p id="7de0" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这个过程指的是用词类位置来标记单词，例如，动词、形容词和名词。模块以元组的形式返回结果，为了方便以后的工作，通常我会将它们转换成数据帧。在[6]中，POS标签是在标记化之后直接执行的任务，当你知道你只需要像形容词和名词这样的特定词类时，这是一个聪明的举动。</p><pre class="kt ku kv kw gu oe of og oh aw oi bi"><span id="4d88" class="oj ni iu of b gz ok ol l om on">data_tagset = nltk.pos_tag(tokens)<br/>df_tagset = pd.DataFrame(data_tagset, columns=['Word', 'Tag'])</span></pre><p id="7d10" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><strong class="ll je"> 2c。词汇化</strong></p><p id="6d91" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">词汇化和词干化都有助于通过将单词返回到其词根形式(词汇化)或删除所有后缀、词缀、前缀等(词干化)来减少词汇的维数。词干对于减少词汇的维度是很好的，但是大多数时候这个单词变得毫无意义，因为词干只是砍掉了后缀，而不是将单词还原成它们的基本形式。比如<em class="nd">宅</em>词干化后会变成<em class="nd">宅</em>，完全失去意义。因此，词汇化对于文本分析更为可取。</p><p id="1153" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">以下脚本用于获取名词、形容词和动词的词根形式。</p><pre class="kt ku kv kw gu oe of og oh aw oi bi"><span id="2b2b" class="oj ni iu of b gz ok ol l om on"># Create lemmatizer object <br/>lemmatizer = WordNetLemmatizer()</span><span id="4cc4" class="oj ni iu of b gz pf ol l om on"># Lemmatize each word and display the output<br/>lemmatize_text = []<br/>for word in tokens:<br/>    output = [word, lemmatizer.lemmatize(word, pos='n'),       lemmatizer.lemmatize(word, pos='a'),lemmatizer.lemmatize(word, pos='v')]<br/>    lemmatize_text.append(output)</span><span id="0580" class="oj ni iu of b gz pf ol l om on"># create DataFrame using original words and their lemma words<br/>df = pd.DataFrame(lemmatize_text, columns =['Word', 'Lemmatized Noun', 'Lemmatized Adjective', 'Lemmatized Verb']) <br/><br/>df['Tag'] = df_tagset['Tag']</span></pre><p id="f859" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">将形容词、名词和动词分别进行词汇化的原因是为了提高词汇化的准确性。</p><pre class="kt ku kv kw gu oe of og oh aw oi bi"><span id="d7d7" class="oj ni iu of b gz ok ol l om on"># replace with single character for simplifying<br/>df = df.replace(['NN','NNS','NNP','NNPS'],'n')<br/>df = df.replace(['JJ','JJR','JJS'],'a')<br/>df = df.replace(['VBG','VBP','VB','VBD','VBN','VBZ'],'v')</span><span id="47d8" class="oj ni iu of b gz pf ol l om on">'''<br/>define a function where take the lemmatized word when tagset is noun, and take lemmatized adjectives when tagset is adjective<br/>'''</span><span id="33b2" class="oj ni iu of b gz pf ol l om on">df_lemmatized = df.copy()<br/>df_lemmatized['Tempt Lemmatized Word']=df_lemmatized['Lemmatized Noun'] + ' | ' + df_lemmatized['Lemmatized Adjective']+ ' | ' + df_lemmatized['Lemmatized Verb']</span><span id="44e4" class="oj ni iu of b gz pf ol l om on">df_lemmatized.head(5)<br/>lemma_word = df_lemmatized['Tempt Lemmatized Word']<br/>tag = df_lemmatized['Tag']<br/>i = 0<br/>new_word = []<br/>while i&lt;len(tag):<br/>    words = lemma_word[i].split('|')<br/>    if tag[i] == 'n':        <br/>        word = words[0]<br/>    elif tag[i] == 'a':<br/>        word = words[1]<br/>    elif tag[i] == 'v':<br/>        word = words[2]<br/>    new_word.append(word)<br/>    i += 1</span><span id="1b0e" class="oj ni iu of b gz pf ol l om on">df_lemmatized['Lemmatized Word']=new_word</span></pre><p id="4f73" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">上面的脚本是根据单词的POS标签将正确的词汇化单词分配给原始单词。</p><p id="5d7f" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><strong class="ll je"> 3。获取清理后的令牌</strong></p><pre class="kt ku kv kw gu oe of og oh aw oi bi"><span id="3085" class="oj ni iu of b gz ok ol l om on"># calculate frequency distribution of the tokens<br/>lemma_word = [str(x) for x in df_lemmatized['Lemmatized Word']]</span></pre><p id="a93c" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">在对文本进行了标记化和规范化之后，现在我们获得了一个干净标记的列表，可以插入到WordCloud或其他文本分析模型中。</p></div><div class="ab cl mf mg hy mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="in io ip iq ir"><h2 id="fb31" class="oj ni iu bd nj oo op dn nn oq or dp nr ls os ot nt lw ou ov nv ma ow ox nx ja bi translated">空间文本处理</h2><ol class=""><li id="9682" class="mm mn iu ll b lm nz lp oa ls oy lw oz ma pa me pb ms mt mu bi translated"><strong class="ll je">标记化+词条化</strong></li></ol><p id="83c0" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">与NLTK相比，SpaCy的优势在于它简化了文本处理过程。</p><pre class="kt ku kv kw gu oe of og oh aw oi bi"><span id="82ce" class="oj ni iu of b gz ok ol l om on"># Import spaCy and load the language library<br/>import spacy<br/>#you will need this line below to download the package<br/>!python -m spacy download en_core_web_sm<br/>nlp = spacy.load('en_core_web_sm')</span><span id="6831" class="oj ni iu of b gz pf ol l om on"># Create a Doc object<br/>doc = nlp(text)<br/>token_list = []<br/># collect each token separately with their POS Tag, dependencies and lemma<br/>for token in doc:<br/>    output = [token.text, token.pos_, token.dep_,token.lemma_]<br/>    token_list.append(output)</span><span id="47e5" class="oj ni iu of b gz pf ol l om on"># create DataFrame using data <br/>df = pd.DataFrame(token_list, columns =['Word', 'POS Tag', 'Dependencies', 'Lemmatized Word']) </span></pre><p id="665d" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">在spaCy中，您可以在执行标记化时获得POS标签和lemma(单词的词根形式),这样可以节省一些精力。</p><p id="e8da" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><strong class="ll je"> 2。标准化</strong></p><p id="d31a" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">由于词汇化是在最开始执行的，因此规范化步骤只剩下去除噪声和停用词。</p><p id="3197" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><strong class="ll je"> 2a。去除噪音</strong></p><pre class="kt ku kv kw gu oe of og oh aw oi bi"><span id="03d4" class="oj ni iu of b gz ok ol l om on">df_nopunct = df[df['POS Tag']!='PUNCT']<br/>df_nopunct</span></pre><p id="989a" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">spaCy POS标签包含一个显示为<code class="fe pc pd pe of b">PUNCT</code>的标点符号，因此我们可以通过删除带有<code class="fe pc pd pe of b">PUNCT</code>标签的标记来删除所有标点符号。</p><p id="4b71" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><strong class="ll je"> 2b。移除停用字词</strong></p><pre class="kt ku kv kw gu oe of og oh aw oi bi"><span id="6e82" class="oj ni iu of b gz ok ol l om on">import numpy as np<br/>lemma_word = df_nopunct['Lemmatized Word'].values.tolist()</span><span id="5ed9" class="oj ni iu of b gz pf ol l om on">stopword = nlp.Defaults.stop_words<br/># Add the word to the set of stop words. Use lowercase!<br/>nlp.Defaults.stop_words.add('btw')</span><span id="19bd" class="oj ni iu of b gz pf ol l om on">is_stopword_list = []<br/>for word in lemma_word:<br/>    is_stopword = nlp.vocab[word].is_stop<br/>    is_stopword_list.append(is_stopword)<br/>df_nopunct["is_stopword"] = is_stopword_list<br/>df_nopunct<br/>clean_df = df_nopunct[df_nopunct["is_stopword"]==False]</span></pre><p id="2fdb" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">SpaCy有一个类<code class="fe pc pd pe of b">.is_stop</code>，可以用来检测一个令牌是否是一个停用词，我们可以用它来删除停用词，如上面的脚本所示。</p><p id="5f3c" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><strong class="ll je"> 3。获取清理后的令牌</strong></p><pre class="kt ku kv kw gu oe of og oh aw oi bi"><span id="a998" class="oj ni iu of b gz ok ol l om on">clean_list = clean_df["Lemmatized Word"].values.tolist()</span></pre><p id="774b" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">现在我们获得了已清理令牌的列表！</p></div><div class="ab cl mf mg hy mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="in io ip iq ir"><p id="c163" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">spaCy比NLTK更快、更先进，但是NLTK更适合初学者理解文本处理中的每个过程，因为它发生的时间更长，并且有很多文档和解释。您可以在这里找到spaCy和NLTK在其他特性方面的比较，如GPU支持、效率、性能、艺术水平、词向量和灵活性<a class="ae li" rel="noopener" target="_blank" href="/text-normalization-with-spacy-and-nltk-1302ff430119">。</a></p><p id="6517" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">谢谢你读到这里！希望上面的演示能帮助你理解Python中的文本处理。</p></div><div class="ab cl mf mg hy mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="in io ip iq ir"><p id="0c94" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">对于文本处理，理解数据结构与正则表达式(RegEx)一样重要。例如，类或模块的返回可能是元组和列表的形式，要操作返回输出，我们首先必须理解它们。下面是我经常使用的一些脚本，用于根据不同的目的改变变量的数据结构。</p><ol class=""><li id="f24b" class="mm mn iu ll b lm ln lp lq ls mo lw mp ma mq me pb ms mt mu bi translated"><strong class="ll je">从数据框架(多列)到嵌套列表，从嵌套列表到列表</strong></li></ol><p id="03f9" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">当我们创建主题建模或情感分析模型时，通常我们根据句子或段落单独保存令牌，例如，评论和推文单独保存，以获得每个评论或推文的准确情感或主题。因此，如果我们有10个评论，就会有10个标记列表，将它们保存在一个变量中创建一个嵌套列表。另一个例子是一个数据框架，它有几列包含文本数据，可以是调查的问答题，也可以是对某个产品的评论。我们可能希望将这些列直接转换成列表，但是将几列数据转换成列表也会创建一个嵌套列表。</p><pre class="kt ku kv kw gu oe of og oh aw oi bi"><span id="29c7" class="oj ni iu of b gz ok ol l om on"># df_essays is a dataframe with few columns of text data<br/>essay_list = df_essays.values.tolist() # this create a nested list</span><span id="e3d1" class="oj ni iu of b gz pf ol l om on"># this create a flat list<br/>flatEssayList = [item for elem in essay_list for item in elem if str(item) != 'nan']</span></pre><p id="f674" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><strong class="ll je"> 2。从数据框架(单列)或列表到文本</strong></p><p id="50a6" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">文本分析中最流行的应用之一是WordCloud，它主要用于分析文本中最常见的讨论。这个模块只接受文本作为输入，所以在这个场景中，我们需要改变列表或数据帧。列到文本。</p><pre class="kt ku kv kw gu oe of og oh aw oi bi"><span id="2353" class="oj ni iu of b gz ok ol l om on"># dataframe.column to text<br/>text = ‘ ‘.join(str(x) for x in df[‘review’])<br/># list to text<br/>text = ‘ ‘.join(str(x) for x in any_list)</span></pre><p id="b2ac" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><strong class="ll je"> 3。从数据帧(单列)到列表</strong></p><pre class="kt ku kv kw gu oe of og oh aw oi bi"><span id="19da" class="oj ni iu of b gz ok ol l om on">reviews_list = df["reviews"].values.tolist()</span></pre><p id="86ed" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><strong class="ll je"> 4。从文本到列表</strong></p><p id="cca4" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">这既简单又方便。</p><pre class="kt ku kv kw gu oe of og oh aw oi bi"><span id="f9a2" class="oj ni iu of b gz ok ol l om on">token_list = text.split()</span></pre><p id="19a6" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><strong class="ll je"> 5。从元组到数据帧</strong></p><p id="285a" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">我上面提到过这个，在这里重复一遍，以便更好的参考。</p><p id="72ab" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><code class="fe pc pd pe of b">nltk.pos_tag</code>模块以<code class="fe pc pd pe of b">(word, pos tag)</code>的形式返回标签集作为元组。</p><pre class="kt ku kv kw gu oe of og oh aw oi bi"><span id="f751" class="oj ni iu of b gz ok ol l om on">data_tagset = nltk.pos_tag(tokens)<br/>df_tagset = pd.DataFrame(data_tagset, columns=['Word', 'Tag'])</span></pre></div><div class="ab cl mf mg hy mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="in io ip iq ir"><h1 id="7788" class="nh ni iu bd nj nk nl nm nn no np nq nr kj ns kk nt km nu kn nv kp nw kq nx ny bi translated">一些旁注</h1><p id="ee82" class="pw-post-body-paragraph lj lk iu ll b lm nz ke lo lp oa kh lr ls ob lu lv lw oc ly lz ma od mc md me in bi translated">如果你对从关键词列表中提取包含关键词的句子感兴趣:<br/> <a class="ae li" rel="noopener" target="_blank" href="/text-extraction-using-regular-expression-python-186369add656">使用正则表达式(Python)的文本提取</a>。</p><p id="cb8b" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">如果你有兴趣用词云探索巨大的文本:<br/> <a class="ae li" rel="noopener" target="_blank" href="/text-exploration-with-python-cb8ea710e07c">用词云探索文本</a>。</p><h1 id="a115" class="nh ni iu bd nj nk pg nm nn no ph nq nr kj pi kk nt km pj kn nv kp pk kq nx ny bi translated">保持联系</h1><p id="458a" class="pw-post-body-paragraph lj lk iu ll b lm nz ke lo lp oa kh lr ls ob lu lv lw oc ly lz ma od mc md me in bi translated">订阅<a class="ae li" href="https://www.youtube.com/channel/UCiMtx0qbILP41Ot-pkk6eJw" rel="noopener ugc nofollow" target="_blank"> YouTube </a></p><h1 id="7d94" class="nh ni iu bd nj nk pg nm nn no ph nq nr kj pi kk nt km pj kn nv kp pk kq nx ny bi translated">参考</h1><p id="8a99" class="pw-post-body-paragraph lj lk iu ll b lm nz ke lo lp oa kh lr ls ob lu lv lw oc ly lz ma od mc md me in bi translated">[1] M. Iqbal，“脸书收入和使用统计(2020年)”，2021年3月8日。【在线】。可用:<a class="ae li" href="https://www.businessofapps.com/data/facebook-statistics/." rel="noopener ugc nofollow" target="_blank">https://www.businessofapps.com/data/facebook-statistics/.</a></p><p id="ac3b" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">[2] M. Mayo，“预处理文本数据的一般方法”，2017。【在线】。可用:<a class="ae li" href="https://www.kdnuggets.com/2017/12/general-approach-preprocessing-text-data.html." rel="noopener ugc nofollow" target="_blank">https://www . kdnugges . com/2017/12/general-approach-预处理-text-data . html</a>【2020年6月12日访问】。</p><p id="55e9" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">[3] D. Subramanian，“Python中的文本挖掘:步骤和示例”，2019年8月22日。【在线】。可用:<a class="ae li" href="https://medium.com/towards-artificial-intelligence/text-mining-in-python-steps-and-examples-78b3f8fd913b." rel="noopener">https://medium . com/forward-artificial-intelligence/text-mining-in-python-steps-and-examples-78 B3 F8 FD 913 b。</a>【2020年6月12日进入】。</p><p id="501f" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">[4] M. Mayo，《自然语言处理关键术语解释》，2017。【在线】。可用:<a class="ae li" href="https://www.kdnuggets.com/2017/02/natural-language-processing-key-terms-explained.html." rel="noopener ugc nofollow" target="_blank">https://www . kdnugges . com/2017/02/natural-language-processing-key-terms-explained . html .</a></p><p id="59a6" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">[5]《Julia中的自然语言处理(文本分析)》，JCharisTech，2018年5月1日。【在线】。可用:<a class="ae li" href="https://jcharistech.wordpress.com/2018/05/01/natural-language-processing-in-julia-text-analysis/." rel="noopener ugc nofollow" target="_blank">https://jcharistech . WordPress . com/2018/05/01/natural-language-processing-in-Julia-text-analysis/。</a></p><p id="58ff" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">[6] D. Jurafsky和J. H. Martin，“语音和语言处理”，2020年12月3日。【在线】。可用:<a class="ae li" href="https://web.stanford.edu/~jurafsky/slp3/." rel="noopener ugc nofollow" target="_blank">https://web.stanford.edu/~jurafsky/slp3/.</a></p><p id="1687" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated">[7] M.F. Goh，“使用spaCy和NLTK进行文本规范化”，2020年11月29日。【在线】。可用:<a class="ae li" rel="noopener" target="_blank" href="/text-normalization-with-spacy-and-nltk-1302ff430119.">https://towardsdatascience . com/text-normalization-with-spacy-and-nltk-1302 ff 430119。</a></p></div><div class="ab cl mf mg hy mh" role="separator"><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk ml"/><span class="mi bw bk mj mk"/></div><div class="in io ip iq ir"><p id="0208" class="pw-post-body-paragraph lj lk iu ll b lm ln ke lo lp lq kh lr ls lt lu lv lw lx ly lz ma mb mc md me in bi translated"><em class="nd">祝贺并感谢你阅读到最后。希望你喜欢这篇文章。</em> ☺️</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pl"><img src="../Images/26eb9950ccaeafba69e8f996cfb2dedb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cnuTrXFhcpC3GgDS8RxFjw.jpeg"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">照片由<a class="ae li" href="https://unsplash.com/@cmhedger?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Courtney hedge</a>在<a class="ae li" href="https://unsplash.com/s/photos/thank-you?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure></div></div>    
</body>
</html>