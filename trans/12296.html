<html>
<head>
<title>Overcoming Input Length Constraints of Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">克服变压器的输入长度限制</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/overcoming-input-length-constraints-of-transformers-b0dd5c557f7e?source=collection_archive---------19-----------------------#2021-12-14">https://towardsdatascience.com/overcoming-input-length-constraints-of-transformers-b0dd5c557f7e?source=collection_archive---------19-----------------------#2021-12-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="f187" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="2af0" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">一种面向长文档训练的抽取摘要方法</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/f6aa49e2b2732ef60413e9b984a4b86a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AddVFc_WmDm5EwzS6vOOeA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">由<a class="ae le" href="https://unsplash.com/@jaredd_craig?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">贾瑞德·克雷格</a>在<a class="ae le" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="a61f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">名副其实，Transformers<strong class="lh ja">【1】</strong>在过去几年中真正改变了NLP领域，主要是因为它们的并行化能力允许大型预训练模型，如BERT<strong class="lh ja">【2】</strong>。虽然BERT及其衍生工具已经在NLP的大多数领域显示出了最先进的结果，但是Transformer模型有一个主要缺点:它很难应用于非常长的文档。这个困难是由于自我注意操作，其相对于输入长度具有O(n)的指数复杂度。随着许多公司将人工智能模型集成到他们的工作流程中，这可能会成为一个问题，因为并非所有公司都有资源来有效处理他们可以访问的数据的大小和复杂性。例如律师事务所(法律文件)和医院(医疗记录)。</p><p id="acaf" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">已经提出了几种解决方案来减轻长文档带来的问题。最简单的解决方案是<strong class="lh ja">截断</strong>，其中只将文档的前N个标记作为输入，但是这从原始文档中丢弃了许多潜在的有价值的信息。另一个解决方案是<strong class="lh ja">分块</strong>，将一个文档分割成更小的块，每个块都作为模型的一个独立输入。虽然这确实允许更长的输入，但是每个块必须被单独编码，并且必须在下游模型中使用，这仍然使得它在计算上是昂贵的。这种方法的另一个缺点是长时间的注意力丢失了，因为组块之间没有注意力。一个有希望的新解决方案是通过用一种称为局部敏感哈希自关注<strong class="lh ja">【9】</strong>的近似方法来替代全局自关注，潜在地降低计算复杂度，这种近似方法在重整器模型<strong class="lh ja">【10】</strong>中使用，有效地将复杂度降低到O(L log L)。</p><p id="3ad8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">然而，我建议<strong class="lh ja">针对长文档输入的另一种解决方案:摘要</strong>。我将展示如何使用提取摘要首先从长文档中提取重要信息，然后利用这些摘要训练一个基于Transformer的模型。换句话说，我将演示如何应用摘要来减少输入大小，从而减少计算需求。</p><p id="f759" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了评估这种方法，我们比较了“引文预测”任务的几个输入预测科学文献的引用数量是一项任务，通过利用文献的全文与仅使用摘要相比，已经取得了明显的改进<strong class="lh ja"> [3]。</strong>对于我们的实验来说，引用预测任务有两个独特的性质:首先，我们知道需要长文档输入才能在这项任务中获得更好的结果。第二，我们有一个摘要形式的人工摘要，我们可以将自己的摘要与之进行比较。我们提出的模型如图1所示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mb"><img src="../Images/85a2aa81687497ead044d692b3388a28.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mjc13lQb3P3a3_rg9YX93g.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd mc">图1: T </strong>他提出了引文预测模型。使用LexRank从输入文档中提取摘要，然后将其用作引用预测的Transformer模型的输入。图片作者。</p></figure><h1 id="37a4" class="md me iq bd mf mg mh mi mj mk ml mm mn kf mo kg mp ki mq kj mr kl ms km mt mu bi translated"><strong class="ak">摘录摘要的复习</strong></h1><p id="17c3" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo mx lq lr ls my lu lv lw mz ly lz ma ij bi translated">为了理解我们的方法是如何工作的，重要的是理解我们的总结算法是如何工作的。摘要算法有两种风格:提取的和抽象的。</p><p id="e51d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">提取摘要算法从源文档中选择一组被组合成摘要的句子，而抽象摘要算法生成包含不一定出现在源文档中的句子的摘要。</p><p id="6c72" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">虽然在过去的几年中在抽象概括方面已经取得了很大的进步，主要是由于基于预训练的编码器-解码器的模型(例如变换器)的出现，但是抽象概括模型要复杂得多，并且在计算上非常昂贵；违背了我们实验的目的。出于这个原因，我们使用了lex rank<strong class="lh ja">【4】，</strong>一种2004年的快速摘要算法，它仍然是摘要中最先进的算法之一。</p><p id="2483" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">LexRank使用无监督的基于图的方法从文档中选择最相关的句子。LexRank的工作方式如下:输入文档中的每一个句子首先被转化为一个嵌入(使用句子中单词的平均值TF-IDF)，然后被视为图中的一个节点。通过计算句子嵌入之间的余弦相似度来创建连接或边缘。在实践中，这些信息被放在一个连接矩阵中，该矩阵包含所有句子的所有相似之处。然后，LexRank对每个连接应用相似性阈值，以确保在连接矩阵中只使用“强”连接。这就产生了一个由0和1组成的矩阵。之后，每个节点除以它的度数:它拥有的连接数。最后，使用幂迭代方法计算每个句子的得分，并返回得分最高的前N个句子作为摘要。因此，我们的摘要是原始文档的严格子集。对于算法的更广泛的解释，我鼓励你阅读Erkan等人的论文<strong class="lh ja">【4</strong>】。</p><h1 id="bc62" class="md me iq bd mf mg mh mi mj mk ml mm mn kf mo kg mp ki mq kj mr kl ms km mt mu bi translated"><strong class="ak">实验装置</strong></h1><p id="8f12" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo mx lq lr ls my lu lv lw mz ly lz ma ij bi translated">为了评估我们的方法，我们使用LexRank从所有的全文输入中生成了摘要。我们将这些摘要与训练数据中每个文档的摘要和全文进行了比较。此外，我们还为每个文档生成了一组随机句子，作为我们的随机基线。为了公平的比较，作为随机句子的两个生成的摘要具有与人类书面摘要相同数量的句子。</p><p id="15c4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">请注意，该数据集没有可用的小节信息，这就是为什么我们随机选取句子，而不是每个小节的前N个句子(这是文献中常见的基线)。虽然可以将平均引用计数作为基线，但这将总是导致R2值为0，因为我们的预测无法解释我们数据中的任何差异。</p><p id="e2d9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">作为我们的训练数据，我们使用了<strong class="lh ja">【3】</strong>中提出的ACL-文献计量学数据集。该数据集包含来自ACL选集数据库的30950个文档的全文和摘要信息。每篇论文都有一个标签，标明论文发表后前10年的引用情况。由于引用数量的增加遵循齐普斯-曼德勃罗定律(或幂定律)<strong class="lh ja">【8】，</strong>，我们感兴趣的是“从谷壳中分离出小麦”，而不是预测极端影响的论文，因此我们使用了等式1中所示的公式，其中我们取了对数并将引用数量n加1，以确保我们的公式是针对引用数量为0的情况定义的。图2显示了训练集中非标准化引用的图表。请注意，我们数据中的引用计数严重倾斜，并且遵循幂定律。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi na"><img src="../Images/b00a9c2e85c497c0c33e347698a30213.png" data-original-src="https://miro.medium.com/v2/resize:fit:368/format:webp/1*d3oO1bAI8VuVhvSmEQJ7vg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd mc">等式1: </strong>引用的对数归一化函数</p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/0418eeedbced999b684de78460769049.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*zeNq4cT8KrFIx4iU7CzAeA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd mc">图2: </strong>训练数据中引用计数的直方图。图片作者。</p></figure><p id="edc9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">因此，我们有一个文档的四个输入，每个输入都训练了一个引用预测模型:全文、摘要、随机基线和我们的LexRank摘要。我们的假设是，我们可以通过摘要对全文文档中的重要信息进行编码，并匹配甚至提高摘要的性能。</p><p id="9465" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">摘要和概要中的平均句子数是6。相比之下，全文文档中的平均句子数量是150，因此我们的摘要在输入大小上平均减少了25倍。</p><p id="3fa8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们使用了在<strong class="lh ja">【3】</strong>中提出的舒伯特模型，除了我们用SciBERT<strong class="lh ja">【5】</strong>代替了BERT-base，因为我们处理的是科学文档。在这个模型中，使用我们的SciBERT层将全文文档编码为512个标记的块，之后使用单个丢弃层训练GRU层，然后使用单个线性层进行预测。对于架构的更全面的解释，我鼓励你阅读<strong class="lh ja"> [3]。为了生成我们的LexRank摘要，我们使用了Sumy包(https://pypi.org/project/sumy/)。</strong></p><p id="b00a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了比较我们的结果，我们报告了三个常见的回归指标:R2、均方误差(MSE)和平均绝对误差(MAE)。对于R2，分数越高越好，对于MSE和MAE，分数越低越好。</p><p id="45ef" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">除了评估我们的摘要在预测引用数量方面的表现，我们还计算了摘要和摘要之间的(F1) Rouge-1、Rouge-2和Rouge-L得分<strong class="lh ja">【6】</strong>。我们这样做是为了评估两者之间有多少重叠，因为我们希望它们包括全文中的类似信息。</p><h1 id="ed1a" class="md me iq bd mf mg mh mi mj mk ml mm mn kf mo kg mp ki mq kj mr kl ms km mt mu bi translated"><strong class="ak">建议实验的结果</strong><strong class="ak"/></h1><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nc"><img src="../Images/8b2a5f963419778e7f892efe46c28dfe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t9X0PA9Gsf1r3knu6PMJxg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd mc">表1: </strong>引文预测不同输入类型的结果。</p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nd"><img src="../Images/c7cc588fd9aa31b300b3cd6a96ba8253.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fEc7KFkjeY5har-oJTUtPg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><strong class="bd mc">表2:</strong>F-1·鲁日在摘要和LexRank摘要之间的得分。</p></figure><p id="421c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">表1中列出了测试集的R2、MSE和MAE分数，它们是三次运行的平均值。正如所料，我们选择N个随机句子的基线模型的表现明显比其他输入差。人工撰写的摘要略优于LexRank摘要，而全文输入则显著优于两者。请注意，当我们将这些分数转换回它们的非标准化值时，对于全文文档和摘要文档，我们得到的MAE分别为13.09和13.62。这些高MAE分数受到大量引用的论文的严重影响，这就是为什么我们训练对数标准化引用计数。供参考:如果我们从1474篇论文的测试集中删除引用率最高的前50篇论文，这些分数分别为6.92和7.19。</p><p id="ff90" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">表2列出了摘要和摘要之间的Rouge-1、Rouge-2和RougeL。Rouge的得分与最近的研究相当，在最近的研究中，LexRank被用作基线<strong class="lh ja">【7】</strong>。</p><h1 id="499b" class="md me iq bd mf mg mh mi mj mk ml mm mn kf mo kg mp ki mq kj mr kl ms km mt mu bi translated"><strong class="ak">讨论</strong></h1><p id="efca" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo mx lq lr ls my lu lv lw mz ly lz ma ij bi translated">我们的结果表明，使用LexRank获得的摘要在用作下游任务的输入时，提供了与人工编写的摘要相似的结果。LexRank摘要和摘要的结果的接近程度表明，我们通过使用全文输入的精简版本所能获得的下游结果可能有一个上限。尽管我们提取的摘要并没有提高摘要的性能，但它们具有相似性能的事实表明，当全文文档输入过于昂贵时，提取的摘要可以作为一种很好的输入。这在经常发现长文档，但通常没有压缩版本的领域中特别有用，例如法律或医疗文档。但是，如果可能的话，全文文档仍然是首选，性能的显著提高就证明了这一点。</p><p id="8d62" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">rouge分数表明，就内容而言，摘要和摘要之间存在显著差异，这很有趣，因为表1中的结果非常接近。我们尝试使用摘要和概要的连接作为输入来平衡这些差异，但是这仅仅给出了非常小的改进。</p><p id="6cbe" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">一个有趣的后续研究是比较不同的摘要长度，看看我们是否可以用更短的摘要来匹配全文输入的性能，例如，通过生成长度为我们输入长度一半的摘要。另一个有趣的研究是比较不同的摘要算法，因为在这个实验中只使用了LexRank。最后，由于LexRank为每个句子输出一个分数，我们也可以用它来减少噪音，去掉最不突出的句子。</p><h1 id="9f20" class="md me iq bd mf mg mh mi mj mk ml mm mn kf mo kg mp ki mq kj mr kl ms km mt mu bi translated"><strong class="ak">结论</strong></h1><p id="e755" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo mx lq lr ls my lu lv lw mz ly lz ma ij bi translated">我们表明，摘录摘要在引用预测任务中用作转换器的输入时，具有与人类编写的摘要相似的性能。对于资源有限的长文档培训任务，这是一个很有前途的解决方案。然而，与摘要相比，全文输入仍然有很大的改进。</p><h1 id="2861" class="md me iq bd mf mg mh mi mj mk ml mm mn kf mo kg mp ki mq kj mr kl ms km mt mu bi translated"><strong class="ak">参考文献</strong></h1><p id="4ccd" class="pw-post-body-paragraph lf lg iq lh b li mv ka lk ll mw kd ln lo mx lq lr ls my lu lv lw mz ly lz ma ij bi translated">[1] Ashish Vaswani、Noam Shazeer、Niki Parmar、Jakob Uszkoreit、Llion Jones、Aidan N. Gomez、Lukasz Kaiser和Illia Polosukhin。你只需要关注，2017。</p><p id="6bdd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[2] Jacob Devlin、张明蔚、Kenton Lee和Kristina Toutanova。Bert:用于语言理解的深度双向转换器的预训练。arXiv预印本arXiv:1810.04805，2018。</p><p id="0ecc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[3]托马斯·范东恩、吉迪恩·迈莱特·德·布伊·温尼日尔和兰伯特·肖梅克。舒伯特:带有伯特编码的学术文档块促进引用计数预测。2020年第一届学术文献处理研讨会会议录。</p><p id="1dac" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[4]居内斯·埃尔坎和德拉戈米尔·拉杰夫。基于图的词汇中心性作为文本摘要中的显著性。j .阿提夫。里面的第22(1):457-479号决议，2004年12月。</p><p id="924c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[5] Iz Beltagy、Kyle Lo和Arman Cohan。科学文本的预训练语言模型。在EMNLP，2019。</p><p id="99a6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[6]林金耀。ROUGE:一个自动评估摘要的包。《文本摘要分支》, 74-81页，西班牙巴塞罗那，2004年7月。计算语言学协会。</p><p id="6444" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[7]董玥、安德烈·米尔恰和成龙。长科学文献的话语感知无监督摘要，2021。</p><p id="7dfa" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[8]祖拉布·西拉加德泽。引文和齐夫-曼德勃罗定律。复杂系统，1999年2月11日。</p><p id="7b8e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[9]亚历山大·巴甫洛夫·安多尼、皮奥特·因迪克、蒂伊斯·拉霍文、伊利亚·拉赞施泰因和路德维希·施密特。角距离的实用和最佳lsh，2015。</p><p id="eb52" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[10]尼基塔·基塔耶夫、祖卡斯·凯泽和安塞姆·列夫斯卡娅。改革者:高效的变压器，2020。</p></div></div>    
</body>
</html>