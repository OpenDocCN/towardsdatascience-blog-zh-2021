<html>
<head>
<title>How to Mitigate Overfitting with Regularization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何通过正则化减轻过度拟合</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-mitigate-overfitting-with-regularization-befcf4e41865?source=collection_archive---------26-----------------------#2021-09-24">https://towardsdatascience.com/how-to-mitigate-overfitting-with-regularization-befcf4e41865?source=collection_archive---------26-----------------------#2021-09-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="320a" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">解决过度拟合问题</h2><div class=""/><div class=""><h2 id="ffe2" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">解决过度拟合问题—第2部分</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/25d47cd999115b3ae2fb46e712a1c00c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WCtsn_sq6WTCoDAkKU-LQQ.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">在<a class="ae lh" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae lh" href="https://unsplash.com/@kayaan?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">kayan uda chia</a>拍摄的照片</p></figure><p id="ae5b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">今天，我们继续<strong class="lk jd">“解决过度合身问题”</strong>系列文章的<a class="ae lh" rel="noopener" target="_blank" href="/how-to-mitigate-overfitting-with-k-fold-cross-validation-518947ed7428">第1部分</a>。<strong class="lk jd">正则化</strong>是另一种有用的技术，可用于减轻机器学习模型中的过拟合。今天将更着重讨论正则化背后的<em class="me">直觉</em>而不是讨论它的数学公式。通过这种方式，你可以清楚地了解将正则化应用于机器学习模型的效果。</p><p id="448c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">一般来说，<strong class="lk jd">【规则化】</strong>的意思是<strong class="lk jd">限制</strong> / <strong class="lk jd">控制</strong>。在机器学习的背景下，正则化处理模型的复杂性。它限制了模型的复杂性或者限制了模型在训练阶段的学习过程。一般来说，我们更喜欢简单而准确的模型，因为复杂的模型更容易过度拟合。通过限制模型的复杂性，过度拟合试图使模型尽可能简单，同时这些模型仍能做出准确的预测。</p><p id="1cda" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">有两种方法可以将正则化应用于机器学习模型:</p><ul class=""><li id="b331" class="mf mg it lk b ll lm lo lp lr mh lv mi lz mj md mk ml mm mn bi translated"><strong class="lk jd">通过在损失函数</strong>中增加另一项，我们试图使其最小化。现在，目标函数由两部分组成:<strong class="lk jd">损失函数</strong>和<strong class="lk jd">正则项</strong>:</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/8dbb74989ce79fd724213aa4a65f8492.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*GzPfxQi45IYys6PYaV1_NA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">(图片由作者提供)</p></figure><ul class=""><li id="2a4c" class="mf mg it lk b ll lm lo lp lr mh lv mi lz mj md mk ml mm mn bi translated"><strong class="lk jd">通过在培训阶段提前停止学习过程</strong>。这种方法的完美例子是在早期阶段停止决策树的生长。</li></ul><p id="6c03" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们将通过编写Python代码用例子来讨论每种方法。最后，您将获得将正则化应用于这里讨论的模型的实践经验。</p><h1 id="994d" class="mp mq it bd mr ms mt mu mv mw mx my mz ki na kj nb kl nc km nd ko ne kp nf ng bi translated">方法1:向损失函数添加一个<strong class="ak">正则化</strong>项</h1><p id="192f" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr nj lt lu lv nk lx ly lz nl mb mc md im bi translated">这里，我们将在名为<strong class="lk jd"/><a class="ae lh" href="https://drive.google.com/file/d/19s5qMRjssBoohFb2NY4FFYQ3YW2eCxP4/view?usp=sharing" rel="noopener ugc nofollow" target="_blank"><strong class="lk jd">heart _ disease . CSV</strong></a><strong class="lk jd"/>的数据集上建立一个逻辑回归模型。首先，我们在不添加正则项(<strong class="lk jd"> penalty=none </strong>)的情况下构建模型，并查看输出。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nm nn l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">无任何正则化的逻辑回归模型</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi no"><img src="../Images/24aa45c816b7f8b9eb30eb3bee1214b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*GtDAjltRwzBUV3x_Cq6ZsA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">(图片由作者提供)</p></figure><p id="3273" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">模型已经很好了。让我们看看是否可以通过增加一个正则项来进一步提高测试精度。这里，我们将L2正则化(<strong class="lk jd">罚=l2 </strong>)添加到模型中。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nm nn l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">L2正则化的Logistic回归模型</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi np"><img src="../Images/a09872e997318c0dbeab2c5b316e3cf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*66neO0qMOQ5bEJFqVmthXQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">(图片由作者提供)</p></figure><p id="80fa" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">加入正则项后，测试精度提高了3%。假阳性的数量也减少了。因此，该模型能够对新的未知数据(测试数据集)进行归纳。</p><h1 id="13c9" class="mp mq it bd mr ms mt mu mv mw mx my mz ki na kj nb kl nc km nd ko ne kp nf ng bi translated">方法2:尽早停止学习过程</h1><p id="07f9" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr nj lt lu lv nk lx ly lz nl mb mc md im bi translated">这里，我们将在同一个数据集上构建一个决策树分类模型。首先，我们将构建完整的决策树，而不限制树的增长(即，不应用正则化/不提前停止学习过程),并查看输出。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nm nn l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">没有任何正则化(没有提前停止)的决策树模型</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/0474501ef5592b8f11d351aea795d1ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*zbwrPjwNGiPVV5wOj2VYVg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">(图片由作者提供)</p></figure><p id="9d96" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们得到了非常高的训练精度和低的测试精度。因此，模型明显过拟合。这是正常的，因为我们允许树完全成长。现在，我们正试图限制这棵树的生长。这是通过设置<strong class="lk jd">最大_深度</strong>、<strong class="lk jd">最小_样本_叶</strong>和<strong class="lk jd">最小_样本_分割</strong>超参数的最佳值来实现的(在上述模型中，这些值已被设置为默认值)。在机器学习中，限制树的生长在技术上叫做<strong class="lk jd"> <em class="me">修剪</em> </strong>。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nm nn l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">带有正则化的决策树模型(带有提前停止/修剪)</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/10b18f23213b561fd66cffde0cfc8062.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*QjoWjKBIxOPV6T8bCfb-Tw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">(图片由作者提供)</p></figure><p id="bd2b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">模型得到了显著改进(测试准确率提高了16%！).</p><p id="bfaa" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">注:</strong>寻找一组超参数的最佳值在技术上称为<strong class="lk jd">超参数调整</strong>或<strong class="lk jd">超参数优化</strong>。如果你不知道这个程序，你可以通过阅读我写的下面两个帖子来了解:</p><div class="ns nt gp gr nu nv"><a rel="noopener follow" target="_blank" href="/k-fold-cross-validation-explained-in-plain-english-659e33c0bc0"><div class="nw ab fo"><div class="nx ab ny cl cj nz"><h2 class="bd jd gy z fp oa fr fs ob fu fw jc bi translated">用简单的英语解释k倍交叉验证</h2><div class="oc l"><h3 class="bd b gy z fp oa fr fs ob fu fw dk translated">用于评估模型的性能和超参数调整</h3></div><div class="od l"><p class="bd b dl z fp oa fr fs ob fu fw dk translated">towardsdatascience.com</p></div></div><div class="oe l"><div class="of l og oh oi oe oj lb nv"/></div></div></a></div><div class="ns nt gp gr nu nv"><a rel="noopener follow" target="_blank" href="/4-useful-techniques-that-can-mitigate-overfitting-in-decision-trees-87380098bd3c"><div class="nw ab fo"><div class="nx ab ny cl cj nz"><h2 class="bd jd gy z fp oa fr fs ob fu fw jc bi translated">4种可以减轻决策树过度拟合的有用技术</h2><div class="od l"><p class="bd b dl z fp oa fr fs ob fu fw dk translated">towardsdatascience.com</p></div></div><div class="oe l"><div class="ok l og oh oi oe oj lb nv"/></div></div></a></div><h1 id="bb4f" class="mp mq it bd mr ms mt mu mv mw mx my mz ki na kj nb kl nc km nd ko ne kp nf ng bi translated">结论</h1><p id="53da" class="pw-post-body-paragraph li lj it lk b ll nh kd ln lo ni kg lq lr nj lt lu lv nk lx ly lz nl mb mc md im bi translated">过拟合经常发生在建模中。正则化是减轻过度拟合的另一种有用的技术。今天，我们通过例子讨论了两种正则化方法。请注意，通过为最大迭代次数指定较低的值，早期停止方法可以应用于逻辑回归、线性回归等算法。</p><p id="0b2a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">不得混淆术语<strong class="lk jd">【正规化】</strong>和<strong class="lk jd">【普遍化】</strong>。应用正则化来实现一般化。正则化模型将很好地概括看不见的数据。它<strong class="lk jd"> <em class="me">学习数据中的</em> </strong>重要模式，对看不见的数据做出准确预测，而不是<strong class="lk jd"> <em class="me">记忆</em> </strong>数据集中的噪音。</p><p id="680f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">更新(2021–09–27):</strong>第3部分现已推出！<br/> [ <a class="ae lh" rel="noopener" target="_blank" href="/how-to-mitigate-overfitting-with-dimensionality-reduction-555b755b3d66">如何通过降维来减轻过拟合</a></p></div><div class="ab cl ol om hx on" role="separator"><span class="oo bw bk op oq or"/><span class="oo bw bk op oq or"/><span class="oo bw bk op oq"/></div><div class="im in io ip iq"><p id="130a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">今天的帖子到此结束。我的读者可以通过下面的链接注册成为会员，以获得我写的每个故事的全部信息，我将收到你的一部分会员费。</p><p id="519f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">报名链接:</strong>T19】https://rukshanpramoditha.medium.com/membership</p><p id="7acd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">非常感谢你一直以来的支持！下一个故事再见。祝大家学习愉快！</p><p id="657b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">特别感谢Unsplash网站上的<strong class="lk jd"> Kayaan Udachia </strong>，<strong class="lk jd"> </strong>为我提供了这篇文章的封面图片(我对图片做了一些修改:添加了一些文字并删除了一些部分)。</p><p id="494d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="os ot ep" href="https://medium.com/u/f90a3bb1d400?source=post_page-----befcf4e41865--------------------------------" rel="noopener" target="_blank">鲁克山普拉莫迪塔</a><br/><strong class="lk jd">2021–09–24</strong></p></div></div>    
</body>
</html>