<html>
<head>
<title>Tutorial: Running PySpark inside Docker containers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">教程:在Docker容器中运行PySpark</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/tutorial-running-pyspark-inside-docker-containers-84970d12b20e?source=collection_archive---------15-----------------------#2021-10-28">https://towardsdatascience.com/tutorial-running-pyspark-inside-docker-containers-84970d12b20e?source=collection_archive---------15-----------------------#2021-10-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/061126088828a62021170fa02fe26fac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oDSNy8s0OC3TW9TSMox5jg.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">由<a class="ae kc" href="https://unsplash.com/photos/sWOvgOOFk1g" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的图像</p></figure><p id="ec80" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文中，我们将向您展示如何开始在Docker容器中运行PySpark应用程序，通过一步一步的教程和代码示例(<a class="ae kc" href="https://github.com/datamechanics/examples" rel="noopener ugc nofollow" target="_blank">参见github repo </a>)。</p><p id="ad82" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在Docker容器中运行Spark应用程序有多种动机(我们在之前的文章<a class="ae kc" href="https://www.datamechanics.co/blog-post/spark-and-docker-your-spark-development-cycle-just-got-ten-times-faster" rel="noopener ugc nofollow" target="_blank"> <em class="lb"> Spark &amp; Docker中讨论过——您的开发工作流程刚刚快了10倍</em> </a>):</p><ul class=""><li id="b97e" class="lc ld iq kf b kg kh kk kl ko le ks lf kw lg la lh li lj lk bi translated">Docker容器简化了依赖项的打包和管理，如外部java库(jar)或python库，它们可以帮助处理数据或连接到外部数据存储。添加或升级库可能会中断您的管道(例如，由于冲突)。使用Docker意味着您可以在开发时在本地捕获这个错误，修复它，然后发布您的映像，并且确信无论代码在哪里运行，jar和环境都是相同的。</li><li id="7edd" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la lh li lj lk bi translated">Docker容器也是在集群(例如一个<a class="ae kc" href="https://www.datamechanics.co/blog-post/pros-and-cons-of-running-apache-spark-on-kubernetes" rel="noopener ugc nofollow" target="_blank"> Kubernetes集群</a>)上大规模运行Spark代码之前，在本地开发和测试Spark代码的好方法。</li></ul><blockquote class="lq lr ls"><p id="00c4" class="kd ke lb kf b kg kh ki kj kk kl km kn lt kp kq kr lu kt ku kv lv kx ky kz la ij bi translated">注意:我们维护着一系列Docker映像，这些映像内置了一系列有用的库，如数据湖、数据仓库、流数据源等的数据连接器。你可以在这里阅读关于这些图片<a class="ae kc" href="https://www.datamechanics.co/blog-post/optimized-spark-docker-images-now-available." rel="noopener ugc nofollow" target="_blank">的更多信息，并在</a><a class="ae kc" href="https://hub.docker.com/r/datamechanics/spark" rel="noopener ugc nofollow" target="_blank"> Dockerhub </a>上免费下载。</p></blockquote><p id="586a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本教程中，我们将带您完成从我们的一个基础映像构建一个新的Docker映像的过程，添加新的依赖项，并通过使用考拉库和将一些数据写入postgres数据库来测试您已经安装的功能。</p><h1 id="15de" class="lw lx iq bd ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt bi translated">‍Requirements</h1><p id="fc83" class="pw-post-body-paragraph kd ke iq kf b kg mu ki kj kk mv km kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">为了这个演示的目的，我们将创建一个简单的PySpark应用程序，它从公共数据集中读取每个国家的人口密度数据—<a class="ae kc" href="https://registry.opendata.aws/dataforgood-fb-hrsl" rel="noopener ugc nofollow" target="_blank">https://registry.opendata.aws/dataforgood-fb-hrsl</a>，应用转换来找到人口中值，并将结果写入postgres实例。为了找到中间值，我们将利用熊猫API的Spark实现<a class="ae kc" href="https://koalas.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">考拉</a>。</p><blockquote class="lq lr ls"><p id="9ac2" class="kd ke lb kf b kg kh ki kj kk kl km kn lt kp kq kr lu kt ku kv lv kx ky kz la ij bi translated">从Spark 3.2+开始，熊猫库将自动与开源的Spark捆绑。在本教程中，我们使用的是Spark 3.1，但在未来你不需要安装考拉，它会开箱即用。</p></blockquote><p id="f2aa" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用Spark查找中间值可能非常繁琐，因此我们将利用koalas功能来获得一个简洁的解决方案(注意:要从公共数据源读取数据，您需要访问AWS_ACCESS_KEY_ID和AWS_SECRET_ACCESS_KEY。桶是公共的，但是AWS需要用户创建才能从公共桶中读取)。</p><p id="056b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果手头没有postgres数据库，可以使用以下步骤在本地创建一个:</p><pre class="mz na nb nc gt nd ne nf ng aw nh bi"><span id="be62" class="ni lx iq ne b gy nj nk l nl nm">docker pull postgres<br/>docker run -e POSTGRES_PASSWORD=postgres -e POSTGRES_USER=postgres -d -p 5432:5432 postgres</span></pre><h1 id="d750" class="lw lx iq bd ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt bi translated">码头工人形象‍Building</h1><figure class="mz na nb nc gt jr"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="f8df" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，我们将使用最新的数据力学基础映像—<a class="ae kc" href="https://gcr.io/datamechanics/spark:platform-3.1-latest" rel="noopener ugc nofollow" target="_blank">gcr.io/datamechanics/spark:platform-3.1-latest</a>构建我们的Docker映像。有几种方法可以在Spark应用程序中包含外部库。对于特定语言的库，您可以使用类似python的<a class="ae kc" href="https://docs.python.org/3/installing/index.html" rel="noopener ugc nofollow" target="_blank"> pip </a>或scala的<a class="ae kc" href="https://www.scala-sbt.org/" rel="noopener ugc nofollow" target="_blank"> sbt </a>这样的包管理器来直接安装库。或者，您可以将库jar下载到您的Docker映像中，并将jar移动到<strong class="kf ir"><em class="lb">/opt/spark/jars</em></strong>。这两种方法可以达到相同的效果，但是某些库或包可能只能用一种方法安装。</p><p id="a4fb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要安装考拉，我们将使用pip将包直接添加到python环境中。标准惯例是创建一个列出所有python依赖项的需求文件，并使用pip将每个库安装到您的环境中。在您的应用程序repo中，创建一个名为<em class="lb"> requirements.txt </em>的文件，并添加下面一行—<strong class="kf ir"><em class="lb">koalas = = 1 . 8 . 1</em></strong>。将该文件复制到您的Docker映像中，并添加以下命令<strong class="kf ir"><em class="lb">RUN pip 3 install-r requirements . txt</em></strong>。现在，您应该能够将考拉直接导入到您的python代码中。</p><p id="1616" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，我们将使用jar方法安装必要的sql驱动程序，以便Spark可以直接写入postgres。首先，让我们将postgres驱动程序jar放到您的映像中。将下面一行添加到Dockerfile中—<strong class="kf ir"><em class="lb">RUN wget</em></strong><a class="ae kc" href="https://jdbc.postgresql.org/download/postgresql-42.2.5.jar" rel="noopener ugc nofollow" target="_blank"><strong class="kf ir"><em class="lb">【https://jdbc.postgresql.org/download/postgresql-42.2.5.jar】</em></strong></a>，并将返回的jar移动到<strong class="kf ir"><em class="lb">/opt/spark/jars</em></strong>。当您启动Spark应用程序时，postgres驱动程序将位于类路径中，您将能够成功地写入数据库。</p><h1 id="ff2e" class="lw lx iq bd ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt bi translated">‍Developing你的应用代码</h1><figure class="mz na nb nc gt jr"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="da53" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们将添加我们的应用程序代码。在上面的代码片段中，我们有一个简单的Spark应用程序，它从公共bucket源读取数据帧。接下来，我们通过对country列进行分组、将population列转换为字符串并进行聚合来转换Spark数据框架。然后，我们将Spark数据帧转换成考拉数据帧，并应用中值函数(Spark中没有的操作)。最后，我们将考拉数据帧转换回Spark数据帧，并添加一个日期列，这样我们就可以利用<strong class="kf ir"> <em class="lb"> write.jdbc </em> </strong>函数将数据输出到SQL表中。‍</p><h1 id="10c4" class="lw lx iq bd ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt bi translated">‍Monitoring和调试</h1><figure class="mz na nb nc gt jr"><div class="bz fp l di"><div class="nn no l"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">运行图像的脚本(注意，这是一个<a class="ae kc" href="https://github.com/casey/just" rel="noopener ugc nofollow" target="_blank">just文件</a>)</p></figure><p id="b247" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了帮助在本地运行应用程序，我们已经包含了一个justfile，其中包含一些有用的命令。</p><ul class=""><li id="8153" class="lc ld iq kf b kg kh kk kl ko le ks lf kw lg la lh li lj lk bi translated">要在本地构建您的Docker映像，运行<strong class="kf ir"> <em class="lb">只需构建</em> </strong></li><li id="0012" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la lh li lj lk bi translated">要运行PySpark应用程序，运行<strong class="kf ir"> <em class="lb">只需运行</em> </strong></li><li id="1fde" class="lc ld iq kf b kg ll kk lm ko ln ks lo kw lp la lh li lj lk bi translated">要访问Docker映像中的PySpark shell，运行<strong class="kf ir"> <em class="lb"> just shell </em> </strong></li></ul><p id="f1a5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">也可以通过运行<strong class="kf ir"><em class="lb">Docker run-it&lt;image name&gt;/bin/bash</em></strong>直接执行到Docker容器中。这将创建一个交互式shell，可用于探索Docker/Spark环境，以及监控性能和资源利用。</p><h1 id="b46c" class="lw lx iq bd ly lz ma mb mc md me mf mg mh mi mj mk ml mm mn mo mp mq mr ms mt bi translated">结论</h1><p id="d096" class="pw-post-body-paragraph kd ke iq kf b kg mu ki kj kk mv km kn ko mw kq kr ks mx ku kv kw my ky kz la ij bi translated">我们希望你觉得这个教程有用！如果你想更深入地研究这段代码，或者探索其他的应用程序演示，请访问我们的<a class="ae kc" href="https://github.com/datamechanics/examples" rel="noopener ugc nofollow" target="_blank"> Github示例</a> repo。</p></div></div>    
</body>
</html>