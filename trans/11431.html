<html>
<head>
<title>Fuzzy C-Means Clustering with Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用 Python 实现模糊 C 均值聚类</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/fuzzy-c-means-clustering-with-python-f4908c714081?source=collection_archive---------4-----------------------#2021-11-10">https://towardsdatascience.com/fuzzy-c-means-clustering-with-python-f4908c714081?source=collection_archive---------4-----------------------#2021-11-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="de5a" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">无监督学习</h2><div class=""/><div class=""><h2 id="9c90" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">在这篇文章中，我简要介绍了无监督学习方法的概念，模糊 C 均值聚类，以及它在 Python 中的实现。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/8802cad16a198dd9534af983613e9354.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zw5I98-mQqVqefOF"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">亚历山大·Cvetanovic 在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="9d8b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi me translated"><span class="l mf mg mh bm mi mj mk ml mm di"> F </span> uzzy C-means 聚类算法是一种无监督学习方法。在了解细节之前，我先来解密一下它的花里胡哨的名字。</p><p id="6780" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">所以，“模糊”在这里的意思是“不确定”，表示这是一种<strong class="lk jd">软聚类</strong>方法。“C-means”的意思是 C 个聚类中心，只是把“K-means”中的“K”换成了一个“C”，让它看起来不一样。</p><p id="7495" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在一个聚类算法中，如果一个数据点属于一个聚类的概率只能取值 1 或 0，这就是<strong class="lk jd">硬聚类</strong>。硬聚类方法中的聚类的边界可以被可视化为清晰的边界。相反，在软聚类方法中，一个数据点属于一个聚类的概率可以取 0 到 1 之间的任何值，例如 75%，为此，聚类的边界可以被可视化为模糊边界。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi mn"><img src="../Images/bffd8ac30b7791e0ef29187fb3811cf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*O5Ynz1UI6ClCs-Bdf-MG9A.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">硬聚类和软聚类。每个点代表一个数据点，每个点中的数字是它属于类 1 的概率，黑色圆圈代表类 1 的类边界。(图片由作者提供)</p></figure><p id="906a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">好了，在了解了软聚类方法的概念之后，我们可以直观地认识到可能驱动聚类的几个重要参数。首先，当然是集群中心。第二，一个点属于特定聚类的概率。</p><h2 id="779a" class="mo mp it bd mq mr ms dn mt mu mv dp mw lr mx my mz lv na nb nc lz nd ne nf iz bi translated">了解参数</h2><p id="4a62" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">在<em class="nl">模糊 c 均值</em> ( <strong class="lk jd"> FCM </strong>)聚类方法中，我们有两个参数，<strong class="lk jd"> <em class="nl"> μ_ij </em> </strong>和<strong class="lk jd"> <em class="nl"> c_i </em> </strong>和一个超参数，<strong class="lk jd"> <em class="nl"> m </em> </strong>。</p><p id="743f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"><em class="nl">【μ_ ij】</em></strong><em class="nl">隶属值</em> <strong class="lk jd"> <em class="nl">，</em> </strong>是第<em class="nl">个</em>个数据点属于第<em class="nl">个</em>聚类的概率，约束为每个数据点<em class="nl"> j </em>的<strong class="lk jd"> <em class="nl"> μ_ij </em> </strong>与<em class="nl"> C </em>聚类中心之和为<strong class="lk jd"> 1 </strong><strong class="lk jd"> <em class="nl"> c_i </em> </strong>是<em class="nl">与</em>簇的中心(与<strong class="lk jd"> <em class="nl"> X </em> </strong>同维)。而<strong class="lk jd"> <em class="nl"> m </em> </strong>是<em class="nl">模糊化器</em>，控制聚类边界应该模糊到什么程度。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nm"><img src="../Images/c523fec52c7211490ec02e1d456d18a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MeiaDwvDCGjiPK-rNf4Ocg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">参数示例。(图片由作者提供)</p></figure><p id="cf26" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在上面的示例图中，我们正在查看第 5 个数据点<em class="nl"> X_5 </em>，并且假设我们知道只有两个聚类，并且当前聚类中心是<em class="nl">C1</em>和<em class="nl">C2</em>。<strong class="lk jd"> <em class="nl"> μ </em> </strong> _25 是第 5 个数据点属于第 2 类的概率，<strong class="lk jd"> <em class="nl"> μ </em> </strong> _15 是第 5 个数据点属于第 1 类的概率。然后，我们看到第 5 个数据点比 C1 更靠近 C2，所以<strong class="lk jd"> <em class="nl"> μ </em> </strong> _25 (0.6)大于<strong class="lk jd"> <em class="nl"> μ </em> </strong> _15 (0.4)。并且它们满足每个数据点的<strong class="lk jd"> <em class="nl"> μ </em> </strong>之和为 1 的约束，其中<strong class="lk jd"><em class="nl">μ</em></strong>_ 15+<strong class="lk jd"><em class="nl">μ</em></strong>_ 25 = 1。</p><p id="d0ba" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">上面的例子只是为了说明 FCM 中的参数，但是实际的值并不一定是那样的。</p><h2 id="a35f" class="mo mp it bd mq mr ms dn mt mu mv dp mw lr mx my mz lv na nb nc lz nd ne nf iz bi translated">目标函数</h2><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/2f019f3fa58ae1e575127c3d4566e70e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*S93u0KtMP-8zwPReuzm5Bw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">FCM 的目标函数。(图片由作者提供)</p></figure><p id="3e82" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我选择在引入参数后显示目标函数，因为它在这里看起来更清晰。</p><p id="c4ee" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">你可以把目标函数理解为数据点(<em class="nl"> X_j </em>)和聚类中心(<em class="nl"> C_i </em>)之间距离的加权和。“距离”项是上式中的<em class="nl"> L2 范数</em>，在上面的示例(第 5 个数据点)中，它正好是箭头的长度。</p><p id="9485" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果<em class="nl"> m = 1 </em>，那么目标函数就是数据点和聚类中心之间距离的概率加权和。这是什么意思？这意味着靠近聚类中心的数据点被赋予较高的权重。</p><p id="b6f0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在上面的例子中，第 5 个数据点和第 2 聚类中心之间的距离比第 5 个数据点和第 1 聚类中心之间的距离对目标函数的贡献更大，因为<strong class="lk jd"> <em class="nl"> μ </em> </strong> _25 = 0.6 和<strong class="lk jd"> <em class="nl"> μ </em> </strong> _15 = 0.4。</p><p id="ac0d" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">记住，我们希望最小化目标函数，因此对于那些长距离(数据点和聚类中心之间)，使用小的<strong class="lk jd"> <em class="nl"> μ </em> </strong>是很好的。</p><p id="60e7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果 m =2，3，…会怎么样？然后距离的贡献差越来越小，如下图所示(<strong class="lk jd"> <em class="nl"> μ </em> </strong> _15 和<strong class="lk jd"> <em class="nl"> μ </em> </strong> _25)，都接近于 0。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi no"><img src="../Images/deb99ddca818b2ec6b1303cc429c8c4d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*QvtMKutv1JpyvLhwidNG2w.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">重量衰减曲线。红色曲线为<strong class="bd np"> <em class="nq"> μ </em> </strong> _25，蓝色曲线为<strong class="bd np"> <em class="nq"> μ </em> </strong> _15。(图片由作者提供)</p></figure><p id="a5cc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因此，对于<strong class="lk jd">一个非常大的 m </strong>，聚类<strong class="lk jd">中心</strong>倾向于位于与所有数据点的<strong class="lk jd">距离</strong>几乎相等的地方，因为每个点到中心的距离对目标函数的贡献<strong class="lk jd"> <em class="nl">相等</em> </strong>。那么，聚类中心在哪里呢？是的，所有数据点的中心。</p><p id="b8b2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果 m 超级大，所有聚类中心都位于所有数据点的质心，那么聚类就会超级“<strong class="lk jd"> <em class="nl">模糊</em> </strong>”因为根本不聚类！</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nr"><img src="../Images/4c1ccf53e2079c951477b98023885e05.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KQwVCWM4OJr54jhPYdn8zA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">FCM 中非常大的 m 的例子。(图片由作者提供)</p></figure><p id="5fb4" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因此，为了进行有意义的聚类，我们在大多数情况下使用 m = 2。</p><h2 id="f5ce" class="mo mp it bd mq mr ms dn mt mu mv dp mw lr mx my mz lv na nb nc lz nd ne nf iz bi translated">寻找最小化目标函数的参数</h2><p id="d40d" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">受约束的参数优化可以使用拉格朗日函数手动解决，但我们不会在这篇文章中讨论它。相反，我现在列出的是计算<strong class="lk jd"> <em class="nl"> c_i </em> </strong>和<strong class="lk jd"> <em class="nl"> μ_ij 的最终方程。</em>T49】</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/1e8f7da069ad255c6c28aeda981f46c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*F-NuOqazEzwsMNQJ-uKa9g.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">更新 FCM 方程(图片由作者提供)</p></figure><p id="494c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">求解<strong class="lk jd"> <em class="nl"> c_i </em> </strong>的方程相对简单，其中它表示 X 到簇中心<em class="nl"> i </em>的加权平均值。为什么一定要除以(<strong class="lk jd"><em class="nl">μ_ ij</em></strong>)<em class="nl">^m</em>之和？那是因为只有当<strong class="lk jd"> <em class="nl"> m=1 </em> </strong>时，权重之和才等于 1(这种情况下我们可以去掉整个分母)。当<em class="nl"> m &gt; 1 </em>时，权重之和为聚类中心加权和计算的定标器。</p><p id="fca9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">但是<strong class="lk jd"> <em class="nl"> μ_ij </em> </strong>的方程看起来没那么好理解吧？如果我们把它转换成下面这个，应该就容易解读多了。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/2180677adedf891763844cc92e33b949.png" data-original-src="https://miro.medium.com/v2/resize:fit:1096/format:webp/1*iiD3wPlMMWKE2MTW7yr7xA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">u_ij 的另一种看法(图片由作者提供)</p></figure><p id="5cae" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">假设<em class="nl"> m=2 </em>，那么分子部分就是第<em class="nl">个第</em>个数据点到第<em class="nl">个第</em>个聚类中心的<strong class="lk jd"> <em class="nl">距离</em> </strong>的倒数(上图中箭头的长度)。如前所述，较大的距离应对应较小的<strong class="lk jd"> <em class="nl"> μ_ij </em> </strong>，这也反映在这个等式中。等式的分母是从<em class="nl"> x_j </em>到每个聚类中心的距离的倒数之和。</p><h2 id="2717" class="mo mp it bd mq mr ms dn mt mu mv dp mw lr mx my mz lv na nb nc lz nd ne nf iz bi translated">该算法</h2><p id="1454" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">完整的 FCM 算法可以在下图中描述。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nu"><img src="../Images/079de77e49e18ad4c5fc9a7ad56b9cde.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZgtCK_u-tiOwQChOkIXoZA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">FCM(作者图片)算法</p></figure><p id="2a85" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">该过程实际上与<strong class="lk jd"> EM </strong>算法相同。如果你有兴趣了解 EM，可以去下面的帖子。</p><div class="nv nw gp gr nx ny"><a rel="noopener follow" target="_blank" href="/gaussian-mixture-models-with-python-36dabed6212a"><div class="nz ab fo"><div class="oa ab ob cl cj oc"><h2 class="bd jd gy z fp od fr fs oe fu fw jc bi translated">基于 Python 的高斯混合模型</h2><div class="of l"><h3 class="bd b gy z fp od fr fs oe fu fw dk translated">用 Python 实现高斯混合模型。在这篇文章中，我简要介绍了无监督学习方法的概念…</h3></div><div class="og l"><p class="bd b dl z fp od fr fs oe fu fw dk translated">towardsdatascience.com</p></div></div><div class="oh l"><div class="oi l oj ok ol oh om lb ny"/></div></div></a></div><h2 id="3847" class="mo mp it bd mq mr ms dn mt mu mv dp mw lr mx my mz lv na nb nc lz nd ne nf iz bi translated">实施 FCM</h2><p id="5a4b" class="pw-post-body-paragraph li lj it lk b ll ng kd ln lo nh kg lq lr ni lt lu lv nj lx ly lz nk mb mc md im bi translated">模糊 c 均值聚类在 Python 中的实现非常简单。安装程序如下所示，</p><pre class="ks kt ku kv gt on oo op oq aw or bi"><span id="c418" class="mo mp it oo b gy os ot l ou ov">import numpy as np<br/>from fcmeans import FCM</span><span id="593b" class="mo mp it oo b gy ow ot l ou ov">my_model = FCM(n_clusters=2) # we use two cluster as an example<br/>my_model.fit(X) ## X, numpy array. rows:samples columns:features</span></pre><p id="d3a0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">从聚类模型中提取信息，</p><pre class="ks kt ku kv gt on oo op oq aw or bi"><span id="7ca6" class="mo mp it oo b gy os ot l ou ov">centers = my_model.centers<br/>labels = my_model.predict(X)</span></pre><p id="1cfc" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">就是这样！</p><p id="0605" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">希望文章有用。</p><div class="nv nw gp gr nx ny"><a href="https://jianan-lin.medium.com/membership" rel="noopener follow" target="_blank"><div class="nz ab fo"><div class="oa ab ob cl cj oc"><h2 class="bd jd gy z fp od fr fs oe fu fw jc bi translated">用我的推荐链接-裕丰加入 Medium</h2><div class="of l"><h3 class="bd b gy z fp od fr fs oe fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="og l"><p class="bd b dl z fp od fr fs oe fu fw dk translated">jianan-lin.medium.com</p></div></div><div class="oh l"><div class="ox l oj ok ol oh om lb ny"/></div></div></a></div></div></div>    
</body>
</html>