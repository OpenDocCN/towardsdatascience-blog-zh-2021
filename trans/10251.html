<html>
<head>
<title>Hyperparameter Tuning with Grid Search and Random Search</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于网格搜索和随机搜索的超参数调谐</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hyperparameter-tuning-with-grid-search-and-random-search-6e1b5e175144?source=collection_archive---------5-----------------------#2021-09-29">https://towardsdatascience.com/hyperparameter-tuning-with-grid-search-and-random-search-6e1b5e175144?source=collection_archive---------5-----------------------#2021-09-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0b74" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">并深入探讨如何将它们结合起来</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/704410b33223f0983c0d5824d223d0a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PNtDZ2IUxgVeIrIm"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@joshstyle?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="541b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">超参数调整</strong>也称为超参数优化，是任何机器学习模型训练中的重要步骤，直接影响模型性能。</p><p id="9c48" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文涵盖了两种非常流行的超参数调优技术:<strong class="lb iu">网格搜索</strong>和<strong class="lb iu">随机搜索</strong>，并展示了如何将这两种算法与<strong class="lb iu">从粗到细的调优结合起来。到本文结束时，你将知道它们的工作原理和主要区别，这将帮助你自信地决定使用哪一种。</strong></p><p id="89e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在阅读本文的同时，我鼓励您查看我的GitHub 上的J <a class="ae ky" href="https://github.com/Idilismiguzel/Machine-Learning/blob/master/Hyperparameter_Tuning/Hyperparameter_Tuning_Grid_RandomSearch.ipynb" rel="noopener ugc nofollow" target="_blank"> upyter笔记本，以获得完整的分析和代码。🌠</a></p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="417c" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">背景</h1><p id="3b10" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated"><strong class="lb iu">超参数</strong>是在训练之前<strong class="lb iu">定义的参数</strong> <strong class="lb iu">，用于指定我们希望模型训练如何进行。<strong class="lb iu"> </strong>我们可以完全控制超参数设置，这样我们就可以控制学习过程。</strong></p><p id="ea6c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，在随机森林模型中<code class="fe mz na nb nc b">n_estimators</code>(我们想要的决策树数量)是一个超参数。它可以设置为任何整数值，但是当然，设置为10或1000会显著改变学习过程。</p><p id="aeee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">参数</strong>、<strong class="lb iu">、</strong>相反，<strong class="lb iu">、</strong>是在训练时发现的<strong class="lb iu">。我们无法控制参数值，因为它们是模型训练的结果。例如，在线性回归<em class="nd">中，系数</em>和<em class="nd">截距</em>是在模型训练结束时找到的参数。</strong></p><p id="a6af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了学习模型超参数和它们的值，我们可以简单地在Python中调用<code class="fe mz na nb nc b">get_params</code>。🔍</p><pre class="kj kk kl km gt ne nc nf ng aw nh bi"><span id="a7c4" class="ni md it nc b gy nj nk l nl nm">from sklearn.ensemble import RandomForestClassifier</span><span id="2799" class="ni md it nc b gy nn nk l nl nm"># Instantiate the model<br/>rf_model = RandomForestClassifier()</span><span id="e7ba" class="ni md it nc b gy nn nk l nl nm"># Print hyperparameters<br/>rf_model.get_params</span></pre><blockquote class="no np nq"><p id="2b35" class="kz la nd lb b lc ld ju le lf lg jx lh nr lj lk ll ns ln lo lp nt lr ls lt lu im bi translated">RandomForestClassifier(<strong class="lb iu">bootstrap</strong>= True，<strong class="lb iu">CCP _阿尔法</strong> =0.0，<strong class="lb iu"> class_weight </strong> =None，<strong class="lb iu"> criterion </strong> = '基尼'，<strong class="lb iu"> max_depth </strong> =None，<strong class="lb iu"> max_features </strong> = '自动'，<strong class="lb iu"> max_leaf_nodes </strong> =None，<strong class="lb iu"> max_samples </strong> =None，<strong class="lb iu">min _ infinity _ decrease【T11</strong></p></blockquote><p id="678d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，随机森林分类器有许多超参数，如果模型在没有定义超参数的情况下被实例化，那么它将有默认值。在随机森林中，默认情况下，n_estimators=100，这将产生一个中等大小的森林。(只有100棵树🌳)</p><p id="e27a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您需要知道，在任何模型中(在本例中为随机森林),一些超参数比其他超参数更重要，例如:</p><pre class="kj kk kl km gt ne nc nf ng aw nh bi"><span id="eae6" class="ni md it nc b gy nj nk l nl nm"><strong class="nc iu">n_estimators</strong>: Number of decision trees<br/><strong class="nc iu">max_features</strong>: Maximum number of features considered while splitting<br/><strong class="nc iu">max_depth</strong>: Max depth of the tree<br/><strong class="nc iu">min_samples_leaf</strong>: Minimum number of data points in a leaf node<br/><strong class="nc iu">bootstrap</strong>: Sampling with or without replacement</span></pre><p id="0914" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一些超参数不影响模型性能，例如:</p><pre class="kj kk kl km gt ne nc nf ng aw nh bi"><span id="f08c" class="ni md it nc b gy nj nk l nl nm"><strong class="nc iu">n_jobs</strong>: Number of jobs to run in parallel<br/><strong class="nc iu">random_state</strong>: Seed<br/><strong class="nc iu">verbose</strong>: Printing information while training continues<br/><strong class="nc iu">oob_score</strong>: Whether or not to use out-of-bag samples</span></pre><p id="f04d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，<strong class="lb iu">超参数调整</strong>是根据定义的评分标准，寻找能够提供最佳性能的最佳超参数组合。</p><h1 id="8985" class="mc md it bd me mf nu mh mi mj nv ml mm jz nw ka mo kc nx kd mq kf ny kg ms mt bi translated">数据和初始模型</h1><p id="4977" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">在本文中，我们将使用来自UCI 的<a class="ae ky" href="https://www.kaggle.com/uciml/glass" rel="noopener ugc nofollow" target="_blank">玻璃识别数据集，其中我们有9个属性来预测玻璃的类型(从7个离散值中)。</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nz"><img src="../Images/1cd7d08a3fd16f88dde0c353f231c496.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Dg783BUT9fvOx7WkMIifxQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">来自数据的5个随机行</p></figure><p id="2ed5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我将分离X和y，并生成训练集和测试集。</p><pre class="kj kk kl km gt ne nc nf ng aw nh bi"><span id="9090" class="ni md it nc b gy nj nk l nl nm"># Seperate X and y</span><span id="0634" class="ni md it nc b gy nn nk l nl nm">X = df.drop(columns=['Type'], axis=1)<br/>y = df['Type']</span><span id="7368" class="ni md it nc b gy nn nk l nl nm"># Generate training and test sets for X and y</span><span id="1d00" class="ni md it nc b gy nn nk l nl nm">X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=1)</span></pre><p id="aa02" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">之后，我们简单地运行一个带有默认值的随机森林分类器，并获得测试集的预测。</p><pre class="kj kk kl km gt ne nc nf ng aw nh bi"><span id="5fc7" class="ni md it nc b gy nj nk l nl nm"># Instantiate and fit random forest classifier</span><span id="0c8f" class="ni md it nc b gy nn nk l nl nm">rf_model = RandomForestClassifier()<br/>rf_model.fit(X_train, y_train)</span><span id="0418" class="ni md it nc b gy nn nk l nl nm"># Predict on the test set and call accuracy</span><span id="e535" class="ni md it nc b gy nn nk l nl nm">y_pred = rf_model.predict(X_test)<br/>accuracy = accuracy_score(y_test, y_pred)</span><span id="3c72" class="ni md it nc b gy nn nk l nl nm">print(accuracy)</span></pre><blockquote class="no np nq"><p id="f670" class="kz la nd lb b lc ld ju le lf lg jx lh nr lj lk ll ns ln lo lp nt lr ls lt lu im bi">0.81</p></blockquote><p id="33c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如你所见，默认模型的准确率为81%。现在，我们将了解如何使用网格搜索来优化选定的超参数。</p><h1 id="262b" class="mc md it bd me mf nu mh mi mj nv ml mm jz nw ka mo kc nx kd mq kf ny kg ms mt bi translated">网格搜索</h1><p id="ea73" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">网格搜索从定义搜索空间<strong class="lb iu">网格</strong>开始。网格由选定的超参数名称和值组成，并且<strong class="lb iu">网格搜索</strong>彻底搜索这些给定值的最佳组合。🚀</p><p id="280a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们决定定义以下参数网格来优化我们的随机森林分类器的一些超参数。</p><pre class="kj kk kl km gt ne nc nf ng aw nh bi"><span id="4ac5" class="ni md it nc b gy nj nk l nl nm"><strong class="nc iu">param_grid:</strong><br/>n_estimators = [50, 100, 200, 300]<br/>max_depth = [2, 4, 6, 8, 10]<br/>min_samples_leaf = [1, 5, 10]<br/>max_features = ['auto', 'sqrt']<br/>bootstrap = [True, False]</span></pre><p id="c05c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用Scikit-Learn库中的<code class="fe mz na nb nc b">GridSearchCV</code>类进行优化。首先要提到的是，网格搜索必须运行并比较240个模型(=4*5*3*2*2，所选数值的乘积)。</p><p id="e53a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，<code class="fe mz na nb nc b">GridSearchCV</code>类可以选择执行<a class="ae ky" href="https://scikit-learn.org/stable/modules/cross_validation.html" rel="noopener ugc nofollow" target="_blank">交叉验证</a>，将训练和测试数据重新采样到多个文件夹中。通过应用交叉验证，我们使用数据中的每条记录进行训练和测试，而不是在训练和测试时一次性拆分数据集。如果我们决定使用交叉验证(假设有5个折叠)，这意味着网格搜索必须评估1200 (=240*5)个模型性能。</p><p id="b783" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看<code class="fe mz na nb nc b">GridSearchCV</code>类的所有输入参数:</p><pre class="kj kk kl km gt ne nc nf ng aw nh bi"><span id="192b" class="ni md it nc b gy nj nk l nl nm"><em class="nd">class </em>sklearn.model_selection.<strong class="nc iu">GridSearchCV</strong>(<em class="nd">estimator</em>, <em class="nd">param_grid</em>, <em class="nd">scoring=None</em>, <em class="nd">n_jobs=None</em>, <em class="nd">refit=True</em>, <em class="nd">cv=None</em>, <em class="nd">return_train_score=False</em>)</span></pre><p id="66fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们首先为网格定义一个字典，我们将把它作为GridSeachCv的输入。</p><pre class="kj kk kl km gt ne nc nf ng aw nh bi"><span id="bfad" class="ni md it nc b gy nj nk l nl nm"># Define the grid</span><span id="e6d4" class="ni md it nc b gy nn nk l nl nm">param_grid = {<br/>'n_estimators': [50, 100, 200, 300],<br/>'min_samples_leaf': [1, 5, 10],<br/>'max_depth': [2, 4, 6, 8, 10],<br/>'max_features': ['auto', 'sqrt'],<br/>'bootstrap': [True, False]}</span><span id="b8fa" class="ni md it nc b gy nn nk l nl nm"># Instantiate GridSearchCV</span><span id="6f50" class="ni md it nc b gy nn nk l nl nm">model_gridsearch = GridSearchCV(<br/>estimator=rf_model,<br/>param_grid=param_grid,<br/>scoring='accuracy',<br/>n_jobs=4,<br/>cv=5,<br/>refit=True,<br/>return_train_score=True)</span></pre><p id="edfa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如前所述，<code class="fe mz na nb nc b">estimator</code>是RandomForestClassifier(RF _ model ),而<code class="fe mz na nb nc b">param_grid</code>是我们上面定义的参数网格。<code class="fe mz na nb nc b">scoring</code>是期望的评估指标，比如分类任务的准确性，而<code class="fe mz na nb nc b">n_jobs</code>并行执行模型评估，但是如果设置n_jobs=-1，就要小心了，它使用所有的处理器！由于网格搜索是一种不知情的调优，我们可以利用并行运行的模型，因为它们的结果不会影响其他模型的运行。</p><p id="f9f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">设置<code class="fe mz na nb nc b">refit=True</code>最终用找到的最佳超参数值重新拟合估计器，因此我们不需要在额外的步骤中对它们进行编码。<code class="fe mz na nb nc b">cv</code>定义交叉验证策略和设置<code class="fe mz na nb nc b">return_train_score=True</code>我们可以打印模型运行的日志以进行进一步分析。</p><pre class="kj kk kl km gt ne nc nf ng aw nh bi"><span id="f182" class="ni md it nc b gy nj nk l nl nm"># Record the current time <br/>start = time()</span><span id="e53d" class="ni md it nc b gy nn nk l nl nm"># Fit the selected model<br/>model_gridsearch.fit(X_train, y_train)</span><span id="437e" class="ni md it nc b gy nn nk l nl nm"># Print the time spend and number of models ran<br/>print("GridSearchCV took %.2f seconds for %d candidate parameter settings." % ((time() - start), len(model_gridsearch.cv_results_['params'])))</span></pre><blockquote class="no np nq"><p id="06f1" class="kz la nd lb b lc ld ju le lf lg jx lh nr lj lk ll ns ln lo lp nt lr ls lt lu im bi translated">GridSearchCV对240个候选参数设置耗时247.79秒。</p></blockquote><pre class="kj kk kl km gt ne nc nf ng aw nh bi"><span id="a3b8" class="ni md it nc b gy nj nk l nl nm"># Predict on the test set and call accuracy</span><span id="5772" class="ni md it nc b gy nn nk l nl nm">y_pred_grid = model_gridsearch.predict(X_test)<br/>accuracy_grid = accuracy_score(y_test, y_pred_grid)</span></pre><blockquote class="no np nq"><p id="0e42" class="kz la nd lb b lc ld ju le lf lg jx lh nr lj lk ll ns ln lo lp nt lr ls lt lu im bi">0.88</p></blockquote><p id="e4d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，简单地调整一些超参数将初始准确性从81%提高到88%,花费247秒来调整超参数。</p><p id="879c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">网格搜索总是找到网格中提到的具有超参数值的最佳执行模型。它也很容易实现和解释。然而，随着要测试的超参数和值数量的增加，它很容易变得计算昂贵，因为它对超参数的所有组合进行建模。不从已经运行的模型中学习的缺点使得网格搜索低效且耗时。此外，参数网格起着极其重要的作用:即使网格搜索总是能找到最佳组合，但如果参数网格选择不当，最佳组合的性能也不会很好。</p><p id="e57b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">运行GridSeachCV后，我们可以返回以下属性以供进一步研究:</p><ul class=""><li id="21e5" class="oa ob it lb b lc ld lf lg li oc lm od lq oe lu of og oh oi bi translated"><strong class="lb iu"> cv_results_ </strong></li><li id="9d74" class="oa ob it lb b lc oj lf ok li ol lm om lq on lu of og oh oi bi translated"><strong class="lb iu">最佳估算者_ </strong></li><li id="c334" class="oa ob it lb b lc oj lf ok li ol lm om lq on lu of og oh oi bi translated"><strong class="lb iu">最好成绩_ </strong></li><li id="b037" class="oa ob it lb b lc oj lf ok li ol lm om lq on lu of og oh oi bi translated"><strong class="lb iu"> best_params_ </strong></li></ul><p id="3c96" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们来看看其中的一些:</p><pre class="kj kk kl km gt ne nc nf ng aw nh bi"><span id="a489" class="ni md it nc b gy nj nk l nl nm">print(model_gridsearch.best_params_)</span></pre><blockquote class="no np nq"><p id="4a81" class="kz la nd lb b lc ld ju le lf lg jx lh nr lj lk ll ns ln lo lp nt lr ls lt lu im bi translated">{'bootstrap': True，' max_depth': 10，' max_features': 'sqrt '，' min_samples_leaf': 1，' n_estimators': 300}</p></blockquote><pre class="kj kk kl km gt ne nc nf ng aw nh bi"><span id="7901" class="ni md it nc b gy nj nk l nl nm">print(model_gridsearch.best_estimator_)</span></pre><blockquote class="no np nq"><p id="68fa" class="kz la nd lb b lc ld ju le lf lg jx lh nr lj lk ll ns ln lo lp nt lr ls lt lu im bi translated">RandomForestClassifier(bootstrap = True，CCP _阿尔法=0.0，class_weight=None，criterion= '基尼'，max_depth=10，max_features='sqrt '，max_leaf_nodes=None，max_samples=None，min _ infinity _ decrease = 0.0，min_samples_leaf=1，min_samples_split=2，min_weight_fraction_leaf=0.0，n_estimators=300，n_jobs=None，oob_score=False，random</p></blockquote><h1 id="3d41" class="mc md it bd me mf nu mh mi mj nv ml mm jz nw ka mo kc nx kd mq kf ny kg ms mt bi translated">随机搜索</h1><p id="53cf" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">在随机搜索中，我们为每个超参数定义<strong class="lb iu">分布</strong>，超参数可以统一定义<em class="nd"/>或使用<em class="nd">采样方法</em>。与网格搜索的关键区别在于随机搜索，不是所有的值都被测试，测试的值是随机选择的。</p><p id="710c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，如果分布中有500个值，如果我们输入<code class="fe mz na nb nc b">n_iter=50</code>，那么随机搜索将随机抽取50个值进行测试。通过这样做，随机搜索优化了时间花费，并且不定义绝对网格允许它探索给定分布中的其他值。</p><p id="04b2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于随机搜索并不尝试每个超参数组合，它不一定返回最佳性能值，但它会在<em class="nd">显著</em>更短的时间内返回相对较好的性能模型。⏰</p><pre class="kj kk kl km gt ne nc nf ng aw nh bi"><span id="b52d" class="ni md it nc b gy nj nk l nl nm"><strong class="nc iu">param_distributions</strong><br/>n_estimators = list(range(100, 300, 10))<br/>min_samples_leaf = list(range(1, 50))<br/>max_depth = list(range(2, 20)<br/>max_features = ['auto', 'sqrt']<br/>bootstrap = [True, False]</span></pre><p id="0056" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe mz na nb nc b">RandomizedSearchCV</code>从Scikit-Learn有以下输入参数:</p><pre class="kj kk kl km gt ne nc nf ng aw nh bi"><span id="f4ca" class="ni md it nc b gy nj nk l nl nm"><em class="nd">class </em>sklearn.model_selection.<strong class="nc iu">RandomizedSearchCV</strong>(<em class="nd">estimator</em>, <em class="nd">param_distributions</em>, <em class="nd">n_iter=10</em>, <em class="nd">scoring=None</em>, <em class="nd">n_jobs=None</em>, <em class="nd">refit=True</em>, <em class="nd">cv=None</em>, <em class="nd">verbose=0</em>, <em class="nd">pre_dispatch='2*n_jobs'</em>, <em class="nd">random_state=None</em>, <em class="nd">error_score=nan</em>, <em class="nd">return_train_score=False</em>)</span></pre><p id="3999" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们首先为参数分布定义一个字典，它将成为RandomizedSearchCV的输入。</p><pre class="kj kk kl km gt ne nc nf ng aw nh bi"><span id="1dec" class="ni md it nc b gy nj nk l nl nm"># specify distributions to sample from</span><span id="3b51" class="ni md it nc b gy nn nk l nl nm">param_dist = {<br/>'n_estimators': list(range(50, 300, 10)),<br/>'min_samples_leaf': list(range(1, 50)),<br/>'max_depth': list(range(2, 20)),<br/>'max_features': ['auto', 'sqrt'],<br/>'bootstrap': [True, False]}</span><span id="03e9" class="ni md it nc b gy nn nk l nl nm"># specify number of search iterations</span><span id="d1bb" class="ni md it nc b gy nn nk l nl nm">n_iter = 50</span><span id="bd07" class="ni md it nc b gy nn nk l nl nm"># Instantiate RandomSearchCV</span><span id="6aad" class="ni md it nc b gy nn nk l nl nm">model_random_search = RandomizedSearchCV(<br/>estimator=rf_model,<br/>param_distributions=param_dist,<br/>n_iter=n_iter)</span></pre><p id="0eb8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">大多数参数与GridSearchCV相似，然而，在RandomizedSearchCV中，我们有<code class="fe mz na nb nc b">param_distributions</code>来定义搜索分布。<code class="fe mz na nb nc b">n_iter</code>用于限制模型运行的总数(换句话说，从网格中取样的参数组合)。这里要小心权衡，因为设置<strong class="lb iu"> <em class="nd">高n_iter </em> </strong>增加搜索运行时间，设置<strong class="lb iu"> <em class="nd">低n_iter </em> </strong>降低模型质量。</p><pre class="kj kk kl km gt ne nc nf ng aw nh bi"><span id="339f" class="ni md it nc b gy nj nk l nl nm"># Record the current time <br/>start = time()</span><span id="b43e" class="ni md it nc b gy nn nk l nl nm"># Fit the selected model<br/>model_random_search.fit(X_train, y_train)</span><span id="0dbf" class="ni md it nc b gy nn nk l nl nm"># Print the time spend and number of models ran<br/>print("RandomizedSearchCV took %.2f seconds for %d candidate parameter settings." % ((time() - start), len(model_random_search.cv_results_['params'])))</span></pre><blockquote class="no np nq"><p id="53f5" class="kz la nd lb b lc ld ju le lf lg jx lh nr lj lk ll ns ln lo lp nt lr ls lt lu im bi translated">RandomizedSearchCV对50个候选参数设置耗时64.17秒。</p></blockquote><pre class="kj kk kl km gt ne nc nf ng aw nh bi"><span id="3adf" class="ni md it nc b gy nj nk l nl nm"># Predict on the test set and call accuracy</span><span id="6a32" class="ni md it nc b gy nn nk l nl nm">y_pred_random = model_random_search.predict(X_test)<br/>accuracy_random = accuracy_score(y_test, y_pred_random)</span></pre><blockquote class="no np nq"><p id="9970" class="kz la nd lb b lc ld ju le lf lg jx lh nr lj lk ll ns ln lo lp nt lr ls lt lu im bi">0.86</p></blockquote><p id="b268" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如你所看到的，仅在64秒内，我们就能将初始模型的准确率从81%提高到86%。随机搜索没有达到网格搜索88%的准确率，然而，这是两种调优方法之间的权衡。</p><p id="79cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，让我们看看用随机搜索找到的最佳参数。</p><pre class="kj kk kl km gt ne nc nf ng aw nh bi"><span id="9d8e" class="ni md it nc b gy nj nk l nl nm">print(model_random_search.best_params_)</span></pre><blockquote class="no np nq"><p id="1cac" class="kz la nd lb b lc ld ju le lf lg jx lh nr lj lk ll ns ln lo lp nt lr ls lt lu im bi translated">{'n_estimators': 230，' min_samples_leaf': 4，' max_features': 'auto '，' max_depth': 13，' bootstrap': False}</p></blockquote><h1 id="4f02" class="mc md it bd me mf nu mh mi mj nv ml mm jz nw ka mo kc nx kd mq kf ny kg ms mt bi translated">粗调至微调</h1><p id="4c9a" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">从文章开始，我们已经看到了如何应用网格搜索和随机搜索进行超参数优化。</p><p id="e155" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用网格搜索，我们能够测试网格中给定的所有超参数值，从中找出最佳值。然而，超参数数量的增加很容易成为瓶颈。理想情况下，我们可以结合网格搜索和随机搜索来防止这种低效率。</p><p id="1f8a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在从粗到细的调整中，我们从随机搜索开始，为每个超参数找到有希望的值范围。例如，如果随机搜索在150到200之间返回高性能，这是我们希望网格搜索关注的范围。使用随机搜索获得每个超参数的焦点区域后，我们可以相应地定义网格进行网格搜索，以找到其中的<strong class="lb iu">最佳</strong>值。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/6a030c15947b7bf01d85dba0891b174a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OM-G422s32hcu9SyYDYtww.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="22dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上图中，我们首先随机测试值，通过随机搜索找到一个焦点区域。其次，我们正在用网格搜索测试这个重点区域中的所有值，并最终找到最佳值。</p><h1 id="101a" class="mc md it bd me mf nu mh mi mj nv ml mm jz nw ka mo kc nx kd mq kf ny kg ms mt bi translated">奖金</h1><p id="c5c5" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">另一个微调机器学习模型的有用策略是使用<a class="ae ky" rel="noopener" target="_blank" href="/practical-guide-to-ensemble-learning-d34c74e022a0"> <strong class="lb iu">集成学习</strong> </a>技术。与超参数调整不同，集成学习旨在通过将多个模型组合成一个组模型来提高模型性能。这种群体模式旨在比每个单独的模式表现得更好。最常用的集成学习方法是<strong class="lb iu">投票、bagging、boosting </strong>和<strong class="lb iu">堆叠</strong>，如果你想了解更多或更新你的知识，你可以<a class="ae ky" rel="noopener" target="_blank" href="/practical-guide-to-ensemble-learning-d34c74e022a0">阅读我关于这个主题的文章</a>。🐬</p><h1 id="12c7" class="mc md it bd me mf nu mh mi mj nv ml mm jz nw ka mo kc nx kd mq kf ny kg ms mt bi translated">结论</h1><p id="76d2" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">在本文中，我们使用了一个随机森林分类器，使用9种不同的属性来预测“玻璃的类型”。具有默认超参数值的初始随机森林分类器在测试中达到81%的准确度。使用网格搜索，我们能够在247秒内调整选定的超参数，并将准确性提高到88%。接下来，我们使用随机搜索做了同样的工作，在64秒内，我们将准确率提高到了86%。最后但同样重要的是，我们讨论了从粗到细的调优，以结合这两种方法并从两者中获得优势！</p><p id="4d10" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我希望您喜欢阅读关于超参数调优的文章，并发现这篇文章对您的分析有用！</p><p id="443e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="nd">如果你喜欢这篇文章，你可以</em> <strong class="lb iu"> <em class="nd"> </em> </strong> <a class="ae ky" href="https://medium.com/@idilismiguzel" rel="noopener"> <strong class="lb iu"> <em class="nd">在这里阅读我的其他文章</em></strong></a><strong class="lb iu"><em class="nd"/></strong><em class="nd">和</em> <a class="ae ky" href="http://medium.com/@idilismiguzel/follow" rel="noopener"> <strong class="lb iu"> <em class="nd">关注我上媒</em></strong></a><strong class="lb iu"><em class="nd"/></strong>如果有任何问题或建议，请告诉我。✨</p><p id="ac37" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">喜欢这篇文章吗？ <a class="ae ky" href="https://idilismiguzel.medium.com/membership" rel="noopener"> <strong class="lb iu">成为会员求更！</strong> </a></p></div></div>    
</body>
</html>