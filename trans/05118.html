<html>
<head>
<title>VarifocalNet (VF-Net): New State of the Art Object Detection Network</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">VarifocalNet (VF-Net):最新的物体探测网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/varifocalnet-vf-net-new-state-of-the-art-object-detection-network-a1d54e0f7c1e?source=collection_archive---------17-----------------------#2021-05-05">https://towardsdatascience.com/varifocalnet-vf-net-new-state-of-the-art-object-detection-network-a1d54e0f7c1e?source=collection_archive---------17-----------------------#2021-05-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="51f6" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">引入IoU感知和变焦损失，提高SOTA物体检测分数</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/945ca2e66eea07140fcc653ccf6de273.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*8-PbAKMQp0KRgskl"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@sstoppo?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">阳光摄影</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><blockquote class="kz la lb"><p id="88f4" class="lc ld le lf b lg lh ju li lj lk jx ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated"><em class="it">在COCO女士上的大量实验表明，我们的VFNet在不同主干上持续超越强基线</em>∩<em class="it">2.0 AP。我们最好的型号VFNet-X-1200和Res2Net-101-DCN在COCO test-dev上实现了55.1的单模型单尺度AP，这是各种对象检测器中最先进的。代码可在:</em><a class="ae ky" href="https://github.com/hyz-xmaster/VarifocalNet." rel="noopener ugc nofollow" target="_blank"><em class="it">https://github.com/hyz-xmaster/VarifocalNet.</em></a>获得</p></blockquote><p id="d243" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">来源:<a class="ae ky" href="https://arxiv.org/abs/2008.13367" rel="noopener ugc nofollow" target="_blank"> VarifocalNet </a></p><p id="ec4f" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">几周前，当我在做一个物体探测Kaggle比赛时，我偶然发现了VarifocalNet。我很惊讶地看到它匹配许多SOTA对象检测模型，如YoloV5和EfficientDet，在某些情况下甚至超过他们。我亲自检查了这篇论文，我非常喜欢它。它引入了许多我感兴趣的新概念，比如变焦距损耗、IoU感知分类分数等等。</p><p id="a276" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">该论文着重于精确地挑选出所产生的包围盒、优化它们的精度以及过滤它们的挑战，这是对象检测的本质。在一个小区域内存在多个对象的<strong class="lf iu">密集对象检测</strong>任务中，对象检测模型通常表现不佳。在这些情况下，挑选出正确的边界框变得困难，并且简单的损失函数(例如交集/并集)通常表现不佳(因为框重叠太多，即使它们是正确的)。</p><p id="5094" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">VarifocalNet使用变焦距损失来预测每幅图像的IoU感知分类得分。变焦损失是由聚焦损失引起的。如果你对此感兴趣，那就继续读下去吧！</p><h2 id="a881" class="mc md it bd me mf mg dn mh mi mj dp mk lz ml mm mn ma mo mp mq mb mr ms mt mu bi translated"><strong class="ak">变焦损失介绍:</strong></h2><p id="3a89" class="pw-post-body-paragraph lc ld it lf b lg mv ju li lj mw jx ll lz mx lo lp ma my ls lt mb mz lw lx ly im bi translated">在我们开始解释变焦距损失之前，我们需要看一下它的前身，焦距损失。焦点损失是经典交叉熵损失的升级。它试图通过向困难的数据点分配更多的权重来优化类别不平衡问题。你可能会想，是什么带来了阶级不平衡的问题。典型的对象检测网络评估每个图像的非常大量的候选框位置，但是这些候选框位置中只有一小部分实际上包含有效对象，这模拟了类别不平衡问题。</p><p id="5e71" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">焦点损失本质上是在交叉熵中引入了一个“调制因子”,它减少了损失贡献[1]简单的例子，并增加了假阴性/阳性的重要性(这使得网络更加精确)。</p><p id="b186" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">这里要注意的一件重要事情是，在典型的对象检测任务中，正样本比负样本少得多(特别是在密集对象检测任务中)。在这里停下来想一想如何利用这个提示来改善聚焦损失可能是值得的。</p><p id="806d" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">变焦距损失不对称地对待正例和负例[1],不同于焦距损失平等地对待它们。这意味着正面例子的损失减少量与负面例子的减少量不同。</p><blockquote class="kz la lb"><p id="f69c" class="lc ld le lf b lg lh ju li lj lk jx ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">我们用训练目标q来衡量正面例子。如果正面例子具有高gt IoU，则其对损失的贡献将因此相对较大。这就把训练的重点放在了那些高质量的正面例子上，这些例子对于获得更高的AP比那些低质量的例子更重要。</p><p id="43fb" class="lc ld le lf b lg lh ju li lj lk jx ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">为了平衡正例和负例之间的损耗，我们在负损耗项中增加了一个可调的比例因子α。</p></blockquote><p id="6d0a" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">来源:<a class="ae ky" href="https://arxiv.org/abs/2008.13367" rel="noopener ugc nofollow" target="_blank"> VarifocalNet </a></p><p id="6207" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">我总是喜欢强调我所解释的任何新模型的不同之处。我认为损失函数可能是ML模型中最重要的事情之一(如果不是最重要的话)。因此，了解新模型中损失函数的变化非常重要。</p><h2 id="8fc3" class="mc md it bd me mf mg dn mh mi mj dp mk lz ml mm mn ma mo mp mq mb mr ms mt mu bi translated">IACS和星形盒要素制图表达</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/7bf8b22c697bf32e14d2bdf2f6d904b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4ARYckBscG1kwJtW"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@mbaumi?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">米卡·鲍梅斯特</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="9184" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">在我们解释VFNet为每个图像预测的新分数之前，我们必须探索FCOS+ATSS(具有自适应训练样本选择的全卷积单级)架构，因为这是VFNet所基于的。许多对象检测网络是基于锚点的，这意味着预测的框依赖于平铺在图像上的预设锚点。然而，FCOS试图摆脱锚，提供无锚网络(不需要IoU匹配)，无提议(使检测只发生在一个阶段)，并最终只使用卷积(使它更简单)。</p><blockquote class="kz la lb"><p id="8de2" class="lc ld le lf b lg lh ju li lj lk jx ll lm ln lo lp lq lr ls lt lu lv lw lx ly im bi translated">自适应训练样本选择，或ATSS，是一种根据对象的统计特征自动选择正样本和负样本的方法。它弥补了基于锚和无锚检测器之间的差距。</p></blockquote><p id="d5ba" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">来源:<a class="ae ky" href="https://arxiv.org/abs/1912.02424" rel="noopener ugc nofollow" target="_blank"> Arxiv </a></p><p id="ba48" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">如果你有兴趣了解更多关于FCOS的信息，我建议你看看这篇文章:</p><div class="nb nc gp gr nd ne"><a href="https://medium.com/swlh/fcos-walkthrough-the-fully-convolutional-approach-to-object-detection-777f614268c" rel="noopener follow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">FCOS演练:物体探测的完全卷积方法</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">无锚目标检测</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">medium.com</p></div></div><div class="nn l"><div class="no l np nq nr nn ns ks ne"/></div></div></a></div><p id="e1ab" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">我知道我们很快引入了很多概念，但是请耐心听我说。新SOTA模型的美妙之处在于，它们几乎总是建立在几种新颖技术的基础上，了解每种技术以及它们是如何组合成一个单一模型的，这是优秀的数据科学家与众不同的地方(当然是在我看来)。这篇文章不足以深入每个概念，所以我将尽力简要地解释它们，如果您需要，只需查看整篇文章中列出的附加资源来增强您的理解。</p><p id="1dad" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">FCOS网络预测每个图像的分类分数(除了边界框坐标之外)。VFNet的作者发现，用预测的边界框和地面真实值之间的IoU[1](gt _ IoU)替换这个分类分数可以大大提高性能(COCO上的74.7 AP vs 56.1 AP)。这主要是因为高质量对象检测的主要秘密是从大量的预测框中选择正确的边界框，而gt_IoU在这方面比传统的分类得分更有帮助。</p><p id="e349" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">作者还设计了一个更不同的边界框，一个更符合IACS分数的星形框。这个星形盒子有9个固定的采样点[1]，可以更准确地捕捉上下文信息。该星形框还允许在最终预测之前进行更加有效和准确的边界框细化阶段。VFNet还在最后的边界框细化阶段使用NMS(非最大抑制)来进一步消除冗余框。</p><p id="d133" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">最后要注意的一点是，我喜欢的是，在他们的结果部分，他们有一个名为“单个组件贡献”的部分。本质上，他们把他们引入的每个组件与他们的标准组件互换，并显示结果的差异。这使我们能够看到每个组件对性能改进的贡献有多大。</p><p id="2c82" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated"><strong class="lf iu">结论</strong></p><p id="cacd" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">请随意检查论文的最终实验和结果，显示它如何优于其他SOTA模型。我觉得在这里复制和粘贴一个巨大的结果表没有意义，我对网络如何工作更感兴趣。我希望您现在对整个网络的工作原理有了一个更全面的了解。总而言之，VFNet建立在3个主要组件之上。第一个是FPN(特征提议网络)，第二个是主干CNN，第三个是VFNet头(使用上面讨论的变焦损失和IACS)。如果您对编写VFNet比深入理论部分更感兴趣，我建议查看我的关于在自定义数据集上实现VFNet的MMdetection文章:</p><div class="nb nc gp gr nd ne"><a rel="noopener follow" target="_blank" href="/mmdetection-tutorial-an-end2end-state-of-the-art-object-detection-library-59064deeada3"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">MMDetection教程——最先进的对象检测库</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">MMDetection为使用许多SOTA物体检测模型提供了巨大的潜力，例如FasterRCNN，DETR，VFNet…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">towardsdatascience.com</p></div></div><div class="nn l"><div class="nt l np nq nr nn ns ks ne"/></div></div></a></div><h1 id="b0ab" class="nu md it bd me nv nw nx mh ny nz oa mk jz ob ka mn kc oc kd mq kf od kg mt oe bi translated">谢谢你</h1><p id="d6ea" class="pw-post-body-paragraph lc ld it lf b lg mv ju li lj mw jx ll lz mx lo lp ma my ls lt mb mz lw lx ly im bi translated">感谢您阅读本文:)如果您喜欢这篇文章，请考虑在这里给我买杯咖啡(咖啡有助于我写作):</p><div class="nb nc gp gr nd ne"><a href="https://www.buymeacoffee.com/mostafaibrahim" rel="noopener  ugc nofollow" target="_blank"><div class="nf ab fo"><div class="ng ab nh cl cj ni"><h2 class="bd iu gy z fp nj fr fs nk fu fw is bi translated">mostafaibrahim</h2><div class="nl l"><h3 class="bd b gy z fp nj fr fs nk fu fw dk translated">嘿，你好👋谢谢你考虑给我买一杯咖啡，☕️，说清楚一点，你的钱不会帮我付账单，我…</h3></div><div class="nm l"><p class="bd b dl z fp nj fr fs nk fu fw dk translated">www.buymeacoffee.com</p></div></div><div class="nn l"><div class="of l np nq nr nn ns ks ne"/></div></div></a></div><p id="ae91" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">如果你想定期收到关于人工智能和机器学习的最新论文的评论，请在这里添加你的电子邮件并订阅！</p><p id="6f30" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated"><a class="ae ky" href="https://artisanal-motivator-8249.ck.page/5524b8f934" rel="noopener ugc nofollow" target="_blank">https://artisanal-motivator-8249.ck.page/5524b8f934</a></p><p id="f66f" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated"><strong class="lf iu">参考文献:</strong></p><p id="7ece" class="pw-post-body-paragraph lc ld it lf b lg lh ju li lj lk jx ll lz ln lo lp ma lr ls lt mb lv lw lx ly im bi translated">[1] <a class="ae ky" href="https://arxiv.org/abs/2008.13367" rel="noopener ugc nofollow" target="_blank">变焦镜头</a></p></div></div>    
</body>
</html>