<html>
<head>
<title>Deep Learning Text Corrector using Monotonic Attention (with dataset creation)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用单调注意力的深度学习文本校正器(带有数据集创建)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-text-corrector-using-monotonic-attention-with-dataset-creation-1e1a3f5a1b9e?source=collection_archive---------14-----------------------#2021-03-28">https://towardsdatascience.com/deep-text-corrector-using-monotonic-attention-with-dataset-creation-1e1a3f5a1b9e?source=collection_archive---------14-----------------------#2021-03-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h2 id="6706" class="ju jv iq bd jw jx jy dn jz ka kb dp kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">深度学习模型从语法上纠正句子。使用单调注意的Seq2Seq模型。</h2></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi kq"><img src="../Images/46068eac2d2dc03f8e914d9cd0b7a8bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pwnVXFO3Nk7Yl3VaFbklIQ.jpeg"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated">图片<a class="ae lg" href="https://www.123rf.com/photo_84772415_english-sentences-with-red-pen-for-proofreading.html" rel="noopener ugc nofollow" target="_blank">演职员表</a></p></figure><h1 id="541b" class="lh jv iq bd jw li lj lk jz ll lm ln kc lo lp lq kg lr ls lt kk lu lv lw ko lx bi translated">目录</h1><ol class=""><li id="9e1d" class="ly lz iq ma b mb mc md me kd mf kh mg kl mh mi mj mk ml mm bi translated"><strong class="ma ir">简介</strong></li><li id="4d6b" class="ly lz iq ma b mb mn md mo kd mp kh mq kl mr mi mj mk ml mm bi translated"><strong class="ma ir">数据集创建</strong></li><li id="b83e" class="ly lz iq ma b mb mn md mo kd mp kh mq kl mr mi mj mk ml mm bi translated"><strong class="ma ir">注意机制</strong></li><li id="7f44" class="ly lz iq ma b mb mn md mo kd mp kh mq kl mr mi mj mk ml mm bi translated"><strong class="ma ir">单调注意</strong></li><li id="6408" class="ly lz iq ma b mb mn md mo kd mp kh mq kl mr mi mj mk ml mm bi translated"><strong class="ma ir">实现和代码演练</strong></li><li id="c509" class="ly lz iq ma b mb mn md mo kd mp kh mq kl mr mi mj mk ml mm bi translated"><strong class="ma ir">推理→波束搜索v/s贪婪搜索</strong></li><li id="af59" class="ly lz iq ma b mb mn md mo kd mp kh mq kl mr mi mj mk ml mm bi translated"><strong class="ma ir">结果和误差分析</strong></li><li id="82ef" class="ly lz iq ma b mb mn md mo kd mp kh mq kl mr mi mj mk ml mm bi translated"><strong class="ma ir">端到端管道</strong></li></ol></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="4f73" class="lh jv iq bd jw li ms lk jz ll mt ln kc lo mu lq kg lr mv lt kk lu mw lw ko lx bi translated">介绍</h1><p id="0a20" class="pw-post-body-paragraph mx my iq ma b mb mc mz na md me nb nc kd nd ne nf kh ng nh ni kl nj nk nl mi ij bi translated">序列对序列模型被证明是人工智能在自然语言处理领域的最佳应用之一。注意力机制通过模仿人类理解句子的方式这一简单概念，大大改进了seq2seq模型。</p><blockquote class="nm"><p id="759a" class="nn no iq bd np nq nr ns nt nu nv mi dk translated">在这篇博客中，我建立了一个机器学习模型，它使用单音注意力纠正句子中的基本语法错误。</p></blockquote><p id="fafe" class="pw-post-body-paragraph mx my iq ma b mb nw mz na md nx nb nc kd ny ne nf kh nz nh ni kl oa nk nl mi ij bi translated"><strong class="ma ir">我的模型解决的一些扰动是——</strong></p><ol class=""><li id="4450" class="ly lz iq ma b mb ob md oc kd od kh oe kl of mi mj mk ml mm bi translated">纠正使用限定词(a，an，the)的错误。</li><li id="739d" class="ly lz iq ma b mb mn md mo kd mp kh mq kl mr mi mj mk ml mm bi translated">删除从句——“that”。</li><li id="c19b" class="ly lz iq ma b mb mn md mo kd mp kh mq kl mr mi mj mk ml mm bi translated">替换单词modal(“可能”→“将”)</li><li id="e09b" class="ly lz iq ma b mb mn md mo kd mp kh mq kl mr mi mj mk ml mm bi translated">删除动词形式(“is”、“are”、“was”、“was”)</li><li id="ce16" class="ly lz iq ma b mb mn md mo kd mp kh mq kl mr mi mj mk ml mm bi translated">将“than”替换为“then”，反之亦然。</li><li id="c4bf" class="ly lz iq ma b mb mn md mo kd mp kh mq kl mr mi mj mk ml mm bi translated">用“他们”代替“他/她”。</li></ol></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="e592" class="lh jv iq bd jw li ms lk jz ll mt ln kc lo mu lq kg lr mv lt kk lu mw lw ko lx bi translated">为此案例研究创建数据集</h1><p id="d3fd" class="pw-post-body-paragraph mx my iq ma b mb mc mz na md me nb nc kd nd ne nf kh ng nh ni kl nj nk nl mi ij bi translated">为了这个项目，连同电影语料库数据集(<a class="ae lg" href="https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html" rel="noopener ugc nofollow" target="_blank">康奈尔电影对话语料库</a>)，我从头开始创建了一个数据集。我用哈利波特小说作为我的原始资料。使用正则表达式完成了文本预处理。查看完数据中的差异后，这些是我进行的预处理步骤。其中一些是- </p><ol class=""><li id="2b1d" class="ly lz iq ma b mb ob md oc kd od kh oe kl of mi mj mk ml mm bi translated">从Mr和Mrs中删除点号- 这一步是确保段落被正确分割成句子所必需的。(第4点)</li></ol><pre class="kr ks kt ku gt og oh oi oj aw ok bi"><span id="3360" class="ju jv iq oh b gy ol om l on oo">xx_v2 = re.sub(r'Mr\.', 'Mr', str(xx), flags=re.IGNORECASE)</span><span id="dc83" class="ju jv iq oh b gy op om l on oo">xx_v2 = re.sub(r'Mrs\.', 'Mrs', xx_v2, flags=re.IGNORECASE)</span></pre><p id="1cc9" class="pw-post-body-paragraph mx my iq ma b mb ob mz na md oc nb nc kd oq ne nf kh or nh ni kl os nk nl mi ij bi translated"><strong class="ma ir"> 2。删除一些特殊字符- </strong>当这本书从pdf转换成txt时，一些特殊符号变成了文本代码，如—</p><p id="66f8" class="pw-post-body-paragraph mx my iq ma b mb ob mz na md oc nb nc kd oq ne nf kh or nh ni kl os nk nl mi ij bi translated">\ xe2 \ X80 \ x94 →""<br/>\ xe2 \ X80 \ x99→" ' "<br/>\ xe2 \ X80 \ xa6→"…"</p><p id="faf3" class="pw-post-body-paragraph mx my iq ma b mb ob mz na md oc nb nc kd oq ne nf kh or nh ni kl os nk nl mi ij bi translated">因此，使用正则表达式检测并删除了这些内容。</p><pre class="kr ks kt ku gt og oh oi oj aw ok bi"><span id="9c1c" class="ju jv iq oh b gy ol om l on oo">xx_v3 = re.sub(r"\\xe2\\x80\\x94|\\xe2\\x80\\xa6|\\xe2\\x80\\x98|\\n|\\t",  " ", xx_v2, flags=re.IGNORECASE)</span></pre><p id="8ef2" class="pw-post-body-paragraph mx my iq ma b mb ob mz na md oc nb nc kd oq ne nf kh or nh ni kl os nk nl mi ij bi translated"><strong class="ma ir"> 3。</strong> <strong class="ma ir">修复资料- </strong>一部小说包含了大量不同人物所说的引用文字。在将pdf转换为文本时，它们也被转换为一些文本代码。然后这些被检测到并被替换为其原始形式，即<br/>\ \ xe2 \ X80 \ x9c→"<br/>\ \ xe2 \ X80 \ x9d→"</p><pre class="kr ks kt ku gt og oh oi oj aw ok bi"><span id="bb1c" class="ju jv iq oh b gy ol om l on oo">xx_v4 = re.sub(r"\\xe2\\x80\\x9c", '"', xx_v3, flags=re.IGNORECASE)</span><span id="2bb5" class="ju jv iq oh b gy op om l on oo">xx_v4 = re.sub(r"\\xe2\\x80\\x9d", '"', xx_v4, flags=re.IGNORECASE)</span></pre><p id="c4b8" class="pw-post-body-paragraph mx my iq ma b mb ob mz na md oc nb nc kd oq ne nf kh or nh ni kl os nk nl mi ij bi translated"><strong class="ma ir"> 4。在句号上拆分数据- </strong>将数据转换成句子。</p><pre class="kr ks kt ku gt og oh oi oj aw ok bi"><span id="e777" class="ju jv iq oh b gy ol om l on oo">dat_v4 = xx_v4.split(".")</span></pre><p id="38c5" class="pw-post-body-paragraph mx my iq ma b mb ob mz na md oc nb nc kd oq ne nf kh or nh ni kl os nk nl mi ij bi translated"><strong class="ma ir"> 5。最后一步——提取句子</strong></p><p id="b9f8" class="pw-post-body-paragraph mx my iq ma b mb ob mz na md oc nb nc kd oq ne nf kh or nh ni kl os nk nl mi ij bi translated">→在一部小说中，有很多这样的例子，当一个角色说了一些写在引用的文本中的话，然后是这句话“这个角色说的”。<br/> <strong class="ma ir">例如→“我要走了”哈利说。这种类型的句子需要删除，因为它们是半句。</strong></p><p id="594b" class="pw-post-body-paragraph mx my iq ma b mb ob mz na md oc nb nc kd oq ne nf kh or nh ni kl os nk nl mi ij bi translated">因此，我们通过识别诸如“说”、“小声说”、“问”等词来删除这些半句***</p><p id="81c9" class="pw-post-body-paragraph mx my iq ma b mb ob mz na md oc nb nc kd oq ne nf kh or nh ni kl os nk nl mi ij bi translated">→最后，同样在这之后，有些<strong class="ma ir">句子是疑问句</strong>。所以，我们再次用<strong class="ma ir">分割。拆分("？)</strong>但是保留问号，以确保<strong class="ma ir">我们的模型理解肯定句和疑问句之间的区别。</strong></p><p id="274e" class="pw-post-body-paragraph mx my iq ma b mb ob mz na md oc nb nc kd oq ne nf kh or nh ni kl os nk nl mi ij bi translated"><strong class="ma ir">* * *注意-同样，如果报价中有完整的句子，它们在步骤4中已经被正确拆分。</strong></p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="ot ou l"/></div><p class="lc ld gj gh gi le lf bd b be z dk translated">不</p></figure><blockquote class="ov ow ox"><p id="c7f8" class="mx my oy ma b mb ob mz na md oc nb nc oz oq ne nf pa or nh ni pb os nk nl mi ij bi translated"><strong class="ma ir">我们也对电影语料库数据集进行类似的预处理。</strong></p></blockquote></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="0e99" class="lh jv iq bd jw li ms lk jz ll mt ln kc lo mu lq kg lr mv lt kk lu mw lw ko lx bi translated">注意机制概述</h1><p id="202a" class="pw-post-body-paragraph mx my iq ma b mb mc mz na md me nb nc kd nd ne nf kh ng nh ni kl nj nk nl mi ij bi translated">注意:假设读者精通注意力，我将只给出注意力机制的概述。如果你是新关注者，请阅读<a class="ae lg" rel="noopener" target="_blank" href="/intuitive-understanding-of-attention-mechanism-in-deep-learning-6c9482aecf4f"> <strong class="ma ir">这篇</strong> </a>的博客。</p><h2 id="1e86" class="ju jv iq bd jw jx jy dn jz ka kb dp kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">概观</h2><p id="2e24" class="pw-post-body-paragraph mx my iq ma b mb mc mz na md me nb nc kd nd ne nf kh ng nh ni kl nj nk nl mi ij bi translated">“注意力”这个名字本身给了我们这个算法做什么的想法。在普通的编码器-解码器模型中，编码器的所有中间状态都被丢弃，最终状态被用作解码器的初始状态。背后的想法很简单-最终状态包含输入句子的所有信息，解码器可以使用这些信息来预测输出。但是，当序列很长时，这个模型很难将所有信息编码到一个向量中。</p><p id="e371" class="pw-post-body-paragraph mx my iq ma b mb ob mz na md oc nb nc kd oq ne nf kh or nh ni kl os nk nl mi ij bi translated">而在注意机制中，我们试图模仿人类处理顺序信息的方式。我们通过给每个单词(编码器中间状态)加权来做到这一点，从而给予<strong class="ma ir">注意</strong>特定预测的特定单词。这是通过前馈神经网络实现的，该网络将编码器的所有中间状态以及先前的解码器状态作为输入，并给出该<strong class="ma ir">时间戳</strong>的每个字的权重。</p></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="bdb5" class="lh jv iq bd jw li ms lk jz ll mt ln kc lo mu lq kg lr mv lt kk lu mw lw ko lx bi translated">单调注意</h1><p id="808d" class="pw-post-body-paragraph mx my iq ma b mb mc mz na md me nb nc kd nd ne nf kh ng nh ni kl nj nk nl mi ij bi translated">注意，在每个时间戳，我们计算所有单词的权重，以确保我们不排除say- <strong class="ma ir">第一个单词影响第三个单词的预测或者say最后一个单词影响第一个预测的可能性。</strong>换句话说，注意力<strong class="ma ir">独立地</strong>考虑所有的中间状态，因此，时间戳的输出可以依赖于任何输入单词(意味着任何单词可以具有更大的权重)</p><p id="f81f" class="pw-post-body-paragraph mx my iq ma b mb ob mz na md oc nb nc kd oq ne nf kh or nh ni kl os nk nl mi ij bi translated">该图描述了注意力模型</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi pe"><img src="../Images/48a21741928a401f7175e9f0d64e53bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*5Frzl4RBbQfv-K8P.png"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated">注意机制权重图</p></figure><blockquote class="nm"><p id="302d" class="nn no iq bd np nq pf pg ph pi pj mi dk translated"><strong class="ak"> <em class="pk">输出可以根据任何字对其进行预测</em> </strong></p></blockquote><p id="1efe" class="pw-post-body-paragraph mx my iq ma b mb nw mz na md nx nb nc kd ny ne nf kh nz nh ni kl oa nk nl mi ij bi translated"><strong class="ma ir">T11】</strong></p><p id="511a" class="pw-post-body-paragraph mx my iq ma b mb ob mz na md oc nb nc kd oq ne nf kh or nh ni kl os nk nl mi ij bi translated">也就是说，一旦输入序列元素在给定的输出时间步被关注，在它之前出现的元素就不能在随后的输出时间步被关注。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi pe"><img src="../Images/cbf9b56a42abab8dcbb283256adc45ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*rwJrHSSy3xvnqo9o.png"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated">单调注意力权重图</p></figure><p id="9cc6" class="pw-post-body-paragraph mx my iq ma b mb ob mz na md oc nb nc kd oq ne nf kh or nh ni kl os nk nl mi ij bi translated">从图中我们可以清楚地看到，第一个输出依赖于第一个字，因此第二个输出不能使用第一个字，而是使用第二和第三个字进行预测。它严格地从左向右进行。</p><blockquote class="nm"><p id="4027" class="nn no iq bd np nq nr ns nt nu nv mi dk translated">在单调注意中，只有以前的输出没有使用过的词才能对现在的输出产生影响，并且严格地从左到右</p></blockquote><blockquote class="ov ow ox"><p id="7975" class="mx my oy ma b mb nw mz na md nx nb nc oz ny ne nf pa nz nh ni pb oa nk nl mi ij bi translated"><strong class="ma ir">这使得模型变得不那么复杂，并且在这个案例研究中对我特别有帮助，它的表现远远好于注意力。</strong></p></blockquote></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="1706" class="lh jv iq bd jw li ms lk jz ll mt ln kc lo mu lq kg lr mv lt kk lu mw lw ko lx bi translated">实现和代码演练</h1><p id="5dda" class="pw-post-body-paragraph mx my iq ma b mb mc mz na md me nb nc kd nd ne nf kh ng nh ni kl nj nk nl mi ij bi translated">让我们在数据集上实现单调注意，并创建一个文本校正器模型。</p><h2 id="8365" class="ju jv iq bd jw jx jy dn jz ka kb dp kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">我们将从数据集的基本预处理开始</strong></h2><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="ot ou l"/></div><p class="lc ld gj gh gi le lf bd b be z dk translated">文本预处理</p></figure><p id="a76d" class="pw-post-body-paragraph mx my iq ma b mb ob mz na md oc nb nc kd oq ne nf kh or nh ni kl os nk nl mi ij bi translated"><strong class="ma ir">现在是时候添加一些扰动了</strong></p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="ot ou l"/></div></figure><h2 id="fd8c" class="ju jv iq bd jw jx jy dn jz ka kb dp kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">使用教师强制*为编码器-解码器创建输入-输出对。</strong></h2><p id="6b3d" class="pw-post-body-paragraph mx my iq ma b mb mc mz na md me nb nc kd nd ne nf kh ng nh ni kl nj nk nl mi ij bi translated">教师强制是一种方法，其中我们显式地将<strong class="ma ir">输入</strong>给解码器，而不是将解码器的<strong class="ma ir">输出传递给下一个时间戳。这显然只在训练时使用，有助于减少训练时间和提高准确性。</strong></p><pre class="kr ks kt ku gt og oh oi oj aw ok bi"><span id="00bb" class="ju jv iq oh b gy ol om l on oo">dataframe['decoder_input'] = '&lt;start&gt; ' + dataframe['output'].astype(str)</span><span id="16f4" class="ju jv iq oh b gy op om l on oo">dataframe['decoder_output'] = dataframe['output'].astype(str) + ' &lt;end&gt;'</span><span id="1996" class="ju jv iq oh b gy op om l on oo">dataframe = dataframe.drop(['output'], axis=1)</span></pre><p id="c211" class="pw-post-body-paragraph mx my iq ma b mb ob mz na md oc nb nc kd oq ne nf kh or nh ni kl os nk nl mi ij bi translated"><strong class="ma ir">举例- </strong></p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi pl"><img src="../Images/9a9385ac41cdd41ea200979e904d1908.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FfqYh3hUSsZuP9RikTL3zA.png"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated">编码器输入缺少“a”</p></figure><p id="3c27" class="pw-post-body-paragraph mx my iq ma b mb ob mz na md oc nb nc kd oq ne nf kh or nh ni kl os nk nl mi ij bi translated">→ <strong class="ma ir">之后，我们将数据集分为训练、验证和测试。</strong></p><p id="ff51" class="pw-post-body-paragraph mx my iq ma b mb ob mz na md oc nb nc kd oq ne nf kh or nh ni kl os nk nl mi ij bi translated">→ <strong class="ma ir">然后我们对数据进行标记化，将单词转换成整数标记来训练模型。</strong></p></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h2 id="5aad" class="ju jv iq bd jw jx jy dn jz ka kb dp kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">预训练向量的嵌入矩阵- </strong></h2><p id="4dc9" class="pw-post-body-paragraph mx my iq ma b mb mc mz na md me nb nc kd nd ne nf kh ng nh ni kl nj nk nl mi ij bi translated">在对数据进行标记化之后，我们使用GLOVE-300预训练向量为我们的正确_标记和扰动_标记创建包含我们的单词的预训练嵌入向量的嵌入_矩阵。</p><p id="fd1e" class="pw-post-body-paragraph mx my iq ma b mb ob mz na md oc nb nc kd oq ne nf kh or nh ni kl os nk nl mi ij bi translated">正确标记的嵌入矩阵</p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="ot ou l"/></div></figure><h2 id="6eb5" class="ju jv iq bd jw jx jy dn jz ka kb dp kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">实现我们的深度文本校正器模型</strong></h2><p id="0554" class="pw-post-body-paragraph mx my iq ma b mb mc mz na md me nb nc kd nd ne nf kh ng nh ni kl nj nk nl mi ij bi translated">在所有的预处理和创建嵌入矩阵之后，我们准备好创建我们的<strong class="ma ir">单调关注编码器-解码器模型。</strong></p><p id="f248" class="pw-post-body-paragraph mx my iq ma b mb ob mz na md oc nb nc kd oq ne nf kh or nh ni kl os nk nl mi ij bi translated"><strong class="ma ir">我们将使用模型子类化的方法从Keras的模型和层类中导出我们的模型。</strong></p><p id="896c" class="pw-post-body-paragraph mx my iq ma b mb ob mz na md oc nb nc kd oq ne nf kh or nh ni kl os nk nl mi ij bi translated"><strong class="ma ir">编码器模型实现如下- </strong></p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="ot ou l"/></div></figure><blockquote class="ov ow ox"><p id="6daf" class="mx my oy ma b mb ob mz na md oc nb nc oz oq ne nf pa or nh ni pb os nk nl mi ij bi translated"><strong class="ma ir">之后，我们创建→单调注意层、单步解码器层、解码器模型和包含编码器和解码器的最终模型。</strong></p></blockquote><h2 id="65b5" class="ju jv iq bd jw jx jy dn jz ka kb dp kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">关于我的实现的全部细节，请查看我的Github简介</h2><h2 id="fe7c" class="ju jv iq bd jw jx jy dn jz ka kb dp kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">数据管道</h2><p id="d017" class="pw-post-body-paragraph mx my iq ma b mb mc mz na md me nb nc kd nd ne nf kh ng nh ni kl nj nk nl mi ij bi translated">现在我们需要一些东西，可以把我们的数据，转换成所需维度的批量数据。为此，我们通过从Keras的序列层派生来创建数据管道</p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="ot ou l"/></div></figure><h2 id="bb17" class="ju jv iq bd jw jx jy dn jz ka kb dp kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">培养</h2><p id="bfd5" class="pw-post-body-paragraph mx my iq ma b mb mc mz na md me nb nc kd nd ne nf kh ng nh ni kl nj nk nl mi ij bi translated">终于到了我们训练模型的时候了！</p><p id="1fa8" class="pw-post-body-paragraph mx my iq ma b mb ob mz na md oc nb nc kd oq ne nf kh or nh ni kl os nk nl mi ij bi translated">我们正在对“ADAM”优化器和自定义损失函数进行培训，以确保我们不会计算填充值的损失，因为我们已经大量填充了数据。</p><pre class="kr ks kt ku gt og oh oi oj aw ok bi"><span id="7a43" class="ju jv iq oh b gy ol om l on oo">train_steps=train.shape[0]//512</span><span id="c916" class="ju jv iq oh b gy op om l on oo">valid_steps=validation.shape[0]//512</span><span id="257b" class="ju jv iq oh b gy op om l on oo">attention__.fit(train_dataloader, steps_per_epoch=train_steps, epochs=120, validation_data=validation_dataloader, callbacks=[es,chkpt,tfboard], validation_steps=valid_steps, )</span></pre><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi pm"><img src="../Images/26f6f33a03b8201b1a8a8715049fe978.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o0Qb5e6qkFF5et7Ujxgn8w.png"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated">训练图(蓝色-&gt;验证)</p></figure></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="53e4" class="lh jv iq bd jw li ms lk jz ll mt ln kc lo mu lq kg lr mv lt kk lu mw lw ko lx bi translated">推理</h1><p id="0cef" class="pw-post-body-paragraph mx my iq ma b mb mc mz na md me nb nc kd nd ne nf kh ng nh ni kl nj nk nl mi ij bi translated">训练完模型后，我们该检查它的效率了。<strong class="ma ir">我们将使用BLEU分数作为我们的衡量标准。</strong></p><h2 id="6cb0" class="ju jv iq bd jw jx jy dn jz ka kb dp kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated"><strong class="ak">贪婪搜索v/s光束搜索</strong></h2><p id="e710" class="pw-post-body-paragraph mx my iq ma b mb mc mz na md me nb nc kd nd ne nf kh ng nh ni kl nj nk nl mi ij bi translated">我们使用两种类型的搜索来预测我们的产出。</p><p id="3423" class="pw-post-body-paragraph mx my iq ma b mb ob mz na md oc nb nc kd oq ne nf kh or nh ni kl os nk nl mi ij bi translated"><strong class="ma ir">剧透预警！！！→ </strong>它们的表现相似，因此我最终使用了贪婪搜索。</p><p id="9990" class="pw-post-body-paragraph mx my iq ma b mb ob mz na md oc nb nc kd oq ne nf kh or nh ni kl os nk nl mi ij bi translated"><strong class="ma ir">贪婪搜索→ </strong></p><p id="cbbb" class="pw-post-body-paragraph mx my iq ma b mb ob mz na md oc nb nc kd oq ne nf kh or nh ni kl os nk nl mi ij bi translated">该模型的输出是一个巨大的概率表，包含我们的单词在词汇表中的概率。在贪婪搜索中，我们只是简单地选择概率最高的单词。</p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="ot ou l"/></div><p class="lc ld gj gh gi le lf bd b be z dk translated">贪婪搜索的推理</p></figure><h2 id="e745" class="ju jv iq bd jw jx jy dn jz ka kb dp kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">波束搜索</h2><p id="a4bd" class="pw-post-body-paragraph mx my iq ma b mb mc mz na md me nb nc kd nd ne nf kh ng nh ni kl nj nk nl mi ij bi translated">在beam搜索算法中，我们不是选择一个概率最高的词，而是选择前3个词(beam_index=3)。基于这前3个单词，我们预测下一个单词，并用我们之前的单词计算这些单词的条件概率。这个过程一直持续到我们到达终点。</p><blockquote class="ov ow ox"><p id="4021" class="mx my oy ma b mb ob mz na md oc nb nc oz oq ne nf pa or nh ni pb os nk nl mi ij bi translated"><strong class="ma ir"> <em class="iq">要想恰当地理解梁寻欢，观看本</em> </strong> <a class="ae lg" href="https://www.youtube.com/watch?v=RLWuzLLSIgw" rel="noopener ugc nofollow" target="_blank"> <strong class="ma ir"> <em class="iq">视频</em> </strong> </a> <strong class="ma ir"> <em class="iq">作者吴恩达，阅读本</em> </strong> <a class="ae lg" href="https://medium.com/@dhartidhami/beam-search-in-seq2seq-model-7606d55b21a5" rel="noopener"> <strong class="ma ir"> <em class="iq">文章</em> </strong> </a> <strong class="ma ir"> <em class="iq">作者</em> </strong> <a class="pc pd ep" href="https://medium.com/u/adb57bd98a88?source=post_page-----1e1a3f5a1b9e--------------------------------" rel="noopener" target="_blank"> <strong class="ma ir"> <em class="iq">达蒂达米</em> </strong> </a></p></blockquote><p id="0d62" class="pw-post-body-paragraph mx my iq ma b mb ob mz na md oc nb nc kd oq ne nf kh or nh ni kl os nk nl mi ij bi translated"><strong class="ma ir">波束索引= 3的波束搜索的实现</strong></p><figure class="kr ks kt ku gt kv"><div class="bz fp l di"><div class="ot ou l"/></div><p class="lc ld gj gh gi le lf bd b be z dk translated">波束搜索推理</p></figure></div><div class="ab cl jn jo hu jp" role="separator"><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js jt"/><span class="jq bw bk jr js"/></div><div class="ij ik il im in"><h1 id="8f29" class="lh jv iq bd jw li ms lk jz ll mt ln kc lo mu lq kg lr mv lt kk lu mw lw ko lx bi translated"><strong class="ak">结果和误差分析</strong></h1><p id="68ab" class="pw-post-body-paragraph mx my iq ma b mb mc mz na md me nb nc kd nd ne nf kh ng nh ni kl nj nk nl mi ij bi translated">正常情况下，我在训练数据集上的BLEU分数约为<strong class="ma ir"> 0.90 </strong>，而验证和测试数据集的BLEU分数约为<strong class="ma ir"> 0.75 </strong>。</p><p id="f9bc" class="pw-post-body-paragraph mx my iq ma b mb ob mz na md oc nb nc kd oq ne nf kh or nh ni kl os nk nl mi ij bi translated">但是随着单调的关注，我得到了训练数据集的<strong class="ma ir"> 0.98 </strong> BLEU分数，而测试和验证数据集的<strong class="ma ir">0.90</strong>BLEU分数。<strong class="ma ir">这是一个显著的改进！</strong></p><h2 id="956f" class="ju jv iq bd jw jx jy dn jz ka kb dp kc kd ke kf kg kh ki kj kk kl km kn ko kp bi translated">误差分析</h2><p id="2b81" class="pw-post-body-paragraph mx my iq ma b mb mc mz na md me nb nc kd nd ne nf kh ng nh ni kl nj nk nl mi ij bi translated">我分析了错误，发现我们的模型存在以下问题，这些问题可能是导致<strong class="ma ir">在我们的分数中遗漏0.10 </strong>的原因。</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi pn"><img src="../Images/4f50e4fecd1d2ad970436f556835c2b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*INHro5R4nBxCWiJG4TY00Q.png"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk">1</p></figure><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi po"><img src="../Images/567d711440fbda6117bba07812538c8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YkreXCyUC22C7KdF-j1mUw.png"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk">2</p></figure><h1 id="53e6" class="lh jv iq bd jw li lj lk jz ll lm ln kc lo lp lq kg lr ls lt kk lu lv lw ko lx bi translated">端到端管道</h1><p id="e9ee" class="pw-post-body-paragraph mx my iq ma b mb mc mz na md me nb nc kd nd ne nf kh ng nh ni kl nj nk nl mi ij bi translated">在这个项目的最后阶段，我创建了一个端到端的部署管道，它接收输入并预测输出，可以直接用于任何应用程序。</p><p id="7408" class="pw-post-body-paragraph mx my iq ma b mb ob mz na md oc nb nc kd oq ne nf kh or nh ni kl os nk nl mi ij bi translated">首先，我创建了一个模块(。py文件)，名为monotonic_attention，包含我们的编码器-解码器模型。</p><p id="6f2b" class="pw-post-body-paragraph mx my iq ma b mb ob mz na md oc nb nc kd oq ne nf kh or nh ni kl os nk nl mi ij bi translated">然后，我创建了另一个名为model.py的模块，它导入上述模块并预测输出。它包含一个predict()方法，该方法使用贪婪搜索执行推理</p><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi pp"><img src="../Images/7389b3a37ca4bb729badefb68ace35e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jFGqric06wklemJApoz2LA.png"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated">导入后，我们用所有文件和一个model_weight初始化我们的模型</p></figure><figure class="kr ks kt ku gt kv gh gi paragraph-image"><div role="button" tabindex="0" class="kw kx di ky bf kz"><div class="gh gi pq"><img src="../Images/4441ed2fd4fa09f6a827510b75b3da13.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1c3_2cBIogP0L5_lIdCTCw.png"/></div></div><p class="lc ld gj gh gi le lf bd b be z dk translated">预测</p></figure><p id="2e3e" class="pw-post-body-paragraph mx my iq ma b mb ob mz na md oc nb nc kd oq ne nf kh or nh ni kl os nk nl mi ij bi translated">如果你已经走到这一步。哇！谢谢你！看了我的文章。我希望你没有直接跳到这里！；)</p><h1 id="86e5" class="lh jv iq bd jw li lj lk jz ll lm ln kc lo lp lq kg lr ls lt kk lu lv lw ko lx bi translated">参考</h1><ol class=""><li id="3cde" class="ly lz iq ma b mb mc md me kd mf kh mg kl mh mi mj mk ml mm bi translated"><strong class="ma ir">单调关注论文—</strong><a class="ae lg" href="https://colinraffel.com/blog/online-and-linear-time-attention-by-enforcing-monotonic-alignments.html" rel="noopener ugc nofollow" target="_blank">https://Colin raffel . com/blog/online-and-linear-time-Attention-by-enforcing-Monotonic-alignments . html</a></li><li id="1615" class="ly lz iq ma b mb mn md mo kd mp kh mq kl mr mi mj mk ml mm bi translated"><a class="ae lg" href="https://github.com/UdiBhaskar/TfKeras-Custom-Layers/blob/master/Seq2Seq/clayers.py" rel="noopener ugc nofollow" target="_blank">https://github . com/UdiBhaskar/tfk eras-Custom-Layers/blob/master/seq 2 seq/clayers . py</a></li><li id="5904" class="ly lz iq ma b mb mn md mo kd mp kh mq kl mr mi mj mk ml mm bi translated"><a class="ae lg" href="https://appliedaicourse-1.medium.com/faqs-about-attention-assignment-3033ac9f05ac" rel="noopener">https://applied ai course-1 . medium . com/FAQs-about-attention-assignment-3033 ac9 f 05 AC</a></li><li id="b1bb" class="ly lz iq ma b mb mn md mo kd mp kh mq kl mr mi mj mk ml mm bi translated">在乌代·派拉的指导下在AppliedAI球场完成的项目。<a class="ae lg" href="https://www.appliedaicourse.com/" rel="noopener ugc nofollow" target="_blank">https://www.appliedaicourse.com/</a></li></ol></div></div>    
</body>
</html>