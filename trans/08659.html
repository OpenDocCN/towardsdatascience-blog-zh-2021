<html>
<head>
<title>XGBoost: Order Does Matter</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">XGBoost:顺序很重要</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/xgboost-order-does-matter-60d8d0d5aa71?source=collection_archive---------11-----------------------#2021-08-10">https://towardsdatascience.com/xgboost-order-does-matter-60d8d0d5aa71?source=collection_archive---------11-----------------------#2021-08-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="7f54" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">要素的顺序对XGBoost模型有什么影响，如何使用要素重要性来识别数据中的相关要素？</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/e73da5918d07f7482f3b7be1ae85554a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*3M9j9MhYWkb6kuDc"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">米克·豪普特在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="644c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可能会问自己-为什么我要使用要素重要性来查找数据中的相关要素？例如，查看变量对之间的皮尔逊相关性要简单得多。</p><p id="1c6e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">没错。</p><p id="33e0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">看一看(皮尔逊)相关矩阵:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ls"><img src="../Images/3bfb7f96dfda4a1f93e943535bc06696.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TwlglMkrbZrt8bvL28RJww.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><em class="lt">作者图片</em></p></figure><p id="ea36" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="lu"> x1 </em>和<em class="lu"> x4 </em>相关性高是没有问题的，但是<em class="lu"> x3 </em>和<em class="lu"> x4 </em>呢？0.37的相关性意味着什么？</p></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><p id="bd3d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">数据科学家使用机器学习模型，如XGBoost，将特征(X)映射到目标变量(Y)。理想情况下，我们希望映射尽可能类似于成对数据(X，Y)的真实生成器函数。我们使用标记数据和几个成功指标来衡量给定的学习映射与真实映射相比有多好。</p><p id="a916" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">XGBoost是一个高性能的决策树梯度提升集成，广泛用于表格数据的分类和回归任务。像其他决策树算法一样，它由分裂组成，即迭代选择最能把数据分成两组的特征。该算法在每次迭代中为每个特性分配一个分数，并根据该分数选择最佳分割(要了解更多关于XGBoost的信息，我推荐<a class="ae kv" href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html" rel="noopener ugc nofollow" target="_blank">【1】</a>)。然而，如果在模型训练过程中，两个特征在给定的级别上具有相同的分数，会发生什么呢？算法会优先选择哪一个？原来在一些XGBoost实现中，首选特性会是第一个(与特性的插入顺序有关)；然而，在其他实现中，随机选择两个特征中的一个。</p></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><p id="5d5b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在继续之前，我想说几句XGBoost的随机性。在建立树集合的过程中，一些决定可能是随机的:从数据中取样，为每棵树选择特征的子组，等等。使用默认参数运行XGBoost，并且不进行并行计算，会产生一组完全确定的树。如果您将参数<code class="fe mc md me mf b">subsample</code>的值更改为小于1，您将获得随机行为，并且需要设置一个种子以使其可重现(使用<code class="fe mc md me mf b">random_state</code>参数)。在这篇文章中，我使用了<code class="fe mc md me mf b">subsample=1</code>来避免随机性，所以我们可以假设结果不是随机的。</p></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><p id="30e2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里有一个简单的例子:</p><p id="4386" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我创建了一个简单的数据集，它有两个特征，<em class="lu"> x1 </em>和<em class="lu"> x2 </em>，这两个特征是高度相关的(皮尔逊相关系数为0.96)，并生成了<em class="lu">目标(</em>真目标)，仅作为<em class="lu"> x1 </em>的函数。</p><pre class="kg kh ki kj gt mg mf mh mi aw mj bi"><span id="6a18" class="mk ml iq mf b gy mm mn l mo mp">num_samples = 1000<br/>x1 = (np.random.randn(num_samples) <br/>      + 2 * np.random.randn(num_samples) <br/>      + 3 * np.random.randn(num_samples) + 4)<br/>x2 = x1 * 0.5 + np.power(x1, 2) * 0.06 + 0.07<br/>df_simple = pd.DataFrame({'x1': x1, 'x2': x2})<br/>df_simple['target'] <strong class="mf ir">=</strong> df_simple['x1'] <strong class="mf ir">*</strong> 0.08</span></pre><p id="d3ef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">使用默认参数训练XGboost模型，并查看特征重要性值(我使用了增益特征重要性类型。要阅读更多关于XGBoost类型的特性重要性，我推荐<a class="ae kv" rel="noopener" target="_blank" href="/interpretable-machine-learning-with-xgboost-9ec80d148d27">【2】</a>，我们可以看到<em class="lu"> x1 </em>是最重要的特性。太好了！</p><pre class="kg kh ki kj gt mg mf mh mi aw mj bi"><span id="8f0a" class="mk ml iq mf b gy mm mn l mo mp">simple_model = xgb.XGBRegressor()<br/>simple_model.fit(X_train, y_train)<br/>...<br/>x1 importance: 0.295<br/>x2 importance: 0.0</span></pre><p id="0356" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们将用相同的参数训练一个XGBoost模型，只改变特征的插入顺序。<em class="lu"> x2 </em>得到了几乎所有的重要性。</p><p id="0d46" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">不太好…</p><pre class="kg kh ki kj gt mg mf mh mi aw mj bi"><span id="d364" class="mk ml iq mf b gy mm mn l mo mp">simple_model_reverse = xgb.XGBRegressor()<br/>simple_model_reverse.fit(X_train[['x2', 'x1']], y_train)<br/>...<br/>x1 importance: 0.03<br/>x2 importance: 0.118</span></pre><p id="bfd3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">显然，0.96的相关性非常高。从一开始，我们就不应该包含这两个特性。</p><p id="dc38" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">回到我们关于0.37相关性的问题，这里有另一个非常简单的例子:</p><p id="6a54" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">数据集由4个特征组成，其中<em class="lu"> x3 </em>是<em class="lu"> x2 </em>的噪声变换，<em class="lu"> x4 </em>是<em class="lu"> x1 </em>、<em class="lu"> x2 </em>和<em class="lu"> x3 </em>的非线性组合，而<em class="lu">目标</em>只是<em class="lu"> x1 </em>和<em class="lu"> x3 </em>的函数。</p><pre class="kg kh ki kj gt mg mf mh mi aw mj bi"><span id="38f7" class="mk ml iq mf b gy mm mn l mo mp">x1 = np.random.randn(num_samples)<br/>x2 = np.random.uniform(0, 1, num_samples)<br/>x3 = x2 * 2 - 0.4345 + np.random.randn(num_samples)<br/>x4 = [x1[i] if x1[i] &lt; 0.1 else <br/>      x2[np.random.choice(num_samples)] <br/>      + x3[i] for i in range(num_samples)]<br/>df = pd.DataFrame({'x1': x1, 'x2': x2, 'x3': x3, 'x4': x4})<br/>df['target'] = (0.2 * df['x1'] <br/>               + 0.67 * np.sqrt(np.power((df['x1'] - df['x3']), 2))<br/>               + df['x1'] * df['x3'])</span></pre><p id="771f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了模拟这个问题，我用相同的默认参数为4个特性的每个可能的排列(24个不同的排列)重新构建了一个XGBoost模型。下图可以看到，MSE是一致的。但是，与模型的性能一致性相反，特性重要性排序确实发生了变化。</p><div class="kg kh ki kj gt ab cb"><figure class="mq kk mr ms mt mu mv paragraph-image"><img src="../Images/9383fc16c3ddf2b306744d674c4b1557.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*l_4pm-IzQz5yUeXSFxo_lw.png"/></figure><figure class="mq kk mw ms mt mu mv paragraph-image"><img src="../Images/5b382485f09a08f7f2f99f1bc02808cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:776/format:webp/1*KDLNyI5FkvgGw4HwcqYNMw.png"/><p class="kr ks gj gh gi kt ku bd b be z dk mx di my mz translated">(左)测试MSE分布|(右)特征重要性分布| <em class="lt">作者图片</em></p></figure></div><p id="ca23" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在75%的排列中，<em class="lu"> x4 </em>是最重要的特征，其次是<em class="lu"> x1 </em>或<em class="lu"> x3 </em>，但在另外25%的排列中，<em class="lu"> x1 </em>是最重要的特征。这里有两个问题:</p><ol class=""><li id="85bc" class="na nb iq ky b kz la lc ld lf nc lj nd ln ne lr nf ng nh ni bi translated">顺序不一致。</li><li id="9f34" class="na nb iq ky b kz nj lc nk lf nl lj nm ln nn lr nf ng nh ni bi translated">目标只是一个<em class="lu"> x1 </em>和<em class="lu"> x3 </em>的算术表达式！<em class="lu"> x4 </em>不是产生真正目标的方程式的一部分。</li></ol></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><p id="1aa6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">不同的特征排序在特征和目标变量之间产生不同的映射。原因可能是变量之间复杂的间接关系。但我为什么要在乎呢？</p><p id="8076" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将特征重要性视为每个特征对<strong class="ky ir">真</strong>目标的贡献的良好近似值可能是不正确的。请务必记住，它仅反映每个要素对模型所做预测的贡献。有时候这正是我们需要的。但是，在其他情况下，我们想知道特征重要性值是解释模型还是数据(<a class="ae kv" href="https://arxiv.org/abs/2006.16234" rel="noopener ugc nofollow" target="_blank">【3】</a>)。</p></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><h2 id="0b5c" class="mk ml iq bd no np nq dn nr ns nt dp nu lf nv nw nx lj ny nz oa ln ob oc od oe bi translated">结论</h2><p id="309d" class="pw-post-body-paragraph kw kx iq ky b kz of jr lb lc og ju le lf oh lh li lj oi ll lm ln oj lp lq lr ij bi translated">一个特征可能与另一个特征不相关(线性地或以另一种方式)。然而，在训练过程中，在特征空间的某个子空间，它可能得到与另一个特征相同的分数，并被选择来分割数据。如果两个特征可以被模型互换使用，这意味着它们以某种方式相关，可能是通过一个混杂的特征。</p><p id="05c9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意功能顺序。如果您不确定，请尝试不同的顺序。使用您的领域知识和统计数据，如皮尔逊相关或交互图，来选择排序。批评特征重要性的输出。同样，用你的领域知识去理解另一个订单是否同样合理。</p></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><h2 id="4424" class="mk ml iq bd no np nq dn nr ns nt dp nu lf nv nw nx lj ny nz oa ln ob oc od oe bi translated">参考</h2><p id="efa7" class="pw-post-body-paragraph kw kx iq ky b kz of jr lb lc og ju le lf oh lh li lj oi ll lm ln oj lp lq lr ij bi translated">[1] <a class="ae kv" href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html" rel="noopener ugc nofollow" target="_blank"> XGBoost教程—助推树简介</a></p><p id="5d1b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2] <a class="ae kv" rel="noopener" target="_blank" href="/interpretable-machine-learning-with-xgboost-9ec80d148d27">斯科特·伦德伯格<a class="ok ol ep" href="https://medium.com/u/3a739af9ef3a?source=post_page-----60d8d0d5aa71--------------------------------" rel="noopener" target="_blank">用XGBoost </a>进行可解释的机器学习</a></p><p id="dddc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[3]陈，h .，Janizek，J. D .，Lundberg，s .，&amp; Lee，S. I .，<a class="ae kv" href="https://arxiv.org/abs/2006.16234" rel="noopener ugc nofollow" target="_blank">忠于模型还是忠于数据？</a> (2020)，arXiv预印本arXiv:2006.16234。‏</p><p id="48c5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[4]<a class="ae kv" href="https://vishesh-gupta.medium.com/correlation-in-xgboost-8afa649bd066" rel="noopener">XG boost</a>中的相关性由<a class="ok ol ep" href="https://medium.com/u/866f05f2fd9?source=post_page-----60d8d0d5aa71--------------------------------" rel="noopener" target="_blank">维谢什·古普塔</a></p><p id="80f5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[5] <a class="ae kv" href="https://github.com/dmlc/xgboost/issues/3362" rel="noopener ugc nofollow" target="_blank">特征重要性结果对特征顺序敏感</a></p></div></div>    
</body>
</html>