<html>
<head>
<title>Why does Markov Decision Process matter in Reinforcement Learning?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么马尔可夫决策过程在强化学习中很重要？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-does-malkov-decision-process-matter-in-reinforcement-learning-b111b46b41bd?source=collection_archive---------18-----------------------#2021-09-01">https://towardsdatascience.com/why-does-malkov-decision-process-matter-in-reinforcement-learning-b111b46b41bd?source=collection_archive---------18-----------------------#2021-09-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="36cf" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">比较MDP和k-武装土匪的问题，看看MDP如何更适合在现实世界中</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3b5ccc5bbcee7eb3381b336452fb25e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*90TK_OJgI3nRyl9E"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">贾南·拉格沃尔在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="8592" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于大多数学习者来说，马尔可夫决策过程(MDP)框架是第一个了解强化学习(RL)的框架。然而，你能解释为什么它如此重要吗？为什么不是另一个框架？在这篇文章中，我将解释MDP相对于另一个流行的RL框架k-armed bandit问题的优势。</p><p id="ef7d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个帖子的灵感来自于<a class="ae ky" href="https://www.coursera.org/specializations/reinforcement-learning" rel="noopener ugc nofollow" target="_blank">由阿尔伯塔大学和阿尔伯塔机器智能研究所在Coursera </a>上提供的RL专业。我写这篇文章是为了总结一些视频，并对专业化有更深入的了解。</p><h1 id="2166" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">强化学习的目标</h1><p id="6089" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">在深入我们的主要问题之前，我想提一下RL的主要目标，因为当我们选择一个框架来构建一个模型时，它很重要。在RL问题中，我们希望<strong class="lb iu">随着时间的推移采取行动，使我们获得的总回报</strong>最大化。没有人告诉我们什么行为会带来更高的回报。相反，我们从经验中学习。我们重复试错的尝试，观察哪种行为给我们更高的回报，哪种行为给我们更低的回报。此外，我们甚至不知道什么时候开始给予奖励。它们可能会立即给出，也可能在我们采取行动后的几个时间步骤后给出。因此，我们需要一个动态框架来捕捉这两个特征，“试错搜索”和“延迟回报”</p><h1 id="61ec" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">例子:兔子探索食物</h1><p id="153f" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">我们考虑一个简单的情况。有一只饥饿的兔子正在寻找食物来满足她的食欲。它的右边有一个胡萝卜，左边有一个花椰菜。兔子更喜欢胡萝卜而不是西兰花，所以当她吃一根胡萝卜时，她会得到+10奖励。另一方面，花椰菜只能产生+3的回报。每次，兔子都会选择一根胡萝卜，因为她知道选择一根胡萝卜是一个最优行动。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/7a071efada972b7b07d1ae45e8dcc0a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aUgixynlaXNAcz96Y5KgoQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1:兔子右侧的胡萝卜(图片由作者提供)</p></figure><p id="62d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果胡萝卜和西兰花的位置互换了呢？兔子也会再次选择胡萝卜，因为食物的位置不会影响兔子得到的奖励。这其实是一个简单的“k臂土匪”问题的例子。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/e067e641f77536172158263539ce6e8d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Op0dpL0msHuEIlOINLZVpw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2:兔子左侧的胡萝卜(图片由作者提供)</p></figure><h1 id="37d5" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">k武装匪徒</h1><p id="ce61" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated"><strong class="lb iu">k臂土匪问题</strong>是RL中最简单但功能强大的框架之一。它被命名为“独臂强盗”(=老虎机)，虽然框架有<em class="mt"> k </em>杠杆，而不是一个。在这个问题中，你面临着在<em class="mt"> k </em>个不同选项中做出选择。每次选择后，你会收到一份奖励，它是从与你选择的选项相对应的固定概率分布中选出的。你重复这样的选择，目的是最大化你获得的奖励总额。要做到这一点，你需要找到一个最有可能产生最佳回报的杠杆(或期权)，并把你的选择集中在它上面。</p><p id="86e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">兔子的例子可以被视为一个“双臂强盗”问题，因为兔子总是有两种选择，胡萝卜和花椰菜。选择胡萝卜有100%的概率产生+10，选择西兰花有100%的概率产生+3。因为兔子想让奖励总量最大化，所以总是选择胡萝卜。</p><p id="404b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面这篇文章将有助于更详细地理解k臂土匪问题。</p><div class="mu mv gp gr mw mx"><a rel="noopener follow" target="_blank" href="/multi-armed-bandits-and-reinforcement-learning-dc9001dcb8da"><div class="my ab fo"><div class="mz ab na cl cj nb"><h2 class="bd iu gy z fp nc fr fs nd fu fw is bi translated">多臂土匪与强化学习</h2><div class="ne l"><h3 class="bd b gy z fp nc fr fs nd fu fw dk translated">用Python例子温和地介绍了经典问题</h3></div><div class="nf l"><p class="bd b dl z fp nc fr fs nd fu fw dk translated">towardsdatascience.com</p></div></div><div class="ng l"><div class="nh l ni nj nk ng nl ks mx"/></div></div></a></div><h1 id="b380" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">例子续:胡萝卜后面的老虎</h1><p id="a915" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">虽然k-armed bandit框架很好地捕捉了前面的简单情况，但它有一个限制，这将在下一个情况中显示。</p><p id="90d9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这里，我们想象一只老虎躲在胡萝卜后面猎食兔子。当兔子吃了胡萝卜，兔子和老虎之间不再有任何障碍。老虎比兔子跑得快，所以她跑不掉。兔子最终被猎杀并获得-100奖励。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/469b5fa5436d2e75f3363626a80aa526.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UGPueLq7eQSGvne8SWl6YQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3:兔子去抓胡萝卜，老虎在后面(图片由作者提供)</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/fdf9ead20c835b01aeaf46745505abf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SXZ5CCWECBZYtm3t5P4VcQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4:兔子在吃完胡萝卜后被追捕(图片由作者提供)</p></figure><p id="1d2f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果兔子选择吃西兰花，兔子会得到+3并且保持安全。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/e3215befbd08f6cbd146c46860cb6387.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0wmN5zfydNUizxrFHI4IuA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图5:兔子只要选择西兰花就是安全的(图片由作者提供)</p></figure><p id="bbc6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在k臂土匪问题中，兔子即使知道胡萝卜后面有老虎，也继续选择胡萝卜，因为变化的情况不影响它的最优行动。然而，继续选择胡萝卜不会永远帮助兔子。我们应该如何在我们的问题定义中考虑情况的变化？</p><h1 id="6fa5" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">马尔可夫决策过程</h1><p id="c4cb" class="pw-post-body-paragraph kz la it lb b lc mn ju le lf mo jx lh li mp lk ll lm mq lo lp lq mr ls lt lu im bi translated">最后，我们引入<strong class="lb iu">马尔可夫决策过程(MDP) </strong>来解决这样一个问题。MDP由两个元素组成:<strong class="lb iu">代理</strong>和<strong class="lb iu">环境</strong>。代理是学习者或决策者。在上面的例子中，代理是兔子。环境是代理周围的一切。在这个例子中，环境包括兔子和老虎在田野里的一切。在代理采取行动后，我们面临一个不同的情况。我们称这些不同的情况为状态。比如兔子还没动的情况被认为是一种状态，兔子吃了胡萝卜之后在西兰花和老虎之间的情况被认为是另一种状态。</p><p id="5160" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在MDP，病原体与环境“互动”。代理选择一个动作，并观察采取该动作后环境中发生了什么。然后，它会收到与动作和她转换到的状态相对应的奖励。代理多次重复交互，并学习在每个状态下什么动作是最佳的。</p><p id="60e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下图是MDP的形式化。在时间<em class="mt"> t </em>处，处于状态St的代理从动作空间中选择处于处的动作<em class="mt">，并且环境从状态空间返回新的状态<em class="mt"> S_t+1 </em>。然后，代理根据开始状态、采取的行动和后续状态接收奖励<em class="mt"> R_t+1 </em>。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nm"><img src="../Images/3a25e23d052a902b406c0a51a9892c1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y3pH5qHuvtEw7gJl86wpFQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图6:代理和环境之间的交互(图片由萨顿和巴尔托提供，由CC BY-NC-ND 2.0授权)</p></figure><p id="2627" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在兔子的例子中，动作空间由左右移动组成。状态空间包括所有四种可能的情况，1)兔子在初始位置，2)兔子在西兰花和老虎之间，3)兔子吃完西兰花后在最左边，4)兔子被老虎吃掉。可能的奖励是+3(情况2)，+10(情况3)，和-100(情况4)。下图描述了详细信息。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/1169485c1ff7677b9a5ac3da85a7c05a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QAJWajqlayJD7Ee1bkDmjg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图7:可能状态和相关奖励的可视化(图片由作者提供)</p></figure><p id="0b69" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑到MDP框架和图表，兔子知道最佳行动是向右移动，因为吃胡萝卜最终会产生负的总回报。</p><p id="a2c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从技术上讲，MDP通过定义MDP的动态函数来捕捉RL问题的动态性，这是一个状态和后续奖励对的概率分布，取决于其先前的状态和在那里采取的行动。不过，我会把这个解释留到以后写的另一篇文章中。</p><h1 id="6803" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">摘要</h1><ul class=""><li id="fa2c" class="no np it lb b lc mn lf mo li nq lm nr lq ns lu nt nu nv nw bi translated">MDP框架很重要，因为它考虑了可能导致最优行动改变的情况变化，而这在k臂土匪框架中是没有考虑的</li><li id="6f0b" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">一个MDP由它的主体和环境组成。代理与环境交互，观察状态转换并接收奖励，以在每个状态下找到最佳行动</li></ul><h1 id="76ed" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">参考</h1><ul class=""><li id="33ee" class="no np it lb b lc mn lf mo li nq lm nr lq ns lu nt nu nv nw bi translated">[1] R .萨顿和A .巴尔托，强化学习导论，第2版(2018)，麻省理工学院出版社</li><li id="6410" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated"><a class="ae ky" href="https://www.coursera.org/specializations/reinforcement-learning" rel="noopener ugc nofollow" target="_blank">Coursera上的强化学习专业化</a></li></ul></div></div>    
</body>
</html>