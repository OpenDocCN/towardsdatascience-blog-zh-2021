<html>
<head>
<title>Stay updated with Neuroscience: June 2021 Must-Reads</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">与神经科学保持同步:2021年6月必读</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/stay-updated-with-neuroscience-june-2021-must-reads-d4bb078d03b8?source=collection_archive---------29-----------------------#2021-07-02">https://towardsdatascience.com/stay-updated-with-neuroscience-june-2021-must-reads-d4bb078d03b8?source=collection_archive---------29-----------------------#2021-07-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="fec3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">稀疏神经连接🖇️如何影响功能行为？一个更符合生物学原理的反向传播:误差向量广播📡，又是如何关注work⚡⚡⚡的？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f1dbffab9e775f5a12f62d3007c21782.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dUluu7PrzGG7oNob5w2Lzg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://unsplash.com/photos/Lej_oqHljbk" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><div class="kz la gp gr lb lc"><a href="https://medium.com/@stefanobosisio1/membership" rel="noopener follow" target="_blank"><div class="ld ab fo"><div class="le ab lf cl cj lg"><h2 class="bd iu gy z fp lh fr fs li fu fw is bi translated">通过我的推荐链接加入Medium-Stefano Bosisio</h2><div class="lj l"><h3 class="bd b gy z fp lh fr fs li fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="lk l"><p class="bd b dl z fp lh fr fs li fu fw dk translated">medium.com</p></div></div><div class="ll l"><div class="lm l ln lo lp ll lq ks lc"/></div></div></a></div><p id="2d13" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">你为什么要关心神经科学？</strong></p><p id="63a2" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">神经科学是当今人工智能🧠的根源🤖。阅读并意识到神经科学中的进化和新见解不仅会让你成为一个更好的“人工智能”的家伙😎而且还是一个更好的神经网络体系结构的创造者👩‍💻！</p><p id="3c31" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">今天这里有三篇来自arxiv.org的新论文。第一个研究稀疏连接性及其对功能模块性的影响。第二篇文章提出了一种反向传播的新方法，这可能在生物学上更合理——毫无疑问，这里应该进行大量的研究。最后，第三篇论文研究深度学习神经网络中的注意力，试图捕捉注意力在大脑中是如何工作的。</p><h1 id="2209" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">极度的稀疏导致了功能的专门化</h1><p id="7b57" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">加布里埃尔·贝娜，丹F.M古德曼，<a class="ae ky" href="https://arxiv.org/pdf/2106.02626.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">论文</strong> </a></p><p id="71de" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">神经网络是<em class="nk">模块化</em>，也就是说，它们可以分解成独立的子网。这些子网络呈现出<em class="nk">结构模块化</em>，其中神经元被划分为不同的模块，以及<em class="nk">功能模块化</em>，其中每个模块都可以执行独立的操作。给定这些概念，结构模块化和功能模块化之间有关系吗？我们能评估结构模块化在多大程度上影响功能模块化吗？作者对这些方面进行了研究，提出了一种可控的结构模块化神经网络，并监测功能模块化度量，了解这两种模块化在多大程度上相互影响，试图找到神经生物学对应物的答案。</p><p id="1f6f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">具有高模块化的网络在模块内的节点之间具有密集的连接，但是在不同模块的节点之间具有稀疏的连接</p><p id="be63" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">所提出的架构由两个子网络组成。这些网络作者具有不同程度的互连，从不同子网络模块中的节点之间的稀疏连接，到实现全密集架构时的无模块性。作为输入任务，每个子网接收一个MNIST数字，并且该网络必须与所有其他子网通信，以返回跨网络的两个数字是否具有奇偶性。同时，每个子网能够专门识别它们自己的数字。为了测量功能模块性，作者监控了三个度量标准:</p><ul class=""><li id="70c7" class="nl nm it lt b lu lv lx ly ma nn me no mi np mm nq nr ns nt bi translated">瓶颈度量:在读出以检查它们是否能够识别输入数字之后，窄的5个神经元层的平均准确度</li><li id="656d" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated">权重屏蔽度量:这是一个专门化的度量，其中保留了来自一个子网络的参数的子集<em class="nk"> q% </em>,以检查给定任务的性能</li><li id="f275" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated">相关性度量:对隐藏层进行分析，计算相同数字示例的子网络<em class="nk"> n </em>的隐藏状态之间的皮尔逊相关系数</li></ul><p id="9168" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">图1显示了所研究的子网的三个度量中的每一个的最终结果。总的来说，所有这三个指标都随着网络的结构模块性而变化，用稀疏性(活跃连接的比例)和<a class="ae ky" href="https://en.wikipedia.org/wiki/Modularity_(networks)" rel="noopener ugc nofollow" target="_blank"> Q模块性</a>来衡量。可以得出以下结论:</p><ul class=""><li id="d883" class="nl nm it lt b lu lv lx ly ma nn me no mi np mm nq nr ns nt bi translated">强加结构模块化(Q模块化的高值)导致更多的功能模块化(活动连接比例的高值)</li><li id="10d9" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated">在稀疏性或结构模块化的极端水平上，我们在所有三个度量上都有高度的结构专门化</li><li id="d5ef" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated">需要高水平的结构模块性(Q模块性)来确保功能模块性——即功能模块性可以通过子网之间非常稀疏的连接来实现</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/5df83c97db2bea81320e5d5df21edb64.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*YVMdtGsM91kLYulHWQgSmQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1:左边是两个子网络之间的稀疏度，右边是Q模块度。线条表示该指标的平均值，而阴影区域表示同一实验的10次重复的1个标准偏差</p></figure><p id="71f5" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这些结论对生物神经网络有显著的意义。连接组学的整个思想，即知道网络的结构属性就足以让我们理解功能属性，可能不再正确。正如作者所写的:</p><blockquote class="oa ob oc"><p id="ea35" class="lr ls nk lt b lu lv ju lw lx ly jx lz od mb mc md oe mf mg mh of mj mk ml mm im bi translated">我们不应该仅仅通过观察适度的结构模块化来得出任何程度的功能模块化</p></blockquote></div><div class="ab cl og oh hx oi" role="separator"><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol"/></div><div class="im in io ip iq"><h1 id="b307" class="mn mo it bd mp mq on ms mt mu oo mw mx jz op ka mz kc oq kd nb kf or kg nd ne bi translated">通过广播全局误差向量的信用分配</h1><p id="9cf7" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">大卫·g·克拉克，L.F阿博特，钟素妍，<a class="ae ky" href="https://arxiv.org/pdf/2106.04089.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">论文</strong> </a></p><p id="ad33" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">神经回路不实现反向传播方法(BP)，因为进化已经找到了另一种算法或路线，允许回路可塑性工作并训练所有的大脑网络。BP的一个可能的生物学解决方案是学分分配——这里是我们讨论学分分配问题的回顾——一个神秘的全球学习信号在整个网络中传播。</p><p id="4cc7" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">从这里，作者提出了一种新的替代BP的方法，称为<em class="nk">误差向量广播(GEVB) </em>，其中全局学习信号被广播到神经网络中的所有隐藏单元以更新权重。特别地，在网络上分发的信号是关于<em class="nk">输出误差</em>的信息，充当无<em class="nk">单元特定反馈。</em>实现GEVB的神经网络被称为<em class="nk">矢量化非负网络(VNNs) </em>。此外，GEVB回忆起生物神经网络，对第一层施加非负权重，因为它发生在皮质投射神经元的兴奋行为上。最后，通过由突触前激活和全局误差向量的内积给出的量来更新每个权重。图2显示了理解BP的各种替代方案的差异的图形方式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi os"><img src="../Images/3881588493e4818f8ad7e938a840b5d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wq5EKnrNp7RZJZHfRMsIOQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2:关于反向传播(BP)算法的替代实例:a)BP算法，其中通过逐步转置误差来逐层更新权重；b)反馈对准(FA ),其中误差被逐层向后发送；c)直接反馈对准(DFA ),其中误差被直接广播到每个隐藏层；d)提出的全局误差向量广播(GEVB ),其中全局误差向量被广播到所有隐藏单元，而没有单元特定的反馈。</p></figure><p id="45ed" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们有什么结果？首先，作者尝试使用矢量化非负网络(非负权重网络)和传统网络，针对不同的连接范围，针对MNIST和CIFAR-10数据集，比较BP和GEVB。我们的注意力将集中在矢量非负网络上，如表1和表2所示。总的来说，误差比较在不同的连接性和权重更新算法上是兼容的，在GEVB和BP之间有显著的重叠</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ot ou l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表1矢量网络的MNIST检验误差(%)比较，在不同连接范围(完全连接、卷积连接、局部连接)下，GEVB和BP之间具有非负权重或混合符号wegiths。</p></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ot ou l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">标签。2: CIFAR-10测试误差(%)比较，术语根据表1</p></figure><p id="f929" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">当使用t-SNE对CIFAR-10图像进行聚类时，获得了进一步令人鼓舞的结果，其中GEVB聚类质量在统计上优于BP方法。</p><p id="0a95" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这些显著结果可能为使人工神经网络更类似于生物神经网络铺平新的道路。GEVB算法提出了一个新的问题，关于矢量化的生物学实现，以及突触前和突触后神经元的响应如何相互干扰，在网络中传播“权重更新”信号。伙计们，请继续关注！</p></div><div class="ab cl og oh hx oi" role="separator"><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol"/></div><div class="im in io ip iq"><h1 id="872a" class="mn mo it bd mp mq on ms mt mu oo mw mx jz op ka mz kc oq kd nb kf or kg nd ne bi translated">通过内部门控的基于对象的注意</h1><p id="d574" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">乔丹·雷，阿里·本雅明，康拉德·保罗·柯丁，<a class="ae ky" href="https://arxiv.org/pdf/2106.04540.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lt iu">论文</strong> </a></p><blockquote class="oa ob oc"><p id="ff6b" class="lr ls nk lt b lu lv ju lw lx ly jx lz od mb mc md oe mf mg mh of mj mk ml mm im bi translated">注意力是大脑用来选择给定刺激或特征的有意义子集的机制</p></blockquote><p id="bb11" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">大脑中的一个神秘机制是注意力。计算神经科学和机器学习已经成功地为我们提供了使用注意机制在简单任务中检测和识别对象的模型。使用深度神经网络已经取得了更彻底的结果，这些网络被证明能够理解更复杂的场景，但是，它们仍然远远不能达到人脑的水平。人类大脑的注意力可以被认为是这些给定元素的混合:</p><ul class=""><li id="83f4" class="nl nm it lt b lu lv lx ly ma nn me no mi np mm nq nr ns nt bi translated"><em class="nk">神经激活的调制:</em>当受试者识别一个物体时，视觉神经元表现出活动的变化，从大约5%到30%的调制，增加了视觉皮层中的注意力机制</li><li id="a5fe" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated"><em class="nk">注意力不变调谐:</em>虽然神经活动的调节是开启的，但是注意力保持神经元的调谐特性不变</li><li id="c0c8" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated"><em class="nk">内部门控:</em>注意力是过滤不相关的特征，返回更清晰的信号</li><li id="b775" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated"><em class="nk">分层处理:</em>在大脑中，存在着分层组织的细胞，这些细胞呈现出广泛的调谐特性。这使得大脑能够学习复杂的非线性特征。这些细胞的输出增加了注意力层收集的信息</li><li id="f00c" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated"><em class="nk">自上而下的注意神经调节:</em>以视觉问题为例，信息是以前馈、反馈、横向流动的方式行进的。反馈和横向流动使自上而下的注意成为可能，而前馈路径则定义了早期层中的感受野。</li><li id="9220" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated"><em class="nk">返回抑制:</em>由视觉输入激活的区域被抑制，以允许受试者从一个检测到的物体移动到一个新的物体。</li></ul><p id="277e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">作者提供了一个新的神经网络模型，它可以包含所有这些特征，以便研究对视觉刺激的注意力的本质，试图复制生物学上正在发生的事情，以给出一个似乎合理的生物学答案。</p><p id="e0f4" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">图1显示了实现的神经网络，其被细分为3个主要区域，反映了大脑视觉路径。存在三条线，前馈(黑线)，其可以生成特征图并返回预测；实现注意力屏蔽的反馈通路(橙色线)和将前馈通路投射到反馈通路上的水平连接(绿色线)。注意力屏蔽在V2中充当内部注意力门控，并且在最后阶段，创建像素空间可解释的图像。最后的结果对注意及其机制给出了一个可能的解释</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/7da1c0114db54a305ec3f7ab38e21dc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XrSxRrYWUAboFJU0ypVU9A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3:左边是实现的基于注意力的神经网络，右边是视觉注意力路径，因为它可能在大脑中。所实现的神经网络的特点是保持前馈、横向和反馈连接。</p></figure><p id="3151" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">图4显示了MNIST和可可数据集的结果。根据注意机制，原始输入在不同阶段被处理。门控输入显示了注意力如何作用于输入图像，黑白区域是受抑制的区域，黑色区域是不受抑制的区域。注意掩码驱动门控机制，集中在最重要的区域，最后，IOR掩码(返回的抑制)指定哪些区域已经被检查，哪些区域将被抑制用于将来的迭代。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/8c5c42a79fabc05aabc337c4cbde83d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4xGrBC37T95k9sog1uyQbQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4是MNIST数据集(左侧)和可可数据集(右侧)的关注层的结果。</p></figure><p id="836f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这种学习方法促进了对大脑中注意力机制的理解，这种机制是前馈、反馈和与内部门控和返回抑制的横向联系的混合。从神经网络输出来看，似乎有一个处理对象的一般注意规则:</p><ul class=""><li id="cec4" class="nl nm it lt b lu lv lx ly ma nn me no mi np mm nq nr ns nt bi translated">神经元的调谐曲线不会改变</li><li id="3051" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated">在不同的神经层次都有抑制</li><li id="1818" class="nl nm it lt b lu nu lx nv ma nw me nx mi ny mm nq nr ns nt bi translated">在深层神经层有一个注意力的峰值调制</li></ul><p id="12a7" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这项研究为更深入地理解注意力奠定了基础，但未来的工作必须继续。与前一篇文章相关的一个注意事项是:如果我们对网络实现一个更符合生物学原理的反向传播算法，比如GEVB，会怎么样？</p></div><div class="ab cl og oh hx oi" role="separator"><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol om"/><span class="oj bw bk ok ol"/></div><div class="im in io ip iq"><p id="1dfc" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我希望你喜欢2021年6月神经科学<code class="fe ox oy oz pa b">arxivg.org</code>论文的这篇综述。请随时给我发电子邮件询问问题或评论，地址:stefanobosisio1@gmail.com</p></div></div>    
</body>
</html>