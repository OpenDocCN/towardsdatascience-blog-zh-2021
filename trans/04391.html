<html>
<head>
<title>11 Dimensionality reduction techniques you should know in 2021</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">2021年你应该知道的11种降维技术</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/11-dimensionality-reduction-techniques-you-should-know-in-2021-dcb9500d388b?source=collection_archive---------0-----------------------#2021-04-14">https://towardsdatascience.com/11-dimensionality-reduction-techniques-you-should-know-in-2021-dcb9500d388b?source=collection_archive---------0-----------------------#2021-04-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="fd1a" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">减少数据集的大小，同时尽可能保留变化</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3e99eb0c860375f425020dadb8a75ac7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CRPHVUh41pHmarzlqv_krw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@nika_benedictova?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">卡尼·贝内迪克托娃</a>在<a class="ae ky" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><p id="b238" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在统计学和机器学习中，数据集的属性、特征或输入变量的数量被称为其<strong class="lb iu">维度</strong>。例如，让我们来看一个非常简单的数据集，它包含两个属性，分别叫做<em class="lv">高度</em>和<em class="lv">重量</em>。这是一个二维数据集，该数据集的任何观察值都可以绘制成2D图。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lw"><img src="../Images/9006080afcacfe26644c9a3d7636a5b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*GL1S7IF83m8wwWi38vLZmQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="c64e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们将另一个称为<em class="lv">年龄</em>的维度添加到同一个数据集，它就成为一个三维数据集，任何观察都位于三维空间中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi lx"><img src="../Images/17ab3ddf2d80518f2bc2567b98e229f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*lRI-1S8SOk_r1nLkXrrVeg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="db31" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样，真实世界的数据集也有许多属性。这些数据集的观测值位于难以想象的高维空间中。以下是数据科学家、统计学家和机器学习工程师对与维度相关的数据集的一般几何解释。</p><blockquote class="ly lz ma"><p id="758b" class="kz la lv lb b lc ld ju le lf lg jx lh mb lj lk ll mc ln lo lp md lr ls lt lu im bi translated">在包含行和列的表格数据集中，列表示n维特征空间的维度，行是位于该空间中的数据点。</p></blockquote><p id="da28" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> <em class="lv">降维</em> </strong> <em class="lv"> </em>简单来说就是减少数据集中的属性数量，同时尽可能保留原始数据集中的变异的过程。这是一个数据预处理步骤，意味着我们在训练模型之前执行降维。在本文中，我们将讨论11种这样的降维技术，并使用Python和Scikit-learn库在真实世界的数据集上实现它们。</p><h1 id="4d28" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">降维的重要性</h1><p id="d124" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">当我们降低数据集的维度时，我们会丢失原始数据中一定百分比(通常为1%-15%，具体取决于我们保留的组件或特征的数量)的可变性。但是，不要担心丢失原始数据中那么多的可变性，因为维度减少将带来以下优势。</p><ul class=""><li id="9ae7" class="nb nc it lb b lc ld lf lg li nd lm ne lq nf lu ng nh ni nj bi translated"><strong class="lb iu">数据中更低的维度数量意味着更少的训练时间和更少的计算资源，并提高了机器学习算法的整体性能</strong> —涉及许多特征的机器学习问题使得训练极其缓慢。高维空间中的大多数数据点都非常接近该空间的边界。这是因为在高维空间中有足够的空间。在高维数据集中，大多数数据点可能彼此相距很远。因此，这些算法不能有效地在高维数据上训练。在机器学习中，这种问题被称为<strong class="lb iu"> <em class="lv">维数灾难</em></strong>——这只是一个你不需要担心的技术术语！</li><li id="6a94" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated"><strong class="lb iu">降维避免了</strong> <strong class="lb iu"> <em class="lv">过拟合</em> </strong>的问题——当数据中有很多特征时，模型变得更加复杂，往往会对训练数据过拟合。要了解这一点，请阅读我的文章<em class="lv"/><a class="ae ky" rel="noopener" target="_blank" href="/how-to-mitigate-overfitting-with-dimensionality-reduction-555b755b3d66"><em class="lv">如何通过降维来减轻过度拟合</em></a><em class="lv"/>。</li><li id="2569" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated"><strong class="lb iu">降维对于<em class="lv">数据可视化</em> </strong>非常有用——当我们将高维数据降维为两个或三个分量时，数据可以很容易地绘制在2D或3D图上。要看到这一点，请阅读我的<em class="lv"/><a class="ae ky" rel="noopener" target="_blank" href="/principal-component-analysis-pca-with-scikit-learn-1e84a0c731b0"><em class="lv">主成分分析(PCA)与Scikit-learn</em></a><em class="lv"/>文章。</li><li id="512a" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated"><strong class="lb iu">降维处理<em class="lv">多重共线性</em> </strong> —在回归中，当一个自变量与一个或多个其他自变量高度相关时，就会出现多重共线性。降维利用了这一点，将那些高度相关的变量组合成一组不相关的变量。这将解决多重共线性问题。要看到这一点，请阅读我的<em class="lv"/><a class="ae ky" rel="noopener" target="_blank" href="/how-do-you-apply-pca-to-logistic-regression-to-remove-multicollinearity-10b7f8e89f9b"><em class="lv">如何将PCA应用于逻辑回归以消除多重共线性？</em></a><em class="lv"/>文章。</li><li id="7db4" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated"><strong class="lb iu">降维对于<em class="lv">因子分析</em> </strong>非常有用——这是一种发现潜在变量的有用方法，这些变量不是在单个变量中直接测量的，而是从数据集中的其他变量中推断出来的。这些潜在变量被称为<strong class="lb iu"> <em class="lv">因素</em> </strong>。要了解这一点，请阅读我的<em class="lv"/><a class="ae ky" rel="noopener" target="_blank" href="/factor-analysis-on-women-track-records-data-with-r-and-python-6731a73cd2e0"><em class="lv">关于女性跟踪记录数据的因素分析与R和Python</em></a><em class="lv"/>文章。</li><li id="1118" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated"><strong class="lb iu">降维去除数据中的噪声</strong>——通过只保留最重要的特征并去除冗余特征，降维去除了数据中的噪声。这将提高模型精度。</li><li id="76e9" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated"><strong class="lb iu">降维可用于图像压缩</strong> — <em class="lv">图像压缩</em>是一种在尽可能保持图像质量的同时最小化图像字节大小的技术。构成图像的像素可以被认为是图像数据的维度(列/变量)。我们执行PCA以保持最佳数量的组件，从而平衡图像数据和图像质量中解释的可变性。要看到这一点，请阅读我的<em class="lv"/><a class="ae ky" rel="noopener" target="_blank" href="/image-compression-using-principal-component-analysis-pca-253f26740a9f"><em class="lv">使用主成分分析(PCA)进行图像压缩</em></a><em class="lv"/>文章。</li><li id="5181" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated"><strong class="lb iu">降维可用于将非线性数据转换成线性可分的形式</strong> — <strong class="lb iu"> </strong>阅读本文的<em class="lv">内核PCA </em>部分，了解实际操作！</li></ul><h1 id="b9b4" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">降维方法</h1><p id="4dec" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">有几种降维方法可用于不同类型的数据，以满足不同的需求。下图总结了这些降维方法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/4cf0a50fe033a1123c545182f26da9f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WhKA9Jboj_1sHa0MbWQQ7w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="7a6a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">主要有两种类型的降维方法。这两种方法都减少了维数，但方式不同。区分这两种方法非常重要。一种方法仅保留数据集中最重要的特征，并移除冗余特征。没有应用于特征集的变换。向后淘汰、向前选择和随机森林就是这种方法的例子。另一种方法是寻找新特征的组合。对该组特征应用适当的变换。新的要素集包含不同的值，而不是原始值。这种方法可以进一步分为线性方法和非线性方法。非线性方法是众所周知的流形学习。主成分分析(PCA)、因子分析(FA)、线性判别分析(LDA)和截断奇异值分解(SVD)是线性降维方法的例子。核主成分分析、t分布随机邻居嵌入(t-SNE)、多维标度(MDS)和等距映射(Isomap)是非线性降维方法的例子。</p><p id="afc6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们详细讨论每种方法。请注意，对于PCA和FA，我包含了我以前工作内容的链接，这些内容最好地描述了这两种方法的理论和实现。对于所有其他方法，我将在本文中包括理论、Python代码和可视化。</p><h1 id="05af" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">线性方法</h1><p id="a332" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">线性方法包括<strong class="lb iu"> <em class="lv">线性</em> </strong>将原始数据投影到低维空间。我们将在线性方法下讨论PCA、FA、LDA和截断SVD。这些方法可以应用于线性数据，但不适用于非线性数据。</p><h2 id="5259" class="nq mf it bd mg nr ns dn mk nt nu dp mo li nv nw mq lm nx ny ms lq nz oa mu ob bi translated">主成分分析</h2><p id="aa53" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">PCA是我最喜欢的机器学习算法之一。PCA是一种线性降维技术(算法)，它将一组相关变量(p)转换成较小的k (k <p number="" of="" uncorrelated="" variables="" called="" class="lb iu"> <em class="lv">主成分</em> ，同时尽可能多地保留原始数据集中的变化。在机器学习(ML)的背景下，PCA是一种用于降维的无监督机器学习算法。</p></p><p id="a9e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于这是我最喜欢的算法之一，我以前为PCA写过几个内容。如果你有兴趣了解PCA背后的理论及其Scikit-learn实现，你可以阅读我写的以下内容。</p><ul class=""><li id="0e0c" class="nb nc it lb b lc ld lf lg li nd lm ne lq nf lu ng nh ni nj bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/principal-component-analysis-pca-with-scikit-learn-1e84a0c731b0">使用Scikit-learn进行主成分分析(PCA)</a></li><li id="33ab" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated"><a class="ae ky" href="https://medium.com/data-science-365/statistical-and-mathematical-concepts-behind-pca-a2cb25940cd4" rel="noopener">PCA背后的统计和数学概念</a></li><li id="73a5" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/principal-component-analysis-for-breast-cancer-data-with-r-and-python-b312d28e911f">用R和Python对乳腺癌数据进行主成分分析</a></li><li id="6fbf" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/image-compression-using-principal-component-analysis-pca-253f26740a9f">使用主成分分析的图像压缩</a></li></ul><h2 id="d073" class="nq mf it bd mg nr ns dn mk nt nu dp mo li nv nw mq lm nx ny ms lq nz oa mu ob bi translated">因素分析</h2><p id="1f90" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated"><strong class="lb iu">因子分析</strong>和<strong class="lb iu">主成分分析</strong>都是降维技术。因子分析的主要目的不仅仅是降低数据的维度。因子分析是发现潜在变量的有用方法，这些潜在变量不是直接在单个变量中测量的，而是从数据集中的其他变量中推断出来的。这些潜在变量被称为<strong class="lb iu"> <em class="lv">因素</em> </strong>。</p><p id="23a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你有兴趣了解FA背后的理论和它的Scikit-learn实现，你可以阅读我写的以下内容。</p><ul class=""><li id="a28e" class="nb nc it lb b lc ld lf lg li nd lm ne lq nf lu ng nh ni nj bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/factor-analysis-on-women-track-records-data-with-r-and-python-6731a73cd2e0">用R和Python对“女子跟踪记录”数据进行因子分析</a></li></ul><h2 id="6abc" class="nq mf it bd mg nr ns dn mk nt nu dp mo li nv nw mq lm nx ny ms lq nz oa mu ob bi translated">线性判别分析(LDA)</h2><p id="2720" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">LDA通常用于多类分类。它也可以用作降维技术。LDA最好按类别分离或区分(因此得名LDA)训练实例。LDA和PCA之间的主要区别在于，LDA查找优化类可分性的输入特征的线性组合，而PCA试图在数据集中查找一组方差最大的不相关分量。两者之间的另一个关键区别是，PCA是一种非监督算法，而LDA是一种监督算法，它将类别标签考虑在内。</p><p id="d447" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">LDA有一些局限性。要应用LDA，数据应该是正态分布的。数据集还应该包含已知的类标签。LDA可以找到的最大组件数是类数减1。如果数据集中只有3个类别标签，LDA在降维中只能找到2(3–1)个分量。不需要执行特征缩放来应用LDA。另一方面，PCA需要缩放的数据。但是，PCA不需要类别标签。PCA可以找到的最大分量数是原始数据集中输入要素的数量。</p><p id="f4f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不应将用于降维的LDA与用于多类分类的LDA相混淆。这两种情况都可以使用Scikit-learn<strong class="lb iu">LinearDiscriminantAnalysis()</strong>函数来实现。在使用<strong class="lb iu"> fit(X，y) </strong>拟合模型之后，我们使用<strong class="lb iu"> predict(X) </strong>方法对LDA对象进行多类分类。这将为原始数据集中的类分配新的实例。我们可以使用LDA对象的<strong class="lb iu"> transform(X) </strong>方法进行降维。这将找到优化类可分性的新特征的线性组合。</p><p id="103e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下Python代码描述了对Iris数据集实施LDA和PCA技术，并展示了两者之间的差异。原始虹膜数据集有四个特征。LDA和PCA将特征数量减少为两个，并实现2D可视化。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等到加载python代码！</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/0c9e0290e3fa83eb94c9a2eb63b9d733.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wWFI6KMdIbtIKW9lC7K_jA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><h2 id="1911" class="nq mf it bd mg nr ns dn mk nt nu dp mo li nv nw mq lm nx ny ms lq nz oa mu ob bi translated">截断奇异值分解</h2><p id="5e3e" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">该方法通过截断奇异值分解(SVD)进行线性降维。它适用于许多行值为零的稀疏数据。相比之下，PCA在处理密集数据时效果很好。截断SVD也可以用于密集数据。截断SVD和PCA之间的另一个关键区别是，SVD的因子分解是在数据矩阵上进行的，而PCA的因子分解是在协方差矩阵上进行的。</p><p id="6479" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">截断SVD的Scikit-learn实现要容易得多。这可以通过使用<strong class="lb iu"> TruncatedSVD() </strong>函数来完成。以下Python代码描述了对Iris数据集实施截断SVD和PCA技术。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等到加载python代码！</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/d7292550ea03a927838b403dcbab7e1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9x2VLe0L66NWtbB_l89Nsw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><h1 id="c5fb" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">非线性方法(流形学习)</h1><p id="6f5f" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">如果我们处理的是现实应用中经常使用的非线性数据，那么到目前为止讨论的线性方法在降维方面表现不佳。在本节中，我们将讨论四种可用于非线性数据的非线性降维方法。</p><h2 id="8a93" class="nq mf it bd mg nr ns dn mk nt nu dp mo li nv nw mq lm nx ny ms lq nz oa mu ob bi translated">核主成分分析</h2><p id="e7a1" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">核PCA是一种使用<strong class="lb iu"> <em class="lv">核</em> </strong>的非线性降维技术。它也可以被认为是正常PCA的非线性形式。核主成分分析可以很好地处理非线性数据集，而正常的主成分分析不能有效地使用。</p><p id="b12e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">内核PCA背后的直觉很有趣。数据首先通过一个核函数运行，并将它们临时投影到一个新的高维特征空间中，在该空间中，类变得线性可分(可以通过画一条直线来划分类)。然后，该算法使用正常的PCA将数据投影回低维空间。通过这种方式，核PCA将非线性数据转换成可以与线性分类器一起使用的低维数据空间。</p><p id="99c3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在内核PCA中，我们需要指定3个重要的超参数——我们想要保留的组件数量、内核类型和内核系数(也称为<em class="lv"> gamma </em>)。对于核的类型，我们可以用<strong class="lb iu"> <em class="lv">【线性】【poly】【RBF】【sigmoid】【余弦】</em> </strong>。被称为<strong class="lb iu">径向基函数核</strong>的<strong class="lb iu"> <em class="lv"> rbf </em> </strong> <strong class="lb iu"> <em class="lv">核</em> </strong>是最流行的一种。</p><p id="e15a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们将对使用Scikit-learn<strong class="lb iu">make _ moons()</strong>函数生成的非线性数据实现一个RBF内核PCA。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等到加载python代码！</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/cd95d8e701c2aff21bbc1b880f143a25.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*5ZVliOhmYx9fARbDYfY8pA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">非线性数据(图片由作者提供)</p></figure><p id="05a9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以清楚地看到，上述两类非线性数据无法通过画一条直线来区分。</p><p id="7330" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们对上述数据同时执行PCA和内核PCA，看看会发生什么！</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等到加载python代码！</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/47252ba2c0212c1ba1512c7eedf8b355.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qj6mWjJeK9snUu5ey3Ip8Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="272b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上图可以看出，普通的PCA无法将非线性数据转换成线性形式。对相同的数据应用核主成分分析后，两个类线性分离得很好(现在，可以通过画一条垂直的直线来划分类)。</p><p id="5141" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里，原始数据的维数为2，绘制的数据的维数也为2。那么，核PCA真的降低了数据的维数吗？答案是<strong class="lb iu">“是”</strong>因为RBF核函数临时将二维数据投影到一个新的高维特征空间中，在该空间中，类变得可线性分离，然后算法将该高维数据投影回二维数据中，该二维数据可绘制成2D图。当类变得线性可分时，降维过程在幕后发生。</p><p id="c3db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用核PCA进行维数缩减的一个限制是，我们必须在运行算法之前为<strong class="lb iu"><em class="lv"/></strong><em class="lv"/>超参数指定一个值。这需要实现超参数调谐技术，例如网格搜索，以找到<strong class="lb iu"> <em class="lv">伽马</em> </strong>的最佳值。这超出了本文的范围。但是你可以通过阅读我的<strong class="lb iu"/><a class="ae ky" rel="noopener" target="_blank" href="/k-fold-cross-validation-explained-in-plain-english-659e33c0bc0"><strong class="lb iu">用通俗易懂的英语解释的k重交叉验证</strong></a><strong class="lb iu"/>来获得超参数调优过程的帮助。</p><h2 id="38ba" class="nq mf it bd mg nr ns dn mk nt nu dp mo li nv nw mq lm nx ny ms lq nz oa mu ob bi translated">t分布随机邻居嵌入(t-SNE)</h2><p id="c099" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">这也是一种主要用于数据可视化的非线性降维方法。除此之外，它还广泛应用于图像处理和自然语言处理。如果数据集中的要素数量超过50，Scikit-learn文档建议您在t-SNE之前使用PCA或截断奇异值分解。以下是在PCA之后执行t-SNE的一般语法。此外，请注意，在PCA之前需要进行特征缩放。</p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="3b71" class="nq mf it oh b gy ol om l on oo">from sklearn.decomposition import PCA<br/>from sklearn.manifold import TSNE<br/>from sklearn.preprocessing import StandardScaler</span><span id="39bf" class="nq mf it oh b gy op om l on oo">sc = StandardScaler()<br/>X_scaled = sc.fit_transform(X)</span><span id="626b" class="nq mf it oh b gy op om l on oo">pca = PCA()<br/>X_pca = pca.fit_transform(X_scaled)</span><span id="890a" class="nq mf it oh b gy op om l on oo">tsne = TSNE()<br/>X_tsne = tsne.fit_transform(X_pca)</span></pre><p id="b94b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用Scikit-learn管道可以简化上述代码。</p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="9583" class="nq mf it oh b gy ol om l on oo">from sklearn.pipeline import Pipeline<br/>from sklearn.decomposition import PCA<br/>from sklearn.manifold import TSNE<br/>from sklearn.preprocessing import StandardScaler</span><span id="3054" class="nq mf it oh b gy op om l on oo">sc = StandardScaler()<br/>pca = PCA()<br/>tsne = TSNE()</span><span id="ae06" class="nq mf it oh b gy op om l on oo">tsne_after_pca = Pipeline([<br/>    ('std_scaler', sc),<br/>    ('pca', pca),<br/>    ('tsne', tsne)<br/>])</span><span id="47e0" class="nq mf it oh b gy op om l on oo">X_tsne = tsne_after_pca.fit_transform(X)</span></pre><p id="720d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们将t-SNE应用于虹膜数据集。它只有4个特点。因此，我们不需要在SNE霸王龙之前运行PCA。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等到加载python代码！</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/dd5ff22b288d9906e7bea3a99fda7392.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*dBLl36oRDUivuuwgGEuUZw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><h2 id="ac9e" class="nq mf it bd mg nr ns dn mk nt nu dp mo li nv nw mq lm nx ny ms lq nz oa mu ob bi translated">多维标度(MDS)</h2><p id="39da" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">MDA是另一种非线性降维技术，它试图保留实例之间的距离，同时降低非线性数据的维度。有两种类型的MDS算法:公制和非公制。Scikit-learn中的<strong class="lb iu"> MDS() </strong>类通过将<strong class="lb iu"> <em class="lv">公制</em> </strong> <em class="lv"> </em>超参数设置为True(对于公制类型)或False(对于非公制类型)来实现这两者。</p><p id="d0d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下代码实现了对Iris数据集的MDS。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等到加载python代码！</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/080f3a54f1ac0947ba1c12e8ce2c0183.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*PLriOUuXm8xPhVXWm3klkw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><h2 id="890f" class="nq mf it bd mg nr ns dn mk nt nu dp mo li nv nw mq lm nx ny ms lq nz oa mu ob bi translated">等距映射</h2><p id="19d0" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">该方法通过等距映射进行非线性降维。它是MDS或内核PCA的扩展。它通过计算<em class="lv">曲线</em>或<em class="lv">测地线</em>到其最近邻居的距离来连接每个实例，并减少维度。可以通过实现Scikit-learn中Isomap算法的<strong class="lb iu"> Isomap() </strong>类的<strong class="lb iu"> n_neighbors </strong>超参数来指定每个点要考虑的邻居数量。</p><p id="a6fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面的代码实现了Iris数据集的Isomap。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等到加载python代码！</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/a70d39ed98a6145f9929d4881117f0c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*rEQ19fEgZoanjPS-OKMseA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><h1 id="d93c" class="me mf it bd mg mh mi mj mk ml mm mn mo jz mp ka mq kc mr kd ms kf mt kg mu mv bi translated">其他方法</h1><p id="e1a0" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">在这一类别下，我们将讨论3种方法。这些方法仅保留数据集中最重要的特征，并移除冗余特征。因此，它们主要用于特征选择。但是，在选择最佳特征时，降维会自动发生！因此，它们也可以被认为是降维方法。这些方法将提高模型的精度分数，或提升高维数据集的性能。</p><h2 id="6e57" class="nq mf it bd mg nr ns dn mk nt nu dp mo li nv nw mq lm nx ny ms lq nz oa mu ob bi translated">反向消除</h2><p id="7a56" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">该方法通过递归要素消除(RFE)过程从数据集中消除(移除)要素。该算法首先尝试在数据集中的初始要素集上训练模型，并计算模型的性能(通常，分类模型的精度得分和回归模型的RMSE)。然后，该算法一次丢弃一个特征(变量)，在剩余的特征上训练模型，并计算性能分数。该算法重复消除特征，直到它检测到模型的性能分数有小的(或没有)变化，并停止在那里！</p><p id="3cd4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在Scikit-learn中，可以通过使用<strong class="lb iu">sk learn . feature _ selection</strong>模块中的<strong class="lb iu"> RFE() </strong>类来实现反向消除。该类的第一个参数应该是具有<strong class="lb iu"> fit() </strong>方法和<strong class="lb iu"> coef_ </strong>或<strong class="lb iu"> feature_importances_ </strong>属性的<strong class="lb iu"> <em class="lv">监督的</em> </strong> <strong class="lb iu"> <em class="lv">学习估计器</em> </strong>。第二个应该是要选择的特性的数量。根据Scikit-learn文档，如果我们不指定要选择的特征数量，则选择一半的特征(<strong class="lb iu"> n_features_to_select </strong>参数)。这种方法的一个主要限制是我们不知道要选择的特征的数量。在这些情况下，最好通过为<strong class="lb iu"> n_features_to_select </strong>指定不同的值来多次运行该算法。</p><p id="9628" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们在虹膜数据上训练逻辑回归模型，并通过反向特征消除来识别最重要的特征。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等到加载python代码！</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/3abbaaac5c17a7ca3173345209f9ea5a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*mrUkP_ieXze7P23uo2R5dw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="ef5f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从输出中，我们可以看到递归特征消除(RFE)算法已经从逻辑回归模型中消除了<strong class="lb iu">萼片长度(cm) </strong>。<strong class="lb iu">萼片长度(cm) </strong>是最不重要的特征。其余要素包含初始数据集中的原始值。如图所示，最好保留模型中的其他3个特征。</p><h2 id="72e9" class="nq mf it bd mg nr ns dn mk nt nu dp mo li nv nw mq lm nx ny ms lq nz oa mu ob bi translated">预选</h2><p id="3fab" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">这种方法可以认为是逆向淘汰的相反过程。该算法不是递归地消除要素，而是尝试针对数据集中的单个要素训练模型，并计算模型的性能(通常，分类模型的精度得分和回归模型的RMSE)。然后，该算法一次添加(选择)一个特征(变量)，根据这些特征训练模型，并计算性能分数。该算法重复添加特征，直到它检测到模型的性能分数有小的(或没有)变化，并停止在那里！</p><p id="6205" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在Scikit-learn中，没有用于前进特征选择的直接功能。相反，我们可以将<strong class="lb iu"> f_regression </strong>(用于回归任务)和<strong class="lb iu"> f_classif </strong>(用于分类任务)类与<strong class="lb iu"> SelectKBest </strong>类一起使用。<strong class="lb iu"> f_regression </strong>返回回归任务的标签/特征之间的F值。<strong class="lb iu"> f_classif </strong>返回分类任务的标签/特征之间的方差分析F值。这些F值可用于特征选择过程。<strong class="lb iu">选择测试</strong>根据基于要素F值的最高分数自动选择要素。<strong class="lb iu"> SelectKBest </strong>的<strong class="lb iu"> score_func </strong>参数应指定为<strong class="lb iu"> <em class="lv"> f_classif </em> </strong>或<strong class="lb iu"> <em class="lv"> f_regression </em> </strong>。<strong class="lb iu"> k </strong>参数定义了要选择的顶部特征的数量。</p><p id="8c87" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们对虹膜数据执行前向特征选择，并识别最重要的特征。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等到加载python代码！</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/4cf82f5fc4511bf298dec91037fb3254.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*TGPXoR_Wt0KslADoouhPnw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="74ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从输出中，我们可以看到正向特征选择过程选择了具有较高F值的<strong class="lb iu">萼片长度(cm) </strong>、<strong class="lb iu">花瓣长度(cm) </strong>和<strong class="lb iu">花瓣宽度(cm) </strong>。</p><p id="bb6e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了基于截止F值定义<strong class="lb iu"> k </strong>参数的值，我们可以使用下面两行代码。</p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="ff10" class="nq mf it oh b gy ol om l on oo">F_values = f_classif(X,y)[0]<br/>k = len([num for num in F_values if num &gt; 50])</span></pre><p id="a743" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这将计算大于50的F值的数量，并将其分配给<strong class="lb iu"> k </strong>。这与上面的实现完全相同。</p><h2 id="5cc3" class="nq mf it bd mg nr ns dn mk nt nu dp mo li nv nw mq lm nx ny ms lq nz oa mu ob bi translated">随机森林</h2><p id="31fe" class="pw-post-body-paragraph kz la it lb b lc mw ju le lf mx jx lh li my lk ll lm mz lo lp lq na ls lt lu im bi translated">随机森林是一种基于树的模型，广泛用于非线性数据的回归和分类任务。它还可以通过其内置的<strong class="lb iu"> feature_importances_ </strong>属性用于特征选择，该属性在训练模型时根据<strong class="lb iu">‘Gini’</strong>标准(内部节点分割质量的度量)计算每个特征的特征重要性分数。如果你有兴趣了解更多关于随机森林是如何进行预测的，你可以阅读我的“<a class="ae ky" rel="noopener" target="_blank" href="/random-forests-an-ensemble-of-decision-trees-37a003084c6c">随机森林——决策树的集合</a>”文章。</p><p id="5a6b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下Python代码实现了Iris数据的随机森林分类器，计算并可视化了要素重要性。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等到加载python代码！</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/1e4a7ed4b21067c19c0d719e4ff2dbb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/format:webp/1*Fh2PV8vsV3F7js_PzPKdJQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="8d96" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过查看特征重要性，我们可以决定放弃<strong class="lb iu">萼片宽度(cm) </strong>特征，因为它对制作模型的贡献不足。让我们看看如何！</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">等到加载python代码！</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/a8884def49c6a0cef85b2f14a3a74989.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*Z9VXVyVAroodq_KqRri7CA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)</p></figure><p id="ac97" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">sci kit-learn<strong class="lb iu">SelectFromModel</strong>仅选择重要性大于或等于指定阈值的特征。由<strong class="lb iu"> SelectFromModel </strong>返回的值可以用作随机森林分类器的新输入X，该分类器现在仅在所选特征上被训练！</p><pre class="kj kk kl km gt og oh oi oj aw ok bi"><span id="1d19" class="nq mf it oh b gy ol om l on oo">rf = RandomForestClassifier(n_estimators=100, max_depth=3,<br/>                            bootstrap=True, n_jobs=-1,<br/>                            random_state=0)</span><span id="ccec" class="nq mf it oh b gy op om l on oo">rf.fit(features_important, y)</span></pre></div><div class="ab cl ou ov hx ow" role="separator"><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz"/></div><div class="im in io ip iq"><p id="5894" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">今天的帖子到此结束。我的读者可以通过下面的链接注册成为会员，以获得我写的每个故事的全部信息，我将收到你的一部分会员费。</p><p id="ee24" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">【https://rukshanpramoditha.medium.com/membership】报名链接:<a class="ae ky" href="https://rukshanpramoditha.medium.com/membership" rel="noopener"/></p><p id="1909" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">非常感谢你一直以来的支持！下一个故事再见。祝大家学习愉快！</p><p id="6a81" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">特别感谢Unsplash上的<strong class="lb iu">卡尼·贝内迪克托娃</strong>，<strong class="lb iu"> </strong>为我提供了这篇文章的精美封面图片。</p><p id="af51" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="pb pc ep" href="https://medium.com/u/f90a3bb1d400?source=post_page-----dcb9500d388b--------------------------------" rel="noopener" target="_blank">鲁克山普拉莫迪塔</a><br/><strong class="lb iu">2021–04–14</strong></p></div></div>    
</body>
</html>