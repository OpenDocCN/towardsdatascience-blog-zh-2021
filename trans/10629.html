<html>
<head>
<title>Monocular BEV Perception with Transformers in Autonomous Driving</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自动驾驶中使用变压器的单目BEV感知</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/monocular-bev-perception-with-transformers-in-autonomous-driving-c41e4a893944?source=collection_archive---------0-----------------------#2021-10-12">https://towardsdatascience.com/monocular-bev-perception-with-transformers-in-autonomous-driving-c41e4a893944?source=collection_archive---------0-----------------------#2021-10-12</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="75b0" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><em class="ki">截至2021年末的学术文献和行业实践回顾</em></h2></div><p id="3c66" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">更新:</p><ul class=""><li id="16f9" class="lf lg it kl b km kn kp kq ks lh kw li la lj le lk ll lm ln bi translated">添加DETR3D，2021/11/07</li><li id="51ca" class="lf lg it kl b km lo kp lp ks lq kw lr la ls le lk ll lm ln bi translated">添加STSU，将图像转换成地图，2021/12/27</li></ul></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><p id="d20f" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">量产级自动驾驶需要可扩展的世界三维推理。随着自动驾驶汽车和其他交通代理在道路上移动，大多数时候推理不需要考虑高度，这使得鸟瞰图(BEV)成为一种足够的表示。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ma"><img src="../Images/ca0b36907a3c40feccfbfcd4f1487fd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bTYZSbMYEUNoExvobbdx2g.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">传统自动驾驶堆栈的极度简化架构(图片由作者提供)</p></figure><p id="3e1a" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">上图说明了一个<strong class="kl iu">传统的</strong>自动驾驶栈(为了简单起见，这里省略了本地化等很多方面)。在此图中，圆圈代表功能模块，并根据它们所在的空间进行颜色编码。绿色模块发生在2D，蓝色模块发生在BEV。只有相机感知发生在2D空间，或者更准确地说，在获得机载相机图像的<strong class="kl iu">透视空间</strong> e。它依赖于传感器融合和大量手工制作的规则来将2D检测提升到3D，可选地借助于来自雷达或激光雷达的3D测量。</p><blockquote class="mq mr ms"><p id="18c2" class="kj kk mt kl b km kn ju ko kp kq jx kr mu kt ku kv mv kx ky kz mw lb lc ld le im bi translated">这里我说传统至少有两个原因。首先，相机感知仍然发生在透视空间中(与最近的单目3D物体检测趋势相反，可以在<a class="ae mx" rel="noopener" target="_blank" href="/monocular-3d-object-detection-in-autonomous-driving-2476a3c7f57e">这里</a>找到对其的回顾)。第二，来自多模态传感器的结果以后期融合的方式进行融合(与早期融合相反，早期融合中传感器数据被送入神经网络进行数据驱动的关联和几何推理)。</p></blockquote><h1 id="c25a" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">BEV感知是相机感知的未来</h1><p id="d915" class="pw-post-body-paragraph kj kk it kl b km nq ju ko kp nr jx kr ks ns ku kv kw nt ky kz la nu lc ld le im bi translated">该图暗示，对于唯一的异常值(摄像机感知)来说，转向BEV将是非常有益的。首先，直接在BEV中执行相机感知可以直接与来自雷达或激光雷达等其他设备的感知结果相结合，因为它们已经在BEV中表示和使用。BEV空间中的感知结果也容易被下游组件使用，例如预测和规划。第二，单纯依靠手工制作的规则将2D观测提升到3D是不可扩展的。BEV表示有助于过渡到早期融合管道，使融合过程完全由数据驱动。最后，在只有视觉的系统中(没有雷达或激光雷达)，在BEV中执行感知任务几乎成为<strong class="kl iu">强制</strong>，因为在传感器融合中没有其他3D提示可用于执行这种视图转换。</p><div class="nv nw gp gr nx ny"><a rel="noopener follow" target="_blank" href="/monocular-birds-eye-view-semantic-segmentation-for-autonomous-driving-ee2f771afb59"><div class="nz ab fo"><div class="oa ab ob cl cj oc"><h2 class="bd iu gy z fp od fr fs oe fu fw is bi translated">面向自动驾驶的单目鸟瞰语义分割</h2><div class="of l"><h3 class="bd b gy z fp od fr fs oe fu fw dk translated">2020年BEV语义分割综述</h3></div><div class="og l"><p class="bd b dl z fp od fr fs oe fu fw dk translated">towardsdatascience.com</p></div></div><div class="oh l"><div class="oi l oj ok ol oh om mk ny"/></div></div></a></div><p id="1d70" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">一年前的2020年末，我写了一篇<a class="ae mx" rel="noopener" target="_blank" href="/monocular-birds-eye-view-semantic-segmentation-for-autonomous-driving-ee2f771afb59">评论博文</a>，总结了学术界关于<strong class="kl iu">单眼BEV感知</strong>的论文。该领域研究如何将单目图像提升到BEV空间进行感知任务。从那以后，我一直在更新我读过的更多的文章，以保持这篇博文的更新和相关性。这个领域的范围已经从语义分割稳步扩展到全景分割、对象检测，甚至其他下游任务，如预测或规划。</p><p id="6d4c" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">在过去的一年里，单眼BEV感知出现了三种方法。</p><ul class=""><li id="756c" class="lf lg it kl b km kn kp kq ks lh kw li la lj le lk ll lm ln bi translated"><strong class="kl iu"> IPM: </strong>这是基于平地假设的简单基线。Cam2BEV 也许不是第一部这样做的作品，但却是最近的相关作品。它使用IPM来执行特征变换，并使用CNN来校正不在2D路面上的3D对象的失真。</li><li id="5773" class="lf lg it kl b km lo kp lp ks lq kw lr la ls le lk ll lm ln bi translated"><strong class="kl iu"> Lift-splat </strong>:利用单深度估计提升至3D，并在BEV上进行splat。这股潮流是由<a class="ae mx" href="https://arxiv.org/abs/2008.05711" rel="noopener ugc nofollow" target="_blank">Lift-Splat-shot</a>发起的，后续还有<a class="ae mx" href="https://arxiv.org/abs/2006.11436" rel="noopener ugc nofollow" target="_blank"> BEV-Seg </a>、<a class="ae mx" href="https://arxiv.org/abs/2103.01100" rel="noopener ugc nofollow" target="_blank"> CaDDN </a>、<a class="ae mx" href="https://arxiv.org/abs/2104.10490" rel="noopener ugc nofollow" target="_blank"> FIERY </a>等诸多作品。</li><li id="0ae0" class="lf lg it kl b km lo kp lp ks lq kw lr la ls le lk ll lm ln bi translated"><strong class="kl iu"> MLP </strong>:使用MLP为视图转换建模。这一行是由<a class="ae mx" href="https://arxiv.org/abs/1906.03560" rel="noopener ugc nofollow" target="_blank"> VPN </a>发起，<a class="ae mx" href="https://arxiv.org/abs/2006.09917" rel="noopener ugc nofollow" target="_blank">渔网</a>，以及<a class="ae mx" href="https://arxiv.org/abs/2107.06307" rel="noopener ugc nofollow" target="_blank">HDP mapnet</a><strong class="kl iu"/>跟进。</li><li id="6e6b" class="lf lg it kl b km lo kp lp ks lq kw lr la ls le lk ll lm ln bi translated"><strong class="kl iu">变形金刚</strong>:使用基于<strong class="kl iu">注意力</strong>的变形金刚来模拟视图变换。或者更具体地说，基于<strong class="kl iu">交叉关注</strong>的变压器模块。这一趋势开始显示出最初的牵引力，因为自2020年年中以来，至少到目前为止，截至2021年底，《变形金刚》席卷了计算机视觉领域。</li></ul><p id="1593" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">在这篇评论性的博文中，我将关注最后一个趋势——使用变形金刚进行视图转换。</p><blockquote class="mq mr ms"><p id="534c" class="kj kk mt kl b km kn ju ko kp kq jx kr mu kt ku kv mv kx ky kz mw lb lc ld le im bi translated">几乎具有讽刺意味的是，许多文献中的论文，有些是在CV中的变形金刚兴起之前，有些是在最近的这波浪潮中，将它们的专用视图转换模块称为“视图变形金刚”。这使得在文献中寻找那些确实使用注意模块进行观点转换的人变得更加困难。</p><p id="902e" class="kj kk mt kl b km kn ju ko kp kq jx kr mu kt ku kv mv kx ky kz mw lb lc ld le im bi translated">为了避免混淆，在这篇博文的后面，我将使用大写的<strong class="kl iu">变形金刚</strong>来指代基于注意力的架构。也就是说，使用<em class="it">变形金刚</em>通过提升图像到BEV来执行视图<em class="it">转换</em>似乎是一个很好的双关语。</p></blockquote><h1 id="785c" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">使用变压器查看变换</h1><p id="90ac" class="pw-post-body-paragraph kj kk it kl b km nq ju ko kp nr jx kr ks ns ku kv kw nt ky kz la nu lc ld le im bi translated">变形金刚的一般架构已经在许多其他博客中进行了广泛的解释(例如著名的<a class="ae mx" href="https://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">插图变形金刚</a>)，因此我们在此不再赘述。由于全局注意机制，变形金刚更适合执行视图变换的工作。目标域中的每个位置到源域中的任何位置具有相同的距离，克服了CNN中卷积层的局部受限感受野。</p><h2 id="4559" class="on mz it bd na oo op dn ne oq or dp ni ks os ot nk kw ou ov nm la ow ox no oy bi translated">交叉关注与自我关注</h2><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi oz"><img src="../Images/60e49ef43479c1a3a9be5d88bdec58cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GYm1FYctpyEKCuVe76Kd3g.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">《变形金刚》中交叉注意力和自我注意力的运用(<a class="ae mx" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="ea30" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">变压器中有两种关注机制，编码器中的<strong class="kl iu">自关注</strong>和解码器中的<strong class="kl iu">交叉关注</strong>。它们之间的主要区别是查询Q。在自我注意中，Q、K、V输入是相同的，而在交叉注意中，Q与K和V在不同的域中。</p><p id="cbce" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">正如<a class="ae mx" rel="noopener" target="_blank" href="/illustrated-difference-between-mlp-and-transformers-for-tensor-reshaping-52569edaf89">我之前的博客</a>中所详述的，关注模块的输出形状与查询q相同，就此而言，自我关注可以被视为原始特征域中的特征助推器，而交叉关注则可以被视为跨域生成器。</p><blockquote class="mq mr ms"><p id="dff0" class="kj kk mt kl b km kn ju ko kp kq jx kr mu kt ku kv mv kx ky kz mw lb lc ld le im bi translated"><strong class="kl iu">交叉注意</strong>的想法其实是最初的注意机制，甚至早于变形金刚的创造。注意机制在ICLR 2015年的论文“<a class="ae mx" href="https://arxiv.org/abs/1409.0473" rel="noopener ugc nofollow" target="_blank">通过联合学习对齐和翻译</a>的神经机器翻译”中首次提到。原始NeurIPS 2017 Transformer论文“<a class="ae mx" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a>”的更具创新性的贡献实际上是用自我注意力模块取代了双向RNN编码器。这也许是为什么许多人在提到交叉注意力时仍然更喜欢注意力这个词而不是变形金刚的原因。更多精彩的叙述请见<a class="ae mx" href="https://datascience.stackexchange.com/a/85999" rel="noopener ugc nofollow" target="_blank">这里</a>。</p></blockquote><h2 id="f39e" class="on mz it bd na oo op dn ne oq or dp ni ks os ot nk kw ou ov nm la ow ox no oy bi translated">交叉注意力是你所需要的</h2><p id="ee71" class="pw-post-body-paragraph kj kk it kl b km nq ju ko kp nr jx kr ks ns ku kv kw nt ky kz la nu lc ld le im bi translated">《变形金刚》在CV中的许多最新进展实际上只是利用了<strong class="kl iu">自我关注</strong>机制，比如被大量引用的<strong class="kl iu"> ViT </strong> ( <a class="ae mx" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank">一幅图像值16x16个字:变形金刚在比例上的图像识别</a>，ICLR 2021)或者<strong class="kl iu"> Swin Transformer </strong> ( <a class="ae mx" href="arxiv.org/abs/2103.14030" rel="noopener ugc nofollow" target="_blank">使用移位窗口的分级视觉变形金刚</a>，Arxiv 2021/03)。它们作为主干特征提取器的增强。然而，考虑到在大规模生产车辆上典型的资源有限的嵌入式系统中部署通用变压器架构的困难，自我关注相对于得到良好支持的CNN的增量好处可能难以证明。在我们看到一些突破性的自我关注超过CNN之前，专注于CNN的行业应用(如量产自动驾驶)将是一个明智的选择。</p><p id="a868" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">另一方面，交叉注意有更充分的理由。将交叉注意力应用于计算机视觉的一项开创性研究是<strong class="kl iu"> DETR </strong> ( <a class="ae mx" href="https://arxiv.org/abs/2005.12872" rel="noopener ugc nofollow" target="_blank">用变形金刚进行端到端的物体检测</a>，ECCV 2020)。DETR最具创新性的部分之一是基于称为对象查询的固定数量槽的交叉注意力解码器。不同于原始的Transformer文件，其中每个查询被一个接一个地(自动回归地)馈送到解码器，这些查询被并行地(同时地)馈送到DETR解码器。查询的内容也是学习的，并且除了查询的数量之外，不必在训练之前指定。这些查询可以被视为一个空白的、预先分配的模板，用于保存对象检测结果，而交叉注意解码器负责填补空白。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi pa"><img src="../Images/c43b55ee948200afc61cc182b8a52db9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oBZmch7Wt0VRfbZnpEAWgw.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">DETR的交叉注意力解码器部分可以看作是一个跨域生成器(<a class="ae mx" href="https://arxiv.org/abs/2005.12872" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="6ef5" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">这引发了使用交叉注意解码器进行视图转换的想法。输入视图被馈入特征编码器(基于自我关注或基于CNN)，编码后的特征作为K和v，目标视图格式的查询Q可以被学习，只需要被光栅化为模板。Q的值可以与网络的其余部分一起学习。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi pb"><img src="../Images/3c11998356d6463ffbb26811563db670.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1c4AW7c7osPocB9KgvWgjg.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">DETR架构可适用于BEV转型(图片由作者提供)</p></figure><p id="00cf" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">在接下来的会议中，我们将回顾一些最相关的工作，并且我们还将深入探讨由<a class="ae mx" href="https://karpathy.ai/" rel="noopener ugc nofollow" target="_blank"> Andrej Karpathy </a>在<a class="ae mx" href="https://youtu.be/j0z4FweCy4M?t=2925" rel="noopener ugc nofollow" target="_blank">特斯拉AI日</a> (08/20/2021)分享的变压器在特斯拉FSD中的使用。</p></div><div class="ab cl lt lu hx lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="im in io ip iq"><h1 id="08ea" class="my mz it bd na nb pc nd ne nf pd nh ni jz pe ka nk kc pf kd nm kf pg kg no np bi translated"><strong class="ak"> PYVA (CVPR 2021) </strong></h1><p id="d242" class="pw-post-body-paragraph kj kk it kl b km nq ju ko kp nr jx kr ks ns ku kv kw nt ky kz la nu lc ld le im bi translated">PYVA ( <a class="ae mx" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Projecting_Your_View_Attentively_Monocular_Road_Scene_Layout_Estimation_via_CVPR_2021_paper.pdf" rel="noopener ugc nofollow" target="_blank">专注地投影您的视图:通过交叉视图变换进行单目道路场景布局估计</a>，CVPR 2021)是第一个明确提到交叉注意力解码器可用于视图变换以将图像特征提升到BEV空间的项目。与早期的单目BEV感知工作类似，PYVA对转换后的BEV特征执行道路布局和车辆分割。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi ph"><img src="../Images/677e586fcfd6f8e54cdb112fac39a319.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OhVhUFA6g6EbMEqaUm1o3w.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">PYVA的架构使用了MLP和交叉关注(<a class="ae mx" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Projecting_Your_View_Attentively_Monocular_Road_Scene_Layout_Estimation_via_CVPR_2021_paper.pdf" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="fbed" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">PYVA首先使用MLP将透视空间中的图像特征X提升到(所要求的)BEV空间中的X’。第二MLP将X’映射回图像空间X”，并使用X和X”之间的循环一致性损失来确保该映射过程保留尽可能多的相关信息。</p><p id="3cc5" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">PYVA使用的Transformer是一个<strong class="kl iu">交叉注意</strong>模块，查询Q要映射到BEV空间中的BEV特征X’，V和K都是透视空间中的输入X(如果忽略透视空间中X和X”的区别)。</p><p id="de8f" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">注意，在BEV空间中没有对X '的显式监管，而是由BEV空间中的下游任务损失隐式监管。</p><p id="4f93" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">在PYVA中，似乎是MLP做了视图转换的繁重工作，而交叉注意用于增强BEV中被提升的特征。然而，由于对BEV空间中生成的查询没有明确的监督，从技术上来说，很难将这两个组件的贡献分开。对此进行消融研究将有助于澄清这一点。</p><h1 id="ad2d" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">整洁(ICCV 2021)</h1><p id="507d" class="pw-post-body-paragraph kj kk it kl b km nq ju ko kp nr jx kr ks ns ku kv kw nt ky kz la nu lc ld le im bi translated">NEAT ( <a class="ae mx" href="https://arxiv.org/abs/2109.04456" rel="noopener ugc nofollow" target="_blank">端到端自动驾驶的神经注意力场</a>，ICCV 2021)在使用基于MLP的迭代注意力将图像特征提升到BEV空间之前，使用变压器来增强图像特征空间中的特征。本文的目标是可解释的、高性能的、端到端的自动驾驶，但我们在这里将只关注可解释的中间BEV表示的生成。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi pi"><img src="../Images/cca15fa026e2c4f1e7373d5adf15c800.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4bnC51_5JIzDom2llXjWGg.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">整洁的建筑(<a class="ae mx" href="https://arxiv.org/abs/2109.04456" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="53ec" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">编码器模块中使用的转换器基于自我关注。作者还承认“变压器可以从我们的编码器中移除，而不会改变输出维度，但我们将它包括在内，因为它根据我们的消融研究提供了改进”。正如我们上面所讨论的，配备自我关注模块的编码器可以被视为一个美化的主干，这不是本研究的重点。</p><p id="3e9c" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">最有趣的部分发生在神经注意力场(NEAT)模块。对于给定的输出位置(x，y)，使用MLP将输出位置和图像特征作为输入，生成与输入特征图像具有相同空间维度的注意图。注意力图然后用于点积原始图像特征，以生成给定输出位置的目标BEV特征<strong class="kl iu">。如果我们遍历所有可能的BEV网格位置，那么我们可以将NEAT模块的输出平铺到一个BEV特征图</strong>。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi pj"><img src="../Images/8f72e7fa9f399239df82664b58d10b39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dt1a3Q_UIYBh0pvmtQCPuQ.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">交叉注意模块vs神经注意场模块(图片由作者提供)</p></figure><p id="c508" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">这个简洁的模块非常类似于交叉注意机制。主要区别在于Q和K之间的相似性度量步骤被MLP代替。这里我们忽略了其他一些小细节，例如Softmax运算，以及值v的线性投影。从数学上讲，我们有以下MLP、交叉注意力和整洁的公式。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi pk"><img src="../Images/834c029f68d8838082d165527e153136.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lLNSRcGTnjFb1UHbGQVzSw.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">MLP、交叉注意力和整洁的区别(改编自<a class="ae mx" rel="noopener" target="_blank" href="/illustrated-difference-between-mlp-and-transformers-for-tensor-reshaping-52569edaf89">来源</a>)</p></figure><p id="4123" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">符号约定遵循我之前<a class="ae mx" rel="noopener" target="_blank" href="/illustrated-difference-between-mlp-and-transformers-for-tensor-reshaping-52569edaf89">关于MLP和变形金刚</a>的区别的博文。简而言之，很明显，NEAT保持了交叉注意机制的数据依赖性，但它不再具有交叉注意的排列不变性。</p><blockquote class="mq mr ms"><p id="1e23" class="kj kk mt kl b km kn ju ko kp kq jx kr mu kt ku kv mv kx ky kz mw lb lc ld le im bi translated">为了清楚地与交叉注意机制进行比较，在上面的讨论中省略了一个细节。在NEAT的实现中，MLP的输入不是完全成熟的图像特征c，而是不具有任何空间范围的全局汇集的c_i。采用迭代注意。作者认为，把比c_i维数高得多的图象特征c送入MLP要复杂得多。也许MLP的一次通过不足以补偿空间内容的损失，因此需要多次通过。本文没有提供这种设计选择的消融研究。</p></blockquote><p id="fa70" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">解码器部分还使用MLP来生成所查询位置(x，y)的期望语义含义。如果我们将净输出平铺到BEV特征图中，以特定位置的特征和位置坐标作为输入的MLP相当于BEV特征图上的1x1卷积，其中(x，y)连接到特征图。这个操作和<a class="ae mx" href="https://arxiv.org/abs/1807.03247" rel="noopener ugc nofollow" target="_blank"> CoordConv </a> (NeurIPS 2018)非常相似。这是利用BEV特征图进行下游BEV感知任务的相当标准的做法。我们甚至可以超越1x1卷积，通过堆叠的3x3卷积进一步提高性能，以增加BEV空间中的感受域。</p><p id="99ce" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">总之，NEAT使用交叉注意力的变体(MLP来代替相似性度量)来将相机图像提升到BEV空间。</p><h1 id="2089" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">STSU (ICCV 2021)</h1><p id="9df5" class="pw-post-body-paragraph kj kk it kl b km nq ju ko kp nr jx kr ks ns ku kv kw nt ky kz la nu lc ld le im bi translated"><strong class="kl iu"> STSU </strong> ( <a class="ae mx" href="https://arxiv.org/abs/2110.01997" rel="noopener ugc nofollow" target="_blank">车载图像结构化鸟瞰交通场景理解</a>，ICCV 2021)采用稀疏查询进行目标检测，沿袭了DETR的做法。STSU不仅能检测动态物体，还能检测静态道路布局。这是同一作者的<a class="ae mx" href="https://arxiv.org/abs/2012.03040" rel="noopener ugc nofollow" target="_blank"> BEV特征拼接</a>的后续工作，在<a class="ae mx" rel="noopener" target="_blank" href="/monocular-birds-eye-view-semantic-segmentation-for-autonomous-driving-ee2f771afb59">我的另一个博客</a>中评论了一篇关于BEV语义分割的论文。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi pl"><img src="../Images/6c16a896cf2176db2f96e1dded12181d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tL4wkR0bUH5RuakzIiaZVA.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">STSU的建筑(<a class="ae mx" href="https://arxiv.org/abs/2110.01997" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="96ca" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">STSU使用两组查询向量，一组用于中心线，一组用于对象。最有趣的是它对结构化道路布局的预测。车道分支包括几个预测头。</p><ul class=""><li id="791e" class="lf lg it kl b km kn kp kq ks lh kw li la lj le lk ll lm ln bi translated">检测头预测由某个查询向量编码的通道是否存在。</li><li id="164a" class="lf lg it kl b km lo kp lp ks lq kw lr la ls le lk ll lm ln bi translated">控制头预测R <a class="ae mx" href="https://en.wikipedia.org/wiki/B%C3%A9zier_curve" rel="noopener ugc nofollow" target="_blank">贝塞尔曲线</a>控制点的位置。</li><li id="2e09" class="lf lg it kl b km lo kp lp ks lq kw lr la ls le lk ll lm ln bi translated">关联头预测用于聚类的嵌入向量。</li><li id="323a" class="lf lg it kl b km lo kp lp ks lq kw lr la ls le lk ll lm ln bi translated">关联分类器接受2个嵌入向量，并判断中心线对是否关联。</li></ul><blockquote class="mq mr ms"><p id="2788" class="kj kk mt kl b km kn ju ko kp kq jx kr mu kt ku kv mv kx ky kz mw lb lc ld le im bi translated">贝塞尔曲线非常适合中心线，因为它允许我们用固定数量的2D点来建模任意长度的曲线。</p><p id="84e5" class="kj kk mt kl b km kn ju ko kp kq jx kr mu kt ku kv mv kx ky kz mw lb lc ld le im bi translated">在<strong class="kl iu"> LSTR </strong> ( <a class="ae mx" href="https://arxiv.org/abs/2011.04233" rel="noopener ugc nofollow" target="_blank">利用变压器进行端到端车道形状预测</a>，WACV 2011)中也使用了用于车道预测的变压器，其仍然在图像空间中。结构化的道路布局预测也可以在<a class="ae mx" href="https://arxiv.org/abs/2107.06307" rel="noopener ugc nofollow" target="_blank"> <strong class="kl iu"> HDMapNet </strong>(一个在线高清地图构建与评估框架</a>，CVPR 2021工作坊)中找到，没有使用变形金刚。</p></blockquote><h1 id="cee3" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">DETR3D (CoRL 2021)</h1><p id="6ec9" class="pw-post-body-paragraph kj kk it kl b km nq ju ko kp nr jx kr ks ns ku kv kw nt ky kz la nu lc ld le im bi translated">DETR3D <strong class="kl iu">、T21(<a class="ae mx" href="https://arxiv.org/abs/2110.06922" rel="noopener ugc nofollow" target="_blank">通过3D到2D查询从多视图图像进行3D对象检测</a>，CoRL 2021)也使用稀疏查询进行对象检测，遵循DETR的实践。类似于STSU，但DETR3D侧重于动态对象。查询在BEV空间中，并且它们使得DETR3D能够直接在BEV空间中操纵预测，而不是对图像特征进行密集变换。</strong></p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi pm"><img src="../Images/f8d300d48c6c368ae41dd47b0a56c2fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sE_ICEEUr_6um0ZNrgZZtA.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">DETR3D的架构(<a class="ae mx" href="https://arxiv.org/abs/2110.06922" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><p id="578e" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">BEV感知优于mono3D的一个优势在于相机重叠区域，在该区域中，物体更有可能被相机视野剪切。Mono3D方法必须基于来自每个摄像机视点的有限信息来预测每个摄像机中的裁剪对象，并依赖全局NMS来抑制冗余框。DETR3D专门评估了图像边界(约占整个数据集的9%)处的这种裁剪对象，并发现DETR3D比mono3D方法有显著的改进。这在特斯拉AI日也有报道。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi pn"><img src="../Images/73f34dd390bf661dfd9a41783e95d38e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BwwI_x5SlZ2AsMn8I16qxw.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">多凸轮预测优于单凸轮结果(<a class="ae mx" href="https://youtu.be/j0z4FweCy4M?t=3677" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><blockquote class="mq mr ms"><p id="9909" class="kj kk mt kl b km kn ju ko kp kq jx kr mu kt ku kv mv kx ky kz mw lb lc ld le im bi translated">DETR3D使用了几个技巧来提高性能。首先是对象查询的迭代优化。本质上，预测BEV中的bbox中心被重新投射回具有摄像机变换矩阵(内部和外部)的图像，并且多摄像机图像特征被采样和集成以改进查询。这个过程可以重复多次(本文中为6次)以提高性能。</p><p id="fd5f" class="kj kk mt kl b km kn ju ko kp kq jx kr mu kt ku kv mv kx ky kz mw lb lc ld le im bi translated">第二个技巧是使用预训练的mono3D网络主干来提高性能。对于基于变形金刚的BEV感知网络来说，初始化似乎非常重要。</p></blockquote><h1 id="7713" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated"><strong class="ak">将图像转换成地图(2021/10，Arxiv) </strong></h1><p id="a3bf" class="pw-post-body-paragraph kj kk it kl b km nq ju ko kp nr jx kr ks ns ku kv kw nt ky kz la nu lc ld le im bi translated"><a class="ae mx" href="https://arxiv.org/abs/2110.00966" rel="noopener ugc nofollow" target="_blank">将图像转换成贴图</a>注意到，无论图像像素的深度如何，图像(图像列)中的垂直扫描线与穿过BEV贴图中摄像机位置的极线之间都存在1–1的对应关系。这类似于<a class="ae mx" href="https://arxiv.org/abs/1811.08188" rel="noopener ugc nofollow" target="_blank"> OFT (BMVC 2019) </a>和<a class="ae mx" href="https://arxiv.org/abs/2003.13402" rel="noopener ugc nofollow" target="_blank"> PyrOccNet (CVPR 2020) </a>的想法，沿着投射回3D空间的光线在像素位置涂抹特征。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi po"><img src="../Images/fa281817a0cb074ccfe33364fbedf850.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wsHbyCsOJdid5ZaiI1SL1w.png"/></div></div></figure><p id="6638" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">在列方向使用轴向交叉注意力转换器和在行方向使用卷积大大节省了计算。</p><h1 id="1ef8" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">特斯拉的方法</h1><p id="7ca1" class="pw-post-body-paragraph kj kk it kl b km nq ju ko kp nr jx kr ks ns ku kv kw nt ky kz la nu lc ld le im bi translated">在2021年的特斯拉AI日，特斯拉揭示了为特斯拉FSD提供动力的神经网络的许多复杂的内部工作原理。最有趣的构建模块之一是一个被称为“图像到BEV转换+多相机融合”的模块。这个模块的中心是一个转换器模块，或者更具体地说，是一个<strong class="kl iu">交叉关注</strong>模块。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi pn"><img src="../Images/70ff3b92d530192451d6c7be226f2926.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4n7upPbcN00_3Rweva8lGw.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">特斯拉的FSD架构(<a class="ae mx" href="https://youtu.be/j0z4FweCy4M?t=3677" rel="noopener ugc nofollow" target="_blank">来源</a>)</p></figure><blockquote class="mq mr ms"><p id="f6dc" class="kj kk mt kl b km kn ju ko kp kq jx kr mu kt ku kv mv kx ky kz mw lb lc ld le im bi translated">你初始化一个你想要的输出空间大小的栅格，你用输出空间中的正弦和余弦的位置编码来平铺它，然后这些用MLP编码成一组<strong class="kl iu">查询</strong>向量，然后所有的图像和它们的特征也发出它们自己的<strong class="kl iu">键</strong>和<strong class="kl iu">值</strong>，然后查询键和值馈入多头自我注意(作者注:这实际上是交叉注意)。</p><p id="eb58" class="kj kk mt kl b km kn ju ko kp kq jx kr mu kt ku kv mv kx ky kz mw lb lc ld le im bi translated">— Andrej Karpathy，2021年特斯拉人工智能日，<a class="ae mx" href="https://youtu.be/j0z4FweCy4M?t=3677" rel="noopener ugc nofollow" target="_blank">来源</a></p></blockquote><p id="b978" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">虽然Andrej提到他们使用了多头自我注意，但他描述的显然是一种<strong class="kl iu">交叉注意</strong>机制，他幻灯片中右边的图表也指向了最初变形金刚论文中的交叉注意块。</p><p id="573a" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">这个视图转换中最有趣的部分是BEV空间中的查询。它由BEV空间中的栅格生成(空白、预分配的模板，如在DETR)，并与位置编码(PE)连接。还有一个<strong class="kl iu">上下文概要</strong>，使用位置编码平铺显示。该图没有显示如何生成上下文摘要并与位置编码一起使用的细节，但我认为有一个全局池，它折叠透视空间中的所有空间信息，以及一个平铺操作，它将这个1x1张量平铺在预定义的BEV网格上。</p><figure class="mb mc md me gt mf gh gi paragraph-image"><div role="button" tabindex="0" class="mg mh di mi bf mj"><div class="gh gi pp"><img src="../Images/77c0037da89d6ecb3cccf92a5e710b68.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FHw6ZPGCK8JGjLdqt8Wkgw.png"/></div></div><p class="mm mn gj gh gi mo mp bd b be z dk translated">使用交叉注意力转换器的图像到BEV转换的构建块和张量形状(图片由作者提供)</p></figure><p id="2023" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">在上图中，我根据我的理解，列出了视图转换模块中更详细的块(圆圈)和相应的张量及其形状(正方形)。BEV空间中的张量颜色编码为蓝色，核心交叉注意模块颜色编码为红色。希望这能帮助学术界感兴趣的读者在这个方向上更深入地挖掘。</p><h2 id="a5f0" class="on mz it bd na oo op dn ne oq or dp ni ks os ot nk kw ou ov nm la ow ox no oy bi translated">关于变形金刚vs MLP的最后一句话</h2><div class="nv nw gp gr nx ny"><a rel="noopener follow" target="_blank" href="/illustrated-difference-between-mlp-and-transformers-for-tensor-reshaping-52569edaf89"><div class="nz ab fo"><div class="oa ab ob cl cj oc"><h2 class="bd iu gy z fp od fr fs oe fu fw is bi translated">说明了张量整形的MLP和变压器之间的差异</h2><div class="of l"><h3 class="bd b gy z fp od fr fs oe fu fw dk translated">深入探究数学细节，并附有插图。</h3></div><div class="og l"><p class="bd b dl z fp od fr fs oe fu fw dk translated">towardsdatascience.com</p></div></div><div class="oh l"><div class="pq l oj ok ol oh om mk ny"/></div></div></a></div><blockquote class="mq mr ms"><p id="fa68" class="kj kk mt kl b km kn ju ko kp kq jx kr mu kt ku kv mv kx ky kz mw lb lc ld le im bi translated">(将图像提升到BEV空间)是数据相关的，很难对此组件进行固定的转换，因此为了解决这个问题，我们使用转换器来表示这个空间。—安德烈·卡帕西，2021年特斯拉人工智能日</p></blockquote><p id="3de7" class="pw-post-body-paragraph kj kk it kl b km kn ju ko kp kq jx kr ks kt ku kv kw kx ky kz la lb lc ld le im bi translated">Andrej还提到视图转换问题依赖于数据，他们选择了转换器。关于交叉注意在张量整形中的详细用法及其与MLP的区别，在<a class="ae mx" rel="noopener" target="_blank" href="/illustrated-difference-between-mlp-and-transformers-for-tensor-reshaping-52569edaf89">我之前的</a>博客中有详细介绍，有一些数学细节和具体图解。它还强调了为什么变压器的张量整形是数据依赖的，而MLP不是。</p><h1 id="07a7" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">外卖食品</h1><ul class=""><li id="04a9" class="lf lg it kl b km nq kp nr ks pr kw ps la pt le lk ll lm ln bi translated">变形金刚在学术界和工业界越来越受欢迎，用于视图转换。</li><li id="852f" class="lf lg it kl b km lo kp lp ks lq kw lr la ls le lk ll lm ln bi translated">正如在<a class="ae mx" rel="noopener" target="_blank" href="/illustrated-difference-between-mlp-and-transformers-for-tensor-reshaping-52569edaf89">我之前的博客</a>中所讨论的，尽管《变形金刚》的数据依赖性使其更具表现力，但这也使其难以训练，MLP的盈亏平衡点可能需要大量的数据、GPU和工程努力。</li><li id="ddd9" class="lf lg it kl b km lo kp lp ks lq kw lr la ls le lk ll lm ln bi translated">在大规模生产的自动驾驶汽车中，在资源有限的嵌入式系统中部署变压器也可能是一个重大挑战。特别是，当前的神经网络加速器或GPU针对卷积神经网络(例如，3x3卷积)进行了高度优化。</li></ul><h1 id="0189" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">承认</h1><p id="d9a8" class="pw-post-body-paragraph kj kk it kl b km nq ju ko kp nr jx kr ks ns ku kv kw nt ky kz la nu lc ld le im bi translated">我和<a class="ae mx" href="https://www.zhihu.com/people/gu-yi-62-83" rel="noopener ugc nofollow" target="_blank">一谷</a>进行了几轮讨论，他目前正在澳门大学做博士研究。我们的讨论促使我重新审视单眼BEV感知领域的最新趋势。</p><h1 id="5728" class="my mz it bd na nb nc nd ne nf ng nh ni jz nj ka nk kc nl kd nm kf nn kg no np bi translated">参考</h1><ul class=""><li id="9e07" class="lf lg it kl b km nq kp nr ks pr kw ps la pt le lk ll lm ln bi translated"><a class="ae mx" href="https://arxiv.org/abs/2109.04456" rel="noopener ugc nofollow" target="_blank"> <strong class="kl iu">整洁</strong>:端到端自动驾驶的神经注意场</a>，ICCV 2021</li><li id="5c85" class="lf lg it kl b km lo kp lp ks lq kw lr la ls le lk ll lm ln bi translated"><a class="ae mx" href="https://openaccess.thecvf.com/content/CVPR2021/papers/Yang_Projecting_Your_View_Attentively_Monocular_Road_Scene_Layout_Estimation_via_CVPR_2021_paper.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kl iu"> PYVA </strong>:用心投射你的视角:通过交叉视角变换的单目道路场景布局估计</a>，CVPR 2021</li><li id="5b9b" class="lf lg it kl b km lo kp lp ks lq kw lr la ls le lk ll lm ln bi translated">特斯拉人工智能日于2021年8月20日在Youtube上直播</li><li id="3066" class="lf lg it kl b km lo kp lp ks lq kw lr la ls le lk ll lm ln bi translated"><a class="ae mx" href="https://arxiv.org/abs/2008.05711" rel="noopener ugc nofollow" target="_blank"/></li><li id="7435" class="lf lg it kl b km lo kp lp ks lq kw lr la ls le lk ll lm ln bi translated"><a class="ae mx" href="https://arxiv.org/abs/2103.01100" rel="noopener ugc nofollow" target="_blank"> <strong class="kl iu"> CaDDN </strong>:用于单目3D物体检测的分类深度分布网络</a>，CVPR 2021口述</li><li id="d151" class="lf lg it kl b km lo kp lp ks lq kw lr la ls le lk ll lm ln bi translated"><a class="ae mx" href="https://arxiv.org/abs/2104.10490" rel="noopener ugc nofollow" target="_blank"><strong class="kl iu"/>:从周围单目摄像机鸟瞰未来实例预测</a>，ICCV 2021</li><li id="e825" class="lf lg it kl b km lo kp lp ks lq kw lr la ls le lk ll lm ln bi translated"><a class="ae mx" href="https://arxiv.org/abs/2006.11436" rel="noopener ugc nofollow" target="_blank"> <strong class="kl iu"> BEV-Seg </strong>:利用几何和语义点云进行鸟瞰语义分割</a>，CVPR 2020研讨会</li><li id="50bc" class="lf lg it kl b km lo kp lp ks lq kw lr la ls le lk ll lm ln bi translated"><a class="ae mx" href="https://arxiv.org/abs/2107.06307" rel="noopener ugc nofollow" target="_blank"> <strong class="kl iu">高清地图网</strong>:在线高清地图构建与评估框架</a>，CVPR 2021研讨会</li><li id="335d" class="lf lg it kl b km lo kp lp ks lq kw lr la ls le lk ll lm ln bi translated"><a class="ae mx" href="https://arxiv.org/abs/2005.12872" rel="noopener ugc nofollow" target="_blank"> <strong class="kl iu"> DETR </strong>:用变形金刚进行端到端的物体检测</a>，ECCV 2020</li><li id="35cd" class="lf lg it kl b km lo kp lp ks lq kw lr la ls le lk ll lm ln bi translated"><a class="ae mx" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">注意力是你所需要的全部</a>，NeurIPS 2017</li><li id="f6a7" class="lf lg it kl b km lo kp lp ks lq kw lr la ls le lk ll lm ln bi translated"><a class="ae mx" href="https://arxiv.org/abs/1409.0473" rel="noopener ugc nofollow" target="_blank">联合学习对齐和翻译的神经机器翻译</a>，ICLR 2015</li><li id="2ab3" class="lf lg it kl b km lo kp lp ks lq kw lr la ls le lk ll lm ln bi translated"><a class="ae mx" href="https://arxiv.org/abs/2010.11929" rel="noopener ugc nofollow" target="_blank"> <strong class="kl iu"> ViT </strong>:一张图像抵得上16x16字:大规模图像识别的变形金刚</a>，ICLR 2021</li><li id="ffba" class="lf lg it kl b km lo kp lp ks lq kw lr la ls le lk ll lm ln bi translated"><a class="ae mx" href="arxiv.org/abs/2103.14030" rel="noopener ugc nofollow" target="_blank"> <strong class="kl iu"> Swin变换器</strong>:使用移位窗口的分级视觉变换器</a>，Arxiv 2021/03</li><li id="ed92" class="lf lg it kl b km lo kp lp ks lq kw lr la ls le lk ll lm ln bi translated"><a class="ae mx" href="https://arxiv.org/abs/1807.03247" rel="noopener ugc nofollow" target="_blank"> <strong class="kl iu"> CoordConv: </strong>卷积神经网络的一个耐人寻味的失败和CoordConv解决方案</a>，NeurIPS 2018</li><li id="e545" class="lf lg it kl b km lo kp lp ks lq kw lr la ls le lk ll lm ln bi translated"><a class="ae mx" href="https://arxiv.org/abs/2110.01997" rel="noopener ugc nofollow" target="_blank"> <strong class="kl iu"> STSU </strong>:车载图像结构化鸟瞰交通场景理解</a>，ICCV 2021</li><li id="3627" class="lf lg it kl b km lo kp lp ks lq kw lr la ls le lk ll lm ln bi translated"><a class="ae mx" href="https://arxiv.org/abs/2110.06922" rel="noopener ugc nofollow" target="_blank"> <strong class="kl iu"> DETR3D </strong>:通过3D到2D查询从多视图图像中检测3D对象</a>，CoRL 2021</li><li id="5f69" class="lf lg it kl b km lo kp lp ks lq kw lr la ls le lk ll lm ln bi translated"><a class="ae mx" href="https://arxiv.org/abs/2110.00966" rel="noopener ugc nofollow" target="_blank"> <strong class="kl iu">将图像翻译成地图</strong> </a>，Arxiv 2021/10</li></ul></div></div>    
</body>
</html>