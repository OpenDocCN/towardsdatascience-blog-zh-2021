<html>
<head>
<title>Bayesian Hierarchical Modeling in PyMC3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">PyMC3中的贝叶斯分层建模</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/bayesian-hierarchical-modeling-in-pymc3-d113c97f5149?source=collection_archive---------2-----------------------#2021-08-24">https://towardsdatascience.com/bayesian-hierarchical-modeling-in-pymc3-d113c97f5149?source=collection_archive---------2-----------------------#2021-08-24</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><h2 id="d0b1" class="is it iu bd b dl iv iw ix iy iz ja dk jb translated" aria-label="kicker paragraph"><a class="ae ep" href="https://medium.com/tag/bayesian-statistics" rel="noopener">贝叶斯统计</a></h2><div class=""/><div class=""><h2 id="4044" class="pw-subtitle-paragraph ka jd iu bd b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr dk translated">治愈你模特的健忘症</h2></div><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ks"><img src="../Images/67997b9b7b08b6db188fd6034a904936.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uw-n_DSnqAtyFI9N"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">在<a class="ae li" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae li" href="https://unsplash.com/@robpumphrey?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Rob Pumphrey </a>拍摄的照片</p></figure><h1 id="ac80" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">两种方法的故事</h1><p id="0186" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">有时，当你试图在一个完全异构的数据集上进行机器学习时，你会面临选择的痛苦。作为我所说的异构数据集的一个例子，考虑一个简单的身高数据集。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj mx"><img src="../Images/3e0a1eccb13bb3509838de4fab930bee.png" data-original-src="https://miro.medium.com/v2/resize:fit:670/format:webp/1*nY6xNbZN-s3Jtag7BLcpCA.png"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="4e11" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">这个数据真正重要的特征是身高。然而，我们也可以在这里找到两个不同的群体:男性和女性。仅由男性组成的子数据集本身是同质的，仅由女性组成的子数据集也是如此，但是将两者放在一起会产生异质数据集。</p><p id="6bfe" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">给定一些异构数据，您可以:</p><ul class=""><li id="b975" class="nd ne iu md b me my mh mz mk nf mo ng ms nh mw ni nj nk nl bi translated">在完整的数据集上构建一个大模型，或者</li><li id="76d5" class="nd ne iu md b me nm mh nn mk no mo np ms nq mw ni nj nk nl bi translated">在数据集更小、更同质的部分上构建多个模型。</li></ul><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj nr"><img src="../Images/3f71996d44abd7e767ae0885329ae7a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SMk3zSntAm5dTRjsnwO8wQ.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><h2 id="477c" class="ns lk iu bd ll nt nu dn lp nv nw dp lt mk nx ny lv mo nz oa lx ms ob oc lz ja bi translated">利弊</h2><p id="36c6" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">构建一个大模型，也被称为<strong class="md je">(完全)池化</strong>，通常是最简单的方法:你把所有的样本放在一起，忘记不同的组。然而，如果大模型过于简单，它可能会忽略数据中的不同细微差别，从而导致<strong class="md je">不符合</strong>。当使用高度可解释的模型(如线性模型)时，这个问题会很快发生。如果您可以并且想要使用黑盒方法，如梯度增强，该模型可以自己发现并学习不同的子数据集，但代价是降低了可解释性。</p></div><div class="ab cl od oe hy of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="in io ip iq ir"><p id="a275" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">建立几个更小的模型似乎是一个更好的主意:你将数据集分割成不同的更同质的部分，并为每个部分建立一个更小的模型。由此产生的模型被称为<strong class="md je">非池化</strong>模型。这样，每个模型都可以很好地处理一小部分数据，并且它们应该一起形成一个不错的模型。这种方法的明显缺点是你必须适应许多模型，但是当小数据集变得<em class="ok">太小</em>时，一个更严重的问题出现了:<strong class="md je">过度适应</strong>。</p><p id="4435" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">为了说明这一点，假设我们的身高数据集由100个样本组成:97名女性和3名男性。我们现在想通过取身高的样本平均值来估计每组的平均身高。虽然女性的猜测应该是相当不错的，但男性的结果将非常不可靠，因为观察的数量很少。主要问题如下:</p><blockquote class="ol"><p id="f9f5" class="om on iu bd oo op oq or os ot ou mw dk translated">子模型是完全独立的。被训练的每个子模型不向其他子模型传递任何信息。在某种意义上，总模型忘记了它在每个子数据集上做什么。</p></blockquote><figure class="ov ow ox oy oz kx"><div class="bz fq l di"><div class="pa pb l"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">未冷却的模型。</p></figure><h1 id="b262" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">模特的健忘症</h1><p id="5172" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">虽然训练孤立的模型<em class="ok">在某些情况下可能</em>有意义，但在我看来，子模型共享知识往往更好。</p><p id="74d5" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">举个现实生活中的例子，在<a class="ae li" href="https://de.wikipedia.org/wiki/McDonald%E2%80%99s" rel="noopener ugc nofollow" target="_blank"> <em class="ok">小酒馆到德国金色M </em> </a>买个芝士汉堡的等待时间平均需要两分钟(这个数字是我编的)。如果我现在去法国，平均等待时间很可能与这两分钟相差不远。日本可能也是如此。当然，一些国家更快，一些国家平均更慢，但是不管是哪个国家，等待一个汉堡一个小时应该是一个相当罕见的例外。所以，如果我饿了，想在法国吃一个芝士汉堡，我会想到要等多久，也许我是对的。我可以将从一个小组(德国)学到的知识转移到另一个小组(法国)进行预测。<em class="ok">这同样适用于heights数据集。</em></p><h2 id="fcb3" class="ns lk iu bd ll nt nu dn lp nv nw dp lt mk nx ny lv mo nz oa lx ms ob oc lz ja bi translated">一个有用的默认</h2><p id="286b" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">当然，你也可以创造一些情景，让人们误以为记住了你学过的东西。在医生办公室的等待时间可能不会给你任何关于交通灯绿色阶段持续时间的指示，所以你不应该试图在这两个不相关的变量之间分享知识。</p><p id="2eae" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">然而，如果我们诚实，通常我们不会看到来自完全不相关和随机来源的数据。通常情况下，这是相当一致的，只是在国家、性别、商店等群体中有一些变化，因此</p><blockquote class="ol"><p id="74dd" class="om on iu bd oo op oq or os ot ou mw dk translated">我觉得分享知识往往是一个很好的默认。</p></blockquote><p id="f694" class="pw-post-body-paragraph mb mc iu md b me pc ke mg mh pd kh mj mk pe mm mn mo pf mq mr ms pg mu mv mw in bi translated">那么，如何让我们的模型有更好的记忆呢？或者说，换一种方式，如何教会子模型之间的沟通和协同工作？这有几种方法，例如，在神经网络的情况下，<em class="ok">参数</em>或<em class="ok">权重分配</em>，这里我不打算详细说明。</p><p id="4a19" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">然而，正如你从这篇文章的标题中所知道的，我们将转向贝叶斯，并学习如何做<strong class="md je">贝叶斯分层建模</strong>作为我们问题的可能解决方案。我们将再次使用<a class="ae li" href="https://docs.pymc.io/" rel="noopener ugc nofollow" target="_blank"> PyMC3 </a>，因为它是一个不错的包。</p></div><div class="ab cl od oe hy of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="in io ip iq ir"><p id="f924" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">如果您从未听说过贝叶斯统计或PyMC3，并想学习它，请查看我关于这个有趣主题的其他介绍性文章。</p><div class="ph pi gq gs pj pk"><a rel="noopener follow" target="_blank" href="/a-gentle-introduction-to-bayesian-inference-6a7552e313cb"><div class="pl ab fp"><div class="pm ab pn cl cj po"><h2 class="bd je gz z fq pp fs ft pq fv fx jd bi translated">贝叶斯推理的简明介绍</h2><div class="pr l"><h3 class="bd b gz z fq pp fs ft pq fv fx dk translated">了解频率主义者和贝叶斯推理方法之间的区别</h3></div><div class="ps l"><p class="bd b dl z fq pp fs ft pq fv fx dk translated">towardsdatascience.com</p></div></div><div class="pt l"><div class="pu l pv pw px pt py lc pk"/></div></div></a></div><div class="ph pi gq gs pj pk"><a rel="noopener follow" target="_blank" href="/conducting-bayesian-inference-in-python-using-pymc3-d407f8d934a5"><div class="pl ab fp"><div class="pm ab pn cl cj po"><h2 class="bd je gz z fq pp fs ft pq fv fx jd bi translated">使用PyMC3在Python中进行贝叶斯推理</h2><div class="pr l"><h3 class="bd b gz z fq pp fs ft pq fv fx dk translated">重温硬币的例子，并使用PyMC3计算解决它。</h3></div><div class="ps l"><p class="bd b dl z fq pp fs ft pq fv fx dk translated">towardsdatascience.com</p></div></div><div class="pt l"><div class="pz l pv pw px pt py lc pk"/></div></div></a></div><p id="d601" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated"><strong class="md je">走吧！</strong></p><h1 id="5703" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">PyMC3中的分层建模</h1><p id="db87" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">首先，我们将重新审视两者，在<strong class="md je">贝叶斯</strong>设置中的池化和非池化方法，因为它是</p><ol class=""><li id="56aa" class="nd ne iu md b me my mh mz mk nf mo ng ms nh mw qa nj nk nl bi translated">一个很好的练习，而且</li><li id="2a74" class="nd ne iu md b me nm mh nn mk no mo np ms nq mw qa nj nk nl bi translated">非池化和层次化(也称为<strong class="md je">部分池化</strong>或<strong class="md je">多级</strong>)的代码库非常相似。</li></ol><p id="e602" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">在我们开始之前，让我们创建一个数据集进行实验。我们将创建一个简单的一维回归问题，即只有一个特征和一个目标。有八个不同的组，每个组都有自己的斜率，固定截距为零。</p><pre class="kt ku kv kw gu qb qc qd bn qe qf bi"><span id="45e9" class="qg lk iu qc b be qh qi l qj qk">import numpy as np<br/><br/>np.random.seed(0) # to keep it reproducible<br/><br/>mean_slope = 2 # the 8 different slopes have a mean of 2<br/><br/>slopes = np.random.normal(mean_slope, size=8)<br/>groups = np.array(50*[0, 1, 2, 3, 4, 5, 6] + 5*[7])<br/><br/>x = np.random.randn(355)<br/>y = slopes[groups] * x + 0.1*np.random.randn(355)</span></pre><p id="e721" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated"><code class="fe ql qm qn qc b">groups</code>变量包含每个观察值所属的组。0到6组各有50个观察值。然后，最后是少数群体7，只有五个观察值。</p><p id="fc3a" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">作为一个小指南，要获得属于组2的元素，可以使用<code class="fe ql qm qn qc b">x[groups==2], y[groups==2]</code>。这个子数据集的斜率是<code class="fe ql qm qn qc b">slopes[2]</code>。</p><p id="8d35" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">为了增加趣味，让我们在这个少数群体中加入局外人。</p><pre class="kt ku kv kw gu qb qc qd bn qe qf bi"><span id="2d1c" class="qg lk iu qc b be qh qi l qj qk">y[-1] = 30 # the last 5 observations are from minority group 7<br/>y[-2] = 30</span></pre><p id="4369" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">导入我们最喜欢的贝叶斯库后</p><pre class="kt ku kv kw gu qb qc qd bn qe qf bi"><span id="5b32" class="qg lk iu qc b be qh qi l qj qk">import pymc3 as pm<br/>import arviz as az</span></pre><p id="f68c" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">我们可以开始建模了！</p><h2 id="86b1" class="ns lk iu bd ll nt nu dn lp nv nw dp lt mk nx ny lv mo nz oa lx ms ob oc lz ja bi translated">混合模型</h2><p id="56d5" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">我们将从忽略组开始，将这个数据集视为一个大块。PyMC3中不带截距的简单贝叶斯线性回归如下所示:</p><pre class="kt ku kv kw gu qb qc qd bn qe qf bi"><span id="6450" class="qg lk iu qc b be qh qi l qj qk">with pm.Model() as pooled_model:<br/>    slope = pm.Normal('slope', 0, 20)<br/>    noise = pm.Exponential('noise', 0.1)<br/>    <br/>    obs = pm.Normal('obs', slope*x, noise, observed=y)<br/>    <br/>    pooled_trace = pm.sample(return_inferencedata=True)<br/><br/>az.plot_posterior(pooled_trace, var_names=['slope'])</span></pre><p id="d1e0" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">输出将是</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj qo"><img src="../Images/1d5fc8395ea5ec67d1575a8a8213e0ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*aieRAMQBIh3JiDSdNmeMpw.png"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="e1cc" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">这基本上没用，因为我们只得到一个单一的斜率，但这并不奇怪。下一个最好的方法是为每组引入一个斜率。</p><h2 id="19f7" class="ns lk iu bd ll nt nu dn lp nv nw dp lt mk nx ny lv mo nz oa lx ms ob oc lz ja bi translated">无池模型</h2><pre class="kt ku kv kw gu qb qc qd bn qe qf bi"><span id="adaf" class="qg lk iu qc b be qh qi l qj qk">with pm.Model() as unpooled_model:<br/>    slope = pm.Normal('slope', 0, 20, shape=8)<br/>    noise = pm.Exponential('noise', 10)<br/>    <br/>    obs = pm.Normal('obs', slope[groups]*x, noise, observed=y)<br/>    <br/>    unpooled_trace = pm.sample(return_inferencedata=True)<br/><br/>az.plot_posterior(unpooled_trace, var_names=['slope'])</span></pre><p id="82d7" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">我们现在有八个不同的斜坡，都是独立训练的:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qp"><img src="../Images/f3b2e63aad4e8d33efe718c27ba34ed5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VkZ9alm6aY4PwwhlOjqfNg.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="fb00" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">我们也可以仅标绘94%的高密度区间(HDI ),即包含94%后位质量的短可信区间</p><pre class="kt ku kv kw gu qb qc qd bn qe qf bi"><span id="a52e" class="qg lk iu qc b be qh qi l qj qk">az.plot_forest(unpooled_trace, var_names=['slope'], combined=True)</span></pre><p id="ccdd" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">我们得到了</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qq"><img src="../Images/6b0ca707aab192f59eb8e08f2594ed54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fQIgzqzapHA2VPSaGZr1hA.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="6123" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">你可以看到第0组到第6组的斜率很小，而第7组的斜率很大。但这是完全错误的，因为我们所有的斜率都应该在值<strong class="md je">2</strong>左右。发生了什么事？简单:我们通过在最小的群体中引入异常值来欺骗模型。</p><p id="e61c" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">这正是我之前谈到的问题:组7的子模型没有机会，因为它不知道组0到组6中发生了什么。它不知道斜率通常在2左右。</p><p id="d637" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">所以，让我们来解决这个问题。</p><h2 id="fa61" class="ns lk iu bd ll nt nu dn lp nv nw dp lt mk nx ny lv mo nz oa lx ms ob oc lz ja bi translated">部分池化aka层次模型</h2><pre class="kt ku kv kw gu qb qc qd bn qe qf bi"><span id="faed" class="qg lk iu qc b be qh qi l qj qk">with pm.Model() as hierarchical_model:<br/>    mu_slope = pm.Normal('mu_slope', 0, 1) # hyperprior 1<br/>    sigma_slope = pm.Exponential('sigma_slope', 13) # hyperprior 2<br/>    <br/>    slope = pm.Normal('slope', mu_slope, sigma_slope, shape=8)<br/>    noise = pm.Exponential('noise', 10)<br/>    <br/>    obs = pm.Normal('obs', slope[groups]*x, noise, observed=y)<br/>    <br/>    hierarchical_trace = pm.sample(<br/>        return_inferencedata=True,<br/>        target_accept=0.995<br/>    )<br/><br/>az.plot_posterior(hierarchical_trace)</span></pre><p id="030d" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">那么，现在这是什么？在无池模型中，我们通过</p><pre class="kt ku kv kw gu qb qc qd bn qe qf bi"><span id="b812" class="qg lk iu qc b be qh qi l qj qk">slope = pm.Normal('slope', 0, 20, shape=8)</span></pre><p id="2a11" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">我们告诉模型，斜率应该在零左右，但是具有相当大的20的标准偏差。</p><p id="8374" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">在分层模型中，我们定义了所谓的<strong class="md je">超先验</strong>来为斜率先验找到更好的均值和标准差。重要的是:</p><pre class="kt ku kv kw gu qb qc qd bn qe qf bi"><span id="4dd2" class="qg lk iu qc b be qh qi l qj qk">mu_slope = pm.Normal('mu_slope', 0, 1)<br/>sigma_slope = pm.Exponential('sigma_slope', 13)<br/>    <br/>slope = pm.Normal('slope', mu_slope, sigma_slope, shape=8)</span></pre><p id="0933" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">我们用<code class="fe ql qm qn qc b">mu_slope</code>代替零，用<code class="fe ql qm qn qc b">sigma_slope</code>代替二十，就这么简单。这两个都是随机变量，我们可以用贝叶斯推理来学习。<code class="fe ql qm qn qc b">mu_slope</code>和<code class="fe ql qm qn qc b">sigma_slope</code>被称为超优先级，例如，决策树的最大深度被称为超参数。它们都比<code class="fe ql qm qn qc b">slope</code>高一个<em class="ok">层级</em>，因为在<code class="fe ql qm qn qc b">slope</code>可以计算之前，它们必须首先被评估。</p><blockquote class="qr qs qt"><p id="fa35" class="mb mc ok md b me my ke mg mh mz kh mj qu na mm mn qv nb mq mr qw nc mu mv mw in bi translated">注意:我在<code class="fe ql qm qn qc b">sample</code>方法中添加了<code class="fe ql qm qn qc b">target_accept=0.995</code>来改进采样，因为从这个嵌套的后验模型中采样不再像对非pool模型那样容易。我可以在另一篇文章中进一步解释这一点。</p></blockquote><p id="4092" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">后验看起来像这样:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qp"><img src="../Images/c4547554da0e51e4d37d44e6180f5e26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tDb95UIGeRIi_5JWsPuGuQ.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj gk"><img src="../Images/6a05bdb7d7ae85e70c4f10296e1a189d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FYLou6Wb0NcKGuOZoKbvww.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="6b98" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">这仍然不是完美的，但比无池模型更接近事实。这是因为该模型还试图使用<code class="fe ql qm qn qc b">mu_slope</code>超级函数来计算斜率平均值。真实的先验应该是2，然而，由于异常值，模型认为它大约是3。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj qo"><img src="../Images/e9a1a87e0fc3c0af5b6aa9e5efa8cd00.png" data-original-src="https://miro.medium.com/v2/resize:fit:880/format:webp/1*Tk9TQ_A43wscWiHT6ymelg.png"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="1528" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">尽管如此，我们在非pool模型中看到的大斜率<strong class="md je">15</strong>将<em class="ok">拉向这个大约3的<code class="fe ql qm qn qc b">mu_slope</code>。其他参数也是如此:</em></p><blockquote class="ol"><p id="b9c7" class="om on iu bd oo op oq or os ot ou mw dk translated">在分层建模中，所有参数都被拉向全局平均值。这种效应被称为收缩。</p></blockquote><p id="321f" class="pw-post-body-paragraph mb mc iu md b me pc ke mg mh pd kh mj mk pe mm mn mo pf mq mr ms pg mu mv mw in bi translated">总而言之，层次模型是明显的赢家。作为最后一步，让我们使用<strong class="md je">标牌符号</strong>直观地比较三种型号。这是一个不言自明的模型可视化表示，可能对你们当中的视觉学习者有所帮助。</p><h1 id="5b23" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">使用平板符号比较模型</h1><p id="87ee" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">PyMC3有一个奇妙的功能<code class="fe ql qm qn qc b">pm.model_to_graphviz(model)</code>以一种很好的方式显示你的模型。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj qx"><img src="../Images/40adce39c7da6fe0901d09ecb0374dfb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*v7dspfu3uZsWgn7wufKGyQ.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="e2a7" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">我喜欢这种表示，因为你可以清楚地看到模型的某种演变。在混合模型中，我们从单一斜率开始。然后我们增加斜率的数量，用数字为8的方框表示。</p><blockquote class="qr qs qt"><p id="a278" class="mb mc ok md b me my ke mg mh mz kh mj qu na mm mn qv nb mq mr qw nc mu mv mw in bi translated">这个符号是为斜坡画8个圆的捷径。我猜这个小盒子应该是一张桌子，上面叠放着8个盘子，我们从上面看，因此得名。</p></blockquote><p id="0bd3" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">这个过程的最后一步是在顶层引入一个新的变量层，即超优先级。</p><h1 id="43b2" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">结论</h1><p id="6bd4" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">在本文中，我们讨论了处理异构数据集时不同的建模方法。起初，我声称有两种方式:池化和非池化模型。很明显，我在撒谎，正如我们已经看到的分层建模一样。</p><p id="aa6f" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">一个有趣的观察是，您可以将分层建模看作是对非池化建模的<strong class="md je">推广。如果您将超优先级设置为某个<em class="ok">常量</em>随机变量，您将再次以无池方法结束。否则，层次模型通过跟踪全局参数来做一些不同的事情，例如<code class="fe ql qm qn qc b">mu_slope</code>，它充当我们想要估计的实际参数的种子。作为一个很好的副作用，我们也得到这些全局参数的估计。如果在不同的组中没有值得分享的东西，层次模型也会学习这个，这很好。这就是为什么<strong class="md je">我认为你应该尽可能使用层次化的方法，而不是非池化的方法</strong>:通常这没有坏处。</strong></p><p id="9a0d" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">当使用分层建模而不是无池建模时，唯一可能出错的是有更多的<strong class="md je">参数</strong>要估计，并且由于嵌套的参数，估计也变得更加困难。你必须使用一些技巧，比如改变MCMC采样参数或者引入所谓的<em class="ok">非中心</em>变量。</p><p id="c414" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">一个以<strong class="md je">为中心的</strong> ( <em class="ok">我称之为嵌套的</em>)变量类似于另外两个随机变量<code class="fe ql qm qn qc b">a</code>和<code class="fe ql qm qn qc b">b</code>的<code class="fe ql qm qn qc b">pm.Normal('X', a, b)</code>，这就是我们上面用过的。然而，在正态分布的特殊情况下，你也可以写<code class="fe ql qm qn qc b">a + b*pm.Normal('X', 0, 1)</code>，这从统计学的角度来看是等价的，但对于计算后验分布的MCMC算法来说是一个巨大的差异。</p><p id="bb65" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">尽管如此，如果你能克服计算问题，我认为分层建模通常是一条值得尝试的道路。</p><p id="6d80" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">作为一个额外的资源，看看托马斯·威奇和达内·埃尔伯斯写的这篇不错的博文。</p></div><div class="ab cl od oe hy of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="in io ip iq ir"><p id="8729" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">我希望你今天学到了新的、有趣的、有用的东西。感谢阅读！</p><p id="6038" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated"><strong class="md je">作为最后一点，如果你</strong></p><ol class=""><li id="0b7a" class="nd ne iu md b me my mh mz mk nf mo ng ms nh mw qa nj nk nl bi translated"><strong class="md je">想支持我多写点机器学习和</strong></li><li id="8e7e" class="nd ne iu md b me nm mh nn mk no mo np ms nq mw qa nj nk nl bi translated"><strong class="md je">无论如何，计划获得一个中等订阅量，</strong></li></ol><p id="272e" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated"><strong class="md je">为什么不做</strong> <a class="ae li" href="https://dr-robert-kuebler.medium.com/membership" rel="noopener"> <strong class="md je">通过这个环节</strong> </a> <strong class="md je">？这将对我帮助很大！😊</strong></p><p id="0a14" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">透明地说，给你的价格不变，但大约一半的订阅费直接归我。</p><p id="307b" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated"><strong class="md je">非常感谢，如果你考虑支持我！</strong></p><blockquote class="ol"><p id="29ab" class="om on iu bd oo op oq or os ot ou mw dk translated">如果你有任何问题，请在<a class="ae li" href="https://www.linkedin.com/in/dr-robert-k%C3%BCbler-983859150/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上给我写信！</p></blockquote></div></div>    
</body>
</html>