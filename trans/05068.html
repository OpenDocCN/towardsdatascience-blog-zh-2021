<html>
<head>
<title>EmbedRank: Simple Unsupervised Keyphrase Extraction using Sentence Embeddings</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用句子嵌入的简单无监督关键短语抽取</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/embedviz-simple-unsupervised-keyphrase-extraction-using-sentence-embeddings-97ed5e16ad00?source=collection_archive---------19-----------------------#2021-05-04">https://towardsdatascience.com/embedviz-simple-unsupervised-keyphrase-extraction-using-sentence-embeddings-97ed5e16ad00?source=collection_archive---------19-----------------------#2021-05-04</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a822" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated"><strong class="ak">研究论文E </strong>解说</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f661b084fb27495a4346649a0063dea4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pIAkmDgVtnT7fnvmXCc54w.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h1 id="c69e" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">背景</h1><p id="db44" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">关键词/关键短语提取是<strong class="ls iu">提取与底层文档</strong>相关的重要单词的任务。通过将文档索引为文档别名，它可以让您更快地搜索文档，甚至有助于对这些中心主题的给定文本进行分类。现在，这些可能是<strong class="ls iu">抽象的(来自书面文本之外的相关关键词)或提取的(书面文本中存在的相关关键词)</strong>本质上。两者都有各自的好处，但是在这篇博客中，我们将仔细阅读瑞士电信公司和EPFL的研究人员以无人监督的方式进行的关键短语提取的这项非常有趣的工作。作者特别选择了无监督的方式，因为它比有监督的方式具有优势和灵活性，如<strong class="ls iu">域外概括</strong>，不需要<strong class="ls iu">带有关键字的大型手工注释语料库</strong>等。</p><p id="4243" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">大多数当前的关键词提取系统在速度和为文档生成一些不相关和冗余的关键词方面具有局限性。在这里，本文作者用自己提出的<a class="ae mr" href="https://en.wikipedia.org/wiki/Unsupervised_learning" rel="noopener ugc nofollow" target="_blank"> <strong class="ls iu">无监督算法</strong> </a>解决了这两个问题。</p><p id="16f9" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">有了这个背景和介绍，让我们直接开始理解这个算法。</p><p id="5cc5" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated"><strong class="ls iu">他们提出了一个3步管道— </strong></p><ol class=""><li id="33df" class="ms mt it ls b lt mm lw mn lz mu md mv mh mw ml mx my mz na bi translated">基于词性序列，从文本中提取候选短语。更准确地说，它们遵循的模式是提取由零个或多个形容词后跟一个或多个名词组成的短语。<em class="nb">它可以被认为是一个正则表达式过滤器</em><strong class="ls iu"><em class="nb">JJ * NN+</em></strong><em class="nb">覆盖整个文档的文本。</em></li><li id="da0b" class="ms mt it ls b lt nc lw nd lz ne md nf mh ng ml mx my mz na bi translated">作为第二步的一部分，他们使用句子嵌入(Sent2Vec和Doc2Vec)将候选短语和原始文档嵌入到同一个高维向量空间中。</li><li id="09e0" class="ms mt it ls b lt nc lw nd lz ne md nf mh ng ml mx my mz na bi translated">最后，他们根据给定短语与原始文档的相关程度对每个候选关键短语进行排序，同时最小化他们选择作为最终集合一部分的冗余关键短语的数量。</li></ol><h1 id="97cf" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">Sent2Vec和Doc2Vec</h1><p id="f72f" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">虽然方法<strong class="ls iu"> Sent2Vec </strong>和<strong class="ls iu"> Doc2Vec </strong>都是流行的嵌入技术，允许我们将任意长度的输入嵌入到固定长度的向量表示中，但是它们有不同的训练方法。</p><p id="5ee8" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated"><strong class="ls iu"> Sent2Vec </strong>将句子嵌入定义为句子中出现的上下文单词嵌入的平均值，其中上下文单词嵌入不仅限于单字，还扩展到每个句子中出现的<em class="nb"> n </em>个单字。<a class="ae mr" href="https://pypi.org/project/sent2vec/" rel="noopener ugc nofollow" target="_blank"> <em class="nb">这里有一个python包，你可以用它来实现</em> </a>。</p><p id="bed3" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">在<strong class="ls iu"> Doc2Vec </strong>中，通过添加段落id来扩展word2vec，也作为输入中的一个组件。下图显示了同样的情况。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/f6810197a477b3e1381dccab14000cbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*Pbms2YQlYMs5dKUMgFTlBw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="4f32" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这里，<strong class="ls iu"> D </strong>是文档id，<strong class="ls iu"> w </strong>是来自某个窗口的文档的单词。我们连接表示并学习分类窗口的中心词。直观上，<strong class="ls iu"> w </strong>代表单词概念的单词向量，<strong class="ls iu"> D </strong> —文档向量代表文档的概念。同样你也可以认为Skipgram版本是一样的。</p><p id="d699" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">作者使用Sent2Vec和Doc2Vec的预训练模型，其中<strong class="ls iu"> Sent2Vec表示700维的词汇单元</strong>，而<strong class="ls iu"> Doc2vec表示300维的词汇单元</strong>。</p><h1 id="d943" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">最大边际关联</h1><p id="1b64" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在这种情况下，<a class="ae mr" href="https://www.cs.cmu.edu/~jgc/publication/The_Use_MMR_Diversity_Based_LTMIR_1998.pdf" rel="noopener ugc nofollow" target="_blank"> MMR </a>背后的核心思想是选择与底层文档相关的关键字，并且您选择的每个渐进式关键字与已经选择的关键字集具有最小的相似性。下面的等式显示了相同的数学表达式—</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/267e10e4e4b40e56ea53e34a50e3eff7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pUxFOXzx4zLU21WglC22Rw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae mr" href="http://www.cs.bilkent.edu.tr/~canf/CS533/hwSpring14/eightMinPresentations/handoutMMR.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="2541" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这里，等式中的第一项计算文档Q和不在所选短语中的每个候选短语D(i)之间的余弦相似度，由参数λ正则化。等式中的第二项计算每个候选短语和所选短语之间的余弦相似度，并选择最大相似度。也可以在这个位置选择min，avg fxn，而不是max。Max确保我们对相似性进行最大限度的惩罚。λ是有界的b/w [0，1]，其中，1将意味着没有多样性，并且将归结为仅基于余弦相似性的选择<em class="nb">(信息量)</em>，并且将意味着高精度，而λ= 0将意味着高多样性。这个正则化参数应该基于用例进行调整。理想情况下，应该选择0.45-0.65之间的λ值，以在两者之间保持最佳平衡。</p><p id="272b" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">我鼓励你也通读一下<a class="ae mr" href="http://www.cs.bilkent.edu.tr/~canf/CS533/hwSpring14/eightMinPresentations/handoutMMR.pdf" rel="noopener ugc nofollow" target="_blank">这份写得非常好的文件</a>。</p><p id="38e5" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">从下图中可以看出，没有多样性的结果(MMR)对于形态变化是多余的，而As，embe bead的++版本显然有助于减少这些形态相似的关键短语，同时形成最终集。作者选择多样性参数(λ)为0.5，给予相关性和多样性同等的权重。但是这个参数也可以由用户根据下游应用的要求来调整。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/931558f82fce7e01b884c69944ed06b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u3VJJ4aE2pE0vtPTJK1nZw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae mr" href="https://arxiv.org/pdf/1801.04470.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><h1 id="aa67" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">结果和评价</h1><p id="ab21" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">作者在三个常见的关键词提取数据集上测试了他们的方法—</p><ul class=""><li id="be22" class="ms mt it ls b lt mm lw mn lz mu md mv mh mw ml nk my mz na bi translated"><strong class="ls iu"> Inspec </strong>:由来自科学期刊摘要的<strong class="ls iu">2000篇短文组成。为了对其他算法进行标准测试，他们对来自其他方法的500个常见文档(测试集)进行了测试。</strong></li><li id="6c23" class="ms mt it ls b lt nc lw nd lz ne md nf mh ng ml nk my mz na bi translated"><strong class="ls iu"> DUC </strong>:由TREC-9 的<strong class="ls iu"> 308篇中篇报刊文章组成。它们专门提取原始文档的第一个标签中包含的文本(而不是标题和其他元数据中存在的内容)</strong></li><li id="b556" class="ms mt it ls b lt nc lw nd lz ne md nf mh ng ml nk my mz na bi translated"><strong class="ls iu"> NUS </strong>:由<strong class="ls iu"> 211篇长文件(全科学会议论文)</strong>组成，4到12页不等。每个文档都有一组由作者和其他贡献者创建和注释的关键短语。他们根据这些来评估他们的提取物。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/61b8085d46f9d93fb24fdd03819979a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TQCTB_Ldj7BJ9zqPj_bX4w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">修改自<a class="ae mr" href="https://arxiv.org/pdf/1801.04470.pdf" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="f61f" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">上表展示了所有三个数据集的不同方法在<strong class="ls iu">精度、召回率和F1分数</strong>评估指标上的结果。很明显，在大多数情况下，恩贝拉德的表现都优于其他人。</p><p id="8ef7" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated"><em class="nb">如果你有兴趣了解更多关于关键词提取算法的知识，那么我有一篇文章讨论NLP中的</em> <a class="ae mr" href="https://medium.com/mlearning-ai/10-popular-keyword-extraction-algorithms-in-natural-language-processing-8975ada5750c" rel="noopener"> <em class="nb"> 10种流行的关键词提取算法</em> </a> <em class="nb">。一定要去看看！</em></p></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><p id="21d2" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">我也有一个<strong class="ls iu">多语言字幕的视频漫游</strong>，如果你喜欢看视频而不是文字(就像我一样:D)，一定要看看</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure></div><div class="ab cl nm nn hx no" role="separator"><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr ns"/><span class="np bw bk nq nr"/></div><div class="im in io ip iq"><p id="55f6" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">如果你仍然对某件事感到困惑，一定要看报纸。另外，向作者问好，感谢他们的贡献。</p><blockquote class="nv nw nx"><p id="839d" class="lq lr nb ls b lt mm ju lv lw mn jx ly ny mo mb mc nz mp mf mg oa mq mj mk ml im bi translated"><strong class="ls iu"> <em class="it">论文标题:</em> </strong> <em class="it">使用句子嵌入的简单无监督关键短语抽取</em></p><p id="ba0d" class="lq lr nb ls b lt mm ju lv lw mn jx ly ny mo mb mc nz mp mf mg oa mq mj mk ml im bi translated"><strong class="ls iu">论文链接:</strong><a class="ae mr" href="https://arxiv.org/pdf/1801.04470.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1801.04470.pdf</a></p><p id="e35e" class="lq lr nb ls b lt mm ju lv lw mn jx ly ny mo mb mc nz mp mf mg oa mq mj mk ml im bi translated"><strong class="ls iu"> <em class="it">作者:</em> </strong> <a class="ae mr" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bennani-Smires%2C+K" rel="noopener ugc nofollow" target="_blank"> <em class="it">卡米尔·本纳尼-斯米尔</em></a><em class="it"/><a class="ae mr" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Musat%2C+C" rel="noopener ugc nofollow" target="_blank"><em class="it">克劳迪奥·穆萨特</em></a><em class="it"/><a class="ae mr" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hossmann%2C+A" rel="noopener ugc nofollow" target="_blank"><em class="it">安德雷·霍斯曼</em></a><em class="it"/><a class="ae mr" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Baeriswyl%2C+M" rel="noopener ugc nofollow" target="_blank"><em class="it">迈克尔·贝里斯维尔</em></a><em class="it"/><a class="ae mr" href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jaggi%2C+M" rel="noopener ugc nofollow" target="_blank"><em class="it">马丁</em></a></p></blockquote><p id="05b6" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">希望这本书值得你花时间去读！</p><p id="c319" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated"><em class="nb">谢谢。</em></p></div></div>    
</body>
</html>