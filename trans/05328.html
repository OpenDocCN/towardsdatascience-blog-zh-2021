<html>
<head>
<title>How to Use Transformer-based NLP Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用基于变压器的NLP模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-use-transformer-based-nlp-models-a42adbc292e5?source=collection_archive---------7-----------------------#2021-05-11">https://towardsdatascience.com/how-to-use-transformer-based-nlp-models-a42adbc292e5?source=collection_archive---------7-----------------------#2021-05-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/9659e10db95ee03a5e816c3e2e9d8dad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zRZcgPFRTS9sA5VCAmHSgw.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">由Julia Nikulski创建的图形显示了初学者指南中关于如何使用基于transformer的NLP模型的主题。由<a class="ae jg" href="https://www.flaticon.com" rel="noopener ugc nofollow" target="_blank"> Flaticon </a>制作的<a class="ae jg" href="https://creativemarket.com/Becris" rel="noopener ugc nofollow" target="_blank"> Becris </a>、<a class="ae jg" href="https://www.freepik.com" rel="noopener ugc nofollow" target="_blank"> Freepik </a>、<a class="ae jg" href="https://www.flaticon.com/authors/ultimatearm" rel="noopener ugc nofollow" target="_blank"> ultimatearm </a>、<a class="ae jg" href="https://www.flaticon.com/authors/monkik" rel="noopener ugc nofollow" target="_blank">monk</a>和<a class="ae jg" href="https://www.flaticon.com/authors/eucalyp" rel="noopener ugc nofollow" target="_blank">eucalype</a>图标。</p></figure><h2 id="bf30" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="a37f" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">理解和实现伯特、GPT-2和其他用于下游任务的NLP模型的初学者指南</h2></div><p id="17bc" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">几个月前，我开始从事一个涉及文本分类的项目。我以前只使用基本的NLP技术来准备文本数据和应用简单的ML算法进行分类。然而，我知道基于变压器的NLP模型(如BERT、GPT-3、T5和RoBERTa)可以实现最先进的(SOTA)结果。</p><p id="0a48" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">实际上，让我重新表述这一部分，以准确描述我当时的知识状态:我知道围绕GPT-3的发布和这种模型如何能够很好地自行生成文本有着<a class="ae jg" href="https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/" rel="noopener ugc nofollow" target="_blank">重大的轰动</a>。有人告诉我这个模型可以用于文本分类。我该如何“使用这个模型”并让它预测我的数据标签呢？我不知道。</p><p id="e02d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">有很多很棒的博客文章解释了变形金刚的技术复杂性，T2对比了目前可用的许多语言理解和语言生成模型，并提供了T4的教程和代码来实现特定的自然语言处理任务。然而，我缺乏基本的信息来理解如何使用这些NLP模型。</p><p id="c222" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，本文为<strong class="lj jt">提供了一个高层次的概述，传达了围绕<strong class="lj jt">的基础知识</strong>，这些模型如何工作，在哪里可以访问它们，以及如何将它们应用到您自己的数据集和NLP任务</strong>。它面向熟悉NLP基础知识，但不熟悉更复杂的语言模型和迁移学习的人。为了获得基础知识之外的更多细节，我提供了学术论文、博客文章和代码演练的链接和参考。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi md"><img src="../Images/0184ca29c5884cb06991813c0c114be4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LVwrZ5S1V4ikqYO0"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">劳伦·里奇蒙在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="2eab" class="mi mj jj bd mk ml mm mn mo mp mq mr ms ky mt kz mu lb mv lc mw le mx lf my mz bi translated"><strong class="ak"> 1。什么是变压器？</strong></h1><p id="ee7c" class="pw-post-body-paragraph lh li jj lj b lk na kt lm ln nb kw lp lq nc ls lt lu nd lw lx ly ne ma mb mc im bi translated">变压器<strong class="lj jt">是Vaswani等人在2017年<a class="ae jg" href="http://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">【1】</a>开发的一种神经网络架构</strong>。不涉及太多细节，这个模型架构由一个<strong class="lj jt">多头自关注机制结合一个编码器-解码器结构</strong>组成。它可以实现在评估分数(<a class="ae jg" href="https://machinelearningmastery.com/calculate-bleu-score-for-text-python/" rel="noopener ugc nofollow" target="_blank"> BLEU分数</a>)和训练时间方面都优于利用递归(<a class="ae jg" href="https://www.ibm.com/cloud/learn/recurrent-neural-networks" rel="noopener ugc nofollow" target="_blank"> RNN </a>)或卷积神经网络(<a class="ae jg" href="https://lionbridge.ai/articles/difference-between-cnn-and-rnn/" rel="noopener ugc nofollow" target="_blank"> CNN </a>)的各种其他模型的SOTA结果。</p><p id="f7ea" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">与其他神经网络(NN)结构相比，转换器的一个<strong class="lj jt">关键优势是，单词</strong>周围的<strong class="lj jt">更长距离的上下文以更高效的计算方式被考虑<a class="ae jg" href="http://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">【1</a>，<a class="ae jg" href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="noopener ugc nofollow" target="_blank">2】</a>。“使……更加困难”这一短语可以在“使登记或投票过程更加困难”这句话中找到，尽管“更加困难”是“使……更加困难”的一个相当遥远的从属词<a class="ae jg" href="http://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">【1】</a>。围绕单词的相关上下文的计算可以并行进行，节省了大量的训练资源。</strong></p><p id="f612" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">最初，Transformer是为NLP任务开发的，并由Vaswani等人应用于机器翻译<a class="ae jg" href="http://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">【1】</a>。然而，它也适用于<a class="ae jg" href="https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html" rel="noopener ugc nofollow" target="_blank">图像识别</a>和其他领域。关于变压器的确切机制和功能的更多细节，请看这些<strong class="lj jt">深入解释</strong>:</p><ul class=""><li id="95de" class="nf ng jj lj b lk ll ln lo lq nh lu ni ly nj mc nk nl nm nn bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/transformers-141e32e69591#:~:text=Transformers%20are%20a%20type%20of%20neural%20network%20architecture%20that%20have,a%20top%20professional%20Starcraft%20player.">变形金刚如何工作</a>作者<a class="no np ep" href="https://medium.com/u/d0de4109e381?source=post_page-----a42adbc292e5--------------------------------" rel="noopener" target="_blank">朱利亚诺·吉亚卡利亚</a></li><li id="fd46" class="nf ng jj lj b lk nq ln nr lq ns lu nt ly nu mc nk nl nm nn bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0">变形金刚图解指南——逐步解说</a>作者<a class="no np ep" href="https://medium.com/u/1bdc81ea939d?source=post_page-----a42adbc292e5--------------------------------" rel="noopener" target="_blank">迈克尔·皮</a></li><li id="e262" class="nf ng jj lj b lk nq ln nr lq ns lu nt ly nu mc nk nl nm nn bi translated"><a class="ae jg" href="https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04" rel="noopener">什么是变压器</a>由<a class="no np ep" href="https://medium.com/u/ad76d4149790?source=post_page-----a42adbc292e5--------------------------------" rel="noopener" target="_blank">格言</a>可知</li><li id="38c5" class="nf ng jj lj b lk nq ln nr lq ns lu nt ly nu mc nk nl nm nn bi translated"><a class="ae jg" href="http://jalammar.github.io/illustrated-transformer/" rel="noopener ugc nofollow" target="_blank">插图变压器</a>作者<a class="ae jg" href="http://jalammar.github.io/" rel="noopener ugc nofollow" target="_blank">杰伊·阿拉玛</a></li></ul><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nv"><img src="../Images/1ff1f72d6446a4906e220c420ac06f7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rr0PWmSbBvl1xCSE"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">马库斯·斯皮斯克在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="5c8b" class="mi mj jj bd mk ml mm mn mo mp mq mr ms ky mt kz mu lb mv lc mw le mx lf my mz bi translated">2.NLP模型如何利用Transformer架构，它们的不同之处是什么？</h1><p id="f71f" class="pw-post-body-paragraph lh li jj lj b lk na kt lm ln nb kw lp lq nc ls lt lu nd lw lx ly ne ma mb mc im bi translated">Transformer模型结构已经在很大程度上取代了其他NLP模型实现，比如RNNs<a class="ae jg" href="http://arxiv.org/abs/1910.10683" rel="noopener ugc nofollow" target="_blank">【3】</a>。当前的SOTA <strong class="lj jt"> NLP模型部分或整体使用变压器架构</strong>。GPT模型只使用了变换器结构(单向)<a class="ae jg" href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a>的解码器，而BERT是基于变换器编码器(双向)<a class="ae jg" href="http://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">【4】</a>。T5采用了与原始实现<a class="ae jg" href="http://arxiv.org/abs/1910.10683" rel="noopener ugc nofollow" target="_blank">【3】</a>非常相似的编码器-解码器转换器结构。这些通用架构在组成编码器或解码器的元素的数量和维度方面也有所不同(即，层数、隐藏大小以及它们采用的自关注头的数量<a class="ae jg" href="http://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">【4】</a>)。</p><p id="aaf9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">除了模型结构的这些变化外，<strong class="lj jt">语言模型在用于预训练的数据和任务方面也有差异</strong>，这将在下面的段落中进一步阐述。有关确切模型架构及其预培训设计的更多详细信息，您可以参考介绍这些模型的学术论文:</p><ul class=""><li id="529d" class="nf ng jj lj b lk ll ln lo lq nh lu ni ly nj mc nk nl nm nn bi translated"><a class="ae jg" href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a>:GPT</li><li id="8701" class="nf ng jj lj b lk nq ln nr lq ns lu nt ly nu mc nk nl nm nn bi translated">3:T5</li><li id="d631" class="nf ng jj lj b lk nq ln nr lq ns lu nt ly nu mc nk nl nm nn bi translated">[ <a class="ae jg" href="https://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank"> 4 </a> ]:伯特</li><li id="87c4" class="nf ng jj lj b lk nq ln nr lq ns lu nt ly nu mc nk nl nm nn bi translated"><a class="ae jg" href="https://arxiv.org/pdf/2001.00781.pdf" rel="noopener ugc nofollow" target="_blank">【5】</a>:各种NLP模型及其差异的详尽概述</li></ul><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nw"><img src="../Images/3a166bc434799857dc43197974923c69.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Cb-wdGKEPJwUcbDV"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">照片由<a class="ae jg" href="https://unsplash.com/@timmossholder?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">蒂姆·莫斯霍尔德</a>在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="5ede" class="mi mj jj bd mk ml mm mn mo mp mq mr ms ky mt kz mu lb mv lc mw le mx lf my mz bi translated">3.什么是迁移学习、预培训和微调？</h1><p id="c549" class="pw-post-body-paragraph lh li jj lj b lk na kt lm ln nb kw lp lq nc ls lt lu nd lw lx ly ne ma mb mc im bi translated">许多基于Transformer的NLP模型是专门为迁移学习而创建的[ <a class="ae jg" href="http://arxiv.org/abs/1910.10683" rel="noopener ugc nofollow" target="_blank"> 3 </a>，<a class="ae jg" href="http://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">4】</a>。<strong class="lj jt">迁移学习</strong>描述了一种方法，其中首先使用自我监督学习<a class="ae jg" href="https://arxiv.org/pdf/2001.00781.pdf" rel="noopener ugc nofollow" target="_blank">【5】</a>在大型无标签文本语料库上对模型进行预训练。然后在对特定NLP(下游)任务<a class="ae jg" href="http://arxiv.org/abs/1910.10683" rel="noopener ugc nofollow" target="_blank">【3】</a>进行微调的过程中对其进行最小程度的调整。特定NLP任务的标注数据集通常相对较小。与预训练版本<a class="ae jg" href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a>相比，仅在如此小的数据集上训练模型而不进行预训练会降低结果。相同的预训练模型可以用于微调各种NLP下游任务，包括文本分类、摘要或问题回答。</p><p id="f2b9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">各种未标记的数据源可用于<strong class="lj jt">预训练</strong>。只要数据集足够大，它们<strong class="lj jt"> </strong>在微调期间可以与数据或任务完全无关。使用40 GB的文本[ <a class="ae jg" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="noopener ugc nofollow" target="_blank"> 6 </a> ]对GPT-2进行预训练。因此，预训练非常耗费时间和资源，通常需要使用多个GPU，耗时数小时甚至数天。在预培训期间实施的数据集和学习目标在不同的模型之间有很大的不同。GPT使用标准语言建模目标<a class="ae jg" href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a>，预测句子中的下一个单词，而伯特则接受掩蔽语言建模(MLM)和下一句预测(NSP)<a class="ae jg" href="http://arxiv.org/abs/1910.10683" rel="noopener ugc nofollow" target="_blank">【4】</a>的训练。RoBERTa模型复制了BERT模型架构，但使用更多数据改变了预训练，训练时间更长，并删除了NSP目标<a class="ae jg" href="https://arxiv.org/pdf/1907.11692.pdf" rel="noopener ugc nofollow" target="_blank">【7】</a>。</p><p id="a06f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">预训练模型的模型检查点作为<strong class="lj jt">微调</strong>的起点。特定下游任务的标记数据集用作训练数据。有几种不同的<a class="ae jg" href="https://www.analyticsvidhya.com/blog/2020/07/transfer-learning-for-nlp-fine-tuning-bert-for-text-classification/" rel="noopener ugc nofollow" target="_blank">微调方法</a>，包括以下几种:</p><ol class=""><li id="df99" class="nf ng jj lj b lk ll ln lo lq nh lu ni ly nj mc nx nl nm nn bi translated">根据标记的数据训练整个模型。</li><li id="d51f" class="nf ng jj lj b lk nq ln nr lq ns lu nt ly nu mc nx nl nm nn bi translated">只训练较高层，冻结较低层。</li><li id="7812" class="nf ng jj lj b lk nq ln nr lq ns lu nt ly nu mc nx nl nm nn bi translated">冻结整个模型并训练一个或多个添加在顶部的附加层。</li></ol><p id="d133" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在<a class="ae jg" href="http://arxiv.org/abs/1910.10683" rel="noopener ugc nofollow" target="_blank">【3】</a>中解释了另外两种类型的微调(适配器层和逐渐解冻)。无论采用哪种方法，<strong class="lj jt">特定于任务的输出层通常需要附加到模型</strong>上。如果您仅在新的文本数据集上微调模型，但学习目标保持不变，则不需要这一层。如果在特定的文本数据集上使用GPT-2进行语言生成，就会出现这种情况。</p><p id="2327" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">几篇介绍预训练的基于变压器的模型的研究论文也进行了微调实验，以展示它们的迁移学习性能。<strong class="lj jt">例如，BERT </strong>接受了11个NLP任务<strong class="lj jt">的训练，微调整个模型中的所有参数，并将输出提供给特定任务的输出层</strong>。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ny"><img src="../Images/4865b30dce0cf61336eaef923b4b08b4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*If7Kxf3VYhyNZSbO"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">杰斯·贝利在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="12cb" class="mi mj jj bd mk ml mm mn mo mp mq mr ms ky mt kz mu lb mv lc mw le mx lf my mz bi translated">4.除了迁移学习还有其他方法吗？</h1><p id="e06c" class="pw-post-body-paragraph lh li jj lj b lk na kt lm ln nb kw lp lq nc ls lt lu nd lw lx ly ne ma mb mc im bi translated"><strong class="lj jt">多任务学习</strong>旨在提高跨任务和领域的模型泛化能力<a class="ae jg" href="https://arxiv.org/pdf/2007.16008.pdf" rel="noopener ugc nofollow" target="_blank"> 8 </a>。同一个模型可以执行各种下游NLP任务，而不需要微调或改变模型参数或架构。GPT-2仅使用语言建模目标在大型和多样化的文本数据集上进行训练。对于各种NLP任务，它达到了SOTA结果，而没有对这些任务进行微调。在推理期间只提供了几个例子来帮助模型理解所请求的任务。</p><p id="d66c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">元学习</strong>特别适用于特定任务数据集只有少量样本的情况(低资源任务)。它使模型能够开发良好的初始化和广泛的技能，在推理过程中快速适应新的或以前看到的任务[ <a class="ae jg" href="http://arxiv.org/abs/2005.14165" rel="noopener ugc nofollow" target="_blank"> 9 </a>，<a class="ae jg" href="http://arxiv.org/abs/1908.10423" rel="noopener ugc nofollow" target="_blank"> 10 </a> ]。多任务学习和元学习之间的一个关键区别是，多任务学习的性能偏向于具有大数据集的任务。元学习能很好地适应任何任务。类似于多任务学习，元学习模型不需要微调。</p><p id="6608" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">零触发、一次触发和少量触发学习</strong>有时被称为元学习。这些规格指的是在推理时提供给模型[ <a class="ae jg" href="http://arxiv.org/abs/2005.14165" rel="noopener ugc nofollow" target="_blank"> 9 </a> ]的演示次数。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nz"><img src="../Images/fd60c86311cf23fa55c523de454d3160.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*00vl0l_4OytdParo"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">照片由<a class="ae jg" href="https://unsplash.com/@jazminantoinette?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Jazmin Quaynor </a>在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><h1 id="5d20" class="mi mj jj bd mk ml mm mn mo mp mq mr ms ky mt kz mu lb mv lc mw le mx lf my mz bi translated">5.在哪里可以获得这些模型？</h1><p id="4ed2" class="pw-post-body-paragraph lh li jj lj b lk na kt lm ln nb kw lp lq nc ls lt lu nd lw lx ly ne ma mb mc im bi translated">研究人员开发的大多数基于变压器的模型都是开源的(目前除了GPT-3)。你可以在GitHub上找到他们的模型实现(例如，<a class="ae jg" href="https://github.com/google-research/bert" rel="noopener ugc nofollow" target="_blank"> BERT </a>)。然而，如果您想要将模型应用到您自己的NLP任务中，您需要<strong class="lj jt">模型检查点</strong>。你可以通过 <a class="ae jg" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt">抱脸</strong> </a>提供的<a class="ae jg" href="https://huggingface.co/transformers/index.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt">变形金刚库</strong> </a> <strong class="lj jt">来访问这些。这个图书馆给你超过32个预先训练的SOTA模型。它提供了一个API，允许您使用PyTorch或TensorFlow轻松地将模型集成到代码中。</strong></p><p id="35dc" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">拥抱脸被谷歌、微软和脸书等组织使用。然而，<strong class="lj jt">它的变形金刚库对初学者来说也很棒</strong>。它有大量的文档和各种Google Colab笔记本，提供了使用<a class="ae jg" href="https://huggingface.co/transformers/model_doc/distilbert.html" rel="noopener ugc nofollow" target="_blank"> DistilBERT </a>进行文本分类的<a class="ae jg" href="https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/text_classification.ipynb#scrollTo=n9qywopnIrJH" rel="noopener ugc nofollow" target="_blank">示例实现</a>。他们还会带你通过<a class="ae jg" href="https://huggingface.co/transformers/training.html" rel="noopener ugc nofollow" target="_blank">更一般的例子</a>了解如何训练和微调一个模型。</p><p id="454e" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">有了<a class="ae jg" href="https://huggingface.co/transformers/main_classes/pipelines.html" rel="noopener ugc nofollow" target="_blank">管道</a>对象，你只需要写几行代码就可以对各种不同的任务做推理。这些管道利用已经微调过的模型，并且<a class="ae jg" href="https://huggingface.co/transformers/main_classes/pipelines.html" rel="noopener ugc nofollow" target="_blank">文档</a>解释了哪些模型与哪些特定于任务的管道一起工作。抱抱脸甚至创造了一个<a class="ae jg" href="https://huggingface.co/transformers/main_classes/pipelines.html#zeroshotclassificationpipeline" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jt">零拍分类流水线</strong> </a>。它接受任何文本输入和标签数量，并返回每个标签的概率。在微调过程中，没有必要看到这些标签中的任何一个(因此称为零触发)。</p><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi oa"><img src="../Images/eb0acd3c624296b40dc69fe3f2dc04a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*A1CMMvc4P_aeg9L6"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">照片由<a class="ae jg" href="https://unsplash.com/@dsmacinnes?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">丹尼尔·麦金尼斯</a>在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="ceb8" class="mi mj jj bd mk ml mm mn mo mp mq mr ms ky mt kz mu lb mv lc mw le mx lf my mz bi translated">6.如何将这些模型应用到NLP任务中？</h1><p id="2950" class="pw-post-body-paragraph lh li jj lj b lk na kt lm ln nb kw lp lq nc ls lt lu nd lw lx ly ne ma mb mc im bi translated">如果你知道你想要执行什么NLP任务，你将需要<strong class="lj jt">决定哪个基于Transformer的模型最适合这个目的</strong>。例如，GPT-2通过其单向架构为语言生成进行了预训练和设计，使其对语言生成任务最有用。但是，也可以对<a class="ae jg" href="https://huggingface.co/transformers/model_doc/gpt2.html#gpt2forsequenceclassification" rel="noopener ugc nofollow" target="_blank">序列分类</a>进行微调。</p><p id="fbd0" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">你可以<strong class="lj jt">查阅这些模型的原始学术论文，以了解它们针对哪些任务进行了微调，以及它们在哪些方面表现得特别好</strong>。你也可以查看<a class="ae jg" href="https://huggingface.co/transformers/index.html" rel="noopener ugc nofollow" target="_blank">拥抱脸变形金刚文档</a>，里面详细介绍了各种模型及其应用领域。</p><p id="ef55" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">选择型号后，您可以对其进行微调。或者，你可以<strong class="lj jt">选择一个已经调好的模型、多任务或元学习者，然后马上开始推理</strong>(见<a class="ae jg" href="https://huggingface.co/transformers/main_classes/pipelines.html" rel="noopener ugc nofollow" target="_blank">管道</a>拥抱脸的对象)。抛开拥抱脸提供的32+预训练模型，其社区提供了超过<a class="ae jg" href="https://huggingface.co/models" rel="noopener ugc nofollow" target="_blank">9000种模型规格</a>。</p><p id="d789" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">你将<strong class="lj jt">必须熟悉神经网络(NN)和深度学习至少一点点</strong>才能在大多数情况下微调模型。请记住，您通常会在预训练的模型中添加一个特定于任务的NN层，并实施一个训练过程。此外，您需要将数据集转换成正确的数据结构，以便与您的训练过程兼容。如果你还没有和NNs合作过，我建议你在这里阅读一下:</p><ul class=""><li id="0414" class="nf ng jj lj b lk ll ln lo lq nh lu ni ly nj mc nk nl nm nn bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/machine-learning-for-beginners-an-introduction-to-neural-networks-d49f22d238f9">初学者的机器学习:神经网络介绍</a>作者<a class="no np ep" href="https://medium.com/u/dd190d205cab?source=post_page-----a42adbc292e5--------------------------------" rel="noopener" target="_blank"> Victor Zhou </a></li><li id="a0ae" class="nf ng jj lj b lk nq ln nr lq ns lu nt ly nu mc nk nl nm nn bi translated"><a class="ae jg" href="https://www.kaggle.com/learn/intro-to-deep-learning" rel="noopener ugc nofollow" target="_blank">深度学习简介</a>关于<a class="ae jg" href="https://www.kaggle.com/" rel="noopener ugc nofollow" target="_blank"> Kaggle </a></li><li id="d44b" class="nf ng jj lj b lk nq ln nr lq ns lu nt ly nu mc nk nl nm nn bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/first-neural-network-for-beginners-explained-with-code-4cfd37e06eaf">第一个神经网络初学者讲解(附代码)</a>作者<a class="no np ep" href="https://medium.com/u/4aa81c733935?source=post_page-----a42adbc292e5--------------------------------" rel="noopener" target="_blank">阿瑟·阿恩克斯</a></li></ul><p id="f748" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为了<strong class="lj jt">开始一个特定的任务并微调你的模型</strong>，看一下这些用于各种下游NLP任务的示例实现:</p><ul class=""><li id="8534" class="nf ng jj lj b lk ll ln lo lq nh lu ni ly nj mc nk nl nm nn bi translated"><a class="ae jg" href="https://medium.com/swlh/fine-tuning-gpt-2-for-magic-the-gathering-flavour-text-generation-3bafd0f9bb93" rel="noopener">为万智牌风味文本生成微调GPT-2</a>作者<a class="no np ep" href="https://medium.com/u/c972b9933eed?source=post_page-----a42adbc292e5--------------------------------" rel="noopener" target="_blank">理查德·鲍恩斯</a></li><li id="d0cf" class="nf ng jj lj b lk nq ln nr lq ns lu nt ly nu mc nk nl nm nn bi translated"><a class="ae jg" rel="noopener" target="_blank" href="/fine-tuning-a-t5-transformer-for-any-summarization-task-82334c64c81">微调T5变压器用于任何摘要任务</a>由<a class="no np ep" href="https://medium.com/u/b040ce924438?source=post_page-----a42adbc292e5--------------------------------" rel="noopener" target="_blank"> Priya Dwivedi </a></li><li id="2a2c" class="nf ng jj lj b lk nq ln nr lq ns lu nt ly nu mc nk nl nm nn bi translated"><a class="ae jg" href="https://www.youtube.com/watch?v=wG2J_MJEjSQ" rel="noopener ugc nofollow" target="_blank">微调BERT，使用HuggingFace和PyTorch Lightning进行多标签文本分类|数据集</a>作者<a class="ae jg" href="https://www.youtube.com/channel/UCoW_WzQNJVAjxo4osNAxd_g" rel="noopener ugc nofollow" target="_blank">维尼林·瓦尔科夫</a></li></ul><figure class="me mf mg mh gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ob"><img src="../Images/cda95b6d8d21797c952100d2d56aefbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0bxc1K-h5GgWfv_O"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">照片由<a class="ae jg" href="https://unsplash.com/@jannerboy62?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">缺口缺口</a>在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">缺口</a>处拍摄</p></figure><h1 id="cd32" class="mi mj jj bd mk ml mm mn mo mp mq mr ms ky mt kz mu lb mv lc mw le mx lf my mz bi translated">结论</h1><p id="0ddf" class="pw-post-body-paragraph lh li jj lj b lk na kt lm ln nb kw lp lq nc ls lt lu nd lw lx ly ne ma mb mc im bi translated">起初，利用基于Transformer的NLP模型似乎令人望而生畏。已经发表了许多文章，非常详细地解释了与这些模型相关的概念。然而，在我深入这些细节之前，我想先有一个更广泛的概述。这是对与模型、训练过程以及将其应用于您自己的NLP用例相关的最重要元素的高级概述，希望能阐明如何使用它们。对于我的文本分类任务，我最终使用了<a class="ae jg" href="https://huggingface.co/transformers/model_doc/longformer.html" rel="noopener ugc nofollow" target="_blank"> Longformer </a>模型，并使用<a class="ae jg" href="https://www.pytorchlightning.ai/" rel="noopener ugc nofollow" target="_blank"> PyTorch Lightning </a>对其进行了微调。</p></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><p id="3b8a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">你想在媒体上阅读更多高质量的故事吗？考虑注册一个支持我和其他媒体作者的会员。</p><div class="is it gp gr iu oj"><a href="https://medium.com/@julia.nikulski/membership" rel="noopener follow" target="_blank"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd jt gy z fp oo fr fs op fu fw js bi translated">通过我的推荐链接加入Medium-Julia Nikulski</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">medium.com</p></div></div><div class="os l"><div class="ot l ou ov ow os ox ja oj"/></div></div></a></div></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><p id="5f4f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">你想开始使用基于Transformer的NLP模型，但是你还没有一个具体的项目想法吗？为什么不看看我的关于如何提出独特的数据科学项目想法的5步指南:</p><div class="is it gp gr iu oj"><a rel="noopener follow" target="_blank" href="/5-steps-to-develop-unique-data-science-project-ideas-6c2b3a0014b"><div class="ok ab fo"><div class="ol ab om cl cj on"><h2 class="bd jt gy z fp oo fr fs op fu fw js bi translated">开发独特数据科学项目创意的5个步骤</h2><div class="oq l"><h3 class="bd b gy z fp oo fr fs op fu fw dk translated">帮助您识别值得一试的新颖独特的数据项目的指南</h3></div><div class="or l"><p class="bd b dl z fp oo fr fs op fu fw dk translated">towardsdatascience.com</p></div></div><div class="os l"><div class="oy l ou ov ow os ox ja oj"/></div></div></a></div></div><div class="ab cl oc od hx oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="im in io ip iq"><h1 id="41d2" class="mi mj jj bd mk ml oz mn mo mp pa mr ms ky pb kz mu lb pc lc mw le pd lf my mz bi translated">学术论文参考文献</h1><p id="f7ca" class="pw-post-body-paragraph lh li jj lj b lk na kt lm ln nb kw lp lq nc ls lt lu nd lw lx ly ne ma mb mc im bi translated">[1] Vaswani，a .，Shazeer，n .，Parmar，n .，Uszkoreit，j .，Jones，l .，Gomez，A. N .，Kaiser，l .，&amp; Polosukhin，I. (2017)。你需要的只是关注。ArXiv:1706.03762 [Cs]。<a class="ae jg" href="http://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/1706.03762</a></p><p id="a2be" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[2]拉德福德、纳拉辛汉、萨利曼斯和苏茨基弗(2018年)。<em class="pe">通过生成性预训练提高语言理解</em>。<a class="ae jg" href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="noopener ugc nofollow" target="_blank">https://cdn . open ai . com/research-covers/language-unsupervised/language _ understanding _ paper . pdf</a></p><p id="e6c7" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[3] Raffel，c .，Shazeer，n .，Roberts，a .，Lee，k .，Narang，s .，Matena，m .，周，y .，Li，w .，，刘，P. J. (2020)。用统一的文本到文本转换器探索迁移学习的局限性。ArXiv:1910.10683 [Cs，Stat]。<a class="ae jg" href="http://arxiv.org/abs/1910.10683" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/1910.10683</a></p><p id="7f12" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[4] Devlin，j .，Chang，m-w .，Lee，k .，&amp; Toutanova，K. (2019年)。BERT:用于语言理解的深度双向转换器的预训练。ArXiv:1810.04805 [Cs]。<a class="ae jg" href="http://arxiv.org/abs/1810.04805" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/1810.04805</a></p><p id="8a30" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[5]阿恩马赫，m .，，霍伊曼，C. (2020年)。预训练语言模型的可比性。<em class="pe"> ArXiv:2001.00781 [Cs，Stat] </em>。<a class="ae jg" href="http://arxiv.org/abs/2001.00781" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/2001.00781</a></p><p id="c021" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[6]a .、吴j .、蔡尔德r .、栾d .、阿莫代伊d .、&amp;苏茨基弗I. (2019)。语言模型是无人监督的多任务学习者。<em class="pe"> OpenAI博客</em>，<em class="pe"> 1 </em> (8)，9。<a class="ae jg" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="noopener ugc nofollow" target="_blank">https://cdn . open ai . com/better-language-models/language _ models _ are _ unsupervised _多任务_学习者. pdf </a></p><p id="1cac" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[7]刘，y .，奥特，m .，戈亚尔，n .，杜，j .，乔希，m .，陈，d .，利维，o .，刘易斯，m .，泽特勒莫耶，l .，&amp;斯托扬诺夫，V. (2019)。RoBERTa:稳健优化的BERT预训练方法。ArXiv:1907.11692 [Cs]。<a class="ae jg" href="http://arxiv.org/abs/1907.11692" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/1907.11692</a></p><p id="8638" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[8] Worsham，j .，&amp; Kalita，J. (2020年)。面向21世纪20年代自然语言处理的多任务学习:我们要去哪里？<em class="pe">模式识别字母</em>、<em class="pe"> 136 </em>，120–126。<a class="ae jg" href="https://arxiv.org/pdf/2007.16008.pdf" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1016/j.patrec.2020.05.031</a></p><p id="1c1d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[9] Brown，T. B .，Mann，b .，Ryder，n .，Subbiah，m .，Kaplan，j .，Dhariwal，p .，Neelakantan，a .，Shyam，p .，Sastry，g .，Askell，a .，Agarwal，s .，Herbert-Voss，a .，Krueger，g .，Henighan，t .，Child，r .，Ramesh，a .，Ziegler，D. M .，Wu，j .，Winter，c .，Amodei，D. (2020年)。语言模型是一次性学习者。ArXiv:2005.14165 [Cs] 。http://arxiv.org/abs/2005.14165<a class="ae jg" href="http://arxiv.org/abs/2005.14165" rel="noopener ugc nofollow" target="_blank"/></p><p id="ee20" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[10]窦，张志勇，于，k .，，阿纳斯塔西索普洛斯，A. (2019).低资源自然语言理解任务的元学习算法研究。<em class="pe">ArXiv:1908.10423【Cs】</em>。<a class="ae jg" href="http://arxiv.org/abs/1908.10423" rel="noopener ugc nofollow" target="_blank">http://arxiv.org/abs/1908.10423</a></p></div></div>    
</body>
</html>