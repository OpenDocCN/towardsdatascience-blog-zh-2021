<html>
<head>
<title>7 of the Most Used Regression Algorithms and How to Choose the Right One</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">7种最常用的回归算法以及如何选择正确的算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/7-of-the-most-commonly-used-regression-algorithms-and-how-to-choose-the-right-one-fc3c8890f9e3?source=collection_archive---------1-----------------------#2021-06-21">https://towardsdatascience.com/7-of-the-most-commonly-used-regression-algorithms-and-how-to-choose-the-right-one-fc3c8890f9e3?source=collection_archive---------1-----------------------#2021-06-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="f0a9" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/getting-started" rel="noopener" target="_blank">入门</a></h2><div class=""/><div class=""><h2 id="2c0d" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">线性和多项式回归、RANSAC、决策树、随机森林、高斯过程和支持向量回归</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/27350f5618841ea7576bc3e7ca227d27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3T8Hbxo5C7IGOHgsMXNtyA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">回归算法—图片由作者提供</p></figure><p id="eb65" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">回归是<em class="md">监督学习</em>的子集。它根据训练数据集学习模型，对未知或未来数据进行预测。描述'<em class="md">监督的</em>'来自于目标输出值已经定义并且是训练数据的一部分。子类别回归和分类之间的差异仅取决于输出值。分类将数据集分为不同的类，而回归用于输出连续值。[Ras16]</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi me"><img src="../Images/2939ebf4947b491c79f9cf1df89da547.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kcBsTJsIiDEC9XcizmnmYg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">学习类型概述—作者图片</p></figure><p id="741a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">本文介绍了一些最常用的回归方法，解释了一些评估模型性能的指标，并描述了模型构建过程的工作原理。</p><ul class=""><li id="f61e" class="mf mg it lj b lk ll ln lo lq mh lu mi ly mj mc mk ml mm mn bi translated"><a class="ae mo" href="#b9a9" rel="noopener ugc nofollow"> <strong class="lj jd">回归方法</strong> </a> <br/> - <a class="ae mo" href="#b9a9" rel="noopener ugc nofollow">多元线性回归</a> <br/> - <a class="ae mo" href="#443b" rel="noopener ugc nofollow">多项式回归</a> <br/> - <a class="ae mo" href="http://3f73" rel="noopener ugc nofollow" target="_blank">稳健回归— RANSAC </a> <br/> - <a class="ae mo" href="#fc86" rel="noopener ugc nofollow">决策树</a> <br/> - <a class="ae mo" href="#e145" rel="noopener ugc nofollow">随机森林</a> <br/> - <a class="ae mo" href="#7f1c" rel="noopener ugc nofollow">高斯过程回归</a> <br/> - <a class="ae mo" href="#f3f7" rel="noopener ugc nofollow">支持向量回归</a></li><li id="0ede" class="mf mg it lj b lk mp ln mq lq mr lu ms ly mt mc mk ml mm mn bi translated"><a class="ae mo" href="#b419" rel="noopener ugc nofollow"> <strong class="lj jd">模型评估</strong> </a> <strong class="lj jd"> : </strong>如何评估生成的模型？</li><li id="6e78" class="mf mg it lj b lk mp ln mq lq mr lu ms ly mt mc mk ml mm mn bi translated"><a class="ae mo" href="#ad64" rel="noopener ugc nofollow"> <strong class="lj jd">模型建立流程</strong> </a> <strong class="lj jd"> : </strong>我如何为手头的问题找到最佳的回归方法和模型？</li></ul></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h1 id="b9a9" class="nb nc it bd nd ne nf ng nh ni nj nk nl ki nm kj nn kl no km np ko nq kp nr ns bi translated">1.回归方法</h1><h2 id="56b7" class="nt nc it bd nd nu nv dn nh nw nx dp nl lq ny nz nn lu oa ob np ly oc od nr iz bi translated">多元线性回归</h2><p id="a0cf" class="pw-post-body-paragraph lh li it lj b lk oe kd lm ln of kg lp lq og ls lt lu oh lw lx ly oi ma mb mc im bi translated">线性回归模型假设输入和输出变量之间的关系是线性的。这些模型非常简单，但在许多情况下提供了充分和易处理的关系表示。该模型旨在通过给定的输入数据<code class="fe oj ok ol om b">X = (x_1, x_2, …, x_p)</code>预测实际输出数据<code class="fe oj ok ol om b">Y</code>，并且具有以下形式:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi on"><img src="../Images/e0c02a779a86b4b19ccf0ffb6a9ebf8a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h5-KaLRPXN6vEl2_nr88xg.png"/></div></div></figure><p id="6372" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><em class="md"> β </em>描述最初未知的系数。具有一个以上输入变量<code class="fe oj ok ol om b">p &gt; 1</code>的线性模型称为多元线性回归模型。最著名的线性回归估计方法是<em class="md">最小二乘法</em>。在这种方法中，系数<em class="md"> β = β_0，β_1…，β_p </em>的确定方式使得<strong class="lj jd"> <em class="md">残差</em> </strong> <strong class="lj jd"> <em class="md">平方和(RSS) </em> </strong>最小。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oo"><img src="../Images/ed42ab716a37eb66f7b5642598a6af53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PM2oxwbrqFxTooVmc6l29w.png"/></div></div></figure><p id="f39c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这里，<code class="fe oj ok ol om b">y_i-f(x_i)</code>描述了残差，β_0是对<em class="md"> </em> <strong class="lj jd"> <em class="md">截距项</em> </strong>的估计，β_j是对<strong class="lj jd"> <em class="md">斜率参数</em></strong>【has 09，p.44】。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi op"><img src="../Images/ea348729cf324f62c6dc46723d22fa9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aI76gg7bUEKj5m7ZTkFuUQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">线性回归:截距项和回归系数—图片由作者提供</p></figure><p id="6654" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">多项式回归</strong></p><p id="2a0e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">通过变换输入变量，例如通过对数函数、根函数等。可以表示非线性和多项式关系。然而，这些都是线性模型，因为这一名称是基于输入参数的线性度。[Has09，第44页]</p><p id="a245" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这种相关性的建模是使用所谓的趋势模型来完成的。如果粗略的过程已经从数据中明显可见，可以指定回归方法。[Fah16，p.512]下表显示了简单线性回归常用的趋势模型。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oq"><img src="../Images/c3e24d9888f6b7f84a592f00ab4548ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5xOZNEtmwppsjkSpssMxkA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">全球趋势模型[Fah16，第512页]</p></figure><p id="cbff" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">多项式回归的直接函数不存在，至少在<code class="fe oj ok ol om b">Scikit-learn</code>中不存在。为了实现，使用了<code class="fe oj ok ol om b">pipeline</code>功能。该模块在一个链中结合了几种变换和估计方法，从而允许数据处理中的固定步骤序列。[Sci18g]</p><p id="3a17" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">与以前已知的线性回归的区别是初步步骤。函数<code class="fe oj ok ol om b">PolynomialFeatures</code>创建一个包含输入矩阵<code class="fe oj ok ol om b">X</code>特征的所有多项式组合的新矩阵。[Sci18h][Sci18]</p><p id="9813" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">以下代码片段:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="92b5" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">将<em class="md">输入向量</em> <code class="fe oj ok ol om b"><em class="md">X</em></code>变换如下:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ot"><img src="../Images/8dc2cb5f0bb35ad0ef974757b25c467f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Uct_VC_mfA5NysGcUYu9Jg.png"/></div></div></figure><p id="8109" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">线性模型的功能是:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ou"><img src="../Images/ccf895314effb4ee04906d4945441737.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qt0fhkA9IHxM08ZruSvPKA.png"/></div></div></figure><p id="90b7" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下面的片段展示了多项式回归在<code class="fe oj ok ol om b">scikit-learn</code>中的应用。这里<code class="fe oj ok ol om b">pipeline</code>功能并不是绝对必要的，输入矩阵<code class="fe oj ok ol om b">X</code>的变换和后续的模型建立也可以通过相应的命令一个接一个地执行。然而，如果在<em class="md">交叉验证</em>函数中应用多项式模型构建，则需要管道函数(更多信息请参见本文的评估部分)。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="a468" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><em class="md">多项式回归</em>允许通过<code class="fe oj ok ol om b">polynomial degree</code>控制模型复杂性。用户在算法执行前设置的参数称为<strong class="lj jd"> <em class="md">超参数</em> </strong>。大多数回归方法包括几个<em class="md">超参数</em>，它们会显著影响最终回归模型的准确性。您可以在“模型评估”一节中找到如何找到最佳<em class="md">超参数</em>的解释。</p><p id="1232" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">以下示例显示了多项式次数为2的多项式回归模型。该模型来自于预测铣床能耗的尝试。目标值y是能耗[kJ]，使用的属性是轴转速[1/min]和进给速度[mm/min]。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ov"><img src="../Images/f354c1a97b7f72bb32af53d981a83add.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cCnVGS84DdwHI5g271TEYg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">多项式回归:样本模型—作者图片</p></figure></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h2 id="3f73" class="nt nc it bd nd nu nv dn nh nw nx dp nl lq ny nz nn lu oa ob np ly oc od nr iz bi translated">稳健回归— RANSAC</h2><p id="f5b0" class="pw-post-body-paragraph lh li it lj b lk oe kd lm ln of kg lp lq og ls lt lu oh lw lx ly oi ma mb mc im bi translated">基于<em class="md">最小二乘估计</em>的回归程序非常容易受到异常值的影响，因为方差是以二次方式评估的。下图说明了单个异常值对线性回归结果的影响。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ow"><img src="../Images/1aa11ff326ae01ca24847f989b849cb9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9Ni6-r89F3hvdu1tq9peaw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">个别异常值对线性回归模型的影响—图片由作者提供</p></figure><p id="cacf" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">稳健的回归方法避开了这一弱点。术语“<em class="md">稳健性”</em>描述了静态方法对不符合正态分布的分布进行建模的能力。[Wie12]稳健性的一个衡量标准是所谓的“<em class="md">崩溃点”</em>，它表示统计方法容许的数据(如异常值)的比例。[Hub05]</p><p id="a35c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">最著名的稳健回归算法可能是由Martin Fischler和Robert Bolles于1981年推出的<em class="md">随机样本一致性</em> <em class="md"> (RANSAC) </em> <em class="md">算法。RANSAC广泛应用于机器视觉领域。[Fis80]</em></p><p id="dda8" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">该算法的操作可以用迭代执行的五个步骤来解释。(1)开始时，程序从数据中选择一个随机样本，并将其用于建模。在下图中，<strong class="lj jd"> <em class="md">样本</em> </strong>包括两个圈起来的数据点。(2)然后计算模型f(x)的所有数据点的误差，并与用户定义的阈值进行比较。如果偏差低于该值，该数据点被视为<strong class="lj jd"> <em class="md">内层</em> </strong>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ox"><img src="../Images/a68e7e724d5b6d42aaac840468235f98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YkvhW9ajl4Ern8KnoObgcg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">RANSAC算法—图片由作者提供</p></figure><p id="d5af" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">(3)重复该过程，直到已经运行了指定数量的迭代或者已经实现了模型的指定性能。作为模型结果是产生最多<em class="md">内联符</em>的函数。[Ras18]</p><p id="9986" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下面的Python片段描述了使用<code class="fe oj ok ol om b">scikit-learn</code>的实现。最大迭代次数设置为4，最小样本大小设置为2。这些值根据数据集的大小进行调整。为了确定<em class="md">内角</em>，计算数据点和回归线之间垂直距离的绝对值。[Ras18][Sci18b]</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="5d63" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下图显示了执行上述代码片段后迭代模型构建的示例。迭代2显示了模型的最佳性能。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oy"><img src="../Images/b4043156c594e839f800cc8ec8b51ee4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oKIlJkJ9k2VSr2HrKNcnpw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">RANSAC算法:模型构建过程的四次迭代(Min_Samples =2，threshold = 20)-图片由作者提供</p></figure><p id="af94" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">以某个<em class="md">概率p </em>从数据点中至少选择一次无离群子集所需的迭代次数<em class="md"> n </em>可以确定如下[Rod04][Dan18]:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oz"><img src="../Images/37a920d4a9544871c6ff97464b0320f0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZbgnOwKPzxdyWR2GCCDwuw.png"/></div></div></figure><p id="20a5" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">假设离群值δ的相对比例约为10%，以概率<code class="fe oj ok ol om b">p = 99%</code>选择至少一次无离群值子集的迭代次数<code class="fe oj ok ol om b">n</code>计算如下:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pa"><img src="../Images/70415dd98a33b4ec81519dde4a1c7a11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P9Qze298a4170nNVw_FZRw.png"/></div></div></figure></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h2 id="fc86" class="nt nc it bd nd nu nv dn nh nw nx dp nl lq ny nz nn lu oa ob np ly oc od nr iz bi translated">决策树和随机森林</h2><p id="ddc1" class="pw-post-body-paragraph lh li it lj b lk oe kd lm ln of kg lp lq og ls lt lu oh lw lx ly oi ma mb mc im bi translated"><em class="md">决策树</em>通过迭代分裂树节点来增长，直到“叶子”不再包含杂质或者达到终止条件。<em class="md">决策树</em>的创建从树根开始，以产生最大<strong class="lj jd"> <em class="md">信息增益IG </em> </strong>的方式分割数据。[Ras18，第107页][Aun18][Has09，第587页][May02][Sci18c]</p><p id="e08b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">一般来说，特征a的<em class="md">信息增益IG </em>定义如下[Qui86][Bel15，p.47][Ras18，p.107]:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pb"><img src="../Images/192e29bc6efa7cfaf9cd0fb64ef113aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AxhtLTQZDoQa2QV0YWLEbA.png"/></div></div></figure><p id="e8e5" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在二元<em class="md">决策树</em>中，通过<em class="md">属性</em> <code class="fe oj ok ol om b"><em class="md">a</em></code>将整个数据集<code class="fe oj ok ol om b">D_p</code>划分为<code class="fe oj ok ol om b">D_left</code>和<code class="fe oj ok ol om b">D_right</code>完成。因此，<em class="md">信息增益</em>定义为:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ou"><img src="../Images/fe19e2ffbce8d64a4340856b2ed89ec3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kk546gEo1E3bv42FZmHM9g.png"/></div></div></figure><p id="3ece" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">该算法以最大化<em class="md">信息增益</em>为目标，即该方法希望以最大程度减少<em class="md">子节点</em>中的<em class="md">杂质</em>的方式分割整个数据集。</p><p id="1445" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">分类使用<code class="fe oj ok ol om b">entropy</code>或<code class="fe oj ok ol om b">Gini coefficient</code>作为杂质的度量，回归使用<code class="fe oj ok ol om b">Mean Squared Error (MSE)</code>作为节点的<em class="md">杂质</em>的度量。[Ras18，第347页]。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pc"><img src="../Images/1cee39b479b82a88782f3f5e67b99a5c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*djKo7ESE_qIqEU-C7uNjYQ.png"/></div></div></figure><p id="25b3" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">使用<code class="fe oj ok ol om b">Mean Squared Error</code>确定杂质的分割方法也称为<em class="md">方差减少方法</em>。通常情况下，树的大小由<em class="md">最大节点数</em> <code class="fe oj ok ol om b">max_depth</code>控制，此时数据集的划分停止。[第09条，第307条]</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="8333" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">决策树的节点和叶子的可视化可以使用<code class="fe oj ok ol om b">graphviz</code>函数来完成:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="14c3" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下图显示了简单数据集的<em class="md">决策树</em>的结果。该方法以尽可能减少<em class="md">方差</em>的方式将数据集分成两个部分子集(左和右)。对于显示的数据集，数据集第一次分割的限制是6.5。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pd"><img src="../Images/1d0e43831f625941e02c5d6ca2b88772.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cKotpv2PDxnW3vGFy48NYw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">深度为1的简单二维案例的决策树—图片由作者提供</p></figure><h2 id="e145" class="nt nc it bd nd nu nv dn nh nw nx dp nl lq ny nz nn lu oa ob np ly oc od nr iz bi translated">随机森林</h2><p id="2e8a" class="pw-post-body-paragraph lh li it lj b lk oe kd lm ln of kg lp lq og ls lt lu oh lw lx ly oi ma mb mc im bi translated">通过合并几个不相关的<em class="md">决策树</em>，通常可以实现模型准确性的显著提高。这种方法叫做<em class="md">随机森林</em>。这些树在生长时会受到某些随机过程(随机化)的影响。最终模型反映了树的平均值。</p><p id="bd3d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">存在不同的随机化方法。根据Breiman，他在1999年创造了术语'<em class="md">随机森林</em>'，随机森林是根据以下过程建立的。首先，从每棵树的总数据集中选择一个随机样本。随着树的增长，在每个节点选择特征的子集。这些用作分割数据集的标准。然后分别为每个<em class="md">决策树</em>确定目标值。这些预测的平均值代表随机森林的预测。[Bre01][Jam13]</p><p id="9178" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">随机森林有许多超参数。最关键的一个，除了<code class="fe oj ok ol om b">max_depth</code>树的<em class="md">最大深度</em>之外，就是<em class="md">棵决策树的数量</em> <code class="fe oj ok ol om b">n_estimators</code>。默认情况下，随着树的增长，M <em class="md"> ean Square Error (MSE) </em>用作分割数据集的标准。[Sci18d]</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="3bb2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下图显示了随机森林的示例模型。其工作方式导致了特有的“<em class="md">步骤”</em>形式。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pe"><img src="../Images/dc5128286cf0d4e23fa7ad51eb2d2ef1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*poF9RGqhvzeAw9H53dkxlQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">随机森林:样本模型——作者图片</p></figure><p id="8412" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">由于随机森林将几个模型组合成一个，所以属于<strong class="lj jd"> <em class="md">集成学习</em> </strong>的领域。更准确地说，随机森林是一种所谓的<strong class="lj jd"> <em class="md">装袋</em> </strong>技术。</p><p id="138a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">除了<strong class="lj jd"> <em class="md">打包</em> </strong>之外，最著名的集成学习技术是<strong class="lj jd"> <em class="md"> Boosting </em> </strong>，该领域最著名的算法是<strong class="lj jd"> <em class="md"> AdaBoost </em> </strong>和<strong class="lj jd"> <em class="md"> XGboost </em> </strong>算法。</p><p id="1209" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果你对boosting和bagging(以及AdaBoost和Random Forest)之间的区别感兴趣，你可以在这里找到更详细的介绍:</p><div class="pf pg gp gr ph pi"><a rel="noopener follow" target="_blank" href="/adaboost-in-7-simple-steps-a89dc41ec4"><div class="pj ab fo"><div class="pk ab pl cl cj pm"><h2 class="bd jd gy z fp pn fr fs po fu fw jc bi translated">7个简单步骤中的AdaBoost</h2><div class="pp l"><h3 class="bd b gy z fp pn fr fs po fu fw dk translated">AdaBoost和Boosting简单解释</h3></div><div class="pq l"><p class="bd b dl z fp pn fr fs po fu fw dk translated">towardsdatascience.com</p></div></div><div class="pr l"><div class="ps l pt pu pv pr pw lb pi"/></div></div></a></div></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h2 id="7f1c" class="nt nc it bd nd nu nv dn nh nw nx dp nl lq ny nz nn lu oa ob np ly oc od nr iz bi translated"><strong class="ak">高斯过程回归</strong></h2><p id="3840" class="pw-post-body-paragraph lh li it lj b lk oe kd lm ln of kg lp lq og ls lt lu oh lw lx ly oi ma mb mc im bi translated"><strong class="lj jd"> <em class="md">高斯过程</em> </strong>在系统观察的基础上捕捉系统的典型行为，并作为结果传递手头问题的可能插值函数的概率分布。</p><p id="8457" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd"> <em class="md">高斯过程回归</em> </strong>在下面利用了贝叶斯定理，这也是为什么要提前简要说明的原因。</p><p id="8792" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">一般来说，<em class="md">贝叶斯定理</em>定义如下:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi px"><img src="../Images/8625f9bd1cc2637e1a0650db86b49e24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b3JOfLwkxOB19nO8AYXj7A.png"/></div></div></figure><p id="08da" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">它允许从已知值推断未知值。一个经常使用的应用例子是疾病检测。例如，在快速检测的情况下，人们感兴趣的是被检测为阳性的人实际患有该疾病的实际概率有多高。[Fah16]</p><p id="82de" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在下文中，我们将把这一原理应用于<em class="md">高斯过程</em>。</p><p id="2c74" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><em class="md">高斯过程</em>由每个随机变量的期望值、<code class="fe oj ok ol om b">mean function m(x)</code>和<code class="fe oj ok ol om b">covariance function k(x,x´)</code>定义。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi py"><img src="../Images/6578e987b7c1e4388d21d536b12e93b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZfZ36yuKbnH01LqxWFIdDw.png"/></div></div></figure><p id="ebff" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><code class="fe oj ok ol om b">mean function m(x)</code>反映了手头问题的<em class="md">先验</em>函数，并基于数据中的已知趋势或偏差。如果期望值(均值函数)为常数0，则称为中心高斯过程。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pz"><img src="../Images/7d070a40a078b4b3e9a4cdbb8c834253.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4mcqRmiDQ55v4dvEjdNkFQ.png"/></div></div></figure><p id="fc49" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><code class="fe oj ok ol om b">covariance function k(x, x´)</code>也称为“<em class="md">核</em>”，描述了随机变量x和x’的协方差。这些函数是解析定义的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qa"><img src="../Images/395b8fc90b160cfae20d8c3c3811653e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5Zyq_ZwDyOGS85jRVp63AQ.png"/></div></div></figure><p id="4122" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">内核定义了模型函数的形状和过程，并用于描述例如抽象属性，如<em class="md">平滑度、</em>粗糙度和<em class="md">噪声</em>。更多的内核可以通过一定的计算规则来组合，以模拟具有叠加属性的系统。[EBD 08][ku 06][ras 06][va f17]</p><p id="f89e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下面将介绍三种最常用的内核:</p><p id="e918" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">平方指数核</strong></p><p id="f066" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">一个流行的<em class="md">内核</em>是<code class="fe oj ok ol om b">Squared Exponential Kernel</code>(径向基函数)，并且已经被确立为<em class="md">高斯过程</em>和<em class="md">支持向量机</em>的<em class="md">标准内核</em>。[Sci18l]</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qb"><img src="../Images/a55bf1ad3941d73dfbbfe99bab26f1bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*STd_aFf82s3Ume1VGsG5Yg.png"/></div></div></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="bbc3" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下图通过<code class="fe oj ok ol om b">mean function m(x)</code>(黑线)和<code class="fe oj ok ol om b">confidence interval</code>(灰色背景)展示了一个A- <em class="md">先验</em>-高斯过程<code class="fe oj ok ol om b">p(f)</code>的例子。一般来说，置信区间表示在某种概率下，给定随机实验的无限重复的范围，参数的真实位置位于[Fah16][Enc18]。在这种情况下，置信区间的边界由<strong class="lj jd"> <em class="md"> </em> </strong> <em class="md">标准差σ </em>定义。</p><p id="ba77" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">彩色曲线代表<em class="md">高斯过程</em>的一些随机函数。示例曲线仅用于抽象可能的输出函数的形式。原则上，可以创建无限数量的这些曲线。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qc"><img src="../Images/fd1762b89c6a6841a2318a9a5442052b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4vIOg5wHqf6ZVRvgBhYiuA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">使用平方指数核的先验高斯过程—图片由作者提供(受[Sci18n][Duv14]启发)</p></figure><p id="0099" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">内核只有两个超参数:</p><ul class=""><li id="f000" class="mf mg it lj b lk ll ln lo lq mh lu mi ly mj mc mk ml mm mn bi translated"><strong class="lj jd"> l (length_scale) </strong>描述协方差函数的特征长度尺度。<strong class="lj jd"> length_scale </strong>影响高斯函数的“<em class="md">波</em>的长度。</li><li id="b27e" class="mf mg it lj b lk mp ln mq lq mr lu ms ly mt mc mk ml mm mn bi translated"><strong class="lj jd">方差σ </strong>定义了函数与其均值的平均距离。对于y轴上覆盖较大范围的函数，该值应选择较高。[Ebd08]</li></ul><p id="33e3" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下图显示了超参数对<em class="md">先验高斯过程</em>及其函数的影响。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qd"><img src="../Images/babfcff203d75caef7636955d089be55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k5BLlz8hKzh5sMseMpTRXg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">平方指数核:超参数的影响—作者图片</p></figure><p id="b542" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">有理二次核</strong></p><p id="b224" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><code class="fe oj ok ol om b">Rational Quadratic Kernel</code>可以被视为具有不同<code class="fe oj ok ol om b">length_scale</code>设置(l)的几个<em class="md">平方指数核</em>的组合。参数α决定了'<em class="md">大规模</em>和'<em class="md">小规模</em>功能的相对权重。当α接近无穷大时，有理二次核等于平方指数核。[Duv14][Sci18k][Mur12]</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qe"><img src="../Images/a12222a109d545ac8d981984092644d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2aOMW590JQNm89pV6HaOpA.png"/></div></div></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="or os l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qf"><img src="../Images/3a7e1bc9ec82164525ba71caf025b767.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HCW41WbxLhQMliAsQyVudA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">使用有理二次核的先验高斯过程—图片由作者提供(受[Sci18n][Duv14]启发)</p></figure><p id="8a51" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">周期性内核</strong></p><p id="f015" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><code class="fe oj ok ol om b">Periodic Kernel</code>允许函数自我重复。周期p描述了函数重复之间的距离。“<em class="md">长度刻度</em>”参数(l)的使用如前所述。[Sci18j]</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qg"><img src="../Images/1f7857ca1378132a035eb692af0dac7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G0GMr6oC6hJCOae-KMof9w.png"/></div></div></figure><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="or os l"/></div></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qh"><img src="../Images/b6164637e511cc969aacdd5d22654467.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qO0ncvCudl3dvSrlD64dZw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">使用周期核的先验高斯过程—图片由作者提供(受[Sci18n][Duv14]启发)</p></figure><p id="4295" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><code class="fe oj ok ol om b">kernel funnction</code>和<code class="fe oj ok ol om b">mean function</code>一起描述了<em class="md">先验高斯过程</em>。借助于一些测量值，可以定义一个<strong class="lj jd"> <em class="md">后验高斯过程</em> </strong>，其考虑了关于问题的所有可用信息。更准确地说，不会产生单一的解，而是插值的所有可能的函数，这些函数以不同的概率加权。具体来说，在回归任务的情况下，具有最高概率的解(函数)是至关重要的。[ras 06][维基18a][维基18a]</p><p id="7dac" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">对于回归，通常会给出一个数据集，其中包含自变量X ∈ R的值和因变量f ∈ R的相关值，并且希望预测新值X∫的输出值f∫。[Vaf17]</p><p id="1f01" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">对于最简单的情况，没有噪声的过程，多维高斯分布定义如下:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qi"><img src="../Images/1c248b53959c6d64236542c592c2f503.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*STenKNJ5i8ak0LfUgK0Z-A.png"/></div></div></figure><p id="98aa" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">协方差矩阵可以分为四部分。未知值K_XX∫内的协方差，未知和已知K _ X∫X值之间的协方差，以及已知值K _ XX内的协方差。</p><p id="5955" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">由于<code class="fe oj ok ol om b">f</code>是完全已知的，将概率密度代入贝叶斯定理得到<em class="md">后验高斯分布</em>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qj"><img src="../Images/70d39a8069dfd1a794bd78358dd6bbf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4sQVTixuhja5wkTr69GAxQ.png"/></div></div></figure><p id="f8bf" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">拉斯姆森在他的著作《<a class="ae mo" href="http://www.gaussianprocess.org/gpml/chapters/RW.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jd">机器学习的高斯过程</strong> </a>》中给出了详细的推导。[Ras06，第8页起。]</p><p id="a433" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">从先验到后验的高斯过程:用一个简单的例子说明</strong></p><p id="335d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在实践中，使用了许多其他内核，包括几个内核函数的组合。例如,<em class="md">常量内核</em>通常与其他内核一起使用。使用这个核而不与其他核结合，通常是没有意义的，因为只有常数相关性可以被建模。然而，在下文中，常数核用于以简单的方式解释和说明高斯过程回归。</p><p id="8341" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下图显示了方差为1的<em class="md">先验高斯过程</em>。通过将常数核定义为协方差函数，所有样本函数都显示一条与x轴平行的直线。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qk"><img src="../Images/7d030b95e988151779a0dc6df6042594.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ofnTvPryg7MD7gpLTMzc7g.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">具有所谓的常数核和一个支持数据点的高斯过程的表示——作者的图像</p></figure><p id="6b37" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">由于没有预先声明测量数据的可能噪声，该过程假设给定的测量点是真实函数的一部分。这将可能的函数方程的数量限制在直接通过该点的直线上。因为常量内核只允许水平线，所以在这个简单的例子中，可能的行数减少到只有一个可能的函数。因此<em class="md">后验高斯过程</em>的协方差为零。</p><p id="a5c9" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">使用<code class="fe oj ok ol om b">RBF Kernel</code>，可以绘制任意过程，但这次的结果不是像<em class="md">后验高斯</em>那样的单一直线，而是多个函数。概率最高的函数是<em class="md">后验高斯过程</em>的<code class="fe oj ok ol om b">mean function</code>。下图显示了<em class="md">后验高斯过程</em>和使用的测量点。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ql"><img src="../Images/0d8ea50267f7122fcf034fd038cd340b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kuLviOcqMmJnD55TrXcroQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">具有平方指数核的后验高斯过程和使用的数据点—图片由作者提供</p></figure><p id="8134" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为了在Python中实现高斯过程，必须预先定义<em class="md">先验高斯过程</em>。<code class="fe oj ok ol om b">mean function m(x)</code>通常被假定为常数和零。通过设置参数<code class="fe oj ok ol om b">normalize_y = True</code>，该过程使用数据集值的平均值作为常量期望值函数。通过选择核来选择协方差函数。[Sci18m]</p><p id="7df0" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">sci kit-learn中的高斯过程回归</strong></p><p id="1905" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">以下源代码描述了如何使用scikit learn和用作协方差函数的<code class="fe oj ok ol om b">RBF Kernel</code>实现高斯过程回归。第一个优化过程从内核的预设值(<code class="fe oj ok ol om b">length_scale</code>和<code class="fe oj ok ol om b">variance</code>)开始。通过参数<code class="fe oj ok ol om b">alpha</code>，可以预先假设训练数据的噪声强度。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="c414" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">优化过程:使用最大似然法超参数估计</strong></p><p id="aa95" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在模型拟合期间，通过最大化<em class="md">对数边际似然(LML) </em>来优化超参数。<em class="md">最大似然估计(MLE) </em>是一种确定统计模型参数的方法。虽然已经提出的回归方法(如线性回归)旨在最小化<em class="md">均方误差</em>，但高斯过程回归试图最大化<em class="md">似然函数</em>。换句话说，模型的参数是以这样一种方式选择的，即观察到的数据根据它们的分布看起来是最合理的。</p><p id="c630" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">一般来说，<code class="fe oj ok ol om b">random variable X</code>的<code class="fe oj ok ol om b">probability function f</code>定义为:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qm"><img src="../Images/dbd616930bbd649df9003a9b013a9c2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rz-sQkeGQL9Qrz_H8OREGQ.png"/></div></div></figure><p id="9721" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">假设这种分布取决于参数ϑ.给定观测数据，概率可以被认为是ϑ的函数:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qn"><img src="../Images/5f5fe2b0bc166c8610bcedd593092f66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KQLEwzLWWH2kZC19_qf3Vw.png"/></div></div></figure><p id="e9f1" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">最大似然估计旨在最大化该函数。最大值通常是通过对函数求微分，然后将其设置为零来确定的。由于<em class="md">对数似然函数</em>在与似然函数相同的点具有最大值，但更容易计算，因此通常使用。[谷歌16，第128页]</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qo"><img src="../Images/30f860288b6d3afa8763569555a23f10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ghPA258010vP3qg8ghVjXw.png"/></div></div></figure><p id="8973" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果随机变量X具有以下概率密度，则称之为正态或高斯分布[Fah16，第83页]:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qp"><img src="../Images/01ec7b34f6b2530ff04ea2418f85a433.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_UTI3VsxrZf2opm9jwvDEA.png"/></div></div></figure><p id="0131" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下面将使用一个简单的一维例子来解释最大似然法。<strong class="lj jd"> </strong>下图显示了数据集。所有三个绘制的概率分布反映了数据的分布。对于最大似然，人们感兴趣的是最有可能的分布。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qq"><img src="../Images/5948d94f155bdfa3f5ef22e1b19013de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CWEWr2iaBL0PZ0TelSJqeg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">取决于参数期望值和方差的正态分布—图片由作者提供(受[BB18]启发)</p></figure><p id="4892" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">目标是定义参数σ，并以这种方式使所有考虑的数据点的概率最大化。例如，给定x值为9、10和13的三个数据点，类似于图中的数据点，联合概率从各个概率计算如下:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qr"><img src="../Images/4f108c3cf441117890ff84b8acb8e191.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jiXpcBQjjwXNZxpncqgwOQ.png"/></div></div></figure><p id="c321" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这个功能必须最大化。平均值则对应于最有可能发生的<code class="fe oj ok ol om b">x-value</code>。</p><p id="723f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下图显示了高斯过程回归的示例模型。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qs"><img src="../Images/144dc7536f10ae922a0afbaf67ee5cd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UgHGL4S9Fq6O0CKrxg8f_Q.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">高斯过程回归:样本模型—图片由作者提供</p></figure></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h2 id="f3f7" class="nt nc it bd nd nu nv dn nh nw nx dp nl lq ny nz nn lu oa ob np ly oc od nr iz bi translated">支持向量回归</h2><p id="cb97" class="pw-post-body-paragraph lh li it lj b lk oe kd lm ln of kg lp lq og ls lt lu oh lw lx ly oi ma mb mc im bi translated">支持向量回归(SVR)的功能基于支持向量机(SVM ),首先用一个简单的例子来解释。我们正在寻找线性函数:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qt"><img src="../Images/c21d7f6558d45d060221c31701e2676c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LnzsCwFCifzywmEYETsRxw.png"/></div></div></figure><p id="8f5b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">x⟩·⟨w描述了叉积。SV回归的目标是找到一条直线作为数据点的模型，而直线的参数应该以这样一种方式定义，即直线尽可能的“<em class="md">平</em>”。这可以通过最小化规范来实现</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qu"><img src="../Images/1c95c3966b29efc6c7f1e7f52ff13f93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_gRfRf44xFj7Ai300ajyhg.png"/></div></div></figure><p id="d9fa" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">对于模型建立过程，只要数据点在定义的范围内(-ϵ到+ϵ). ),数据点离建模的直线有多远并不重要不允许偏差超过规定的ϵ限值。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qv"><img src="../Images/1483209f88caad51ab708697b53ae914.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TZ2gC4mYhDZZVw_OVGl7FQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">支持向量回归机的功能—作者图片(受[Smo04]启发)</p></figure><p id="3ab0" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这些条件可以描述为凸优化问题:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qw"><img src="../Images/b8317a284651a1143c6e36ab94fca15a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TAGsAfCWH5mRZh0MNDoqXQ.png"/></div></div></figure><p id="8852" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果所有数据点都可以在ϵ的精度内近似，那么这个优化问题就找到了解决方案(上图显示了这个场景的一个简单例子)。然而，这是一个简化的假设，在实践中通常不成立。为了能够绕过无法解决的优化问题，引入了变量ζi，ζ∫——所谓的<em class="md">松弛变量</em>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qx"><img src="../Images/c842d92fc8562517c419dd2c171c6a82.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2YTFDC6OglZqfHCDHDoJVA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">线性SVM的软边界损失设置—图片由作者提供(受[Smo04]启发)</p></figure><p id="5690" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">上图描述了使用线性<em class="md">损失函数</em>对超过ϵ量的偏差的<em class="md">惩罚</em>。损失函数被称为<em class="md">内核</em>。除了线性核，多项式或RBF核也经常被使用。[Smo04][Yu12][Bur98]</p><p id="2890" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，根据Vapnik的配方如下:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qy"><img src="../Images/418fc8ceaa2cad261fc6ca79f1f76c2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BK387-9nAwtWiljlHGr4pQ.png"/></div></div></figure><p id="efbb" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><em class="md">常数C </em>描述了<em class="md">平坦度</em>条件和容许的大于ϵ的偏差之间的平衡。</p><p id="a269" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为了能够用支持向量回归来模拟非线性关系，使用了所谓的“<em class="md">核技巧</em>”。因此，原始特征被映射到更高维的空间中。[Pai12]</p><p id="b82d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">使用<code class="fe oj ok ol om b">scikit-learn</code>和<em class="md"> RBF内核</em>的实现如下所示:</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="or os l"/></div></figure></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h2 id="8657" class="nt nc it bd nd nu nv dn nh nw nx dp nl lq ny nz nn lu oa ob np ly oc od nr iz bi translated">本文中介绍的回归方法的总结</h2><p id="0bce" class="pw-post-body-paragraph lh li it lj b lk oe kd lm ln of kg lp lq og ls lt lu oh lw lx ly oi ma mb mc im bi translated">下图概述了所介绍的回归方法，并简要总结了它们的工作原理。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qz"><img src="../Images/4baa2866ade74b4ad5f7cc0f9395cf8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*s6JA6ABCOYKRvUwTThZVEQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">呈现的回归方法概述—图片由作者提供</p></figure></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h1 id="b419" class="nb nc it bd nd ne nf ng nh ni nj nk nl ki nm kj nn kl no km np ko nq kp nr ns bi translated">2.模型评估</h1><p id="6f42" class="pw-post-body-paragraph lh li it lj b lk oe kd lm ln of kg lp lq og ls lt lu oh lw lx ly oi ma mb mc im bi translated">有各种方法和程序来评估模型的准确性。</p><h2 id="dade" class="nt nc it bd nd nu nv dn nh nw nx dp nl lq ny nz nn lu oa ob np ly oc od nr iz bi translated">度量函数</h2><p id="e0a9" class="pw-post-body-paragraph lh li it lj b lk oe kd lm ln of kg lp lq og ls lt lu oh lw lx ly oi ma mb mc im bi translated"><code class="fe oj ok ol om b">sklearn.metrics</code>模块包括几个损失和评估函数来测量回归模型的质量。<em class="md">均方差(MSE) </em>是评估回归模型质量的关键标准【Ras18，p.337】。如果yˇ_ I描述的是模型在第I个数据样本上预测的值，y_i描述的是对应的真值，那么模型在n个样本上的<strong class="lj jd"><em class="md">【MSE】</em></strong>描述为【Sci18a】:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qw"><img src="../Images/80a7fc3f71df8f7eb7893b6c1da9af24.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L2ZN791SCN9y9tlB2t93zA.png"/></div></div></figure><p id="73fc" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">确定回归模型精度的另一个参数是<strong class="lj jd"> <em class="md">【平均绝对误差(MAE)】</em></strong>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ra"><img src="../Images/e44ad7308fce7b01e741b5b9745a28cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IRMoLVnF7NYz7dcR56xn0w.png"/></div></div></figure><p id="0ed8" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这两个指标都可以在模块<code class="fe oj ok ol om b">sklearn.metrics</code>中找到。它们比较测试数据集的预测值和实际值。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="9af2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">决定系数(R ) </strong></p><p id="4311" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">所谓的决定系数(R)可以理解为MSE的标准化版本。这允许更容易地解释模型的性能。值1.0表示可能的最佳性能。如果模型显示出与真实值的任意偏差，R值也可能变为负值。常数模型在不考虑输入特征的情况下预测值，其R值为0.0。</p><p id="1a09" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果yˇ_ I描述了模型在第I个数据样本预测的值，y_i描述了相关联的真实值，则n_Samples上的决定系数R定义为[Sci18a]:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi rb"><img src="../Images/ae60a8bef7b4bc7b2dc60edd3ae95388.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8DM-GbfOPKI0YqUQ2yzL7g.png"/></div></div></figure><p id="d497" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">Python中的输出是函数<code class="fe oj ok ol om b">r2_score</code>，其中<code class="fe oj ok ol om b">y_true</code>是因变量的真实值，<code class="fe oj ok ol om b">y_pred</code>是模型预测的值。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="568c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">回归中的交叉验证</strong></p><p id="7e16" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">交叉验证是一种模型选择的统计方法。为了评估一种方法，整个数据集被分成训练数据集和测试数据集，其中训练数据集通常包括整个数据集的80%到90 %。为了实现模型的最佳评估，目标是拥有尽可能大的测试数据集。通过拥有尽可能大的<strong class="lj jd">训练数据集</strong>可以实现良好的模型构建。</p><p id="d542" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><em class="md">交叉验证</em>就是用来规避这个困境的。这种方法允许将整个数据集用于训练和测试。与训练和测试数据的固定划分相比，<em class="md">交叉验证</em>因此允许对未来数据或未包含在数据集中的数据进行更准确的模型精度估计。</p><p id="1325" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">k倍交叉验证将整个数据集<code class="fe oj ok ol om b">X</code>分成<code class="fe oj ok ol om b">k</code>个大小相等的块(<code class="fe oj ok ol om b">X_1, …, X_k</code>)。然后，该算法在<code class="fe oj ok ol om b">k-1</code>块上被训练<code class="fe oj ok ol om b">k</code>次，并用剩余的块进行测试。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi rc"><img src="../Images/7c756ee0b928c18f4cefdb3633c53f9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-p_6l7n1vtRMHtkYhBDZ_Q.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">交叉验证的功能，以五重交叉验证为例—图片由作者提供</p></figure><p id="4767" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">许多学习方法允许通过一个或多个超参数来调整模型复杂度。这通常会导致过度配合或配合不足的问题。交叉验证用于找到最佳的模型复杂度。通过最小化在学习期间未知的测试数据集上的近似误差来实现最佳复杂度。[Du14，第27页][Has09，第242页]</p><p id="400f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">对于不同的参数设置和模型复杂性，执行已经描述的过程。对于最终模型，选择显示最低e <em class="md">误差(如MSE或MAE) </em>的设置参数(γ_opt)。对于较小的训练数据集，<code class="fe oj ok ol om b">k</code>可以等同于特征向量的数量<code class="fe oj ok ol om b">n</code>。这种方法叫做<em class="md">留一交叉验证</em>。[Ert16，第233页][Bow15，第100页]</p><p id="f564" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">sklearn的实现是通过模块<code class="fe oj ok ol om b">cross_validate</code>完成的。以下代码片段显示了交叉验证的应用，以评估<em class="md">线性回归的性能。</em></p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="b515" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><code class="fe oj ok ol om b">cv</code>值定义了数据集被划分成的分区的数量<code class="fe oj ok ol om b">k</code>。在这种情况下，使用<code class="fe oj ok ol om b">Negativ Mean Squared Error</code>作为评分参数。每次运行后，平方误差被传递到列表<code class="fe oj ok ol om b">scores</code>。在程序代码执行之后，<code class="fe oj ok ol om b">scores</code>表示一个列表，在这种情况下，该列表具有三个条目，即每个回归模型的<em class="md">均方误差</em>。这些模型的不同之处仅在于测试和训练数据集的选择，如前所述，这些数据集在每次运行后都会发生变化。[Sci18f][Coe13]</p><p id="a2c7" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">功能<code class="fe oj ok ol om b">cross_validate</code>使用<code class="fe oj ok ol om b">.metrics</code>模块的评分参数。下表概述了用于评估回归模型的所谓评分参数[Sci18e][Cod18]。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi rd"><img src="../Images/7c8ffe07f08189bc02bc67694a20244a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EBPv1qHwzOk0LEt8zHbxkw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">回归模型评估的评分参数—图片由作者提供</p></figure><p id="b67f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">以<code class="fe oj ok ol om b">_score</code>结尾的函数返回一个应该尽可能最大化的值。以<code class="fe oj ok ol om b">_error</code>或<code class="fe oj ok ol om b">_loss</code>结尾的函数返回值最小化。</p><p id="f96a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果您查看一下<code class="fe oj ok ol om b">sklearn.metrics.scorer</code>模块的源代码，您可以看到，对于所有的损失或错误函数，参数<code class="fe oj ok ol om b">greater_is_better</code>都被设置为FALSE，计分参数被取反并用表达式<code class="fe oj ok ol om b">neg_</code>进行补充。这允许以相同的方式处理所有评分参数。[代码18][Git18]</p><p id="35b8" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下面的源代码展示了一个典型的<em class="md">交叉验证</em>的应用例子。该示例使用<em class="md">多项式回归</em>进行建模，这允许通过指定<em class="md">多项式次数</em>来设置模型复杂度。下图显示了使用的数据集和过度拟合的问题。如果使用训练数据来执行模型的评估，则具有较高复杂性的模型通常显示出较高的准确性。由于数据集是使用正弦函数生成的，因此true函数可用于比较。对于这个简单的例子，一眼就可以看出多项式次数为15的多项式回归模型并不能正确地表示回归问题——它是一个<strong class="lj jd">过度拟合的模型。</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi re"><img src="../Images/b1df19b77d3899f67f18cd709af41b38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EnO0VaVlw7_aXQSSYb9_sg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">多项式回归:过度拟合—作者图片</p></figure><p id="bc69" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">为了能够确定“最佳”多项式次数，下面使用了<em class="md">交叉验证</em>。</p><p id="58c0" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">当应用于测试数据集时，好的模型的特征在于模型的最低可能误差。为了获得相对小数据集的模型的最佳可能评估，对不同的设置参数执行“<em class="md">留一交叉验证</em>”。这是通过将分区的数量(数据集在交叉验证期间被划分成的分区)设置为数据点的数量来实现的。交叉验证得到的<code class="fe oj ok ol om b">scores</code>列表包括每次运行的<em class="md">均方误差</em>。对这些值进行平均，以评估所用的回归方法。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="or os l"/></div></figure><p id="d24f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下图左图显示了不同多项式次数的交叉验证结果。此外，还显示了来自训练数据集的模型误差。随着模型复杂度的增加，误差减小。这解释了为什么基于训练数据的模型的评估和优化是不可行的。</p><p id="f784" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">通过交叉验证确定的<em class="md">均方误差</em>显示在三至七次多项式范围内的稳定低值。更复杂的系统不再充分代表该过程，这就是为什么不包括在训练数据集中的数据和该过程的未来数据的计算精度显著降低。最佳值显示在多项式次数为3时。如果绘制出最终模型，它显示出与“真实函数”的良好近似。(在这种情况下，可以给出数据集的“真实函数”，因为数据点是以对给定cos函数的随机偏移生成的)。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi rf"><img src="../Images/d8a3fc3a09e1d7632065ae2b04f7a426.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WLJoLQd0FrShplH60hPT2w.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">使用交叉验证选择多项式次数—图片由作者提供</p></figure></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h1 id="ad64" class="nb nc it bd nd ne nf ng nh ni nj nk nl ki nm kj nn kl no km np ko nq kp nr ns bi translated">3.模型建立过程</h1><p id="2b0a" class="pw-post-body-paragraph lh li it lj b lk oe kd lm ln of kg lp lq og ls lt lu oh lw lx ly oi ma mb mc im bi translated">下图显示了方法选择和后续模型生成的示意流程。特征选择已经发生在模型建立之前，并且定义了后面的回归模型的输入属性。数据集在创建过程中已经以只包含相关属性的方式进行了结构化。</p><p id="65e2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">回归方法适用于不同的问题，效果也不同。为了评估，在建模之前，数据集被分成训练和测试数据集。这个步骤在源代码中被省略了，因为这个过程是在<em class="md">交叉验证</em>期间自动迭代执行的。交叉验证的执行由<em class="md"> scikit </em>库的<code class="fe oj ok ol om b">cross_val_score</code>函数完成。</p><p id="a821" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><em class="md">交叉验证</em>提供了每种回归方法的性能指标。对于具有少量实例的数据集，通常会执行一个'<em class="md">遗漏一个</em> ' c <em class="md">交叉验证</em>。为此，<em class="md">交叉验证</em>的分区号被设置为等于数据集的长度。</p><p id="c2e2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">通过不同超参数设置的重复交叉验证进行超参数优化:</strong></p><p id="6ca9" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><em class="md">交叉验证</em>的结果代表一个列表，其中包含所选评分参数的值。由于评估是在每次运行之后执行的，因此如果数据集被划分为五个分区，那么也有一个包含五个评估值的列表。这些值的平均值允许对回归程序的性能进行评估。由于大多数回归方法允许通过一个或多个超参数来调整模型复杂性，因此超参数的调整对于回归方法的有意义比较是必要的。这些最佳超参数设置的发现是通过迭代模型建立来完成的。对于不同的超参数设置，重复执行<em class="md">交叉验证</em>。最后，选择在评估期间显示最佳模型精度的参数设置。该过程由循环执行，该循环在一定限度内自动改变超参数并存储评估值。然后，通过手动或自动搜索最佳评估结果来选择最佳设置。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi rg"><img src="../Images/93794673aac6b517265abb1fb87dd475.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fvx0DMip5tOa-Q1KbqPsPw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">不同回归方法的评估和随后的模型构建的示意图——图片由作者提供</p></figure><p id="1349" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">虽然线性回归不允许设置模型复杂性，但大多数算法都包含多个超参数。为了优化模型，在具有多个超参数设置选项的程序中，仅改变其中一个超参数通常是不够的。必须注意，不能单独考虑超参数，因为参数变化的影响会部分地相互影响。</p><p id="dda4" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">下图显示了所提出方法的一些重要超参数的列表。特别是对于使用核函数来寻找解决方案的方法，可能的设置数量远远超过了列出的数量。有关更详细的描述，您可以在:<a class="ae mo" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank">scikit-learn.org</a>找到这些方法及其超参数的综合文档。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi rh"><img src="../Images/f6e36e1a42ceb646529f906c33473a1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KG6Rkk7fZMzxUwsdyOwbqA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">回归方法中最重要的超参数概述—图片由作者提供</p></figure><p id="ba3c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">您可以在此找到超参数优化领域的更详细介绍，以及网格搜索或贝叶斯优化等常用方法:</p><div class="pf pg gp gr ph pi"><a rel="noopener follow" target="_blank" href="/a-step-by-step-introduction-to-bayesian-hyperparameter-optimization-94a623062fc"><div class="pj ab fo"><div class="pk ab pl cl cj pm"><h2 class="bd jd gy z fp pn fr fs po fu fw jc bi translated">超参数调整、网格搜索和贝叶斯优化的逐步介绍</h2><div class="pp l"><h3 class="bd b gy z fp pn fr fs po fu fw dk translated">使用回归问题的贝叶斯优化的说明性解释</h3></div><div class="pq l"><p class="bd b dl z fp pn fr fs po fu fw dk translated">towardsdatascience.com</p></div></div><div class="pr l"><div class="ri l pt pu pv pr pw lb pi"/></div></div></a></div></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h1 id="1b0b" class="nb nc it bd nd ne nf ng nh ni nj nk nl ki nm kj nn kl no km np ko nq kp nr ns bi translated">摘要</h1><p id="c9d9" class="pw-post-body-paragraph lh li it lj b lk oe kd lm ln of kg lp lq og ls lt lu oh lw lx ly oi ma mb mc im bi translated">希望我能给你一个用于回归分析的不同技术的概述。当然，这篇文章并没有声称给出了回归的完整图景。无论是回归领域还是所提出的概念。很多重要的算法根本没有提到。然而，这7种算法给了你一个很好的概述，介绍了所使用的技术以及它们在工作方式上的区别。</p><p id="af79" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果您觉得这篇文章很有帮助，您还可以找到一篇关于用于<em class="md">异常检测</em>的概念和算法的类似文章:</p><div class="pf pg gp gr ph pi"><a rel="noopener follow" target="_blank" href="/a-comprehensive-beginners-guide-to-the-diverse-field-of-anomaly-detection-8c818d153995"><div class="pj ab fo"><div class="pk ab pl cl cj pm"><h2 class="bd jd gy z fp pn fr fs po fu fw jc bi translated">异常检测多样化领域的初学者综合指南</h2><div class="pp l"><h3 class="bd b gy z fp pn fr fs po fu fw dk translated">隔离森林，局部异常因子，一类SVM，自动编码器，稳健协方差估计和时间序列…</h3></div><div class="pq l"><p class="bd b dl z fp pn fr fs po fu fw dk translated">towardsdatascience.com</p></div></div><div class="pr l"><div class="rj l pt pu pv pr pw lb pi"/></div></div></a></div><p id="7705" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><a class="ae mo" href="https://dmnkplzr.medium.com/membership" rel="noopener">如果你还不是中级高级会员并想成为其中一员，你可以通过使用这个推荐链接注册来支持我。</a></p><p id="fe08" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">感谢您的阅读！！</p></div><div class="ab cl mu mv hx mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="im in io ip iq"><h1 id="12af" class="nb nc it bd nd ne nf ng nh ni nj nk nl ki nm kj nn kl no km np ko nq kp nr ns bi translated">参考</h1><p id="9b3e" class="pw-post-body-paragraph lh li it lj b lk oe kd lm ln of kg lp lq og ls lt lu oh lw lx ly oi ma mb mc im bi translated">[Aun18] Aunkofer，b . Entscheidungsbaum-algorithm us ID3-数据科学<br/>博客，2018年。网址<a class="ae mo" href="https://data-science-blog.com/blog/2017/08/13/entscheidungsbaum-algorithmus-id3/" rel="noopener ugc nofollow" target="_blank">https://data-science-blog . com/blog/2017/08/13/entscheidungsbaum-algorithm us-ID3/</a></p><p id="603f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[BB18] Brooks-Bartlett，j .概率概念解释:最大似然估计，2018。网址<a class="ae mo" rel="noopener" target="_blank" href="/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1">https://towards data science . com/probability/concepts-explained-maximum-likelihood-estimation-c7b 4342 fdbb 134</a></p><p id="86d8" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">《机器学习:开发者和专业技术人员的实践》。2015年，印第安纳州印第安纳波利斯，威利。ISBN 1118889061</p><p id="4e70" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">统计决策理论和贝叶斯分析。统计学中的斯普林格级数。施普林格，纽约，纽约州，第二版Auflage，1985年。ISBN 9781441930743。doi:10.1007/978–1–4757–4286–2。网址http://dx.doi.org/10.1007/978-1-4757-4286-2<a class="ae mo" href="http://dx.doi.org/10.1007/978-1-4757-4286-2" rel="noopener ugc nofollow" target="_blank">T3</a></p><p id="2f02" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">《Python中的机器学习:预测分析的基本技术》。约翰·威利父子公司，印第安纳波利斯，2015年。ISBN 1118961749B</p><p id="4f92" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[Bre01] Breiman，l .兰登森林。2001.</p><p id="30a8" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[Bur98]伯格斯；考夫曼湖；斯莫拉，A. J。支持向量回归机。1998.网址【http://papers.nips.cc/paper/1238- T4】<br/>支持向量回归机. pdfine </p><p id="7ac7" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[Cal03] Callan，r .神经元网络在Klartext中。我是Klartext。皮尔逊工作室，Műnchen和哈洛，2003年。ISBN 9783827370716</p><p id="29dd" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">代码示例。3.3.modellbewertung:qualitizering der Quali[1]t von Vorhersagen | sci kit-学习文档|代码示例</p><p id="8b7b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">科埃略公司；用Python构建机器学习系统。2013</p><p id="88fc" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[Dan18] Daniilidis，K. RANSAC:随机样本共识I —姿势估计| Coursera，2018。URL<a class="ae mo" href="https://www.coursera.org/lecture/robotics-perception/ransac-random-sample-consensus-i-z0GWq" rel="noopener ugc nofollow" target="_blank">https://www . coursera . org/lecture/robotics-perception/ran sac-random-sample-consensus-I-z0 gwq</a></p><p id="e9d9" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[14]杜，k .-l；神经网络和统计学习。伦敦斯普林格，伦敦，2014。ISBN 978–1–4471–5570–6。doi:10.1007/978–1–4471-5571–3</p><p id="bbc8" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[duv 14]d . k . Duvenaud,《高斯过程的自动模型构建》。2014.网址<a class="ae mo" href="https://www.cs.toronto.edu/~duvenaud/thesis.pdfin" rel="noopener ugc nofollow" target="_blank">https://www.cs.toronto.edu/~duvenaud/thesis.pdfin</a></p><p id="ff52" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">回归的高斯过程:快速介绍。2008.</p><p id="5b8c" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">【Enc18】显著性检验争议与贝叶斯替代，<br/> 21.06.2018。网址<a class="ae mo" href="https://www.encyclopediaofmath.org/index.php/" rel="noopener ugc nofollow" target="_blank">https://www.encyclopediaofmath.org/index.php/</a><br/>非显著性_检验_争议_和_贝叶斯_替代</p><p id="3627" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[Ert16] Ertel，w . grund kurs küNST liche Intelligenz:一种实践或科学。计算智能。施普林格威斯巴登有限公司和施普林格观点，威斯巴登，4。，überab。aufl。2017 Auflage，2016。ISBN 9783658135485</p><p id="5887" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[Fah16]法赫迈尔湖；霍伊曼角；Künstler，r . Statistik:Weg zur数据分析。施普林格-莱尔布奇。柏林和海德堡，8。，2016年。ISBN 978–3–662 50371–3。doi:10.1007/978–3–662–50372–0</p><p id="4fe8" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">菲施勒，m。随机样本一致性:模型拟合范例及其在图像分析和自动制图中的应用。1980.网址<a class="ae mo" href="http://www.dtic.mil/dtic/tr/fulltext/u2/a460585.pdf0585.p" rel="noopener ugc nofollow" target="_blank">http://www.dtic.mil/dtic/tr/fulltext/u2/a460585.pdf0585.p</a></p><p id="1c37" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[Git18] GitHub。sklearn.metrics-Quellcode，2018。</p><p id="6d7a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[Goo16]古德费勒，我；纽约州本吉奥；库维尔，深度学习。麻省理工学院出版社，马萨诸塞州剑桥和英国伦敦，2016年。ISBN 9780262035613。网址http://www.deeplearningbook.org/<a class="ae mo" href="http://www.deeplearningbook.org/" rel="noopener ugc nofollow" target="_blank">T3</a></p><p id="910f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">哈斯蒂，t。蒂布拉尼河；统计学习的要素:数据挖掘、推理和预测。斯普林格纽约，纽约，纽约州，2009年。ISBN 978–0–387–84857–0。doi:10.1007/b94608</p><p id="7833" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">Huber，P. J .稳健统计。纽约威利，纽约州，2005年。国际标准书号0–47141805-6</p><p id="3b18" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">稳健回归、分类和强化学习的高斯过程模型。2006.网址<a class="ae mo" href="http://tuprints.ulb.tu-darmstadt.de/epda/000674/GaussianProcessModelsKuss.pdf" rel="noopener ugc nofollow" target="_blank">http://tu prints . ulb . tu-Darmstadt . DDE/epda/000674/gaussianprocessmodelskus . pdf ku</a></p><p id="c841" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[Mur12] Murphy，K. P.《机器学习:概率观点》。自适应计算和机器学习系列。麻省理工学院出版社，剑桥，麻省。, 2012.ISBN 9780262018029。URL<a class="ae mo" href="https://ebookcentral.proquest.com/auth/lib/subhh/login.action?returnURL=https%3A%2F%2Febookcentral.proquest.com%2Flib%2Fsubhh%2Fdetail.action%3FdocID%3D3339490" rel="noopener ugc nofollow" target="_blank">https://ebook central . proquest . com/auth/lib/subhh/log in . action？returnURL = https % 3A % 2F % 2 febookcentral . proquest . com % 2f lib % 2f subhh % 2f detail . action % 3f docid % 3d 3339490</a></p><p id="df26" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[Pai12] Paisitkriangkrai，p .线性回归和支持向量回归。2012.网址<a class="ae mo" href="https://cs.adelaide.edu.au/~chhshen/teaching/ML_SVR.pdf" rel="noopener ugc nofollow" target="_blank">https://cs.adelaide.edu.au/~chhshen/teaching/ML_SVR.pdf</a></p><p id="366f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[Qui86] Quinlan，J. R .决策树的归纳。机器学习，1(1):81–106，1986。ISSN 0885–6125。doi:10.1007/BF00116251。网址<a class="ae mo" href="https://link.springer.com/content/pdf/10.1007%2FBF00116251.pdf" rel="noopener ugc nofollow" target="_blank">https://link . springer . com/content/pdf/10.1007% 2 fbf 00116251 . pdf</a></p><p id="644b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">拉斯姆森；机器学习的高斯过程。2006</p><p id="868a" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">拉什卡；Mirjalili，v .机器学习与Python和Scikit-Learn和tensor flow:Das umfassende Praxis-数据科学、深度学习和预测分析手册。mitp，Frechen，2。，aktualiserte und erweiterte Auflage Auflage，2018。ISBN 9783958457331</p><p id="1b42" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[Rod04] Rodehorst，v . Nahbereich dur ch Auto-kali briering中的摄影测量学3D-rekon structure与几何投影师:Zugl。:柏林，Techn。大学，Diss。, 2004.wvb Wiss。Verl。柏林，柏林，2004。ISBN 978–3–936846–83–6</p><p id="1e08" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">【Sci18a】ScikitLearn。3.3.模型评估:量化预测质量-sci kit-learn 0 . 20 . 0文档，2018年10月5日。URL<a class="ae mo" href="https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/model _ evaluation . html # regression-metrics</a></p><p id="3f8e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">【Sci18c】ScikitLearn。sk learn . tree . decision tree regressor-sci kit-learn 0 . 20 . 0<br/>文档，2018年11月8日。网址<a class="ae mo" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . tree . decision tree regressor . html</a></p><p id="6a44" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[Sci18e] ScikitLearn。3.3.模型评估:量化预测质量-sci kit-learn 0 . 20 . 0文档，2018年10月24日。网址<a class="ae mo" href="https://scikit-learn.org/stable/modules/model_evaluation.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/model _ evaluation . html</a></p><p id="17f3" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[Sci18f] ScikitLearn。sk learn . model _ selection . cross _ val _ score—scikit learn 0 . 20 . 0文档，24.10.2018。URL<a class="ae mo" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . model _ selection . cross _ val _ score . html # sk learn . model _ selection . cross _ val _ score</a></p><p id="5d91" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[Sci18g] ScikitLearn。4.1.管道和复合估算器-sci kit-了解0.20.0文档，2018年10月26日。网址<a class="ae mo" href="https://scikit-learn.org/stable/modules/compose.html" rel="noopener ugc nofollow" target="_blank">https://scikit-learn.org/stable/modules/compose.html</a></p><p id="46d0" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[Sci18h] ScikitLearn。sk learn . preprocessing . polynomial features-sci kit-learn 0 . 20 . 0文档，26.10.2018。URL<a class="ae mo" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . preprocessing . polynomial features . html # sk learn . preprocessing . polynomial features</a></p><p id="2bec" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[Sci18j] ScikitLearn。sk learn . Gaussian _ process . kernels . expsinesquared—<br/>sci kit-learn 0 . 20 . 0文档，27.10.2018。网址<a class="ae mo" href="https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.ExpSineSquared.html#sklearn.gaussian_process.kernels.ExpSineSquared" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . Gaussian _ process . kernels . expsinesquared . html # sk learn . Gaussian _ process . kernels . expsinesquared</a></p><p id="b4d2" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[Sci18k] ScikitLearn。sk learn . Gaussian _ process . kernels . rational quadratic—<br/>sci kit-learn 0 . 20 . 0文档，27.10.2018。网址<a class="ae mo" href="https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RationalQuadratic.html#sklearn.gaussian_process.kernels.RationalQuadratic" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . Gaussian _ process . kernels . rational quadratic . html # sk learn . Gaussian _ process . kernels . rational quadratic</a></p><p id="bc73" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">【sci 18m】ScikitLearn。1.7.高斯过程-sci kit-学习0.20.1文档，28.11.2018。网址<a class="ae mo" href="https://scikit-learn.org/stable/modules/gaussian_process.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/Gaussian _ process . html</a></p><p id="ef09" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">ScikitLearn。<br/>不同内核的先验和后验高斯过程图解-sci kit-learn 0 . 20 . 0文档，2018年10月31日。URL<a class="ae mo" href="https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_prior_posterior.html#sphx-glr-auto-examples-gaussian-process-plot-gpr-prior-posterior-py" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/auto _ examples/Gaussian _ process/plot _ GPR _ prior _ posterior . html # sphx-glr-auto-examples-Gaussian-process-plot-GPR-prior-posterior-py</a></p><p id="f170" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">斯莫拉，A. J。支持向量回归教程。统计与计算，14(3):199–222，2004。ISSN 0960–3174。doi:10.1023/B:STCO。0000035301.49549.8849549.</p><p id="b1ba" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[Vaf17] Vafa，k .高斯过程教程，2017。网址<a class="ae mo" href="http://keyonvafa.com/gp-tutorial/" rel="noopener ugc nofollow" target="_blank">http://keyonvafa.com/gp-tutorial/</a></p><p id="0c8d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[Wei18]韦斯斯坦，e . L2-诺姆，2018年。网址<a class="ae mo" href="http://mathworld.wolfram.com/L2-Norm.html" rel="noopener ugc nofollow" target="_blank">http://mathworld.wolfram.com/L2-Norm.html</a></p><p id="bafe" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[Wie12]维兰德；应对供应链风险。《国际物流与物流管理杂志》，42(10):887–905，2012年。ISSN 0960–0035。doi:10.1108/09600031211281411。网址<a class="ae mo" href="https://www.emeraldinsight.com/doi/pdfplus/10.1108/09600031211281411" rel="noopener ugc nofollow" target="_blank">https://www . emerald insight . com/doi/pdf plus/10.1108/09600031211281411</a></p><p id="7dbe" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[wiki18a]gau-Prozess，2018年10月13日。网址<a class="ae mo" href="https://de.wikipedia.org/w/index.php?oldid=181728459" rel="noopener ugc nofollow" target="_blank">https://de.wikipedia.org/w/index.php?oldid=181728459</a></p><p id="e5bd" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">[Yu12]余，h；金，SVM教程-分类，回归和排名。<br/> G .罗森堡；t .贝克；自然计算手册，479–506。施普林格柏林海德堡，柏林，海德堡，2012。ISBN 978–3–540–92909–3。</p></div></div>    
</body>
</html>