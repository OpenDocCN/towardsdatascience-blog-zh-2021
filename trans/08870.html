<html>
<head>
<title>NLP Data Augmentation using 🤗 Transformers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">NLP数据扩充使用🤗变形金刚(电影名)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/nlp-data-augmentation-using-transformers-89a44a993bab?source=collection_archive---------14-----------------------#2021-08-16">https://towardsdatascience.com/nlp-data-augmentation-using-transformers-89a44a993bab?source=collection_archive---------14-----------------------#2021-08-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b297" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">减少您的数据标注工作</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/9ee10d34aec537faf5a52cc8a4455ca5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NyT2xZPFl98ymTOZjLQE9A.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">托拜厄斯·菲舍尔在<a class="ae kv" href="https://unsplash.com/s/photos/data?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="708b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">NLP(自然语言处理)项目面临的一个最常见的问题是缺乏标记数据。标注数据既昂贵又耗时，而且繁琐。数据扩充技术通过防止过度拟合和使模型更加稳健来帮助我们建立更好的模型。在这篇文章中，我将讲述我们如何使用<a class="ae kv" href="https://huggingface.co/transformers/" rel="noopener ugc nofollow" target="_blank">变形金刚</a>库和预训练模型，如<a class="ae kv" href="https://github.com/google-research/bert" rel="noopener ugc nofollow" target="_blank">伯特</a>、<a class="ae kv" href="https://openai.com/blog/better-language-models/" rel="noopener ugc nofollow" target="_blank"> GPT-2 </a>、<a class="ae kv" href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html" rel="noopener ugc nofollow" target="_blank"> T5 </a>等。轻松扩充我们的文本数据。我还想提一下这篇关于<a class="ae kv" href="https://github.com/google-research/uda" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> <em class="ls">无监督数据增强</em></strong></a><strong class="ky ir"><em class="ls">(UDA)</em></strong>的有趣论文，这篇论文来自谷歌的研究人员，他们在论文中表明，在只有20个标记的例子和数据增强与其他技术相结合的情况下，他们的模型在IMDB数据集上的表现优于最先进的模型，同样的技术在图像分类任务上也显示出良好的结果。这里是UDA的<a class="ae kv" href="https://ai.googleblog.com/2019/07/advancing-semi-supervised-learning-with.html" rel="noopener ugc nofollow" target="_blank">博文</a>、<a class="ae kv" href="https://arxiv.org/abs/1904.12848" rel="noopener ugc nofollow" target="_blank">论文</a>和<a class="ae kv" href="https://github.com/google-research/uda" rel="noopener ugc nofollow" target="_blank"> github </a>代码的链接。这项工作的一部分是基于<a class="ae kv" href="https://github.com/tensorflow/models/tree/master/research/autoaugment" rel="noopener ugc nofollow" target="_blank">自动增强</a> <a class="ae kv" href="https://arxiv.org/abs/1805.09501" rel="noopener ugc nofollow" target="_blank">论文</a>。</p><p id="6806" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">希望我已经让你相信了数据增强的威力，请继续阅读，看看如何使用transformer <a class="ae kv" href="https://huggingface.co/transformers/main_classes/pipelines.html#" rel="noopener ugc nofollow" target="_blank"> pipelines </a>特性，用几行代码实现4种强大的增强技术。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><p id="ca3f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">反向翻译</strong> —这是我发现最有趣的技术，这里我们首先使用一个模型将句子转换成不同的语言，然后再转换回目标语言。当我们对此使用ML模型时，它产生与原始句子等价但具有不同单词的句子。在<a class="ae kv" href="https://huggingface.co/models" rel="noopener ugc nofollow" target="_blank"> Huggingface model hub </a>上有各种预先训练好的模型，比如<a class="ae kv" href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html" rel="noopener ugc nofollow" target="_blank">谷歌T5 </a>，脸书NMT(神经机器翻译)等。在下面的代码中，我使用T5-base进行英语到德语的翻译，然后使用<a class="ae kv" href="https://arxiv.org/abs/1907.12461" rel="noopener ugc nofollow" target="_blank"> Bert2Bert </a>模型进行德语到英语的翻译。我们也可以使用Fairseq <a class="ae kv" href="https://github.com/pytorch/fairseq/blob/master/examples/translation/README.md" rel="noopener ugc nofollow" target="_blank">模型</a>，它既适用于英语到德语，也适用于德语到英语。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ma mb l"/></div></figure><p id="e3d9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以看到，对于输入文本“我去电影院看了一场电影”，我们得到的输出文本是“我去电影院看了一场电影”，它传达了相同的意思，但使用了不同的单词和不同的顺序！我们也可以使用不同的语言，如英语到法语等。创造更多的变化。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><p id="3588" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">随机插入</strong> —在这种技术中，我们在给定的句子中随机插入一个单词。一种方法是随机插入任何单词，但是我们也可以使用像<a class="ae kv" href="https://huggingface.co/bert-base-uncased" rel="noopener ugc nofollow" target="_blank"> BERT </a>这样的预训练模型来根据上下文插入单词。这里我们可以使用transformer pipeline中的“填充-遮罩”任务来插入一个单词。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ma mb l"/></div></figure><p id="c44c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以看到，对于一个输入文本“我去电影院看电影”，BERT模型在随机位置<em class="ls"> </em>插入一个单词<em class="ls">【新】</em>来创建一个新句子“我去电影院看了一部<strong class="ky ir"> <em class="ls">新</em> </strong>电影”，它实际上给出了5个不同的选项，如“我去电影院看了一部<strong class="ky ir"> <em class="ls">小</em> </strong>电影”。因为我们随机选择索引，所以每次都在不同的地方插入单词。在此之后，我们可以使用相似性度量，使用<a class="ae kv" href="https://tfhub.dev/google/universal-sentence-encoder/4" rel="noopener ugc nofollow" target="_blank">通用句子编码器</a>来选择最相似的句子。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><p id="071f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">随机替换</strong> —在这种技术中，我们用新单词替换随机单词，我们可以使用预先构建的字典来替换同义词，或者我们可以使用预先训练的模型，如<a class="ae kv" href="https://huggingface.co/bert-base-uncased" rel="noopener ugc nofollow" target="_blank"> BERT </a>。这里我们再次使用“填充-遮罩”管道。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ma mb l"/></div></figure><p id="86d7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上述代码的一个例子中，我们随机选择单词<em class="ls">“see”</em>，并使用<em class="ls"> </em> BERT将其替换为单词<em class="ls">“watch”</em>，生成一个句子<em class="ls">“I go to watch a movie in the theater”</em>，意思相同但单词不同。我们也可以用同样的方法替换多个单词。对于随机插入和替换，我们也可以使用其他支持“填充-遮罩”任务的模型，如<a class="ae kv" href="https://huggingface.co/distilbert-base-uncased" rel="noopener ugc nofollow" target="_blank"> Distilbert </a>(小而快)<a class="ae kv" href="https://huggingface.co/roberta-base" rel="noopener ugc nofollow" target="_blank"> Roberta </a>甚至多语言模型！</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><p id="e7a7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">文本生成</strong> —在这项技术中，我们使用了生成模型，如<a class="ae kv" href="https://huggingface.co/gpt2" rel="noopener ugc nofollow" target="_blank"> GPT2 </a>、<a class="ae kv" href="https://huggingface.co/distilgpt2" rel="noopener ugc nofollow" target="_blank"> distilgpt2 </a>等。让句子变长。我们输入原始文本作为开始，然后模型根据输入文本生成额外的单词，这样我们就可以在句子中添加随机噪声。如果我们只添加几个单词，并使用相似性得分来确保句子与原始句子相似，那么我们可以在不改变意思的情况下生成附加的句子！</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="ma mb l"/></div></figure><p id="f073" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这里，我们使用“文本生成”管道和GPT-2模型向我们的原始句子添加5个新单词，以获得一个新句子，如<em class="ls">“我去电影院看电影，导演是”，</em>如果我们决定添加10个新单词，我们可以获得一个句子，如<em class="ls">“我去电影院看电影，当我看着另一边并思考时”。</em>所以我们可以<em class="ls"> </em>看到，根据我们的用例，我们可以生成许多不同长度的句子。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><p id="db0d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> Streamlit演示— </strong>下面是我创建的一个Streamlit应用程序的截图，在这里，人们可以以互动的方式使用不同的输入句子来玩4种不同的增强技术。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mc"><img src="../Images/5a81166cab0e2f43afc98dbb024ce1ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iUvPustISjbd2SIOrSpoEw.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Streamlit演示截图(图片由作者提供)</p></figure><p id="d78d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">你可以在我的<a class="ae kv" href="https://github.com/suryavanshi/nlp_augment_streamlit" rel="noopener ugc nofollow" target="_blank">Github</a>T10】repo中找到上述Streamlit应用的完整代码。</p></div><div class="ab cl lt lu hu lv" role="separator"><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly lz"/><span class="lw bw bk lx ly"/></div><div class="ij ik il im in"><p id="6353" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">资源</strong></p><p id="4d99" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">调查论文— <a class="ae kv" href="https://arxiv.org/abs/2110.01852" rel="noopener ugc nofollow" target="_blank">自然语言处理中的数据扩充方法:调查</a></p><p id="f3dd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要了解更多关于变形金刚和自然语言处理的知识，你可以通过拥抱脸来学习这门伟大的课程——<a class="ae kv" href="https://huggingface.co/course/chapter1" rel="noopener ugc nofollow" target="_blank">https://huggingface.co/course/chapter1</a></p><p id="5f2e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我想提一下另一个很棒的库<a class="ae kv" href="https://github.com/makcedward/nlpaug" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> nlpaug </strong> </a>，它提供了很多扩充NLP数据的方法。关于这个库的博文-<a class="ae kv" rel="noopener" target="_blank" href="/data-augmentation-in-nlp-2801a34dfc28">https://towardsdatascience . com/data-augmentation-in-NLP-2801 a 34 DFC 28</a></p><p id="b15d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">UDA博文—<a class="ae kv" href="https://ai.googleblog.com/2019/07/advancing-semi-supervised-learning-with.html" rel="noopener ugc nofollow" target="_blank">https://ai . Google Blog . com/2019/07/advancing-semi-supervised-learning-with . html</a></p><p id="8310" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">语义相似度使用USE—<a class="ae kv" href="https://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/semantic_similarity_with_tf_hub_universal_encoder.ipynb" rel="noopener ugc nofollow" target="_blank">https://colab . research . Google . com/github/tensor flow/hub/blob/master/examples/colab/semantic _ Similarity _ with _ TF _ hub _ universal _ encoder . ipynb</a></p></div></div>    
</body>
</html>