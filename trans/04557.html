<html>
<head>
<title>Stop Using All Your Features for Modeling</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">停止使用所有特征进行建模</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/stop-using-all-your-features-for-modeling-82d487ca8ddd?source=collection_archive---------23-----------------------#2021-04-19">https://towardsdatascience.com/stop-using-all-your-features-for-modeling-82d487ca8ddd?source=collection_archive---------23-----------------------#2021-04-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="81d2" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用递归特征选择来选择最佳特征集</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b096409b3fe2152352bf0fb380fae7f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GXqgjKVSZXaPtdzTYeFMdw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://pixabay.com/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=1690423" rel="noopener ugc nofollow" target="_blank">皮克斯拜</a></p></figure><p id="fe18" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">真实世界的数据集包含大量相关和冗余的要素。的确，就实例或行数而言，更多数据的存在导致训练更好的机器学习模型。</p><p id="9801" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在继续之前，我们必须知道为什么不建议使用所有的功能集。为了训练稳健的机器学习模型，数据必须没有冗余特征。特征选择之所以重要有多种原因:</p><ul class=""><li id="c284" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><strong class="lb iu">垃圾输入，垃圾输出:</strong>用于训练模型的数据质量决定了输出模型的质量。真实世界的数据包含大量冗余特征，需要移除这些特征，以便训练稳健的模型。</li><li id="25bf" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">维数灾难:</strong>随着数据维数或数据中特征数量的增加，特征覆盖的配置数量减少。如果与实例数量相比，数据包含更多的特征，则训练模型不会推广到新的样本。</li><li id="fda2" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><strong class="lb iu">奥卡姆剃刀:</strong>当输入数据具有大量特征时，模型的可解释性降低，因此难以解释模型。</li></ul><p id="f7e9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，从数据集中移除不相关的要素至关重要。数据科学家应该选择他/她用于模型训练的特征。选择所有的特征组合，然后选择最佳的特征集是一个多项式解决方案。选择最佳特征集有各种技巧，阅读<a class="ae ky" rel="noopener" target="_blank" href="/top-7-feature-selection-techniques-in-machine-learning-94e08730cd09">这篇文章</a>，就知道7种特征选择技巧。</p><p id="ca8c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将讨论如何使用递归特征选择算法及其实现来选择最佳特征集。</p><h1 id="8b9e" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">什么是递归特征选择？</h1><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/d2311f792b15c3ff851c06909e19e050.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6wEIOPfh2LXolRwiGe56gw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/File:Feature_selection_Wrapper_Method.png" rel="noopener ugc nofollow" target="_blank">来源</a>，特征选择包装方法</p></figure><p id="70a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">递归特征选择是一种包装方法，它使用贪婪优化算法来选择k个特征的最佳集合。它递归地训练模型，考虑越来越小的特征集，并消除不太相关的特征。逐步递归特征选择算法；</p><ol class=""><li id="3102" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu nc mb mc md bi translated">估计器在所有初始特征集上被训练。</li><li id="e6b6" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nc mb mc md bi translated">使用sklearn库中的特定属性，如<code class="fe nd ne nf ng b"><strong class="lb iu">coef_ </strong></code>或<code class="fe nd ne nf ng b"><strong class="lb iu">feature_importance_</strong></code> <strong class="lb iu">，计算每个特征的重要性。</strong></li><li id="29a0" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nc mb mc md bi translated">从特征集中删除最不重要的特征。</li><li id="b9b8" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nc mb mc md bi translated">递归地重复步骤2和3，直到获得所需数量的特征。</li></ol><h2 id="a5dc" class="nh mk it bd ml ni nj dn mp nk nl dp mt li nm nn mv lm no np mx lq nq nr mz ns bi translated">实施:</h2><p id="8cb0" class="pw-post-body-paragraph kz la it lb b lc nt ju le lf nu jx lh li nv lk ll lm nw lo lp lq nx ls lt lu im bi translated">Sklearn提出了RFE(递归特征消除)实现。开发人员只需要指定估计器并更新参数。</p><pre class="kj kk kl km gt ny ng nz oa aw ob bi"><span id="3fc7" class="nh mk it ng b gy oc od l oe of"><strong class="ng iu">Some important Parameter of RFE function:</strong></span><span id="bb1b" class="nh mk it ng b gy og od l oe of"><strong class="ng iu">estimator: </strong>Supervised learning estimator</span><span id="3233" class="nh mk it ng b gy og od l oe of"><strong class="ng iu">n_features_to_select:</strong> The final number of features to select. If 'None', then 50% of features are selected.</span><span id="9d6f" class="nh mk it ng b gy og od l oe of"><strong class="ng iu">step: </strong>If step&gt;=1 then<strong class="ng iu"> </strong>Number of features to remove at each iteration. els if step in [0,1) then percentage (rounded down) of features to remove at each iteration.</span><span id="a79e" class="nh mk it ng b gy og od l oe of"><strong class="ng iu">importance_getter: </strong>If 'auto', uses the feature importance either through a <em class="oh">coef_ </em>or <em class="oh">feature_importances_</em><em class="oh"> </em>attributes of estimator.</span></pre><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oi oj l"/></div></figure><h1 id="6e97" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">结论:</h1><p id="5cf4" class="pw-post-body-paragraph kz la it lb b lc nt ju le lf nu jx lh li nv lk ll lm nw lo lp lq nx ls lt lu im bi translated">在本文中，我们讨论了如何使用递归特征选择技术来选择k个特征的最佳集合。Sklearn用一行代码实现了RFE。</p><p id="3ad9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还有其他各种包装器方法，包括<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> SelectKBest </strong> </a>，这也是Sklearn实现的一部分。SelectKBest算法根据k个最高分选择相关特征。每个数据科学家都应该知道各种其他的特征选择技术，阅读下面提到的文章<a class="ae ky" rel="noopener" target="_blank" href="/top-7-feature-selection-techniques-in-machine-learning-94e08730cd09">来了解7种这样的特征选择技术。</a></p><div class="ok ol gp gr om on"><a rel="noopener follow" target="_blank" href="/top-7-feature-selection-techniques-in-machine-learning-94e08730cd09"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd iu gy z fp os fr fs ot fu fw is bi translated">机器学习中的7大特征选择技术</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">选择最佳功能的流行策略</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">towardsdatascience.com</p></div></div><div class="ow l"><div class="ox l oy oz pa ow pb ks on"/></div></div></a></div><h1 id="1f97" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">参考资料:</h1><p id="4018" class="pw-post-body-paragraph kz la it lb b lc nt ju le lf nu jx lh li nv lk ll lm nw lo lp lq nx ls lt lu im bi translated">[1] RFE文档:<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/generated/sk learn . feature _ selection。RFE.html</a></p><blockquote class="pc"><p id="55c9" class="pd pe it bd pf pg ph pi pj pk pl lu dk translated">感谢您的阅读</p></blockquote></div></div>    
</body>
</html>