<html>
<head>
<title>A Guide To Encoding Text In Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python文本编码指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-guide-to-encoding-text-in-python-ef783e50f09e?source=collection_archive---------11-----------------------#2021-03-20">https://towardsdatascience.com/a-guide-to-encoding-text-in-python-ef783e50f09e?source=collection_archive---------11-----------------------#2021-03-20</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="f7ca" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/nlpnotes" rel="noopener" target="_blank">自然语言处理笔记</a></h2><div class=""/><div class=""><h2 id="5d7c" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">教计算机理解人类语言</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/e4d1aba28f2b6b3cb9c0433ebf5f84c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*uu5bCHVJry--FmIM"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@goshua13?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">约书亚·阿拉贡</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="84c9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们对文本进行编码的原因是计算机不理解字符、单词或句子；计算机只能处理数字，因此，如果我们希望教会计算机理解自然语言，那么我们应该在将文本数据输入任何机器之前将其编码成数字格式。</p><p id="5b36" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">因此，文本编码可以定义为将文本转换成有意义的数字/矢量表示的过程。在这个过程中，我们希望保留单词和句子之间的上下文和依赖关系，以便机器能够检测与文本相关的模式以及理解上下文。</p><p id="dc7f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">对文本进行编码有多种方式:</p><ul class=""><li id="6fe5" class="me mf it lk b ll lm lo lp lr mg lv mh lz mi md mj mk ml mm bi translated">一键编码</li><li id="170d" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">基于索引的编码</li><li id="c23d" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">单词袋(蝴蝶结)</li><li id="200a" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">术语频率—反向文档频率(TF-IDF)</li><li id="7d12" class="me mf it lk b ll mn lo mo lr mp lv mq lz mr md mj mk ml mm bi translated">单词嵌入</li></ul><p id="8399" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这不是全部的清单，但足够开始了。每种编码都有自己的优缺点，每种编码都可能更适合一项任务。BOW等最基本的编码不保留单词的顺序，而其他编码则利用Word2Vec等神经网络。</p><p id="0fcf" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们用Python实现这些。</p><h2 id="07d6" class="ms mt it bd mu mv mw dn mx my mz dp na lr nb nc nd lv ne nf ng lz nh ni nj iz bi translated">一键编码</h2><p id="ccd5" class="pw-post-body-paragraph li lj it lk b ll nk kd ln lo nl kg lq lr nm lt lu lv nn lx ly lz no mb mc md im bi translated">一键编码将分类变量表示为二进制向量。要实现这一点，首先应该将分类值映射到一个整数，然后将每个整数表示为一个二进制向量——这意味着该向量除了整数的索引之外都是0。</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="2905" class="ms mt it nq b gy nu nv l nw nx"># define the documents<br/>corpus = ["i cant wait to get out of lockdown", "the uk is soon going to be free soon", "linkedin is social media"]</span><span id="3a24" class="ms mt it nq b gy ny nv l nw nx"># converting text to intergers<br/>token_docs = [doc.split() for doc in corpus]<br/>all_tokens = set([word for sentence in token_docs for word in sentence])<br/>word_to_idx = {token:idx+1 for idx, token in enumerate(all_tokens)}</span><span id="097e" class="ms mt it nq b gy ny nv l nw nx"># converting the docs to their token ids<br/>token_ids = np.array([[word_to_idx[token] for token in token_doc] for token_doc in token_docs], dtype=object)<br/>token_ids_padded = pad_sequences(token_ids, padding="post")<br/>token_ids = token_ids.reshape(-1, 1)</span><span id="8a91" class="ms mt it nq b gy ny nv l nw nx"># convert the token ids to one hot representation<br/>one_hot = OneHotEncoder()<br/>X = one_hot.fit_transform(token_ids_padded)</span><span id="b647" class="ms mt it nq b gy ny nv l nw nx"># converting to dataframe<br/>X_df = pd.DataFrame(X.toarray())<br/>X_df</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nz"><img src="../Images/2874fb81d8256940fda4ee9670cfda51.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gnz98m8C0xRQ1oclUuCWcw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">一键编码实施—按作者分类的图像</p></figure><p id="1bf7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">每个文档由一个张量表示，每个张量由一系列0和1组成。尽管一键编码保留了单词顺序，但随着文档变长，我们最终会得到非常大的稀疏向量。</p><h2 id="00ae" class="ms mt it bd mu mv mw dn mx my mz dp na lr nb nc nd lv ne nf ng lz nh ni nj iz bi translated">基于索引的编码</h2><p id="601a" class="pw-post-body-paragraph li lj it lk b ll nk kd ln lo nl kg lq lr nm lt lu lv nn lx ly lz no mb mc md im bi translated">基于索引的编码也保留了单词顺序。要实现基于索引的编码，只需用一个索引映射每个单词。我们还需要给每个句子添加填充，这样它们的长度就都相等了。基于字典中的映射，每个文档通过一系列索引来表示，其中每个数字是一个单词的编码。</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="7a26" class="ms mt it nq b gy nu nv l nw nx">import numpy as np<br/>import pandas as pd<br/>from sklearn.preprocessing import OneHotEncoder<br/>from tensorflow.keras.preprocessing.sequence import pad_sequences</span><span id="2163" class="ms mt it nq b gy ny nv l nw nx"># define the documents<br/>corpus = ["i cant wait to get out of lockdown", "the uk is soon going to be free soon", "linkedin is social media"]</span><span id="c6c7" class="ms mt it nq b gy ny nv l nw nx"># converting text to intergers<br/>token_docs = [doc.split() for doc in corpus]<br/>all_tokens = set([word for sentence in token_docs for word in sentence])<br/>word_to_idx = {token:idx+1 for idx, token in enumerate(all_tokens)}</span><span id="2555" class="ms mt it nq b gy ny nv l nw nx"># converting the docs to their token ids<br/>X = np.array([[word_to_idx[token] for token in token_doc] for token_doc in token_docs], dtype=object)</span><span id="e74e" class="ms mt it nq b gy ny nv l nw nx"># padding the sequences<br/>X_padded = pad_sequences(X, padding="post")</span><span id="5c4d" class="ms mt it nq b gy ny nv l nw nx"># converting to pandas df<br/>X_df = pd.DataFrame(X_padded) </span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/99615bb348fa99d3105cee43435f31f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*ZItXuIUbfuwQc-54mEo47Q.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">索引编码数据帧—按作者分类的图像</p></figure><p id="aad3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">注意</strong>:列名代表单词在文本中出现的位置</p><p id="9b58" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">基于索引的编码之所以失败，是因为它在文本之间引入了不真实的数字距离，例如，为什么单词“cat”的索引是数字1，而“dog”是数字324？</p><h2 id="f055" class="ms mt it bd mu mv mw dn mx my mz dp na lr nb nc nd lv ne nf ng lz nh ni nj iz bi translated">单词袋(蝴蝶结)</h2><p id="5f62" class="pw-post-body-paragraph li lj it lk b ll nk kd ln lo nl kg lq lr nm lt lu lv nn lx ly lz no mb mc md im bi translated">单词包编码技术的名称来源于这样一个事实，即文档中单词的任何信息或结构都将被丢弃，因此，这就像您将一组单词放入一个包中并摇晃它，它只关心一个单词是否出现在文档中以及它出现的次数。它不在乎它出现在哪里。</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="4185" class="ms mt it nq b gy nu nv l nw nx">import pandas as pd<br/>from sklearn.feature_extraction.text import CountVectorizer</span><span id="2159" class="ms mt it nq b gy ny nv l nw nx"># define the documents<br/>corpus = ["i cant wait to get out of lockdown", "the uk is soon going to be free soon", "linkedin is social media"]</span><span id="e145" class="ms mt it nq b gy ny nv l nw nx"># implementing BoW<br/>bow = CountVectorizer()<br/>bow.fit(corpus)<br/>X = bow.transform(corpus)</span><span id="65c0" class="ms mt it nq b gy ny nv l nw nx"># converting to dataframe<br/>X_df = pd.DataFrame(X.toarray(), columns=sorted(bow.vocabulary_))<br/>X_df</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ob"><img src="../Images/dc6fa1df0f4900d7c6312c17f9459f1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*d5JnINaHRSnGVZEvSsC9ZA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">BoW数据框—作者图片</p></figure><p id="9a45" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">虽然BoW模型是一个非常容易掌握和实现的模型，但它在许多方面都存在不足，最明显的是失去了意义，因为我们放弃了词序，我们失去了使用该词的上下文。</p><h2 id="6951" class="ms mt it bd mu mv mw dn mx my mz dp na lr nb nc nd lv ne nf ng lz nh ni nj iz bi translated">术语频率—反向文档频率(TF-IDF)</h2><p id="209b" class="pw-post-body-paragraph li lj it lk b ll nk kd ln lo nl kg lq lr nm lt lu lv nn lx ly lz no mb mc md im bi translated">对词的频率进行评分的问题是，尽管没有给文档增加多少价值，但高频词开始在文档中占据主导地位。例如“和”、“该”、“一个”等词语。在英语中很常见，所以这些单词在文档中出现得更频繁是很常见的。TF-IDF认识到了这个问题，并对所有文档中最突出的单词进行了惩罚。</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="137d" class="ms mt it nq b gy nu nv l nw nx">import pandas as pd<br/>from sklearn.feature_extraction.text import TfidfVectorizer</span><span id="b828" class="ms mt it nq b gy ny nv l nw nx"># define the documents<br/>corpus = ["i cant wait to get out of lockdown", "the uk is soon going to be free soon", "linkedin is social media"]</span><span id="c7cc" class="ms mt it nq b gy ny nv l nw nx"># implement tfidf<br/>tfidf = TfidfVectorizer()<br/>tfidf.fit(corpus)<br/>X = tfidf.transform(corpus)</span><span id="6ba2" class="ms mt it nq b gy ny nv l nw nx"># convert to dataframe<br/>X_df = pd.DataFrame(X.toarray(), columns=sorted(tfidf.vocabulary_))<br/>X_df.T</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/7cb6e41ab8e18b0bfeae9ea7d65b8d65.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*jXbQJoolXmKPCEIuBCSLFA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">TF-IDF数据帧的转置—作者提供的图像</p></figure><p id="4fd8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然而，由于TF-IDF编码是基于BOW编码的，它也不能捕获文本中的单词位置、语义以及在不同文档中的共现，但是它也很容易计算。</p><h2 id="1b14" class="ms mt it bd mu mv mw dn mx my mz dp na lr nb nc nd lv ne nf ng lz nh ni nj iz bi translated">单词嵌入</h2><p id="5615" class="pw-post-body-paragraph li lj it lk b ll nk kd ln lo nl kg lq lr nm lt lu lv nn lx ly lz no mb mc md im bi translated">单词嵌入是迁移学习的一个很好的例子。迁移学习是机器学习中的一个研究问题，它专注于存储在解决一个问题时获得的知识，并将其应用于不同但相关的问题。【<strong class="lk jd">来源</strong> : <a class="ae lh" href="https://en.wikipedia.org/wiki/Transfer_learning" rel="noopener ugc nofollow" target="_blank">维基百科</a>。</p><p id="edac" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">深入各种单词嵌入或迁移学习如何工作的全部细节超出了本文的范围，但是我们仍然会看到如何实现Word2Vec编码。</p><pre class="ks kt ku kv gt np nq nr ns aw nt bi"><span id="18ea" class="ms mt it nq b gy nu nv l nw nx">from gensim.models import Word2Vec</span><span id="a737" class="ms mt it nq b gy ny nv l nw nx"># define the documents<br/>corpus = ["i cant wait to get out of lockdown", "the uk is soon going to be free soon", "linkedin is social media"]</span><span id="8590" class="ms mt it nq b gy ny nv l nw nx"># Word2Vec model<br/>w2v = Word2Vec(min_count=1,<br/>               size=20,<br/>               window=2,<br/>               alpha=0.03)</span><span id="3c32" class="ms mt it nq b gy ny nv l nw nx"># building the vocab<br/>w2v.build_vocab(sentences=corpus)</span><span id="9258" class="ms mt it nq b gy ny nv l nw nx"># training the model<br/>w2v.train(sentences=corpus, total_examples=w2v.corpus_count, epochs=10)</span></pre><blockquote class="od oe of"><p id="d6c0" class="li lj og lk b ll lm kd ln lo lp kg lq oh ls lt lu oi lw lx ly oj ma mb mc md im bi translated"><strong class="lk jd">注</strong>:NLP会专门为迁移学习做一篇文章。</p></blockquote><h2 id="dee5" class="ms mt it bd mu mv mw dn mx my mz dp na lr nb nc nd lv ne nf ng lz nh ni nj iz bi translated">包裹</h2><p id="ea14" class="pw-post-body-paragraph li lj it lk b ll nk kd ln lo nl kg lq lr nm lt lu lv nn lx ly lz no mb mc md im bi translated">对文本数据[和所有其他类型的数据]进行编码的目的是确保数据可以被机器正确地使用。在本文中，我介绍了5种不同的文本数据编码方法。当上下文真的很重要时，我喜欢使用分布式表示，比如单词嵌入，但是有很多例子表明像单词袋这样简单的技术是有效的，比如分类任务。</p><p id="330a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">感谢阅读！在<a class="ae lh" href="https://www.linkedin.com/in/kurtispykes/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>和<a class="ae lh" href="https://twitter.com/KurtisPykes" rel="noopener ugc nofollow" target="_blank"> Twitter </a>上与我联系，了解我关于人工智能、数据科学和自由职业的最新帖子。</p><h2 id="49fa" class="ms mt it bd mu mv mw dn mx my mz dp na lr nb nc nd lv ne nf ng lz nh ni nj iz bi translated">相关文章</h2><div class="ok ol gp gr om on"><a rel="noopener follow" target="_blank" href="/a-guide-to-cleaning-text-in-python-943356ac86ca?source=collection_tagged---------0----------------------------"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd jd gy z fp os fr fs ot fu fw jc bi translated">Python文本清理指南</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">文本是非结构化数据的一种形式。根据维基百科，非结构化数据被描述为“信息…</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">towardsdatascience.com</p></div></div><div class="ow l"><div class="ox l oy oz pa ow pb lb on"/></div></div></a></div><div class="ok ol gp gr om on"><a rel="noopener follow" target="_blank" href="/5-ideas-for-your-next-nlp-project-c6bf5b86935c"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd jd gy z fp os fr fs ot fu fw jc bi translated">下一个NLP项目的5个想法</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">产生一些想法</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">towardsdatascience.com</p></div></div><div class="ow l"><div class="pc l oy oz pa ow pb lb on"/></div></div></a></div><div class="ok ol gp gr om on"><a rel="noopener follow" target="_blank" href="/sentiment-analysis-predicting-whether-a-tweet-is-about-a-disaster-c004d09d7245"><div class="oo ab fo"><div class="op ab oq cl cj or"><h2 class="bd jd gy z fp os fr fs ot fu fw jc bi translated">情绪分析:预测一条推文是否是关于一场灾难</h2><div class="ou l"><h3 class="bd b gy z fp os fr fs ot fu fw dk translated">探索自然语言处理</h3></div><div class="ov l"><p class="bd b dl z fp os fr fs ot fu fw dk translated">towardsdatascience.com</p></div></div><div class="ow l"><div class="pd l oy oz pa ow pb lb on"/></div></div></a></div></div></div>    
</body>
</html>