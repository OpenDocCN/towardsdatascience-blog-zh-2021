<html>
<head>
<title>How to Mitigate Overfitting with K-Fold Cross-Validation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用K-Fold交叉验证减轻过度拟合</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-mitigate-overfitting-with-k-fold-cross-validation-518947ed7428?source=collection_archive---------12-----------------------#2021-09-21">https://towardsdatascience.com/how-to-mitigate-overfitting-with-k-fold-cross-validation-518947ed7428?source=collection_archive---------12-----------------------#2021-09-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="e2eb" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">解决过度拟合问题</h2><div class=""/><div class=""><h2 id="fe78" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">解决过度拟合问题—第1部分</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/e7bc82b829be0541b1c0188d753e7199.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-YlibfVJD55fQDfxzi9JUw.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">约书亚·苏考夫在<a class="ae lh" href="https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="1b67" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">过度拟合</strong>几乎每次在我们构建机器学习模型时都会发生，尤其是基于树的模型，比如决策树。<strong class="lk jd"> <em class="me">完全避免</em> </strong>过拟合的问题是不可能的。然而，我们可以尝试一些标准技术来<strong class="lk jd"> <em class="me">减轻</em> </strong>过度拟合。有几个这样的技术要讨论。它们不可能在一个岗位上完成。所以，我决定一个一个讨论。这是第1部分，我们在其中讨论如何用<strong class="lk jd"> <em class="me"> k倍交叉验证</em> </strong>来减轻过拟合。这一部分也是讨论其他技术的基础。</p><p id="4dbe" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们的最终目标应该是建立一个对看不见的数据(测试数据)和训练数据都表现良好的模型。这种模式再好不过了。另一方面，当您的模型过度拟合数据时，它会在训练数据上给出高得多的性能分数(例如100%的准确性)，而在看不见的数据(测试数据)上给出较低的性能分数。在这种情况下，<strong class="lk jd"> <em class="me">无法针对新的未见过的数据(测试数据)推广</em> </strong>。当这种情况发生时，模型试图记住训练数据中的噪声，而不是试图学习数据中的重要模式。这相当于现实生活中的情况，即学生试图通过记住模拟考试的答案来面对期末考试，而不是应用从模拟考试中获得的知识。这个学生将无法在期末考试中取得好成绩！</p><h1 id="a588" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">无k倍交叉验证的训练</h1><p id="5fd4" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">我们将在一个名为<strong class="lk jd"/><a class="ae lh" href="https://drive.google.com/file/d/19s5qMRjssBoohFb2NY4FFYQ3YW2eCxP4/view?usp=sharing" rel="noopener ugc nofollow" target="_blank"><strong class="lk jd">heart _ disease . CSV</strong></a><strong class="lk jd"/><em class="me">的数据集上建立一个决策树分类模型，而不需要</em>做k重交叉验证。然后，我们用混淆矩阵得到训练和测试的准确度分数。通过查看这些输出，我们可以确定模型是否过度拟合。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nc nd l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">无k重交叉验证的决策树分类模型</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/e2f2f2f3a11ab4a4414110fcc4c0b6e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:748/format:webp/1*PVnvbLQlcNAp1bEAQHBdOQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">(图片由作者提供)</p></figure><p id="52a8" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这一次，我们的模型明显过度拟合。我们如何决定？该模型在训练集上表现得非常好(100%的训练准确率！)并且在测试数据上表现不佳(只有71%的准确率)。</p><h1 id="8fa8" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">k倍交叉验证训练</h1><p id="24ef" class="pw-post-body-paragraph li lj it lk b ll mx kd ln lo my kg lq lr mz lt lu lv na lx ly lz nb mb mc md im bi translated">现在，我们将训练相同的决策树模型，但是这一次，<em class="me">使用</em> k倍交叉验证。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nc nd l"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">同一个k重交叉验证的决策树模型</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/421de7109a12749356eed39931eb7804.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/format:webp/1*Oz87XpI_yotzzsiRmJsRJQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">(图片由作者提供)</p></figure><p id="e5e6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这一次，我们的模型可能仍然是过度拟合。但是，精度(实际上是测试精度)提高了6%。</p><h1 id="30aa" class="mf mg it bd mh mi mj mk ml mm mn mo mp ki mq kj mr kl ms km mt ko mu kp mv mw bi translated">洞察力</h1><blockquote class="ng nh ni"><p id="824c" class="li lj me lk b ll lm kd ln lo lp kg lq nj ls lt lu nk lw lx ly nl ma mb mc md im bi translated">实际上，k重交叉验证本身并不能减轻过度拟合<em class="it"/>。然而，它有助于我们检测大量选项(我们有增加模型准确性的空间)来减轻过度拟合。当将k-fold交叉验证与网格搜索等超参数调整技术相结合时，我们肯定可以减轻过度拟合。对于像决策树这样的基于树的模型，有一些特殊的技术可以减轻过度拟合。几种这样的技术是:预修剪、后修剪和创建集合。如果你想详细了解它们，请阅读我的文章:</p></blockquote><div class="nm nn gp gr no np"><a rel="noopener follow" target="_blank" href="/4-useful-techniques-that-can-mitigate-overfitting-in-decision-trees-87380098bd3c"><div class="nq ab fo"><div class="nr ab ns cl cj nt"><h2 class="bd jd gy z fp nu fr fs nv fu fw jc bi translated">4种可以减轻决策树过度拟合的有用技术</h2><div class="nw l"><p class="bd b dl z fp nu fr fs nv fu fw dk translated">towardsdatascience.com</p></div></div><div class="nx l"><div class="ny l nz oa ob nx oc lb np"/></div></div></a></div><p id="e866" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在应用k倍交叉验证后，准确度分数增加了6%的原因是，交叉验证过程通过将数据集分成10个不同的折叠(指定为<strong class="lk jd"> cv=10 </strong>)平均出了10组准确度分数。这样，模型在每个训练阶段看到不同类型的实例(数据点)。因此，该模型有许多不同的数据来学习新的模式。因此，它将很好地推广新的看不见的数据(测试数据)。这就是为什么我们看到了准确性得分的增加。</p><p id="a448" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在第一种情况下，我们构建了我们的模型<em class="me">而没有</em>进行k重交叉验证，该模型只看到一重随机数据，指定为<strong class="lk jd"> random_state=0 </strong>。</p><p id="a68e" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">如果你不明白最后几段，不要担心！你可以阅读下面这篇文章，在这篇文章中，我解释了几乎所有与k-fold交叉验证相关的内容:</p><p id="19b3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/k-fold-cross-validation-explained-in-plain-english-659e33c0bc0">用简单的英语解释k倍交叉验证</a></p><p id="8389" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">更新(2021–09–24):</strong>第2部分现已推出！<br/> [ <a class="ae lh" href="https://rukshanpramoditha.medium.com/how-to-mitigate-overfitting-with-regularization-befcf4e41865" rel="noopener">如何通过正则化减轻过拟合</a></p></div><div class="ab cl od oe hx of" role="separator"><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi oj"/><span class="og bw bk oh oi"/></div><div class="im in io ip iq"><p id="6d9f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">今天的帖子到此结束。我的读者可以通过下面的链接注册成为会员，以获得我写的每个故事的全部信息，我将收到你的一部分会员费。</p><p id="a223" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">报名链接:</strong>【https://rukshanpramoditha.medium.com/membership T2】</p><p id="35fa" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">非常感谢你一直以来的支持！下一个故事再见。祝大家学习愉快！</p><p id="1874" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">特别要感谢Unsplash网站上的<strong class="lk jd">约书亚·苏考夫</strong>，<strong class="lk jd"> </strong>为我提供了这篇文章的封面图片(我对图片做了一些修改:添加了一些文字，删除了一些部分)。</p><p id="52da" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ok ol ep" href="https://medium.com/u/f90a3bb1d400?source=post_page-----518947ed7428--------------------------------" rel="noopener" target="_blank">鲁克山普拉莫迪塔</a><br/><strong class="lk jd">2021–09–21</strong></p></div></div>    
</body>
</html>