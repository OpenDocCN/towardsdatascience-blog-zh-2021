<html>
<head>
<title>Building a Convolutional Neural Network Model to Understand Scenes</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">构建卷积神经网络模型以理解场景</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/building-a-convolutional-neural-network-model-to-understand-scenes-1673abd9884d?source=collection_archive---------18-----------------------#2021-11-02">https://towardsdatascience.com/building-a-convolutional-neural-network-model-to-understand-scenes-1673abd9884d?source=collection_archive---------18-----------------------#2021-11-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="e432" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">R 中的数据科学</h2><div class=""/><div class=""><h2 id="1d78" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">一个完整的循序渐进的数据扩充和转移学习教程</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/3b308ef7988c0bbcc2d9d144ddd2e78f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GFkgBlLtlUfPV3VL2UkVBw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">照片由<a class="ae lh" href="https://unsplash.com/@boontohhgraphy" rel="noopener ugc nofollow" target="_blank">索拉萨克</a>、<a class="ae lh" href="https://unsplash.com/@michael_g_krahn" rel="noopener ugc nofollow" target="_blank">迈克尔·克拉恩</a>、<a class="ae lh" href="https://unsplash.com/@prevailz" rel="noopener ugc nofollow" target="_blank">肖恩·皮尔斯</a>、<a class="ae lh" href="https://unsplash.com/@mister_guiz" rel="noopener ugc nofollow" target="_blank">纪尧姆·布里亚德</a>、<a class="ae lh" href="https://unsplash.com/@sotti" rel="noopener ugc nofollow" target="_blank">希法兹·沙蒙</a>和<a class="ae lh" href="https://unsplash.com/@ryoji__iwata" rel="noopener ugc nofollow" target="_blank">岩田良治</a>在<a class="ae lh" href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="f193" class="ln lo it lj b gy lp lq l lr ls"><strong class="lj jd">Table of Contents</strong></span><span id="d810" class="ln lo it lj b gy lt lq l lr ls">· <a class="ae lh" href="#dcd8" rel="noopener ugc nofollow">Library</a><br/>· <a class="ae lh" href="#44f0" rel="noopener ugc nofollow">Dataset</a><br/>· <a class="ae lh" href="#7f82" rel="noopener ugc nofollow">Exploratory Data Analysis</a><br/>· <a class="ae lh" href="#e848" rel="noopener ugc nofollow">Data Preprocessing</a><br/>· <a class="ae lh" href="#8635" rel="noopener ugc nofollow">Modeling</a><br/>  ∘ <a class="ae lh" href="#a08c" rel="noopener ugc nofollow">Simple CNN</a><br/>  ∘ <a class="ae lh" href="#161c" rel="noopener ugc nofollow">Deeper CNN</a><br/>  ∘ <a class="ae lh" href="#0f40" rel="noopener ugc nofollow">Deeper CNN with Pretrained Weights</a><br/>· <a class="ae lh" href="#fb48" rel="noopener ugc nofollow">Conclusion</a></span></pre><p id="18e7" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi mq translated">自从我开始在<a class="mz na ep" href="https://medium.com/u/504c7870fdb6?source=post_page-----1673abd9884d--------------------------------" rel="noopener" target="_blank">介质</a>上写作，我就非常依赖<a class="ae lh" href="http://unsplash.com" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>。这是一个美丽的地方，创造高品质的图像。但是你知道 Unsplash 用机器学习来帮助标记照片吗？</p><blockquote class="nb"><p id="1b22" class="nc nd it bd ne nf ng nh ni nj nk mp dk translated">对于上传到 Unsplash […]的每张图片，我们都会通过一系列机器学习算法来理解照片的内容，从而消除了投稿人手动标记照片的需要。— <a class="ae lh" href="https://unsplash.com/blog/introducing-unsplashs-new-uploaders/" rel="noopener ugc nofollow" target="_blank"> Unsplash 博客</a></p></blockquote><p id="f6a1" class="pw-post-body-paragraph lu lv it lw b lx nl kd lz ma nm kg mc md nn mf mg mh no mj mk ml np mn mo mp im bi translated">给照片贴标签是一项非常重要的任务，可以由机器快速完成。因此，我们将建立一个模型，可以从图像中提取信息，并给出正确的标签，以根据主题位置对图像数据库进行分类。我们将使用卷积神经网络(CNN)进行预测，以对图像是关于“建筑物”、“森林”、“冰川”、“山”、“海”还是“街道”进行分类。所以，这是一个图像分类问题。</p><h1 id="dcd8" class="nq lo it bd nr ns nt nu nv nw nx ny nz ki oa kj ob kl oc km od ko oe kp of og bi translated">图书馆</h1><p id="371b" class="pw-post-body-paragraph lu lv it lw b lx oh kd lz ma oi kg mc md oj mf mg mh ok mj mk ml ol mn mo mp im bi translated">除了我们通常在 R 中使用的循环库，我们还将利用<a class="ae lh" href="https://keras.rstudio.com/" rel="noopener ugc nofollow" target="_blank"> keras </a>。Keras 是一个高级神经网络 API，开发的目的是实现快速实验。</p><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="d324" class="ln lo it lj b gy lp lq l lr ls">library(keras)        # deep learning<br/>library(tidyverse)    # data wrangling<br/>library(imager)       # image manipulation<br/>library(caret)        # model evaluation<br/>library(grid)         # display images in a grid<br/>library(gridExtra)    # display images in a grid</span><span id="3fd4" class="ln lo it lj b gy lt lq l lr ls">RS &lt;- 42              # random state constant</span></pre><p id="6f31" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">注意，我们创建了一个名为<code class="fe om on oo lj b">RS</code>的变量，它只是一个数字，用于未来随机过程的再现性。</p><h1 id="44f0" class="nq lo it bd nr ns nt nu nv nw nx ny nz ki oa kj ob kl oc km od ko oe kp of og bi translated">资料组</h1><p id="107b" class="pw-post-body-paragraph lu lv it lw b lx oh kd lz ma oi kg mc md oj mf mg mh ok mj mk ml ol mn mo mp im bi translated">数据由带有 6 个不同标签的图像组成:“建筑物”、“森林”、“冰川”、“山”、“海”和“街道”。不像<a class="ae lh" href="https://medium.com/data-folks-indonesia/hand-gesture-recognition-8c0e2927a8bb" rel="noopener">上一篇</a>文章中的图像像素数据已经被转换成一个<code class="fe om on oo lj b">.csv</code>文件的列，这次我们使用数据生成器直接读取图像。</p><div class="op oq gp gr or os"><a href="https://medium.com/data-folks-indonesia/hand-gesture-recognition-8c0e2927a8bb" rel="noopener follow" target="_blank"><div class="ot ab fo"><div class="ou ab ov cl cj ow"><h2 class="bd jd gy z fp ox fr fs oy fu fw jc bi translated">手势识别</h2><div class="oz l"><p class="bd b dl z fp ox fr fs oy fu fw dk translated">medium.com</p></div></div><div class="pa l"><div class="pb l pc pd pe pa pf lb os"/></div></div></a></div><p id="8199" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">为此，我们需要知道图像文件夹结构，如下所示。</p><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="ab82" class="ln lo it lj b gy lp lq l lr ls">seg_train<br/>└── seg_train<br/>    ├── buildings<br/>    ├──<!-- --> forest<br/>    ├──<!-- --> glacier<br/>    ├── mountain<br/>    ├── sea<br/>    └──<!-- --> street</span><span id="5b62" class="ln lo it lj b gy lt lq l lr ls">seg_test<br/>└── seg_test<br/>    ├── buildings<br/>    ├──<!-- --> forest<br/>    ├──<!-- --> glacier<br/>    ├── mountain<br/>    ├── sea<br/>    └──<!-- --> street</span></pre><p id="8aaf" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">在每个<code class="fe om on oo lj b">buildings</code>、<code class="fe om on oo lj b">forest</code>、<code class="fe om on oo lj b">glacier</code>、<code class="fe om on oo lj b">mountain</code>、<code class="fe om on oo lj b">sea</code>和<code class="fe om on oo lj b">street</code>子文件夹中，保存有相应的图像。顾名思义，我们将使用<code class="fe om on oo lj b">seg_train</code>进行模型训练，使用<code class="fe om on oo lj b">seg_test</code>进行模型验证。</p><h1 id="7f82" class="nq lo it bd nr ns nt nu nv nw nx ny nz ki oa kj ob kl oc km od ko oe kp of og bi translated">探索性数据分析</h1><p id="d312" class="pw-post-body-paragraph lu lv it lw b lx oh kd lz ma oi kg mc md oj mf mg mh ok mj mk ml ol mn mo mp im bi translated">首先，我们需要定位每个类别的父文件夹地址。</p><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="6a70" class="ln lo it lj b gy lp lq l lr ls">folder_list &lt;- list.files("seg_train/seg_train/")<br/>folder_path &lt;- paste0("seg_train/seg_train/", folder_list, "/")<br/>folder_path</span><span id="6e61" class="ln lo it lj b gy lt lq l lr ls">#&gt; [1] "seg_train/seg_train/buildings/" "seg_train/seg_train/forest/"    "seg_train/seg_train/glacier/"   "seg_train/seg_train/mountain/" <br/>#&gt; [5] "seg_train/seg_train/sea/"       "seg_train/seg_train/street/"</span></pre><p id="c09e" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">然后，列出来自每个父文件夹地址的所有<code class="fe om on oo lj b">seg_train</code>图像地址。</p><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="4900" class="ln lo it lj b gy lp lq l lr ls">file_name &lt;- <br/>  map(folder_path, function(x) paste0(x, list.files(x))) %&gt;% <br/>  unlist()</span></pre><p id="47cd" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">下面我们可以看到总共有 14034 张<code class="fe om on oo lj b">seg_train</code>图片。</p><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="c444" class="ln lo it lj b gy lp lq l lr ls">cat("Number of train images:", length(file_name))</span><span id="4d1d" class="ln lo it lj b gy lt lq l lr ls">#&gt; Number of train images: 14034</span></pre><p id="75dd" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">作为理智检查，让我们看几张<code class="fe om on oo lj b">seg_train</code>图片。</p><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="e198" class="ln lo it lj b gy lp lq l lr ls">set.seed(RS)<br/>sample_image &lt;- sample(file_name, 18)<br/>img &lt;- map(sample_image, load.image)<br/>grobs &lt;- lapply(img, rasterGrob)<br/>grid.arrange(grobs=grobs, ncol=6)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pg"><img src="../Images/605b4f2e0261130a603adeb60b4cc945.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VMjDkC6FiozuAHTqgTCNmg.png"/></div></div></figure><p id="d8b3" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">拿第一张图片来说。</p><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="d2bc" class="ln lo it lj b gy lp lq l lr ls">img &lt;- load.image(file_name[1])<br/>img</span><span id="a574" class="ln lo it lj b gy lt lq l lr ls">#&gt; Image. Width: 150 pix Height: 150 pix Depth: 1 Colour channels: 3</span></pre><p id="aee7" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">下面可以看到，这张图的尺寸是 150 × 150 × 1 × 3。这意味着这个特定的图像具有 150 像素的宽度、150 像素的高度、1 像素的深度和 3 个颜色通道(用于红色、绿色和蓝色，也称为 RGB)。</p><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="1099" class="ln lo it lj b gy lp lq l lr ls">dim(img)</span><span id="1584" class="ln lo it lj b gy lt lq l lr ls">#&gt; [1] 150 150   1   3</span></pre><p id="0306" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">现在，我们将构建一个函数来获取图像的宽度和高度，并将该函数应用于所有图像。</p><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="94e5" class="ln lo it lj b gy lp lq l lr ls">get_dim &lt;- function(x){<br/>  img &lt;- load.image(x) <br/>  df_img &lt;- data.frame(<br/>    width = width(img),<br/>    height = height(img),<br/>    filename = x<br/>  )<br/>  return(df_img)<br/>}<br/><br/>file_dim &lt;- map_df(file_name, get_dim)<br/>head(file_dim)</span><span id="d7dc" class="ln lo it lj b gy lt lq l lr ls">#&gt;   width height                                filename<br/>#&gt; 1   150    150     seg_train/seg_train/buildings/0.jpg<br/>#&gt; 2   150    150 seg_train/seg_train/buildings/10006.jpg<br/>#&gt; 3   150    150  seg_train/seg_train/buildings/1001.jpg<br/>#&gt; 4   150    150 seg_train/seg_train/buildings/10014.jpg<br/>#&gt; 5   150    150 seg_train/seg_train/buildings/10018.jpg<br/>#&gt; 6   150    150 seg_train/seg_train/buildings/10029.jpg</span></pre><p id="da46" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">我们得到了图像的宽度和高度的如下分布。</p><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="2d2d" class="ln lo it lj b gy lp lq l lr ls">hist(file_dim$width, breaks = 20)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="ab gu cl ph"><img src="../Images/f2afcaa247186d9058775c025ca7674f.png" data-original-src="https://miro.medium.com/v2/format:webp/1*fz14Yaq2ACsRcTJZV4qDMA.png"/></div></figure><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="005e" class="ln lo it lj b gy lp lq l lr ls">hist(file_dim$height, breaks = 20)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="ab gu cl ph"><img src="../Images/b2227fea7d62dee4ff88f5dcbbb9831e.png" data-original-src="https://miro.medium.com/v2/format:webp/1*E5YZBWgyv7bUMYJ4_vdvtw.png"/></div></figure><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="b5f5" class="ln lo it lj b gy lp lq l lr ls">summary(file_dim)</span><span id="bc3c" class="ln lo it lj b gy lt lq l lr ls">#&gt;      width         height        filename        <br/>#&gt;  Min.   :150   Min.   : 76.0   Length:14034      <br/>#&gt;  1st Qu.:150   1st Qu.:150.0   Class :character  <br/>#&gt;  Median :150   Median :150.0   Mode  :character  <br/>#&gt;  Mean   :150   Mean   :149.9                     <br/>#&gt;  3rd Qu.:150   3rd Qu.:150.0                     <br/>#&gt;  Max.   :150   Max.   :150.0</span></pre><p id="029e" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">正如我们所见，数据集包含不同维度的图像。所有的宽度都是 150 像素。但是，最大和最小高度分别为 150 和 76 像素。在适合模型之前，所有这些图像必须在相同的维度上。这一点至关重要，因为:</p><ol class=""><li id="b0d8" class="pi pj it lw b lx ly ma mb md pk mh pl ml pm mp pn po pp pq bi translated">每个图像像素值所适合的模型的输入层具有固定数量的神经元，</li><li id="1af0" class="pi pj it lw b lx pr ma ps md pt mh pu ml pv mp pn po pp pq bi translated">如果图像尺寸太高，训练模型可能会花费太长时间</li><li id="3b91" class="pi pj it lw b lx pr ma ps md pt mh pu ml pv mp pn po pp pq bi translated">如果图像尺寸太低，就会丢失太多信息。</li></ol><h1 id="e848" class="nq lo it bd nr ns nt nu nv nw nx ny nz ki oa kj ob kl oc km od ko oe kp of og bi translated">数据预处理</h1><p id="46d4" class="pw-post-body-paragraph lu lv it lw b lx oh kd lz ma oi kg mc md oj mf mg mh ok mj mk ml ol mn mo mp im bi translated">神经网络模型可能出现的一个问题是，它们倾向于<em class="pw">记忆<code class="fe om on oo lj b">seg_train</code>数据集中的</em>图像，以至于当新的<code class="fe om on oo lj b">seg_test</code>数据集进来时，它们无法识别它。数据扩充是解决这一问题的众多技术之一。给定一幅图像，数据增强将对其进行轻微转换，以创建一些新图像。然后将这些新图像放入模型中。这样，模型知道原始图像的许多版本，并且希望<em class="pw">理解</em>图像的意思，而不是<em class="pw">记住</em>它。我们将只使用一些简单的转换，例如:</p><ol class=""><li id="335b" class="pi pj it lw b lx ly ma mb md pk mh pl ml pm mp pn po pp pq bi translated">随机水平翻转图像</li><li id="dac4" class="pi pj it lw b lx pr ma ps md pt mh pu ml pv mp pn po pp pq bi translated">随机旋转 10 度</li><li id="8f21" class="pi pj it lw b lx pr ma ps md pt mh pu ml pv mp pn po pp pq bi translated">随机放大 0.1 倍</li><li id="6e80" class="pi pj it lw b lx pr ma ps md pt mh pu ml pv mp pn po pp pq bi translated">随机水平移动总宽度的 0.1 分之一</li><li id="1090" class="pi pj it lw b lx pr ma ps md pt mh pu ml pv mp pn po pp pq bi translated">随机水平移动总高度的 0.1 分之一</li></ol><p id="ee1f" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">我们不使用垂直翻转，因为在我们的情况下，他们可以改变图像的意义。这种数据扩充可以使用<code class="fe om on oo lj b">image_data_generator()</code>功能完成。将生成器保存到名为<code class="fe om on oo lj b">train_data_gen</code>的对象中。注意<code class="fe om on oo lj b">train_data_gen</code>仅在训练时应用，我们在预测时不使用它。</p><p id="58e7" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">在<code class="fe om on oo lj b">train_data_gen</code>中，我们还进行了归一化处理，以减少光照差异的影响。此外，CNN 模型在[0..1]数据比[0..255].为此，只需将每个像素值除以 255。</p><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="68a6" class="ln lo it lj b gy lp lq l lr ls">train_data_gen &lt;- image_data_generator(<br/>  rescale = 1/255,            # scaling pixel value<br/>  horizontal_flip = T,        # flip image horizontally<br/>  vertical_flip = F,          # flip image vertically <br/>  rotation_range = 10,        # rotate image from 0 to 45 degrees<br/>  zoom_range = 0.1,           # zoom in or zoom out range<br/>  width_shift_range = 0.1,    # shift horizontally to the width<br/>  height_shift_range = 0.1,   # shift horizontally to the height<br/>)</span></pre><p id="7005" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">我们将使用 150 × 150 像素作为输入图像的形状，因为 150 像素是所有图像中最常见的宽度和高度(再次查看 EDA)，并将大小存储为<code class="fe om on oo lj b">target_size</code>。此外，我们将分批训练模型，每批 32 个观测值，存储为<code class="fe om on oo lj b">batch_size</code>。</p><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="2d90" class="ln lo it lj b gy lp lq l lr ls">target_size &lt;- c(150, 150)<br/>batch_size &lt;- 32</span></pre><p id="6747" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">现在，构建生成器以从各自的目录中流动训练和验证数据集。填写参数<code class="fe om on oo lj b">target_size</code>和<code class="fe om on oo lj b">batch_size</code>。由于我们有彩色的 rgb 图像，将<code class="fe om on oo lj b">color_mode</code>设置为“RGB”。最后，使用<code class="fe om on oo lj b">train_data_gen</code>作为<code class="fe om on oo lj b">generator</code>来应用之前创建的数据扩充。</p><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="77b4" class="ln lo it lj b gy lp lq l lr ls"># for training dataset<br/>train_image_array_gen &lt;- flow_images_from_directory(<br/>  directory = "seg_train/seg_train/",   # folder of the data<br/>  target_size = target_size,   # target of the image dimension<br/>  color_mode = "rgb",          # use RGB color<br/>  batch_size = batch_size ,    # number of images in each batch<br/>  seed = RS,                   # set random seed<br/>  generator = train_data_gen   # apply data augmentation<br/>)<br/><br/># for validation dataset<br/>val_image_array_gen &lt;- flow_images_from_directory(<br/>  directory = "seg_test/seg_test/",<br/>  target_size = target_size, <br/>  color_mode = "rgb", <br/>  batch_size = batch_size ,<br/>  seed = RS,<br/>  generator = train_data_gen<br/>)</span></pre><p id="fc33" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">接下来，我们将看到目标变量中标签的比例，以检查类不平衡。如果有的话，分类器倾向于做出有偏差的学习模型，与多数类相比，该模型对少数类具有较差的预测准确性。我们可以通过对训练数据集进行上采样或下采样来以最简单的方式解决这个问题。</p><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="2094" class="ln lo it lj b gy lp lq l lr ls">output_n &lt;- n_distinct(train_image_array_gen$classes)<br/>table("Frequency" = factor(train_image_array_gen$classes)) %&gt;% <br/>  prop.table()</span><span id="e626" class="ln lo it lj b gy lt lq l lr ls">#&gt; Frequency<br/>#&gt;         0         1         2         3         4         5 <br/>#&gt; 0.1561208 0.1618213 0.1712983 0.1789939 0.1620351 0.1697307</span></pre><p id="bac2" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">幸运的是，从上面可以看出，所有的职业都相对平衡！我们不需要进一步的治疗。</p><h1 id="8635" class="nq lo it bd nr ns nt nu nv nw nx ny nz ki oa kj ob kl oc km od ko oe kp of og bi translated">建模</h1><p id="a54d" class="pw-post-body-paragraph lu lv it lw b lx oh kd lz ma oi kg mc md oj mf mg mh ok mj mk ml ol mn mo mp im bi translated">首先，让我们保存我们使用的训练和验证图像的数量。除了训练数据之外，我们还需要不同的数据来进行验证，因为我们不希望我们的模型只擅长预测它已经看到的图像，而且还可以推广到未看到的图像。这种对看不见的图像的需求正是我们也必须在验证数据集上看到模型性能的原因。</p><p id="0138" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">因此，我们可以在下面观察到，我们有 14034 个图像用于训练(如前所述)，3000 个图像用于验证模型。</p><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="0cac" class="ln lo it lj b gy lp lq l lr ls">train_samples &lt;- train_image_array_gen$n<br/>valid_samples &lt;- val_image_array_gen$n<br/>train_samples</span><span id="439c" class="ln lo it lj b gy lt lq l lr ls">#&gt; [1] 14034</span><span id="a274" class="ln lo it lj b gy lt lq l lr ls">valid_samples</span><span id="76a7" class="ln lo it lj b gy lt lq l lr ls">#&gt; [1] 3000</span></pre><p id="f549" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">我们将从最简单的开始逐步构建三个模型。</p><h2 id="a08c" class="ln lo it bd nr px py dn nv pz qa dp nz md qb qc ob mh qd qe od ml qf qg of iz bi translated">简单 CNN</h2><p id="3e9e" class="pw-post-body-paragraph lu lv it lw b lx oh kd lz ma oi kg mc md oj mf mg mh ok mj mk ml ol mn mo mp im bi translated">该模型只有 4 个隐藏层，包括最大池化和展平，以及 1 个输出层，详情如下:</p><ol class=""><li id="d5c2" class="pi pj it lw b lx ly ma mb md pk mh pl ml pm mp pn po pp pq bi translated">卷积层:滤波器 16，核大小 3 × 3，相同填充，relu 激活函数</li><li id="ecca" class="pi pj it lw b lx pr ma ps md pt mh pu ml pv mp pn po pp pq bi translated">最大池层:池大小 2 × 2</li><li id="eb44" class="pi pj it lw b lx pr ma ps md pt mh pu ml pv mp pn po pp pq bi translated">展平图层</li><li id="f32d" class="pi pj it lw b lx pr ma ps md pt mh pu ml pv mp pn po pp pq bi translated">密集层:16 节点，relu 激活功能</li><li id="739f" class="pi pj it lw b lx pr ma ps md pt mh pu ml pv mp pn po pp pq bi translated">密集层(输出):6 节点，softmax 激活功能</li></ol><p id="e3b9" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">请注意，我们使用扁平化层作为从网络的卷积部分到密集部分的桥梁。基本上，展平层所做的就是——顾名思义——将最后一个卷积层的维度展平为单个密集层。例如，假设我们有一个大小为(8，8，32)的卷积层。这里，32 是过滤器的数量。展平层将把这个张量整形为大小为 2048 矢量。</p><p id="fe42" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">在输出层，我们使用 softmax 激活函数，因为这是一个多类分类问题。最后，我们需要指定 CNN 输入层所需的图像大小。如前所述，我们将使用存储在<code class="fe om on oo lj b">target_size</code>中的 150 × 150 像素的图像尺寸和 3 个 RGB 通道。</p><p id="b664" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">现在，我们准备好了。</p><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="cbe4" class="ln lo it lj b gy lp lq l lr ls"># Set Initial Random Weight<br/>tensorflow::tf$random$set_seed(RS)<br/><br/>model &lt;- keras_model_sequential(name = "simple_model") %&gt;% <br/>  <br/>  # Convolution Layer<br/>  layer_conv_2d(filters = 16,<br/>                kernel_size = c(3,3),<br/>                padding = "same",<br/>                activation = "relu",<br/>                input_shape = c(target_size, 3) <br/>                ) %&gt;% <br/><br/>  # Max Pooling Layer<br/>  layer_max_pooling_2d(pool_size = c(2,2)) %&gt;% <br/>  <br/>  # Flattening Layer<br/>  layer_flatten() %&gt;% <br/>  <br/>  # Dense Layer<br/>  layer_dense(units = 16,<br/>              activation = "relu") %&gt;% <br/>  <br/>  # Output Layer<br/>  layer_dense(units = output_n,<br/>              activation = "softmax",<br/>              name = "Output")<br/>  <br/>summary(model)</span><span id="a6d4" class="ln lo it lj b gy lt lq l lr ls">#&gt; Model: "simple_model"<br/>#&gt; _________________________________________________________________<br/>#&gt; Layer (type)                                                  Output Shape                                           Param #              <br/>#&gt; =================================================================<br/>#&gt; conv2d (Conv2D)                                               (None, 150, 150, 16)                                   448                  <br/>#&gt; _________________________________________________________________<br/>#&gt; max_pooling2d (MaxPooling2D)                                  (None, 75, 75, 16)                                     0                    <br/>#&gt; _________________________________________________________________<br/>#&gt; flatten (Flatten)                                             (None, 90000)                                          0                    <br/>#&gt; _________________________________________________________________<br/>#&gt; dense (Dense)                                                 (None, 16)                                             1440016              <br/>#&gt; _________________________________________________________________<br/>#&gt; Output (Dense)                                                (None, 6)                                              102                  <br/>#&gt; =================================================================<br/>#&gt; Total params: 1,440,566<br/>#&gt; Trainable params: 1,440,566<br/>#&gt; Non-trainable params: 0<br/>#&gt; _________________________________________________________________</span></pre><p id="923c" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">建立后，我们编译和训练模型。我们对损失函数使用分类交叉熵，因为这又是一个多类分类问题。我们使用默认学习率为 0.001 的 adam 优化器，因为 adam 是最有效的优化器之一。我们还使用准确性作为简单性的衡量标准。更重要的是，因为我们不喜欢一个类高于其他类，并且每个类都是平衡的，所以与精确度、灵敏度或特异性相比，准确性更受青睐。我们将训练 10 个时期的模型。</p><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="5844" class="ln lo it lj b gy lp lq l lr ls">model %&gt;% <br/>  compile(<br/>    loss = "categorical_crossentropy",<br/>    optimizer = optimizer_adam(lr = 0.001),<br/>    metrics = "accuracy"<br/>  )<br/><br/># fit data into model<br/>history &lt;- model %&gt;% <br/>  fit_generator(<br/>    # training data<br/>    train_image_array_gen,<br/>  <br/>    # training epochs<br/>    steps_per_epoch = as.integer(train_samples / batch_size), <br/>    epochs = 10, <br/>    <br/>    # validation data<br/>    validation_data = val_image_array_gen,<br/>    validation_steps = as.integer(valid_samples / batch_size)<br/>  )</span><span id="bc87" class="ln lo it lj b gy lt lq l lr ls">plot(history)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="ab gu cl ph"><img src="../Images/313c63eb8e87b91f990e704991cb1afd.png" data-original-src="https://miro.medium.com/v2/format:webp/1*dZdgcq95GTpgA-vVIlYXJQ.png"/></div></figure><p id="9916" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">从第十个时期的最终训练和验证精度，我们可以看到它们具有相似的值并且相对较高，这意味着没有发生过拟合。接下来，我们将对验证数据集上的所有图像执行预测(而不是像在训练中那样对每批图像执行预测)。首先，让我们将每个图像的路径和它对应的类列表。</p><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="5cfe" class="ln lo it lj b gy lp lq l lr ls">val_data &lt;- data.frame(file_name = paste0("seg_test/seg_test/", val_image_array_gen$filenames)) %&gt;% <br/>  mutate(class = str_extract(file_name, "buildings|forest|glacier|mountain|sea|street"))<br/><br/>head(val_data)</span><span id="788e" class="ln lo it lj b gy lt lq l lr ls">#&gt;                                file_name     class<br/>#&gt; 1 seg_test/seg_test/buildings\\20057.jpg buildings<br/>#&gt; 2 seg_test/seg_test/buildings\\20060.jpg buildings<br/>#&gt; 3 seg_test/seg_test/buildings\\20061.jpg buildings<br/>#&gt; 4 seg_test/seg_test/buildings\\20064.jpg buildings<br/>#&gt; 5 seg_test/seg_test/buildings\\20073.jpg buildings<br/>#&gt; 6 seg_test/seg_test/buildings\\20074.jpg buildings</span></pre><p id="d2ac" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">然后，我们将每个图像转换成一个数组。不要忘记将像素值归一化，即除以 255。</p><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="153c" class="ln lo it lj b gy lp lq l lr ls">image_prep &lt;- function(x, target_size) {<br/>  arrays &lt;- lapply(x, function(path) {<br/>    img &lt;- image_load(<br/>      path, <br/>      target_size = target_size, <br/>      grayscale = F<br/>    )<br/>    x &lt;- image_to_array(img)<br/>    x &lt;- array_reshape(x, c(1, dim(x)))<br/>    x &lt;- x/255<br/>  })<br/>  do.call(abind::abind, c(arrays, list(along = 1)))<br/>}<br/><br/>test_x &lt;- image_prep(val_data$file_name, target_size)<br/>dim(test_x)</span><span id="1bbd" class="ln lo it lj b gy lt lq l lr ls">#&gt; [1] 3000  150  150    3</span></pre><p id="fd28" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">接下来，预测。</p><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="98a8" class="ln lo it lj b gy lp lq l lr ls">pred_test &lt;- predict_classes(model, test_x) <br/>head(pred_test)</span><span id="4407" class="ln lo it lj b gy lt lq l lr ls">#&gt; [1] 4 0 0 0 4 3</span></pre><p id="2a57" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">现在，将每个预测解码到相应的类别中。</p><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="b66a" class="ln lo it lj b gy lp lq l lr ls">decode &lt;- function(x){<br/>  case_when(<br/>    x == 0 ~ "buildings",<br/>    x == 1 ~ "forest",<br/>    x == 2 ~ "glacier",<br/>    x == 3 ~ "mountain",<br/>    x == 4 ~ "sea",<br/>    x == 5 ~ "street",<br/>  )<br/>}<br/><br/>pred_test &lt;- sapply(pred_test, decode)<br/>head(pred_test)</span><span id="493f" class="ln lo it lj b gy lt lq l lr ls">#&gt; [1] "sea"       "buildings" "buildings" "buildings" "sea"       "mountain"</span></pre><p id="b570" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">最后，分析混淆矩阵。</p><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="e21d" class="ln lo it lj b gy lp lq l lr ls">cm_simple &lt;- confusionMatrix(as.factor(pred_test), as.factor(val_data$class))<br/>acc_simple &lt;- cm_simple$overall['Accuracy']<br/>cm_simple</span><span id="c980" class="ln lo it lj b gy lt lq l lr ls">#&gt; Confusion Matrix and Statistics<br/>#&gt; <br/>#&gt;            Reference<br/>#&gt; Prediction  buildings forest glacier mountain sea street<br/>#&gt;   buildings       348     24      14       20  35    106<br/>#&gt;   forest            8    418       3        4   4     19<br/>#&gt;   glacier           7      5     357       53  38      5<br/>#&gt;   mountain         19      6      98      381  61      5<br/>#&gt;   sea              13      1      75       65 363      6<br/>#&gt;   street           42     20       6        2   9    360<br/>#&gt; <br/>#&gt; Overall Statistics<br/>#&gt;                                                <br/>#&gt;                Accuracy : 0.7423               <br/>#&gt;                  95% CI : (0.7263, 0.7579)     <br/>#&gt;     No Information Rate : 0.1843               <br/>#&gt;     P-Value [Acc &gt; NIR] : &lt; 0.00000000000000022<br/>#&gt;                                                <br/>#&gt;                   Kappa : 0.6909               <br/>#&gt;                                                <br/>#&gt;  Mcnemar's Test P-Value : 0.0000000001327      <br/>#&gt; <br/>#&gt; Statistics by Class:<br/>#&gt; <br/>#&gt;                      Class: buildings Class: forest Class: glacier Class: mountain Class: sea Class: street<br/>#&gt; Sensitivity                    0.7963        0.8819         0.6456          0.7257     0.7118        0.7186<br/>#&gt; Specificity                    0.9224        0.9850         0.9559          0.9236     0.9357        0.9684<br/>#&gt; Pos Pred Value                 0.6362        0.9167         0.7677          0.6684     0.6941        0.8200<br/>#&gt; Neg Pred Value                 0.9637        0.9780         0.9227          0.9407     0.9407        0.9449<br/>#&gt; Prevalence                     0.1457        0.1580         0.1843          0.1750     0.1700        0.1670<br/>#&gt; Detection Rate                 0.1160        0.1393         0.1190          0.1270     0.1210        0.1200<br/>#&gt; Detection Prevalence           0.1823        0.1520         0.1550          0.1900     0.1743        0.1463<br/>#&gt; Balanced Accuracy              0.8593        0.9334         0.8007          0.8247     0.8238        0.8435</span></pre><p id="d7df" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">从混淆矩阵中可以看出，该模型很难区分每个类别。我们在验证数据集上获得了 74%的准确率。有 106 个街道图像被预测为建筑物，这超过了所有街道图像的 20%。这是有意义的，因为在许多街道图像中，建筑物也存在。</p><p id="78f7" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">我们可以通过各种方式来提高模型的性能。但是现在，让我们通过简单地改变架构来改进它。</p><h2 id="161c" class="ln lo it bd nr px py dn nv pz qa dp nz md qb qc ob mh qd qe od ml qf qg of iz bi translated">深度 CNN</h2><p id="6348" class="pw-post-body-paragraph lu lv it lw b lx oh kd lz ma oi kg mc md oj mf mg mh ok mj mk ml ol mn mo mp im bi translated">现在我们用更多的卷积层制作一个更深的 CNN。这是它的架构:</p><ol class=""><li id="5112" class="pi pj it lw b lx ly ma mb md pk mh pl ml pm mp pn po pp pq bi translated">块 1: 2 个卷积层和 1 个最大汇集层</li><li id="f7a8" class="pi pj it lw b lx pr ma ps md pt mh pu ml pv mp pn po pp pq bi translated">块 2: 1 个卷积层和 1 个最大池层</li><li id="5061" class="pi pj it lw b lx pr ma ps md pt mh pu ml pv mp pn po pp pq bi translated">块 3: 1 个卷积层和 1 个最大池层</li><li id="ab1a" class="pi pj it lw b lx pr ma ps md pt mh pu ml pv mp pn po pp pq bi translated">块 4: 1 个卷积层和 1 个最大池层</li><li id="2bd0" class="pi pj it lw b lx pr ma ps md pt mh pu ml pv mp pn po pp pq bi translated">展平图层</li><li id="e0c5" class="pi pj it lw b lx pr ma ps md pt mh pu ml pv mp pn po pp pq bi translated">一个致密层</li><li id="b661" class="pi pj it lw b lx pr ma ps md pt mh pu ml pv mp pn po pp pq bi translated">输出层</li></ol><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="d634" class="ln lo it lj b gy lp lq l lr ls">tensorflow::tf$random$set_seed(RS)<br/><br/>model_big &lt;- keras_model_sequential(name = "model_big") %&gt;%<br/>  <br/>  # First convolutional layer<br/>  layer_conv_2d(filters = 32,<br/>                kernel_size = c(5,5), # 5 x 5 filters<br/>                padding = "same",<br/>                activation = "relu",<br/>                input_shape = c(target_size, 3)<br/>                ) %&gt;% <br/>  <br/>  # Second convolutional layer<br/>  layer_conv_2d(filters = 32,<br/>                kernel_size = c(3,3), # 3 x 3 filters<br/>                padding = "same",<br/>                activation = "relu"<br/>                ) %&gt;% <br/>  <br/>  # Max pooling layer<br/>  layer_max_pooling_2d(pool_size = c(2,2)) %&gt;% <br/>  <br/>  # Third convolutional layer<br/>  layer_conv_2d(filters = 64,<br/>                kernel_size = c(3,3),<br/>                padding = "same",<br/>                activation = "relu"<br/>                ) %&gt;% <br/><br/>  # Max pooling layer<br/>  layer_max_pooling_2d(pool_size = c(2,2)) %&gt;% <br/>  <br/>  # Fourth convolutional layer<br/>  layer_conv_2d(filters = 128,<br/>                kernel_size = c(3,3),<br/>                padding = "same",<br/>                activation = "relu"<br/>                ) %&gt;% <br/>  <br/>  # Max pooling layer<br/>  layer_max_pooling_2d(pool_size = c(2,2)) %&gt;% <br/><br/>  # Fifth convolutional layer<br/>  layer_conv_2d(filters = 256,<br/>                kernel_size = c(3,3),<br/>                padding = "same",<br/>                activation = "relu"<br/>                ) %&gt;% <br/>  <br/>  # Max pooling layer<br/>  layer_max_pooling_2d(pool_size = c(2,2)) %&gt;% <br/>  <br/>  # Flattening layer<br/>  layer_flatten() %&gt;% <br/>  <br/>  # Dense layer<br/>  layer_dense(units = 64,<br/>              activation = "relu") %&gt;% <br/>  <br/>  # Output layer<br/>  layer_dense(name = "Output",<br/>              units = output_n, <br/>              activation = "softmax")<br/><br/>summary(model_big)</span><span id="8e41" class="ln lo it lj b gy lt lq l lr ls">#&gt; Model: "model_big"<br/>#&gt; _________________________________________________________________<br/>#&gt; Layer (type)                                                  Output Shape                                           Param #              <br/>#&gt; =================================================================<br/>#&gt; conv2d_5 (Conv2D)                                             (None, 150, 150, 32)                                   2432                 <br/>#&gt; _________________________________________________________________<br/>#&gt; conv2d_4 (Conv2D)                                             (None, 150, 150, 32)                                   9248                 <br/>#&gt; _________________________________________________________________<br/>#&gt; max_pooling2d_4 (MaxPooling2D)                                (None, 75, 75, 32)                                     0                    <br/>#&gt; _________________________________________________________________<br/>#&gt; conv2d_3 (Conv2D)                                             (None, 75, 75, 64)                                     18496                <br/>#&gt; _________________________________________________________________<br/>#&gt; max_pooling2d_3 (MaxPooling2D)                                (None, 37, 37, 64)                                     0                    <br/>#&gt; _________________________________________________________________<br/>#&gt; conv2d_2 (Conv2D)                                             (None, 37, 37, 128)                                    73856                <br/>#&gt; _________________________________________________________________<br/>#&gt; max_pooling2d_2 (MaxPooling2D)                                (None, 18, 18, 128)                                    0                    <br/>#&gt; _________________________________________________________________<br/>#&gt; conv2d_1 (Conv2D)                                             (None, 18, 18, 256)                                    295168               <br/>#&gt; _________________________________________________________________<br/>#&gt; max_pooling2d_1 (MaxPooling2D)                                (None, 9, 9, 256)                                      0                    <br/>#&gt; _________________________________________________________________<br/>#&gt; flatten_1 (Flatten)                                           (None, 20736)                                          0                    <br/>#&gt; _________________________________________________________________<br/>#&gt; dense_1 (Dense)                                               (None, 64)                                             1327168              <br/>#&gt; _________________________________________________________________<br/>#&gt; Output (Dense)                                                (None, 6)                                              390                  <br/>#&gt; =================================================================<br/>#&gt; Total params: 1,726,758<br/>#&gt; Trainable params: 1,726,758<br/>#&gt; Non-trainable params: 0<br/>#&gt; _________________________________________________________________</span></pre><p id="1c16" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">剩下的和之前做的一样。</p><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="e22f" class="ln lo it lj b gy lp lq l lr ls">model_big %&gt;%<br/>  compile(<br/>    loss = "categorical_crossentropy",<br/>    optimizer = optimizer_adam(lr = 0.001),<br/>    metrics = "accuracy"<br/>  )<br/><br/>history &lt;- model_big %&gt;%<br/>  fit_generator(<br/>    train_image_array_gen,<br/>    steps_per_epoch = as.integer(train_samples / batch_size),<br/>    epochs = 10,<br/>    validation_data = val_image_array_gen,<br/>    validation_steps = as.integer(valid_samples / batch_size)<br/>  )</span><span id="a3ec" class="ln lo it lj b gy lt lq l lr ls">plot(history)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="ab gu cl ph"><img src="../Images/a2b4b1fdc12ea0b998a0d792941e4e1b.png" data-original-src="https://miro.medium.com/v2/format:webp/1*mpEMreRcjftvN8-EzxH3PA.png"/></div></figure><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="d02c" class="ln lo it lj b gy lp lq l lr ls">pred_test &lt;- predict_classes(model_big, test_x)<br/>pred_test &lt;- sapply(pred_test, decode)<br/>cm_big &lt;- confusionMatrix(as.factor(pred_test), as.factor(val_data$class))<br/>acc_big &lt;- cm_big$overall['Accuracy']<br/>cm_big</span><span id="e0ee" class="ln lo it lj b gy lt lq l lr ls">#&gt; Confusion Matrix and Statistics<br/>#&gt; <br/>#&gt;            Reference<br/>#&gt; Prediction  buildings forest glacier mountain sea street<br/>#&gt;   buildings       390      3      24       24  11     34<br/>#&gt;   forest            3    465      11        7   8     11<br/>#&gt;   glacier           2      0     367       35   9      1<br/>#&gt;   mountain          0      2      82      415  17      1<br/>#&gt;   sea               3      1      57       42 461      6<br/>#&gt;   street           39      3      12        2   4    448<br/>#&gt; <br/>#&gt; Overall Statistics<br/>#&gt;                                                <br/>#&gt;                Accuracy : 0.8487               <br/>#&gt;                  95% CI : (0.8353, 0.8613)     <br/>#&gt;     No Information Rate : 0.1843               <br/>#&gt;     P-Value [Acc &gt; NIR] : &lt; 0.00000000000000022<br/>#&gt;                                                <br/>#&gt;                   Kappa : 0.8185               <br/>#&gt;                                                <br/>#&gt;  Mcnemar's Test P-Value : &lt; 0.00000000000000022<br/>#&gt; <br/>#&gt; Statistics by Class:<br/>#&gt; <br/>#&gt;                      Class: buildings Class: forest Class: glacier Class: mountain Class: sea Class: street<br/>#&gt; Sensitivity                    0.8924        0.9810         0.6637          0.7905     0.9039        0.8942<br/>#&gt; Specificity                    0.9625        0.9842         0.9808          0.9588     0.9562        0.9760<br/>#&gt; Pos Pred Value                 0.8025        0.9208         0.8865          0.8027     0.8088        0.8819<br/>#&gt; Neg Pred Value                 0.9813        0.9964         0.9281          0.9557     0.9798        0.9787<br/>#&gt; Prevalence                     0.1457        0.1580         0.1843          0.1750     0.1700        0.1670<br/>#&gt; Detection Rate                 0.1300        0.1550         0.1223          0.1383     0.1537        0.1493<br/>#&gt; Detection Prevalence           0.1620        0.1683         0.1380          0.1723     0.1900        0.1693<br/>#&gt; Balanced Accuracy              0.9275        0.9826         0.8222          0.8746     0.9301        0.9351</span></pre><p id="2a23" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">这个结果总体上比之前的<code class="fe om on oo lj b">model</code>要好，因为<code class="fe om on oo lj b">model_big</code>更复杂，因此能够捕捉更多的特征。我们在验证数据集上获得了 85%的准确率。虽然对街道图像的预测已经变得更好，但对冰川图像的预测仍然存在。</p><h2 id="0f40" class="ln lo it bd nr px py dn nv pz qa dp nz md qb qc ob mh qd qe od ml qf qg of iz bi translated">预训练权重的深度 CNN</h2><p id="6a60" class="pw-post-body-paragraph lu lv it lw b lx oh kd lz ma oi kg mc md oj mf mg mh ok mj mk ml ol mn mo mp im bi translated">实际上，研究人员已经为图像分类问题开发了许多模型，从 VGG 模型家族到谷歌开发的最新艺术级 EfficientNet。出于学习目的，在本节中，我们将使用 VGG16 模型，因为它是所有模型中最简单的模型之一，仅由卷积层、最大池层和密集层组成，如我们之前介绍的那样。这个过程叫做迁移学习，将预先训练好的模型的知识进行迁移，来解决我们的问题。</p><p id="fbaa" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">最初的 VGG16 模型是在 1000 个类上训练的。为了使它适合我们的问题，我们将排除模型的顶部(密集)层，并插入我们版本的预测层，该预测层由一个全局平均池层(作为扁平化层的替代)、一个具有 64 个节点的密集层和一个具有 6 个节点的输出层(用于 6 个类)组成。</p><p id="87ea" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">我们来看看整体架构。</p><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="2eea" class="ln lo it lj b gy lp lq l lr ls"># load original model without top layers<br/>input_tensor &lt;- layer_input(shape = c(target_size, 3))<br/>base_model &lt;- application_vgg16(input_tensor = input_tensor, <br/>                                weights = 'imagenet', <br/>                                include_top = FALSE)<br/><br/># add our custom layers<br/>predictions &lt;- base_model$output %&gt;%<br/>  layer_global_average_pooling_2d() %&gt;%<br/>  layer_dense(units = 64, activation = 'relu') %&gt;%<br/>  layer_dense(units = output_n, activation = 'softmax')<br/><br/># this is the model we will train<br/>vgg16 &lt;- keras_model(inputs = base_model$input, outputs = predictions)<br/>summary(vgg16)</span><span id="24d8" class="ln lo it lj b gy lt lq l lr ls">#&gt; Model: "model"<br/>#&gt; _________________________________________________________________<br/>#&gt; Layer (type)                                                  Output Shape                                           Param #              <br/>#&gt; =================================================================<br/>#&gt; input_1 (InputLayer)                                          [(None, 150, 150, 3)]                                  0                    <br/>#&gt; _________________________________________________________________<br/>#&gt; block1_conv1 (Conv2D)                                         (None, 150, 150, 64)                                   1792                 <br/>#&gt; _________________________________________________________________<br/>#&gt; block1_conv2 (Conv2D)                                         (None, 150, 150, 64)                                   36928                <br/>#&gt; _________________________________________________________________<br/>#&gt; block1_pool (MaxPooling2D)                                    (None, 75, 75, 64)                                     0                    <br/>#&gt; _________________________________________________________________<br/>#&gt; block2_conv1 (Conv2D)                                         (None, 75, 75, 128)                                    73856                <br/>#&gt; _________________________________________________________________<br/>#&gt; block2_conv2 (Conv2D)                                         (None, 75, 75, 128)                                    147584               <br/>#&gt; _________________________________________________________________<br/>#&gt; block2_pool (MaxPooling2D)                                    (None, 37, 37, 128)                                    0                    <br/>#&gt; _________________________________________________________________<br/>#&gt; block3_conv1 (Conv2D)                                         (None, 37, 37, 256)                                    295168               <br/>#&gt; _________________________________________________________________<br/>#&gt; block3_conv2 (Conv2D)                                         (None, 37, 37, 256)                                    590080               <br/>#&gt; _________________________________________________________________<br/>#&gt; block3_conv3 (Conv2D)                                         (None, 37, 37, 256)                                    590080               <br/>#&gt; _________________________________________________________________<br/>#&gt; block3_pool (MaxPooling2D)                                    (None, 18, 18, 256)                                    0                    <br/>#&gt; _________________________________________________________________<br/>#&gt; block4_conv1 (Conv2D)                                         (None, 18, 18, 512)                                    1180160              <br/>#&gt; _________________________________________________________________<br/>#&gt; block4_conv2 (Conv2D)                                         (None, 18, 18, 512)                                    2359808              <br/>#&gt; _________________________________________________________________<br/>#&gt; block4_conv3 (Conv2D)                                         (None, 18, 18, 512)                                    2359808              <br/>#&gt; _________________________________________________________________<br/>#&gt; block4_pool (MaxPooling2D)                                    (None, 9, 9, 512)                                      0                    <br/>#&gt; _________________________________________________________________<br/>#&gt; block5_conv1 (Conv2D)                                         (None, 9, 9, 512)                                      2359808              <br/>#&gt; _________________________________________________________________<br/>#&gt; block5_conv2 (Conv2D)                                         (None, 9, 9, 512)                                      2359808              <br/>#&gt; _________________________________________________________________<br/>#&gt; block5_conv3 (Conv2D)                                         (None, 9, 9, 512)                                      2359808              <br/>#&gt; _________________________________________________________________<br/>#&gt; block5_pool (MaxPooling2D)                                    (None, 4, 4, 512)                                      0                    <br/>#&gt; _________________________________________________________________<br/>#&gt; global_average_pooling2d (GlobalAveragePooling2D)             (None, 512)                                            0                    <br/>#&gt; _________________________________________________________________<br/>#&gt; dense_3 (Dense)                                               (None, 64)                                             32832                <br/>#&gt; _________________________________________________________________<br/>#&gt; dense_2 (Dense)                                               (None, 6)                                              390                  <br/>#&gt; =================================================================<br/>#&gt; Total params: 14,747,910<br/>#&gt; Trainable params: 14,747,910<br/>#&gt; Non-trainable params: 0<br/>#&gt; _________________________________________________________________</span></pre><p id="c346" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">哇，这么多层次啊！我们可以直接使用<code class="fe om on oo lj b">vgg16</code>进行训练和预测，但同样，出于学习的目的，让我们自己从头开始制作 VGG16 模型。</p><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="679f" class="ln lo it lj b gy lp lq l lr ls">model_bigger &lt;- keras_model_sequential(name = "model_bigger") %&gt;% <br/>  <br/>  # Block 1<br/>  layer_conv_2d(filters = 64, <br/>                kernel_size = c(3, 3), <br/>                activation='relu', <br/>                padding='same', <br/>                input_shape = c(94, 94, 3),<br/>                name='block1_conv1') %&gt;% <br/>  <br/>  layer_conv_2d(filters = 64, <br/>                kernel_size = c(3, 3), <br/>                activation='relu', <br/>                padding='same', <br/>                name='block1_conv2') %&gt;% <br/>    <br/>  layer_max_pooling_2d(pool_size = c(2, 2), <br/>                       strides=c(2, 2), <br/>                       name='block1_pool') %&gt;% <br/>  <br/>  # Block 2<br/>  layer_conv_2d(filters = 128, <br/>                kernel_size = c(3, 3), <br/>                activation='relu', <br/>                padding='same', <br/>                name='block2_conv1') %&gt;% <br/>    <br/>  layer_conv_2d(filters = 128, <br/>                kernel_size = c(3, 3), <br/>                activation='relu', <br/>                padding='same', <br/>                name='block2_conv2') %&gt;% <br/>    <br/>  layer_max_pooling_2d(pool_size = c(2, 2), <br/>                       strides=c(2, 2), <br/>                       name='block2_pool') %&gt;% <br/>  <br/>  # Block 3<br/>  layer_conv_2d(filters = 256, <br/>                kernel_size = c(3, 3), <br/>                activation='relu', <br/>                padding='same', <br/>                name='block3_conv1') %&gt;% <br/>  <br/>  layer_conv_2d(filters = 256, <br/>                kernel_size = c(3, 3), <br/>                activation='relu', <br/>                padding='same', <br/>                name='block3_conv2') %&gt;% <br/>    <br/>  layer_conv_2d(filters = 256, <br/>                kernel_size = c(3, 3), <br/>                activation='relu', <br/>                padding='same', <br/>                name='block3_conv3') %&gt;% <br/>    <br/>  layer_max_pooling_2d(pool_size = c(2, 2), <br/>                       strides=c(2, 2), <br/>                       name='block3_pool') %&gt;% <br/>  <br/>  # Block 4<br/>  layer_conv_2d(filters = 512, <br/>                kernel_size = c(3, 3), <br/>                activation='relu', <br/>                padding='same', <br/>                name='block4_conv1') %&gt;% <br/>  <br/>  layer_conv_2d(filters = 512, <br/>                kernel_size = c(3, 3), <br/>                activation='relu', <br/>                padding='same', <br/>                name='block4_conv2') %&gt;% <br/>    <br/>  layer_conv_2d(filters = 512, <br/>                kernel_size = c(3, 3), <br/>                activation='relu', <br/>                padding='same', <br/>                name='block4_conv3') %&gt;% <br/>  <br/>  layer_max_pooling_2d(pool_size = c(2, 2), <br/>                       strides=c(2, 2), <br/>                       name='block4_pool') %&gt;% <br/>  <br/>  # Block 5<br/>  layer_conv_2d(filters = 512, <br/>                kernel_size = c(3, 3), <br/>                activation='relu', <br/>                padding='same', <br/>                name='block5_conv1') %&gt;% <br/>    <br/>  layer_conv_2d(filters = 512, <br/>                kernel_size = c(3, 3), <br/>                activation='relu', <br/>                padding='same', <br/>                name='block5_conv2') %&gt;% <br/>    <br/>  layer_conv_2d(filters = 512, <br/>                kernel_size = c(3, 3), <br/>                activation='relu', <br/>                padding='same', <br/>                name='block5_conv3') %&gt;% <br/>    <br/>  layer_max_pooling_2d(pool_size = c(2, 2), <br/>                       strides=c(2, 2), <br/>                       name='block5_pool') %&gt;% <br/>  <br/>  # Dense<br/>  layer_global_average_pooling_2d() %&gt;%<br/>  layer_dense(units = 64, activation = 'relu') %&gt;%<br/>  layer_dense(units = output_n, activation = 'softmax')<br/><br/>model_bigger</span><span id="bf02" class="ln lo it lj b gy lt lq l lr ls">#&gt; Model<br/>#&gt; Model: "model_bigger"<br/>#&gt; _________________________________________________________________<br/>#&gt; Layer (type)                                                  Output Shape                                           Param #              <br/>#&gt; =================================================================<br/>#&gt; block1_conv1 (Conv2D)                                         (None, 94, 94, 64)                                     1792                 <br/>#&gt; _________________________________________________________________<br/>#&gt; block1_conv2 (Conv2D)                                         (None, 94, 94, 64)                                     36928                <br/>#&gt; _________________________________________________________________<br/>#&gt; block1_pool (MaxPooling2D)                                    (None, 47, 47, 64)                                     0                    <br/>#&gt; _________________________________________________________________<br/>#&gt; block2_conv1 (Conv2D)                                         (None, 47, 47, 128)                                    73856                <br/>#&gt; _________________________________________________________________<br/>#&gt; block2_conv2 (Conv2D)                                         (None, 47, 47, 128)                                    147584               <br/>#&gt; _________________________________________________________________<br/>#&gt; block2_pool (MaxPooling2D)                                    (None, 23, 23, 128)                                    0                    <br/>#&gt; _________________________________________________________________<br/>#&gt; block3_conv1 (Conv2D)                                         (None, 23, 23, 256)                                    295168               <br/>#&gt; _________________________________________________________________<br/>#&gt; block3_conv2 (Conv2D)                                         (None, 23, 23, 256)                                    590080               <br/>#&gt; _________________________________________________________________<br/>#&gt; block3_conv3 (Conv2D)                                         (None, 23, 23, 256)                                    590080               <br/>#&gt; _________________________________________________________________<br/>#&gt; block3_pool (MaxPooling2D)                                    (None, 11, 11, 256)                                    0                    <br/>#&gt; _________________________________________________________________<br/>#&gt; block4_conv1 (Conv2D)                                         (None, 11, 11, 512)                                    1180160              <br/>#&gt; _________________________________________________________________<br/>#&gt; block4_conv2 (Conv2D)                                         (None, 11, 11, 512)                                    2359808              <br/>#&gt; _________________________________________________________________<br/>#&gt; block4_conv3 (Conv2D)                                         (None, 11, 11, 512)                                    2359808              <br/>#&gt; _________________________________________________________________<br/>#&gt; block4_pool (MaxPooling2D)                                    (None, 5, 5, 512)                                      0                    <br/>#&gt; _________________________________________________________________<br/>#&gt; block5_conv1 (Conv2D)                                         (None, 5, 5, 512)                                      2359808              <br/>#&gt; _________________________________________________________________<br/>#&gt; block5_conv2 (Conv2D)                                         (None, 5, 5, 512)                                      2359808              <br/>#&gt; _________________________________________________________________<br/>#&gt; block5_conv3 (Conv2D)                                         (None, 5, 5, 512)                                      2359808              <br/>#&gt; _________________________________________________________________<br/>#&gt; block5_pool (MaxPooling2D)                                    (None, 2, 2, 512)                                      0                    <br/>#&gt; _________________________________________________________________<br/>#&gt; global_average_pooling2d_1 (GlobalAveragePooling2D)           (None, 512)                                            0                    <br/>#&gt; _________________________________________________________________<br/>#&gt; dense_5 (Dense)                                               (None, 64)                                             32832                <br/>#&gt; _________________________________________________________________<br/>#&gt; dense_4 (Dense)                                               (None, 6)                                              390                  <br/>#&gt; =================================================================<br/>#&gt; Total params: 14,747,910<br/>#&gt; Trainable params: 14,747,910<br/>#&gt; Non-trainable params: 0<br/>#&gt; _________________________________________________________________</span></pre><p id="a6f2" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">注意<code class="fe om on oo lj b">model_bigger</code>与<code class="fe om on oo lj b">vgg16</code>每层的参数数量完全相同。迁移学习的优势在于，我们不必从随机权重开始训练模型，而是从原始模型的预训练权重开始。这些预先训练的权重已经针对图像分类问题进行了优化，我们只需要对它们进行微调以符合我们的目的。因此有了这个比喻:</p><blockquote class="nb"><p id="278a" class="nc nd it bd ne nf ng nh ni nj nk mp dk translated">我们正站在巨人的肩膀上。</p></blockquote><p id="1169" class="pw-post-body-paragraph lu lv it lw b lx nl kd lz ma nm kg mc md nn mf mg mh no mj mk ml np mn mo mp im bi translated">话虽如此，我们还是把<code class="fe om on oo lj b">vgg16</code>的权重全部赋给<code class="fe om on oo lj b">model_bigger</code>吧。</p><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="68a0" class="ln lo it lj b gy lp lq l lr ls">set_weights(model_bigger, get_weights(vgg16))</span></pre><p id="df0c" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">这里是我们的<code class="fe om on oo lj b">model_bigger</code>层的总结:</p><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="0724" class="ln lo it lj b gy lp lq l lr ls">layers &lt;- model_bigger$layers<br/>for (i in 1:length(layers))<br/>  cat(i, layers[[i]]$name, "\n")</span><span id="3885" class="ln lo it lj b gy lt lq l lr ls">#&gt; 1 block1_conv1 <br/>#&gt; 2 block1_conv2 <br/>#&gt; 3 block1_pool <br/>#&gt; 4 block2_conv1 <br/>#&gt; 5 block2_conv2 <br/>#&gt; 6 block2_pool <br/>#&gt; 7 block3_conv1 <br/>#&gt; 8 block3_conv2 <br/>#&gt; 9 block3_conv3 <br/>#&gt; 10 block3_pool <br/>#&gt; 11 block4_conv1 <br/>#&gt; 12 block4_conv2 <br/>#&gt; 13 block4_conv3 <br/>#&gt; 14 block4_pool <br/>#&gt; 15 block5_conv1 <br/>#&gt; 16 block5_conv2 <br/>#&gt; 17 block5_conv3 <br/>#&gt; 18 block5_pool <br/>#&gt; 19 global_average_pooling2d_1 <br/>#&gt; 20 dense_5 <br/>#&gt; 21 dense_4</span></pre><p id="e230" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">请注意，第 19-21 层仍然具有随机权重，因为它们是由我们创建的，而不是来自原始模型。因此，我们需要先冻结所有层，然后只训练这些层。</p><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="2d9e" class="ln lo it lj b gy lp lq l lr ls">freeze_weights(model_bigger, from = 1, to = 18)</span></pre><p id="21fb" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">为了训练这些预测层，我们将简单地使用先前的设置。</p><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="1c19" class="ln lo it lj b gy lp lq l lr ls"># compile the model (should be done *after* setting base layers to non-trainable)<br/>model_bigger %&gt;% compile(loss = "categorical_crossentropy",<br/>                         optimizer = optimizer_adam(lr = 0.001),<br/>                         metrics = "accuracy")<br/><br/>history &lt;- model_bigger %&gt;%<br/>  fit_generator(<br/>  train_image_array_gen,<br/>  steps_per_epoch = as.integer(train_samples / batch_size),<br/>  epochs = 10,<br/>  validation_data = val_image_array_gen,<br/>  validation_steps = as.integer(valid_samples / batch_size)<br/>)</span></pre><p id="62de" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">现在，微调模型。要做到这一点，我们应该对优化器应用一个低的学习率，这样就不会打乱预先训练好的权重。我们将使用 0.00001 的学习率。此外，为了节省时间，我们只训练 4 个时期的模型。</p><p id="9c54" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">在微调之前，不要忘记解冻要训练的层。在我们的例子中，我们将解冻所有层。</p><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="4e99" class="ln lo it lj b gy lp lq l lr ls">unfreeze_weights(model_bigger)<br/><br/># compile again with low learning rate<br/>model_bigger %&gt;% compile(loss = "categorical_crossentropy",<br/>                         optimizer = optimizer_adam(lr = 0.00001),<br/>                         metrics = "accuracy")<br/><br/>history &lt;- model_bigger %&gt;%<br/>  fit_generator(<br/>  train_image_array_gen,<br/>  steps_per_epoch = as.integer(train_samples / batch_size),<br/>  epochs = 4,<br/>  validation_data = val_image_array_gen,<br/>  validation_steps = as.integer(valid_samples / batch_size)<br/>)</span><span id="ae0e" class="ln lo it lj b gy lt lq l lr ls">plot(history)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="ab gu cl ph"><img src="../Images/d688c8209a3855720160fa6ff358e999.png" data-original-src="https://miro.medium.com/v2/format:webp/1*oXBQQr0TvWUQTPqxbU6bJw.png"/></div></figure><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="8305" class="ln lo it lj b gy lp lq l lr ls">pred_test &lt;- predict_classes(model_bigger, test_x)<br/>pred_test &lt;- sapply(pred_test, decode)<br/>cm_bigger &lt;- confusionMatrix(as.factor(pred_test), as.factor(val_data$class))<br/>acc_bigger &lt;- cm_bigger$overall['Accuracy']<br/>cm_bigger</span><span id="fd88" class="ln lo it lj b gy lt lq l lr ls">#&gt; Confusion Matrix and Statistics<br/>#&gt; <br/>#&gt;            Reference<br/>#&gt; Prediction  buildings forest glacier mountain sea street<br/>#&gt;   buildings       396      0       2        1   2     13<br/>#&gt;   forest            1    469       2        2   4      0<br/>#&gt;   glacier           1      2     479       61   5      0<br/>#&gt;   mountain          0      0      50      452   4      0<br/>#&gt;   sea               1      1      16        7 492      2<br/>#&gt;   street           38      2       4        2   3    486<br/>#&gt; <br/>#&gt; Overall Statistics<br/>#&gt;                                               <br/>#&gt;                Accuracy : 0.9247              <br/>#&gt;                  95% CI : (0.9146, 0.9339)    <br/>#&gt;     No Information Rate : 0.1843              <br/>#&gt;     P-Value [Acc &gt; NIR] : &lt; 0.0000000000000002<br/>#&gt;                                               <br/>#&gt;                   Kappa : 0.9095              <br/>#&gt;                                               <br/>#&gt;  Mcnemar's Test P-Value : 0.00281             <br/>#&gt; <br/>#&gt; Statistics by Class:<br/>#&gt; <br/>#&gt;                      Class: buildings Class: forest Class: glacier Class: mountain Class: sea Class: street<br/>#&gt; Sensitivity                    0.9062        0.9895         0.8662          0.8610     0.9647        0.9701<br/>#&gt; Specificity                    0.9930        0.9964         0.9718          0.9782     0.9892        0.9804<br/>#&gt; Pos Pred Value                 0.9565        0.9812         0.8741          0.8933     0.9480        0.9084<br/>#&gt; Neg Pred Value                 0.9841        0.9980         0.9698          0.9707     0.9927        0.9939<br/>#&gt; Prevalence                     0.1457        0.1580         0.1843          0.1750     0.1700        0.1670<br/>#&gt; Detection Rate                 0.1320        0.1563         0.1597          0.1507     0.1640        0.1620<br/>#&gt; Detection Prevalence           0.1380        0.1593         0.1827          0.1687     0.1730        0.1783<br/>#&gt; Balanced Accuracy              0.9496        0.9929         0.9190          0.9196     0.9769        0.9752</span></pre><p id="911d" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">我们已经找到了赢家。<code class="fe om on oo lj b">model_bigger</code>在验证数据集上有 92%的准确率！尽管如此，还是有一些错误的分类，因为没有模型是完美的。以下是预测的摘要:</p><ol class=""><li id="4924" class="pi pj it lw b lx ly ma mb md pk mh pl ml pm mp pn po pp pq bi translated">有些建筑被误预测为街道，反之亦然。再次，这是由于一些建筑物的图像也包含街道，这使模型混淆。</li><li id="97cc" class="pi pj it lw b lx pr ma ps md pt mh pu ml pv mp pn po pp pq bi translated">森林预测几乎是完美的。</li><li id="bf67" class="pi pj it lw b lx pr ma ps md pt mh pu ml pv mp pn po pp pq bi translated">许多冰川被预测为山脉和海洋，也有许多山脉被预测为冰川。这可能来自冰川、山脉和海洋图像的相同蓝色。</li><li id="9d82" class="pi pj it lw b lx pr ma ps md pt mh pu ml pv mp pn po pp pq bi translated">海上的好预测。</li></ol><h1 id="fb48" class="nq lo it bd nr ns nt nu nv nw nx ny nz ki oa kj ob kl oc km od ko oe kp of og bi translated">结论</h1><pre class="ks kt ku kv gt li lj lk ll aw lm bi"><span id="bc3a" class="ln lo it lj b gy lp lq l lr ls">rbind(<br/>  "Simple CNN" = acc_simple,<br/>  "Deeper CNN" = acc_big,<br/>  "Fine-tuned VGG16" = acc_bigger<br/>)</span><span id="ecb3" class="ln lo it lj b gy lt lq l lr ls">#&gt;                   Accuracy<br/>#&gt; Simple CNN       0.7423333<br/>#&gt; Deeper CNN       0.8486667<br/>#&gt; Fine-tuned VGG16 0.9246667</span></pre><p id="8e36" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">我们已经成功地用 6 个类别进行了图像分类:“建筑物”、“森林”、“冰川”、“山”、“海”和“街道”。由于图像是非结构化数据，这个问题可以通过使用神经网络的机器学习来解决，神经网络在没有人工干预的情况下自动进行特征提取。为了获得更好的性能，我们使用卷积神经网络来实现这一点，并继续使用密集层进行预测。最后，我们使用 VGG16 模型和 IMAGENET 权重初始化，达到 92%的准确率。</p><p id="e84a" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">使用图像分类，拥有大型可视化数据库网站的公司可以轻松地组织和分类他们的数据库，因为它允许对大量图像进行自动分类。这有助于他们将视觉内容货币化，而无需投入大量时间进行手动分类和标记。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi qh"><img src="../Images/e1a6e3674ab93bcb99796285f9d0175c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*6HsoGpmIb1oibJc_JWbqJA.gif"/></div></div></figure></div><div class="ab cl qi qj hx qk" role="separator"><span class="ql bw bk qm qn qo"/><span class="ql bw bk qm qn qo"/><span class="ql bw bk qm qn"/></div><div class="im in io ip iq"><p id="4859" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">🔥<em class="pw">你好！如果你喜欢这个故事，想支持我这个作家，可以考虑</em> <a class="ae lh" href="https://dwiuzila.medium.com/membership" rel="noopener"> <strong class="lw jd"> <em class="pw">成为会员</em> </strong> </a> <em class="pw">。每月只需 5 美元，你就可以无限制地阅读媒体上的所有报道。如果你注册使用我的链接，我会赚一小笔佣金。</em></p><p id="bd58" class="pw-post-body-paragraph lu lv it lw b lx ly kd lz ma mb kg mc md me mf mg mh mi mj mk ml mm mn mo mp im bi translated">🔖<em class="pw">想了解更多关于经典机器学习模型如何工作以及如何优化其参数的信息？或者 MLOps 大型项目的例子？有史以来最优秀的文章呢？继续阅读:</em></p><div class="op oq gp gr or"><div role="button" tabindex="0" class="ab bv gv cb fp qp qq bn qr lb ex"><div class="qs l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw qt qu fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l qt qu fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----1673abd9884d--------------------------------" rel="noopener follow" target="_top">艾伯斯·乌兹拉</a></p></div></div><div class="qx qy gw l"><h2 class="bd jd wb pi fp wc fr fs oy fu fw jc bi translated">从零开始的机器学习</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wd au we wf wg ss wh an eh ei wi wj wk el em eo de bk ep" href="https://dwiuzila.medium.com/list/machine-learning-from-scratch-b35db8650093?source=post_page-----1673abd9884d--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wl l fo"><span class="bd b dl z dk">8 stories</span></div></div></div><div class="rk dh rl fp ab rm fo di"><div class="di rc bv rd re"><div class="dh l"><img alt="" class="dh" src="../Images/4b97f3062e4883b24589972b2dc45d7e.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*CWNoicci28F2TUQc-vKijw.png"/></div></div><div class="di rc bv rf rg rh"><div class="dh l"><img alt="" class="dh" src="../Images/b1f7021514ba57a443fe0db4b7001b26.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*wSRsSHYnIiGJFAqC"/></div></div><div class="di bv ri rj rh"><div class="dh l"><img alt="" class="dh" src="../Images/deb73e42c79667024a46c2c8902b81fa.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*HVEz7KwzO0tv1Q4d"/></div></div></div></div></div><div class="op oq gp gr or"><div role="button" tabindex="0" class="ab bv gv cb fp qp qq bn qr lb ex"><div class="qs l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw qt qu fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l qt qu fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----1673abd9884d--------------------------------" rel="noopener follow" target="_top">艾伯斯乌兹拉</a></p></div></div><div class="qx qy gw l"><h2 class="bd jd wb pi fp wc fr fs oy fu fw jc bi translated">高级优化方法</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wd au we wf wg ss wh an eh ei wi wj wk el em eo de bk ep" href="https://dwiuzila.medium.com/list/advanced-optimization-methods-26e264a361e4?source=post_page-----1673abd9884d--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wl l fo"><span class="bd b dl z dk">7 stories</span></div></div></div><div class="rk dh rl fp ab rm fo di"><div class="di rc bv rd re"><div class="dh l"><img alt="" class="dh" src="../Images/15b3188b0f29894c2bcf3d0965515f44.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*BVMamoNudzn9UlAE"/></div></div><div class="di rc bv rf rg rh"><div class="dh l"><img alt="" class="dh" src="../Images/3249ba2cf680952e2ccdff36d8ebf4a7.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*C1fv3HJdh1RBspwN"/></div></div><div class="di bv ri rj rh"><div class="dh l"><img alt="" class="dh" src="../Images/a73f0494533d8a08b01c2b899373d2b9.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*QZvzgiM2VnhYyx8M"/></div></div></div></div></div><div class="op oq gp gr or"><div role="button" tabindex="0" class="ab bv gv cb fp qp qq bn qr lb ex"><div class="qs l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw qt qu fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l qt qu fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----1673abd9884d--------------------------------" rel="noopener follow" target="_top">艾伯斯乌兹拉</a></p></div></div><div class="qx qy gw l"><h2 class="bd jd wb pi fp wc fr fs oy fu fw jc bi translated">MLOps 大型项目</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wd au we wf wg ss wh an eh ei wi wj wk el em eo de bk ep" href="https://dwiuzila.medium.com/list/mlops-megaproject-6a3bf86e45e4?source=post_page-----1673abd9884d--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wl l fo"><span class="bd b dl z dk">6 stories</span></div></div></div><div class="rk dh rl fp ab rm fo di"><div class="di rc bv rd re"><div class="dh l"><img alt="" class="dh" src="../Images/41b5d7dd3997969f3680648ada22fd7f.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*EBS8CP_UnStLesXoAvjeAQ.png"/></div></div><div class="di rc bv rf rg rh"><div class="dh l"><img alt="" class="dh" src="../Images/41befac52d90334c64eef7fc5c4b4bde.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*XLpRKnIMcJzBzCwvXrLvsw.png"/></div></div><div class="di bv ri rj rh"><div class="dh l"><img alt="" class="dh" src="../Images/80908ef475e97fbc42efe3fae0dfcff5.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/1*K_gzBmjv-ZHlU0Q6HeXclQ.jpeg"/></div></div></div></div></div><div class="op oq gp gr or"><div role="button" tabindex="0" class="ab bv gv cb fp qp qq bn qr lb ex"><div class="qs l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw qt qu fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l qt qu fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----1673abd9884d--------------------------------" rel="noopener follow" target="_top">艾伯斯乌兹拉</a></p></div></div><div class="qx qy gw l"><h2 class="bd jd wb pi fp wc fr fs oy fu fw jc bi translated">我最好的故事</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wd au we wf wg ss wh an eh ei wi wj wk el em eo de bk ep" href="https://dwiuzila.medium.com/list/my-best-stories-d8243ae80aa0?source=post_page-----1673abd9884d--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wl l fo"><span class="bd b dl z dk">24 stories</span></div></div></div><div class="rk dh rl fp ab rm fo di"><div class="di rc bv rd re"><div class="dh l"><img alt="" class="dh" src="../Images/0c862c3dee2d867d6996a970dd38360d.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*K1SQZ1rzr4cb-lSi"/></div></div><div class="di rc bv rf rg rh"><div class="dh l"><img alt="" class="dh" src="../Images/392d63d181090365a63dc9060573bcff.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*hSKy6kKorAfHjHOK"/></div></div><div class="di bv ri rj rh"><div class="dh l"><img alt="" class="dh" src="../Images/f51725806220b60eccf5d4c385c700e9.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*HiyGwoGOMI5Ao_fd"/></div></div></div></div></div><div class="op oq gp gr or"><div role="button" tabindex="0" class="ab bv gv cb fp qp qq bn qr lb ex"><div class="qs l"><div class="ab q"><div class="l di"><img alt="Albers Uzila" class="l de bw qt qu fe" src="../Images/b4f51438d99b29f789091dd239d7cfa6.png" width="20" height="20" loading="lazy" data-original-src="https://miro.medium.com/v2/resize:fill:40:40/1*WVCVz-tM5PW1TaWqnVXRLQ.jpeg"/><div class="fb bw l qt qu fc n aw fd"/></div><div class="hh l fo"><p class="bd b dl z fp fq fr fs ft fu fv fw dk translated"><a class="ae af ag ah ai aj ak al am an ao ap aq ar as" href="https://dwiuzila.medium.com/?source=post_page-----1673abd9884d--------------------------------" rel="noopener follow" target="_top">艾伯斯·乌兹拉</a></p></div></div><div class="qx qy gw l"><h2 class="bd jd wb pi fp wc fr fs oy fu fw jc bi translated">R 中的数据科学</h2></div><div class="ab q"><div class="l fo"><a class="bd b be z bi wd au we wf wg ss wh an eh ei wi wj wk el em eo de bk ep" href="https://dwiuzila.medium.com/list/data-science-in-r-0a8179814b50?source=post_page-----1673abd9884d--------------------------------" rel="noopener follow" target="_top">View list</a></div><div class="wl l fo"><span class="bd b dl z dk">7 stories</span></div></div></div><div class="rk dh rl fp ab rm fo di"><div class="di rc bv rd re"><div class="dh l"><img alt="" class="dh" src="../Images/e52e43bf7f22bfc0889cc794dcf734dd.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*10B3radiyQGAp-QA"/></div></div><div class="di rc bv rf rg rh"><div class="dh l"><img alt="" class="dh" src="../Images/945fa9100c2a00b46f8aca3d3975f288.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*o6A863Vdwq7ThlmW"/></div></div><div class="di bv ri rj rh"><div class="dh l"><img alt="" class="dh" src="../Images/3ca9e4b148297dbc4e7da0a180cf9c99.png" width="194" height="194" loading="lazy" role="presentation" data-original-src="https://miro.medium.com/v2/resize:fill:388:388/0*ekmX89TW6N8Bi8bL"/></div></div></div></div></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><a href="https://dwiuzila.medium.com/membership"><div class="gh gi rq"><img src="../Images/f767019309a71e9b3b70d2f9b1016aad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*q_dzEmnimgItuotIZ-y73A.png"/></div></a></figure></div></div>    
</body>
</html>