<html>
<head>
<title>Deep Learning Model Interpretation Using SHAP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用SHAP的深度学习模型解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-learning-model-interpretation-using-shap-a21786e91d16?source=collection_archive---------1-----------------------#2021-12-14">https://towardsdatascience.com/deep-learning-model-interpretation-using-shap-a21786e91d16?source=collection_archive---------1-----------------------#2021-12-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4cb2" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">图像和表格数据的Python实现</h2></div><p id="2c42" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">传统上，机器学习和深度学习的批评者说，即使他们获得了准确的预测，我们也在创建“黑盒”模型。但这是一种误解。机器学习和深度学习模型是可以解释的。模型解释在学术界和工业界的研究者中是一个非常活跃的领域。Christoph Molnar在他的书<a class="ae lb" href="https://christophm.github.io/interpretable-ml-book/" rel="noopener ugc nofollow" target="_blank">“可解释的机器学习”</a>中，将可解释性定义为人类能够理解决策原因的程度，或者人类能够持续预测ML模型结果的程度。</p><p id="7e91" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于可解释性，今天越来越多的公司在决策过程中有效地使用机器学习和深度学习，或者计划将它们纳入未来的战略，即使是在医药和金融等高度监管的领域。</p><p id="4a2e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">根据数据集的性质，一些数据科学家更喜欢经典的机器学习方法。有时深度学习在非表格领域表现出色，如计算机视觉、语言和语音识别。</p><p id="0cd5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当我们谈论模型可解释性时，理解全局方法和局部方法之间的区别是很重要的:</p><ul class=""><li id="ca67" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la lh li lj lk bi translated"><strong class="kh ir">全局方法</strong>是理解模型如何做出决策的整体结构。</li><li id="9213" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated"><strong class="kh ir">本地方法</strong>是理解模型如何为单个实例做出决策。</li></ul><p id="6b33" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有许多方法旨在提高模型的可解释性。<a class="ae lb" href="https://arxiv.org/abs/1705.07874" rel="noopener ugc nofollow" target="_blank"> SHAP值</a>是解释模型和理解数据特征与输出之间关系的最常用方法之一。这是一种源自联盟博弈理论的方法，它提供了一种在特性之间公平分配“支出”的方式。</p><p id="9b87" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">SHAP价值观的最大优势之一是它们提供了全局和局部的可解释性。本文重点介绍使用SHAP对表格数据和非结构化数据(本文图片)进行深度学习模型解释。</p><h2 id="8c69" class="lq lr iq bd ls lt lu dn lv lw lx dp ly ko lz ma mb ks mc md me kw mf mg mh mi bi translated">由SHAP深度讲解器讲解图像分类</h2><p id="fdbd" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">图像分类任务可以通过预测图像上每个像素的分数来解释，该分数指示它对将该图像分类到特定类别的贡献程度。在本文中，我将:</p><ul class=""><li id="3f8b" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la lh li lj lk bi translated">在<a class="ae lb" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank"> CIFAR10数据集</a>上训练CNN模型</li><li id="ff17" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">计算SHAP值</li><li id="3a84" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">可视化预测结果和SHAP值</li></ul><p id="ee38" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，让我们加载并转换keras内置数据集。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="bbaa" class="lq lr iq mt b gy mx my l mz na"># load package<br/>import shap<br/>import numpy as np<br/>import tensorflow as tf<br/>from tensorflow import keras<br/>import matplotlib.pyplot as plt<br/>from keras.models import Sequential<br/>import ssl</span><span id="e3ec" class="lq lr iq mt b gy nb my l mz na"># load build-in dataset<br/>ssl._create_default_https_context = ssl._create_unverified_context<br/>(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()</span><span id="c72e" class="lq lr iq mt b gy nb my l mz na"># reshape and normalize data<br/>x_train = x_train.reshape(50000, 32, 32, 3).astype("float32") / 255<br/>x_test = x_test.reshape(10000, 32, 32, 3).astype("float32") / 255<br/>y_train = y_train.reshape(50000,)<br/>y_test = y_test.reshape(10000,)</span></pre><p id="07ff" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们建立一个简单的神经网络，编译并拟合模型。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="e5b7" class="lq lr iq mt b gy mx my l mz na"># define the model architecture<br/>inputs = keras.Input(shape=(32, 32, 3))<br/>x = keras.layers.Conv2D(32, (3, 3), activation='relu')(inputs)<br/>x = keras.layers.MaxPooling2D((2, 2))(x)<br/>x = keras.layers.Conv2D(128, (3, 3), activation='relu')(inputs)<br/>x = keras.layers.MaxPooling2D((2, 2))(x)<br/>x = keras.layers.Conv2D(64, (3, 3), activation='relu')(inputs)<br/>x = keras.layers.MaxPooling2D((2, 2))(x)<br/>x = keras.layers.Conv2D(32, (3, 3), activation='relu')(inputs)<br/>x = keras.layers.MaxPooling2D((2, 2))(x)<br/>x = keras.layers.Flatten()(x)<br/>x = keras.layers.Dense(256, activation='relu')(x)<br/>x = keras.layers.Dense(64, activation='relu')(x)<br/>outputs = keras.layers.Dense(10, activation='softmax')(x)</span><span id="0127" class="lq lr iq mt b gy nb my l mz na"># inputs and outputs<br/>model = keras.Model(inputs=inputs, outputs=outputs, name="test_for_shap")</span><span id="6449" class="lq lr iq mt b gy nb my l mz na"># compile the model<br/>model.compile(<br/>      loss=tf.keras.losses.SparseCategoricalCrossentropy(),<br/>      optimizer=keras.optimizers.Adam(),<br/>      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]<br/>  )</span><span id="9b48" class="lq lr iq mt b gy nb my l mz na">model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs = 10)</span></pre><p id="1172" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来，让我们在测试集上进行预测(每个类一个图像)。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="c264" class="lq lr iq mt b gy mx my l mz na"># class label list<br/>class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',<br/>               'dog', 'frog', 'horse', 'ship', 'truck']</span><span id="daf2" class="lq lr iq mt b gy nb my l mz na"># example image for each class<br/>images_dict = dict()<br/>for i, l in enumerate(y_train):<br/>  if len(images_dict)==10:<br/>    break<br/>  if l not in images_dict.keys():<br/>    images_dict[l] = x_train[i].reshape((32, 32,3))<br/>images_dict = dict(sorted(images_dict.items()))<br/>    <br/># example image for each class for test set<br/>x_test_dict = dict()<br/>for i, l in enumerate(y_test):<br/>  if len(x_test_dict)==10:<br/>    break<br/>  if l not in x_test_dict.keys():<br/>    x_test_dict[l] = x_test[i]</span><span id="bed8" class="lq lr iq mt b gy nb my l mz na"># order by class<br/>x_test_each_class = [x_test_dict[i] for i in sorted(x_test_dict)]<br/>x_test_each_class = np.asarray(x_test_each_class)</span><span id="367e" class="lq lr iq mt b gy nb my l mz na"># Compute predictions<br/>predictions = model.predict(x_test_each_class)<br/>predicted_class = np.argmax(predictions, axis=1)</span></pre><p id="7345" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，我们为实际类和预测类定义绘图函数。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="d05e" class="lq lr iq mt b gy mx my l mz na"># plot actual and predicted class<br/>def plot_actual_predicted(images, pred_classes):<br/>  fig, axes = plt.subplots(1, 11, figsize=(16, 15))<br/>  axes = axes.flatten()<br/>  <br/>  # plot<br/>  ax = axes[0]<br/>  dummy_array = np.array([[[0, 0, 0, 0]]], dtype='uint8')<br/>  ax.set_title("Base reference")<br/>  ax.set_axis_off()<br/>  ax.imshow(dummy_array, interpolation='nearest')</span><span id="dae4" class="lq lr iq mt b gy nb my l mz na">  # plot image<br/>  for k,v in images.items():<br/>    ax = axes[k+1]<br/>    ax.imshow(v, cmap=plt.cm.binary)<br/>    ax.set_title(f"True: %s \nPredict: %s" % (class_names[k], class_names[pred_classes[k]]))<br/>    ax.set_axis_off()</span><span id="6bea" class="lq lr iq mt b gy nb my l mz na">  plt.tight_layout()<br/>  plt.show()</span></pre><p id="574e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们可以使用SHAP库来生成SHAP值:</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="497b" class="lq lr iq mt b gy mx my l mz na"># select backgroud for shap<br/>background = x_train[np.random.choice(x_train.shape[0], 1000, replace=False)]</span><span id="316b" class="lq lr iq mt b gy nb my l mz na"># DeepExplainer to explain predictions of the model<br/>explainer = shap.DeepExplainer(model, background)</span><span id="2629" class="lq lr iq mt b gy nb my l mz na"># compute shap values<br/>shap_values = explainer.shap_values(x_test_each_class)</span></pre><p id="37b9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，我们可以看到SHAP的价值观。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="18bf" class="lq lr iq mt b gy mx my l mz na"># plot SHAP values<br/>plot_actual_predicted(images_dict, predicted_class)<br/>print()<br/>shap.image_plot(shap_values, x_test_each_class * 255)</span></pre><figure class="mo mp mq mr gt nd gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nc"><img src="../Images/0bfc33ef3935a8289b16db7c3fa35349.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qh-6QD_mJK4BcibsJTwPtQ.png"/></div></div></figure><figure class="mo mp mq mr gt nd gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nk"><img src="../Images/4ddad5b0954c693d4ec493264ffc937f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gENX13LsXnPBuaF9D8ifUQ.png"/></div></div><p class="nl nm gj gh gi nn no bd b be z dk translated">按作者</p></figure><p id="1623" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这个可视化中:</p><ul class=""><li id="98e8" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la lh li lj lk bi translated">红色像素代表有助于将该图像分类为特定类别的正SHAP值。</li><li id="628d" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">蓝色像素表示负SHAP值，这导致没有将该图像分类为该特定类别。</li></ul><h2 id="8f9f" class="lq lr iq bd ls lt lu dn lv lw lx dp ly ko lz ma mb ks mc md me kw mf mg mh mi bi translated">用SHAP深层解释器解释表格数据分类</h2><p id="a53d" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">大量的数据可以用表格的形式表示。下图是一个完全连接的神经网络，通过SHAP深度解释器，我们可以知道哪个输入特征实际上对模型输出和大小有贡献。</p><figure class="mo mp mq mr gt nd gh gi paragraph-image"><div class="gh gi np"><img src="../Images/c5440442c0618f7a9017a27df3e08f8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*ytmnkYCsse8cESuTkGyy7Q.jpeg"/></div><p class="nl nm gj gh gi nn no bd b be z dk translated">按作者</p></figure><p id="379e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文中，我将使用开源的<a class="ae lb" href="https://www.kaggle.com/c/titanic/data" rel="noopener ugc nofollow" target="_blank"> Titanic数据集</a>进行演示:</p><ul class=""><li id="1b16" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la lh li lj lk bi translated">训练多层感知模型</li><li id="90d1" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">计算SHAP值</li><li id="c935" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">视觉全局和局部图</li></ul><p id="34cd" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，我加载包和泰坦尼克号数据。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="5fc9" class="lq lr iq mt b gy mx my l mz na"># import package<br/>import shap<br/>import numpy as np <br/>import pandas as pd <br/>import matplotlib.pyplot as plt<br/>from sklearn.preprocessing import StandardScaler<br/>from tensorflow.keras.models import Sequential<br/>from tensorflow.keras.layers import Dense, Dropout<br/>from tensorflow.keras import optimizers<br/>import warnings <br/>warnings.filterwarnings('ignore')<br/>%matplotlib inline<br/>import os</span><span id="1eda" class="lq lr iq mt b gy nb my l mz na"># load data<br/>os.chdir('/titanic/')<br/>train_data = pd.read_csv('./train.csv', index_col=0)<br/>test_data = pd.read_csv('./test.csv', index_col=0)<br/>train_data.head()</span></pre><p id="78d0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，我们通过删除不必要的列、处理丢失的数据、将分类特征转换为数字特征以及进行一次性编码来处理原始数据。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="e019" class="lq lr iq mt b gy mx my l mz na">def data_preprocessing(df):<br/>    df = df.drop(columns=['Name', 'Ticket', 'Cabin'])<br/>    <br/>    # fill na<br/>    df[['Age']] = df[['Age']].fillna(value=df[['Age']].mean())<br/>    df[['Embarked']] = df[['Embarked']].fillna(value=df['Embarked'].value_counts().idxmax())<br/>    df[['Fare']] = df[['Fare']].fillna(value=df[['Fare']].mean())<br/>    <br/>    # categorical features into numeric<br/>    df['Sex'] = df['Sex'].map( {'female': 1, 'male': 0} ).astype(int)<br/>    <br/>    # one-hot encoding<br/>    embarked_one_hot = pd.get_dummies(df['Embarked'], prefix='Embarked')<br/>    df = df.drop('Embarked', axis=1)<br/>    df = df.join(embarked_one_hot)<br/>    <br/>    return df</span><span id="6291" class="lq lr iq mt b gy nb my l mz na"># train data processing<br/>train_data = data_preprocessing(train_data)<br/>train_data.isnull().sum()</span><span id="6e04" class="lq lr iq mt b gy nb my l mz na"># create data for training<br/>x_train = train_data.drop(['Survived'], axis=1).values</span><span id="3ed8" class="lq lr iq mt b gy nb my l mz na"># Check test data<br/>test_data.isnull().sum()</span><span id="c29d" class="lq lr iq mt b gy nb my l mz na"># scale<br/>scale = StandardScaler()<br/>x_train = scale.fit_transform(x_train)</span><span id="30ea" class="lq lr iq mt b gy nb my l mz na"># prepare y_train<br/>y_train = train_data['Survived'].values</span><span id="a24c" class="lq lr iq mt b gy nb my l mz na">test_data = data_preprocessing(test_data)<br/>x_test = test_data.values.astype(float)</span><span id="1fc3" class="lq lr iq mt b gy nb my l mz na"># scaling<br/>x_test = scale.transform(x_test)</span><span id="9986" class="lq lr iq mt b gy nb my l mz na"># Check test data<br/>test_data.isnull().sum()</span></pre><p id="5401" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">接下来是定义、编译和拟合keras模型。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="54de" class="lq lr iq mt b gy mx my l mz na"># build mlp<br/>model = Sequential()<br/>model.add(Dense(32, input_dim=x_train.shape[1], activation='relu'))<br/>model.add(Dropout(0.25))<br/>model.add(Dense(128, activation='relu'))<br/>model.add(Dropout(0.25))<br/>model.add(Dense(32, activation='relu'))<br/>model.add(Dropout(0.25))<br/>model.add(Dense(8, activation='relu'))<br/>model.add(Dropout(0.25))<br/>model.add(Dense(2, activation='softmax'))</span><span id="68f4" class="lq lr iq mt b gy nb my l mz na"># compile model<br/>model.compile(loss='sparse_categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])</span><span id="1606" class="lq lr iq mt b gy nb my l mz na"># fit model<br/>model.fit(x_train, y_train, epochs=10, batch_size=64)</span></pre><p id="8fdb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后我计算SHAP值。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="48f8" class="lq lr iq mt b gy mx my l mz na"># compute SHAP values<br/>explainer = shap.DeepExplainer(model, x_train)<br/>shap_values = explainer.shap_values(x_test)</span></pre><p id="7168" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> SHAP全球解读</strong></p><p id="7571" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">汇总图</strong>显示了最重要的特征及其对模型的影响程度。这是全球的解释。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="ea68" class="lq lr iq mt b gy mx my l mz na">shap.summary_plot(shap_values[0], plot_type = 'bar', feature_names = test_data.columns)</span></pre><figure class="mo mp mq mr gt nd gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/649fb96b2170321285c6eebbd619ad09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*iF4ZVGzsRCrjYrVOfk8c3Q.png"/></div><p class="nl nm gj gh gi nn no bd b be z dk translated">按作者</p></figure><p id="1c0b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> SHAP当地解读</strong></p><p id="5512" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文中，我将展示一个示例的三种可视化解释:</p><ul class=""><li id="dcc4" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la lh li lj lk bi translated">力图</li><li id="e4e4" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">决策图</li><li id="66af" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">瀑布图</li></ul><p id="8ebc" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">力图有助于了解“输出值”相对于“基础值”的位置。我们还可以看到哪些要素对预测有正面(红色)或负面(蓝色)影响，以及影响的大小。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="4fa2" class="lq lr iq mt b gy mx my l mz na">shap.initjs()<br/>shap.force_plot(explainer.expected_value[0].numpy(), shap_values[0][0], features = test_data.columns)</span></pre><figure class="mo mp mq mr gt nd gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi nr"><img src="../Images/1360dbbfa002018ce1d29d2b7ec2547e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Q-X1Zg19b4sb26IwJXcVeA.jpeg"/></div></div></figure><p id="1550" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">决策图可以观察每个变化的幅度，由样本对显示的特征值进行处理。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="39c9" class="lq lr iq mt b gy mx my l mz na">shap.decision_plot(explainer.expected_value[0].numpy(), shap_values[0][0], features = test_data.iloc[0,:], feature_names = test_data.columns.tolist())</span></pre><figure class="mo mp mq mr gt nd gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/32c2663953b4b7d12f61cf43f0179bd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*hdrD8Y9rZYqWHXm0WMJwNg.png"/></div><p class="nl nm gj gh gi nn no bd b be z dk translated">按作者</p></figure><p id="a8f6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">瀑布图还允许我们看到某个特性影响的幅度和性质。它还允许查看特征的重要性顺序以及样本的每个特征所取的值。</p><pre class="mo mp mq mr gt ms mt mu mv aw mw bi"><span id="b0a6" class="lq lr iq mt b gy mx my l mz na">shap.plots._waterfall.waterfall_legacy(explainer.expected_value[0].numpy(), shap_values[0][0], feature_names = test_data.columns)</span></pre><figure class="mo mp mq mr gt nd gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/b441c60c3290ca964a44b805aad0f5fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*bvWw3M5UIbfCW7w3m3raog.png"/></div><p class="nl nm gj gh gi nn no bd b be z dk translated">按作者</p></figure><h2 id="7833" class="lq lr iq bd ls lt lu dn lv lw lx dp ly ko lz ma mb ks mc md me kw mf mg mh mi bi translated">结论和下一步措施</h2><p id="6c68" class="pw-post-body-paragraph kf kg iq kh b ki mj jr kk kl mk ju kn ko ml kq kr ks mm ku kv kw mn ky kz la ij bi translated">AI算法在我们生活中占据的空间越来越大，被广泛应用于各个行业。对于数据科学家、决策者和监管者来说，模型可解释性是一个非常重要的话题。在这篇文章中，我主要讲述了使用python代码一步步对图像和表格数据进行深度学习模型解释。我将在未来的帖子中分享更多关于文本数据的深度学习模型解释，而不是指导实现。</p><h2 id="4181" class="lq lr iq bd ls lt lu dn lv lw lx dp ly ko lz ma mb ks mc md me kw mf mg mh mi bi translated">参考</h2><ol class=""><li id="55cc" class="lc ld iq kh b ki mj kl mk ko ns ks nt kw nu la nv li lj lk bi translated"><a class="ae lb" href="https://christophm.github.io/interpretable-ml-book/" rel="noopener ugc nofollow" target="_blank">https://christophm.github.io/interpretable-ml-book/</a></li><li id="dd46" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la nv li lj lk bi translated"><a class="ae lb" href="https://cloud.google.com/blog/products/ai-machine-learning/why-you-need-to-explain-machine-learning-models" rel="noopener ugc nofollow" target="_blank">https://cloud . Google . com/blog/products/ai-machine-learning/why-you-need-to-explain-machine-learning-models</a></li><li id="af76" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la nv li lj lk bi translated">https://github.com/slundberg/shap<a class="ae lb" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank"/></li><li id="5978" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la nv li lj lk bi translated">【https://www.kaggle.com/c/titanic/data T4】</li><li id="9863" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la nv li lj lk bi translated"><a class="ae lb" href="https://www.openml.org/d/40945" rel="noopener ugc nofollow" target="_blank">https://www.openml.org/d/40945</a></li><li id="f2be" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la nv li lj lk bi translated"><a class="ae lb" href="https://en.wikipedia.org/wiki/Explainable_artificial_intelligence" rel="noopener ugc nofollow" target="_blank">https://en . Wikipedia . org/wiki/explable _ artificial _ intelligence</a></li><li id="b64a" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la nv li lj lk bi translated">neuralnetworksanddeeplearning.com</li><li id="592a" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la nv li lj lk bi translated"><a class="ae lb" href="https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf" rel="noopener ugc nofollow" target="_blank">https://www . cs . Toronto . edu/~ kriz/learning-features-2009-tr . pdf</a></li><li id="3e19" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la nv li lj lk bi translated"><a class="ae lb" href="https://www.cs.toronto.edu/~kriz/cifar.html" rel="noopener ugc nofollow" target="_blank">https://www.cs.toronto.edu/~kriz/cifar.html</a></li></ol></div></div>    
</body>
</html>