<html>
<head>
<title>Deep Q-Learning for the Cliff Walking Problem</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">悬崖行走问题的深度Q学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-q-learning-for-the-cliff-walking-problem-b54835409046?source=collection_archive---------21-----------------------#2021-09-08">https://towardsdatascience.com/deep-q-learning-for-the-cliff-walking-problem-b54835409046?source=collection_archive---------21-----------------------#2021-09-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="851f" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="d06a" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">一个完整的Python实现，用TensorFlow 2.0导航悬崖。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/0e7078785350ebf89bc72541963c1512.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*divJg1bLbHNPTiQU"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@nate_dumlao?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">内森·杜姆劳</a>在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure></div><div class="ab cl li lj hx lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="im in io ip iq"><p id="f3bc" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">乍一看，从普通Q-学习到深度Q-学习似乎是一小步。只要把查找表换成神经网络就大功告成了。然而，事情远不止如此——即使对于最简单的问题，深度Q学习也可能难以取得结果。</p><p id="aa9c" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">为了展示如何避免一些常见的陷阱，本文讨论了深度Q学习的TensorFlow 2.0实现，以解决众所周知的<strong class="lr jd">悬崖行走问题</strong>。首先，我们展示了标准化和一键编码如何将输入和输出与神经网络对齐。然后，我们部署了三种据说可以显著提高深度Q学习常见技术:<strong class="lr jd">经验重放</strong>、<strong class="lr jd">目标网络</strong>和<strong class="lr jd">小批量</strong>。</p><h1 id="b3d9" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">悬崖行走问题</h1><p id="5325" class="pw-post-body-paragraph lp lq it lr b ls nd kd lu lv ne kg lx ly nf ma mb mc ng me mf mg nh mi mj mk im bi translated">悬崖行走问题(关于普通Q-learning和SARSA实现的文章<a class="ae lh" rel="noopener" target="_blank" href="/walking-off-the-cliff-with-off-policy-reinforcement-learning-7fdbcdfe31ff">这里</a>)相当简单[1]。代理从左下角开始，必须到达右下角。走进分隔这些瓦片的悬崖会产生巨大的负面奖励，并结束这一集。否则，每一步都要付出很小的代价，这意味着最短的路径是最优策略。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ni"><img src="../Images/5f4d13cbe56bfec804201001f9a70090.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M79MspX4MckRX3wqKCgAxA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">悬崖行走词示例。目标瓦片产生正奖励，每走一步产生小负奖励，掉下悬崖产生大负奖励[图片作者]</p></figure><p id="3b05" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">这个问题可以通过Q-learning解决。这是一种强化学习方法，它存储代表每个状态-动作对(总48⋅4)的预期未来回报的q值。值得一提的是，Q-learning是一种<strong class="lr jd">非策略</strong>方法——对于下游动作，我们假设最佳可能动作，而不是实际样本轨迹的动作。因此，Q-learning鼓励探索，因为失败的影响相对有限。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi nj"><img src="../Images/7730bea28ef55933653e2445cb721cf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wu23e-rTianWOYi7E58D6A.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">Q-learning是一种非策略强化学习方法。基于在t+1时采取的最佳动作而不是实际动作来更新时间t时的Q值。因此，勘探的负面影响是有限的。</p></figure><h1 id="3866" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">从普通Q学习到深度Q学习</h1><p id="6f8e" class="pw-post-body-paragraph lp lq it lr b ls nd kd lu lv ne kg lx ly nf ma mb mc ng me mf mg nh mi mj mk im bi translated">在之前的一篇文章中，我提供了一个在TensorFlow 2.0中实现<a class="ae lh" rel="noopener" target="_blank" href="/a-minimal-working-example-for-deep-q-learning-in-tensorflow-2-0-e0ca8a944d5e">深度Q学习</a>的最小工作问题。这里，我假设你熟悉普通Q学习，至少熟悉深度Q学习的基础。简单回顾一下:在深度Q学习中，我们训练一个神经网络，它将状态作为输入，并输出每个动作的Q值。虽然查找表对于大的状态空间来说是爆炸式的，但是深度Q网络提供了一种对所有状态都适用的紧凑表示。然而，仅仅替换查找表通常是不够的，我们通常需要以下技术(在这里用实现细节<a class="ae lh" rel="noopener" target="_blank" href="/how-to-model-experience-replay-batch-learning-and-target-networks-c1350db93172">描述)来获得更好的结果[2]:</a></p><ul class=""><li id="4788" class="nk nl it lr b ls lt lv lw ly nm mc nn mg no mk np nq nr ns bi translated">标准化和一次性编码</li><li id="268a" class="nk nl it lr b ls nt lv nu ly nv mc nw mg nx mk np nq nr ns bi translated">体验回放</li><li id="4f46" class="nk nl it lr b ls nt lv nu ly nv mc nw mg nx mk np nq nr ns bi translated">目标网络</li><li id="5c87" class="nk nl it lr b ls nt lv nu ly nv mc nw mg nx mk np nq nr ns bi translated">小批量</li></ul><h2 id="85c5" class="ny mm it bd mn nz oa dn mr ob oc dp mv ly od oe mx mc of og mz mg oh oi nb iz bi translated">标准化和一次性编码</h2><p id="fac0" class="pw-post-body-paragraph lp lq it lr b ls nd kd lu lv ne kg lx ly nf ma mb mc ng me mf mg nh mi mj mk im bi translated">重要的事情先来。当处理相同大小的输入和输出时，神经网络往往工作得更好。因此，我们将所有奖励除以100，所以它们(大约)在-1和1之间。累积奖励信号是网络的<strong class="lr jd">目标输出</strong>。</p><p id="cd48" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">我们还必须注意定义作为神经网络输入的状态。在普通的Q学习中，我们可能会将目标瓦片称为“状态48”，将相邻的悬崖瓦片称为“状态47”，但自然地，这样的数值对神经网络来说意义不大。它只是简单地乘以某个权重向量，无法区分47和48具有完全不同的含义。为了解决这个问题，我们应用了<strong class="lr jd"> one-hot </strong>编码，将输入定义为长度为48的数组。对应于当前图块的元素具有值1，所有其他元素具有值0。因此，我们可以学习对应于每个瓦片的唯一的一组权重</p><p id="cc59" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">输入和输出现在都被规范化了，所以我们可以安全地用类似于<strong class="lr jd"> He-或Glorot初始化</strong>的东西初始化权重，不再担心标度差异。</p><h2 id="4ca0" class="ny mm it bd mn nz oa dn mr ob oc dp mv ly od oe mx mc of og mz mg oh oi nb iz bi translated">体验回放</h2><p id="7b8d" class="pw-post-body-paragraph lp lq it lr b ls nd kd lu lv ne kg lx ly nf ma mb mc ng me mf mg nh mi mj mk im bi translated">Q-learning依赖于时间差异，使用“预期”值<code class="fe oj ok ol om b">Q_t</code>和“观察”值<code class="fe oj ok ol om b">r_t+Q_t+1</code>之间的差异作为误差。不幸的是，<code class="fe oj ok ol om b">Q_t+1</code>和<code class="fe oj ok ol om b">Q_t</code>一样都是猜测——它们是由同一个神经网络决定的。</p><p id="40c1" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">更糟糕的是，随后的观察结果往往高度相似，使用几乎相同的状态作为输入。这在相邻的观测值之间产生了很强的相关性。特别是<strong class="lr jd">非线性近似</strong>(如神经网络)往往很难处理这种相关性。</p><p id="0dc7" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">用不那么抽象的术语来说:考虑一个被困在悬崖世界角落里的代理人。这个代理人可能会不断地收到相同的奖励信号，并在网络上对这个特定角落的行动进行过度拟合。你会经常看到这种情况发生:代理人了解到向右走不好(由于悬崖)并且倾向于停留在世界的左边，不断地调整网络以适应那个区域。</p><p id="8b9a" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">为了打破这种关联，我们部署了<strong class="lr jd">体验回放</strong>。不是总是选择最近的观察来更新我们的网络，而是每当我们更新时，我们存储<em class="on">我们过去的所有</em>观察和来自这个<strong class="lr jd">重放缓冲器</strong>的样本。每个观察由一个<code class="fe oj ok ol om b">(s,a,r,s’)</code>元组表示。为了计算<em class="on">当前的</em> Q值(从我们获得观测值的时间起<em class="on">而不是</em>，我们将<code class="fe oj ok ol om b">s</code>和<code class="fe oj ok ol om b">s’</code>馈送到主要网络，使用<code class="fe oj ok ol om b">a</code>获得<code class="fe oj ok ol om b">Q_t</code>，使用<em class="on"> argmax </em>动作获得<code class="fe oj ok ol om b">Q_t+1</code>。</p><h2 id="12c4" class="ny mm it bd mn nz oa dn mr ob oc dp mv ly od oe mx mc of og mz mg oh oi nb iz bi translated">小批量</h2><p id="08f7" class="pw-post-body-paragraph lp lq it lr b ls nd kd lu lv ne kg lx ly nf ma mb mc ng me mf mg nh mi mj mk im bi translated">理论上，我们可以收集许多观察结果，并将神经网络拟合到单个批次，通过单次更新来确定最终策略。在许多监督学习问题中，在大型数据集上一次性训练网络是非常常见的。然而，在强化学习的情况下，我们将总是基于我们的初始策略进行观察(这可能非常糟糕)。我们希望探索足够多的东西，以避免陷入局部极小，同时也主要从好的动作中学习。</p><p id="392c" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">因此，大批量不是很有用。我们希望将观察和更新结合起来，逐步改进我们的政策。然而，这并不意味着我们必须更新每一个观察结果——悬崖行走问题中的一个步骤往往教会我们很少东西，因为我们没有观察到任何有意义的东西。显而易见的妥协是<strong class="lr jd">小批量</strong>，这意味着我们经常用大量的观察数据更新我们的网络。特别是与经验重放相结合，这是一种强大的技术，可以基于大量以前的观察获得稳定的更新。</p><h2 id="a409" class="ny mm it bd mn nz oa dn mr ob oc dp mv ly od oe mx mc of og mz mg oh oi nb iz bi translated">目标网络</h2><p id="66bb" class="pw-post-body-paragraph lp lq it lr b ls nd kd lu lv ne kg lx ly nf ma mb mc ng me mf mg nh mi mj mk im bi translated">到目前为止，我们已经从同一个网络中得出期望值<code class="fe oj ok ol om b">Q_t</code>和“真实”值<code class="fe oj ok ol om b">r_t+Q_t+1</code>。因此，观察和目标相互关联，再次使用一个猜测来更新另一个猜测。</p><p id="df53" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">为了缓解这个问题，我们使用了一个目标网络。目标网络只不过是Q网络的周期性副本，以较低的频率更新(比如每100集一次)。因此，我们将期望值和目标值分离，减少了两者之间的相关性。目标保持相当稳定，而政策本身则在逐步改进。</p><h1 id="760e" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">深度Q-将学习付诸行动</h1><p id="2957" class="pw-post-body-paragraph lp lq it lr b ls nd kd lu lv ne kg lx ly nf ma mb mc ng me mf mg nh mi mj mk im bi translated">在概述了实现深度Q学习所需的理论之后，让我们看看它在实践中的实际表现。在我的<a class="ae lh" href="https://github.com/woutervanheeswijk/cliff_walking_public" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>中可以找到关于悬崖行走问题的学习方法的完整实现。</p><p id="6c1c" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">让我们给我们的Q网络——3个隐藏层，每个层有25个神经元——一个旋转。与表格变量相同的0.1学习率，没有重大调整。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/ff2bbcbcda289ced4ff54aa45d12961c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*1bjqH_cgTNu9F5Kos8yOdg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">所有算法学习率α=0.1的强化学习。深度Q学习在这里实际上什么也学不到，而Q学习和SARSA很快收敛。[图片由作者提供]</p></figure><p id="bd67" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">呀！Q-learning和SARSA迅速收敛到稳定的政策，而深度Q-learning似乎没有学到任何值得注意的东西。最重要的是，后者需要更多的计算工作。在我的笔记本电脑上，10，000次迭代需要大约20分钟，而Q-learning和SARSA只需要几秒钟。</p><p id="7c87" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">幸运的是，经过一些调整——特别是，将学习率降低到0.001——事情看起来好多了。收敛仍然需要比表格变量长得多的时间，但是整体深度Q-学习达到了与常规Q-学习相同的策略。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/fc4ff9db7ca810719078baa495b1dd6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*VfCUm-OdZtAyheOBGAmPHQ.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">用于深度Q学习的学习速率α=0.001的强化学习在这里有效地不学习任何东西，而Q学习和SARSA快速收敛。[图片由作者提供]</p></figure><p id="8867" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">让我们拿出我们剩下的武器库:体验重播，一批5个(更新频率也减少了5倍，以保留相同数量的数据点)和每10集更新一次的目标网络。最终结果:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/e5c8a9b70ea561cc88a0cdc067f7ad4f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*pf4Grruyuzm_0MFV92BcdA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">深度Q学习，包括经验回放、小批量和目标网络。部署这些稳定技术并没有导致更快的收敛。[图片由作者提供]</p></figure><p id="5f02" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">那…实际上很令人失望。尽管我们做了所有的努力，一个简单的Q表仍然打败了我们花哨的神经网络。发生了什么事？哪里出了问题？</p><p id="6998" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">什么都没发生。没出什么差错。深度学习很有挑战性。我们试图学习一个适用于所有状态的通用函数。通过多层反向传播错误也需要时间。我们的未过滤重放缓冲区保存了许多糟糕的观察结果。为了适应目标网络和批量学习，降低更新频率也会降低收敛速度。神经网络不是神奇的黑匣子。机器学习并不等同于超自然智能。</p><p id="875c" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">当然，我们本可以在调优上花更多的时间来提高性能。在一个小批量中应该有多少个观察值？什么是理想的网络架构？我们应该给予体验什么样的优先权？我们必须多久更新一次目标网络？所有重要的问题，也是需要时间来回答的问题。<strong class="lr jd">参数调整成本高。</strong></p><p id="008b" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">有些发人深省的结论是:尽管潜在的难以置信的强大，深度学习也很难成功实施。根据你的问题和你的目标，这可能并不总是最好的方法。</p><h1 id="d138" class="ml mm it bd mn mo mp mq mr ms mt mu mv ki mw kj mx kl my km mz ko na kp nb nc bi translated">外卖食品</h1><ul class=""><li id="2a58" class="nk nl it lr b ls nd lv ne ly op mc oq mg or mk np nq nr ns bi translated">深度Q学习不仅仅是用神经网络代替查找表。它的性能通常不太稳定，需要更多的建模和调优工作。</li><li id="6e03" class="nk nl it lr b ls nt lv nu ly nv mc nw mg nx mk np nq nr ns bi translated">使用适当的<strong class="lr jd">归一化</strong>和<strong class="lr jd">一键编码</strong>使状态和动作适合神经网络。</li><li id="f00a" class="nk nl it lr b ls nt lv nu ly nv mc nw mg nx mk np nq nr ns bi translated"><strong class="lr jd">经验回放</strong>——从过去<code class="fe oj ok ol om b">(s,a,r,s)</code>元组的缓冲区中随机取样——打破了后续观察之间的相关性。</li><li id="ad9c" class="nk nl it lr b ls nt lv nu ly nv mc nw mg nx mk np nq nr ns bi translated"><strong class="lr jd">目标网络</strong>——Q网络的周期性副本——可用于计算Q_t+1。这降低了期望值<code class="fe oj ok ol om b">Q_t</code>和目标值<code class="fe oj ok ol om b">r_t+Q_t+1</code>之间的相关性。</li><li id="3fdb" class="nk nl it lr b ls nt lv nu ly nv mc nw mg nx mk np nq nr ns bi translated"><strong class="lr jd">小批量</strong>稳定更新，一次利用多个观察更新网络。</li></ul></div><div class="ab cl li lj hx lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="im in io ip iq"><p id="b946" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated"><em class="on">深度Q学习算法的完整代码可以在我的</em> <a class="ae lh" href="https://github.com/woutervanheeswijk/cliff_walking_public" rel="noopener ugc nofollow" target="_blank"> <em class="on"> GitHub资源库</em> </a> <em class="on">上找到。</em></p><p id="e2bc" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated"><em class="on">我实现的表格</em> <strong class="lr jd"> <em class="on"> Q-learning和SARSA </em> </strong> <em class="on">对于悬崖行走的问题在这里有详细介绍:</em></p><div class="os ot gp gr ou ov"><a rel="noopener follow" target="_blank" href="/walking-off-the-cliff-with-off-policy-reinforcement-learning-7fdbcdfe31ff"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd jd gy z fp pa fr fs pb fu fw jc bi translated">用非策略强化学习走下悬崖</h2><div class="pc l"><h3 class="bd b gy z fp pa fr fs pb fu fw dk translated">政策外强化学习和政策内强化学习的深入比较</h3></div><div class="pd l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">towardsdatascience.com</p></div></div><div class="pe l"><div class="pf l pg ph pi pe pj lb ov"/></div></div></a></div><p id="e149" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated"><em class="on"/><strong class="lr jd"><em class="on">离散策略梯度</em> </strong> <em class="on">变体如本文所示:</em></p><div class="os ot gp gr ou ov"><a rel="noopener follow" target="_blank" href="/cliff-walking-problem-with-the-discrete-policy-gradient-algorithm-59d1900d80d8"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd jd gy z fp pa fr fs pb fu fw jc bi translated">基于离散策略梯度算法的悬崖行走问题</h2><div class="pc l"><h3 class="bd b gy z fp pa fr fs pb fu fw dk translated">用Python实现了一个完整的增强算法。手动执行这些步骤来说明内部…</h3></div><div class="pd l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">towardsdatascience.com</p></div></div><div class="pe l"><div class="pk l pg ph pi pe pj lb ov"/></div></div></a></div><p id="2bb7" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated"><em class="on">为</em> <strong class="lr jd"> <em class="on">深策渐变</em> </strong> <em class="on">，勾选:</em></p><div class="os ot gp gr ou ov"><a rel="noopener follow" target="_blank" href="/deep-policy-gradient-for-cliff-walking-37d5014fd4bc"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd jd gy z fp pa fr fs pb fu fw jc bi translated">悬崖漫步的深度政策梯度</h2><div class="pc l"><h3 class="bd b gy z fp pa fr fs pb fu fw dk translated">用Python实现TensorFlow 2.0。在这个解决方案中，参与者由一个神经网络来表示，它是…</h3></div><div class="pd l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">towardsdatascience.com</p></div></div><div class="pe l"><div class="pl l pg ph pi pe pj lb ov"/></div></div></a></div><p id="e9dc" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated"><em class="on">下面的文章给出了</em> <strong class="lr jd"> <em class="on">深度Q学习</em> </strong> <em class="on">的一个最小工作示例:</em></p><div class="os ot gp gr ou ov"><a rel="noopener follow" target="_blank" href="/a-minimal-working-example-for-deep-q-learning-in-tensorflow-2-0-e0ca8a944d5e"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd jd gy z fp pa fr fs pb fu fw jc bi translated">TensorFlow 2.0中深度Q学习的最小工作示例</h2><div class="pc l"><h3 class="bd b gy z fp pa fr fs pb fu fw dk translated">一个多臂土匪的例子来训练一个Q网络。使用TensorFlow，更新过程只需要几行代码</h3></div><div class="pd l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">towardsdatascience.com</p></div></div><div class="pe l"><div class="pm l pg ph pi pe pj lb ov"/></div></div></a></div><p id="5084" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated"><em class="on">最后，我在这里讨论经验回放、批量学习和目标学习的实现示例:</em></p><div class="os ot gp gr ou ov"><a rel="noopener follow" target="_blank" href="/how-to-model-experience-replay-batch-learning-and-target-networks-c1350db93172"><div class="ow ab fo"><div class="ox ab oy cl cj oz"><h2 class="bd jd gy z fp pa fr fs pb fu fw jc bi translated">如何对经验重放、批量学习和目标网络进行建模</h2><div class="pc l"><h3 class="bd b gy z fp pa fr fs pb fu fw dk translated">使用TensorFlow 2.0，快速学习稳定和成功的深度Q学习的三个基本技巧</h3></div><div class="pd l"><p class="bd b dl z fp pa fr fs pb fu fw dk translated">towardsdatascience.com</p></div></div><div class="pe l"><div class="pn l pg ph pi pe pj lb ov"/></div></div></a></div><h2 id="b1b3" class="ny mm it bd mn nz oa dn mr ob oc dp mv ly od oe mx mc of og mz mg oh oi nb iz bi translated">参考</h2><p id="647e" class="pw-post-body-paragraph lp lq it lr b ls nd kd lu lv ne kg lx ly nf ma mb mc ng me mf mg nh mi mj mk im bi translated">[1]萨顿和巴尔托(2018年)。<em class="on">强化学习:简介</em>。麻省理工出版社。</p><p id="68e2" class="pw-post-body-paragraph lp lq it lr b ls lt kd lu lv lw kg lx ly lz ma mb mc md me mf mg mh mi mj mk im bi translated">[2]t .马蒂森(2015年12月19日)。<a class="ae lh" href="http://neuro.cs.ut.ee/demystifying-deep-reinforcement-learning/" rel="noopener ugc nofollow" target="_blank">《揭秘深度强化学习》</a>。<em class="on">神经外科</em>。计算神经科学实验室。检索时间2021年9月。</p></div></div>    
</body>
</html>