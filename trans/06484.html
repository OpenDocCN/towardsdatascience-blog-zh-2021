<html>
<head>
<title>Walking Off The Cliff With Off-Policy Reinforcement Learning</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用非策略强化学习走下悬崖</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/walking-off-the-cliff-with-off-policy-reinforcement-learning-7fdbcdfe31ff?source=collection_archive---------21-----------------------#2021-06-10">https://towardsdatascience.com/walking-off-the-cliff-with-off-policy-reinforcement-learning-7fdbcdfe31ff?source=collection_archive---------21-----------------------#2021-06-10</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4b39" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">政策外强化学习和政策内强化学习的深入比较</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/48bed341d624d3181c5cdd96d203f15e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*oKTiXkF848czhvs-"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一座悬崖，作为背景。艾伦·卡里略在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="527e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">为了学习强化学习(RL)的基础知识，萨顿和巴尔托[1]的教科书悬崖行走的例子是一个很好的开始，提供了Q-学习和SARSA的例子。你可能知道，SARSA是<strong class="li iu"> S </strong> tate、<strong class="li iu">A</strong>action、<strong class="li iu"> R </strong> eward、<strong class="li iu"> S </strong> tate、<strong class="li iu">A</strong>action的缩写——用来更新价值函数的轨迹。为了保持类比，Q-learning可以概括为<strong class="li iu"> S </strong> tate，<strong class="li iu">A</strong>action，<strong class="li iu"> R </strong> eward，<strong class="li iu"> S </strong> tate或者SARS(注意第二个动作无所谓！).不同之处在于，SARSA是<strong class="li iu"> on-policy </strong>而Q-learning是<strong class="li iu"> off-policy </strong>。这到底是什么意思？</p><h1 id="6514" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">操纵悬崖</h1><p id="fa6f" class="pw-post-body-paragraph lg lh it li b lj mu ju ll lm mv jx lo lp mw lr ls lt mx lv lw lx my lz ma mb im bi translated">在进入那个问题之前，我们先来定义一下我们的悬崖世界。我们勇敢的特工站在悬崖边，试图走最少的步数(允许的步数有<em class="mz">左</em>、<em class="mz">右</em>、<em class="mz">上</em>、<em class="mz">下</em>)到达悬崖的另一边。为了建立这个目标的模型，达到目标结束这一集并给予+10的奖励，每个中间步骤花费-1，掉下悬崖花费高达-100。不用说，应该避免走下悬崖。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/5f4d13cbe56bfec804201001f9a70090.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M79MspX4MckRX3wqKCgAxA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">悬崖行走环境，有奖励，起点和终点。步入悬崖或球门瓷砖结束了学习插曲。</p></figure><p id="0bda" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">一开始，我们完全不知道，但是我们通过观察很快就学会了。在几次从悬崖上滚下来或者在无人区里漫游之后，我们应该学会如何到达目的地。每走一步，我们观察一个奖励，并使用时间差(TD)更新方程。我们先来看看<strong class="li iu"> SARSA </strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nb"><img src="../Images/d4ba4e1b3b5f87a254bdbae1060769bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZSr8_pMUBaSu5A2jt_gskQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">SARSA:时间差分更新方程</p></figure><p id="c9ee" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">简单地说:我们将自己的预测与观察到的回报进行比较，并利用这种差异做出更好的预测。在简单的数学中:我们测量误差<code class="fe nc nd ne nf b">(r_t+1 + Q_t+1)-Q_t</code>，并用该误差更新我们对<code class="fe nc nd ne nf b">Q_t</code>的估计。注意，当<code class="fe nc nd ne nf b">Q_t=(r_t+1+Q_t+1)</code>适用于所有状态和动作时，我们的算法已经收敛。</p><p id="af12" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">让我们写出Q值代表什么，因为这对理解很重要。考虑一整集的回报轨迹。使用简化符号，我们可以将Q值定义如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/183a949661596c45713c2eb878374fbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1068/format:webp/1*gXFOwW4L4SgtE3Sug4HapQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">q值(没有状态、动作和折扣的简化表示)。请注意，每个Q值估计的是累积的未来回报。</p></figure><p id="8b0f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">当然，回报是不确定的，所以我们不能简单地将我们的Q值设定为等于单一的回报轨迹。这个想法很简单。Q值预测轨迹的值。当<code class="fe nc nd ne nf b">r_T=-100</code>由于坠入悬崖，所有的Q值都会受到严重影响。</p><p id="bc7e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">现在，<strong class="li iu"> Q-learning </strong>的工作方式与SARSA略有不同:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nh"><img src="../Images/e82094cf1c40316ac5ec008ad7e01aad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S3tsU3mRBB0wWlwOeIcH3w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Q-learning:时间差分更新方程</p></figure><p id="e686" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">你知道为什么Q-learning可能被称为SARS算法:对于我们的更新，当评估我们的行动<code class="fe nc nd ne nf b">a_t</code> — <code class="fe nc nd ne nf b"> r_t+1</code>时，我们在<code class="fe nc nd ne nf b">a_t+1</code>采取的实际行动无关紧要。也许<code class="fe nc nd ne nf b">a_t+1</code>在我们真实的样本轨迹中把我们引下悬崖；这无关紧要，因为我们取的是未来行动的<em class="mz">最大值</em>，而不是真实值。</p><h1 id="a46d" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">在探索中学习</h1><p id="eb1b" class="pw-post-body-paragraph lg lh it li b lj mu ju ll lm mv jx lo lp mw lr ls lt mx lv lw lx my lz ma mb im bi translated">在训练过程中，两种算法都选择每块瓷砖的最佳动作，<em class="mz">给定我们当前的信念</em>(“贪婪”动作选择):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ni"><img src="../Images/828e9b06637904e04947c59e5ed9c122.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dWOiUYspoMZIpdYYK4q3dA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">为当前Q值选择最佳行动的最大化问题</p></figure><p id="4922" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">特别是在早期，这些信念(Q值)可能是完全错误的——如果我们总是走最长的路，因为它曾经有效，我们永远不会知道有捷径。探索是获得更好政策的关键。</p><p id="2b6e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">ε-贪婪策略通常遵循贪婪方法，但有时也会做些别的事情。假设有5% ( <code class="fe nc nd ne nf b">ϵ=0.05</code>)的时间，我们采取随机行动。当站在边缘时，那个随机的动作可能只是告诉我们<em class="mz">‘向右走’</em>当我们真的，真的不应该的时候。对于我们的Q-learning代理和SARSA代理来说，这样的行动不会有好结果。</p><p id="47f4" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">站在离悬崖一瓦远的地方会变得更有趣。我们可以看到为什么我们稳健的Q-learning非常舒服地靠近边缘，而我们摇摆不定的SARSA朋友可能更喜欢保持安全的距离。对于奖励更新，Q-learning只是简单地看不到未来多一步；下一个图块的<em class="mz">实际</em>动作无关紧要。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nj"><img src="../Images/cecd4e3dc77ee70e5c2a9ecd8dce72b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5i_eooWI0skZxY2WJgt8dA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用SARSA更新时间差(按策略)。使用<strong class="bd nk">实际轨迹</strong>，在跌落悬崖时获得的大的负奖励被用于更新所有先前状态和动作的Q(s，a)值。[作者自己的作品]</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nl"><img src="../Images/7730bea28ef55933653e2445cb721cf3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wu23e-rTianWOYi7E58D6A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用Q学习的时间差异更新(偏离策略)。掉下悬崖时获得的巨大负回报不会影响早期的Q值，因为<strong class="bd nk">最优轨迹</strong>用于更新。[作者自己的作品]</p></figure><p id="1b00" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">结果呢？一个训练有素的Q-learning agent通常遵循最优路径，而SARSA agent则需要走一点弯路。例如，参见下面的模拟样本路径:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/dc63d50b706d0f5aa879c4131f2dcc72.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*kq1BAQh0cUl6OGfs3LYZKg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">学习完成后Q-learning和SARSA的示例路径。注意，SARSA绕过了悬崖，因为政策更新更重视跌入悬崖。</p></figure><h1 id="62ce" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">超越悬崖(政策上与政策外)</h1><p id="280e" class="pw-post-body-paragraph lg lh it li b lj mu ju ll lm mv jx lo lp mw lr ls lt mx lv lw lx my lz ma mb im bi translated">到目前为止还好，但是悬崖漫步是一个程式化的教科书例子。一只山羊可以弄明白这一点(它们实际上每天都在这么做)。对于实际的RL问题，我们可以用这些信息做什么呢？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/fded9feab86135b5a5c903c287d849c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Qj3zsnBvx2GzZpxxt2zavw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一只山羊。<a class="ae ky" href="https://unsplash.com/@saikturi?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">穆罕默德·努里·伊塞诺鲁</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="2858" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">你不太可能使用SARSA或Q-learning来解决更具挑战性的强化学习问题。在这两种情况下，我们都需要用Q值来填充表格。为了获得准确的查找表，我们必须多次观察每个状态-动作对，以了解所有的<code class="fe nc nd ne nf b">Q(s,a)</code>。悬崖行走问题只有192个状态-动作对(48*4)，但典型的RL问题太大了，无法在查找表中捕获。</p><p id="beeb" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">尽管没有使用实际的算法，策略学习和非策略学习的<em class="mz">原则</em>是相同的。简而言之:(I)使用实际回报轨迹来更新Q值的更新函数是符合政策的，(ii)包括最大值算子的更新函数可能不符合政策。无论我们使用什么样的探索方案，政策上/政策外的两难选择仍然是相关的——我们将在后面深入探讨。悬崖漫步问题仅仅阐明了权衡和行为。</p><h1 id="c8f1" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">行为策略还是目标策略？</h1><p id="eb6a" class="pw-post-body-paragraph lg lh it li b lj mu ju ll lm mv jx lo lp mw lr ls lt mx lv lw lx my lz ma mb im bi translated">为了充分理解政策上和政策外之间的困境，我们应该掌握行为政策和目标政策之间的区别。评估RL算法时，明确我们实际测量的性能非常重要。</p><p id="ec37" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">行为策略</strong>是包含随机探索的策略，<strong class="li iu">目标策略</strong>不包含随机探索。如果我们总是遵循我们学到的最佳路径(目标策略)，就没有理由绕过悬崖。如果我们在现实中部署行为策略，我们应该谨慎。通常，我们使用行为策略在安全的环境中学习——很可能是模拟——并在现实生活中部署最终的目标策略。然而，情况并非总是如此。</p><p id="33ae" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">举例来说:假设我们训练一个真实的机器人(而不是虚拟代理)安全地机动到悬崖的另一边。你不希望你昂贵的机器人在快乐探索的同时驶下悬崖。如果新的观察有现实生活中的后果，你在探索时显然会谨慎得多。还可以考虑用历史数据来测试你的交易算法，或者用真钱在股票市场上释放它来学习——这是非常不同的含义。</p><p id="665f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">仔细探索的一个更常见的原因是实验通常计算量很大。由于Q-learning具有不太一致的回报模式，轨迹比SARSA具有更大的方差，并且收敛更具挑战性。此外，嵌入式最大化问题<code class="fe nc nd ne nf b">max a` ∈ A(s_t)</code>可能需要大量的努力来解决——不是每个动作空间都只包含四个移动。</p><p id="47d5" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">探索很少是“免费的”，即使是在虚拟环境中。</p><h1 id="1417" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">正面交锋</h1><p id="8de5" class="pw-post-body-paragraph lg lh it li b lj mu ju ll lm mv jx lo lp mw lr ls lt mx lv lw lx my lz ma mb im bi translated">燃眉之急:哪种算法更好？你应该用哪一个？Q-learning还是SARSA？</p><p id="8a5c" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">从表面上看，Q-learning显然提供了一条更短的路径(通常是最优的)，从而提供了一个更好的解决方案，但这忽略了隐藏在下面的细微差别。下图显示了一个清晰的<strong class="li iu">权衡</strong>；平均来说，SARSA会得到更大的奖励，而Q-learning平均来说需要更少的步骤。当然，这只适用于<em class="mz">行为</em>策略——用Q-learning得到的<em class="mz">目标</em>策略显然更胜一筹。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi no"><img src="../Images/d0e73a3e6dbdfe0c1065c74f3483a9c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AQMGi9rHEX6WYCZ5nsanjA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><strong class="bd nk">左:</strong>奖励对比。平均而言，由于坠入悬崖的次数较少，SARSA会产生更高的回报。<strong class="bd nk">右:</strong>步骤对比。平均来说，Q-learning需要更少的步骤，因为它对跌落悬崖不太敏感。</p></figure><p id="7960" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">再说一次，我们不能简单地比较目标政策并得出Q-learning更好的结论。恰好对<code class="fe nc nd ne nf b">ϵ=0.05</code>成立，但是用<code class="fe nc nd ne nf b">ϵ=0.001</code>代替它，我们可以用两者获得最优路径。随着探索趋向于0，SARSA和Q-learning收敛到相同的方法。学习过程中探索通常会减少——例如通过设置<code class="fe nc nd ne nf b">ϵ=1/n</code>——并且表面上看起来Q-learning的好处也随之消失。</p><p id="e8c7" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu"><em class="mz"/></strong>“视情况而定”将是对“哪一个更好？”这个问题最忠实的回答，但不是最有用的一个。让我们试着推导出一些经验法则。</p><p id="7fa5" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">在下列情况下使用策略学习(SARSA):</strong></p><ul class=""><li id="2750" class="np nq it li b lj lk lm ln lp nr lt ns lx nt mb nu nv nw nx bi translated">学习是昂贵的</li><li id="71f6" class="np nq it li b lj ny lm nz lp oa lt ob lx oc mb nu nv nw nx bi translated">计算工作量是一个瓶颈</li><li id="76e0" class="np nq it li b lj ny lm nz lp oa lt ob lx oc mb nu nv nw nx bi translated">你不需要探索一切</li><li id="f4f9" class="np nq it li b lj ny lm nz lp oa lt ob lx oc mb nu nv nw nx bi translated">行动问题很难解决</li><li id="fbbd" class="np nq it li b lj ny lm nz lp oa lt ob lx oc mb nu nv nw nx bi translated">你不介意微调探索参数</li><li id="2e52" class="np nq it li b lj ny lm nz lp oa lt ob lx oc mb nu nv nw nx bi translated">你愿意为了稳健而牺牲一些回报</li><li id="90df" class="np nq it li b lj ny lm nz lp oa lt ob lx oc mb nu nv nw nx bi translated">学习时的奖励很重要</li><li id="1a05" class="np nq it li b lj ny lm nz lp oa lt ob lx oc mb nu nv nw nx bi translated">你厌恶风险</li></ul><p id="196b" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><strong class="li iu">在下列情况下使用偏离策略学习(Q-learning):</strong></p><ul class=""><li id="592e" class="np nq it li b lj lk lm ln lp nr lt ns lx nt mb nu nv nw nx bi translated">学习是廉价的</li><li id="1c2e" class="np nq it li b lj ny lm nz lp oa lt ob lx oc mb nu nv nw nx bi translated">计算工作量不太相关</li><li id="d887" class="np nq it li b lj ny lm nz lp oa lt ob lx oc mb nu nv nw nx bi translated">你想探索很多东西</li><li id="9b07" class="np nq it li b lj ny lm nz lp oa lt ob lx oc mb nu nv nw nx bi translated">行动问题很容易解决</li><li id="3247" class="np nq it li b lj ny lm nz lp oa lt ob lx oc mb nu nv nw nx bi translated">你不想麻烦微调探索</li><li id="c51d" class="np nq it li b lj ny lm nz lp oa lt ob lx oc mb nu nv nw nx bi translated">学习时的奖励并不重要</li><li id="72f4" class="np nq it li b lj ny lm nz lp oa lt ob lx oc mb nu nv nw nx bi translated">只有目标策略的性能才是重要的</li><li id="3139" class="np nq it li b lj ny lm nz lp oa lt ob lx oc mb nu nv nw nx bi translated">你喜欢冒险</li></ul><p id="118d" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">还是那句话，两者都能达到同样的解决质量，适当的调优是必不可少的，这取决于问题和目的，不存在<em class="mz">一刀切</em>等答案。等等。毕竟，它们是经验法则。确定您需要哪种方法的最佳方式？去站在悬崖边，感受一下这种感觉。</p></div><div class="ab cl kz la hx lb" role="separator"><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le lf"/><span class="lc bw bk ld le"/></div><div class="im in io ip iq"><p id="a57e" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated">想看完整的代码吗？在我的<a class="ae ky" href="https://github.com/woutervanheeswijk/cliff_walking_public" rel="noopener ugc nofollow" target="_blank"> GitHub库</a>上查看一下吧。</p><p id="a097" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><em class="mz">有兴趣用深度Q-learning解决悬崖行走问题？请查看我关于TensorFlow实现的文章:</em></p><div class="od oe gp gr of og"><a rel="noopener follow" target="_blank" href="/deep-q-learning-for-the-cliff-walking-problem-b54835409046"><div class="oh ab fo"><div class="oi ab oj cl cj ok"><h2 class="bd iu gy z fp ol fr fs om fu fw is bi translated">悬崖行走问题的深度Q学习</h2><div class="on l"><h3 class="bd b gy z fp ol fr fs om fu fw dk translated">一个完整的Python实现，用TensorFlow 2.0导航悬崖。</h3></div><div class="oo l"><p class="bd b dl z fp ol fr fs om fu fw dk translated">towardsdatascience.com</p></div></div><div class="op l"><div class="oq l or os ot op ou ks og"/></div></div></a></div><p id="a02f" class="pw-post-body-paragraph lg lh it li b lj lk ju ll lm ln jx lo lp lq lr ls lt lu lv lw lx ly lz ma mb im bi translated"><em class="mz">转而寻找政策梯度实施？</em></p><div class="od oe gp gr of og"><a rel="noopener follow" target="_blank" href="/cliff-walking-problem-with-the-discrete-policy-gradient-algorithm-59d1900d80d8"><div class="oh ab fo"><div class="oi ab oj cl cj ok"><h2 class="bd iu gy z fp ol fr fs om fu fw is bi translated">基于离散策略梯度算法的悬崖行走问题</h2><div class="on l"><h3 class="bd b gy z fp ol fr fs om fu fw dk translated">用Python实现了一个完整的增强算法。手动执行这些步骤来说明内部…</h3></div><div class="oo l"><p class="bd b dl z fp ol fr fs om fu fw dk translated">towardsdatascience.com</p></div></div><div class="op l"><div class="ov l or os ot op ou ks og"/></div></div></a></div><h2 id="fe82" class="ow md it bd me ox oy dn mi oz pa dp mm lp pb pc mo lt pd pe mq lx pf pg ms ph bi translated">参考</h2><p id="a30f" class="pw-post-body-paragraph lg lh it li b lj mu ju ll lm mv jx lo lp mw lr ls lt mx lv lw lx my lz ma mb im bi translated">[1]萨顿和巴尔托(2018年)。<em class="mz">强化学习:简介</em>。麻省理工出版社。</p></div></div>    
</body>
</html>