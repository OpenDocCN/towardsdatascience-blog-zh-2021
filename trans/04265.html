<html>
<head>
<title>Dimensionality reduction with Autoencoders versus PCA</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自动编码器与PCA的降维比较</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/dimensionality-reduction-with-autoencoders-versus-pca-f47666f80743?source=collection_archive---------5-----------------------#2021-04-11">https://towardsdatascience.com/dimensionality-reduction-with-autoencoders-versus-pca-f47666f80743?source=collection_archive---------5-----------------------#2021-04-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="72f9" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">神经网络可以像经典的主成分分析一样进行降维吗？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/9dba8ed3506c65b333b529cc83a67135.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ps6qJWchdjK8bjmNiWfxNA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PCA(左)和Autoencoder(右)降维示例。</p></figure><h1 id="0b1a" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">介绍</h1><p id="9263" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><strong class="ls iu">主成分分析</strong>是最流行的降维算法之一。主成分分析的工作原理是找出在相互正交的数据中占最大方差的轴。<strong class="ls iu"> i </strong> ᵗʰ轴称为<strong class="ls iu"> i </strong> ᵗʰ主分量(PC)。执行PCA的步骤是:</p><ul class=""><li id="9f1c" class="mm mn it ls b lt mo lw mp lz mq md mr mh ms ml mt mu mv mw bi translated">将数据标准化。</li><li id="c70f" class="mm mn it ls b lt mx lw my lz mz md na mh nb ml mt mu mv mw bi translated">从协方差矩阵或相关矩阵中获取特征向量和特征值，或者进行奇异值分解。</li></ul><p id="fa94" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">我们将使用sklearn的实现来执行PCA:它使用来自scipy ( <code class="fe nf ng nh ni b"><a class="ae nj" href="https://docs.scipy.org/doc/scipy/reference/linalg.html#module-scipy.linalg" rel="noopener ugc nofollow" target="_blank">scipy.linalg</a></code>)的奇异值分解(SVD)。SVD是2D矩阵a的因式分解，它可以写成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/bf99997b4acc2be18b4b544c9da69277.png" data-original-src="https://miro.medium.com/v2/resize:fit:180/format:webp/1*WrcAzAddG4wHpZT-Z0GoAg.png"/></div></figure><p id="be47" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">s是一个1D数组，包含了<strong class="ls iu"> A </strong>的奇异值；<strong class="ls iu"> U </strong>和<strong class="ls iu"> V^H </strong>为一元(<strong class="ls iu">u</strong>ᵗ<strong class="ls iu">u</strong>=<strong class="ls iu">uu</strong>ᵗ= I)。大多数PCA实现执行SVD以提高计算效率。</p><p id="a7f5" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated"><strong class="ls iu">另一方面，自动编码器</strong> ( <strong class="ls iu"> AE </strong>)是一种特殊的神经网络，它被训练成将其输入复制到其输出。首先，它将输入映射到一个降维的潜在空间，然后将潜在表示编码回输出。AE通过减少重建误差来学习压缩数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/86ff8cc3be377fd626121ab4664f61e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*ToiwQaoeVxEazk9H3YQ4Bw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">具有编译码器结构的自动编码器的简单结构。</p></figure><p id="b8ef" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">我们马上会看到如何实现和比较PCA和Autoencoder的结果。</p><p id="f3c8" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">我们将使用来自sklearn的<em class="nm"> make_classification </em>生成我们的数据，这也将为我们提供一些标签。我们将使用这些标签来比较我们的方法之间的聚类效率。将会有三个部分。</p><p id="e1b9" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">首先，我们将实现一个简单的欠完整线性自动编码器:也就是说，一个具有比输入维数低的单层的自动编码器。第二部分将介绍堆叠式线性自动编码器。在第三部分，我们将尝试修改激活函数，以解决更多的复杂性。</p><p id="9729" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">结果将与PCA进行图形比较，最后我们将尝试使用简单的随机森林分类算法和交叉验证来预测类别。</p><p id="2b24" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated"><strong class="ls iu">数据准备</strong></p><p id="b8da" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">下面是设置数据的所有代码:首先，我们导入必要的库，然后用scikit-learn的“make classification”方法生成数据。在50个特征中，我们将指定只有15个是信息性的，并且我们将约束我们的归约算法只挑选5个潜在变量。</p><p id="8d5e" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">一旦我们有了数据，我们就把它分成训练-验证-测试。我们将使用神经网络(NN)的验证数据，而测试集将用于检查最终的一些分类性能。</p><p id="7cff" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">最后，我们标准化我们的数据:注意，我们仅在训练数据集上拟合缩放器，然后转换验证和测试集。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">生成—拆分—数据标准化。</p></figure><p id="1d98" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">左侧的数据关联热图显示了一些相关的要素，但大多数并不相关。在右侧，我们绘制了特征的方差，从中我们可以推断出只有10–15个变量可以为我们的数据集提供信息。</p><div class="kj kk kl km gt ab cb"><figure class="np kn nq nr ns nt nu paragraph-image"><img src="../Images/a4c86f2962040fe53b24072ee6f4417b.png" data-original-src="https://miro.medium.com/v2/resize:fit:896/format:webp/1*POM6vzxLbu6Q0PErUzBQUQ.png"/></figure><figure class="np kn nv nr ns nt nu paragraph-image"><img src="../Images/9e95f261f1c6500930c711bef75581ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:954/format:webp/1*UBc1TN3FFfmbgd-1fE7dcQ.png"/><p class="ku kv gj gh gi kw kx bd b be z dk nw di nx ny translated">数据集的关联热图(左)。每个变量的方差(右)。</p></figure></div><h1 id="8ec9" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">欠完整线性自动编码器</h1><p id="69dd" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在本节中，我们将看到如果我们执行这些操作，数据会发生什么变化:</p><ol class=""><li id="6156" class="mm mn it ls b lt mo lw mp lz mq md mr mh ms ml nz mu mv mw bi translated">将数据压缩到5个潜在维度</li><li id="f7a4" class="mm mn it ls b lt mx lw my lz mz md na mh nb ml nz mu mv mw bi translated">使用线性激活函数</li><li id="3d71" class="mm mn it ls b lt mx lw my lz mz md na mh nb ml nz mu mv mw bi translated">使用“MSE”作为损耗度量</li></ol><p id="54b8" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">让我们来设置这个<strong class="ls iu"> AE </strong>:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">自动编码器</p></figure><p id="6171" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">首先，我们定义编码器模型:注意，输入形状被硬编码到数据集维度，并且潜在空间被固定为5维。解码器模型是对称的:在这种情况下，我们指定5(潜在维度)的输入形状，其输出将是原始空间维度。最终的模型“autoencoder”将耦合这两者。</p><p id="06d1" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">然后，我们使用随机梯度下降SGD优化器编译具有均方误差度量的自动编码器，固定学习率为0.1</p><p id="61ab" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">在拟合步骤中，我们简单地指定验证数据，并且如果验证损失没有改善，则使用早期停止回调来停止训练。<br/>最后，我们可以只使用模型的第一部分“<strong class="ls iu">encoder . predict(X _ tr _ STD)</strong>来计算编码。这个模型非常简单:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/036ceff442055926b5c4a5c28d9f874a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1198/format:webp/1*5X-SwpS6n6ZHJhH_xccSyQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">自动编码器的型号摘要。</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/e79b60b7f64269758a5527df7dfcba1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1238/format:webp/1*Xf13lL6HHDX1aZbKgulcww.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">AE的学习曲线。</p></figure><p id="e93c" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">从学习曲线中，我们看到拟合在60个时期后停止，损失约为0.7。产生的前两个编码如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/b3e71fe10044cb6b1aed4eee861da1cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:938/format:webp/1*gaJ68n53FY81KLmvaIWx9A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">编码器原始数据的前2个潜在维度。</p></figure><p id="41b4" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated"><strong class="ls iu"> PCA </strong>实现:使用sklearn很简单。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="181f" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">下面我们绘制了前3个组件的PCA(左图)和编码器(右图)的聚类训练数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi od"><img src="../Images/2e2c631ad563b4e1c4287576786a1811.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jYP2_IDtx-vuI_xqGru6Pw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PCA的组件0–1–2(左)和编码器(右)之间的聚类比较。</p></figure><p id="88e3" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">从上面的图来看，似乎两种算法都以相似的方式进行了降维。很难推断哪个表现更好。让我们来看看这些编码的一些性质。</p><p id="a3b5" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">下面我们绘制了PCA(左)和Autoencoder(右)组件的标准偏差。</p><div class="kj kk kl km gt ab cb"><figure class="np kn oe nr ns nt nu paragraph-image"><img src="../Images/ef220fa2618a8c7ec32528dd8d057c7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*14YgsSsfVyA_SjynQmMbrg.png"/></figure><figure class="np kn of nr ns nt nu paragraph-image"><img src="../Images/ffbb14d66f43391e8c0718592c27178d.png" data-original-src="https://miro.medium.com/v2/resize:fit:756/format:webp/1*lRVz4b63ABF6KllUVcpElg.png"/><p class="ku kv gj gh gi kw kx bd b be z dk og di oh ny translated">成分的标准偏差。</p></figure></div><p id="8be0" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">从这些柱状图中，我们可以看出，对于与PCA相关的数据，主成分的标准偏差随着每一个主成分的减少而减少，这是意料之中的。另一方面，所有编码的标准偏差几乎相等。</p><p id="e4a8" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">下面我们绘制了PCA(左)和编码器(右)组件的相关图。</p><div class="kj kk kl km gt ab cb"><figure class="np kn oi nr ns nt nu paragraph-image"><img src="../Images/8a4e8c4746156e39814c2ff97bba612a.png" data-original-src="https://miro.medium.com/v2/resize:fit:718/format:webp/1*q6EDYDhfkb-BBct6xDIqZg.png"/></figure><figure class="np kn oj nr ns nt nu paragraph-image"><img src="../Images/5087e3751401394aa884003724da4284.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*JoOdB7N_6fbRCcXFvRACdQ.png"/><p class="ku kv gj gh gi kw kx bd b be z dk ok di ol ny translated">PCA成分(左)和编码(右)的皮尔逊相关热图。</p></figure></div><p id="2f9f" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">从该图中我们还可以看到，虽然PC彼此正交:它们不相关，但对于编码来说却不是这样。它们之间显示出某种相关性。事实上，我们的神经网络模型对这种行为没有任何限制。网络找到了在潜在空间中映射原始输入的最佳方式。</p><p id="6863" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">从原始空间到两个不同潜在空间的分类性能如何？有区别吗？我们可以使用交叉验证来试验RandomForestClassifier，看看结果。代码如下:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/8a139ce71688ec492dda8de41aca16d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:962/format:webp/1*X8NkGqNkPlCy5hmm8GzS8A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">三个数据集的分类分数。</p></figure><p id="231a" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">从上面的柱状图中我们可以看到，除了5个以外，其他的都被移除了，这降低了预测的准确性。PCA和AE数据减少的性能是可比较的，然而AE的性能比PCA稍差。</p><p id="381f" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">现在，我们将尝试了解更复杂的AE叠加是否会导致更好的结果。</p><h1 id="5948" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">线性堆栈编码器-解码器</h1><p id="618a" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">这里，我们实现了一个编码器-解码器网络结构，但在这种情况下，我们将堆栈更多的隐藏层。这应该允许网络更灵活地学习不同的、最具信息性的特征。</p><p id="2dff" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">编码器层数为:50(输入)—20–15–5。损失与之前的不良事件相当。解码器正好对称。组件如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/d4021ef57860132e1264c9b20e98afb1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GKNwPuYNAF9RX9PPpwzoQw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">PCA(左)和AE(右)的前3个成分散点图。</p></figure><p id="7501" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">分类性能方面的结果是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/11af518d7872a1ada712cc55c8942ef1.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*1sT5VVKHp1vbVkuUmon1SA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">不同数据集之间的分类性能。</p></figure><p id="1af5" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">显然，叠加隐藏层比简单的声发射效果更好。然而，这些层的确切配置可以针对具体问题进行微调。</p><h1 id="20d4" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">非线性堆叠编码器-解码器</h1><p id="b6de" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">非线性堆叠AE将容易实现为具有激活功能的堆叠AE。我们还在SGD优化器上引入了一个衰减常数，这样学习率就会随着时间的推移而降低。我们选择“卢瑟”作为所有层的激活层。请注意，这里我们已经增加了更多的复杂性:我们可以尝试找到最佳数量的隐藏层，最佳的激活函数和形状的每一层的具体问题。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nn no l"/></div></figure><p id="7ee3" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">以下是前三个组成部分:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/b0ac9e02b952479112c391fab2de4de5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jzQWy9QekdeBaeIm2E2ZTw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">非线性激活的PCA(左)和AE(右)的前三个组件。</p></figure><p id="9a75" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">至于分类性能，允许网络学习非线性特征有助于提高整体性能，这似乎比PCA平均好，但在误差范围内。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/0d4d6b801479951fdd54780b2f6de845.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*Uhw1adF9Z6v9qnmOcJf2pQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">所有数据简化算法的分类性能。</p></figure><h1 id="98ae" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">结论</h1><p id="eac3" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">我们使用来自sklearn的make_classification构建了一个玩具数据集，用于分类练习，包含50个特征。我们的目的是比较PCA和自动编码器神经网络，看看维数减少是否具有可比性。</p><p id="a360" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">我们查看了分数/编码的属性，我们看到来自AE的编码具有一些相关性(协变矩阵不像PCA中那样是对角的)，并且它们的标准偏差是相似的。</p><p id="3a57" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">从只有1层的简单线性欠完整AE开始，我们看到增加的复杂性有助于模型达到更好的性能，根据分类准确性进行评估。</p><p id="d753" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">最后，我们看到，非线性模型仍然可以比其他两个模型(单层AE和堆叠AE)执行得更好，但性能仍然可以与该数据集中的PCA相媲美。</p><p id="8f49" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">我们现在可以回答最初的问题:神经网络可以像经典的主成分分析一样进行降维吗？</p><p id="a391" class="pw-post-body-paragraph lq lr it ls b lt mo ju lv lw mp jx ly lz nc mb mc md nd mf mg mh ne mj mk ml im bi translated">答案是肯定的，它可以像PCA一样执行维数减少，因为网络将找到在潜在空间中编码原始向量的最佳方式。但不，潜在向量的属性是不同的，网络本身可能需要针对特定的任务进行调整，以便更好地执行，就像我们对更多隐藏层所做的那样。</p></div></div>    
</body>
</html>