<html>
<head>
<title>Installing Hadoop on Windows 11 with WSL2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用 WSL2 在 Windows 11 上安装 Hadoop</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/installing-hadoop-on-windows-11-with-wsl2-f11a585e41cf?source=collection_archive---------37-----------------------#2021-11-01">https://towardsdatascience.com/installing-hadoop-on-windows-11-with-wsl2-f11a585e41cf?source=collection_archive---------37-----------------------#2021-11-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="55ea" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何在运行使用 WSL 1 或 2 的 Linux 发行版的 Windows 11 上安装和配置 Hadoop 及其组件。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/a357ef6237545b2eadfbb1792aa4f664.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*ml5DtCX-J_e6bfti.jpg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">来源:<a class="ae kv" href="https://unsplash.com/photos/oyXis2kALVg" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="d2fd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在之前的帖子中，我们看到了<a class="ae kv" href="https://blog.contactsunny.com/tech/installing-zsh-and-oh-my-zsh-on-windows-11-with-wsl2" rel="noopener ugc nofollow" target="_blank">如何使用 WSL2 </a>在 Windows 11 上安装 Linux 发行版，然后<a class="ae kv" href="https://blog.contactsunny.com/tech/installing-zsh-and-oh-my-zsh-on-windows-11-with-wsl2" rel="noopener ugc nofollow" target="_blank">如何安装 Zsh 和 on-my-zsh </a>使终端更加可定制。在这篇文章中，我们将看到如何使用 WSL 在同一台 Windows 11 机器上安装完整的 Hadoop 环境。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="18c2" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">安装依赖项</h1><p id="4f4d" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">为了让<a class="ae kv" href="https://blog.contactsunny.com/?s=hadoop" rel="noopener ugc nofollow" target="_blank"> Hadoop </a>工作，您需要安装两个重要的依赖项。这些不是可选的，除非你已经安装了它们。所以请确保安装了这些依赖项。</p><h2 id="3f2a" class="mw ma iq bd mb mx my dn mf mz na dp mj lf nb nc ml lj nd ne mn ln nf ng mp nh bi translated">安装 JDK</h2><p id="68b4" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">第一个依赖项是 java 开发工具包，或 JDK。建议搭配 Java 8，或者 Hadoop 的 Java 1.8。这是我的建议，因为我在使用新版 Java 时遇到了问题。但是你绝对可以给新版本一个机会。</p><p id="9fd0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，不管你安装甲骨文 JDK 或打开 JDK，或任何其他版本的 JDK。你只需要安装它。我使用以下命令在我已经安装在 Windows 11 上的 Debian Linux 上安装了 JDK 8:</p><pre class="kg kh ki kj gt ni nj nk nl aw nm bi"><span id="1b3e" class="mw ma iq nj b gy nn no l np nq">sudo apt install adoptopenjdk-8-hotspot</span></pre><p id="58e7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">要使这个包在 apt 库中可用，首先需要添加 PPA。为此，请运行以下命令:</p><pre class="kg kh ki kj gt ni nj nk nl aw nm bi"><span id="878c" class="mw ma iq nj b gy nn no l np nq">sudo add-apt-repository --yes <a class="ae kv" href="https://adoptopenjdk.jfrog.io/adoptopenjdk/deb/" rel="noopener ugc nofollow" target="_blank">https://adoptopenjdk.jfrog.io/adoptopenjdk/deb/</a></span></pre><p id="5cb2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一旦安装了 JDK，请确保使用环境变量名 JAVA_HOME 导出 JDK 的路径。导出命令如下所示:</p><pre class="kg kh ki kj gt ni nj nk nl aw nm bi"><span id="ec30" class="mw ma iq nj b gy nn no l np nq">export JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64/bin/java</span></pre><p id="7e6f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果您只是在终端中运行这个命令，那么该变量将只为当前会话导出。为了使其永久化，您必须将该命令添加到<em class="nr">中。zshrc </em>文件。</p><h2 id="e55e" class="mw ma iq bd mb mx my dn mf mz na dp mj lf nb nc ml lj nd ne mn ln nf ng mp nh bi translated">安装 OpenSSH</h2><p id="9550" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">下一个要安装的依赖项是 OpenSSH，这样 Hadoop 就可以通过 SSH 进入本地主机。这也是必要的依赖。如果没有 SSH 到 localhost，Hadoop 的大多数组件都无法工作。要安装 OpenSSH，请在终端中运行以下命令:</p><pre class="kg kh ki kj gt ni nj nk nl aw nm bi"><span id="3bc9" class="mw ma iq nj b gy nn no l np nq">sudo apt install openssh-server openssh-client -y</span></pre><p id="9cb5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一旦我们为 SSH 安装了服务器和客户机，我们就必须为认证生成密钥。为此，运行以下命令，并仔细阅读您将得到的说明:</p><pre class="kg kh ki kj gt ni nj nk nl aw nm bi"><span id="8457" class="mw ma iq nj b gy nn no l np nq">ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa</span></pre><p id="fb81" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">生成密钥后，您必须将它们复制到授权密钥列表中，这样您就不必每次登录机器时都输入密码。这一点尤其重要，因为这是 Hadoop 所期望的。至少我还没有看到改变这种行为的选项。因此，运行下面的命令来<em class="nr"> cat </em>我们刚刚创建的密钥文件的文件内容，然后将其复制到<em class="nr"> authorized_keys </em>文件中:</p><pre class="kg kh ki kj gt ni nj nk nl aw nm bi"><span id="a543" class="mw ma iq nj b gy nn no l np nq">cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span></pre><p id="f504" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，确保公钥文件具有正确的权限。这是因为，如果密钥文件的公共访问比所需的多，系统会认为该密钥可以被复制或篡改，这意味着该密钥是不安全的。这将使系统拒绝该密钥，并且不允许 SSH 登录。因此，运行以下命令来设置正确的权限:</p><pre class="kg kh ki kj gt ni nj nk nl aw nm bi"><span id="45f0" class="mw ma iq nj b gy nn no l np nq">chmod 0600 ~/.ssh/id_rsa.pub</span></pre><p id="178b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，启动 SSH 服务，这样我们就可以测试服务器是否工作正常。为此，运行以下命令:</p><pre class="kg kh ki kj gt ni nj nk nl aw nm bi"><span id="98ae" class="mw ma iq nj b gy nn no l np nq">sudo service ssh start</span></pre><p id="0ecc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果一切都如预期的那样，你就安全了。最后，运行以下命令以确保 SSH 按预期运行:</p><pre class="kg kh ki kj gt ni nj nk nl aw nm bi"><span id="0c20" class="mw ma iq nj b gy nn no l np nq">ssh localhost</span></pre><p id="4fdc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果一切正常，您应该会看到类似下面的截图:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/1709ebb0afe21e811647cb2f948e04c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RAkGXZJ1z4W7ahVp4vk30w.png"/></div></div></figure><p id="72f0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">太棒了。您可以通过按 CTRL + d 组合键退出到上一个会话。这就结束了依赖项安装阶段。现在让我们继续安装 Hadoop。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><h1 id="9ae7" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">安装 Hadoop</h1><h2 id="dc9b" class="mw ma iq bd mb mx my dn mf mz na dp mj lf nb nc ml lj nd ne mn ln nf ng mp nh bi translated">下载 Hadoop</h2><p id="d27a" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">安装 Hadoop 的第一步是实际下载它。截至本文撰写时，Hadoop 的最新版本是 3.3.1 版，你可以从<a class="ae kv" href="https://hadoop.apache.org/releases.html" rel="noopener ugc nofollow" target="_blank">这里</a>下载。你将从那里下载一个<em class="nr"> .tar.gz </em>文件。要解压缩该文件，请使用以下命令:</p><pre class="kg kh ki kj gt ni nj nk nl aw nm bi"><span id="e0f3" class="mw ma iq nj b gy nn no l np nq">tar xzf hadoop-3.3.1.tar.gz</span></pre><p id="998b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这将创建一个名为 hadoop-3.3.1 的目录，并将所有文件和目录放在该目录中。因为我们将在本地机器上安装 Hadoop，所以我们将进行单节点部署，这也称为伪分布式模式部署。</p><h2 id="bec7" class="mw ma iq bd mb mx my dn mf mz na dp mj lf nb nc ml lj nd ne mn ln nf ng mp nh bi translated">设置环境变量</h2><p id="ddb4" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">我们必须设置一些环境变量。最好的部分是，你只需要定制一个变量。其他的只是复制粘贴。无论如何，以下是我所说的变量:</p><pre class="kg kh ki kj gt ni nj nk nl aw nm bi"><span id="e858" class="mw ma iq nj b gy nn no l np nq">export HADOOP_HOME=/mnt/d/bigdata/hadoop-3.3.1 <br/>export HADOOP_INSTALL=$HADOOP_HOME <br/>export HADOOP_MAPRED_HOME=$HADOOP_HOME <br/>export HADOOP_COMMON_HOME=$HADOOP_HOME <br/>export HADOOP_HDFS_HOME=$HADOOP_HOME <br/>export YARN_HOME=$HADOOP_HOME <br/>export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native <br/>export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin <br/>export HADOOP_OPTS"-Djava.library.path=$HADOOP_HOME/lib/nativ"</span></pre><p id="f4bb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如您所见，您只需更改第一个环境变量<em class="nr"> HADOOP_HOME </em>的值。将其设置为反映放置 Hadoop 目录的路径。同样，将这些<em class="nr">导出</em>语句放在<em class="nr">中也是一个好主意。zshrc </em>文件，这样这些变量每次都会自动导出，而不是你必须这样做。一旦将它放入文件中，请确保您对其进行了源处理，以便它立即生效:</p><pre class="kg kh ki kj gt ni nj nk nl aw nm bi"><span id="aac4" class="mw ma iq nj b gy nn no l np nq">source ~/.zshrc</span></pre><h2 id="48e3" class="mw ma iq bd mb mx my dn mf mz na dp mj lf nb nc ml lj nd ne mn ln nf ng mp nh bi translated">配置 Hadoop</h2><p id="265a" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">接下来，我们必须编辑一些文件来更改各种 Hadoop 组件的配置。让我们从文件<em class="nr"> hadoop-env.sh </em>开始。运行以下命令在编辑器中打开文件:</p><pre class="kg kh ki kj gt ni nj nk nl aw nm bi"><span id="075a" class="mw ma iq nj b gy nn no l np nq">sudo vim $HADOOP_HOME/etc/hadoop/hadoop-env.sh</span></pre><p id="d8af" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，找到导出<code class="fe nt nu nv nj b"><strong class="ky ir"><em class="nr">$JAVA_HOME</em></strong></code>变量的那一行并取消注释。这里，您必须提供与之前安装 Java 时相同的路径。对我来说，是这样的:</p><pre class="kg kh ki kj gt ni nj nk nl aw nm bi"><span id="1041" class="mw ma iq nj b gy nn no l np nq">export JAVA_HOME=/usr/lib/jvm/adoptopenjdk-8-hotspot-amd64/bin/java</span></pre><p id="4ae0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">接下来，我们必须编辑<em class="nr"> core-site.xml </em>文件。这里我们必须提供 Hadoop 的临时目录，以及 Hadoop 文件系统的默认名称。使用以下命令在编辑器中打开文件:</p><pre class="kg kh ki kj gt ni nj nk nl aw nm bi"><span id="0354" class="mw ma iq nj b gy nn no l np nq">sudo vim $HADOOP_HOME/etc/hadoop/core-site.xml</span></pre><p id="8d5f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您将在这里找到一个空文件，其中包含一些注释和一个空的配置块。您可以删除所有内容，并用以下内容替换:</p><pre class="kg kh ki kj gt ni nj nk nl aw nm bi"><span id="ed5f" class="mw ma iq nj b gy nn no l np nq">&lt;configuration&gt;<br/>    &lt;property&gt;<br/>        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;<br/>        &lt;value&gt;/mnt/d/hdfs/tmp/&lt;/value&gt;<br/>    &lt;/property&gt;<br/>    &lt;property&gt;<br/>        &lt;name&gt;fs.default.name&lt;/name&gt;<br/>        &lt;value&gt;hdfs://127.0.0.1:9000&lt;/value&gt;<br/>    &lt;/property&gt;<br/>&lt;/configuration&gt;</span></pre><p id="d306" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请确保创建您在此配置的临时目录。接下来，我们必须编辑 HDFS 配置文件<em class="nr"> hdfs-site.xml </em>。为此，使用以下命令在编辑器中打开文件:</p><pre class="kg kh ki kj gt ni nj nk nl aw nm bi"><span id="a263" class="mw ma iq nj b gy nn no l np nq">sudo vim $HADOOP_HOME/etc/hadoop/hdfs-site.xml</span></pre><p id="ba4c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在此配置文件中，我们将设置 HDFS 数据节点目录、HDFS 名称节点目录和 HDFS 复制因子。这里，您应该再次获得一个包含空配置块的文件。替换为以下内容:</p><pre class="kg kh ki kj gt ni nj nk nl aw nm bi"><span id="70ad" class="mw ma iq nj b gy nn no l np nq">&lt;configuration&gt;<br/>  &lt;property&gt;<br/>      &lt;name&gt;dfs.data.dir&lt;/name&gt;<br/>      &lt;value&gt;/mnt/d/hdfs/namenode&lt;/value&gt;<br/>  &lt;/property&gt;<br/>  &lt;property&gt;<br/>      &lt;name&gt;dfs.data.dir&lt;/name&gt;<br/>      &lt;value&gt;/mnt/d/hdfs/datanode&lt;/value&gt;<br/>  &lt;/property&gt;<br/>  &lt;property&gt;<br/>      &lt;name&gt;dfs.replication&lt;/name&gt;<br/>      &lt;value&gt;1&lt;/value&gt;<br/>  &lt;/property&gt;<br/>&lt;/configuration&gt;</span></pre><p id="a860" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">同样，确保您创建了数据节点和名称节点目录。接下来，我们有 MapReduce 配置文件。要在编辑器中打开它，请运行以下命令:</p><pre class="kg kh ki kj gt ni nj nk nl aw nm bi"><span id="b480" class="mw ma iq nj b gy nn no l np nq">sudo vim $HADOOP_HOME/etc/hadoop/mapred-site.xml</span></pre><p id="756d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">您可以用以下内容替换配置块:</p><pre class="kg kh ki kj gt ni nj nk nl aw nm bi"><span id="b024" class="mw ma iq nj b gy nn no l np nq">&lt;configuration&gt; <br/>  &lt;property&gt; <br/>    &lt;name&gt;mapreduce.framework.name&lt;/name&gt; <br/>    &lt;value&gt;yarn&lt;/value&gt; <br/>  &lt;/property&gt; <br/>&lt;/configuration&gt;</span></pre><p id="3b03" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如您所见，这是一个指定 MapReduce 框架名称的简单配置。最后，我们有了 YARN 配置文件，<em class="nr"> yarn-site.xml </em>。使用以下命令在编辑器中打开文件:</p><pre class="kg kh ki kj gt ni nj nk nl aw nm bi"><span id="042b" class="mw ma iq nj b gy nn no l np nq">sudo vim $HADOOP_HOME/etc/hadoop/yarn-site.xml</span></pre><p id="9e97" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将以下配置添加到文件中:</p><pre class="kg kh ki kj gt ni nj nk nl aw nm bi"><span id="b099" class="mw ma iq nj b gy nn no l np nq">&lt;configuration&gt;<br/>  &lt;property&gt;<br/>    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;<br/>    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;<br/>  &lt;/property&gt;<br/>  &lt;property&gt;<br/>    &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;<br/>    &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;<br/>  &lt;/property&gt;<br/>  &lt;property&gt;<br/>    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;<br/>    &lt;value&gt;127.0.0.1&lt;/value&gt;<br/>  &lt;/property&gt;<br/>  &lt;property&gt;<br/>    &lt;name&gt;yarn.acl.enable&lt;/name&gt;<br/>    &lt;value&gt;0&lt;/value&gt;<br/>  &lt;/property&gt;<br/>  &lt;property&gt;<br/>    &lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt;   <br/>    &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PERPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME&lt;/value&gt;<br/>  &lt;/property&gt;<br/>&lt;/configuration&gt;</span></pre><p id="372a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种配置没有什么可改变的。最后，我们完成了 Hadoop 的配置。我们现在可以开始格式化名称节点并启动 Hadoop。</p><h2 id="9d94" class="mw ma iq bd mb mx my dn mf mz na dp mj lf nb nc ml lj nd ne mn ln nf ng mp nh bi translated">格式化 HDFS 名称节点</h2><p id="1241" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">在第一次启动 Hadoop 服务之前，首先格式化 HDFS 名称节点非常重要。显然，这确保了 name 节点中没有垃圾。一旦您开始更频繁地使用 HDFS，您就会意识到您格式化名称节点的频率比您想象的要高，至少在您的开发机器上是这样。无论如何，要格式化名称节点，请使用以下命令:</p><pre class="kg kh ki kj gt ni nj nk nl aw nm bi"><span id="4e3a" class="mw ma iq nj b gy nn no l np nq">hdfs namenode -format</span></pre><p id="6100" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一旦收到名称节点的关闭通知，格式化就完成了。</p><h2 id="9591" class="mw ma iq bd mb mx my dn mf mz na dp mj lf nb nc ml lj nd ne mn ln nf ng mp nh bi translated">启动所有 Hadoop</h2><p id="55c0" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">最后，我们正处于这项活动的最佳阶段，开始使用 Hadoop。现在，根据您实际想要使用的组件，有许多启动 Hadoop 的方法。例如，你可以只开始纱，或 HDFS 随着它，等等。对于这项活动，我们将开始一切。为此，Hadoop 发行版提供了一个方便的脚本。因为您之前已经导出了一些环境变量，所以您甚至不需要搜索那个脚本，它已经在您的路径中了。只需运行以下命令并等待它完成:</p><pre class="kg kh ki kj gt ni nj nk nl aw nm bi"><span id="3b65" class="mw ma iq nj b gy nn no l np nq">start-all.sh</span></pre><p id="218d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这将需要几秒钟的时间，因为脚本只是等待前 10 秒钟，如果您错误地启动了操作，它不会做任何事情来为您提供取消操作的选项。坚持一下，您应该会看到类似于以下屏幕截图的输出:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/e4080c83ed2d3ddcf12552e30b9a6be4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xrHnOibNdHKFoTs5NrAI1Q.png"/></div></div></figure><p id="70f7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这告诉我们 Hadoop 的所有组件都已启动并运行。为了确保这一点，如果您愿意，您可以运行<em class="nr"> jps </em>命令来获取所有正在运行的进程的列表。您至少应该看到以下服务:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/93402aca182ee2df8b9a64e5a1a74887.png" data-original-src="https://miro.medium.com/v2/resize:fit:418/format:webp/1*fjtcoE4EOSweWX-z-4IqWQ.png"/></div></figure><p id="e786" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">仅此而已。您现在在 Windows 11 PC 上运行 Hadoop，使用的是 WSL 1 或 2 上的 Linux 发行版。为了确保这一点，您可以使用以下简单的 HDFS 命令:</p><pre class="kg kh ki kj gt ni nj nk nl aw nm bi"><span id="d30f" class="mw ma iq nj b gy nn no l np nq">hdfs dfs -ls /</span></pre><p id="b77e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该命令将列出 HDFS 根目录下的所有文件和目录。如果这是一个全新的部署，你应该找不到太多。您将得到一个类似如下所示的列表:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/301886e55b600f24653382da83eb9b49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1224/format:webp/1*vzQCf4VfGgoqpbdHBRxR5Q.png"/></div></figure><p id="9a0a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">差不多就是这样。我们完了！</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><p id="328b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你喜欢你在这里看到的，或者在我的<a class="ae kv" href="https://blog.contactsunny.com" rel="noopener ugc nofollow" target="_blank">个人博客</a>和<a class="ae kv" href="https://dev.to/contactsunny" rel="noopener ugc nofollow" target="_blank"> Dev。要写博客</a>，并希望在未来看到更多这样有用的技术帖子，请考虑在<a class="ae kv" href="https://github.com/sponsors/contactsunny" rel="noopener ugc nofollow" target="_blank"> Github </a>上关注我。</p></div><div class="ab cl ls lt hu lu" role="separator"><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx ly"/><span class="lv bw bk lw lx"/></div><div class="ij ik il im in"><p id="9660" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="nr">最初发表于 2021 年 11 月 1 日 https://blog.contactsunny.com</em><em class="nr">的</em> <a class="ae kv" href="https://blog.contactsunny.com/data-science/installing-hadoop-on-windows-11-with-wsl2" rel="noopener ugc nofollow" target="_blank"> <em class="nr">。</em></a></p></div></div>    
</body>
</html>