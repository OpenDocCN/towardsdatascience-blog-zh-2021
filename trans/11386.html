<html>
<head>
<title>How Feature Selection can boost the performance of high-dimensional data</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">特征选择如何提升高维数据的性能</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-feature-selection-can-boost-the-performance-of-high-dimensional-data-720f617bb2f7?source=collection_archive---------31-----------------------#2021-11-08">https://towardsdatascience.com/how-feature-selection-can-boost-the-performance-of-high-dimensional-data-720f617bb2f7?source=collection_archive---------31-----------------------#2021-11-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="30b0" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">选择最佳性能特征的降维基本指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6d3a97a153860ab4cb0d0bb26c28b597.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iB3UdbsN6M6vTzor4Ki2Qw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由<a class="ae ky" href="https://pixabay.com/users/geralt-9301/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=2692575" rel="noopener ugc nofollow" target="_blank">格尔德·奥特曼</a>来自<a class="ae ky" href="https://pixabay.com/?utm_source=link-attribution&amp;amp;utm_medium=referral&amp;amp;utm_campaign=image&amp;amp;utm_content=2692575" rel="noopener ugc nofollow" target="_blank">皮克斯拜</a></p></figure><p id="5024" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">特征选择是端到端数据科学模型开发管道的重要组成部分。数据科学家需要特别关注用于训练健壮的数据科学模型的功能集。数据越多，模型越健壮，但这适用于实例的数量，而不适用于特征的数量。</p><p id="f639" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">原始数据集包含许多需要从训练数据集中排除的冗余要素。对文本特征进行矢量化后，模型的维数增加了很多。处理高维数据集会导致维数灾难的问题。有多种技术可以降低数据集的维度，或者只选择对给定问题陈述表现最佳的最佳特征集。</p><blockquote class="lv lw lx"><p id="95c7" class="kz la ly lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated">在我之前的一篇文章中，我讨论了7种选择最佳性能特性的技巧。</p></blockquote><div class="mc md gp gr me mf"><a rel="noopener follow" target="_blank" href="/top-7-feature-selection-techniques-in-machine-learning-94e08730cd09"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">机器学习中的7大特征选择技术</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">选择最佳功能的流行策略</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">towardsdatascience.com</p></div></div><div class="mo l"><div class="mp l mq mr ms mo mt ks mf"/></div></div></a></div><p id="b631" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们将讨论如何在一行Python代码中使用统计技术来选择性能最佳的特性。</p><h1 id="6e5f" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">想法:</h1><p id="9f1b" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">Scikit-learn带有一个<code class="fe nr ns nt nu b"><strong class="lb iu">feature_selection</strong></code>模块，提供各种功能来执行特性选择。一些特征选择功能是:</p><ul class=""><li id="68be" class="nv nw it lb b lc ld lf lg li nx lm ny lq nz lu oa ob oc od bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html#sklearn.feature_selection.SelectFromModel" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"><em class="ly">【SelectFromModel】</em></strong></a>:基于权重重要性选择特征的元变换器。</li><li id="132e" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu oa ob oc od bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"><em class="ly">Select kbest</em></strong></a>:根据评分函数的k个最高分选择特征。</li><li id="ec7c" class="nv nw it lb b lc oe lf of li og lm oh lq oi lu oa ob oc od bi translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html" rel="noopener ugc nofollow" target="_blank"><strong class="lb iu"><em class="ly">Select percentile</em></strong></a>:根据得分函数中最高分的百分位数选择特征。</li></ul><p id="5f9a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">还有很多。</p><p id="6653" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe nr ns nt nu b"><strong class="lb iu">SelectKBest</strong></code>和<code class="fe nr ns nt nu b"><strong class="lb iu">SelectPercentile</strong></code> <strong class="lb iu"> </strong>选择重要特征的原理是一样的，但是<code class="fe nr ns nt nu b"><strong class="lb iu">SelectKBest</strong></code> <strong class="lb iu"> </strong>取输入参数，如顶部特征的数量来选择，而<code class="fe nr ns nt nu b"><strong class="lb iu">SelectPercentile</strong></code>取特征的百分位数来保留。</p><h1 id="4ef3" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">用法:</h1><p id="730e" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated"><code class="fe nr ns nt nu b"><strong class="lb iu">SelectKBest</strong></code>和<code class="fe nr ns nt nu b"><strong class="lb iu">SelectPercentile</strong></code> <strong class="lb iu"> </strong>根据要保留的特征的数量或百分比选择性能最佳的特征。使用诸如卡方或任何其他统计测试的评分函数来计算表现最佳的特征。</p><p id="af9b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">卡方函数是一种统计技术，用于计算具有非负特征值的独立要素与目标类之间的卡方得分。现在可以使用<code class="fe nr ns nt nu b"><strong class="lb iu">SelectKBest</strong></code>和<code class="fe nr ns nt nu b"><strong class="lb iu">SelectPercentile</strong></code> <strong class="lb iu"> </strong>功能使用卡方得分来选择顶级特征。</p><p id="51f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我将使用一个带有二进制目标类的样本文本数据集。使用<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html" rel="noopener ugc nofollow" target="_blank"> Tf-Idf矢量器</a>将文本特征矢量化成具有21，156个特征的稀疏矩阵。</p><p id="2c65" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以使用<code class="fe nr ns nt nu b"><strong class="lb iu">SelectKBest</strong></code>和<code class="fe nr ns nt nu b"><strong class="lb iu">SelectPercentile</strong></code> <strong class="lb iu"> </strong>函数降低稀疏矩阵的维数，使用<code class="fe nr ns nt nu b"><strong class="lb iu">chi2</strong></code>作为评分函数。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(作者代码)</p></figure><p id="ec6b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用百分位参数为10的<code class="fe nr ns nt nu b"><strong class="lb iu">SelectPercentile</strong></code> <strong class="lb iu"> </strong>函数拟合具有21，156个特征的Tf-Idf矢量化数据后，特征的数量减少到2，115个特征。</p><p id="d1a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在的问题是，如果我们使用上面讨论的技术降低数据的维度，我们是否必须对模型的性能做出妥协。我们来比较一下同一个机器学习模型在不同百分位值(特征选择后的特征数)下的性能。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/aad80fab3285045b456b5629d03c944f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nEsBfRQcApv0fcUV9J7AcA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(图片由作者提供)，在具有不同数量的特征的特征选择管道之后，数据集的模型性能图。</p></figure><p id="c446" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上面的图中，我们可以观察到，对于7，500个特征之后的数量，我们得到了几乎相似的性能指标(AUC-ROC得分、精确度和召回率)。对于近5到10个减少的特征，我们获得了高精度和AUC-ROC分数(在图中的黑框中突出显示)。</p><h1 id="4c53" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">使用管道自动选择特征:</h1><p id="584a" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">人们可以在端到端的模型开发管道中实现特征选择组件，以自动化特征选择工作流。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oj ok l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">(作者代码)</p></figure><h1 id="2d4a" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">结论:</h1><p id="1404" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">特性选择是scikit-learn包中的一个方便的函数，它可以在一行Python代码中执行特性选择。也可以改变评分函数(chi2测试)作为<code class="fe nr ns nt nu b"><strong class="lb iu">SelectKBest</strong></code>和<code class="fe nr ns nt nu b"><strong class="lb iu">SelectPercentile</strong></code> <strong class="lb iu"> </strong>函数的参数。</p><p id="a2e3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">可以对scikit-learn管道实现特性选择功能，以自动化特性选择工作流程。</p><h1 id="755a" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">参考资料:</h1><p id="6832" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">[1] Scikit-learn文档:<a class="ae ky" href="https://scikit-learn.org/stable/modules/feature_selection.html" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/feature _ selection . html</a></p></div><div class="ab cl om on hx oo" role="separator"><span class="op bw bk oq or os"/><span class="op bw bk oq or os"/><span class="op bw bk oq or"/></div><div class="im in io ip iq"><p id="175b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ly">喜欢这篇文章吗？成为</em> <a class="ae ky" href="https://satyam-kumar.medium.com/membership" rel="noopener"> <em class="ly">中等会员</em> </a> <em class="ly">继续无限制学习。如果你使用下面的链接，我会收到你的一小部分会员费，不需要你额外付费。</em></p><div class="mc md gp gr me mf"><a href="https://satyam-kumar.medium.com/membership" rel="noopener follow" target="_blank"><div class="mg ab fo"><div class="mh ab mi cl cj mj"><h2 class="bd iu gy z fp mk fr fs ml fu fw is bi translated">加入我的推荐链接-萨蒂扬库马尔媒体</h2><div class="mm l"><h3 class="bd b gy z fp mk fr fs ml fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="mn l"><p class="bd b dl z fp mk fr fs ml fu fw dk translated">satyam-kumar.medium.com</p></div></div><div class="mo l"><div class="ot l mq mr ms mo mt ks mf"/></div></div></a></div><blockquote class="ou"><p id="ff5b" class="ov ow it bd ox oy oz pa pb pc pd lu dk translated">感谢您的阅读</p></blockquote></div></div>    
</body>
</html>