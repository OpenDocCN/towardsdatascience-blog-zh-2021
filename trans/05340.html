<html>
<head>
<title>Introduction to Modelling Tabular Data: Predicting a student’s chance of gaining admission using ML</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">表格数据建模简介:用最大似然法预测学生被录取的机会</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/introduction-to-modelling-tabular-data-predicting-a-students-chance-of-gaining-admission-using-ml-3a440f709c71?source=collection_archive---------19-----------------------#2021-05-11">https://towardsdatascience.com/introduction-to-modelling-tabular-data-predicting-a-students-chance-of-gaining-admission-using-ml-3a440f709c71?source=collection_archive---------19-----------------------#2021-05-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="0b5c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">我们如何使用回归模型、随机森林和神经网络来预测学生被研究生院录取的机会？</h2></div><h1 id="fe3e" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated"><strong class="ak">简介</strong></h1><p id="e464" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">这种分析的目的是探索学生进入研究生院的最重要因素，并选择最准确的模型来预测学生进入研究生院的机会。</p><p id="f9ee" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">我将使用的数据是<a class="ae ly" href="https://www.kaggle.com/mohansacharya/graduate-admissions" rel="noopener ugc nofollow" target="_blank">大学毕业生录取2 </a>数据集，它可以在Kaggle上找到，灵感来自加州大学洛杉矶分校的录取数据集。</p><p id="06ef" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated"><strong class="kz ir">导入所需的库</strong></p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="2fe8" class="mi kg iq me b gy mj mk l ml mm">#import required libraries<br/>from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype<br/>from fastai.tabular.all import *<br/>from sklearn.ensemble import RandomForestRegressor<br/>from sklearn.tree import DecisionTreeRegressor<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import mean_squared_error<br/>from IPython.display import Image, display_svg, SVG<br/>import pandas as pd<br/>import seaborn as sns<br/>import matplotlib.pyplot as plt<br/>import statsmodels.api as sm<br/>import numpy as np</span></pre><h1 id="86de" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated"><strong class="ak">探索性数据分析</strong></h1><p id="fd77" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我们首先将csv文件加载到Pandas DataFrame中，并删除不需要的列。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="64c7" class="mi kg iq me b gy mj mk l ml mm">#load csv into Pandas dataframes<br/>data_df = pd.read_csv('../input/graduate-admissions/Admission_Predict_Ver1.1.csv')</span><span id="5d3a" class="mi kg iq me b gy mn mk l ml mm">#drop the serial no. of the students as we dont need it <br/>data_df.drop('Serial No.', axis = 1, inplace = True)</span></pre><p id="3510" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">然后，我们将执行EDA，以更好地理解我们的数据，并观察数据中的任何模式。</p><p id="f54a" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">首先，我们可以绘制一个热图来可视化变量之间的相关性。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="8317" class="mi kg iq me b gy mj mk l ml mm">corr = data_df.corr()</span><span id="ac52" class="mi kg iq me b gy mn mk l ml mm">#plot correlation matrix heatmap<br/>hm = sns.heatmap(data = corr, annot = True, cmap = sns.color_palette("flare", as_cmap=True)<br/>)</span></pre><figure class="lz ma mb mc gt mp gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/d5c7cf35f3027472c50b9a470c0a8320.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*qM-vIbFhuoAmgELCKs697w.png"/></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">相关矩阵热图。图片作者。</p></figure><p id="8a0d" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">从相关矩阵热图来看，似乎所有的变量都与录取机会正相关，CGPA、GRE分数和TOEFL分数是与录取机会最相关的变量。</p><p id="e273" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">为了进一步形象化我们的数据，我们绘制了入学机会对变量的散点图。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="ab04" class="mi kg iq me b gy mj mk l ml mm">#plot scatter plots of Chance of Admission to each of the variables<br/>column_names = list(data_df.columns)<br/>column_names.pop(-1)</span><span id="4b50" class="mi kg iq me b gy mn mk l ml mm">fig = plt.figure(figsize = (20,10))<br/>fig.subplots_adjust(hspace=0.4, wspace=0.4)<br/>for i in range(0, len(column_names)):<br/>    ax = fig.add_subplot(2, 4,i+1)<br/>    sns.scatterplot(x = data_df[column_names[i]], y = data_df['Chance of Admit '], hue = data_df[column_names[i]] )</span></pre><figure class="lz ma mb mc gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi mw"><img src="../Images/cafd4b044ffe92574b011064964a9038.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1nZWI-k0HmoH2mLQNigOLw.png"/></div></div></figure><p id="aa25" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">从上面的图来看，CGPA，TOEFL成绩，GRE成绩都显得与录取几率呈线性关系。大学排名较高的学生似乎也有更高的录取机会。具有更高目标陈述(SOP)和推荐信强度(LOR)的人似乎也有更高的录取机会。此外，有研究经验的学生往往有更高的录取机会。</p><h1 id="c8e1" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated"><strong class="ak">造型</strong></h1><p id="ff52" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">我们将拟合3种不同的模型，即多元线性回归模型、随机森林和神经网络。我们还将尝试集成模型以产生更精确的模型。</p><p id="475d" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">我们将使用均方差来评估我们的数据。较低的均方误差表示模型更精确。</p><p id="2a68" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">然后，我们在拟合我们的机器学习模型之前分割我们的数据集。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="bafa" class="mi kg iq me b gy mj mk l ml mm">#split our data into train and test data<br/>Y = data_df['Chance of Admit ']<br/>X = data_df.drop(columns = {'Chance of Admit '})</span><span id="2a90" class="mi kg iq me b gy mn mk l ml mm">X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)</span></pre><p id="f6f6" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated"><strong class="kz ir">线性回归</strong></p><p id="d6d9" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">我们将使用statsmodel库将OLS线性回归模型拟合到我们的数据中。与sklearn库相比，我更喜欢使用statsmodel库进行回归建模，因为stats model提供了关于所用模型的更多见解，但是使用任何一个库都会产生相同的结果。</p><p id="f2a2" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">我们将拟合以下没有交互项的模型:</p><p id="284e" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">录取几率= beta_0 + beta_1 * GRE成绩+ beta_2 *托福成绩+ beta_3 *大学评级+beta _ 4 * SOP+beta _ 5 * LOR+beta _ 6 * CGPA+beta _ 7 *研</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="eb64" class="mi kg iq me b gy mj mk l ml mm">#fit a multiple linear regression model using statsmodels</span><span id="dd6b" class="mi kg iq me b gy mn mk l ml mm">#add a constant<br/>X1_train = sm.add_constant(X_train)<br/>X1_test = sm.add_constant(X_test)</span><span id="8a19" class="mi kg iq me b gy mn mk l ml mm">#fit model<br/>model = sm.OLS(Y_train, X1_train).fit()<br/>predictions = model.predict(X1_test)</span><span id="0ff5" class="mi kg iq me b gy mn mk l ml mm">print_model = model.summary()<br/>print(print_model)</span><span id="bc6a" class="mi kg iq me b gy mn mk l ml mm">#i used statsmodels as it provides additional insights about the model but both would work perfectly fine<br/>#below is the code for fitting with sklearn<br/>"""<br/>from sklearn.linear_model import LinearRegression</span><span id="050e" class="mi kg iq me b gy mn mk l ml mm"># with sklearn<br/>regr = linear_model.LinearRegression()<br/>regr.fit(X_train, Y_train)</span><span id="29ad" class="mi kg iq me b gy mn mk l ml mm">"""</span></pre><p id="8869" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">上面的代码给出了以下输出:</p><figure class="lz ma mb mc gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi nb"><img src="../Images/7dbec64c8ecf11f5e3395fef1a36a08f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4bS9DrReKKU0jeSJKMDllw.png"/></div></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">statsmodels输出。图片来自作者。</p></figure><p id="521a" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">从上面的输出中，我们可以看到模型的R平方为0.821，表明与数据非常吻合。我们还可以观察到，大学评级系数和SOP与p值不显著，分别为0.541和0.721。该模型在测试集上给出了0.003705 的<strong class="kz ir">均方误差。移除不显著变量会导致稍微高一点的均方差<strong class="kz ir"> 0.003773 </strong>以及R平方值稍微下降。因此，我们将使用完整的模型。</strong></p><p id="3d6d" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">请注意，在实践中，如果准确性略有下降，我们通常会选择更简单的模型，因为更简单的模型通常更容易解释和推广。</p><p id="c3b6" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated"><strong class="kz ir">随机森林</strong></p><p id="69de" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">我们将使用sklearn库来使随机森林模型符合我们的数据。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="08c2" class="mi kg iq me b gy mj mk l ml mm">forest = RandomForestRegressor(n_estimators=1100,max_depth =6 ,max_features = 0.5)<br/>forest.fit(X_train, Y_train)<br/>predictions_rf = forest.predict(X_test)</span><span id="1f0a" class="mi kg iq me b gy mn mk l ml mm">#calculate the mean squared error <br/>mean_squared_error(predictions_rf, Y_test)</span></pre><p id="68ce" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">在拟合我们的随机森林模型后，我们不仅想知道我们的模型有多精确，还想知道我们的模型是如何进行预测的。为此，<em class="nc">特性重要性</em>让我们深入了解这一点。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="e0c8" class="mi kg iq me b gy mj mk l ml mm">def rf_feat_importance(m, df):<br/>    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}<br/>                       ).sort_values('imp', ascending=False)</span><span id="ef12" class="mi kg iq me b gy mn mk l ml mm">#check most important features<br/>fi = rf_feat_importance(forest, X_train)</span><span id="4d86" class="mi kg iq me b gy mn mk l ml mm">def plot_fi(fi):<br/>    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)</span><span id="9e1f" class="mi kg iq me b gy mn mk l ml mm">plot_fi(fi);</span></pre><figure class="lz ma mb mc gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi nd"><img src="../Images/800a71881e95a1771dfb58526656a5c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ez37KtEiCqBjOKbBK2PaJQ.png"/></div></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">特征重要性图。作者图片</p></figure><p id="4077" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">从图中，我们可以看到像CGPA，GRE分数和TOEFL分数这样的特征对模型来说是最重要的，而像Research和LOR这样的特征对模型来说是不太重要的。因为没有一个特征具有非常低的特征重要性值，所以我们可以将这些特征保留在模型中。</p><p id="debb" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">随机森林模型在测试集上给出的均方差为<strong class="kz ir"> 0.003922 </strong></p><p id="534e" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated"><strong class="kz ir">神经网络</strong></p><p id="f1d9" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">我们将使用构建在PyTorch之上的强大的FastAI库来为我们的数据拟合神经网络。为此，我们必须对数据进行预处理，并创建数据加载器将数据输入神经网络。为了处理连续变量和分类变量，我们可以使用fastai的函数cont_cat_split()，它会自动拆分连续变量和分类变量。请注意，该函数将“SOP”误认为是一个连续变量，因为它有0.5个值，但实际上它是分类变量。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="cfec" class="mi kg iq me b gy mj mk l ml mm">train_df = X_train<br/>test_df = X_test<br/>train_df = train_df.join(Y_train)<br/>test_df = test_df.join(Y_test)</span><span id="2739" class="mi kg iq me b gy mn mk l ml mm">#split the variables into continuous variables and categorical variables using fastai's factory method<br/>cont_nn,cat_nn = cont_cat_split(train_df, dep_var='Chance of Admit ')</span></pre><p id="0ee6" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">然后，我们可以创建数据加载器。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="07a9" class="mi kg iq me b gy mj mk l ml mm">#create dataloaders<br/>procs_nn = [Categorify, FillMissing, Normalize]<br/>to_nn = TabularPandas(train_df, procs_nn, cat_nn, cont_nn, y_names='Chance of Admit ',splits = RandomSplitter(valid_pct=0.4)(range_of(train_df))<br/>)<br/>dls = to_nn.dataloaders()</span></pre><p id="2880" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">在创建数据加载器之后，我们可以在FastAI中创建一个表格学习器。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="4017" class="mi kg iq me b gy mj mk l ml mm">#create tabular learner<br/>learn = tabular_learner(dls, y_range=(0.3,1),n_out=1, loss_func=F.mse_loss)</span></pre><p id="93f2" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">然后，我们可以使用fit_one_cycle对我们的模型进行15个时期的训练，看看它看起来如何:</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="9b5d" class="mi kg iq me b gy mj mk l ml mm">learn.lr_find()</span></pre><figure class="lz ma mb mc gt mp gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi ne"><img src="../Images/f75483f6d6b0109416cad6eb150a46df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fTOAaBmXo4llJg5tdPmkfg.png"/></div></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">学习率查找器。图片作者。</p></figure><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="9b97" class="mi kg iq me b gy mj mk l ml mm">learn.fit_one_cycle(15, 1e-2)</span></pre><figure class="lz ma mb mc gt mp gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/c56c191504e323c2ba8a0f5953f62b59.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*rBK2fKNjVMes0yyv6uQ3zQ.png"/></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">图片作者。</p></figure><p id="4b4e" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">神经网络在测试集上给出的均方差为<strong class="kz ir"> 0.003744 </strong>。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="9832" class="mi kg iq me b gy mj mk l ml mm">test_df.drop(['Chance of Admit '], axis=1, inplace=True)<br/>dl = learn.dls.test_dl(test_df)</span><span id="5010" class="mi kg iq me b gy mn mk l ml mm">preds = learn.get_preds(dl=dl)</span><span id="d5af" class="mi kg iq me b gy mn mk l ml mm">#convert the predictions into a list<br/>y = []<br/>for i in range(0,len(Y_test)):<br/>    x = preds[0][i].tolist()<br/>    y += x<br/>    <br/>mean_squared_error(y,Y_test)</span></pre><h1 id="d3ae" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated"><strong class="ak">组装</strong></h1><p id="fa7a" class="pw-post-body-paragraph kx ky iq kz b la lb jr lc ld le ju lf lg lh li lj lk ll lm ln lo lp lq lr ls ij bi translated">作为线性回归模型，随机森林和神经网络都有各自的优缺点。我们可以做的事情是把三个世界结合起来，试图产生一个更好的结果。有许多方法可以做到这一点，但我们将只使用一种简单的方法，即取模型预测的平均值。</p><pre class="lz ma mb mc gt md me mf mg aw mh bi"><span id="1654" class="mi kg iq me b gy mj mk l ml mm">ens_preds = (predictions + predictions_rf + y) /3<br/>mean_squared_error(ens_preds, Y_test)</span></pre><p id="c585" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">通过组合模型，我们在测试集上获得了0.003607 的均方误差，这优于所有3个模型本身！</p><h1 id="49ad" class="kf kg iq bd kh ki kj kk kl km kn ko kp jw kq jx kr jz ks ka kt kc ku kd kv kw bi translated"><strong class="ak">结论</strong></h1><figure class="lz ma mb mc gt mp gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/cb575a0feaa44490613ee78cd29cf781.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*J7zwY0RsCrOO1g6umiXMLA.png"/></div><p class="ms mt gj gh gi mu mv bd b be z dk translated">汇总表。图片作者。</p></figure><p id="6170" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">在这种情况下，与神经网络和随机森林相比，线性回归似乎表现得最好，这证明复杂的模型并不总是产生更好的结果。然而，与单独的3个模型相比，组合模型产生了更好的结果。</p><p id="1591" class="pw-post-body-paragraph kx ky iq kz b la lt jr lc ld lu ju lf lg lv li lj lk lw lm ln lo lx lq lr ls ij bi translated">代码可以在我的<a class="ae ly" href="https://github.com/jq28/graduate_analysis" rel="noopener ugc nofollow" target="_blank"> Github </a>中找到</p></div></div>    
</body>
</html>