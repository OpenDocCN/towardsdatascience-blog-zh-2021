<html>
<head>
<title>VAEs: Indirect Sampling from Latent Image Distribution</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">VAEs:从潜像分布中间接取样</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/these-are-not-real-clothes-af58154a98c2?source=collection_archive---------28-----------------------#2021-10-11">https://towardsdatascience.com/these-are-not-real-clothes-af58154a98c2?source=collection_archive---------28-----------------------#2021-10-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="9050" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">机器学习应用程序</h2><div class=""/><div class=""><h2 id="3e3b" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">用可变自动编码器生成图像数据</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/05e5075e9329b23b61d265ccc623f34e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/format:webp/1*a0lN53jFTfpkRzVrF7ITYg.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</p></figure><h1 id="4159" class="la lb iq bd lc ld le lf lg lh li lj lk kf ll kg lm ki ln kj lo kl lp km lq lr bi translated">介绍</h1><p id="f20b" class="pw-post-body-paragraph ls lt iq lu b lv lw ka lx ly lz kd ma mb mc md me mf mg mh mi mj mk ml mm mn ij bi translated">可变自动编码器(VAEs)是一种令人惊叹的机器学习架构。它们允许我们近似高维潜在空间，可以对其进行采样以生成新数据。AEs是机器学习中一个非常新颖的模型分支。这些有时候一开始很难理解。通常，神经网络的输出是我们感兴趣的，然而，在AEs中，输出是重新创建输入的尝试。</p><p id="b8b5" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">在本文中，我将对AEs和VAEs进行概述，然后展示一个使用python的应用程序。</p><h1 id="eb62" class="la lb iq bd lc ld le lf lg lh li lj lk kf ll kg lm ki ln kj lo kl lp km lq lr bi translated">自动编码器</h1><p id="d0b3" class="pw-post-body-paragraph ls lt iq lu b lv lw ka lx ly lz kd ma mb mc md me mf mg mh mi mj mk ml mm mn ij bi translated">VAEs是自动编码器模型的特例。自动编码器由网络组成，其中有两个神经网络，即编码器阶段和解码器阶段。编码器缩小输入的维度，试图保留尽可能多的信息。编码器可以被认为是高维非线性PCA。解码器的工作是试图从编码器的输出中重建原始图像。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi mt"><img src="../Images/51dd5ed04e47cbd2b5ce02cae252f543.png" data-original-src="https://miro.medium.com/v2/resize:fit:1328/format:webp/1*eEWEerGf3ORrprodRRTZwQ.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</p></figure><p id="1391" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">这是一个AE架构示例。模型的输入是展平的28x28图像，该图像被压缩成512维向量，然后由编码器再次压缩成二维向量。解码器获取编码器的输出，并尝试重建原始的28x28图像。</p><p id="8dd3" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">像这样减少输入的维数非常有用。示例应用是异常值检测和去噪。如果试图发现异常值，可以对编码信号使用距离测量。另一个应用是当对信号去噪时，如果输入图像有噪声，通过编码然后解码，模型可以被训练以减少噪声并仅保留最重要的信息。</p><h1 id="2f80" class="la lb iq bd lc ld le lf lg lh li lj lk kf ll kg lm ki ln kj lo kl lp km lq lr bi translated"><strong class="ak">变型自动编码器</strong></h1><p id="2869" class="pw-post-body-paragraph ls lt iq lu b lv lw ka lx ly lz kd ma mb mc md me mf mg mh mi mj mk ml mm mn ij bi translated">VAEs将普通AE向前推进了一步。VAEs尝试在编码器阶段结束时创建一个分布，编码器可以使用该分布来生成新图像。在上图中，编码器将每个图像编码成两个变量。这些可用于参数化正态分布，例如，将一个参数作为平均值，将另一个参数作为标准偏差。</p><p id="ff9f" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">从该分布中采样，并将输入通过解码器有效地为我们提供了一种在图像空间中采样潜在分布的方法。</p><p id="336f" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">为了训练VAEs，编码器的输出用于参数化高斯分布。然后对该分布进行随机采样，然后将随机样本用作解码器的输入。这个看到例子后会更清楚。</p><h1 id="da13" class="la lb iq bd lc ld le lf lg lh li lj lk kf ll kg lm ki ln kj lo kl lp km lq lr bi translated"><strong class="ak">VAEs中的损失函数</strong></h1><p id="045d" class="pw-post-body-paragraph ls lt iq lu b lv lw ka lx ly lz kd ma mb mc md me mf mg mh mi mj mk ml mm mn ij bi translated">VAEs试图最小化两件事:</p><ul class=""><li id="322e" class="mu mv iq lu b lv mo ly mp mb mw mf mx mj my mn mz na nb nc bi translated">编码器的输入和解码器的输出之间的差异</li><li id="4476" class="mu mv iq lu b lv nd ly ne mb nf mf ng mj nh mn mz na nb nc bi translated">编码器分布和数据概率分布之间的差异</li></ul><p id="1ed1" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">为了量化输入和输出之间的差异，如果图像是黑白的，可以利用二进制交叉熵。</p><p id="ee7e" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">在VAEs中，目标是获得P(z)的近似值，其中P是我们数据的概率分布，z是我们的潜在变量。我们尝试从P(z|X)开始，其中X是我们的数据。我们没有P(z|X)的分布，但是，我们可以使用我们的低维编码器分布Q(z|X)来估计它。为了找到最佳Q，我们最小化编码器分布和数据概率分布之间的Kullback-Leibler散度。</p><h1 id="1f19" class="la lb iq bd lc ld le lf lg lh li lj lk kf ll kg lm ki ln kj lo kl lp km lq lr bi translated">VAE应用</h1><p id="2a85" class="pw-post-body-paragraph ls lt iq lu b lv lw ka lx ly lz kd ma mb mc md me mf mg mh mi mj mk ml mm mn ij bi translated">在这个例子中，我正在使用PyTorch，并且我正在一个免费的google colab机器上工作。我使用的训练数据是开源的时尚MNIST数据集。</p><p id="de4c" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">首先，我们构建上图所示的架构:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="8bea" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">编码器有两个隐藏层。第二个隐藏层分为两部分。第一个将输出编码器分布的平均值，第二个将输出标准偏差。解码器从编码器获取输入，并在返回模型的输出之前将它们通过两层。</p><p id="b9b1" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">下面是训练网络的代码:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="3777" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">对于编码器的每个输出，形成正态分布并进行采样。这个新样本被用作解码器的输入。为了优化权重，反向传播的使用方式与传统的神经网络相同，我使用的优化器是Adam。</p><h1 id="a423" class="la lb iq bd lc ld le lf lg lh li lj lk kf ll kg lm ki ln kj lo kl lp km lq lr bi translated">结果</h1><p id="c483" class="pw-post-body-paragraph ls lt iq lu b lv lw ka lx ly lz kd ma mb mc md me mf mg mh mi mj mk ml mm mn ij bi translated">现在，让我们看看这个VAE能生产什么。一、训练数据:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/aa6f54d0ec9109e3c26502dc2be04149.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/format:webp/1*39KabiiiC-DUgUu8yuSe1Q.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</p></figure><p id="a661" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">这些是一些随机的训练数据，你可以看到这些图像有很多细节。</p><p id="8c20" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">现在我将展示由解码器重建的图像。记住，这些图像是由二维向量重建的！</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ko"><img src="../Images/05e5075e9329b23b61d265ccc623f34e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1140/format:webp/1*a0lN53jFTfpkRzVrF7ITYg.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</p></figure><p id="b000" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">对于这样一个简单的架构来说不算太坏。自动编码器能够从编码器的输出中重建图像。然而，你会注意到这些图像是模糊的。这与编码器阶段涉及的信息丢失有关，并且是VAEs的主要缺点。</p><p id="cbf7" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">这些重建是近似图像潜在空间(P)中的随机样本。通过获取每个图像的编码器输出，并围绕这些输出参数化的分布进行采样，我们可以将这些反馈到解码器，以采样我们的近似图像概率函数(Q)。</p><p id="a39d" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">让我们看看当我们在上面看到的第一个图像(左上角的t恤图像)周围采样时会发生什么。</p><p id="8dad" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">第一次采样接近平均值，第二次稍远一点:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/429ba7be99227c4142f7847cd09c96d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*_7LN3z1a67wvEG1raSrKlw.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</p></figure><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/05ead930258a6a8b2505a64580919f72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*OTKb6KBfR18yfheaYr7CGQ.png"/></div><p class="kw kx gj gh gi ky kz bd b be z dk translated">作者图片</p></figure><p id="d3c1" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">在这个潜在空间的中心，我们有t恤。你可以看到，通过在潜在空间中移动，t恤会变形为手袋、不同种类的鞋子、套头衫、裤子等。在第一次采样中，衣服没有太多的变化，因为采样是在平均值附近进行的。大多数图像是衬衫或类似的。第二次采样，各种衣服都有。</p><h1 id="920c" class="la lb iq bd lc ld le lf lg lh li lj lk kf ll kg lm ki ln kj lo kl lp km lq lr bi translated"><strong class="ak">结论</strong></h1><p id="d196" class="pw-post-body-paragraph ls lt iq lu b lv lw ka lx ly lz kd ma mb mc md me mf mg mh mi mj mk ml mm mn ij bi translated">在这篇文章中，我描述了自动编码器、可变自动编码器、它们的功能以及它们的一些应用。VAEs在数据生成中是有用的，因为它们允许我们近似一个潜在的空间，然后可以从中取样。通过从该空间采样，可以生成新数据并用于其他模型的训练，从而提高它们的性能。</p><p id="de55" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">VAEs有一些缺点，特别是在输出中产生的模糊。解决这个问题的一个方法是使用生成转折网络。请继续关注未来关于这些的文章！</p><h2 id="e96a" class="nm lb iq bd lc nn no dn lg np nq dp lk mb nr ns lm mf nt nu lo mj nv nw lq iw bi translated">支持我</h2><p id="2d52" class="pw-post-body-paragraph ls lt iq lu b lv lw ka lx ly lz kd ma mb mc md me mf mg mh mi mj mk ml mm mn ij bi translated">如果你喜欢它，你可以跟我来<a class="ae nx" href="https://medium.com/@diegounzuetaruedas" rel="noopener"> <strong class="lu ja">！</strong> </a></p><p id="3070" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">你也可以通过我的推荐链接成为<strong class="lu ja">中级会员</strong>，访问我所有的文章等等:<a class="ae nx" href="https://diegounzuetaruedas.medium.com/membership" rel="noopener">https://diegounzuetaruedas.medium.com/membership</a></p><h2 id="c9fe" class="nm lb iq bd lc nn no dn lg np nq dp lk mb nr ns lm mf nt nu lo mj nv nw lq iw bi translated">你可能喜欢的其他文章</h2><p id="dade" class="pw-post-body-paragraph ls lt iq lu b lv lw ka lx ly lz kd ma mb mc md me mf mg mh mi mj mk ml mm mn ij bi translated"><a class="ae nx" rel="noopener" target="_blank" href="/kalman-filtering-a-simple-introduction-df9a84307add">卡尔曼滤波:简单介绍</a></p><p id="fc02" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated"><a class="ae nx" rel="noopener" target="_blank" href="/mcmc-a-visual-introduction-38e1d6131e86"> MCMC:可视化介绍</a></p></div></div>    
</body>
</html>