<html>
<head>
<title>A Python library to remove collinearity</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">消除共线性的Python库</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-python-library-to-remove-collinearity-5a4eb05d3d73?source=collection_archive---------10-----------------------#2021-06-28">https://towardsdatascience.com/a-python-library-to-remove-collinearity-5a4eb05d3d73?source=collection_archive---------10-----------------------#2021-06-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e1c7" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">一个处理共线变量的简单Python库</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/6370bf6e83cc004372105f72b4159c2b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*NbnzD0ujwQ1IQ6lg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="a5c9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">共线性是机器学习项目中非常普遍的问题。它是数据集特征之间的相关性，它会降低我们模型的性能，因为它增加了方差和维数。当你不得不处理无人监督的模型时，情况变得更糟。</p><p id="7bd3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">为了解决这个问题，我创建了一个移除共线要素的Python库。</p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><h1 id="63b7" class="mb mc it bd md me mf mg mh mi mj mk ml jz mm ka mn kc mo kd mp kf mq kg mr ms bi translated">什么是共线性？</h1><p id="1ada" class="pw-post-body-paragraph ky kz it la b lb mt ju ld le mu jx lg lh mv lj lk ll mw ln lo lp mx lr ls lt im bi translated">共线性通常称为多重共线性，是一种当数据集的要素显示彼此高度相关时出现的现象。通常用皮尔逊相关系数来衡量。如果相关矩阵显示出绝对值较高的非对角元素，我们就可以讨论共线性。</p><p id="60f3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">共线性是一个非常大的问题，因为它增加了模型方差(特别是对于线性模型)，它增加了维数而没有增加信息，此外，它扭曲了我们解释模型的能力。如果两个要素共线，则不应将它们放在一起考虑，只应考虑信息最丰富的要素。</p><p id="25b2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我多年来一直在处理共线性问题，最终决定创建一个Python库来帮助像我一样的其他数据科学家高效地处理这个问题。</p><h1 id="9fae" class="mb mc it bd md me my mg mh mi mz mk ml jz na ka mn kc nb kd mp kf nc kg mr ms bi translated">如何消除共线性</h1><p id="3e10" class="pw-post-body-paragraph ky kz it la b lb mt ju ld le mu jx lg lh mv lj lk ll mw ln lo lp mx lr ls lt im bi translated">首先，我们必须为相关系数的绝对值定义一个阈值。适当的探索性数据分析可以帮助我们在我们的数据集中确定这样一个阈值，但对于一个通用项目，我的建议是使用0.4。它适用于多种类型的关联，尽管我再次建议执行适当的EDA来找到适合您的数据集的值。</p><p id="b9bc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一旦设置了阈值，我们需要相关矩阵只包含绝对值小于该阈值的非对角线元素。</p><p id="1404" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于无监督的问题，想法是计算相关矩阵，并移除所有那些产生绝对值大于该阈值的元素的特征。我们可以从不太相关的对开始，并且只要考虑阈值，就继续添加特征。这给了我们一个不相关的数据集。</p><p id="85a5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于有监督的问题，我们可以使用例如单变量方法来计算特征重要性。我们可以考虑最重要的特征，然后按照它们的重要性不断添加特征，从最重要的特征到不太重要的特征，只有在考虑阈值约束的情况下才选择它们。</p><p id="9f5f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这个想法是，通过添加一个特征，我们在相关矩阵中添加了一个新行和一个新列，所以我们必须小心。这就是为什么我创建了我的图书馆。</p><h1 id="000f" class="mb mc it bd md me my mg mh mi mz mk ml jz na ka mn kc nb kd mp kf nc kg mr ms bi translated">“共线性”包</h1><p id="fcb3" class="pw-post-body-paragraph ky kz it la b lb mt ju ld le mu jx lg lh mv lj lk ll mw ln lo lp mx lr ls lt im bi translated">我们可以使用pip轻松安装我的“共线性”库。</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="7a42" class="ni mc it ne b gy nj nk l nl nm">!pip install collinearity</span></pre><p id="01e1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们看看Python中的动作。</p><p id="ed7b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，我们需要导入<em class="nn">共线性</em>包的<em class="nn">选择非共线性</em>对象。</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="6da0" class="ni mc it ne b gy nj nk l nl nm">from collinearity import SelectNonCollinear</span></pre><p id="4cf1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是执行特性选择和实现sklearn对象的所有方法的对象。</p><p id="18c0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，让我们导入一些有用的库和<em class="nn">波士顿</em>数据集。</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="420d" class="ni mc it ne b gy nj nk l nl nm">from sklearn.feature_selection import f_regression <br/>import numpy as np <br/>import pandas as pd <br/>import seaborn as sns <br/>from sklearn.datasets import load_boston </span><span id="8ac7" class="ni mc it ne b gy no nk l nl nm">sns.set(rc={'figure.figsize':(12,8)})</span></pre><p id="5a75" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们从无监督方法开始，在这种方法中，我们不知道目标变量，只想减少聚类问题的特征数量，</p><p id="65b3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们导入数据集并计算关联热图。</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="1ef0" class="ni mc it ne b gy nj nk l nl nm">X,y = load_boston(return_X_y=True) <br/>features = load_boston()['feature_names'] <br/>df = pd.DataFrame(X,columns=features) <br/>sns.heatmap(df.corr().abs(),annot=True)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/67b6cbb3d0ca1f177d9938fd10b45c9a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*bFPP_b2lUDCHe6v2.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="2d43" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如我们所见，我们有几个共线的变量(即热图中颜色较浅的变量)</p><p id="ee9f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们现在可以创建一个<em class="nn"> SelectNonCollinear </em>对象的实例，并将阈值设置为0.4。</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="f1d5" class="ni mc it ne b gy nj nk l nl nm">selector = SelectNonCollinear(0.4)</span></pre><p id="5df1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">与每个scikit-learn对象一样，我们有<em class="nn"> fit </em>、<em class="nn"> transform </em>和<em class="nn"> fit_transform </em>方法。我们还有<em class="nn"> get_support </em>方法，它为我们提供了所选特性的数组掩码。</p><p id="03ec" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们来拟合对象，并获取所选特征的蒙版。</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="52ce" class="ni mc it ne b gy nj nk l nl nm">selector.fit(X,y) <br/>mask = selector.get_support()</span></pre><p id="3613" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们可以只考虑选定的功能，并再次绘制关联热图:</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="9465" class="ni mc it ne b gy nj nk l nl nm">df2 = pd.DataFrame(X[:,mask],columns = np.array(features)[mask]) <br/>sns.heatmap(df2.corr().abs(),annot=True)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi np"><img src="../Images/d17f2b441aba074b5ae58eac4dbb9876.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*OvQ8OpbrGGauP1q_.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="ae98" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所选要素现在显示出比原始集合更低的共线性，并且没有系数如预期的那样大于0.4。</p><p id="8ca0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">对于有监督的方法，我们需要设置得分函数，该函数用于计算一个特性相对于给定目标的重要性。对于像这样的回归问题，我们可以使用<em class="nn"> f_regression </em>。对于分类问题，我们可能要用<em class="nn"> f_classif。</em></p><p id="2ade" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们需要在实例的构造函数中设置这个值。然后我们可以重新安装我们的选择器。</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="f2c1" class="ni mc it ne b gy nj nk l nl nm">selector = SelectNonCollinear(correlation_threshold=0.4,scoring=f_regression) </span><span id="9358" class="ni mc it ne b gy no nk l nl nm">selector.fit(X,y) mask = selector.get_support()</span></pre><p id="e0bf" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们现在可以计算新的热图，在这个例子中，它与无监督的情况相同。</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="7bf3" class="ni mc it ne b gy nj nk l nl nm">df3 = pd.DataFrame(X[:,mask],columns = np.array(features)[mask]) <br/>sns.heatmap(df3.corr().abs(),annot=True)</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/273b7f3e4cda1c5feee93002e6361434.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*reuBPPmlFXdd2pgX.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="6963" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果我们只想过滤我们的数据集，我们可以很容易地调用<em class="nn">转换</em>方法。</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="15aa" class="ni mc it ne b gy nj nk l nl nm">selector.transform(X) <br/># array([[ 0. , 15.3 , 396.9 , 4.98], <br/># [ 0. , 17.8 , 396.9 , 9.14], <br/># [ 0. , 17.8 , 392.83, 4.03], <br/># ..., <br/># [ 0. , 21. , 396.9 , 5.64], <br/># [ 0. , 21. , 393.45, 6.48], <br/># [ 0. , 21. , 396.9 , 7.88]])</span></pre><p id="f6b9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这允许我们在sklearn的管道中使用<em class="nn"> SelectNonCollinear </em>对象。</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="8199" class="ni mc it ne b gy nj nk l nl nm">pipeline = make_pipeline( SelectNonCollinear(correlation_threshold=0.4,scoring=f_regression), LinearRegression() ) <br/>pipeline.fit(X,y)</span></pre><p id="9897" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这样，我们可以毫不费力地在ML管道中实现这个对象。</p><h1 id="4393" class="mb mc it bd md me my mg mh mi mz mk ml jz na ka mn kc nb kd mp kf nc kg mr ms bi translated">结论</h1><p id="7e2f" class="pw-post-body-paragraph ky kz it la b lb mt ju ld le mu jx lg lh mv lj lk ll mw ln lo lp mx lr ls lt im bi translated">我实现了这个库来消除无监督和有监督机器学习项目的共线性。阈值的值应该根据适当的EDA来设置。对于像本例中使用的数据集这样的数据集，可能会出现这样的情况:在无监督和有监督方法之间，所选的要素是相同的。我的建议是根据我们的问题使用正确的方法。</p><p id="37fa" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果您有任何意见、问题或建议，请随时使用我的GitHub repo:<a class="ae nr" href="https://github.com/gianlucamalato/collinearity" rel="noopener ugc nofollow" target="_blank">https://github.com/gianlucamalato/collinearity</a></p></div><div class="ab cl lu lv hx lw" role="separator"><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz ma"/><span class="lx bw bk ly lz"/></div><div class="im in io ip iq"><p id="de6a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="nn">原载于2021年6月28日https://www.yourdatateacher.com</em><em class="nn"><a class="ae nr" href="https://www.yourdatateacher.com/2021/06/28/a-python-library-to-remove-collinearity/" rel="noopener ugc nofollow" target="_blank"><em class="nn">。</em></a></em></p></div></div>    
</body>
</html>