<html>
<head>
<title>Variational Bayesian inference with normalizing flows: a simple example</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">具有正常化流的变分贝叶斯推断:一个简单的例子</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/variational-bayesian-inference-with-normalizing-flows-a-simple-example-1db109d91062?source=collection_archive---------8-----------------------#2021-08-02">https://towardsdatascience.com/variational-bayesian-inference-with-normalizing-flows-a-simple-example-1db109d91062?source=collection_archive---------8-----------------------#2021-08-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="b261" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="8e2b" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">使用张量流概率的基本介绍</h2></div><h1 id="8ac8" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">1.0简介</h1><p id="042c" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">变分推理(VI)是一种值得关注的方法论。对于大型或复杂的贝叶斯建模，VI有可能成为首选方法，尤其是因为在计算上VI自然适合机器学习的领域。本文展示了标准化流程——这是简要介绍的——如何极大地提高VI的性能，即使对于非常简单的模型也是如此。Tensorflow Probability (TFP)中提供了标准化流，样板代码借用了TFP文档。VI和规范化流程的结合为推理建模创建了一个强大的工具。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/beb580f9e74ee02883ed9839f97835c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Neh1PnBhweKtK8PEWhV9A.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图一。具有变换高斯替代后验的线性回归模型TFP的变分损失。图片作者。</p></figure><h1 id="5cd1" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">1.1为什么要贝叶斯推断？</h1><p id="ab02" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">近年来，贝叶斯建模已经成为许多科学和技术领域的主流。贝叶斯建模的一个定义特征是能力——实际上也是需求——指定关于要估计的模型参数的先验信念。在一些应用中，这种先验知识的整合是选择贝叶斯方法的关键驱动因素，例如，弥补可用数据的不足。在其他应用程序中，采用贝叶斯方法更多的是一个实用的模型设计问题；随着高质量贝叶斯计算库的可用性，例如<a class="ae mo" href="https://mcmc-jags.sourceforge.io/" rel="noopener ugc nofollow" target="_blank">【JAGS】</a>、<a class="ae mo" href="https://mc-stan.org/" rel="noopener ugc nofollow" target="_blank">斯坦</a>、<a class="ae mo" href="https://r-nimble.org/" rel="noopener ugc nofollow" target="_blank"> R-nimble </a>、<a class="ae mo" href="https://docs.pymc.io/" rel="noopener ugc nofollow" target="_blank"> PyMC3 </a>、<a class="ae mo" href="https://www.tensorflow.org/probability" rel="noopener ugc nofollow" target="_blank"> Tensorflow Probability </a>、<a class="ae mo" href="https://www.r-inla.org/home" rel="noopener ugc nofollow" target="_blank"> R-INLA </a>，有可能快速创建几乎任意复杂度的定制概率模型。</p><p id="056a" class="pw-post-body-paragraph lg lh iq li b lj mp ka ll lm mq kd lo lp mr lr ls lt ms lv lw lx mt lz ma mb ij bi translated">贝叶斯模型构建的灵活性带来了额外的风险和责任，例如，模型实际上有意义吗？先验信念的影响是什么？计算估计稳健吗？—诸如此类。对于贝叶斯模型构建艺术的专家观点，我们强烈推荐阅读James Berger教授的一些文章。<a class="ae mo" href="https://mc-stan.org/docs/2_27/stan-users-guide/index.html" rel="noopener ugc nofollow" target="_blank"> Stan用户指南</a>也是范例和良好实践的宝贵来源。</p><h1 id="a2ac" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">1.2计算挑战</h1><p id="4d2e" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">贝叶斯推理在很大程度上与有效地近似难以解决的积分有关。迄今为止最常见的方法是<a class="ae mo" href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo" rel="noopener ugc nofollow" target="_blank">马尔可夫链蒙特卡罗</a> (MCMC)抽样，它通过从后验分布生成实现来避免计算积分的需要。有多种不同的取样器可用，这是一个有大量文献的活跃研究领域。BUGS——使用吉布斯抽样的贝叶斯推理——项目始于1989年，产生于1997年推出的WinBUGS。通过消除建模者编写自己的采样器的需要，这是使贝叶斯推理和模型构建对非专家来说是可访问的第一个重要步骤。</p><p id="ae47" class="pw-post-body-paragraph lg lh iq li b lj mp ka ll lm mq kd lo lp mr lr ls lt ms lv lw lx mt lz ma mb ij bi translated">MCMC采样的一种替代方法是对难以求解的积分使用<a class="ae mo" href="https://rss.onlinelibrary.wiley.com/doi/10.1111/j.1467-9868.2008.00700.x" rel="noopener ugc nofollow" target="_blank">解析近似法</a>。这是<a class="ae mo" href="https://www.r-inla.org/home" rel="noopener ugc nofollow" target="_blank"> R-INLA </a>采用的方法，它使用集成嵌套拉普拉斯近似。这种方法非常快，R-INLA在地理空间统计建模中特别受欢迎，尽管这种方法不如MCMC普遍适用。</p><p id="9c38" class="pw-post-body-paragraph lg lh iq li b lj mp ka ll lm mq kd lo lp mr lr ls lt ms lv lw lx mt lz ma mb ij bi translated">MCMC的另一种替代方法是<a class="ae mo" href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods" rel="noopener ugc nofollow" target="_blank"><strong class="li ja"/></a><strong class="li ja"/>(VI)变分推理，这种方法的应用，使用张量流概率，是本文的重点。一篇优秀的关于VI的概括性介绍和评论在<a class="ae mo" href="https://arxiv.org/abs/1601.00670" rel="noopener ugc nofollow" target="_blank">这里</a>有<a class="ae mo" href="https://mc-stan.org/" rel="noopener ugc nofollow" target="_blank">stand</a>具体文章<a class="ae mo" href="https://arxiv.org/abs/1506.03431" rel="noopener ugc nofollow" target="_blank">这里</a>。变分推理试图找到一个尽可能接近后验分布的(已知)概率分布。使用已知的分布避免了计算难以解决的积分的需要。变分推理通过最大化通常被称为ELBO的证据下限来实现这一点，ELBO与<a class="ae mo" href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="noopener ugc nofollow" target="_blank">kull back-lei bler-divergence</a>(KL-divergence)密切相关，后者是一种量化两种概率分布彼此相似程度的指标。最大化ELBO使真实后验分布和替代分布之间的KL偏差最小化。</p><h1 id="1c76" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">1.3机器学习</h1><p id="9517" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">近年来，变分推理变得越来越流行，并且在<a class="ae mo" href="https://mc-stan.org/docs/2_19/reference-manual/vi-algorithms-chapter.html" rel="noopener ugc nofollow" target="_blank">标准</a>和<a class="ae mo" href="https://www.tensorflow.org/probability" rel="noopener ugc nofollow" target="_blank">张量流概率</a>中都可用。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mu"><img src="../Images/d0aa8d014f94742777c86f1219730e0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*bXAX2sw9iytfJgPT-CAnug.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">TPF API的截图。图片作者。</p></figure><p id="7cf5" class="pw-post-body-paragraph lg lh iq li b lj mp ka ll lm mq kd lo lp mr lr ls lt ms lv lw lx mt lz ma mb ij bi translated">原因可能是多方面的，但最重要的是因为<strong class="li ja"> VI自然地适合于机器学习(ML) </strong>，而MCMC可能不适合。变分推理需要数值优化，而不是数值积分。机器学习建模通常需要数值优化——最小化一些ML模型和数据之间的损失函数。这意味着使用VI的<strong class="li ja">贝叶斯建模可以利用最新的专门构建的ML框架、软件和硬件</strong>，例如跨CPU的快速并行化、高性能多GPU和其他高性能计算基础设施，如谷歌的<a class="ae mo" href="https://en.wikipedia.org/wiki/Tensor_Processing_Unit" rel="noopener ugc nofollow" target="_blank"> TPUs </a>。这开启了一个令人兴奋的可能性，即在未来，VI可能成为大型或复杂贝叶斯模型的首选计算方法，而使用MCMC在计算上是不可行的。因此，变分推理是一个值得关注的领域。</p><h1 id="77e0" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">1.4双喷射器和标准化流程</h1><p id="def3" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">使用VI进行贝叶斯推理的一个主要实际挑战是<strong class="li ja">确定后验</strong>的候选分布——替代分布。我们需要足够灵活的分布，以紧密匹配真实的后验分布，同时在分析上易于处理，以避免那些难以解决的积分。多元高斯分布，通常经过一些变量的变换，可以说是最常见的选择。例如，Stan中使用的方法(参见<a class="ae mo" href="https://arxiv.org/pdf/1506.03431.pdf" rel="noopener ugc nofollow" target="_blank">第3页，此处为</a>)是首先将模型中的所有潜在变量转换到真实线上，例如，转换约束变量，如方差参数(必须严格为正)。然后为这些变换的变量以及那些已经在真实线上的变量设定多元高斯分布。</p><p id="2aca" class="pw-post-body-paragraph lg lh iq li b lj mp ka ll lm mq kd lo lp mr lr ls lt ms lv lw lx mt lz ma mb ij bi translated">使用高斯代理分布的一个已知问题是，即使在必要的变换之后，模型参数的<strong class="li ja">边际后验估计可能是欠分散的</strong>。这是因为高斯分布不够灵活，不能完全捕捉真实后验分布的形状。一个实际的例子在<a class="ae mo" href="https://www.tensorflow.org/probability/examples/Variational_Inference_and_Joint_Distributions" rel="noopener ugc nofollow" target="_blank">的张量流概率教程</a>中给出。当与来自MCMC抽样的结果相比时，高斯替代不是真实后验的好估计，后者被假定为金标准。在本文的后面，我们将看到一个类似的分散不足的问题，使用一个不同的数据集和一个更简单的贝叶斯模型。所以这不仅仅是一个复杂模型的问题。</p><p id="1968" class="pw-post-body-paragraph lg lh iq li b lj mp ka ll lm mq kd lo lp mr lr ls lt ms lv lw lx mt lz ma mb ij bi translated">定位高斯代理的替代方法是采用上述张量流<a class="ae mo" href="https://www.tensorflow.org/probability/examples/Variational_Inference_and_Joint_Distributions" rel="noopener ugc nofollow" target="_blank">教程</a>中使用的方法；使用一个<strong class="li ja">可训练代理分布</strong> <strong class="li ja">，它是通过使用</strong> <a class="ae mo" href="https://arxiv.org/pdf/1908.09257.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="li ja">归一化流</strong> </a> <strong class="li ja"> (NF) </strong>，特别是<a class="ae mo" href="https://arxiv.org/pdf/1606.04934.pdf" rel="noopener ugc nofollow" target="_blank">反向自回归流</a> (IAF)导出的。从概念上讲，标准化流程通过一系列双射变换将概率分布转换为不同的概率分布。双投影是应用于概率分布的变换，它确保变换产生有效的概率分布。概率密度必须整合为一，因此，如果变换拉伸或压缩参数，则必须相应地调整其密度，以在该密度下保持相同的体积。IAF方法使用神经网络将双射体变换链接在一起，这些变换具有可训练的参数，因此IAF有效地针对后验分布训练代理分布。<strong class="li ja">与高斯替代相比，这可以更好地估计后验参数</strong>。</p><h1 id="88bb" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">2.0建模</h1><p id="0837" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">本文的其余部分是对Colab/Jupyter笔记本的概述——完整版本可从GitHub <a class="ae mo" href="https://github.com/fraseriainlewis/towardsdatascience.git" rel="noopener ugc nofollow" target="_blank">这里</a>获得。我们只展示了足够的代码来提供关键步骤的概念。完整的代码有点长，但有相当多的重复，是现有Tensorflow概率教程加上对最新TFP API的额外调用的混合和匹配。TFP API是一个移动的目标，因此笔记本使用硬编码版本(Tensorflow=2.6.0rc0，Tensorflow Probability=0.13.0)来避免以后的更改破坏代码，但也使用足够新的版本，因此它们包括*。实验API函数。带有GPU运行时的Google Colab工作簿用于所有建模。</p><h1 id="2932" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">2.1数据</h1><p id="ee9d" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">我们使用来自经典的<a class="ae mo" href="http://www.medicine.mcgill.ca/epidemiology/Joseph/courses/common/rats.pdf" rel="noopener ugc nofollow" target="_blank"> WinBUGS Rats示例</a>的数据，这是一个广泛使用的数据集，用于演示贝叶斯推理中的正常层次模型。数据集包括来自n=30只大鼠的观察结果，并且在36天的时间内测量每只大鼠的重量m=5次。这里我们不使用层次模型(排除先验)，而是使用一个简单的线性回归模型，具有固定的效应截距和斜率。对于这些数据来说，这可能不是最好的方法，但选择这种方法是为了给出一个尽可能简单的例子来证明变分推断和标准化流。不难扩展所提供的代码，以将必要的层次结构/随机效应包括到似然函数中。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/de95cbfd43386829e8c0f20812b3db8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1226/format:webp/1*77XKEbsWHTLWzRqS8PJV_g.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图二。Rats数据来自WinBUGS示例手册。图片作者。</p></figure><h1 id="8dd2" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">2.2模型定义</h1><p id="ccd4" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">下面是贝叶斯模型，我们将使用BUGS语言中定义的rats数据，稍后我们还将使用<a class="ae mo" href="https://mcmc-jags.sourceforge.io/" rel="noopener ugc nofollow" target="_blank"> JAGS </a>来拟合该模型(下面的代码与JAGS兼容)。</p><figure class="md me mf mg gt mh"><div class="bz fp l di"><div class="mw mx l"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">要点1。bug中定义的模型1。</p></figure><p id="98bf" class="pw-post-body-paragraph lg lh iq li b lj mp ka ll lm mq kd lo lp mr lr ls lt ms lv lw lx mt lz ma mb ij bi translated">张量流概率(TPF)中定义相同可能性和先验的等效代码如下(要点2)。这使用了对<a class="ae mo" href="https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/JointDistributionNamedAutoBatched" rel="noopener ugc nofollow" target="_blank"><strong class="li ja">JointDistributionNamedAutoBatched</strong></a>的调用，这意味着组件是字典。代码可读性很强，但是使用TFP API构建一个联合分布可能有点复杂，因为有许多不同的形状参数。使用“自动匹配”调用变体有助于降低复杂性。有关有用的代码片段和API文档，请参见官方TFP教程。</p><figure class="md me mf mg gt mh"><div class="bz fp l di"><div class="mw mx l"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">要点2。TFP中定义的模型1。</p></figure><p id="edbf" class="pw-post-body-paragraph lg lh iq li b lj mp ka ll lm mq kd lo lp mr lr ls lt ms lv lw lx mt lz ma mb ij bi translated">我们已经定义了要拟合的模型，接下来需要的是定义模型1中参数的后验密度的替代分布。VI拟合过程将估计该替代中的参数，以便在给定可用数据的情况下给出最佳拟合。</p><figure class="md me mf mg gt mh"><div class="bz fp l di"><div class="mw mx l"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">要点三。模型1的高斯代理。</p></figure><p id="68a4" class="pw-post-body-paragraph lg lh iq li b lj mp ka ll lm mq kd lo lp mr lr ls lt ms lv lw lx mt lz ma mb ij bi translated">上述要点显示了替代密度的定义。模型1中的每个参数都在代理中定义。范围受限的参数，例如tau _ alpha——它是一个方差参数，因此必须严格为正——具有应用于高斯密度的<a class="ae mo" href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)#Softplus" rel="noopener ugc nofollow" target="_blank">软加变换</a>。这个代理是一组独立的高斯密度(在任何变换之前)。</p><h1 id="7269" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">2.3模型拟合</h1><p id="e10b" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">我们已经定义了模型和代理，所以剩下的就是进行拟合——最小化负变分证据下限(ELBO)。简而言之，如果我们查看下面的拟合代码片段(要点4)，我们可以看到两个分布<strong class="li ja">target _ model 1 . un normalized _ log _ prob</strong>和<strong class="li ja"> surrogate_posterior </strong>(其中前者只是要点2中定义的<strong class="li ja"> model1 </strong>分布，稍加修改使其成为<strong class="li ja"> target_model1，</strong>详见Jupyter工作簿)。这里执行的优化是试图最小化这两个分布之间的差异(其中差异由ELBO度量的负值描述)。</p><figure class="md me mf mg gt mh"><div class="bz fp l di"><div class="mw mx l"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">要点4。优化器代码。</p></figure><p id="9d9a" class="pw-post-body-paragraph lg lh iq li b lj mp ka ll lm mq kd lo lp mr lr ls lt ms lv lw lx mt lz ma mb ij bi translated">上述要点基本上是不言自明的。将拟合过程包装到一个<strong class="li ja"> tf.function </strong>中极大地改善了计算时间，这也得益于<strong class="li ja"> jit_compile=True </strong>。<a class="ae mo" href="https://www.tensorflow.org/guide/function" rel="noopener ugc nofollow" target="_blank"> tf.function </a>将代码编译成图形，而不是在eager模式下执行(TF v2中的默认模式)。</p><h1 id="7e2c" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">2.4使用高斯替代的虚拟仪器的结果</h1><p id="4e9e" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">与任何数值优化一样，跟踪模型拟合过程的进度以评估收敛性是很重要的。下图3显示了拟合过程中的损失函数(负ELBO)。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi mc"><img src="../Images/beb580f9e74ee02883ed9839f97835c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1Neh1PnBhweKtK8PEWhV9A.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图三。模型1的损失。图片作者。</p></figure><p id="75c0" class="pw-post-body-paragraph lg lh iq li b lj mp ka ll lm mq kd lo lp mr lr ls lt ms lv lw lx mt lz ma mb ij bi translated">损失图显示了每次迭代中负ELBO的均值和中值估计。其中每一个都基于调用tfp.vi.fit_surrogate_posterior时的4个数据点(我们选择sample_size=4)。默认值为sample_size=1。最小化似乎在大约30K次迭代后稳定下来。损失估计是相当嘈杂的，这并不奇怪，因为它是基于蒙特卡罗样本的变分散度估计。增加sample_size(例如从4增加到20)似乎对损失轨迹中的噪声只有很小的影响。<strong class="li ja">该模型的ELBO估计为ELBO=-663.5，</strong>我们希望最大化该值(图中显示为负ELBO)。</p><p id="aa0b" class="pw-post-body-paragraph lg lh iq li b lj mp ka ll lm mq kd lo lp mr lr ls lt ms lv lw lx mt lz ma mb ij bi translated">图4显示了截距、斜率和标准偏差的边际后验密度估计值(分别为Gist 2中的参数αI、βI和τc)。这些是在拟合过程之后使用从替代密度提取的实现来计算的。后验点估计值和标准差如图5所示。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi my"><img src="../Images/31fe886cfa67c0d35b3737e755054b58.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_t-hnQnv7H6uPAGTk84RjQ.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图4。模型1的边际后验概率。图片作者。</p></figure><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/58c99ee10a0dd14a20c79ea732896ab2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*8yg5MkpJlYofbIRjKjVVng.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图五。模型1的后验点估计。图片作者。</p></figure><h1 id="ffdd" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">3.0基于双投影器的多元正态替代</h1><p id="b621" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">上面使用的代理分布是通过使用<strong class="li ja">JointDistributionNamedAutoBatched</strong>独立定义每个组件的密度(=模型1中的参数)创建的。对于更复杂的替代，例如，允许模型参数之间的协方差的多元正态，则需要使用TFP的双投影器功能的更灵活的方法。我们在这里简单地讨论这一点，因为它是通向更复杂的标准化流程方法的自然跳板。</p><p id="3f1f" class="pw-post-body-paragraph lg lh iq li b lj mp ka ll lm mq kd lo lp mr lr ls lt ms lv lw lx mt lz ma mb ij bi translated">下面的代码片段(要点5)并不完整——缺少了一些基本的样板文件——但是展示了该方法的总体思想。定义了一个基本分布，这里是一组标准的独立法线。还定义了协方差结构，虽然这里使用的编码为简单起见假设了对角协方差矩阵(因此这与上面使用的代理没有很大的不同)，但是这种方法可以使用更复杂的依赖结构。参见<a class="ae mo" href="https://www.tensorflow.org/probability/examples/Variational_Inference_and_Joint_Distributions" rel="noopener ugc nofollow" target="_blank">这里的</a>一个更复杂的使用双投射器的块结构协方差的例子(这超出了本文的范围)。</p><p id="0e2c" class="pw-post-body-paragraph lg lh iq li b lj mp ka ll lm mq kd lo lp mr lr ls lt ms lv lw lx mt lz ma mb ij bi translated">最后一步是将一系列双投影变换链接到基本分布，其中一个是协方差矩阵变换，一个是位置参数，一个是确保先验的域匹配(例如，对于一些参数被约束为严格正的)。这种通用方法允许构建多元正态替代分布，该分布被约束到由模型定义的先验支持区域。这种方法当然不限于高斯分布，因为可以使用其他类型的基本分布。</p><figure class="md me mf mg gt mh"><div class="bz fp l di"><div class="mw mx l"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">要点5。用双投影公式定义协方差矩阵。</p></figure><p id="7286" class="pw-post-body-paragraph lg lh iq li b lj mp ka ll lm mq kd lo lp mr lr ls lt ms lv lw lx mt lz ma mb ij bi translated">下图类似于图3，但这一次使用bijector转换构建了代理。该模型的ELBO估计为<strong class="li ja"> ELBO=-663.9，在ELBO估计值</strong>周围的噪声容限给定的情况下，实际上与之前的模型相同(例如，参见图6中的右图)。鉴于代孕配方如此相似，这是意料之中的。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/59be3b4957ad4eeb6c7777b1bb23a7cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1214/format:webp/1*OZQt33Q1J_uFlX41qyKKWA.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图六。使用双射体的多元正态替代模型1的损失函数。图片作者。</p></figure><p id="41e9" class="pw-post-body-paragraph lg lh iq li b lj mp ka ll lm mq kd lo lp mr lr ls lt ms lv lw lx mt lz ma mb ij bi translated">图7中的点估计也与图5中的类似，这并不意外，尽管标准差不太相似。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/e217d1c75a59516adcfdf9e71940fab3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*r4a9oZYJqohsmm9nNJVp4A.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图7。基于双投影的多元正态替代的点估计。图片作者。</p></figure><h1 id="865c" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">使用规范化流程构建的4.0代理</h1><p id="a74b" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">为了建立在3.0节的基础上，我们现在使用一个通过规范化流程创建的代理。简而言之，虽然在技术上相当复杂，但是规范化流程允许我们做的是创建定制的概率分布，并针对手边的问题进行训练。要了解更多关于标准化流量的信息，这在密度估计中也有应用，参见参考文献<a class="ae mo" href="https://www.tensorflow.org/probability/api_docs/python/tfp/bijectors/MaskedAutoregressiveFlow" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><p id="9e9f" class="pw-post-body-paragraph lg lh iq li b lj mp ka ll lm mq kd lo lp mr lr ls lt ms lv lw lx mt lz ma mb ij bi translated">我们再次使用来自TFP的一些样板代码，其中一部分与第3.0节中用于基于双对象的代理的代码相同。这里的关键区别在于，一种新的双投影变换——反向自回归流(IAF)——被应用于基本分布(同样是一组高斯分布)。根据TFP文档，“反向自回归流(IAF)是使用神经网络捕捉分布成分之间复杂的非线性依赖关系的正常化流”。</p><p id="0af8" class="pw-post-body-paragraph lg lh iq li b lj mp ka ll lm mq kd lo lp mr lr ls lt ms lv lw lx mt lz ma mb ij bi translated">下面的代码片段(要点6)展示了一个用于实现IAF以构建代理发行版的代码示例。这段代码大量借鉴了现有的TFP教程。感兴趣的主要部分在第8–12行，特别是要使用的IAF变换的数量，以及在hidden_units参数和激活函数的选择中定义的每个变换的参数化(第12行)。尝试了不同的IAF变换数、隐藏单元数和激活函数值。下面的方法似乎对当前数据集很有效，给出了可靠的损失函数行为(并不总是如此，对这些参数的每个选择进行检查是很重要的)。</p><figure class="md me mf mg gt mh"><div class="bz fp l di"><div class="mw mx l"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">要点6。标准化流程模型定义。</p></figure><p id="0911" class="pw-post-body-paragraph lg lh iq li b lj mp ka ll lm mq kd lo lp mr lr ls lt ms lv lw lx mt lz ma mb ij bi translated">为了根据数据训练IAF代理，所需的代码(要点7)遵循与其他代理相同的形式，唯一的区别是引用新的IAF代理密度。</p><figure class="md me mf mg gt mh"><div class="bz fp l di"><div class="mw mx l"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">要点7。优化器代码。</p></figure><p id="508e" class="pw-post-body-paragraph lg lh iq li b lj mp ka ll lm mq kd lo lp mr lr ls lt ms lv lw lx mt lz ma mb ij bi translated">损失函数在性质上类似于前面的替代函数，但是请注意图8右侧面板中的不同范围和方差。与图6相比——图6是直接可比的，因为两者都使用sample_size=1来计算损耗，较大的值不稳定——我们可以看到损耗的噪声更大，而且分布略低于图6中的分布。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div role="button" tabindex="0" class="mz na di nb bf nc"><div class="gh gi nf"><img src="../Images/e75f7bda46c1bbada3efa96dd1ed4904.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0W51PGSxczXhfloX7msUuA.png"/></div></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图8。使用正常化流的代理损失函数。图片作者。</p></figure><p id="65e6" class="pw-post-body-paragraph lg lh iq li b lj mp ka ll lm mq kd lo lp mr lr ls lt ms lv lw lx mt lz ma mb ij bi translated">该型号的ELBO估计为<strong class="li ja"> ELBO=-662.6 </strong>，如果与之前的损失数字进行比较，那么ELBO应该略低似乎是合理的，但差异非常小，实际上可以忽略不计。</p><h1 id="1d31" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">5.0规范化流程在VI中表现更好吗？</h1><p id="467e" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">我们使用了一个非常简单的模型——带截距和斜率的线性回归——并使用VI将该模型拟合到数据中，使用和不使用标准化的基于流量的替代密度。确定替代密度表现如何的一个度量是ELBO值。在之前的TFP <a class="ae mo" href="https://www.tensorflow.org/probability/examples/Variational_Inference_and_Joint_Distributions" rel="noopener ugc nofollow" target="_blank">教程</a>中，显示了标准化流的替代密度具有更好的(2倍)ELBO值。从我们上面的结果来看，这里显然不是这种情况；IAF代理的ELBO稍微好一点，但这很难成为支持更复杂的流量标准化方法的令人信服的证据。</p><p id="8d9e" class="pw-post-body-paragraph lg lh iq li b lj mp ka ll lm mq kd lo lp mr lr ls lt ms lv lw lx mt lz ma mb ij bi translated">为了更深入地了解标准化流动是否真的有助于解决我们当前的建模问题，我们将来自VI IAF模型的密度估计值与来自JAGS(使用MCMC)的密度估计值进行了比较。马尔可夫链蒙特卡罗抽样可以说是这里的黄金标准方法，因为它不使用任何替代密度。图9显示了从MCMC (JAGS)获得的结果与使用第3.0节中的高斯代理和IAF代理从VI获得的结果之间的比较。</p><figure class="md me mf mg gt mh gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/46c598c40beaecdc0c4136283f1c8fbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1358/format:webp/1*z-f_qrZMjPwL_RXu2OA9sQ.png"/></div><p class="mk ml gj gh gi mm mn bd b be z dk translated">图九。使用不同方法的边缘后验密度。MCMC——绿色，标准化流——蓝色，变换高斯——粉红色，使用双投影器的约束高斯——红色。图片作者。</p></figure><p id="0083" class="pw-post-body-paragraph lg lh iq li b lj mp ka ll lm mq kd lo lp mr lr ls lt ms lv lw lx mt lz ma mb ij bi translated">从图9中可以明显看出，标准化流模型比任何一种高斯替代模型都更接近MCMC的结果，实际上，对于截距和斜率参数，IAF几乎与MCMC相同。因此，对于这个几乎是最简单的可能模型和数据的例子，当使用变分推断时，使用标准化流替代给出了实质上更好的参数估计。高斯代理明显遭受分散不足，这类似于在一些TPF教程中看到的情况(也使用具有更复杂协方差结构的高斯代理，我们在这里使用简单的对角矩阵)。然而，与TPF教程相比，有趣的是，在这个例子中，如果我们只使用了ELBO指标，考虑到所涉及的额外复杂性，我们可能会选择不使用标准化流程方法。这表明了检查参数估计的密度图的重要性。</p><p id="317f" class="pw-post-body-paragraph lg lh iq li b lj mp ka ll lm mq kd lo lp mr lr ls lt ms lv lw lx mt lz ma mb ij bi translated">这个简单的例子表明，当使用变分推断时，规范化流可以提供相当大的额外灵活性和可靠性。也就是说，基于MCMC的推理仍然可以为当前的许多问题提供最佳的实际解决方案，例如，这里使用的模型可以用Gist 1中的代码来拟合，只需增加几行代码来配置采样器，实际采样只需要几秒钟的计算时间。与任何基于MCMC的建模一样，需要执行各种诊断检查，但总的来说，对于许多问题，这仍然是一种非常有效的方法。</p><h1 id="ec72" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">6.0结论</h1><p id="ba53" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">本文提供了关于如何在TPF中应用变分推理，以及如何使用其规范化流功能来改进建模结果的简要顶级指南。GitHub <a class="ae mo" href="https://github.com/fraseriainlewis/towardsdatascience.git" rel="noopener ugc nofollow" target="_blank">这里</a>提供了一个独立的Colab/Juptyer笔记本，其中包含生成所示结果所需的所有代码。同一个存储库中还提供了通过MCMC适应相同模型所需的JAGS代码/文件。</p><p id="1296" class="pw-post-body-paragraph lg lh iq li b lj mp ka ll lm mq kd lo lp mr lr ls lt ms lv lw lx mt lz ma mb ij bi translated">规范化流程的变分推理是一个令人兴奋的领域，值得关注新的方法发展，尤其是涉及真实世界建模挑战的新用例。</p><h1 id="3019" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">关于作者</h1><p id="3f15" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">这是<a class="ae mo" href="https://www.linkedin.com/in/fraser-lewis-819a5080/" rel="noopener ugc nofollow" target="_blank">我</a>，我在<a class="ae mo" href="https://www.danone.com/" rel="noopener ugc nofollow" target="_blank">达能</a>的研发数据科学部门工作，在那里我管理着一个庞大的数据科学家团队，我也很幸运地做了一些建模工作。</p></div></div>    
</body>
</html>