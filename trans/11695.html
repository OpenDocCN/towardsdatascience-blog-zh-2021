<html>
<head>
<title>Save Money and Prevent Skew: One Container for Sagemaker and Lambda</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">省钱并防止偏斜:Sagemaker 和 Lambda 的一个容器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/save-money-and-prevent-skew-one-container-for-sagemaker-and-lambda-75d1bc6f4925?source=collection_archive---------22-----------------------#2021-11-19">https://towardsdatascience.com/save-money-and-prevent-skew-one-container-for-sagemaker-and-lambda-75d1bc6f4925?source=collection_archive---------22-----------------------#2021-11-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8fb5" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">关于为这两种服务构建容器的教程</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/bd0e9424cd3ae812aa78e818c9bdc400.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*5VJxj_W73nhfcn3J"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@blankerwahnsinn?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">费边布兰克</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><blockquote class="kw"><p id="37eb" class="kx ky iq bd kz la lb lc ld le lf lg dk translated">答<!-- -->这篇文章写完后，AWS 发布了<a class="ae kv" href="https://aws.amazon.com/about-aws/whats-new/2021/12/amazon-sagemaker-serverless-inference/" rel="noopener ugc nofollow" target="_blank"> Sagemaker 无服务器推断</a>，你可以直接用它来代替。</p></blockquote><p id="afe2" class="pw-post-body-paragraph lh li iq lj b lk ll jr lm ln lo ju lp lq lr ls lt lu lv lw lx ly lz ma mb lg ij bi mc translated"><span class="l md me mf bm mg mh mi mj mk di"> P </span>产品生命周期通常需要<em class="ml">不频繁的</em>机器学习推理。例如，测试版可能只会收到少量的流量。在这些场景中，托管模型推理可能是昂贵的:即使没有推理请求正在被处理，模型推理服务器也总是开启的。</p><p id="7650" class="pw-post-body-paragraph lh li iq lj b lk mm jr lm ln mn ju lp lq mo ls lt lu mp lw lx ly mq ma mb lg ij bi translated">解决利用不足的一个好办法是<em class="ml">无服务器</em>产品，如<a class="ae kv" href="https://aws.amazon.com/lambda/" rel="noopener ugc nofollow" target="_blank"> AWS Lambda </a>。这些允许您按需运行代码，但只为您使用的 CPU 时间付费。对于不频繁的推理来说，这是一个很有吸引力的环境，但是通常需要不同的软件栈和打包限制。因此，当需要扩展到无服务器范围之外时(即，当一个功能进入 GA 或我们需要 GPU 进行大批量处理时)，工程师需要为实时端点重写推理代码。</p><p id="91be" class="pw-post-body-paragraph lh li iq lj b lk mm jr lm ln mn ju lp lq mo ls lt lu mp lw lx ly mq ma mb lg ij bi translated">重写推理代码耗费了工程师的时间，并且引入了环境偏斜的风险。与<a class="ae kv" href="https://developers.google.com/machine-learning/guides/rules-of-ml" rel="noopener ugc nofollow" target="_blank">培训/服务偏斜</a>一样，环境偏斜是由于软件依赖性或输入数据处理方式的差异而导致的模型性能变化。换句话说，重写推理代码可能会意外地导致模型性能下降。</p><p id="5619" class="pw-post-body-paragraph lh li iq lj b lk mm jr lm ln mn ju lp lq mo ls lt lu mp lw lx ly mq ma mb lg ij bi translated">在本教程中，我将展示如何为 AWS Lambda 和 Sagemaker 端点创建一个 docker 容器。使用这种方法，您可以在不经常访问的模型的无服务器上开始运行推理，然后在需要时转移到始终在线的 Sagemaker 端点。因此，您可以在最初保持低成本，并有效地扩展，而没有环境偏差或重写推理代码的风险。</p><p id="cee5" class="pw-post-body-paragraph lh li iq lj b lk mm jr lm ln mn ju lp lq mo ls lt lu mp lw lx ly mq ma mb lg ij bi translated">github 上有完整的代码<a class="ae kv" href="https://github.com/lou-k/joint-sagemaker-lambda-container" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="445a" class="mr ms iq bd mt mu mv mw mx my mz na nb jw nc jx nd jz ne ka nf kc ng kd nh ni bi translated">你容器里的叉子</h1><p id="2466" class="pw-post-body-paragraph lh li iq lj b lk nj jr lm ln nk ju lp lq nl ls lt lu nm lw lx ly nn ma mb lg ij bi translated">Sagemaker 和 Lambda 都支持自带容器。Sagemaker <a class="ae kv" href="https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-inference-code.html#your-algorithms-inference-code-how-containe-serves-requests" rel="noopener ugc nofollow" target="_blank">需要 HTTP 服务器</a>，而 Lambda <a class="ae kv" href="https://docs.aws.amazon.com/lambda/latest/dg/images-create.html" rel="noopener ugc nofollow" target="_blank">使用运行时作为入口点</a>。这个项目背后的关键思想是根据容器运行的环境来派生容器逻辑，但是使用相同的推理代码:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/647bb8d6d6007499fdcbcac93ef4e94d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n3XIOIlDTuzdwlmVscET6Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="a168" class="pw-post-body-paragraph lh li iq lj b lk mm jr lm ln mn ju lp lq mo ls lt lu mp lw lx ly mq ma mb lg ij bi translated">在<code class="fe np nq nr ns b">entry.sh</code>中分叉实际上相当容易:Sagemaker 在启动时向容器传递一个<code class="fe np nq nr ns b">serve</code>参数，而 Lambda 传递函数处理程序名。因此，我们的<code class="fe np nq nr ns b">entry.sh</code>脚本只需要检查命令行参数就可以知道它是在 Sagemaker 还是 Lambda 中运行:</p><pre class="kg kh ki kj gt nt ns nu nv aw nw bi"><span id="8ccb" class="nx ms iq ns b gy ny nz l oa ob">#!/bin/bash</span><span id="302a" class="nx ms iq ns b gy oc nz l oa ob">if [ "${1:-}" = "serve" ] ; then<br/>  # Start an http server for Sagemaker endpoints..<br/>  exec /usr/local/bin/python serve.py<br/>else<br/>  # run the Lambda runtime environment<br/>  exec /usr/local/bin/python -m awslambdaric ${1:-}<br/>fi</span></pre><p id="4d50" class="pw-post-body-paragraph lh li iq lj b lk mm jr lm ln mn ju lp lq mo ls lt lu mp lw lx ly mq ma mb lg ij bi translated">(我省略了用于本地测试的 Lambda 仿真器，<a class="ae kv" href="https://github.com/lou-k/joint-sagemaker-lambda-container/blob/main/src/entry.sh" rel="noopener ugc nofollow" target="_blank">，但它在 github repo 中</a>)</p><h1 id="fdfa" class="mr ms iq bd mt mu mv mw mx my mz na nb jw nc jx nd jz ne ka nf kc ng kd nh ni bi translated">推理代码</h1><p id="6dab" class="pw-post-body-paragraph lh li iq lj b lk nj jr lm ln nk ju lp lq nl ls lt lu nm lw lx ly nn ma mb lg ij bi translated">这种方法的一个关键好处是<em class="ml">您可以在两种环境中使用相同的推理代码</em>。在这个例子中，我将使用来自<a class="ae kv" href="https://cv.gluon.ai/model_zoo/classification.html" rel="noopener ugc nofollow" target="_blank"> gluoncv 模型动物园</a>的模型对图像进行分类，但是基本概念应该可以扩展到您自己的模型。</p><p id="b343" class="pw-post-body-paragraph lh li iq lj b lk mm jr lm ln mn ju lp lq mo ls lt lu mp lw lx ly mq ma mb lg ij bi translated">使您的推理代码模块化是这里的关键:文件<code class="fe np nq nr ns b">serve.py</code>和<code class="fe np nq nr ns b">lambda.py</code>应该只有最少的逻辑来验证输入。为此，<code class="fe np nq nr ns b">inference.py</code>有两个主要功能:</p><ul class=""><li id="9ed3" class="od oe iq lj b lk mm ln mn lq of lu og ly oh lg oi oj ok ol bi translated"><code class="fe np nq nr ns b">load_model</code>-将模型加载到内存中</li><li id="49fc" class="od oe iq lj b lk om ln on lq oo lu op ly oq lg oi oj ok ol bi translated"><code class="fe np nq nr ns b">infer</code> —接受输入和模型，返回推理结果。</li></ul><p id="1689" class="pw-post-body-paragraph lh li iq lj b lk mm jr lm ln mn ju lp lq mo ls lt lu mp lw lx ly mq ma mb lg ij bi translated">加载模型需要一些小心，因为 Sagemaker 可以部署来自模型注册中心的模型，而 Lambda 不能。当这种情况发生时，工件被安装到容器中的<code class="fe np nq nr ns b">/opt/ml/model</code>，我们将通过变量<code class="fe np nq nr ns b">model_path</code>来标识它:</p><pre class="kg kh ki kj gt nt ns nu nv aw nw bi"><span id="0e9a" class="nx ms iq ns b gy ny nz l oa ob">def load_model(model_path = None):<br/>  """<br/>  Loads a model from model_path, if found, or a pretrained model<br/>  specified in the MODEL_NAME environment variable.<br/>  """</span><span id="4c83" class="nx ms iq ns b gy oc nz l oa ob">  # Try to load from model_path if we are running on sagemaker.<br/>  if model_path and os.path.exists(model_path):<br/>    symbol_file = glob(join(model_path, '*symbol.json'))[0]<br/>    params_file = glob(join(model_path, '*.params'))[0]<br/>    return SymbolBlock.imports(symbol_file, 'data', params_file)<br/>  else:<br/>    # running in lambda, so load the network from the model zoo<br/>    model_name = os.environ['MODEL_NAME']<br/>    return gluoncv.model_zoo.get_model(model_name,<br/>      pretrained=True, root='/tmp/')</span></pre><p id="36ac" class="pw-post-body-paragraph lh li iq lj b lk mm jr lm ln mn ju lp lq mo ls lt lu mp lw lx ly mq ma mb lg ij bi translated">在 Lambda 示例中，上面的代码让用户在<code class="fe np nq nr ns b">MODEL_NAME</code>环境变量中指定模型，并从模型动物园中加载模型。或者，您可以将模型打包在容器中，或者从 S3 装载。</p><p id="6774" class="pw-post-body-paragraph lh li iq lj b lk mm jr lm ln mn ju lp lq mo ls lt lu mp lw lx ly mq ma mb lg ij bi translated"><code class="fe np nq nr ns b">infer</code>函数是一个简单明了的例子，说明了如何使用 gluon API 进行<a class="ae kv" href="https://mxnet.apache.org/versions/1.8.0/api/python/docs/tutorials/deploy/inference/index.html" rel="noopener ugc nofollow" target="_blank">推理:</a></p><pre class="kg kh ki kj gt nt ns nu nv aw nw bi"><span id="c0d7" class="nx ms iq ns b gy ny nz l oa ob">def infer(uri, net):<br/>  """<br/>  Performs inference on the image pointed to in `uri.`<br/>  """</span><span id="2793" class="nx ms iq ns b gy oc nz l oa ob">  # Download and decompress the image<br/>  img = open_image(uri)</span><span id="75ef" class="nx ms iq ns b gy oc nz l oa ob">  # Preprocess the image<br/>  transformed_img = imagenet.transform_eval(img)</span><span id="3901" class="nx ms iq ns b gy oc nz l oa ob">  # Perform the inference<br/>  pred = net(transformed_img)<br/>  prob = mxnet.nd.softmax(pred)[0].asnumpy()<br/>  ind = mxnet.nd.topk(pred, k=5 [0].astype('int').asnumpy().tolist()</span><span id="a2ac" class="nx ms iq ns b gy oc nz l oa ob">  # accumulate the results<br/>  if hasattr(net, 'classes'):<br/>    results = [{<br/>      'label': net.classes[i], <br/>      'prob': str(prob[i])<br/>    } for i in ind]<br/>  else:<br/>    results = [{'label': i, 'prob': str(prob[i])} for i in ind]</span><span id="6af3" class="nx ms iq ns b gy oc nz l oa ob">  # Compose the results<br/>  return {'uri': uri, 'results': results}</span></pre><h1 id="fbea" class="mr ms iq bd mt mu mv mw mx my mz na nb jw nc jx nd jz ne ka nf kc ng kd nh ni bi translated">Sagemaker 代码</h1><p id="caa6" class="pw-post-body-paragraph lh li iq lj b lk nj jr lm ln nk ju lp lq nl ls lt lu nm lw lx ly nn ma mb lg ij bi translated">当容器在 Sagemaker 端点中启动时，它启动一个在<code class="fe np nq nr ns b">serve.py</code>中指定的 HTTP 服务器。服务器在<code class="fe np nq nr ns b">POST /invocations</code>端点上处理推断请求。我在这个例子中使用了<a class="ae kv" href="https://flask.palletsprojects.com/en/2.0.x/" rel="noopener ugc nofollow" target="_blank"> flask </a>，但是任何 HTTP 框架都应该可以工作:</p><pre class="kg kh ki kj gt nt ns nu nv aw nw bi"><span id="700c" class="nx ms iq ns b gy ny nz l oa ob"># HTTP Server<br/>app = Flask(__name__)</span><span id="204d" class="nx ms iq ns b gy oc nz l oa ob"># The neural network<br/>net = None</span><span id="195f" class="nx ms iq ns b gy oc nz l oa ob"><a class="ae kv" href="http://twitter.com/app" rel="noopener ugc nofollow" target="_blank">@app</a>.route("/ping", methods=["GET"])<br/>def ping():<br/>  return Response(response="\n", status=200)</span><span id="b8fd" class="nx ms iq ns b gy oc nz l oa ob"><a class="ae kv" href="http://twitter.com/app" rel="noopener ugc nofollow" target="_blank">@app</a>.route("/invocations", methods=["POST"])<br/>def predict():<br/>  global net<br/>  # do prediction<br/>  try:<br/>    lines = request.data.decode()<br/>    data = json.loads(lines)<br/>    results = inference.infer(data['uri'], net)<br/>  except ValueError as e:<br/>    error_message = f"Prediction failed with error '{e}'"<br/>    return Response(response=error_message, status=400)<br/>  output = json.dumps(results)<br/>  return Response(response=output, status=200)</span></pre><p id="52c9" class="pw-post-body-paragraph lh li iq lj b lk mm jr lm ln mn ju lp lq mo ls lt lu mp lw lx ly mq ma mb lg ij bi translated"><code class="fe np nq nr ns b">invocations</code>端点简单地解码请求并将输入传递给上面指定的<code class="fe np nq nr ns b">inference.infer</code>函数。这使事情变得简单:<em class="ml">所有的逻辑都在推理文件</em>中指定。</p><p id="d19e" class="pw-post-body-paragraph lh li iq lj b lk mm jr lm ln mn ju lp lq mo ls lt lu mp lw lx ly mq ma mb lg ij bi translated">网络本身应该在启动服务器之前加载 main 函数，这也为一些句柄命令行参数提供了可能性:</p><pre class="kg kh ki kj gt nt ns nu nv aw nw bi"><span id="a697" class="nx ms iq ns b gy ny nz l oa ob">def parse_args(args=None):<br/>  parser = argparse.ArgumentParser(<br/>    description='Server for inference on an image.'<br/>  )<br/>  parser.add_argument(<br/>    "--model-path", type=str, default='/opt/ml/model', <br/>    help="The model artifact to run inference on."<br/>  )<br/>  parser.add_argument(<br/>    "--port", type=int, default=8080,<br/>    help="Port to run the server on."<br/>  )<br/>  parser.add_argument(<br/>    "--host", type=str, default="0.0.0.0",<br/>    help="Host to run the server on."<br/>  )<br/>  return parser.parse_args(args)</span><span id="6888" class="nx ms iq ns b gy oc nz l oa ob">if __name__ == "__main__":<br/>  # parse command line arguments<br/>  args = parse_args()<br/>  # load the model<br/>  net = inference.load_model(args.model_path)<br/>  # start the server<br/>  app.run(host=args.host, port=args.port)</span></pre><p id="7faa" class="pw-post-body-paragraph lh li iq lj b lk mm jr lm ln mn ju lp lq mo ls lt lu mp lw lx ly mq ma mb lg ij bi translated">您会注意到<code class="fe np nq nr ns b">model_path</code>被设置为<code class="fe np nq nr ns b">/opt/ml/model</code>，这是 Sagemaker 从模型注册中心提供工件的默认位置。</p><h1 id="64b2" class="mr ms iq bd mt mu mv mw mx my mz na nb jw nc jx nd jz ne ka nf kc ng kd nh ni bi translated">拉姆达代码</h1><p id="a746" class="pw-post-body-paragraph lh li iq lj b lk nj jr lm ln nk ju lp lq nl ls lt lu nm lw lx ly nn ma mb lg ij bi translated">Lambda 代码甚至更简单:您需要的只是一个处理请求的函数:</p><pre class="kg kh ki kj gt nt ns nu nv aw nw bi"><span id="bfef" class="nx ms iq ns b gy ny nz l oa ob"># Loads the model when the lambda starts up<br/>net = inference.load_model()</span><span id="b6b7" class="nx ms iq ns b gy oc nz l oa ob">def handler(event, context):<br/>  global net<br/>  try:<br/>    return inference.infer(event['uri'], net)<br/>  except Exception as e:<br/>    logging.error(f'Could not perform inference on {event}', e)<br/>    return json.dumps({'error': 'Unable to perform inference!'})</span></pre><p id="9e56" class="pw-post-body-paragraph lh li iq lj b lk mm jr lm ln mn ju lp lq mo ls lt lu mp lw lx ly mq ma mb lg ij bi translated">第一次调用 Lambda 时加载模型，这会增加一些<a class="ae kv" href="https://aws.amazon.com/blogs/compute/operating-lambda-performance-optimization-part-1/" rel="noopener ugc nofollow" target="_blank">冷启动延迟</a>，但只要 Lambda 处于活动状态，它就会留在内存中。</p><h1 id="65c9" class="mr ms iq bd mt mu mv mw mx my mz na nb jw nc jx nd jz ne ka nf kc ng kd nh ni bi translated">保持精益</h1><p id="e8d0" class="pw-post-body-paragraph lh li iq lj b lk nj jr lm ln nk ju lp lq nl ls lt lu nm lw lx ly nn ma mb lg ij bi translated">AWS Lambda 是减少不经常使用的模型的推理成本的一个很好的方法，但是如果由于功能发布或产品流行而导致调用次数增加，成本会更高。切换到始终在线的 Sagemaker 端点降低了成本，但是可能需要重写推理代码，这需要时间，并且可能引入环境偏差。这里描述的容器可以在两种环境中工作，这使得在两种环境之间切换变得容易和快速，并获得最大的收益。</p><p id="c61f" class="pw-post-body-paragraph lh li iq lj b lk mm jr lm ln mn ju lp lq mo ls lt lu mp lw lx ly mq ma mb lg ij bi translated">完整的代码可以在 github 上找到。</p></div><div class="ab cl or os hu ot" role="separator"><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow ox"/><span class="ou bw bk ov ow"/></div><div class="ij ik il im in"><p id="7e8c" class="pw-post-body-paragraph lh li iq lj b lk mm jr lm ln mn ju lp lq mo ls lt lu mp lw lx ly mq ma mb lg ij bi translated">我们使用我在这里描述的技术来节省<a class="ae kv" href="https://www.bazaarvoice.com/" rel="noopener ugc nofollow" target="_blank"> Bazaarvoice </a>的推理成本。如果这种工作吸引了你，看看我们的<a class="ae kv" href="https://jobs.lever.co/bazaarvoice" rel="noopener ugc nofollow" target="_blank">职位空缺。</a></p><p id="5387" class="pw-post-body-paragraph lh li iq lj b lk mm jr lm ln mn ju lp lq mo ls lt lu mp lw lx ly mq ma mb lg ij bi translated">如果你喜欢这个故事，请考虑支持我，请<a class="ae kv" href="https://www.buymeacoffee.com/louk" rel="noopener ugc nofollow" target="_blank">给我买一杯咖啡</a>或者通过<a class="ae kv" href="https://lou-kratz.medium.com/membership" rel="noopener">我的推荐</a>注册媒体。</p></div></div>    
</body>
</html>