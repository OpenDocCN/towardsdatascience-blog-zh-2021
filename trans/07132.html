<html>
<head>
<title>Multi-task learning with Multi-gate Mixture-of-experts</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">多专家混合多任务学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multi-task-learning-with-multi-gate-mixture-of-experts-b46efac3268?source=collection_archive---------24-----------------------#2021-06-28">https://towardsdatascience.com/multi-task-learning-with-multi-gate-mixture-of-experts-b46efac3268?source=collection_archive---------24-----------------------#2021-06-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1c67" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">谷歌的内容推荐神经网络模型</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/7297ab5343e6b1572d5e41c2f3355fc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*w3eyQR6pg7iX1dya"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://unsplash.com/@possessedphotography?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">附身摄影</a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><h1 id="3059" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="690b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">多任务学习是一种机器学习方法，其中一个模型学习同时解决多个任务。假设通过学习用相同的模型完成多个相关的任务，每个任务的性能将比我们在每个任务上训练单独的模型更高。</p><div class="mn mo gp gr mp mq"><a rel="noopener follow" target="_blank" href="/multi-task-learning-in-machine-learning-20a37c796c9c"><div class="mr ab fo"><div class="ms ab mt cl cj mu"><h2 class="bd iu gy z fp mv fr fs mw fu fw is bi translated">机器学习中的多任务学习</h2><div class="mx l"><h3 class="bd b gy z fp mv fr fs mw fu fw dk translated">基于神经网络的深度多任务学习</h3></div><div class="my l"><p class="bd b dl z fp mv fr fs mw fu fw dk translated">towardsdatascience.com</p></div></div><div class="mz l"><div class="na l nb nc nd mz ne ks mq"/></div></div></a></div><p id="b70a" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">然而，这一假设并不总是正确的。简单的多任务学习方法没有考虑任务之间的关系和学习完成所有任务的权衡。</p><p id="32ab" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated"><a class="ae ky" href="https://dl.acm.org/doi/pdf/10.1145/3219819.3220007" rel="noopener ugc nofollow" target="_blank">谷歌的多门专家混合模型</a> (MMoE)试图通过明确学习任务之间的关系来改善基线多任务学习方法。</p><h1 id="2674" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结构</h1><p id="df0b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们将讨论多任务学习的三种架构:1)共享底层模型，2)单门专家混合模型(MoE)，以及3)多门专家混合模型(MMoE)。前两个架构提供了上下文，并展示了最终MMoE架构的渐进步骤。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nk"><img src="../Images/6da747f7c1a5cc6a12e8579986c71ef5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bb1GCM__BdjqLxz0vRXrHw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://dl.acm.org/doi/pdf/10.1145/3219819.3220007" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><h2 id="c4ce" class="nl la it bd lb nm nn dn lf no np dp lj ma nq nr ll me ns nt ln mi nu nv lp nw bi translated">共享底层模型</h2><p id="fe4e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">共享底层模式是最简单和最常见的多任务学习架构。该模型有一个单一的基础(共享的底层),所有特定于任务的子网都从这个基础开始。这意味着这种单一的表征用于所有的任务，个体任务没有办法相对于其他任务来调整它们从共享底层中得到什么信息。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/d19b7a5258ddbbf45a6ba42b66149a47.png" data-original-src="https://miro.medium.com/v2/resize:fit:496/format:webp/1*EZ3PUQez8IOXUjIrVl5miw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://dl.acm.org/doi/pdf/10.1145/3219819.3220007" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><h2 id="97e4" class="nl la it bd lb nm nn dn lf no np dp lj ma nq nr ll me ns nt ln mi nu nv lp nw bi translated">专家混合模型</h2><p id="69f4" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">专家混合架构通过创建多个专家网络并添加门控网络来加权每个专家网络的输出，对共享底层模型进行了改进。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/0502b25be99466936904b675ea3e80d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*3_a2_VSLTYtSA0sS.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://www.researchgate.net/figure/The-architecture-of-the-mixture-of-experts-model_fig1_220216747" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="6298" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">每个专家网络本质上都是一个唯一的共享底层网络，每个网络都使用相同的网络架构。假设每个专家网络能够学习数据中的不同模式，并专注于不同的事情。</p><p id="70de" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">门控网络然后产生加权方案，使得任务能够使用专家网络输出的加权平均值，以输入数据为条件。门控网络的最后一层是softmax层(<strong class="lt iu"> g(x) </strong>)，用于产生专家网络输出的线性组合(<strong class="lt iu"> y </strong>)。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/60040691014bb9055fa257cb1da4aecb.png" data-original-src="https://miro.medium.com/v2/resize:fit:306/format:webp/1*SYlQtjRC9hJUCBalRd-Png.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://dl.acm.org/doi/pdf/10.1145/3219819.3220007" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="a1d0" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">这种架构的主要创新是，该模型能够在每个样本的基础上不同地激活网络的部分。由于门控网络以输入数据为条件(由于门控网络作为训练整个模型的一部分而被训练)，该模型能够学习如何基于输入数据的属性对每个专家网络进行加权。</p><h2 id="477e" class="nl la it bd lb nm nn dn lf no np dp lj ma nq nr ll me ns nt ln mi nu nv lp nw bi translated">多门专家混合模型(MMoE)</h2><p id="3b73" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">最后，我们继续讨论多门专家混合(MMoE)模型架构。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/ad8f60882e8bbaac5bc2cf863c65e18d.png" data-original-src="https://miro.medium.com/v2/resize:fit:914/format:webp/1*w0MonzJA7LMGUO2_Hcsd9w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://dl.acm.org/doi/pdf/10.1145/3219819.3220007" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="d0a3" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">MMoE体系结构类似于MoE体系结构，只是它为每个任务提供了一个单独的门控网络，而不是为整个模型提供一个单独的门控网络。</p><p id="4374" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">这允许模型学习每个专家网络的每个任务和每个样本的权重，而不仅仅是每个样本的权重。这允许MMoE学习对不同任务之间的关系进行建模。彼此几乎没有共同点的任务将导致每个任务的门控网络学习使用不同的专家网络。</p><p id="b2b6" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">MMoE的作者通过在具有不同任务相关性水平的合成数据集上比较共享底层、MoE和MMoE架构来验证这一结论。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/4e62899631f5b8f9ccad735fa0ce8bfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Kapqvo4pQS9-t4f9SSTnYA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://dl.acm.org/doi/pdf/10.1145/3219819.3220007" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="0c56" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">首先，我们看到，相对于MoE和MMoE模型，共享底部模型在所有情况下都表现不佳。</p><p id="1b0f" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">接下来，我们可以看到，随着任务之间的相关性降低，MoE和MMoE模型之间的性能差距增大。</p><p id="1c20" class="pw-post-body-paragraph lr ls it lt b lu nf ju lw lx ng jx lz ma nh mc md me ni mg mh mi nj mk ml mm im bi translated">这表明MMoE能够更好地处理任务互不相关的情况。任务多样性越大，MMoE相对于共享底层或MoE架构的优势就越大。</p></div></div>    
</body>
</html>