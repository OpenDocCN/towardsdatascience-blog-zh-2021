<html>
<head>
<title>The Quora Question Pair Similarity Problem</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Quora问题对相似性问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-quora-question-pair-similarity-problem-3598477af172?source=collection_archive---------18-----------------------#2021-08-31">https://towardsdatascience.com/the-quora-question-pair-similarity-problem-3598477af172?source=collection_archive---------18-----------------------#2021-08-31</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="22bd" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">一个初学者在Kaggle上经历一个问题的不同生命周期的旅程。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/6f3e45fa1921012f43e1e221ed22ace9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZOjoppdWw-zPqO1gJ5Aaeg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://www.instagram.com/sanjaychouhansc/" rel="noopener ugc nofollow" target="_blank"> Me </a>拍摄。</p></figure><p id="0ea0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是我的第一个案例研究，所以你可以期待一个初学者友好的数据分析和模型构建。对于这个问题，我只使用了经典的机器学习模型。然而，参与这个案例研究对我来说是一次很好的学习经历。在这篇博客中，我会尽可能地与你分享。</p><p id="f030" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在博客里，我只会写摘要。你可以在这里查看完整的笔记本<a class="ae kv" href="https://nbviewer.jupyter.org/github/scsanjay/case-studies/blob/main/01.%20Quora%20Quesion%20Pair%20Similarity/quora-duplicate-questions.ipynb" rel="noopener ugc nofollow" target="_blank">，也可以在</a><a class="ae kv" href="https://github.com/scsanjay/case-studies/tree/main/01.%20Quora%20Quesion%20Pair%20Similarity" rel="noopener ugc nofollow" target="_blank"> github </a>上查看代码。</p><p id="ef9d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于所有有经验的人，我希望你们能对未来的案例研究提出反馈意见。🤝🤓</p><h1 id="5bdb" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">目录</h1><ol class=""><li id="60be" class="mk ml iq ky b kz mm lc mn lf mo lj mp ln mq lr mr ms mt mu bi translated">介绍</li><li id="bc04" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">业务目标和约束</li><li id="4d5b" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">数据概述</li><li id="4ff9" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">业务指标</li><li id="dbdc" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">基础EDA</li><li id="08e4" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">数据清理</li><li id="e1b1" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">特征抽出</li><li id="fdac" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">具有特性的EDA</li><li id="5047" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">SentenceBERT <br/> i. EDA关于SentenceBERT的新特性</li><li id="4444" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">数据预处理</li><li id="3dfd" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">训练模型<br/>一、支持向量分类器<br/>二。随机森林<br/>三。XGBoost <br/>四。另一个XGBoost🏆</li><li id="eb20" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">最后的想法</li><li id="b665" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr mr ms mt mu bi translated">参考</li></ol><h1 id="4120" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">介绍</h1><p id="c722" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">Quora是一个问答的平台，就像StackOverflow一样。但quora更像是一个通用的问答平台，这意味着没有像StackOverflow那样的代码。</p><p id="4338" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">quora面临的众多问题之一就是问题重复。重复提问会破坏提问者和回答者的体验。由于提问者在问一个重复的问题，我们可以只给他/她看前一个问题的答案。并且回答者不必为基本相同的问题重复他/她的答案。</p><p id="2097" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">例如，我们有一个问题，比如“我怎样才能成为一个好的地质学家？”这个问题有一些答案。后来有人问了另一个问题，比如“要成为一名伟大的地质学家，我应该做些什么？”。<br/>我们可以看到，这两个问题问的是同一个问题。尽管问题的措辞不同，但两个问题的意图是相同的。<br/>所以这两个问题的答案是一样的。这意味着我们可以只显示第一个问题的答案。这样，提问题的人会立即得到答案，已经回答了第一个问题的人也不必重复。</p><p id="1e05" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个问题在Kaggle上可以作为竞赛。<a class="ae kv" href="https://www.kaggle.com/c/quora-question-pairs" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/c/quora-question-pairs</a></p><p id="fca3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">给定两个问题，我们的主要目标是找出它们是否相似。所以让我们用ML变变魔术吧。🪄</p><h1 id="4ff2" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">业务目标和约束</h1><ul class=""><li id="ab18" class="mk ml iq ky b kz mm lc mn lf mo lj mp ln mq lr nd ms mt mu bi translated">没有严格的等待时间要求。</li><li id="faa8" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr nd ms mt mu bi translated">我们希望有可解释性，但这不是绝对强制性的。</li><li id="2ff6" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr nd ms mt mu bi translated">误分类的代价中等。</li><li id="6eb4" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr nd ms mt mu bi translated">这两个类(重复或不重复)同等重要。</li></ul><h1 id="3f94" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">数据概述</h1><p id="5d17" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">可用列:<strong class="ky ir"> id，qid1，qid2，question1，question2，is_duplicate </strong> <br/>类标签:<strong class="ky ir"> 0，1</strong>T5】总训练数据/行数:<strong class="ky ir"> 404290 </strong> <br/>列数:<strong class="ky ir">6</strong><br/><strong class="ky ir">is _ duplicate</strong>为因变量。<br/>非重复数据点数为<strong class="ky ir">255027</strong>T17】重复数据点数为<strong class="ky ir"> 149263 </strong></p><p id="4230" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们有<strong class="ky ir"> 404290 </strong>训练数据点。而只有<strong class="ky ir"> 36.92% </strong>为阳性。这意味着它是一个不平衡的数据集。</p><h1 id="e286" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">业务指标</h1><p id="5426" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">这是一个二元分类。</p><ul class=""><li id="cc8c" class="mk ml iq ky b kz la lc ld lf ne lj nf ln ng lr nd ms mt mu bi translated">我们需要尽量减少这次挑战的日志损失。</li></ul><h1 id="0591" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">基础EDA</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/04a91a96a398bb9acc6c078e49b3c48b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*qmAxfC775YvYVuuQ"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@andrewtneel?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">安德鲁·尼尔</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="f100" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">测试数据没有问题id。所以自变量是<strong class="ky ir">问题1 </strong>，<strong class="ky ir">问题2 </strong>因变量是<strong class="ky ir"> is_duplicate </strong>。</p><p id="0222" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3行有空值。所以我们删除了它们，现在我们有了用于训练的<strong class="ky ir"> 404287 </strong>问题对。</p><ul class=""><li id="b867" class="mk ml iq ky b kz la lc ld lf ne lj nf ln ng lr nd ms mt mu bi translated"><strong class="ky ir"> 36.92% </strong>的问题对是重复的，而<strong class="ky ir"> 63.08% </strong>的问题对是非重复的。</li><li id="f61f" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr nd ms mt mu bi translated">在<strong class="ky ir"> 808574 </strong>个问题(包括问题1和问题2)中，<strong class="ky ir"> 537929 </strong>是唯一的。</li><li id="f9e7" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr nd ms mt mu bi translated">大多数问题重复的次数很少。只有少数是多次重复的。</li><li id="9039" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr nd ms mt mu bi translated">一个问题重复<strong class="ky ir"> 157 </strong>次，这是最大重复次数。</li></ul><p id="7d4e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有一些问题字符很少，没有意义。稍后将通过数据清理来解决这个问题。</p><h1 id="d9ee" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">数据清理</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/60fd737abfeed579918336b97cfa1351.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*CVcfGd_POFSXouO0"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@pillepriske?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">皮勒r .普里斯克</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><ul class=""><li id="6b29" class="mk ml iq ky b kz la lc ld lf ne lj nf ln ng lr nd ms mt mu bi translated">我们已经将所有内容都转换成小写。</li><li id="3272" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr nd ms mt mu bi translated">我们已经消除了宫缩。</li><li id="d6ac" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr nd ms mt mu bi translated">我们已经用货币名称代替了货币符号。</li><li id="7138" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr nd ms mt mu bi translated">我们还删除了超链接。</li><li id="5f6b" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr nd ms mt mu bi translated">我们已经删除了非字母数字字符。</li><li id="f3d2" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr nd ms mt mu bi translated">我们已经用单词lemmatizer去掉了词形变化。</li><li id="ca39" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr nd ms mt mu bi translated">我们还删除了HTML标签。</li></ul><h1 id="177d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">特征抽出</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/54adbaf6de9400473f15feb42b5fa9b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*rYFoBHN4goDzRwR_"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">斯蒂芬·罗德里格兹在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="f3c5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们从问题中创建了<strong class="ky ir"> 23个</strong>特征。</p><ul class=""><li id="19c9" class="mk ml iq ky b kz la lc ld lf ne lj nf ln ng lr nd ms mt mu bi translated">我们已经为这两个问题创建了特征q1_char_num、q2_char_num以及字符数。</li><li id="bca3" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr nd ms mt mu bi translated">我们已经为这两个问题创建了特征q1_word_num、q2_word_num以及字符数。</li><li id="d3b7" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr nd ms mt mu bi translated">我们已经创建了total_word_num特征，它等于q1_word_num和q2_word_num之和。</li><li id="f86f" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr nd ms mt mu bi translated">我们已经创建了differ_word_num特征，它是q1_word_num和q2_word_num之间的绝对差。</li><li id="c482" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr nd ms mt mu bi translated">我们已经创建了same_first_word功能，如果两个问题的第一个单词相同，则该功能为1，否则为0。</li><li id="e124" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr nd ms mt mu bi translated">我们已经创建了same_last_word功能，如果两个问题具有相同的最后一个单词，则该功能为1，否则为0。</li><li id="96ea" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr nd ms mt mu bi translated">我们已经创建了total_unique_word_num功能，它等于两个问题中唯一单词的总数。</li><li id="9495" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr nd ms mt mu bi translated">我们创建了total _ unique _ word _ without top word _ num功能，该功能等于两个问题中不含停用词的唯一单词总数。</li><li id="d042" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr nd ms mt mu bi translated">总唯一单词数比率等于总唯一单词数除以总单词数。</li><li id="a4d7" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr nd ms mt mu bi translated">我们已经创建了common_word_num功能，它是两个问题中总的常用词的计数。</li><li id="f3a4" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr nd ms mt mu bi translated">common_word_ratio特性等于common_word_num除以total_unique_word_num。</li><li id="5f4a" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr nd ms mt mu bi translated">common_word_ratio_min等于common_word_num除以问题1和问题2之间的最小字数。</li><li id="5298" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr nd ms mt mu bi translated">common_word_ratio_max等于common_word_num除以问题1和问题2之间的最大字数。</li><li id="3709" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr nd ms mt mu bi translated">我们已经创建了common _ word _ withoutstopword _ num功能，它是两个问题中除停用词之外的所有常用词的计数。</li><li id="37a5" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr nd ms mt mu bi translated">common _ word _ without top word _ ratio特性等于common _ word _ withoutstopword _ num除以total _ unique _ word _ without top word _ num。</li><li id="8244" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr nd ms mt mu bi translated">common _ word _ without top word _ ratio _ min等于common _ word _ withoutstopword _ num除以问题1和问题2之间的最小字数，不包括停用词。</li><li id="1196" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr nd ms mt mu bi translated">common _ word _ without top word _ ratio _ max等于common _ word _ withoutstopword _ num除以问题1和问题2之间的最大字数，不包括停用词。</li><li id="b7d5" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr nd ms mt mu bi translated">然后用fuzzywuzzy字符串匹配工具提取了fuzz_ratio、fuzz_partial_ratio、fuzz_token_set_ratio和fuzz_token_sort_ratio特征。参考:<a class="ae kv" href="https://github.com/seatgeek/fuzzywuzzy" rel="noopener ugc nofollow" target="_blank">https://github.com/seatgeek/fuzzywuzzy</a></li></ul><h1 id="7b16" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">具有特性的EDA</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/1b05b5fcbe93e216fba365a910d306cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*yNpeMlNby1eavvAK"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">艾萨克·史密斯在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><ul class=""><li id="56a7" class="mk ml iq ky b kz la lc ld lf ne lj nf ln ng lr nd ms mt mu bi translated">如果第一个单词或最后一个单词相同，则问题对很有可能是重复的。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nl"><img src="../Images/c02710e944a037b45675a91fca182a20.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ru-kvaSw53yiunQA-rrKVA.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/e02dbafb2e6c1cd0f83b7fa6f3135a2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c_Dt34mDzOXPBSJkaQPSAw.png"/></div></div></figure><ul class=""><li id="aebd" class="mk ml iq ky b kz la lc ld lf ne lj nf ln ng lr nd ms mt mu bi translated">如果问题对是重复的，那么包含和不包含停用词的唯一单词总数(q1和q2的总和)会减少。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/08dd314cc21e47b983f6a3fcfd87b5d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-dQbmgg1emAFW9QiMAaWpw.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/0b7fd4f311d33c555f6d16aea2f163d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jybKeGlO7yxr6Clsao7pjw.png"/></div></div></figure><ul class=""><li id="3788" class="mk ml iq ky b kz la lc ld lf ne lj nf ln ng lr nd ms mt mu bi translated">对于重复问题对，总的唯一单词与总单词的比率通常较小。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi no"><img src="../Images/f66a4b48aa0329ba913c2265b236351b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jCwZaWpbL37EHbSlOOqIjQ.png"/></div></div></figure><ul class=""><li id="b19e" class="mk ml iq ky b kz la lc ld lf ne lj nf ln ng lr nd ms mt mu bi translated">重复的问题对往往在两个问题之间有更多的常用词。因此，与常用词相关的提取特征也显示出分布的差异。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/edfacbc664cb37a5fa431b9130d0585a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*57FikO1mSmPLXAfiS48c-w.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nq"><img src="../Images/5ab65a4542cc0b34673dd95f7b431fb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lvjg3UtkjH0fZTW8qFU7uA.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/b5bcd9c111df14dd9a680140a055f086.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tyjjZKi-vjjyz4t5pnXY8Q.png"/></div></div></figure><ul class=""><li id="0895" class="mk ml iq ky b kz la lc ld lf ne lj nf ln ng lr nd ms mt mu bi translated">对于重复的问题对，模糊比率通常更高。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ns"><img src="../Images/b21a55b3cf40397c0fc9711fdb9a9d9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tg7TNhx5ACexPNoEmDc0Fg.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/7c8c3b43cfffca7e0c18b72a2b2ed351.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uPiopOLjUu9r3zsTPp_Tnw.png"/></div></div></figure><h1 id="b069" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">句子的特征化</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/dfa8440dd6b137f62b47ecb837955ea6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gt5yGUHGZyIh2B4M"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@mbaumi?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">米卡·鲍梅斯特</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="ac1d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们需要将问题转换成某种数字形式，以应用机器学习模型。有各种各样的选择，从基本的单词包到通用的句子编码器。</p><p id="34e4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我试着推断句子嵌入。但它返回4096维表示。应用后，训练数据变得巨大。所以我丢弃了它。这个问题我选择了SentenceBERT。</p><p id="b3c5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">句子BERT是一种基于BERT的句子嵌入技术。我们将使用预先训练好的SentenceBERT模型<em class="nu"> paraphrase-mpnet-base-v2 </em>，这是质量最好的推荐。句子伯特产生768个维度的输出。<a class="ae kv" href="https://www.sbert.net/" rel="noopener ugc nofollow" target="_blank">https://www.sbert.net/</a></p><p id="3fbb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们又创建了两个特征<strong class="ky ir">余弦_相似性_伯特</strong>和<strong class="ky ir">欧几里得_距离_伯特</strong>，它们用句子伯特表示来测量两对问题之间的相似性和距离。</p><p id="3100" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">到目前为止，特征总数为<strong class="ky ir"> 25 </strong>。</p><h2 id="cd28" class="nv lt iq bd lu nw nx dn ly ny nz dp mc lf oa ob me lj oc od mg ln oe of mi og bi translated">EDA关于与SentenceBERT相关的新功能</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/6df25aed171dceadb69f6892a38baba8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*M0UJDA70w2cb7_DBugXhHg.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/7bbfd9533d970844d6a808abaf6c8329.png" data-original-src="https://miro.medium.com/v2/resize:fit:1066/format:webp/1*xopO6BzMshmvuldDDtZ5Kg.png"/></div></figure><ul class=""><li id="e6d4" class="mk ml iq ky b kz la lc ld lf ne lj nf ln ng lr nd ms mt mu bi translated"><strong class="ky ir">余弦相似度</strong>对于重复对更大。</li><li id="f63c" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr nd ms mt mu bi translated">80%的非重复问题对和只有20%的重复问题对具有余弦相似度&lt;= .815</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/d1021b3df7e5a48da10b5fe4fda9e7b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3zxx_rKCKlqv3Rw-MMw_TQ.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/a4dc90618cb14f4c07d21a96ec99a283.png" data-original-src="https://miro.medium.com/v2/resize:fit:1094/format:webp/1*WmFT1rvf-FB2eQIIyTC4mA.png"/></div></figure><ul class=""><li id="36b2" class="mk ml iq ky b kz la lc ld lf ne lj nf ln ng lr nd ms mt mu bi translated"><strong class="ky ir">对于重复问题对，欧几里德距离</strong>更小。</li><li id="ca6e" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr nd ms mt mu bi translated">20%的非重复问题对和大约80%的重复问题对具有&lt; = 2的欧几里德距离。</li></ul><p id="4799" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">它展示了帕累托原则(80-20法则)。</p><h1 id="8611" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">数据预处理</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/e56261ca2ffe2b6e6507a02ab0fb7265.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*v1gvhdSK3Y7YNmgi"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">布拉登·科拉姆在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><p id="b8e8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们归一化(最小-最大缩放)提取的特征。我们没有规范化嵌入，因为不推荐这样做。</p><p id="8866" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们有<strong class="ky ir"> 1561 </strong>特色(25 + 768 + 768)。</p><ul class=""><li id="04ce" class="mk ml iq ky b kz la lc ld lf ne lj nf ln ng lr nd ms mt mu bi translated"><strong class="ky ir"> 25 </strong>是提取的特征。</li><li id="ef0e" class="mk ml iq ky b kz mv lc mw lf mx lj my ln mz lr nd ms mt mu bi translated"><strong class="ky ir"> 768+768 </strong>用于问题1和问题2的句子嵌入。</li></ul><p id="83a9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因为数据集是不平衡的。我们通过从少数类中采样来进行<strong class="ky ir">过采样</strong>。<br/>现在我们有<strong class="ky ir"> 510048 </strong>数据点用于训练。每个班255024 。</p><p id="b737" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请注意，我没有留出任何数据用于本地测试。因为我们的主要目标是在Kaggle上取得好成绩。</p><h1 id="accb" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">培训模型</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/9748ce57d4ccef5296b4559e2a9bb5f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WXiK1FtQPFQzc1B6"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@karsten116?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">卡斯滕·怀恩吉尔特</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h2 id="897f" class="nv lt iq bd lu nw nx dn ly ny nz dp mc lf oa ob me lj oc od mg ln oe of mi og bi translated">支持向量分类器</h2><p id="35e5" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">在用参数网格训练对半网格搜索CV时，</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="ab3a" class="nv lt iq oo b gy os ot l ou ov">svc_param_grid = {‘C’:[1e-2, 1e-1, 1e0, 1e1, 1e2]}</span></pre><p id="fa23" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们使用LinearSVC是因为它被推荐用于大型数据集。我们使用了L2罚函数，损失函数是铰链损失的平方。此外，建议对大型数据集使用原始公式。对于C的某些值来说，它是不可转换的，所以我将max_iter增加到3000。</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="287a" class="nv lt iq oo b gy os ot l ou ov">svc_clf = LinearSVC(penalty='l2', loss='squared_hinge', dual=False, max_iter=3000)</span></pre><p id="2f0e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了交叉验证减半网格搜索cv，我使用了1次洗牌和70:30的分割。另外，对选择的评分是准确性。</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="6424" class="nv lt iq oo b gy os ot l ou ov">svc_clf_search = HalvingGridSearchCV(svc_clf, svc_param_grid, cv=splits, factor=2, scoring='accuracy', verbose=3)</span></pre><p id="8a96" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">等分网格搜索cv发现<strong class="ky ir"> C=100 </strong>是最佳参数。而最好的准确率是<strong class="ky ir"> 85.79% </strong>。所以最好的估计是这样的，</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="614f" class="nv lt iq oo b gy os ot l ou ov">LinearSVC(C=100.0, dual=False, max_iter=3000)</span></pre><p id="aa46" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">既然我们需要尽量减少比赛中的木材损耗。我们需要一个好的预测概率。校准的分类器可用于获得良好的预测概率。</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="7eb4" class="nv lt iq oo b gy os ot l ou ov">svc_calibrated = CalibratedClassifierCV(base_estimator=svc_clf_model, method="sigmoid", cv=splits)</span></pre><p id="6553" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在对概率模型进行校准之后。我预测了测试数据的概率，并在Kaggle上提交。Kaggle提交的公共领导委员会分数为<strong class="ky ir"> 0.36980 </strong>。<strong class="ky ir"> </strong>考虑到模型假设线性可分性，非常好。</p><h2 id="4288" class="nv lt iq bd lu nw nx dn ly ny nz dp mc lf oa ob me lj oc od mg ln oe of mi og bi translated">随机森林</h2><p id="1e8b" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">你知道Quora本身使用随机森林来解决这个问题。或者至少他们在2017年6月第一次在Kaggle上发布比赛时是这样做的。</p><p id="a9eb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">与之前相同，我们使用以下参数网格对分网格搜索cv:</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="15fe" class="nv lt iq oo b gy os ot l ou ov">rf_param_grid = {</span><span id="ca89" class="nv lt iq oo b gy ow ot l ou ov">    'n_estimators':[200, 500, 800],</span><span id="0463" class="nv lt iq oo b gy ow ot l ou ov">    'min_samples_split':[5, 15],</span><span id="5b2e" class="nv lt iq oo b gy ow ot l ou ov">    'max_depth': [70, 150, None]</span><span id="8546" class="nv lt iq oo b gy ow ot l ou ov">}</span></pre><p id="01a3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其余的参数是随机森林分类器的默认值。</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="ed6b" class="nv lt iq oo b gy os ot l ou ov">rf_clf = RandomForestClassifier()</span></pre><p id="af6f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们已经像以前一样使用了非常相似的等分网格搜索cv，</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="a3be" class="nv lt iq oo b gy os ot l ou ov">rf_clf_search = HalvingGridSearchCV(rf_clf, rf_param_grid, cv=splits, factor=2, scoring='accuracy', verbose=3)</span></pre><p id="837a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">等分网格搜索cv发现<strong class="ky ir"> {'max_depth': 150，' min_samples_split': 5，' n_estimators': 800} </strong>是最佳参数。而最好的准确率是<strong class="ky ir"> 90.53% </strong>。因此与SVM相比，精确度提高了5%。最佳估计看起来是这样的，</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="5707" class="nv lt iq oo b gy os ot l ou ov">RandomForestClassifier(max_depth=150, min_samples_split=5, n_estimators=800)</span></pre><p id="11e4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，在这一点上，我应该使用校准，但因为它已经花了很多时间，我跳过了它。我应该使用贝叶斯优化技术😞。</p><p id="15c8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Kaggle提交的公共领导委员会得分为<strong class="ky ir"> 0.32372 </strong>，略高于SVC。我期望对数损失少一点，但请记住，我们还没有进行校准(由于时间限制)。我们将尝试使用XGBoost——ka ggle竞赛中ml模型的圣杯——做得更好。</p><h2 id="a5d2" class="nv lt iq bd lu nw nx dn ly ny nz dp mc lf oa ob me lj oc od mg ln oe of mi og bi translated">XGBoost</h2><p id="2bbb" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">由于时间和系统配置的限制，我决定使用200000个数据点来估计一些参数。<br/>起初，我使用Optuna进行超参数调优，但它有一些问题，因为它在试用后没有释放内存。所以系统试了几次就崩溃了。<br/>后来，我决定使用远视进行调优。</p><p id="3391" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">用HyperOpt，我只调了<strong class="ky ir"> max_depth </strong>和<strong class="ky ir"> learning_rate </strong>。这不是一个微调，因为我只用了5次试验。但它给出了一个粗略的概念。</p><p id="578b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我选择以下参数对整个数据进行模型训练，</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="99a6" class="nv lt iq oo b gy os ot l ou ov">params = dict(</span><span id="cc52" class="nv lt iq oo b gy ow ot l ou ov">    objective = "binary:logistic",</span><span id="c029" class="nv lt iq oo b gy ow ot l ou ov">    eval_metric = "logloss",</span><span id="ad24" class="nv lt iq oo b gy ow ot l ou ov">    booster = "gbtree",</span><span id="6c26" class="nv lt iq oo b gy ow ot l ou ov">    tree_method = "hist",</span><span id="0b1e" class="nv lt iq oo b gy ow ot l ou ov">    grow_policy = "lossguide",</span><span id="3ba5" class="nv lt iq oo b gy ow ot l ou ov">    max_depth = 4,</span><span id="110f" class="nv lt iq oo b gy ow ot l ou ov">    eta = 0.14</span><span id="fb94" class="nv lt iq oo b gy ow ot l ou ov">)</span></pre><p id="d3e5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">目标= "二进制:逻辑"</strong>因为我们试图得到概率。我使用了<strong class="ky ir"> tree_method = "hist" </strong>进行快速训练。<strong class="ky ir">grow _ policy = " loss guide "</strong>灵感来自LightGBM，精度更高。</p><p id="fa14" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> num_boost_round </strong>设置为600，<strong class="ky ir"> early_stopping_rounds </strong>为20。</p><p id="8397" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">Kaggle提交的大众领先板得分为<strong class="ky ir"> 0.32105 </strong>，略好于其他车型。我期待比这更好的结果。这可以通过对超参数进行更多的微调来实现。XGBoost有大量的超参数<a class="ae kv" href="https://xgboost.readthedocs.io/en/latest/parameter.html" rel="noopener ugc nofollow" target="_blank">https://xgboost.readthedocs.io/en/latest/parameter.html</a></p><h2 id="f1bf" class="nv lt iq bd lu nw nx dn ly ny nz dp mc lf oa ob me lj oc od mg ln oe of mi og bi translated">另一个XGBoost</h2><p id="be4d" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">我对XGBoost模型的结果不满意，所以我决定凭直觉调整参数。</p><p id="0c01" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我做的第一件事是通过删除重复行来消除过采样数据。</p><p id="8905" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这一次我添加了一些参数来更好地概括，</p><pre class="kg kh ki kj gt on oo op oq aw or bi"><span id="c504" class="nv lt iq oo b gy os ot l ou ov">params = dict(</span><span id="2e46" class="nv lt iq oo b gy ow ot l ou ov">    objective = "binary:logistic",</span><span id="d4fa" class="nv lt iq oo b gy ow ot l ou ov">    eval_metric = "logloss",</span><span id="9c8b" class="nv lt iq oo b gy ow ot l ou ov">    booster = "gbtree",</span><span id="249f" class="nv lt iq oo b gy ow ot l ou ov">    tree_method = "hist",</span><span id="f118" class="nv lt iq oo b gy ow ot l ou ov">    grow_policy = "lossguide",</span><span id="fef8" class="nv lt iq oo b gy ow ot l ou ov">    max_depth = 4,</span><span id="4da1" class="nv lt iq oo b gy ow ot l ou ov">    eta = 0.15,</span><span id="b064" class="nv lt iq oo b gy ow ot l ou ov">    subsample = .8,</span><span id="af70" class="nv lt iq oo b gy ow ot l ou ov">    colsample_bytree = .8,</span><span id="3a8c" class="nv lt iq oo b gy ow ot l ou ov">    reg_lambda = 1,</span><span id="2158" class="nv lt iq oo b gy ow ot l ou ov">    reg_alpha = 1</span><span id="923e" class="nv lt iq oo b gy ow ot l ou ov">)</span></pre><p id="38ea" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另外，我把助推回合数减少到了500。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/02726a1d643751cb0b21c33f9e09c045.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sqk8L10gLSJVvNda"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@fznsr_?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Fauzan Saari </a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="ec7a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">🥁瞧！我们有赢家了。<strong class="ky ir">本次提交的公众LB分数为0.28170 </strong>。<br/>这似乎是一个非常好的结果。</p><h1 id="b44a" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">最后的想法</h1><p id="98a8" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">我从这个案例中学到了很多。我走了一些捷径，要么是因为系统配置限制，要么是时间限制。</p><p id="d4f5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我还亲身经历了机器学习不仅仅是建立模型，在此之前的步骤需要更多的时间。超参数调整可以自动进行，但像特征提取或决定使用什么特征这样的事情需要手动完成。</p><p id="bce0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我花了将近两周的时间😅有一半的时间我在等待一些执行的完成。所以我认为，如果你有资源密集型任务，使用亚马逊SageMaker这样的东西是个好主意。</p><p id="8b17" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">未来可以尝试一些基于深度学习的模型。</p><h1 id="3d23" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">参考</h1><p id="b1a2" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf na lh li lj nb ll lm ln nc lp lq lr ij bi translated">a.<a class="ae kv" href="https://appliedroots.com/" rel="noopener ugc nofollow" target="_blank">https://appliedroots.com/</a></p></div></div>    
</body>
</html>