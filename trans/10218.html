<html>
<head>
<title>Comparison of the collect_list() and collect_set() functions in Spark with Scala</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Spark和Scala中collect_list()和collect_set()函数的比较</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/comparison-of-the-collect-list-and-collect-set-functions-in-spark-with-scala-10b3ba1b74b6?source=collection_archive---------4-----------------------#2021-09-28">https://towardsdatascience.com/comparison-of-the-collect-list-and-collect-set-functions-in-spark-with-scala-10b3ba1b74b6?source=collection_archive---------4-----------------------#2021-09-28</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="556c" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph">利用Scala实现大数据</h2><div class=""/><div class=""><h2 id="0402" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">Scala编程中collect_list()和collect_set()的区别</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/ec031a2b299f1cfc1dbdaa3cdb5cffa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GmIVMJHB-g10OfodoOqh-Q.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><a class="ae le" href="https://pixabay.com/photos/apple-fruit-selection-especially-1632919/" rel="noopener ugc nofollow" target="_blank">信号源</a></p></figure><p id="70f2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">可以在不同的编程语言上用不同的版本创建数组格式的变量。Spark上的Scala语言有两个不同的数组创建函数。这些被称为<code class="fe mb mc md me b">collect_list()</code>和<code class="fe mb mc md me b">collect_set()</code>函数，主要应用于生成的数据帧上的数组类型的列，通常在窗口操作之后。</p><p id="6654" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在本文中，这两个函数之间的差异将通过相应的实例来解释。主要目的是比较和强调以下两个函数之间的差异，因为它们可能会在误导的情况下使用。</p><p id="d1af" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">作为编程语言，Scala被选择用于Spark 3.1.1。您可以通过使用PySpark语言来实践类似的方法。</p><p id="31b7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">出于测试目的，可以生成如下示例结构类型化数据帧。在代码片段中，通过添加相应的内容来创建表的行。在创建行之后，我们可以将这些列添加到我们的数据模式中，方法是使用匹配的数据类型对它们进行格式化，比如对<em class="mf">日</em>列使用<em class="mf"> IntegerType </em>，对<em class="mf">名</em>列使用<em class="mf"> StringType </em>。</p><pre class="kp kq kr ks gt mg me mh mi aw mj bi"><span id="d7f1" class="mk ml iq me b gy mm mn l mo mp">import org.apache.spark.sql._<br/>import org.apache.spark.sql.types._</span><span id="7412" class="mk ml iq me b gy mq mn l mo mp">val dataFrame = Seq(<br/>    Row(1,"Employee_1", "Kafka"), <br/>    Row(2,"Employee_1", "Kibana"), <br/>    Row(3,"Employee_1", "Hadoop"), <br/>    Row(4,"Employee_1", "Hadoop"),  <br/>    Row(4,"Employee_1", "Hadoop"),<br/>    Row(5,"Employee_2", "Spark"), <br/>    Row(6,"Employee_2", "Scala"),  <br/>    Row(7,"Employee_2", "SageMaker"), <br/>    Row(7,"Employee_2", "SageMaker"),<br/>    Row(8,"Employee_3", "GCP"), <br/>    Row(9,"Employee_3", "AWS"),<br/>    Row(10,"Employee_3", "Azure"),<br/>    Row(11,"Employee_4", null)<br/>  )<br/>val dataFrameSchema = new StructType().add("day", IntegerType).add("name", StringType).add("toolSet", StringType)</span><span id="8a8a" class="mk ml iq me b gy mq mn l mo mp">val array_dataframe = spark.createDataFrame(spark.sparkContext.parallelize(dataFrame),dataFrameSchema)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mr"><img src="../Images/bd5d0563cae515ead24731d831b2eaac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MYFNI8ENIulCeOcA0CwA9g.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">产出1，归作者所有</p></figure><p id="86d5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在dataframe创建阶段之后，我们可以打印数据及其内容的模式。</p><pre class="kp kq kr ks gt mg me mh mi aw mj bi"><span id="fa62" class="mk ml iq me b gy mm mn l mo mp">array_dataframe.printSchema()<br/>array_dataframe.show(false)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/5e5be9226628511d0dd2219240b9885c.png" data-original-src="https://miro.medium.com/v2/resize:fit:808/format:webp/1*Tko6D_ZLPmVj0rpr62Otbw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">产出2，归作者所有</p></figure><p id="2dd3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们创建了包含三列的数据框架，分别是<em class="mf">日</em>，员工的<em class="mf">名</em>，以及他们的<em class="mf">工具集</em>。</p><p id="a6e1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们的下一步将通过使用<code class="fe mb mc md me b">collect_list()</code>和<code class="fe mb mc md me b">collect_set()</code>函数为这些员工的相应工具集创建数组。</p><h1 id="5b7c" class="mt ml iq bd mu mv mw mx my mz na nb nc kf nd kg ne ki nf kj ng kl nh km ni nj bi translated">collect_list()</h1><p id="694c" class="pw-post-body-paragraph lf lg iq lh b li nk ka lk ll nl kd ln lo nm lq lr ls nn lu lv lw no ly lz ma ij bi translated">第一个数组创建函数叫做<code class="fe mb mc md me b">collect_list()</code>。它既可以用来对值进行分组，也可以借助窗口操作对它们进行聚合。</p><p id="b636" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在下面的脚本中，有一个名为<em class="mf">名称</em>和<em class="mf">工具集</em>的列。当深入观察时，员工1有三个工具，有两个副本，员工2有三个工具，有一个副本。对于这个具体的例子，我们可以按雇员分组，并将所有的<em class="mf">工具集</em>收集到一个数组中。可以通过首先对员工进行分组并在<em class="mf">工具集</em>上进行聚合来实现。</p><p id="f859" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">collect list的关键亮点是函数<strong class="lh ja">通过保持项目的顺序将所有重复的值</strong>保存在数组中。</p><pre class="kp kq kr ks gt mg me mh mi aw mj bi"><span id="8dbd" class="mk ml iq me b gy mm mn l mo mp">val collect_list_df = array_dataframe.groupBy("name").agg(<strong class="me ja">collect_list</strong>("toolSet").as("toolSet"))</span><span id="7503" class="mk ml iq me b gy mq mn l mo mp">collect_list_df.printSchema()<br/>collect_list_df.show(false)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi np"><img src="../Images/9efe24bd2030b79b8e11042f9f50623d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1396/format:webp/1*_JS6GMQjJgzOoAlyzV_hmg.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">产出3，归作者所有</p></figure><p id="859e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">当我们希望通过保持项目<em class="mf">(日期、时间戳、id等)的顺序来消除不同的值时。)</em>，在应用<strong class="lh ja"> collect_list </strong>函数之前，我们可以先使用<strong class="lh ja"> array_distinct() </strong>函数。在下面的例子中，我们可以清楚地看到元素的初始顺序被保留了。例如，Employee_1，我们看到Kafka、Kibana、Hadoop项目的初始顺序在应用了<code class="fe mb mc md me b">collect_list()</code>操作后保持为Kafka、Kibana、Hadoop，没有丢失序列。</p><pre class="kp kq kr ks gt mg me mh mi aw mj bi"><span id="6f3f" class="mk ml iq me b gy mm mn l mo mp">val collect_list_df = array_dataframe.groupBy("name").agg(<strong class="me ja">array_distinct</strong>(<strong class="me ja">collect_list</strong>("toolSet")).as("toolSet"))</span><span id="fc56" class="mk ml iq me b gy mq mn l mo mp">collect_list_df.printSchema()<br/>collect_list_df.show(false)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/6b87e8c13e4280c4f7cec8f425f13759.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/1*Rn4jTg0VE4VK0iKORXW0XQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">产出4，归作者所有</p></figure><h1 id="1475" class="mt ml iq bd mu mv mw mx my mz na nb nc kf nd kg ne ki nf kj ng kl nh km ni nj bi translated">collect_set()</h1><p id="344a" class="pw-post-body-paragraph lf lg iq lh b li nk ka lk ll nl kd ln lo nm lq lr ls nn lu lv lw no ly lz ma ij bi translated">第二个数组创建函数叫做<code class="fe mb mc md me b">collect_set()</code>。它既可以用来对值进行分组，也可以借助窗口操作对它们进行聚合。</p><p id="69e5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在下面的脚本中，有一个名为<em class="mf"> name </em>和<em class="mf"> toolSet </em>的列。当深入观察时，员工1有三个工具，有两个副本，员工2有三个工具，有一个副本。对于这个特定的例子，我们可以按雇员分组，并将所有的<em class="mf">工具集</em>收集到一个数组中。可以通过首先对员工进行分组并在<em class="mf">工具集</em>上进行聚合来实现。</p><p id="2ccb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">收集列表的关键亮点是函数<strong class="lh ja">消除了数组中的重复值</strong>。然而，它<strong class="lh ja">并不能保证数组中项目的顺序</strong>。通过查看Employee_1，我们看到在应用了<code class="fe mb mc md me b">collect_set()</code>操作之后，Kafka、Kibana、Hadoop的项目的初始顺序被更改为Kibana、Kafka、Hadoop。</p><pre class="kp kq kr ks gt mg me mh mi aw mj bi"><span id="6bd1" class="mk ml iq me b gy mm mn l mo mp">val collect_set_df = array_dataframe.groupBy("name").agg(<strong class="me ja">collect_set</strong>("toolSet").as("toolSet"))</span><span id="f1f4" class="mk ml iq me b gy mq mn l mo mp">collect_set_df.show(false)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/219c0f42a2ba5527d8ace5dc4dd28833.png" data-original-src="https://miro.medium.com/v2/resize:fit:596/format:webp/1*F0UBoqAQDei2b4xt0nsYEA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">产出5，归作者所有</p></figure><h1 id="c1b1" class="mt ml iq bd mu mv mw mx my mz na nb nc kf nd kg ne ki nf kj ng kl nh km ni nj bi translated">结论</h1><p id="fb22" class="pw-post-body-paragraph lf lg iq lh b li nk ka lk ll nl kd ln lo nm lq lr ls nn lu lv lw no ly lz ma ij bi translated">在Spark中，我们可以使用<code class="fe mb mc md me b">collect_list()</code>和<code class="fe mb mc md me b">collect_set()</code>函数来生成具有不同视角的数组。</p><p id="4557" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><code class="fe mb mc md me b">collect_list()</code>操作不负责统一数组列表。它按照元素的现有顺序填充所有元素，并且不删除任何重复的元素。</p><p id="fb15" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">另一方面，<code class="fe mb mc md me b">collect_set()</code>操作确实消除了重复；但是，它不能保存数组中项目的现有顺序。</p><p id="90dd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了统一数组中的元素，我们可以在<strong class="lh ja"> collect_list() </strong>函数之前轻松使用<strong class="lh ja"> array_distinct() </strong>内置函数来消除重复值，而不会丢失数组中元素的顺序。</p></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><p id="bcf0" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">非常感谢您的提问和评论！</p></div><div class="ab cl ns nt hu nu" role="separator"><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx ny"/><span class="nv bw bk nw nx"/></div><div class="ij ik il im in"><h1 id="ce6a" class="mt ml iq bd mu mv nz mx my mz oa nb nc kf ob kg ne ki oc kj ng kl od km ni nj bi translated">参考</h1><ol class=""><li id="eeaa" class="oe of iq lh b li nk ll nl lo og ls oh lw oi ma oj ok ol om bi translated"><a class="ae le" href="https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions$.html#collect_list(columnName:String):org.apache.spark.sql.Column" rel="noopener ugc nofollow" target="_blank">Spark 3中ScalaDoc的收集集合和收集列表操作</a></li><li id="7eee" class="oe of iq lh b li on ll oo lo op ls oq lw or ma oj ok ol om bi translated"><a class="ae le" href="https://spark.apache.org/docs/latest/api/sql/index.html" rel="noopener ugc nofollow" target="_blank"> Spark内置功能</a></li><li id="ebf5" class="oe of iq lh b li on ll oo lo op ls oq lw or ma oj ok ol om bi translated"><a class="ae le" href="https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/1023043053387187/4464261896877850/2840265927289860/latest.html" rel="noopener ugc nofollow" target="_blank">来自Collect_List的数据块示例</a></li></ol></div></div>    
</body>
</html>