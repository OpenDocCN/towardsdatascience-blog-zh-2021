<html>
<head>
<title>Demystified: Wasserstein GAN with Gradient Penalty(WGAN-GP)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">揭秘:Wasserstein GAN与梯度惩罚(WGAN-GP)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/demystified-wasserstein-gan-with-gradient-penalty-ba5e9b905ead?source=collection_archive---------5-----------------------#2021-10-02">https://towardsdatascience.com/demystified-wasserstein-gan-with-gradient-penalty-ba5e9b905ead?source=collection_archive---------5-----------------------#2021-10-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f84c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">什么是梯度惩罚？为什么比渐变裁剪好？如何实施梯度惩罚？</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi kf"><img src="../Images/be95a79952f0f69cef26cbc762216daa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*ieyAKSxgJGqX9lktL_ujnA.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图1(左)使用权重剪裁时的梯度范数不是爆炸就是消失，不使用GP。(右)与GP不同，权重剪裁将权重推向两个值。图像来源:[1]</p></figure><p id="7f04" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在这篇文章中，我们将研究Wasserstein GANs的梯度惩罚。虽然最初的Wasserstein GAN[2]提高了训练的稳定性，但仍然存在产生差的样本或不能收敛的情况。概括一下，WGAN的成本函数是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ln"><img src="../Images/02b65a6b7407858cb89950f5c4c31ac7.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*OScRv3BjfaCrd-o6z3IcUg.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">情商。1: WGAN值函数。</p></figure><p id="154c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">其中<em class="lo"> f </em>是<strong class="kt ir"> </strong> 1-Lipschitz连续。WGAN出现的问题主要是因为用于在critical上加强Lipschitz连续性的权重裁剪方法。WGAN-GP用对critic的梯度范数的约束来代替权重裁剪，以加强Lipschitz连续性。这允许比WGAN更稳定的网络训练，并且需要非常少的超参数调整。WGAN-GP，这篇文章建立在Wasserstein GANs的基础上，这已经在<a class="ae lp" href="https://asankar96.medium.com/list/demystified-deep-learning-1273a227565c" rel="noopener"> <em class="lo">揭秘系列</em> </a>的上一篇文章中讨论过了。查看下面的帖子来了解WGAN。</p><div class="lq lr gp gr ls lt"><a rel="noopener follow" target="_blank" href="/demystified-wasserstein-gans-wgan-f835324899f4"><div class="lu ab fo"><div class="lv ab lw cl cj lx"><h2 class="bd ir gy z fp ly fr fs lz fu fw ip bi translated">揭秘:瓦瑟斯坦·甘斯(WGAN)</h2><div class="ma l"><h3 class="bd b gy z fp ly fr fs lz fu fw dk translated">什么是瓦瑟斯坦距离？用Wasserstein距离训练GANs背后的直觉是什么？怎么样…</h3></div><div class="mb l"><p class="bd b dl z fp ly fr fs lz fu fw dk translated">towardsdatascience.com</p></div></div><div class="mc l"><div class="md l me mf mg mc mh kl lt"/></div></div></a></div><h2 id="3791" class="mi mj iq bd mk ml mm dn mn mo mp dp mq la mr ms mt le mu mv mw li mx my mz na bi translated">报表1</h2><blockquote class="nb"><p id="f6d5" class="nc nd iq bd ne nf ng nh ni nj nk lm dk translated">可微的最优1-Lipschitz函数，<strong class="ak"> <em class="nl"> f* </em> </strong>最小化等式。1在ℙr和ℙg.下几乎处处都有单位梯度范数</p></blockquote><p id="3293" class="pw-post-body-paragraph kr ks iq kt b ku nm jr kw kx nn ju kz la no lc ld le np lg lh li nq lk ll lm ij bi translated">ℙr和ℙg分别是真实和虚假的分布。陈述1的证明可以在[1]中找到。</p><h1 id="d0fc" class="nr mj iq bd mk ns nt nu mn nv nw nx mq jw ny jx mt jz nz ka mw kc oa kd mz ob bi translated">渐变剪辑的问题</h1><h2 id="4c7b" class="mi mj iq bd mk ml mm dn mn mo mp dp mq la mr ms mt le mu mv mw li mx my mz na bi translated">容量未充分利用</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/caeb64eace98b64bb666a87363018a6c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*4hWCkkakgFiq3NU7g9WDbQ.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图WGAN critic(上)使用梯度裁剪(下)使用梯度惩罚学习的值表面。图像来源:[1]</p></figure><blockquote class="nb"><p id="06d8" class="nc nd iq bd ne nf od oe of og oh lm dk translated">使用权重剪辑来加强k-Lipschitz约束导致评论家学习非常简单的函数。</p></blockquote><p id="cd88" class="pw-post-body-paragraph kr ks iq kt b ku nm jr kw kx nn ju kz la no lc ld le np lg lh li nq lk ll lm ij bi translated">从陈述1中，我们知道最优评论家的梯度范数几乎在ℙr和ℙg.的任何地方都是1。在权重剪辑设置中，评论家试图获得其最大梯度范数<em class="lo"> k </em> <strong class="kt ir"> <em class="lo">，</em> </strong>，并最终学习简单的函数。</p><p id="64b8" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">图2显示了这种效果。批评者被训练为收敛于固定生成的distribution(ℙg)作为真实的distribution(ℙr)+单位高斯噪声。我们可以清楚地看到，用权重裁剪训练的评论家最终学习简单的函数，并且不能捕捉更高的矩，而用梯度惩罚训练的评论家没有遭受这个问题。</p><h2 id="1ad1" class="mi mj iq bd mk ml mm dn mn mo mp dp mq la mr ms mt le mu mv mw li mx my mz na bi translated">爆炸和消失渐变</h2><blockquote class="nb"><p id="3d47" class="nc nd iq bd ne nf ng nh ni nj nk lm dk translated">权重约束和损失函数之间的相互作用使得WGAN的训练变得困难，并导致爆炸或消失梯度。</p></blockquote><p id="1fa3" class="pw-post-body-paragraph kr ks iq kt b ku nm jr kw kx nn ju kz la no lc ld le np lg lh li nq lk ll lm ij bi translated">这可以在图1(左)中清楚地看到，对于不同的削波值，评论家的权重爆炸或消失。图1(右)还示出了梯度剪切将评论家的权重推至两个极端剪切值。另一方面，受过梯度惩罚训练的批评家不会受到这些问题的困扰。</p><h1 id="de7b" class="nr mj iq bd mk ns nt nu mn nv nw nx mq jw ny jx mt jz nz ka mw kc oa kd mz ob bi translated">梯度惩罚</h1><blockquote class="nb"><p id="ae67" class="nc nd iq bd ne nf ng nh ni nj nk lm dk translated">梯度惩罚的思想是实施一个约束，使得批评家的输出相对于输入的梯度具有单位范数(陈述1)。</p></blockquote><p id="702e" class="pw-post-body-paragraph kr ks iq kt b ku nm jr kw kx nn ju kz la no lc ld le np lg lh li nq lk ll lm ij bi translated">作者提出了这种约束的软版本，对样本上的梯度范数进行惩罚<strong class="kt ir"> x̂ </strong> ∈ ℙ <strong class="kt ir"> x̂ </strong>。新的目标是</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="oj ok di ol bf om"><div class="gh gi oi"><img src="../Images/e89ec1cdff073004f6d2aa28030638fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OMOCwiwLF4WRXmQ174hUcQ.png"/></div></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">情商。2:批评家损失函数</p></figure><p id="bd0b" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">在Eq中。2总和左边的项是原始临界损失，总和右边的项是梯度损失。</p><p id="b6ce" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">ℙ <strong class="kt ir"> x̂ </strong>是通过沿着真实分布和生成分布ℙr和ℙg.之间的直线均匀采样而获得的分布。这样做是因为最佳评论家在从ℙr和ℙg.耦合的样本之间具有单位梯度范数的直线</p><p id="c3f1" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">λ，惩罚系数用于加权梯度惩罚项。在这篇论文中，作者为他们所有的实验设定λ=10。</p><p id="26b2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">batch normalization不再用于critic</strong>，因为Batch norm将一批输入映射到一批输出。在我们的案例<strong class="kt ir">中，我们希望能够找到每个输出相对于各自输入的梯度。</strong></p><h1 id="b903" class="nr mj iq bd mk ns nt nu mn nv nw nx mq jw ny jx mt jz nz ka mw kc oa kd mz ob bi translated">密码</h1><h2 id="345c" class="mi mj iq bd mk ml mm dn mn mo mp dp mq la mr ms mt le mu mv mw li mx my mz na bi translated">梯度惩罚</h2><p id="6f0f" class="pw-post-body-paragraph kr ks iq kt b ku on jr kw kx oo ju kz la op lc ld le oq lg lh li or lk ll lm ij bi translated">梯度惩罚的实现如下所示。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="os ot l"/></div></figure><h2 id="e7ae" class="mi mj iq bd mk ml mm dn mn mo mp dp mq la mr ms mt le mu mv mw li mx my mz na bi translated">WGAN-GP</h2><p id="57d1" class="pw-post-body-paragraph kr ks iq kt b ku on jr kw kx oo ju kz la op lc ld le oq lg lh li or lk ll lm ij bi translated">训练WGAN-GP模型的代码可在此处找到:</p><div class="lq lr gp gr ls lt"><a href="https://github.com/aadhithya/gan-zoo-pytorch" rel="noopener  ugc nofollow" target="_blank"><div class="lu ab fo"><div class="lv ab lw cl cj lx"><h2 class="bd ir gy z fp ly fr fs lz fu fw ip bi translated">GitHub-aadhithya/GAN-zoo-py torch:GAN实现的动物园</h2><div class="ma l"><h3 class="bd b gy z fp ly fr fs lz fu fw dk translated">GAN实现的动物园。在GitHub上创建一个帐户，为aadhithya/gan-zoo-pytorch的发展做出贡献。</h3></div><div class="mb l"><p class="bd b dl z fp ly fr fs lz fu fw dk translated">github.com</p></div></div></div></a></div><h1 id="5057" class="nr mj iq bd mk ns nt nu mn nv nw nx mq jw ny jx mt jz nz ka mw kc oa kd mz ob bi translated">输出</h1><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/fb974e6540769451731eff0fca7aa876.png" data-original-src="https://miro.medium.com/v2/resize:fit:884/format:webp/1*sagQ0eASN_ODqJzEKQs6jw.png"/></div><p class="kn ko gj gh gi kp kq bd b be z dk translated">图3:WGAN-GP模型生成的图像。请注意，这些结果是早期结果，一旦确认模型正在按预期进行训练，就停止训练。图片来源:[3]</p></figure><p id="e9af" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">图3显示了训练WGAN-GP的一些早期结果。<strong class="kt ir">请注意，图3中的图像是早期结果，一旦确认模型正在如预期那样训练，就停止训练。该模型没有被训练收敛。</strong></p><h1 id="63a0" class="nr mj iq bd mk ns nt nu mn nv nw nx mq jw ny jx mt jz nz ka mw kc oa kd mz ob bi translated">结论</h1><p id="4c29" class="pw-post-body-paragraph kr ks iq kt b ku on jr kw kx oo ju kz la op lc ld le oq lg lh li or lk ll lm ij bi translated">Wasserstein GANs在训练生成性对抗网络方面提供了急需的稳定性。然而，使用梯度裁剪导致各种问题，如爆炸和消失梯度等。梯度惩罚约束没有这些问题，因此与原始WGAN相比，允许更容易的优化和收敛。这篇文章着眼于这些问题，介绍了梯度惩罚约束，并展示了如何使用PyTorch实现梯度惩罚。最后提供了训练WGAN-GP模型的代码以及一些早期输出。</p><p id="0e64" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">如果你喜欢这篇文章，可以考虑关注作者，<a class="ov ow ep" href="https://medium.com/u/82053676fe58?source=post_page-----ba5e9b905ead--------------------------------" rel="noopener" target="_blank"> Aadhithya Sankar </a>。</p></div><div class="ab cl ox oy hu oz" role="separator"><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc"/></div><div class="ij ik il im in"><h1 id="b006" class="nr mj iq bd mk ns pe nu mn nv pf nx mq jw pg jx mt jz ph ka mw kc pi kd mz ob bi translated">参考</h1><p id="2c03" class="pw-post-body-paragraph kr ks iq kt b ku on jr kw kx oo ju kz la op lc ld le oq lg lh li or lk ll lm ij bi translated">[1] Gulrajani，Ishaan等，“改善wasserstein gans的训练”<em class="lo"> arXiv预印本arXiv:1704.00028 </em> (2017)。</p><p id="4c34" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">[2] Arjovsky、Martin、Soumith Chintala和Léon Bottou。"沃瑟斯坦生成性对抗网络."<em class="lo">机器学习国际会议</em>。PMLR，2017。</p><p id="6b14" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">[3]<a class="ae lp" href="https://github.com/aadhithya/gan-zoo-pytorch" rel="noopener ugc nofollow" target="_blank">https://github.com/aadhithya/gan-zoo-pytorch</a></p></div><div class="ab cl ox oy hu oz" role="separator"><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc pd"/><span class="pa bw bk pb pc"/></div><div class="ij ik il im in"><h1 id="b9a6" class="nr mj iq bd mk ns pe nu mn nv pf nx mq jw pg jx mt jz ph ka mw kc pi kd mz ob bi translated">作者的更多帖子</h1><p id="add9" class="pw-post-body-paragraph kr ks iq kt b ku on jr kw kx oo ju kz la op lc ld le oq lg lh li or lk ll lm ij bi translated">如果你喜欢这篇文章，你可能也会喜欢:</p><div class="lq lr gp gr ls lt"><a rel="noopener follow" target="_blank" href="/principal-component-analysis-part-1-the-different-formulations-6508f63a5553"><div class="lu ab fo"><div class="lv ab lw cl cj lx"><h2 class="bd ir gy z fp ly fr fs lz fu fw ip bi translated">主成分分析第1部分:不同的公式。</h2><div class="ma l"><h3 class="bd b gy z fp ly fr fs lz fu fw dk translated">什么是主成分分析？PCA的最大方差和最小误差公式有哪些？我们如何…</h3></div><div class="mb l"><p class="bd b dl z fp ly fr fs lz fu fw dk translated">towardsdatascience.com</p></div></div><div class="mc l"><div class="pj l me mf mg mc mh kl lt"/></div></div></a></div><div class="lq lr gp gr ls lt"><a rel="noopener follow" target="_blank" href="/a-primer-on-atrous-convolutions-and-depth-wise-separable-convolutions-443b106919f5"><div class="lu ab fo"><div class="lv ab lw cl cj lx"><h2 class="bd ir gy z fp ly fr fs lz fu fw ip bi translated">阿特鲁卷积和深度可分卷积的初步研究</h2><div class="ma l"><h3 class="bd b gy z fp ly fr fs lz fu fw dk translated">什么是萎缩/扩张和深度方向可分卷积？与标准卷积有何不同？什么…</h3></div><div class="mb l"><p class="bd b dl z fp ly fr fs lz fu fw dk translated">towardsdatascience.com</p></div></div><div class="mc l"><div class="pk l me mf mg mc mh kl lt"/></div></div></a></div><div class="lq lr gp gr ls lt"><a rel="noopener follow" target="_blank" href="/real-time-artwork-generation-using-deep-learning-a33a2084ae98"><div class="lu ab fo"><div class="lv ab lw cl cj lx"><h2 class="bd ir gy z fp ly fr fs lz fu fw ip bi translated">使用深度学习的实时艺术品生成</h2><div class="ma l"><h3 class="bd b gy z fp ly fr fs lz fu fw dk translated">用于任意内容样式图像对之间的样式转换的自适应实例标准化(AdaIN)。</h3></div><div class="mb l"><p class="bd b dl z fp ly fr fs lz fu fw dk translated">towardsdatascience.com</p></div></div><div class="mc l"><div class="pl l me mf mg mc mh kl lt"/></div></div></a></div></div></div>    
</body>
</html>