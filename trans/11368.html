<html>
<head>
<title>Understanding Object Detection</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解对象检测</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-object-detection-1d810164b8a6?source=collection_archive---------13-----------------------#2021-11-08">https://towardsdatascience.com/understanding-object-detection-1d810164b8a6?source=collection_archive---------13-----------------------#2021-11-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="28b4" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">数据科学家和ML工程师的学习材料</h2><div class=""/><div class=""><h2 id="bfcd" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">利用深度学习检测图像对象(R-CNN、SSD、YOLO)</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/13c245eb82147e70446808869bf79dc3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*BAN0IjY1e7-QMtye"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">来源<a class="ae lh" href="https://unsplash.com/photos/qcqmS0JG58Q" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><blockquote class="li lj lk"><p id="4497" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated"><em class="it">想象一下谷歌照片:对于你所有的照片，你如何用物体来标记它们。要不要一个一个贴标签？</em></p><p id="33fb" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated"><em class="it">自动驾驶汽车怎么样？它们如何检测行人、汽车、交通灯和即将出现的障碍物？</em></p><p id="8cfd" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated"><em class="it">输入物体检测算法</em></p></blockquote><h1 id="2aff" class="mi mj it bd mk ml mm mn mo mp mq mr ms ki mt kj mu kl mv km mw ko mx kp my mz bi translated">从图像分类到物体检测</h1><p id="0898" class="pw-post-body-paragraph ll lm it lo b lp na kd lr ls nb kg lu nc nd lx ly ne nf mb mc ng nh mf mg mh im bi translated">近年来，图像分类获得了巨大的吸引力，特别是在CNN和破坏性应用程序(例如:自动驾驶汽车)中。2015年，ImageNet竞赛产生了比人类表现更好的RESNet。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/5bbeae954c54ce242681ba79dfa4935c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*bXn9ahXnCP-20USkzwK8JA.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">ResNet表现优于人类(作者注释)</p></figure><p id="273a" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu nc lw lx ly ne ma mb mc ng me mf mg mh im bi translated">虽然图像分类(例如:CNN堆栈)在对图像进行分类方面非常有效，但它在检测图像中的多个感兴趣对象以及了解它们在图像中的位置方面仍然存在局限性。</p><p id="b394" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu nc lw lx ly ne ma mb mc ng me mf mg mh im bi translated">物体检测比图像分类更进了一步，画出了一个或多个感兴趣物体的边界框。</p><p id="d120" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu nc lw lx ly ne ma mb mc ng me mf mg mh im bi translated">在本教程中，我们将学习使用R-CNN和YOLO快速检测物体的方法。</p><div class="nj nk gp gr nl nm"><a rel="noopener follow" target="_blank" href="/understanding-cnn-convolutional-neural-network-69fd626ee7d4"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd jd gy z fp nr fr fs ns fu fw jc bi translated">了解CNN(卷积神经网络)</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">你学习图像分类深度学习的第一步</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">towardsdatascience.com</p></div></div><div class="nv l"><div class="nw l nx ny nz nv oa lb nm"/></div></div></a></div><h1 id="b9bb" class="mi mj it bd mk ml mm mn mo mp mq mr ms ki mt kj mu kl mv km mw ko mx kp my mz bi translated">物体检测的一般步骤是什么？</h1><h2 id="4f9d" class="ob mj it bd mk oc od dn mo oe of dp ms nc og oh mu ne oi oj mw ng ok ol my iz bi translated">1.对象定位</h2><blockquote class="li lj lk"><p id="15d5" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated"><strong class="lo jd">哪里的</strong>是对象？</p></blockquote><p id="2513" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu nc lw lx ly ne ma mb mc ng me mf mg mh im bi translated"><strong class="lo jd">要定位对象，我们需要识别兴趣对象的焦点锚点。</strong>这来自区域提案，用于定制水平/垂直细长框。每个位置都有一个锚点。</p><p id="e04e" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu nc lw lx ly ne ma mb mc ng me mf mg mh im bi translated">我们创建一个模型来生成标记感兴趣区域的区域提议。我们指定边界框并注释代表这些特定区域提议的图片片段。这包括多种方法，例如:滑动窗口(强力)、选择性搜索(聚类)和区域提议网络(深度学习)。</p><h2 id="388e" class="ob mj it bd mk oc od dn mo oe of dp ms nc og oh mu ne oi oj mw ng ok ol my iz bi translated">2.对象分类</h2><blockquote class="li lj lk"><p id="96e8" class="ll lm ln lo b lp lq kd lr ls lt kg lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated"><strong class="lo jd">图像中的是什么</strong> <em class="it">？</em></p></blockquote><p id="2d4d" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu nc lw lx ly ne ma mb mc ng me mf mg mh im bi translated">我们将所有视觉特征提取到包围盒中，并在区域提议上评估每个包围盒。这包括核函数和直方图函数。</p><p id="9175" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu nc lw lx ly ne ma mb mc ng me mf mg mh im bi translated">对于每一个形状，我们将计算回归量，以检查锚如何偏离地面真相。我们还讨论了<strong class="lo jd">区域提议</strong>，它维护了由主干架构表示的兴趣对象(例如:CNN/VGG16/etc)</p><h2 id="16d8" class="ob mj it bd mk oc od dn mo oe of dp ms nc og oh mu ne oi oj mw ng ok ol my iz bi translated">3.非最大抑制</h2><p id="8f8a" class="pw-post-body-paragraph ll lm it lo b lp na kd lr ls nb kg lu nc nd lx ly ne nf mb mc ng nh mf mg mh im bi translated">我们寻找可能的包围盒，其捕获并集上的交集(IOU)内的锚。这允许我们将所有重叠的框减少到单个有代表性的边界框。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi om"><img src="../Images/90cd609aa739829ed7457ef552736073.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZcXX8qIbDu6uYcr-0Z890A.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">图像分类和对象检测(来源于作者)</p></figure><h1 id="0273" class="mi mj it bd mk ml mm mn mo mp mq mr ms ki mt kj mu kl mv km mw ko mx kp my mz bi translated"><strong class="ak">现有的物体检测操作有哪些？</strong></h1><h2 id="417b" class="ob mj it bd mk oc od dn mo oe of dp ms nc og oh mu ne oi oj mw ng ok ol my iz bi translated"><strong class="ak">美国有线电视新闻网</strong></h2><p id="5aa7" class="pw-post-body-paragraph ll lm it lo b lp na kd lr ls nb kg lu nc nd lx ly ne nf mb mc ng nh mf mg mh im bi translated">R-CNN通过提出<a class="ae lh" href="https://ivi.fnwi.uva.nl/isis/publications/2013/UijlingsIJCV2013/UijlingsIJCV2013.pdf" rel="noopener ugc nofollow" target="_blank">选择性搜索</a>从图像中提取区域(又名。区域提案)。选择搜索将1)生成子分割以生成候选区域，2)使用贪婪算法来组合相似区域，以及3)使用生成的区域和最终候选区域。</p><p id="e22b" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu nc lw lx ly ne ma mb mc ng me mf mg mh im bi translated">扭曲区域被馈送到CNN分类器，CNN分类器提取特征图以馈送到支持向量机(<a class="ae lh" rel="noopener" target="_blank" href="/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47"> SVM </a>)。如果对象存在于扭曲区域内，SVM进行分类。然后回归器将调整并设置边界框来表示区域建议。</p><p id="1aa9" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu nc lw lx ly ne ma mb mc ng me mf mg mh im bi translated"><strong class="lo jd">尽管R-CNN在确定代表性区域方面很有效，但它仍然效率低下，因为它需要花费大量时间来训练所有2000个扭曲区域。选择性算法也可能导致一代又一代的不良候选人，从而减慢整个过程。</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi on"><img src="../Images/30d0083892024eda2ed76320b5946ccb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*faYkOFuWSi66qWnxo5s4bA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">美国有线电视新闻网(来源于作者)</p></figure><h2 id="c50f" class="ob mj it bd mk oc od dn mo oe of dp ms nc og oh mu ne oi oj mw ng ok ol my iz bi translated"><strong class="ak">快速R-CNN(双摄)</strong></h2><p id="758f" class="pw-post-body-paragraph ll lm it lo b lp na kd lr ls nb kg lu nc nd lx ly ne nf mb mc ng nh mf mg mh im bi translated">你需要有多个CNN分类器。在2014年的论文中。这使用更快的选择搜索。快速R-CNN也被称为双镜头，因为它直接运行地区提案和CNN。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oo"><img src="../Images/02380bfad035e3f80431a28839a20ae4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W1dDYqLwR2mpJpH3Rx66SQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">快速R-CNN(来源于作者)</p></figure><h2 id="9a31" class="ob mj it bd mk oc od dn mo oe of dp ms nc og oh mu ne oi oj mw ng ok ol my iz bi translated"><strong class="ak">更快的RCNN </strong></h2><p id="7bad" class="pw-post-body-paragraph ll lm it lo b lp na kd lr ls nb kg lu nc nd lx ly ne nf mb mc ng nh mf mg mh im bi translated">由于快速RNN地区的建议可能是浪费，我们实际上可以运行地区的建议，使用要素地图作为输入。这更快也更直接，因为要素地图已经包含了感兴趣的对象。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi op"><img src="../Images/7a52936eb9cf513c1ad8b17853d3cf54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ml2qYwyYVkXWsaYitT8NrQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">更快的R-CNN(来源于作者)</p></figure><h2 id="8a5b" class="ob mj it bd mk oc od dn mo oe of dp ms nc og oh mu ne oi oj mw ng ok ol my iz bi translated">单发探测器(SSD)</h2><p id="5040" class="pw-post-body-paragraph ll lm it lo b lp na kd lr ls nb kg lu nc nd lx ly ne nf mb mc ng nh mf mg mh im bi translated">SSD通过从每个卷积层计算包围盒和图像分类来关注速度。SSD用的是VGG19，主干架构，想用什么架构就用什么架构。我们可以使用RESnet，但由于它的复杂性，它违背了目的，因为它意味着速度。另一种选择是MobileNet，用于小物体检测程序。</p><p id="6905" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu nc lw lx ly ne ma mb mc ng me mf mg mh im bi translated">你可以看一下<a class="ae lh" href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md" rel="noopener ugc nofollow" target="_blank"> tf2_detection_zoo </a>中完整的开源算法列表。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oq"><img src="../Images/7df21c0d3f04a44b60a13a98a80d08e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5n8jMp-2VtbWvMmAK3Lqig.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">固态硬盘(来源于作者)</p></figure><h2 id="dbca" class="ob mj it bd mk oc od dn mo oe of dp ms nc og oh mu ne oi oj mw ng ok ol my iz bi translated">其他技术</h2><ul class=""><li id="198f" class="or os it lo b lp na ls nb nc ot ne ou ng ov mh ow ox oy oz bi translated"><strong class="lo jd">你只看一次(YOLO) </strong>类似于SSD，但不是通过特征地图计算边界框，YOLO使用暗网作为CNN堆栈主干，并聚合最后三层YOLO使用暗网作为主干。</li><li id="3789" class="or os it lo b lp pa ls pb nc pc ne pd ng pe mh ow ox oy oz bi translated"><strong class="lo jd">基于变压器的探测器(脸书)。</strong>也用于检测图像中的物体。</li><li id="d711" class="or os it lo b lp pa ls pb nc pc ne pd ng pe mh ow ox oy oz bi translated"><strong class="lo jd">面具RCNN怎么样？蒙版</strong>是图像分割。这是目标检测的另一个应用。这显示了细分的确切含义。</li></ul><h1 id="bf86" class="mi mj it bd mk ml mm mn mo mp mq mr ms ki mt kj mu kl mv km mw ko mx kp my mz bi translated">练习物体探测</h1><p id="8218" class="pw-post-body-paragraph ll lm it lo b lp na kd lr ls nb kg lu nc nd lx ly ne nf mb mc ng nh mf mg mh im bi translated">本教程展示了如何用图像对对象进行分类。你将获得一些实践经验，并对以下概念形成直觉:</p><ul class=""><li id="d408" class="or os it lo b lp lq ls lt nc pf ne pg ng ph mh ow ox oy oz bi translated">使用来自<a class="ae lh" href="https://github.com/tensorflow/models/tree/master/research/object_detection" rel="noopener ugc nofollow" target="_blank"> Tensorflow对象检测库</a>的预训练模型和<a class="ae lh" href="https://cocodataset.org/#home" rel="noopener ugc nofollow" target="_blank"> MSCoco数据集</a>进行开箱检测</li><li id="f57e" class="or os it lo b lp pa ls pb nc pc ne pd ng pe mh ow ox oy oz bi translated">通过用新的开箱即用检测之外的类修改现有的对象检测体系结构来转移学习。本培训将在急切模式下进行(TF2)</li></ul><p id="e08a" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu nc lw lx ly ne ma mb mc ng me mf mg mh im bi translated">用GPU运行这个colab大概需要1个小时。请随意打开下面的colab并开始使用:)</p><div class="nj nk gp gr nl nm"><a href="https://drive.google.com/file/d/15tL9uQXk5T8x1k1sRItW2sdhBqN_nu0O/view?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd jd gy z fp nr fr fs ns fu fw jc bi translated">intro _ to _ Object _ Detection _ with _ R _ CNN . ipynb</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">编辑描述</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">drive.google.com</p></div></div></div></a></div><h2 id="f6f9" class="ob mj it bd mk oc od dn mo oe of dp ms nc og oh mu ne oi oj mw ng ok ol my iz bi translated">图书馆</h2><p id="69cc" class="pw-post-body-paragraph ll lm it lo b lp na kd lr ls nb kg lu nc nd lx ly ne nf mb mc ng nh mf mg mh im bi translated">我们使用TensorFlow对象检测API (ODAPI)库，通过TF后端迭代不同的对象检测配置。</p><ul class=""><li id="c76d" class="or os it lo b lp lq ls lt nc pf ne pg ng ph mh ow ox oy oz bi translated">TF-Slim是一个轻量级的库，用来定义、训练和评估TF模型。这个库是由谷歌人工智能大脑团队开发的，目前仍在开发中，因此使用它需要自担风险(可能会很快被弃用)。</li><li id="dfdc" class="or os it lo b lp pa ls pb nc pc ne pd ng pe mh ow ox oy oz bi translated"><a class="ae lh" href="https://github.com/cocodataset/cocoapi" rel="noopener ugc nofollow" target="_blank"> pycocotools </a>为对象检测和图像分割提供了庞大的图像数据集。<a class="ae lh" href="https://cocodataset.org/#home" rel="noopener ugc nofollow" target="_blank"> COCO数据集</a></li></ul><pre class="ks kt ku kv gt pi pj pk pl aw pm bi"><span id="68ee" class="ob mj it pj b gy pn po l pp pq">!pip install tf_slim<br/>!pip install pycocotools</span></pre><h2 id="a474" class="ob mj it bd mk oc od dn mo oe of dp ms nc og oh mu ne oi oj mw ng ok ol my iz bi translated">基于COCO数据集安装现成模型</h2><p id="6a9c" class="pw-post-body-paragraph ll lm it lo b lp na kd lr ls nb kg lu nc nd lx ly ne nf mb mc ng nh mf mg mh im bi translated">我们将使用“MobileNet SSD”(<a class="ae lh" href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md" rel="noopener ugc nofollow" target="_blank">模型动物园</a>)来尝试我们的开箱即用模型。</p><p id="57c4" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu nc lw lx ly ne ma mb mc ng me mf mg mh im bi translated">使用<code class="fe pr ps pt pj b">export_inference_graph.py</code>工具导出的任何模型都可以通过指定模型名称和路径来加载。</p><pre class="ks kt ku kv gt pi pj pk pl aw pm bi"><span id="4ce9" class="ob mj it pj b gy pn po l pp pq">model_name = ‘ssd_mobilenet_v1_coco_2017_11_17’<br/>detection_model = load_model(model_name) # a helper function to access http://download.tensorflow.org/models/object_detection</span></pre><h2 id="3cc9" class="ob mj it bd mk oc od dn mo oe of dp ms nc og oh mu ne oi oj mw ng ok ol my iz bi translated">创建推理</h2><ul class=""><li id="7eb8" class="or os it lo b lp na ls nb nc ot ne ou ng ov mh ow ox oy oz bi translated">给模特打电话</li><li id="345d" class="or os it lo b lp pa ls pb nc pc ne pd ng pe mh ow ox oy oz bi translated">生成带遮罩的边界框(批注的形状)</li></ul><pre class="ks kt ku kv gt pi pj pk pl aw pm bi"><span id="89aa" class="ob mj it pj b gy pn po l pp pq">model_fn = model.signatures[‘serving_default’]<br/>output_dict = model_fn(input_tensor)</span><span id="555a" class="ob mj it pj b gy pu po l pp pq">if 'detection_masks' in output_dict:</span><span id="b367" class="ob mj it pj b gy pu po l pp pq"># Reframe the the bbox mask to the image size.</span><span id="9ea7" class="ob mj it pj b gy pu po l pp pq">detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(output_dict['detection_masks'], output_dict['detection_boxes'],image.shape[0], image.shape[1])<br/>detection_masks_reframed = tf.cast(detection_masks_reframed &gt; 0.5, tf.uint8)<br/>output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()</span></pre><p id="df46" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu nc lw lx ly ne ma mb mc ng me mf mg mh im bi translated">使用IPyImage <code class="fe pr ps pt pj b">display </code>功能显示图像:</p><pre class="ks kt ku kv gt pi pj pk pl aw pm bi"><span id="3bb1" class="ob mj it pj b gy pn po l pp pq">display(IPyImage(open(anim_file, ‘rb’).read()))</span></pre><p id="575c" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu nc lw lx ly ne ma mb mc ng me mf mg mh im bi translated">注意，注释称浣熊为猫/狗/鸟/棒球手套。这意味着在coco数据集上训练的模型不知道如何识别浣熊。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pv"><img src="../Images/3a41c92d1e83a4f270cb48e90d7c5133.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*S2EF3GAAGRKsuTTzb02rzw.gif"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">浣熊与可可数据集(来源于作者)</p></figure><h2 id="28f9" class="ob mj it bd mk oc od dn mo oe of dp ms nc og oh mu ne oi oj mw ng ok ol my iz bi translated">训练和注释数据</h2><p id="a752" class="pw-post-body-paragraph ll lm it lo b lp na kd lr ls nb kg lu nc nd lx ly ne nf mb mc ng nh mf mg mh im bi translated">我们可以使用我们准备好的训练集来训练我们的模型识别浣熊。一个简单的方法是使用<code class="fe pr ps pt pj b">object_detection.utils</code>中的<code class="fe pr ps pt pj b">colab_utils</code>工具进行注释。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pw"><img src="../Images/c85767e61afd205951bba3790f6e9870.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UYGAXklYCeDPIQtlN22_fg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">使用colab_utils进行手动注释(来源于作者)</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi px"><img src="../Images/54956810a00388d2c3f9474e76c66511.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bBCfR7a7TL68f0gLt3PLLA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">自我注释后的浣熊图像(来源于作者)</p></figure><h1 id="d62f" class="mi mj it bd mk ml mm mn mo mp mq mr ms ki mt kj mu kl mv km mw ko mx kp my mz bi translated">利用RetinaNet进行迁移学习</h1><p id="c045" class="pw-post-body-paragraph ll lm it lo b lp na kd lr ls nb kg lu nc nd lx ly ne nf mb mc ng nh mf mg mh im bi translated">我们可以使用预先训练好的模型根据新的类别进行微调。在这种情况下，我们将使用RetinaNet架构，避免在实验和微调过程中浪费时间。Tensorflow在<a class="ae lh" href="https://github.com/tensorflow/models/tree/master/research/object_detection" rel="noopener ugc nofollow" target="_blank">tensor flow/models/research/object _ detection</a>下有物体检测。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi py"><img src="../Images/1b0f03c05f5c3ca85677a1dcec19f367.png" data-original-src="https://miro.medium.com/v2/resize:fit:706/format:webp/0*TUMhl6DdKTKnDX1a.png"/></div></figure><p id="c015" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu nc lw lx ly ne ma mb mc ng me mf mg mh im bi translated">在此单元中，我们构建了一个单阶段检测架构(RetinaNet ),并恢复了除顶部分类层(将自动随机初始化)之外的所有层。</p><p id="0570" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu nc lw lx ly ne ma mb mc ng me mf mg mh im bi translated">目标是加载管道配置并构建检测模型。因为我们是在一个默认情况下预测90个类槽的COCO架构下工作，所以我们在这里重写了“num_classes”字段，使其只有一个。</p><p id="e705" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu nc lw lx ly ne ma mb mc ng me mf mg mh im bi translated">假设我们只注释了10幅图像。我们不需要创建批量输入，只需通过测试映像运行模型。</p><pre class="ks kt ku kv gt pi pj pk pl aw pm bi"><span id="ca08" class="ob mj it pj b gy pn po l pp pq">#more code in the colab.</span><span id="3d0b" class="ob mj it pj b gy pu po l pp pq">image, shapes = detection_model.preprocess(tf.zeros([1, 640, 640, 3]))<br/>prediction_dict = detection_model.predict(image, shapes)<br/>_ = detection_model.postprocess(prediction_dict, shapes)<br/>print(‘Weights restored!’)</span><span id="858f" class="ob mj it pj b gy pu po l pp pq">optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)<br/>train_step_fn = get_model_train_step_function(<br/>detection_model, optimizer, to_fine_tune)</span></pre><h1 id="4ea7" class="mi mj it bd mk ml mm mn mo mp mq mr ms ki mt kj mu kl mv km mw ko mx kp my mz bi translated">最终结果</h1><p id="551c" class="pw-post-body-paragraph ll lm it lo b lp na kd lr ls nb kg lu nc nd lx ly ne nf mb mc ng nh mf mg mh im bi translated">使用正确的模型和正确的注释进行训练。这10个图像应该足以将图像标记为浣熊</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pv"><img src="../Images/1eec846829a024a4b2f47088c5d5e37c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*P53o72x2g4gej__b_05bRA.gif"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">浣熊在RetinaNet迁移学习后，使用手动注释和COCO数据集(来源于作者)</p></figure><h1 id="293d" class="mi mj it bd mk ml mm mn mo mp mq mr ms ki mt kj mu kl mv km mw ko mx kp my mz bi translated">常见问题</h1><h2 id="a468" class="ob mj it bd mk oc od dn mo oe of dp ms nc og oh mu ne oi oj mw ng ok ol my iz bi translated">还有哪些工具可以帮助图像注释？</h2><p id="450c" class="pw-post-body-paragraph ll lm it lo b lp na kd lr ls nb kg lu nc nd lx ly ne nf mb mc ng nh mf mg mh im bi translated">在微软coco中，大约有1000张图片和80个对象。但是有时我们想在不存在的物体上训练模型。</p><p id="f3ef" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu nc lw lx ly ne ma mb mc ng me mf mg mh im bi translated">我个人认为由亚马逊工程师开源的<a class="ae lh" href="https://github.com/tzutalin/labelImg" rel="noopener ugc nofollow" target="_blank">标签是最有用的标签工具。但是如果你不想使用它，我们总是有Tensorflow助手函数来帮助你标记和注释。</a></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pz"><img src="../Images/029b4ac04f364c22e330a5128727152e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bNmBClVFtACgFm0P36dcjw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">来源于<a class="ae lh" href="https://github.com/tzutalin/labelImg" rel="noopener ugc nofollow" target="_blank"> Github </a></p></figure><h2 id="dbe7" class="ob mj it bd mk oc od dn mo oe of dp ms nc og oh mu ne oi oj mw ng ok ol my iz bi translated">我们需要多少数据？</h2><p id="9a20" class="pw-post-body-paragraph ll lm it lo b lp na kd lr ls nb kg lu nc nd lx ly ne nf mb mc ng nh mf mg mh im bi translated">使用迁移学习，你不需要太多的数据，在我的教程中，你可以在12幅图像内运行它。但是当然这取决于用例。如果您需要在晚上运行SSD，并使用深色图像，那么您可能也需要在上面上传许多图像。</p><p id="bd94" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu nc lw lx ly ne ma mb mc ng me mf mg mh im bi translated">你拥有的数据越多越好，然而随着更多数据的输入，你会发现收益递减。最重要的是拥有代表您需要的数据。这是连体网络的情况，其中具有相同参数和权重的相同配置适用于多个标签。</p><h2 id="a2b0" class="ob mj it bd mk oc od dn mo oe of dp ms nc og oh mu ne oi oj mw ng ok ol my iz bi translated">我们怎么知道生产哪一个？</h2><p id="177b" class="pw-post-body-paragraph ll lm it lo b lp na kd lr ls nb kg lu nc nd lx ly ne nf mb mc ng nh mf mg mh im bi translated">很多时候，我们会用微服务来投放。这意味着我们将安装一台服务器，提供API网关来连接其他服务，并在请求到来时给出结果。对于模型的类型，这真的取决于你是优先考虑准确性(R-CNN)还是速度(SSD)。<a class="ae lh" href="https://arxiv.org/abs/1611.10012" rel="noopener ugc nofollow" target="_blank">这里我提到了一篇非常酷的论文</a></p><h1 id="b7fe" class="mi mj it bd mk ml mm mn mo mp mq mr ms ki mt kj mu kl mv km mw ko mx kp my mz bi translated">参考资料:</h1><h2 id="f219" class="ob mj it bd mk oc od dn mo oe of dp ms nc og oh mu ne oi oj mw ng ok ol my iz bi translated">博客</h2><ul class=""><li id="73bf" class="or os it lo b lp na ls nb nc ot ne ou ng ov mh ow ox oy oz bi translated"><a class="ae lh" rel="noopener" target="_blank" href="/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e"> R-CNN，快速R-CNN，快速R-CNN，YOLO </a></li><li id="f40f" class="or os it lo b lp pa ls pb nc pc ne pd ng pe mh ow ox oy oz bi translated"><a class="ae lh" href="https://github.com/ronygustam/MLSS-INDONESIA-2020" rel="noopener ugc nofollow" target="_blank"> MLSS教程为基础代码</a>归功于罗尼·卡尔法里西</li></ul><h2 id="f264" class="ob mj it bd mk oc od dn mo oe of dp ms nc og oh mu ne oi oj mw ng ok ol my iz bi translated">报纸</h2><div class="nj nk gp gr nl nm"><a href="https://arxiv.org/abs/1611.10012" rel="noopener  ugc nofollow" target="_blank"><div class="nn ab fo"><div class="no ab np cl cj nq"><h2 class="bd jd gy z fp nr fr fs ns fu fw jc bi translated">现代卷积目标探测器的速度/精度权衡</h2><div class="nt l"><h3 class="bd b gy z fp nr fr fs ns fu fw dk translated">本文的目标是作为选择检测架构的指南，以实现正确的</h3></div><div class="nu l"><p class="bd b dl z fp nr fr fs ns fu fw dk translated">arxiv.org</p></div></div><div class="nv l"><div class="qa l nx ny nz nv oa lb nm"/></div></div></a></div><h1 id="1e49" class="mi mj it bd mk ml mm mn mo mp mq mr ms ki mt kj mu kl mv km mw ko mx kp my mz bi translated">关于作者</h1><p id="527b" class="pw-post-body-paragraph ll lm it lo b lp na kd lr ls nb kg lu nc nd lx ly ne nf mb mc ng nh mf mg mh im bi translated">文森特用ML @ Google对抗网络滥用。文森特使用高级数据分析、机器学习和软件工程来保护Chrome和Gmail用户。</p><p id="d291" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu nc lw lx ly ne ma mb mc ng me mf mg mh im bi translated">除了在谷歌的工作，文森特还是乔治亚理工学院计算机科学硕士校友，三项全能运动员和数据科学媒体的特约作家，该媒体在全球拥有超过100万的观众。</p><p id="4e3f" class="pw-post-body-paragraph ll lm it lo b lp lq kd lr ls lt kg lu nc lw lx ly ne ma mb mc ng me mf mg mh im bi translated">最后，请通过<a class="ae lh" href="http://www.linkedin.com/in/vincenttatan/?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"><strong class="lo jd">LinkedIn</strong></a><strong class="lo jd">、</strong><a class="ae lh" href="https://medium.com/@vincentkernn?source=post_page---------------------------" rel="noopener"><strong class="lo jd">Medium</strong></a><strong class="lo jd">或</strong> <a class="ae lh" href="https://www.youtube.com/user/vincelance1/videos?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> <strong class="lo jd"> Youtube频道</strong> </a>联系文森特</p></div></div>    
</body>
</html>