<html>
<head>
<title>Word2vec with PyTorch: Implementing the Original Paper</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用PyTorch的Word2vec:实现原始论文</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/word2vec-with-pytorch-implementing-original-paper-2cd7040120b0?source=collection_archive---------2-----------------------#2021-09-29">https://towardsdatascience.com/word2vec-with-pytorch-implementing-original-paper-2cd7040120b0?source=collection_archive---------2-----------------------#2021-09-29</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="758d" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="9bb9" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated"><em class="ko">涵盖所有实施细节，跳过高层概述。代码附后。</em></h2></div><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi kp"><img src="../Images/e636e1260d5634eff63865d156ac8c61.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ef_BFlHd35VE6TKfccJ_FA.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated">作者图片</p></figure><p id="61d7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">词语嵌入是深层自然语言处理中最基本的概念。word2vec是最早用于训练单词嵌入的算法之一。</p><p id="f912" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这篇文章中，我想更深入地了解关于word2vec的第一篇论文— <a class="ae mb" href="https://arxiv.org/abs/1301.3781" rel="noopener ugc nofollow" target="_blank">向量空间中单词表示的有效估计</a> (2013)，截至目前，该论文已被引用24k次，并且这个数字仍在增长。</p><p id="f785" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们的计划如下:</p><ul class=""><li id="db4b" class="mc md iq lh b li lj ll lm lo me ls mf lw mg ma mh mi mj mk bi translated">回顾论文中描述的模型架构；</li><li id="50d8" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma mh mi mj mk bi translated">使用PyTorch从零开始训练word2vec模型；</li><li id="c97f" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma mh mi mj mk bi translated">评估我们得到的单词嵌入。</li></ul><p id="c7ea" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我附上我的<a class="ae mb" href="https://github.com/OlgaChernytska/word2vec-pytorch" rel="noopener ugc nofollow" target="_blank"> Github项目</a>与word2vec培训。我们将在这篇文章中讨论这个问题。</p><p id="5a05" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">今天我们只复习word2vec的<strong class="lh ja"> <em class="mq">第一</em> </strong>篇。然而，有几篇后来的论文，描述了word2vec的演变:</p><ul class=""><li id="3146" class="mc md iq lh b li lj ll lm lo me ls mf lw mg ma mh mi mj mk bi translated"><a class="ae mb" href="https://arxiv.org/abs/1310.4546" rel="noopener ugc nofollow" target="_blank">单词和短语的分布式表示及其组合性</a> (2013)描述了对原始word2vec的几个扩展，以加速训练并提高嵌入质量。</li><li id="ea7d" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma mh mi mj mk bi translated"><a class="ae mb" href="https://arxiv.org/abs/1405.4053" rel="noopener ugc nofollow" target="_blank">句子和文档的分布式表示</a> (2014)展示了如何使用word2vec背后的思想来创建句子和文档嵌入。这种方法被称为doc2vec。</li><li id="0c02" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma mh mi mj mk bi translated"><a class="ae mb" href="https://arxiv.org/abs/1607.04606" rel="noopener ugc nofollow" target="_blank">用子词信息丰富词向量</a> (2017)为word2vec引入了更多扩展。这种方法在字符级别(而不是像以前那样在单词级别)上操作，被称为fastText。</li></ul><p id="d081" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我相信，如果你理解了第一篇文章，你会很容易理解后面文章中描述的观点。所以我们走吧！</p><p id="b510" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="mq">披露。Wor2vec已经是一个老算法了，还有更新的选项(例如，</em> <a class="ae mb" href="https://en.wikipedia.org/wiki/BERT_(language_model)" rel="noopener ugc nofollow" target="_blank"> <em class="mq">伯特</em> </a> <em class="mq">)。这篇文章是为那些刚刚开始深入NLP之旅的人，或者那些对阅读和实现论文感兴趣的人写的。</em></p><p id="7362" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">内容</strong> <br/> —什么是word2vec？<br/> —模型架构<br/> —数据<br/> —数据准备<br/> —用PyTorch进行文本处理<br/> —训练细节<br/> —检索嵌入<br/> —用t-SNE可视化<br/> — —相似词<br/> — —国王—男人+女人=女王<br/> —接下来呢？</p><h1 id="20c5" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">word2vec是什么？</h1><p id="033c" class="pw-post-body-paragraph lf lg iq lh b li nj ka lk ll nk kd ln lo nl lq lr ls nm lu lv lw nn ly lz ma ij bi translated">以下是我的三句话解释:</p><ol class=""><li id="8204" class="mc md iq lh b li lj ll lm lo me ls mf lw mg ma no mi mj mk bi translated">Word2vec是一种创建单词嵌入的方法。</li><li id="5c9b" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma no mi mj mk bi translated">单词嵌入是将单词表示为数字向量。</li><li id="8055" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma no mi mj mk bi translated">除了word2vec，还存在其他方法来创建单词嵌入，例如fastText、GloVe、ELMO、BERT、GPT-2等。</li></ol><p id="1db5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果您不熟悉单词嵌入的概念，下面是几个重要资源的链接。跳过细节，但抓住背后的直觉。并返回到我的帖子中查看word2vec的详细信息和代码。</p><ul class=""><li id="fe2a" class="mc md iq lh b li lj ll lm lo me ls mf lw mg ma mh mi mj mk bi translated"><a class="ae mb" rel="noopener" target="_blank" href="/why-do-we-use-embeddings-in-nlp-2f20e1b632d2">为什么我们在Natasha Latysheva的NLP </a>中使用单词嵌入</li><li id="e9a1" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma mh mi mj mk bi translated">杰伊·阿拉姆马的插图版Word2vec</li><li id="3f6c" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma mh mi mj mk bi translated">谢恩·林恩的《文本分析中的单词嵌入介绍》</li></ul><p id="f3fc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">现在好些了吗？</p><p id="cc96" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">词嵌入实际上用在每个自然语言处理任务中——文本分类、命名实体识别、问题回答、文本摘要等。到处都是。模型不理解单词和字母，它们理解数字。这就是单词嵌入派上用场的地方。</p><h1 id="1dab" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">模型架构</h1><p id="6a1f" class="pw-post-body-paragraph lf lg iq lh b li nj ka lk ll nk kd ln lo nl lq lr ls nm lu lv lw nn ly lz ma ij bi translated">Word2vec基于这样一种思想，即单词的含义是由其上下文定义的。上下文被表示为周围的单词。</p><p id="ab7b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">想想吧。假设，你正在学习一门新的语言。你正在读一个句子，所有的单词你都很熟悉，除了一个。你以前没见过这个词，但你可以很容易地说出它的词性，对吗？有时候，甚至猜测它的意思。那是因为周围单词的信息对你有帮助。</p><p id="4a49" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">对于word2vec模型，上下文被表示为当前单词之前的N个单词和之后的N个单词。n是一个超参数。使用较大的N，我们可以创建更好的嵌入，但同时，这样的模型需要更多的计算资源。在最初的论文中，N是4–5，在我下面的可视化中，N是2。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi np"><img src="../Images/5a0537da886ed02f2da7df6b9e76c3d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*diS0hP2oyT_dWd9JMKhGOg.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><em class="ko">图片1。一个词和它的上下文。作者图片</em></p></figure><p id="5e17" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">本文提出了两种word2vec架构:</p><ul class=""><li id="bb9b" class="mc md iq lh b li lj ll lm lo me ls mf lw mg ma mh mi mj mk bi translated"><strong class="lh ja">CBOW</strong>(Continuous Bag-of-Words)——基于上下文单词预测当前单词的模型。</li><li id="74ad" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma mh mi mj mk bi translated"><strong class="lh ja"> Skip-Gram </strong> —基于当前单词预测上下文单词的模型。</li></ul><p id="5497" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">例如，CBOW模型将“机器”、“学习”、“方法”作为输入，将“是”作为输出返回。跳格模型正好相反。</p><p id="40c7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">根据定义，CBOW和Skip-Gram模型都是多类分类模型。下面详细的可视化应该会清楚。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nq"><img src="../Images/4c0de016302229b5f0d949cb91c376b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ETcgajy5s0KNIfMgE5xOqg.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><em class="ko">图片2。CBOW模型:高级概述。作者图片</em></p></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nr"><img src="../Images/176c3e931705469916a0b6dd2d4ede0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SVs6xTpD7AYviP24UTOYUA.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><em class="ko">图片3。跳格模型:高层次的概述。作者图片</em></p></figure><p id="fc3b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">黑匣子里发生了什么？</strong></p><p id="21c1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">第一步是用id对所有单词进行编码。ID是一个整数(索引),用于标识单词在词汇表中的位置。“词汇”是描述文本中一组独特词汇的术语。这个集合可以是文本中的所有单词或者仅仅是最频繁出现的单词。“数据准备”一节将详细介绍这一点。</p><p id="70e4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">Word2vec模型非常简单，只有两层:</p><ul class=""><li id="dee5" class="mc md iq lh b li lj ll lm lo me ls mf lw mg ma mh mi mj mk bi translated"><strong class="lh ja">嵌入层</strong>，取word ID，返回其300维向量。Word2vec嵌入是300维的，因为作者证明了这个数字在嵌入质量和计算成本方面是最好的。您可以将嵌入层视为一个简单的具有可学习权重的查找表，或者视为一个没有偏差和激活的线性层。</li><li id="029a" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma mh mi mj mk bi translated">然后是带有Softmax激活的<strong class="lh ja">线性(密集)层</strong>。我们为多类分类任务创建一个模型，其中类的数量等于词汇表中的单词数量。</li></ul><p id="5876" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">CBOW和Skip-Gram模型之间的区别在于输入单词的数量。CBOW模型采用几个单词，每个单词都经过相同的嵌入层，然后在进入线性层之前对单词嵌入向量进行平均。跳格模型取而代之的是一个单词。详细的架构如下图所示。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi ns"><img src="../Images/b1c54979a48288eb1262e96a3f89200d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mLDM3PH12CjhaFoUm5QTow.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><em class="ko">图片4。CBOW模型:细节中的建筑。作者图片</em></p></figure><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi nt"><img src="../Images/d7336b1e98ec7ec8576ec8ff0c1b3b8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eHh1_t8Wms_hqDNBLuAnFg.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><em class="ko">图5。跳跃式模型:细节中的建筑。作者图片</em></p></figure><p id="6f90" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">单词嵌入在哪里？</strong></p><p id="4978" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们训练不会被直接使用的模型。我们不想从上下文中预测一个词，也不想从一个词中预测上下文。相反，我们想要得到单词向量。原来这些向量就是嵌入层的权重。更多细节在“检索嵌入”一节中。</p><h1 id="cc9d" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">数据</h1><p id="7a46" class="pw-post-body-paragraph lf lg iq lh b li nj ka lk ll nk kd ln lo nl lq lr ls nm lu lv lw nn ly lz ma ij bi translated">Word2vec是一个无监督的算法，所以我们只需要一个大的文本语料库。原来word2vec是在Google新闻语料库上训练的，包含6B的令牌。</p><p id="2db5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我用PyTorch中可用的较小数据集进行了试验:</p><ul class=""><li id="c175" class="mc md iq lh b li lj ll lm lo me ls mf lw mg ma mh mi mj mk bi translated"><a class="ae mb" href="https://pytorch.org/text/stable/datasets.html#wikitext-2" rel="noopener ugc nofollow" target="_blank"> WikiText-2 </a>:列车部分36k文本行和2M记号(记号为单词+标点)</li><li id="ea16" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma mh mi mj mk bi translated"><a class="ae mb" href="https://pytorch.org/text/stable/datasets.html#wikitext103" rel="noopener ugc nofollow" target="_blank"> WikiText103 </a>:列车部分180万条线路和100万个令牌</li></ul><p id="e9d3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在为商业/研究任务训练单词嵌入时，请仔细选择数据集。例如，如果你想对机器学习论文进行分类，就用关于机器学习的科学文本来训练word2vec。如果你想对时尚文章进行分类，一个时尚新闻数据集会是更好的选择。这是因为“模型”这个词在机器学习领域意味着“方法”和“算法”，但在时尚领域意味着“人”和“女人”。</p><p id="fb63" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">当重用训练过的单词嵌入时，注意它们被训练的数据集，以及这个数据集是否适合你的任务。</p><h1 id="0228" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">数据准备</h1><p id="7b09" class="pw-post-body-paragraph lf lg iq lh b li nj ka lk ll nk kd ln lo nl lq lr ls nm lu lv lw nn ly lz ma ij bi translated">数据准备的主要步骤是创建一个<strong class="lh ja">词汇表</strong>。词汇表包含将为其训练嵌入的单词。词汇可能是文本语料库中所有独特单词的列表，但通常不是。</p><p id="ad10" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">最好创造词汇:</p><ul class=""><li id="e0f1" class="mc md iq lh b li lj ll lm lo me ls mf lw mg ma mh mi mj mk bi translated">要么通过过滤掉在语料库中出现少于N次的罕见单词；</li><li id="baa8" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma mh mi mj mk bi translated">或者通过选择前N个最频繁出现的单词。</li></ul><p id="e003" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这种过滤很有意义，因为词汇表越小，模型训练就越快。另一方面，您可能不希望对文本语料库中只出现过一次的单词使用嵌入，因为这些嵌入可能不够好。为了创建好的单词嵌入，模型应该在不同的上下文中多次看到一个单词。</p><p id="ef0f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">词汇表中的每个单词都有其唯一的索引。词汇中的单词可以按字母顺序或基于它们的频率排序，也可以不排序——这不应该影响模型训练。词汇通常表示为字典数据结构:</p><pre class="kq kr ks kt gt nu nv nw nx aw ny bi"><span id="148c" class="nz ms iq nv b gy oa ob l oc od">vocab = {<br/>     "a": 1,<br/>     "analysis": 2,<br/>     "analytical": 3,<br/>     "automates": 4,<br/>     "building": 5,<br/>     "data": 6,<br/>     ...<br/>}</span></pre><p id="0d27" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">标点符号和其他特殊符号也可以添加到词汇表中，我们也为它们训练嵌入。你可以小写所有的单词，或者为单词“Apple”和“apple”训练单独的嵌入；在某些情况下，它可能是有用的。</p><p id="31f2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">根据您希望您的词汇表(和单词嵌入)是什么样的，对文本语料库进行适当的预处理。小写与否，去掉标点符号与否，对文本进行记号化。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi oe"><img src="../Images/a624e4b0179a9369ef95ff40899afabf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SUROmUv7uWlISldFb96SoA.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><em class="ko">图片6。如何从文本语料库中创建词汇？作者图片</em></p></figure><p id="300e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">对于我的模型:</p><ul class=""><li id="2f7b" class="mc md iq lh b li lj ll lm lo me ls mf lw mg ma mh mi mj mk bi translated">我只从一篇文章中出现至少50次的单词中创造词汇。</li><li id="841b" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma mh mi mj mk bi translated">我使用了PyTorch的<a class="ae mb" href="https://pytorch.org/text/stable/data_utils.html#get-tokenizer" rel="noopener ugc nofollow" target="_blank"> basic_english tokenizer </a>,它将文本小写，用空格分割成记号，但将标点符号放入单独的记号中。</li></ul><p id="a1f9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">因此，在单词进入模型之前，它们被编码为id。ID对应于词汇表中的单词索引。不在词汇表中的单词(词汇表外的单词)用某个数字进行编码，例如0。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi of"><img src="../Images/65632cc9e2bfd4d56fd5892f462b885e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-6th97Wit9XDnseZUNSuCg.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><em class="ko">图7。如何用词汇id对单词进行编码？作者图片</em></p></figure><h1 id="b4e4" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">使用PyTorch进行文本处理</h1><p id="023e" class="pw-post-body-paragraph lf lg iq lh b li nj ka lk ll nk kd ln lo nl lq lr ls nm lu lv lw nn ly lz ma ij bi translated"><a class="ae mb" href="https://github.com/OlgaChernytska/word2vec-pytorch" rel="noopener ugc nofollow" target="_blank">训练词2vec的完整代码在这里</a>。让我们来看看重要的步骤。</p><p id="4f38" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><strong class="lh ja">模型</strong>是通过从<a class="ae mb" href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" rel="noopener ugc nofollow" target="_blank"> nn子类化在PyTorch中创建的。模块</a>。如前所述，CBOW和Skip-Gram模型都有两层:嵌入和线性。</p><p id="bccb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">下面是CBOW的模型类，这里的<a class="ae mb" href="https://github.com/OlgaChernytska/word2vec-pytorch/blob/87b0418fcc6a0f5b8ac96698f6fc1079014b4615/utils/model.py#L30" rel="noopener ugc nofollow" target="_blank">是Skip-Gram的</a>。</p><pre class="kq kr ks kt gt nu nv nw nx aw ny bi"><span id="6288" class="nz ms iq nv b gy oa ob l oc od">import torch.nn as nn <br/>EMBED_DIMENSION = 300 <br/>EMBED_MAX_NORM = 1 </span><span id="fd90" class="nz ms iq nv b gy og ob l oc od">class CBOW_Model(nn.Module):<br/>    def __init__(self, vocab_size: int):<br/>        super(CBOW_Model, self).__init__()<br/>        self.embeddings = nn.Embedding(<br/>            num_embeddings=vocab_size,<br/>            embedding_dim=EMBED_DIMENSION,<br/>            max_norm=EMBED_MAX_NORM,<br/>        )<br/>        self.linear = nn.Linear(<br/>            in_features=EMBED_DIMENSION,<br/>            out_features=vocab_size,<br/>        )<br/>     def forward(self, inputs_):<br/>        x = self.embeddings(inputs_)<br/>        x = x.mean(axis=1)<br/>        x = self.linear(x)<br/>        return x</span></pre><p id="5d00" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">注意，线性层没有Softmax激活。这是因为PyTorch CrossEntropyLoss <a class="ae mb" href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html?highlight=crossentropy#torch.nn.CrossEntropyLoss" rel="noopener ugc nofollow" target="_blank">期望预测是原始的、非标准化的分数</a>。而在Keras中，您可以定制CrossEntropyLoss的输入是什么— <a class="ae mb" href="https://keras.io/api/losses/probabilistic_losses/#categoricalcrossentropy-class" rel="noopener ugc nofollow" target="_blank">原始值或概率</a>。</p><p id="92ba" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">模型输入是单词ID。模型输出是一个N维向量，其中N是词汇量。</p><p id="10d5" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">EMBED_MAX_NORM是<a class="ae mb" href="https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html" rel="noopener ugc nofollow" target="_blank">限制单词嵌入规范</a>的参数(在我们的例子中为1)。它作为一个正则化参数，防止嵌入层中的权重不受控制地增长。EMBED_MAX_NORM值得一试。我所看到的:当限制嵌入向量范数时，类似的词如“母亲”和“父亲”具有更高的余弦相似度，与EMBED_MAX_NORM=None时相比。</p><p id="2ebd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们使用PyTorch函数<a class="ae mb" href="https://pytorch.org/text/stable/vocab.html#build-vocab-from-iterator" rel="noopener ugc nofollow" target="_blank">build _ vocab _ from _ iterator</a>从数据集迭代器创建<strong class="lh ja">词汇表</strong>。WikiText-2和WikiText103数据集用标记&lt; unk &gt;替换了罕见的单词，我们添加这个标记作为ID=0的特殊符号，并且所有不在词汇表中的单词也用ID=0编码。</p><pre class="kq kr ks kt gt nu nv nw nx aw ny bi"><span id="e7ee" class="nz ms iq nv b gy oa ob l oc od">from torchtext.vocab import build_vocab_from_iterator MIN_WORD_FREQUENCY = 50</span><span id="9417" class="nz ms iq nv b gy og ob l oc od">def build_vocab(data_iter, tokenizer):<br/>    vocab = build_vocab_from_iterator(<br/>        map(tokenizer, data_iter),<br/>        specials=["&lt;unk&gt;"],<br/>        min_freq=MIN_WORD_FREQUENCY,<br/>    )<br/>    vocab.set_default_index(vocab["&lt;unk&gt;"])<br/>    return vocab</span></pre><p id="72c8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><a class="ae mb" href="https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader" rel="noopener ugc nofollow" target="_blank"> <strong class="lh ja">数据加载器</strong> </a>我们用<a class="ae mb" href="https://pytorch.org/docs/stable/data.html" rel="noopener ugc nofollow" target="_blank"> collate_fn </a>创建。该函数实现了如何对单个样本进行批处理的逻辑。当遍历PyTorch WikiText-2和WikiText103数据集时，检索到的每个样本都是一个文本段落。</p><p id="a02e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">例如，在用于CBOW的collate_fn中，我们“说”:</p><ol class=""><li id="e37e" class="mc md iq lh b li lj ll lm lo me ls mf lw mg ma no mi mj mk bi translated">取每一段文字。<br/> —小写，标记，用id编码(函数<a class="ae mb" href="https://github.com/OlgaChernytska/word2vec-pytorch/blob/66ba7adabb56905185e95a00ef0089c6d16dda49/utils/dataloader.py#L133" rel="noopener ugc nofollow" target="_blank"> text_pipeline </a>)。如果段落太短，跳过它。如果太长—将其截断。<br/> —使用大小为9的移动窗口(4个历史单词、中间单词和4个未来单词)在段落中循环。<br/> —所有中间词合并成一个列表——它们将是Ys。<br/> —所有上下文(历史和未来单词)合并成一个列表列表—它们将是Xs。</li><li id="6b7f" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma no mi mj mk bi translated">将所有段落中的x合并在一起，它们将是批x。</li><li id="636c" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma no mi mj mk bi translated">将所有段落中的y合并在一起，它们将成为批量y。</li></ol><p id="0178" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">请注意，当我们调用collate_fn时，最终批次的数量(Xs和Ys)将与Dataloader中指定的参数batch_size不同，并且将因不同的段落而异。</p><p id="730b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">CBOW的collate_fn的代码如下，用于<a class="ae mb" href="https://github.com/OlgaChernytska/word2vec-pytorch/blob/66ba7adabb56905185e95a00ef0089c6d16dda49/utils/dataloader.py#L85" rel="noopener ugc nofollow" target="_blank"> Skip-Gram —在这里</a>。</p><pre class="kq kr ks kt gt nu nv nw nx aw ny bi"><span id="d7b4" class="nz ms iq nv b gy oa ob l oc od">import torch CBOW_N_WORDS = 4 <br/>MAX_SEQUENCE_LENGTH = 256  </span><span id="9b7f" class="nz ms iq nv b gy og ob l oc od">def collate_cbow(batch, text_pipeline):<br/>     batch_input, batch_output = [], []<br/>     for text in batch:<br/>         text_tokens_ids = text_pipeline(text)</span><span id="abbe" class="nz ms iq nv b gy og ob l oc od">         if len(text_tokens_ids) &lt; CBOW_N_WORDS * 2 + 1:<br/>             continue</span><span id="1a5b" class="nz ms iq nv b gy og ob l oc od">         if MAX_SEQUENCE_LENGTH:<br/>             text_tokens_ids = text_tokens_ids[:MAX_SEQUENCE_LENGTH]</span><span id="3cec" class="nz ms iq nv b gy og ob l oc od">         for idx in range(len(text_tokens_ids) - CBOW_N_WORDS * 2):<br/>             token_id_sequence = text_tokens_ids[idx : (idx + CBOW_N_WORDS * 2 + 1)]<br/>             output = token_id_sequence.pop(CBOW_N_WORDS)<br/>             input_ = token_id_sequence<br/>             batch_input.append(input_)<br/>             batch_output.append(output)<br/>     <br/>     batch_input = torch.tensor(batch_input, dtype=torch.long)<br/>     batch_output = torch.tensor(batch_output, dtype=torch.long)<br/>     return batch_input, batch_output</span></pre><p id="8a45" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">下面是如何将collate_fn与PyTorch Dataloader一起使用:</p><pre class="kq kr ks kt gt nu nv nw nx aw ny bi"><span id="bf48" class="nz ms iq nv b gy oa ob l oc od">from torch.utils.data <br/>import DataLoader <br/>from functools import partial  </span><span id="9400" class="nz ms iq nv b gy og ob l oc od">dataloader = DataLoader(<br/>         data_iter,<br/>         batch_size=batch_size,<br/>         shuffle=True,         <br/>         collate_fn=partial(collate_cbow, text_pipeline=text_pipeline),<br/>)</span></pre><p id="7978" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我还创建了<a class="ae mb" href="https://github.com/OlgaChernytska/word2vec-pytorch/blob/main/utils/trainer.py" rel="noopener ugc nofollow" target="_blank">一个职业训练器</a>，用于模型训练和验证。它包含一个典型的PyTorch训练和验证流程，所以对于那些有PyTorch经验的人来说，它看起来非常简单。</p><p id="02f9" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果你想更好地理解代码——我推荐你<a class="ae mb" href="https://github.com/OlgaChernytska/word2vec-pytorch" rel="noopener ugc nofollow" target="_blank">克隆我的库</a>并使用它。</p><h1 id="60cb" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">培训详情</h1><p id="ed2b" class="pw-post-body-paragraph lf lg iq lh b li nj ka lk ll nk kd ln lo nl lq lr ls nm lu lv lw nn ly lz ma ij bi translated">使用交叉熵损失将Word2vec训练为多类分类模型。</p><p id="7bda" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">您选择批量大小以适应内存。只要记住:那个批量大小就是数据集段落的数量，它会被处理成输入输出对，这个数字会大很多。</p><p id="2edc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">论文优化器是AdaGrad，但是我用了一个更新的——Adam。</p><p id="f449" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我已经跳过了层次化的Softmax的论文部分，只使用了普通的Softmax。也没有使用霍夫曼树来构建词汇表。分层的Softmax和Huffman树是加速训练的技巧。但是PyTorch在引擎盖下有很多优化，所以训练已经足够快了。</p><p id="e5d2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">正如论文中所推荐的，我从0.025的学习率开始，并在每个时期线性降低，直到在最后一个时期结束时达到0。这里<a class="ae mb" href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LambdaLR.html#torch.optim.lr_scheduler.LambdaLR" rel="noopener ugc nofollow" target="_blank"> PyTorch LambdaLR调度器</a>帮了大忙；这里是我如何使用它的<a class="ae mb" href="https://github.com/OlgaChernytska/word2vec-pytorch/blob/66ba7adabb56905185e95a00ef0089c6d16dda49/utils/helper.py#L28" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="811f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">作者在大多数实验中只训练了3个时期的模型(但在非常大的数据集上)。我尝试了越来越小的数字，并决定坚持使用5个纪元。</p><p id="6806" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">对于WikiText-2数据集:</p><ul class=""><li id="40a0" class="mc md iq lh b li lj ll lm lo me ls mf lw mg ma mh mi mj mk bi translated">我的词汇量大约是4k个单词(这些单词在文章中至少出现了50次)。</li><li id="b2fd" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma mh mi mj mk bi translated">CBOW和Skip-Gram模型在GPU上的训练时间不到20分钟。</li></ul><p id="6126" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">对于WikiText103数据集:</p><ul class=""><li id="bd59" class="mc md iq lh b li lj ll lm lo me ls mf lw mg ma mh mi mj mk bi translated">我的词汇量大约是50k个单词。</li><li id="76d3" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma mh mi mj mk bi translated">模特通宵训练。</li></ul><h1 id="b8a7" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">检索嵌入</h1><p id="2ed4" class="pw-post-body-paragraph lf lg iq lh b li nj ka lk ll nk kd ln lo nl lq lr ls nm lu lv lw nn ly lz ma ij bi translated">完整的程序在<a class="ae mb" href="https://github.com/OlgaChernytska/word2vec-pytorch/blob/main/notebooks/Inference.ipynb" rel="noopener ugc nofollow" target="_blank">这本笔记本</a>中有描述。</p><p id="233e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">单词嵌入存储在嵌入层中。嵌入层的大小是(vocab_size，300)，这意味着我们可以嵌入词汇表中的所有单词。</p><p id="ffa1" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">当在WikiText-2数据集上训练时，CBOW和Skip-Gram模型都在大小为(4099，300)的嵌入层中具有权重，其中每一行都是单词向量。以下是获取嵌入层权重的方法:</p><pre class="kq kr ks kt gt nu nv nw nx aw ny bi"><span id="df73" class="nz ms iq nv b gy oa ob l oc od">embeddings = list(model.parameters())[0]</span></pre><p id="00b8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">下面是如何获得与嵌入矩阵中相同顺序的单词:</p><pre class="kq kr ks kt gt nu nv nw nx aw ny bi"><span id="4609" class="nz ms iq nv b gy oa ob l oc od">vocab.get_itos()</span></pre><p id="2ff7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在您的模型中使用嵌入之前，有必要检查他们是否接受了适当的培训。对此有几种选择:</p><ol class=""><li id="a264" class="mc md iq lh b li lj ll lm lo me ls mf lw mg ma no mi mj mk bi translated">对单词嵌入进行聚类，并检查相关单词是否形成单独的聚类。</li><li id="c660" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma no mi mj mk bi translated">使用t-SNE可视化单词嵌入，并检查相似的单词是否彼此靠近。</li><li id="17ea" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma no mi mj mk bi translated">为一个随机单词寻找最相似的单词。</li></ol><h2 id="10ec" class="nz ms iq bd mt oh oi dn mx oj ok dp nb lo ol om nd ls on oo nf lw op oq nh iw bi translated">t-SN可视化</h2><p id="c297" class="pw-post-body-paragraph lf lg iq lh b li nj ka lk ll nk kd ln lo nl lq lr ls nm lu lv lw nn ly lz ma ij bi translated">你可以使用<a class="ae mb" href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html?highlight=tsne#sklearn.manifold.TSNE" rel="noopener ugc nofollow" target="_blank"> sklearn t-SNE </a>和<a class="ae mb" href="https://plotly.com/python/" rel="noopener ugc nofollow" target="_blank"> plotly </a>来创建一个两个组件的可视化，如下图所示。这里的数字串是绿色的，它们形成了两个独立的簇。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi or"><img src="../Images/650117971ffbef0c15d07ec7cecf4609.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WFnI53qvzK9IsJR0xENz6A.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><em class="ko">图8。基于WikiText-2语料库的CBOW嵌入可视化。</em> <br/> <em class="ko">带有以绿色绘制的数字字符串的双组分t型SNE。</em></p></figure><p id="6e7e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">放大后，我们可能会发现将数字字符串分成两个簇很有意义。左上角的聚类是由年份组成的，而右下角的聚类是由普通数字组成的。</p><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="kv kw di kx bf ky"><div class="gh gi os"><img src="../Images/3c8bda748f9b0868a5f6caec5466727e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2FCFjpzN8jeFDN1qN9tTPw.png"/></div></div><p class="lb lc gj gh gi ld le bd b be z dk translated"><em class="ko">图片9。放大时的数字聚类。左上角的簇是由年份和组成的</em> <br/> <em class="ko">，右下角的簇是由普通数字组成的。</em></p></figure><p id="450e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">你可以探索<a class="ae mb" href="https://notrocketscience.blog/wp-content/uploads/2021/09/word2vec_visualization.html" rel="noopener ugc nofollow" target="_blank">这种奇妙的视觉化</a>来发现更有趣的关系。创建这个可视化的代码在这里。</p><h2 id="69b8" class="nz ms iq bd mt oh oi dn mx oj ok dp nb lo ol om nd ls on oo nf lw op oq nh iw bi translated">相似的词</h2><p id="a8e6" class="pw-post-body-paragraph lf lg iq lh b li nj ka lk ll nk kd ln lo nl lq lr ls nm lu lv lw nn ly lz ma ij bi translated">单词相似度计算为单词向量之间的<a class="ae mb" href="https://en.wikipedia.org/wiki/Cosine_similarity" rel="noopener ugc nofollow" target="_blank">余弦相似度</a>。显然，余弦相似度越高，假设的相似单词就越多。</p><p id="ee97" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">例如，对于单词“父亲”,下面是词汇表中最相似的单词:</p><pre class="kq kr ks kt gt nu nv nw nx aw ny bi"><span id="d11f" class="nz ms iq nv b gy oa ob l oc od">#CBOW model trained on WikiText-2 <br/>mother: 0.842 <br/>wife: 0.809 <br/>friend: 0.796 <br/>brother: 0.775 <br/>daughter: 0.773 </span><span id="d964" class="nz ms iq nv b gy og ob l oc od">#Skip-Gram model trained on WikiText-2 <br/>mother: 0.626 <br/>brother: 0.600 <br/>son: 0.579 <br/>wife: 0.563 <br/>daughter: 0.542</span></pre><p id="e963" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这里是<a class="ae mb" href="https://github.com/OlgaChernytska/word2vec-pytorch/blob/main/notebooks/Inference.ipynb" rel="noopener ugc nofollow" target="_blank">查找相似单词的代码</a>。</p><h2 id="9d34" class="nz ms iq bd mt oh oi dn mx oj ok dp nb lo ol om nd ls on oo nf lw op oq nh iw bi translated">国王——男人+女人=王后</h2><p id="7c33" class="pw-post-body-paragraph lf lg iq lh b li nj ka lk ll nk kd ln lo nl lq lr ls nm lu lv lw nn ly lz ma ij bi translated">根据这篇论文，一个经过适当训练的word2vec模型可以解方程“king — man + woman =？”(回答:“女王”)，还是“更大—大+小=？”(回答:“小一点”)，还是“巴黎—法国+德国=？”(回答:“柏林”)。</p><p id="d436" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">通过对单词vectors执行数学运算来求解方程:vector(“king”)—vector(“man”)+vector(“woman”)。而最终的矢量应该是最接近矢量的(“皇后”)。</p><p id="35bd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">不幸的是，我不能复制这一部分。我在WikiText-2和WikiText103上训练的CBOW和Skip-Gram模型不能捕捉这种关系(这里的<a class="ae mb" href="https://github.com/OlgaChernytska/word2vec-pytorch/blob/main/notebooks/Inference.ipynb" rel="noopener ugc nofollow" target="_blank">代码是</a>)。</p><p id="99f7" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">与矢量(“国王”)—矢量(“男人”)+矢量(“女人”)最接近的矢量是:</p><pre class="kq kr ks kt gt nu nv nw nx aw ny bi"><span id="c255" class="nz ms iq nv b gy oa ob l oc od">#CBOW model trained on WikiText-2 dataset <br/>king: 0.757 <br/>bishop: 0.536 <br/>lord: 0.529 <br/>reign: 0.519 <br/>pope: 0.501 </span><span id="4e6c" class="nz ms iq nv b gy og ob l oc od">#Skip-Gram model trained on WikiText-2 dataset <br/>king: 0.690 <br/>reign: 0.469 <br/>son: 0.453 <br/>woman: 0.436 <br/>daughter: 0.435 </span><span id="8531" class="nz ms iq nv b gy og ob l oc od">#CBOW model trained on WikiText103 dataset <br/>king: 0.652 <br/>woman: 0.494 <br/>queen: 0.354 <br/>daughter: 0.342 <br/>couple: 0.330</span></pre><p id="0231" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">与vector(" bigger ")-vector(" big ")+vector(" small ")最接近的向量是:</p><pre class="kq kr ks kt gt nu nv nw nx aw ny bi"><span id="1a22" class="nz ms iq nv b gy oa ob l oc od">#CBOW model trained on WikiText-2 dataset <br/>small: 0.588 <br/>&lt;unk&gt;: 0.546 <br/>smaller: 0.396 <br/>architecture: 0.395 <br/>fields: 0.385 </span><span id="2783" class="nz ms iq nv b gy og ob l oc od">#Skip-Gram model trained on WikiText-2 dataset <br/>small: 0.638 <br/>&lt;unk&gt;: 0.384 <br/>wood: 0.373 <br/>large: 0.342 <br/>chemical: 0.339 </span><span id="1240" class="nz ms iq nv b gy og ob l oc od">#CBOW model trained on WikiText103 dataset <br/>bigger: 0.606 <br/>small: 0.526 <br/>smaller: 0.273 <br/>simple: 0.258 <br/>large: 0.258</span></pre><p id="32b6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">有时，正确的单词在前5名之内，但从来不是最接近的。我认为有两个可能的原因:</p><ol class=""><li id="7c66" class="mc md iq lh b li lj ll lm lo me ls mf lw mg ma no mi mj mk bi translated">代码中的一些错误。为了仔细检查，我用<a class="ae mb" href="https://radimrehurek.com/gensim/index.html" rel="noopener ugc nofollow" target="_blank"> Gensim库</a>和相同的数据集——wiki text-2，wiki text 103——训练了word2vec。Gensim单词嵌入也不能解决这些方程。</li><li id="a068" class="mc md iq lh b li ml ll mm lo mn ls mo lw mp ma no mi mj mk bi translated">所以更可能的原因是数据集太小。WikiText-2包含2M标记，WikiText103包含1亿个标记，而本文使用的Google新闻语料库包含6B个标记，即60(！！！)倍大。</li></ol><p id="e61c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在训练单词嵌入时，数据集大小确实很重要。作者在论文中也提到。</p><h1 id="d589" class="mr ms iq bd mt mu mv mw mx my mz na nb kf nc kg nd ki ne kj nf kl ng km nh ni bi translated">下一步是什么？</h1><p id="c4f6" class="pw-post-body-paragraph lf lg iq lh b li nj ka lk ll nk kd ln lo nl lq lr ls nm lu lv lw nn ly lz ma ij bi translated">我希望这篇文章能帮助你在深层NLP中建立一个基础，这样你就可以继续学习更高级的算法。对我来说，深挖原论文，一切从零开始训练，非常有趣。推荐。</p></div><div class="ab cl ot ou hu ov" role="separator"><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy"/></div><div class="ij ik il im in"><p id="1f72" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="mq">原载于2021年9月29日</em><a class="ae mb" href="https://notrocketscience.blog/word2vec-with-pytorch-implementing-original-paper/" rel="noopener ugc nofollow" target="_blank"><em class="mq">https://notrocketseconomy . blog</em></a><em class="mq">。<br/>如果你想阅读更多类似的教程，可以订阅我的博客“非火箭科学”——</em><a class="ae mb" href="https://t.me/notrocketscienceblog" rel="noopener ugc nofollow" target="_blank"><em class="mq">电报</em> </a> <em class="mq">和</em> <a class="ae mb" href="https://twitter.com/nRocketScience" rel="noopener ugc nofollow" target="_blank"> <em class="mq">推特</em> </a> <em class="mq">。</em></p></div></div>    
</body>
</html>