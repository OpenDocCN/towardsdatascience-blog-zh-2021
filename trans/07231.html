<html>
<head>
<title>Basic Concepts of Natural Language Processing (NLP) Models and Python Implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自然语言处理(NLP)模型和Python实现的基本概念</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/basic-concepts-of-natural-language-processing-nlp-models-and-python-implementation-88a589ce1fc0?source=collection_archive---------6-----------------------#2021-07-01">https://towardsdatascience.com/basic-concepts-of-natural-language-processing-nlp-models-and-python-implementation-88a589ce1fc0?source=collection_archive---------6-----------------------#2021-07-01</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/ef75dc06f59116038bd94ed31abc1c7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PNSl-9gs_LRfbvKW8MRWQA.jpeg"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">来源:照片由Ogelamar在Unsplash上拍摄</p></figure><p id="43c0" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在数据科学领域，自然语言处理(NLP)是一个非常重要的组成部分，因为它在各个行业/部门都有广泛的应用。对于一个人来说，理解这种语言是很容易的，但是机器没有足够的能力来轻松识别它。NLP是一种使机器能够解释和理解人类交流方式的技术。</p><p id="6311" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">目前，社交媒体是自然语言的黄金数据矿，无论是来自任何在线网站(亚马逊、谷歌等)的任何类型的评论。)，或者只是在Twitter、脸书、LinkedIn或电子邮件上发帖。每个领域的业务用例(分类、文本摘要、分类、交互式语音应答(IVR)、语言翻译、聊天机器人)可能不同，但NLP定义了这些用例的核心基础解决方案。</p><p id="4075" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">自然语言是文本的自由形式，这意味着它在本质上是非结构化的。因此，在开发任何模型时，清理和准备数据以提取特征对于NLP之旅非常重要。本文将在下面介绍基本但重要的步骤，并展示我们如何使用不同的包在python中实现它们，并开发一个基于NLP的分类模型。</p><blockquote class="la lb lc"><p id="b519" class="kc kd ld ke b kf kg kh ki kj kk kl km le ko kp kq lf ks kt ku lg kw kx ky kz ij bi translated"><strong class="ke ir"> A)数据清理</strong></p><p id="4bea" class="kc kd ld ke b kf kg kh ki kj kk kl km le ko kp kq lf ks kt ku lg kw kx ky kz ij bi translated"><strong class="ke ir"> B)标记化</strong></p><p id="2cfd" class="kc kd ld ke b kf kg kh ki kj kk kl km le ko kp kq lf ks kt ku lg kw kx ky kz ij bi translated"><strong class="ke ir"> C)矢量化/单词嵌入</strong></p><p id="9f57" class="kc kd ld ke b kf kg kh ki kj kk kl km le ko kp kq lf ks kt ku lg kw kx ky kz ij bi translated"><strong class="ke ir"> D)模型开发</strong></p></blockquote></div><div class="ab cl lh li hu lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="ij ik il im in"><h1 id="3c17" class="lo lp iq bd lq lr ls lt lu lv lw lx ly lz ma mb mc md me mf mg mh mi mj mk ml bi translated"><strong class="ak"> <em class="mm"> A)数据清理</em> </strong></h1><p id="12a8" class="pw-post-body-paragraph kc kd iq ke b kf mn kh ki kj mo kl km kn mp kp kq kr mq kt ku kv mr kx ky kz ij bi translated">如上所述，数据清洗是自然语言处理中基本但非常重要的步骤。以下是数据清理的几种方法。让我们考虑下面这条线。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="a7a5" class="nb lp iq mx b gy nc nd l ne nf">line = ‘Reaching out for HELP. Please meet me in LONDON at 6 a.m xyz@abc.com #urgent’</span></pre><p id="874e" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> 1。移除停用词:</strong>有几个词是人类互动时非常常用的，但这些词没有任何意义，也没有增加任何额外的价值。此外，对于手头给出的业务案例，可能有一些词语是不需要的。所以，这些词需要从数据中删除。</p><p id="82c1" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">NLTK包为不同的语言(如英语)定义了一组停用词。在这里，我们将集中讨论“英语”停用词。如果需要，还可以考虑附加的停用词。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="3003" class="nb lp iq mx b gy nc nd l ne nf">import nltk</span><span id="5491" class="nb lp iq mx b gy ng nd l ne nf">import re</span><span id="a8c3" class="nb lp iq mx b gy ng nd l ne nf">from nltk.corpus import stopwords</span><span id="cfa3" class="nb lp iq mx b gy ng nd l ne nf"># Additional stopwords<br/>extra_list = [“let”, “may”, “might”, “must”, “need”, “apologies”, “meet”]</span><span id="784e" class="nb lp iq mx b gy ng nd l ne nf">stopword = stopwords.words(“english”)</span><span id="7476" class="nb lp iq mx b gy ng nd l ne nf">stopword.extend(extra_list)</span><span id="8856" class="nb lp iq mx b gy ng nd l ne nf">line = ‘ ‘.join([i for i in line.split() if i not in stopword])</span></pre><p id="df2a" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> 2。Make小写:</strong>要求所有的单词都用小写，以保持一致性。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="5357" class="nb lp iq mx b gy nc nd l ne nf">line = line.lower()</span></pre><p id="7323" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> 3。词汇化:</strong>这有助于将单词简化为单一形式。例如:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="3641" class="nb lp iq mx b gy nc nd l ne nf">def lemmatize_text(text):</span><span id="519e" class="nb lp iq mx b gy ng nd l ne nf">w_tokenizer = nltk.tokenize.WhitespaceTokenizer()</span><span id="5c5c" class="nb lp iq mx b gy ng nd l ne nf">lemmatizer = nltk.stem.WordNetLemmatizer()</span><span id="7c4b" class="nb lp iq mx b gy ng nd l ne nf">return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]</span><span id="0177" class="nb lp iq mx b gy ng nd l ne nf">lemmatize_text(line)</span></pre><p id="0030" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> 4。词干化:</strong>这有助于将单词还原成它们的词根形式。例如:</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="60c7" class="nb lp iq mx b gy nc nd l ne nf">def stem_porter(text):</span><span id="0792" class="nb lp iq mx b gy ng nd l ne nf">w_tokenizer = nltk.tokenize.WhitespaceTokenizer()</span><span id="9d00" class="nb lp iq mx b gy ng nd l ne nf">ps = nltk.PorterStemmer()</span><span id="3609" class="nb lp iq mx b gy ng nd l ne nf">return [ps.stem(w) for w in w_tokenizer.tokenize(text)]</span><span id="1cc9" class="nb lp iq mx b gy ng nd l ne nf">stem_porter(line)</span></pre><p id="b9c8" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">除了<em class="ld">波特斯特默</em>之外，还有两种词干。那些是兰卡斯特炮泥和雪球。雪球是对波特词干的改进。</p><p id="7513" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> 5。正则表达式的去除:</strong>正则表达式有助于识别和去除文本中不需要的不同模式。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="1244" class="nb lp iq mx b gy nc nd l ne nf">line = re.sub(‘\S*@\S*\s?’,” “,line) #email remove</span><span id="5f34" class="nb lp iq mx b gy ng nd l ne nf">line = re.sub(‘\s+’,” “,line) #new line character remove</span><span id="ab0f" class="nb lp iq mx b gy ng nd l ne nf">line = re.sub(“\’”,” “,line) #single quote remove</span><span id="16d2" class="nb lp iq mx b gy ng nd l ne nf">line = re.sub(‘_’,” “,line) #underscore remove</span><span id="fb33" class="nb lp iq mx b gy ng nd l ne nf">line = re.sub(‘http\S*\s?’,” “,line) #link remove</span><span id="3d2a" class="nb lp iq mx b gy ng nd l ne nf">line = ‘ ‘.join([i for i in line.split() if i.find(‘#’) &lt; 0]) #hasgtag remove</span><span id="1e26" class="nb lp iq mx b gy ng nd l ne nf">line = ‘ ‘.join([i for i in line.split() if i in re.findall(r’\w+’,line)]) #only keep words and numbers</span></pre><p id="6f67" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> 6。词性标注:</strong>这有助于识别词性。基于用例，人们可以保留或删除其中的一些。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="225d" class="nb lp iq mx b gy nc nd l ne nf">import spacy</span><span id="925d" class="nb lp iq mx b gy ng nd l ne nf">from spacy.tokenizer import Tokenizer</span><span id="09b3" class="nb lp iq mx b gy ng nd l ne nf">nlp = spacy.load(“en_core_web_sm”)</span><span id="f7ed" class="nb lp iq mx b gy ng nd l ne nf">tokens_spacy = nlp(line)</span><span id="28df" class="nb lp iq mx b gy ng nd l ne nf">for token in tokens_spacy:</span><span id="5b90" class="nb lp iq mx b gy ng nd l ne nf">print(token.text, ‘: ‘, token.pos_, ‘: ‘, token.is_stop)</span></pre><p id="1f76" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> 7。命名实体识别(NER): </strong>这有助于识别和分类不同的组，包括名称、地点、货币等。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="df4a" class="nb lp iq mx b gy nc nd l ne nf">for ent in tokens_spacy.ents:</span><span id="d59a" class="nb lp iq mx b gy ng nd l ne nf">print(ent.text, ‘: ‘, ent.label_)</span></pre><h1 id="19fe" class="lo lp iq bd lq lr nh lt lu lv ni lx ly lz nj mb mc md nk mf mg mh nl mj mk ml bi translated"><strong class="ak"> B)标记化</strong></h1><p id="137b" class="pw-post-body-paragraph kc kd iq ke b kf mn kh ki kj mo kl km kn mp kp kq kr mq kt ku kv mr kx ky kz ij bi translated">这是处理文本数据时的常见做法之一。这有助于将一个短语、句子或段落分成像单词或术语这样的小单元。每个单元称为一个令牌。有不同类型的标记化。我们已经在上面的例子中使用了词干分析、词性标注和NER。以下是对文本进行标记的不同方法。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="6558" class="nb lp iq mx b gy nc nd l ne nf">str1 = “I am eating pizza and, coke.”</span></pre><p id="81f6" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> 1)使用split()函数的标记化:</strong>返回给定字符串被指定分隔符打断后的字符串列表。默认情况下，分隔符是一个空格。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="a8fa" class="nb lp iq mx b gy nc nd l ne nf">str1.split()</span><span id="430d" class="nb lp iq mx b gy ng nd l ne nf">[‘I’, ‘am’, ‘eating’, ‘pizza’, ‘and,’, ‘coke.’]</span></pre><p id="6ad7" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> 2)使用正则表达式的标记化:</strong>返回基于正则表达式的列表。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="43e9" class="nb lp iq mx b gy nc nd l ne nf">re.findall(“[\w]+”,str1)</span><span id="6a81" class="nb lp iq mx b gy ng nd l ne nf">[‘I’, ‘am’, ‘eating’, ‘pizza’, ‘and’, ‘coke’]</span></pre><p id="193b" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> 3)使用NLTK进行标记化:</strong>NLTK包下有不同类型的标记化器，像word tokenizer (word_tokenize)、regex tokenizer (RegexpTokenizer)、whitespace tokenizer(whitespace tokenizer)等。</p><p id="b336" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi">3.1)</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="b513" class="nb lp iq mx b gy nc nd l ne nf">from nltk import word_tokenize</span><span id="8710" class="nb lp iq mx b gy ng nd l ne nf">word_tokenize(str1)</span><span id="2c71" class="nb lp iq mx b gy ng nd l ne nf">[‘I’, ‘am’, ‘eating’, ‘pizza’, ‘and’, ‘,’, ‘coke’, ‘.’]</span></pre><p id="9f74" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi">3.2)</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="a27a" class="nb lp iq mx b gy nc nd l ne nf">space_tokenizer = nltk.tokenize.WhitespaceTokenizer()</span><span id="96f3" class="nb lp iq mx b gy ng nd l ne nf">space _tokenizer.tokenize(str1)</span><span id="f6a9" class="nb lp iq mx b gy ng nd l ne nf">[‘I’, ‘am’, ‘eating’, ‘pizza’, ‘and,’, ‘coke,’]</span></pre><p id="3fe3" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi">3.3)</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="548c" class="nb lp iq mx b gy nc nd l ne nf">reg_tokenizer = nltk.tokenize.RegexpTokenizer(“[A-Za-z]+”)</span><span id="0ee5" class="nb lp iq mx b gy ng nd l ne nf">reg_tokenizer.tokenize(str1)</span><span id="0641" class="nb lp iq mx b gy ng nd l ne nf">[‘I’, ‘am’, ‘eating’, ‘pizza’, ‘and’, ‘coke’]</span></pre><p id="18bc" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">此外，我们可以使用这些标记化的形式来计算文本中的单词数或单词在文本中的出现频率。</p><h1 id="67f3" class="lo lp iq bd lq lr nh lt lu lv ni lx ly lz nj mb mc md nk mf mg mh nl mj mk ml bi translated"><strong class="ak"> C)矢量化/单词嵌入</strong></h1><p id="b2ae" class="pw-post-body-paragraph kc kd iq ke b kf mn kh ki kj mo kl km kn mp kp kq kr mq kt ku kv mr kx ky kz ij bi translated">一旦清理和标记化完成，从干净的数据中提取特征是非常重要的，因为机器不理解单词，而是数字。矢量化有助于将单词映射到实数向量，这进一步有助于预测。这有助于提取重要的特征。以下是用于此目的的几种技术:</p><p id="d790" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> 1。CountVec: </strong>统计特定单词在文档中出现的次数。CountVectorizer有助于获得这个计数。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="7915" class="nb lp iq mx b gy nc nd l ne nf">from sklearn.feature_extraction.text import CountVectorizer</span><span id="cdd9" class="nb lp iq mx b gy ng nd l ne nf">count_vec = CountVectorizer(analyzer=’word’, ngram_range=(1, 3), stop_words = ‘english’)</span><span id="7bf6" class="nb lp iq mx b gy ng nd l ne nf">count_vec.fit(str2)</span><span id="11c0" class="nb lp iq mx b gy ng nd l ne nf">count = count_vec.transform(str2)</span><span id="b27b" class="nb lp iq mx b gy ng nd l ne nf">vectors = count_vec.get_feature_names()</span><span id="246e" class="nb lp iq mx b gy ng nd l ne nf">smatrix = count_vec.transform(str2)</span><span id="4edd" class="nb lp iq mx b gy ng nd l ne nf">dense = smatrix.todense()</span><span id="c4ce" class="nb lp iq mx b gy ng nd l ne nf">dense_list = dense.tolist()</span><span id="1d53" class="nb lp iq mx b gy ng nd l ne nf">df_countvec = pd.DataFrame(dense_list,columns=vectors)</span></pre><p id="ec5c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> 2。TF-IDF: </strong>词频逆文档频率(TF-IDF)提供了一个词在文档中的总体权重。TfidfVectorizer有助于获得这个加权分数。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="0a0a" class="nb lp iq mx b gy nc nd l ne nf">from sklearn.feature_extraction.text import TfidfVectorizer</span><span id="b006" class="nb lp iq mx b gy ng nd l ne nf">tfidf_vec = TfidfVectorizer (analyzer=’word’, ngram_range=(1, 3), stop_words = ‘english’)</span><span id="d226" class="nb lp iq mx b gy ng nd l ne nf">tfidf_vec.fit(str2)</span><span id="d9af" class="nb lp iq mx b gy ng nd l ne nf">tfidf = tfidf_vec.transform(str2)</span><span id="53b1" class="nb lp iq mx b gy ng nd l ne nf">vectors = tfidf_vec.get_feature_names()</span><span id="6918" class="nb lp iq mx b gy ng nd l ne nf">smatrix = tfidf_vec.transform(str2)</span><span id="c8c1" class="nb lp iq mx b gy ng nd l ne nf">dense = smatrix.todense()</span><span id="4b20" class="nb lp iq mx b gy ng nd l ne nf">dense_list = dense.tolist()</span><span id="c561" class="nb lp iq mx b gy ng nd l ne nf">df_ tfidf = pd.DataFrame(dense_list,columns=vectors)</span></pre><p id="aecd" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">比较:</strong> CountVec可能会提供偏向最常用词的结果。这忽略了可能具有更高重要性的罕见单词。这意味着我们需要惩罚最常用的单词。TF-IDF启用惩罚效果。它通过测量文档中出现的单词来衡量计数。举个例子，</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="543c" class="nb lp iq mx b gy nc nd l ne nf">str2 = [‘I am going to test Covid’,</span><span id="955e" class="nb lp iq mx b gy ng nd l ne nf">‘It seems ABC hospital is doing the Covid test’,</span><span id="d894" class="nb lp iq mx b gy ng nd l ne nf">‘Covaxin is still in WIP phase’]</span></pre><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/08b009f5bcc215c34d65bef7a04955e2.png" data-original-src="https://miro.medium.com/v2/resize:fit:810/format:webp/1*rMdu0sDUQpw6klBzj2FmKg.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">CountVec输出(图片由作者提供)</p></figure><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/2c5acaea56ba63fb163e5719dbf611d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:830/format:webp/1*aF5UV6FDRYoc2CDZL3aZgQ.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">TF-IDF输出(图片由作者提供)</p></figure><p id="9ce2" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">这两种模型都为每个单词提供了一个数字(计数或重量)。但是要理解每个单词的上下文并识别其内容，每个单词一个向量要合适得多。<strong class="ke ir"> Word2Vec </strong>通过浏览给定文档，为每个单词提供一个向量，这比简单的单词包或TF-IDF更有用。但Word2Vec缺乏对这种关系的“本地理解”，这一点由<strong class="ke ir"> Glove </strong>回答。Glove是一种预先训练的矢量化技术，它不仅理解本地上下文，还理解与全局单词的关系。除了Glove，<strong class="ke ir"> FastText </strong>是另一种流行的单词嵌入技术，它对罕见单词或未收录单词(OOV)更有效。</p><p id="6539" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">说了这么多，单词包或TF-IDF(主要)至今仍被广泛使用，并且是日常NLP问题中非常重要的一部分。由于本文只打算介绍基本的NLP概念，Glove或FastText没有详细介绍。</p><h1 id="dcfe" class="lo lp iq bd lq lr nh lt lu lv ni lx ly lz nj mb mc md nk mf mg mh nl mj mk ml bi translated">d)模型开发</h1><p id="98cd" class="pw-post-body-paragraph kc kd iq ke b kf mn kh ki kj mo kl km kn mp kp kq kr mq kt ku kv mr kx ky kz ij bi translated">最后一部分来了！最后，我们有一个基于计数或TF-IDF矩阵和因变量(标签)来开发模型。</p><figure class="ms mt mu mv gt jr gh gi paragraph-image"><div class="gh gi no"><img src="../Images/8da9417a083903e8cce384165f6eacdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1046/format:webp/1*jC-4jin3lOV-fmtUeoPM0A.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="bec6" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">人们可以使用任何分类模型，如逻辑回归、随机森林(RF)、支持向量机(SVM)或任何深度学习模型，如RNN、LSTM或最先进的模型，如BERT、GPT3来预测标签。作为准确性ROC、回忆、F1分数的度量，可以基于手头的问题陈述来使用。</p><p id="9338" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在运行模型之前，让我们将数据分成训练和测试。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="30de" class="nb lp iq mx b gy nc nd l ne nf">X = df.drop(columns=’label’)</span><span id="7520" class="nb lp iq mx b gy ng nd l ne nf">y = df[‘label’]</span><span id="299e" class="nb lp iq mx b gy ng nd l ne nf">X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=420,stratify=y)</span></pre><p id="e8b6" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">现在，建立数据模型并检查准确性指标。</p><pre class="ms mt mu mv gt mw mx my mz aw na bi"><span id="d375" class="nb lp iq mx b gy nc nd l ne nf">from sklearn.linear_model import LogisticRegression, RandomForestClassifier</span><span id="3418" class="nb lp iq mx b gy ng nd l ne nf">model = LogisticRegression()</span><span id="0f2f" class="nb lp iq mx b gy ng nd l ne nf">#model = RandomForestClassifier()</span><span id="0162" class="nb lp iq mx b gy ng nd l ne nf">model.fit(X_train, y_train)</span><span id="47fe" class="nb lp iq mx b gy ng nd l ne nf">y_pred_class = model.predict(X_test)</span><span id="54d0" class="nb lp iq mx b gy ng nd l ne nf">print(‘Accuracy: ‘, metrics.accuracy_score(y_test, y_pred_class))</span><span id="41b0" class="nb lp iq mx b gy ng nd l ne nf">print(confusion_matrix(y_test, y_pred_class))</span></pre></div><div class="ab cl lh li hu lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="ij ik il im in"><p id="fc36" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">除了分类问题之外，自然语言处理还可以用于文本摘要、问答、主题建模(链接)、文本翻译等。</p><p id="c486" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">希望这篇文章能帮助你了解如何解决任何基于NLP的问题。在那之前，学习愉快！</p></div><div class="ab cl lh li hu lj" role="separator"><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm ln"/><span class="lk bw bk ll lm"/></div><div class="ij ik il im in"><p id="a7f8" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><em class="ld">免责声明:本文中表达的观点是作者以个人身份发表的意见，而非其雇主的意见</em></p></div></div>    
</body>
</html>