<html>
<head>
<title>How to Check if a Classification Model is Overfitted using scikit-learn</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何使用scikit-learn检查分类模型是否过度拟合</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-check-if-a-classification-model-is-overfitted-using-scikit-learn-148b6b19af8b?source=collection_archive---------6-----------------------#2021-06-04">https://towardsdatascience.com/how-to-check-if-a-classification-model-is-overfitted-using-scikit-learn-148b6b19af8b?source=collection_archive---------6-----------------------#2021-06-04</a></blockquote><div><div class="fc ik il im in io"/><div class="ip iq ir is it"><h2 id="fbb1" class="iu iv iw bd b dl ix iy iz ja jb jc dk jd translated" aria-label="kicker paragraph">数据分析</h2><div class=""/><div class=""><h2 id="afb7" class="pw-subtitle-paragraph kc jf iw bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">Python中的现成教程，有助于识别和减少过度拟合</h2></div><figure class="kv kw kx ky gu kz gi gj paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gi gj ku"><img src="../Images/28a75606dcf5a489fcf32d8c1e0981fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mSELPVE8Jr2lIQ6i55BSMA.jpeg"/></div></div><p class="lg lh gk gi gj li lj bd b be z dk translated">图片由<a class="ae lk" href="https://pixabay.com/users/rollstein-13853955/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=4727161" rel="noopener ugc nofollow" target="_blank">罗兰·施泰因曼</a>从<a class="ae lk" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=4727161" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>拍摄</p></figure><p id="3a49" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">在处理机器学习算法时，最困难的问题之一是评估经过训练的模型在未知样本下是否表现良好。例如，可能会发生这样的情况:模型在给定的数据集下表现很好，但在部署时却无法预测正确的值。训练数据和测试数据之间的这种不一致可能是由不同的问题造成的。最常见的问题之一是过度拟合。</p><blockquote class="mh mi mj"><p id="99fd" class="ll lm mk ln b lo lp kg lq lr ls kj lt ml lv lw lx mm lz ma mb mn md me mf mg ip bi translated">一个模型很好地适应了训练集，但测试集很差，这种模型被称为对训练集过度适应，而一个模型对两个集都不太适应，这种模型被称为欠适应。摘自<a class="ae lk" rel="noopener" target="_blank" href="/the-relationship-between-bias-variance-overfitting-generalisation-in-machine-learning-models-fb78614a3f1e">这篇非常有趣的文章</a>作者Joe Kadi。</p></blockquote><p id="d966" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">换句话说，<strong class="ln jg">过度拟合意味着机器学习模型能够对训练集进行太好的建模。</strong></p><p id="0df4" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">在本教程中，我利用Python <code class="fe mo mp mq mr b">scikit-learn</code>库来检查分类模型是否过度拟合。同样的程序也可以用于其他模型，如回归。提议的战略包括以下步骤:</p><ul class=""><li id="e318" class="ms mt iw ln b lo lp lr ls lu mu ly mv mc mw mg mx my mz na bi translated">将数据集分成训练集和测试集</li><li id="a38f" class="ms mt iw ln b lo nb lr nc lu nd ly ne mc nf mg mx my mz na bi translated">用训练集训练模型</li><li id="ec9c" class="ms mt iw ln b lo nb lr nc lu nd ly ne mc nf mg mx my mz na bi translated">在训练集和测试集上测试模型</li><li id="fdba" class="ms mt iw ln b lo nb lr nc lu nd ly ne mc nf mg mx my mz na bi translated">计算训练集和测试集的平均绝对误差(MAE)</li><li id="46a8" class="ms mt iw ln b lo nb lr nc lu nd ly ne mc nf mg mx my mz na bi translated">绘制和解释结果</li></ul><p id="0fe9" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">必须对不同的训练集和测试集执行前面的步骤。</p><p id="6a2f" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">作为示例数据集，我使用心脏病发作数据集，可在<a class="ae lk" href="https://www.kaggle.com/rashikrahmanpritom/heart-attack-analysis-prediction-dataset" rel="noopener ugc nofollow" target="_blank"> Kaggle存储库</a>中获得。所有代码都可以从我的<a class="ae lk" href="https://github.com/alod83/data-science/blob/master/DataAnalysis/Overfitting.ipynb" rel="noopener ugc nofollow" target="_blank"> Github库</a>下载。</p><h1 id="b372" class="ng nh iw bd ni nj nk nl nm nn no np nq kl nr km ns ko nt kp nu kr nv ks nw nx bi translated">加载数据</h1><p id="7c5f" class="pw-post-body-paragraph ll lm iw ln b lo ny kg lq lr nz kj lt lu oa lw lx ly ob ma mb mc oc me mf mg ip bi translated">首先，我通过Python <code class="fe mo mp mq mr b">pandas</code>库将数据集作为数据帧加载。数据集包含303条记录、13个输入要素和1个输出类，输出类可以是0或1。</p><pre class="kv kw kx ky gu od mr oe of aw og bi"><span id="0a56" class="oh nh iw mr b gz oi oj l ok ol">import pandas as pd</span><span id="d8f7" class="oh nh iw mr b gz om oj l ok ol">df = pd.read_csv('source/heart.csv')<br/>df.head()</span></pre><figure class="kv kw kx ky gu kz gi gj paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gi gj on"><img src="../Images/68da21b4ec592d2b25222c385168492b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gwV1kUQJHoZx3HpMIk76sA.png"/></div></div><p class="lg lh gk gi gj li lj bd b be z dk translated">作者图片</p></figure><p id="18df" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">我建立数据集。我定义了两个变量，<code class="fe mo mp mq mr b">X</code>和<code class="fe mo mp mq mr b">y</code>，分别对应于输入和输出。</p><pre class="kv kw kx ky gu od mr oe of aw og bi"><span id="e1f9" class="oh nh iw mr b gz oi oj l ok ol">features = []<br/>for column in df.columns:<br/>    if column != 'output':<br/>        features.append(column)<br/>X = df[features]<br/>y = df['output']</span></pre><h1 id="eaf9" class="ng nh iw bd ni nj nk nl nm nn no np nq kl nr km ns ko nt kp nu kr nv ks nw nx bi translated">构建并测试模型</h1><p id="4b12" class="pw-post-body-paragraph ll lm iw ln b lo ny kg lq lr nz kj lt lu oa lw lx ly ob ma mb mc oc me mf mg ip bi translated">通常，<code class="fe mo mp mq mr b">X</code>和<code class="fe mo mp mq mr b">y</code>被分成两个数据集:训练集和测试集。在<code class="fe mo mp mq mr b">scikit-learn</code>中，这可以通过<code class="fe mo mp mq mr b">train_test_split()</code>函数完成，该函数返回训练和测试数据。然后，通过训练数据拟合模型，并通过测试数据检验其性能。然而，所描述的策略不允许验证模型是否过度拟合。</p><p id="5443" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">出于这个原因，我不使用<code class="fe mo mp mq mr b">train_test_split()</code>函数，而是使用K折叠交叉验证。</p><blockquote class="mh mi mj"><p id="f2c5" class="ll lm mk ln b lo lp kg lq lr ls kj lt ml lv lw lx mm lz ma mb mn md me mf mg ip bi translated">K Folds将数据集拆分成k个子集，在不同的训练集上训练模型k次，在不同的测试集上测试模型k次。每次，训练集由k-1个子集组成，而测试集是剩余的子集。</p></blockquote><p id="b0db" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated"><code class="fe mo mp mq mr b">scikit-learn</code>库为K折叠提供了一个类，称为<code class="fe mo mp mq mr b">KFold()</code>，它接收数字<code class="fe mo mp mq mr b">k</code>作为输入。对于每一对(训练集、测试集)，我可以建立模型并计算训练集和测试集的平均绝对误差(MAE)。在这个具体的例子中，我利用了<code class="fe mo mp mq mr b">KNeighborsClassifier()</code>。</p><pre class="kv kw kx ky gu od mr oe of aw og bi"><span id="c8cd" class="oh nh iw mr b gz oi oj l ok ol">from sklearn.model_selection import KFold<br/>from sklearn.neighbors import KNeighborsClassifier<br/>from sklearn.metrics import mean_absolute_error<br/>import matplotlib.pyplot as plt</span><span id="0e00" class="oh nh iw mr b gz om oj l ok ol">kf = KFold(n_splits=4)<br/>mae_train = []<br/>mae_test = []<br/>for train_index, test_index in kf.split(X):<br/>    <br/>   X_train, X_test = X.iloc[train_index], X.iloc[test_index]<br/>   y_train, y_test = y[train_index], y[test_index]</span><span id="136f" class="oh nh iw mr b gz om oj l ok ol">   model = KNeighborsClassifier(n_neighbors=2)<br/>   model.fit(X_train, y_train)<br/>   y_train_pred = model.predict(X_train)<br/>   y_test_pred = model.predict(X_test)<br/>   mae_train.append(mean_absolute_error(y_train, y_train_pred))<br/>   mae_test.append(mean_absolute_error(y_test, y_test_pred))</span></pre><p id="1bb8" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">然后，我可以绘制培训和测试MAEs，并对它们进行比较。</p><pre class="kv kw kx ky gu od mr oe of aw og bi"><span id="9058" class="oh nh iw mr b gz oi oj l ok ol">folds = range(1, kf.get_n_splits() + 1)<br/>plt.plot(folds, mae_train, 'o-', color='green', label='train')<br/>plt.plot(folds, mae_test, 'o-', color='red', label='test')<br/>plt.legend()<br/>plt.grid()<br/>plt.xlabel('Number of fold')<br/>plt.ylabel('Mean Absolute Error')<br/>plt.show()</span></pre><figure class="kv kw kx ky gu kz gi gj paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gi gj oo"><img src="../Images/efa5aa2df7a489862a0acd4dbd5c0d88.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_eWu-i6fcbLcntp-AS4CZg.png"/></div></div><p class="lg lh gk gi gj li lj bd b be z dk translated">作者图片</p></figure><p id="570e" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">我注意到，对于所有k倍，训练MAE非常小(大约0.2)。相反，测试MAE非常大。范围从0.3到0.8。因为训练MAE很小，而测试MAE很大，所以我可以断定这个模型是过度拟合的。</p><p id="f204" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">我将所有之前的操作组合成一个名为<code class="fe mo mp mq mr b">test_model()</code>的函数，它接收模型以及<code class="fe mo mp mq mr b">X</code>和<code class="fe mo mp mq mr b">y</code>变量作为输入。</p><h1 id="0610" class="ng nh iw bd ni nj nk nl nm nn no np nq kl nr km ns ko nt kp nu kr nv ks nw nx bi translated">限制过度拟合</h1><p id="0a38" class="pw-post-body-paragraph ll lm iw ln b lo ny kg lq lr nz kj lt lu oa lw lx ly ob ma mb mc oc me mf mg ip bi translated">可以通过以下两种策略(潜在地)限制过拟合:</p><ul class=""><li id="5d2e" class="ms mt iw ln b lo lp lr ls lu mu ly mv mc mw mg mx my mz na bi translated">降低复杂性</li><li id="2898" class="ms mt iw ln b lo nb lr nc lu nd ly ne mc nf mg mx my mz na bi translated">调整参数</li><li id="a4bd" class="ms mt iw ln b lo nb lr nc lu nd ly ne mc nf mg mx my mz na bi translated">改变模式。</li></ul><h2 id="092b" class="oh nh iw bd ni op oq dn nm or os dp nq lu ot ou ns ly ov ow nu mc ox oy nw jc bi translated">1.降低复杂性</h2><p id="ed78" class="pw-post-body-paragraph ll lm iw ln b lo ny kg lq lr nz kj lt lu oa lw lx ly ob ma mb mc oc me mf mg ip bi translated">我试图通过将所有输入特征缩放到0到1之间的范围来改进模型。我利用了由<code class="fe mo mp mq mr b">scikit-learn</code>库提供的<code class="fe mo mp mq mr b">MinMaxScaler()</code>。</p><pre class="kv kw kx ky gu od mr oe of aw og bi"><span id="5526" class="oh nh iw mr b gz oi oj l ok ol">from sklearn.preprocessing import MinMaxScaler<br/>import numpy as np</span><span id="b934" class="oh nh iw mr b gz om oj l ok ol">for column in X.columns:<br/>    feature = np.array(X[column]).reshape(-1,1)<br/>    scaler = MinMaxScaler()<br/>    scaler.fit(feature)<br/>    feature_scaled = scaler.transform(feature)<br/>    X[column] = feature_scaled.reshape(1,-1)[0]</span></pre><p id="cd7d" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">我构建了一个新模型，并调用了<code class="fe mo mp mq mr b">test_model()</code>函数。现在，该模型在训练集和测试集上的性能都有所提高。然而，测试MAE仍然很大。因此，模型仍然是过度拟合的。</p><pre class="kv kw kx ky gu od mr oe of aw og bi"><span id="b5c6" class="oh nh iw mr b gz oi oj l ok ol">model = KNeighborsClassifier(n_neighbors=2)<br/>test_model(model, X,y)</span></pre><figure class="kv kw kx ky gu kz gi gj paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gi gj oz"><img src="../Images/a3573ca4e418926145ce28049da1998b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-ErdKBtI0dRKaIbY4b6oKg.png"/></div></div><p class="lg lh gk gi gj li lj bd b be z dk translated">作者图片</p></figure><p id="5f65" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">降低复杂性的另一种可能性是减少特征的数量。这可以通过主成分分析来实现。例如，我可以将输入要素的数量从13个减少到2个。为此，<code class="fe mo mp mq mr b">scikit-learn</code>库提供了<code class="fe mo mp mq mr b">PCA()</code>类。</p><pre class="kv kw kx ky gu od mr oe of aw og bi"><span id="0893" class="oh nh iw mr b gz oi oj l ok ol">from sklearn.decomposition import PCA<br/>pca = PCA(n_components=2)<br/>pca.fit(X)<br/>X_pca = pca.transform(X)<br/>X_pca = pd.DataFrame(X_pca)</span></pre><p id="c560" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">我在新特性上测试了模型，我注意到模型的性能提高了。现在测试MAE的范围从0.20到0.45。</p><figure class="kv kw kx ky gu kz gi gj paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gi gj pa"><img src="../Images/5f4e17643644b71f412d997b6ce9d12b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eQQ4rc5ud-vXOwpACWwATw.png"/></div></div><p class="lg lh gk gi gj li lj bd b be z dk translated">作者图片</p></figure><h2 id="5d5d" class="oh nh iw bd ni op oq dn nm or os dp nq lu ot ou ns ly ov ow nu mc ox oy nw jc bi translated">2.调整参数</h2><p id="f327" class="pw-post-body-paragraph ll lm iw ln b lo ny kg lq lr nz kj lt lu oa lw lx ly ob ma mb mc oc me mf mg ip bi translated">另一种可能性是调整算法参数。<code class="fe mo mp mq mr b">scikit-learn</code>库提供了<code class="fe mo mp mq mr b">GridSearchCV()</code>类，允许搜索特定模型的最佳参数。要调整的参数必须作为<code class="fe mo mp mq mr b">dict</code>传递，对于每个参数，必须传递要分析的值列表。另外，<code class="fe mo mp mq mr b">GridSearchCV()</code>战功也是交叉验证的。在变量<code class="fe mo mp mq mr b">best_estimator_</code>中，最佳估计值在拟合后可用。</p><pre class="kv kw kx ky gu od mr oe of aw og bi"><span id="c5a2" class="oh nh iw mr b gz oi oj l ok ol">from sklearn.model_selection import GridSearchCV</span><span id="9d1f" class="oh nh iw mr b gz om oj l ok ol">model = KNeighborsClassifier()</span><span id="d7c2" class="oh nh iw mr b gz om oj l ok ol">param_grid = {<br/>   'n_neighbors': np.arange(1,30),<br/>   'algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute'],<br/>    'metric' : ['euclidean','manhattan','chebyshev','minkowski']<br/>}</span><span id="4cca" class="oh nh iw mr b gz om oj l ok ol">grid = GridSearchCV(model, param_grid = param_grid, cv=4)<br/>grid.fit(X, y)<br/>best_estimator = grid.best_estimator_</span></pre><p id="aa0d" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">我测试模型。我注意到测试MAE的范围从大约0.25到0.45。</p><pre class="kv kw kx ky gu od mr oe of aw og bi"><span id="6eb7" class="oh nh iw mr b gz oi oj l ok ol">test_model(best_estimator, X,y)</span></pre><figure class="kv kw kx ky gu kz gi gj paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gi gj pb"><img src="../Images/a59c92288719939d2b0e319e3a2ccd48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sWZZZLXTvhrevwHzCRkOQQ.png"/></div></div><p class="lg lh gk gi gj li lj bd b be z dk translated">作者图片</p></figure><h2 id="9272" class="oh nh iw bd ni op oq dn nm or os dp nq lu ot ou ns ly ov ow nu mc ox oy nw jc bi translated">3.改变模式</h2><p id="0f72" class="pw-post-body-paragraph ll lm iw ln b lo ny kg lq lr nz kj lt lu oa lw lx ly ob ma mb mc oc me mf mg ip bi translated">先前的尝试减少了过度拟合。然而，模型性能仍然很差。因此，我试图改变模式。我试一个<code class="fe mo mp mq mr b">GaussianNB()</code>型号。我注意到性能有了惊人的提高。事实上，测试MAE的范围在0.20和0.35之间，因此我可以得出结论，该模型没有过度拟合。</p><pre class="kv kw kx ky gu od mr oe of aw og bi"><span id="ab80" class="oh nh iw mr b gz oi oj l ok ol">from sklearn.naive_bayes import GaussianNB<br/>model = GaussianNB()<br/>test_model(model, X,y)</span></pre><figure class="kv kw kx ky gu kz gi gj paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gi gj pc"><img src="../Images/cc837a6519113bbc457f376d26a0ea67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OJcyS3JKmbV-cXjsg5RSFA.png"/></div></div><p class="lg lh gk gi gj li lj bd b be z dk translated">作者图片</p></figure><h1 id="2dc4" class="ng nh iw bd ni nj nk nl nm nn no np nq kl nr km ns ko nt kp nu kr nv ks nw nx bi translated">摘要</h1><p id="9d56" class="pw-post-body-paragraph ll lm iw ln b lo ny kg lq lr nz kj lt lu oa lw lx ly ob ma mb mc oc me mf mg ip bi translated">在本教程中，我已经说明了如何检查分类模型是否过度拟合。此外，我还提出了三种策略来限制过度拟合:降低复杂性、调整参数和改变模型。</p><p id="0395" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">正如这个具体例子中所描述的，我们经常使用错误的模型来处理问题。因此，我的建议是:<strong class="ln jg">请尝试不同的型号！</strong></p><p id="1cd6" class="pw-post-body-paragraph ll lm iw ln b lo lp kg lq lr ls kj lt lu lv lw lx ly lz ma mb mc md me mf mg ip bi translated">如果你想了解我的研究和其他活动的最新情况，你可以在<a class="ae lk" href="https://twitter.com/alod83" rel="noopener ugc nofollow" target="_blank"> Twitter </a>、<a class="ae lk" href="https://www.youtube.com/channel/UC4O8-FtQqGIsgDW_ytXIWOg?view_as=subscriber" rel="noopener ugc nofollow" target="_blank"> Youtube </a>和<a class="ae lk" href="https://github.com/alod83" rel="noopener ugc nofollow" target="_blank"> Github </a>上关注我。</p><h1 id="f046" class="ng nh iw bd ni nj nk nl nm nn no np nq kl nr km ns ko nt kp nu kr nv ks nw nx bi translated">相关文章</h1><div class="pd pe gq gs pf pg"><a rel="noopener follow" target="_blank" href="/a-complete-data-analysis-workflow-in-python-and-scikit-learn-9a77f7c283d3"><div class="ph ab fp"><div class="pi ab pj cl cj pk"><h2 class="bd jg gz z fq pl fs ft pm fv fx jf bi translated">Python和scikit中的完整数据分析工作流程-学习</h2><div class="pn l"><p class="bd b dl z fq pl fs ft pm fv fx dk translated">towardsdatascience.com</p></div></div><div class="po l"><div class="pp l pq pr ps po pt le pg"/></div></div></a></div><div class="pd pe gq gs pf pg"><a rel="noopener follow" target="_blank" href="/how-to-improve-the-performance-of-a-supervised-machine-learning-algorithm-c9f9f2705a5c"><div class="ph ab fp"><div class="pi ab pj cl cj pk"><h2 class="bd jg gz z fq pl fs ft pm fv fx jf bi translated">如何提高(监督)机器学习算法的性能</h2><div class="pu l"><h3 class="bd b gz z fq pl fs ft pm fv fx dk translated">如今，每个数据科学家都能够编写代码来训练机器学习算法:加载一个代码就足够了</h3></div><div class="pn l"><p class="bd b dl z fq pl fs ft pm fv fx dk translated">towardsdatascience.com</p></div></div><div class="po l"><div class="pv l pq pr ps po pt le pg"/></div></div></a></div><div class="pd pe gq gs pf pg"><a rel="noopener follow" target="_blank" href="/data-preprocessing-in-python-pandas-part-6-dropping-duplicates-e35e46bcc9d6"><div class="ph ab fp"><div class="pi ab pj cl cj pk"><h2 class="bd jg gz z fq pl fs ft pm fv fx jf bi translated">Python Pandas中的数据预处理—第6部分删除重复项</h2><div class="pu l"><h3 class="bd b gz z fq pl fs ft pm fv fx dk translated">使用Python熊猫库删除重复项的快速教程。</h3></div><div class="pn l"><p class="bd b dl z fq pl fs ft pm fv fx dk translated">towardsdatascience.com</p></div></div><div class="po l"><div class="pw l pq pr ps po pt le pg"/></div></div></a></div><h1 id="8e6a" class="ng nh iw bd ni nj nk nl nm nn no np nq kl nr km ns ko nt kp nu kr nv ks nw nx bi translated">来自社区</h1><div class="pd pe gq gs pf pg"><a rel="noopener follow" target="_blank" href="/is-your-model-overfitting-or-maybe-underfitting-an-example-using-a-neural-network-in-python-4faf155398d2"><div class="ph ab fp"><div class="pi ab pj cl cj pk"><h2 class="bd jg gz z fq pl fs ft pm fv fx jf bi translated">你的模型是否过拟合？或者不合身？python中使用神经网络的示例</h2><div class="pu l"><h3 class="bd b gz z fq pl fs ft pm fv fx dk translated">过拟合，欠拟合，泛化能力，交叉验证。一切都简单解释了。我还提供了一个…</h3></div><div class="pn l"><p class="bd b dl z fq pl fs ft pm fv fx dk translated">towardsdatascience.com</p></div></div><div class="po l"><div class="px l pq pr ps po pt le pg"/></div></div></a></div><div class="pd pe gq gs pf pg"><a rel="noopener follow" target="_blank" href="/the-relationship-between-bias-variance-overfitting-generalisation-in-machine-learning-models-fb78614a3f1e"><div class="ph ab fp"><div class="pi ab pj cl cj pk"><h2 class="bd jg gz z fq pl fs ft pm fv fx jf bi translated">机器学习模型中偏差、方差、过拟合和泛化的关系</h2><div class="pn l"><p class="bd b dl z fq pl fs ft pm fv fx dk translated">towardsdatascience.com</p></div></div><div class="po l"><div class="py l pq pr ps po pt le pg"/></div></div></a></div><h1 id="2256" class="ng nh iw bd ni nj nk nl nm nn no np nq kl nr km ns ko nt kp nu kr nv ks nw nx bi translated">新到中？您可以每月订阅几美元，并解锁无限的文章— <a class="ae lk" href="https://alod83.medium.com/membership" rel="noopener">点击此处</a>。</h1></div></div>    
</body>
</html>