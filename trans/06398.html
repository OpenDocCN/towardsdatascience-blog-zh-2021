<html>
<head>
<title>Yes, XGBoost is cool, but have you heard of CatBoost?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">是的，XGBoost很酷，但是你听说过CatBoost吗？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/yes-xgboost-is-cool-but-have-you-heard-of-catboost-1930ebacd36d?source=collection_archive---------28-----------------------#2021-06-08">https://towardsdatascience.com/yes-xgboost-is-cool-but-have-you-heard-of-catboost-1930ebacd36d?source=collection_archive---------28-----------------------#2021-06-08</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0b97" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">对这个现代梯度推进库的介绍</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c43f8a349975cd8e719321efb4b8eb1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oRmOhY_vIxYlxeAfmOYU_w.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在<a class="ae ky" href="https://unsplash.com/s/photos/cat?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae ky" href="https://unsplash.com/@madhatterzone?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Manja Vitolic </a>拍摄的照片</p></figure><p id="9bc6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您曾经做过数据科学家，参加过Kaggle竞赛，甚至在互联网上浏览过数据科学文章，那么您很有可能听说过XGBoost。即使在今天，它也经常是许多从事一般机器学习任务的Kagglers和数据科学家的首选算法。</p><p id="8d49" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">虽然XGBoost很受欢迎，但它确实有一些局限性，我在下面的文章中提到了这一点。</p><div class="lv lw gp gr lx ly"><a rel="noopener follow" target="_blank" href="/why-xgboost-cant-solve-all-your-problems-b5003a62d12a"><div class="lz ab fo"><div class="ma ab mb cl cj mc"><h2 class="bd iu gy z fp md fr fs me fu fw is bi translated">为什么XGBoost不能解决你所有的问题。</h2><div class="mf l"><h3 class="bd b gy z fp md fr fs me fu fw dk translated">XGBoost和其他基于树的算法的一个关键限制。</h3></div><div class="mg l"><p class="bd b dl z fp md fr fs me fu fw dk translated">towardsdatascience.com</p></div></div><div class="mh l"><div class="mi l mj mk ml mh mm ks ly"/></div></div></a></div><p id="58ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可能听说过XGBoost，但是您听说过CatBoost吗？CatBoost是Yandex的研究人员创建的另一个开源梯度增强库。虽然它可能比XGBoost慢，但它仍然有几个有趣的特性，可以作为替代方案使用，或者包含在XGBoost的集成模型中。对于一些基准数据集，CatBoost甚至优于XGBoost。</p><p id="94f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">在本文中，我将把这个框架与XGBoost进行比较，并演示如何在一个简单的数据集上训练CatBoost模型。</strong></p><h1 id="bd49" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">CatBoost和XGBoost有什么不同？</h1><p id="bf10" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">和XGBoost一样，CatBoost也是一个梯度提升框架。但是，CatBoost有几个与XGBoost不同的特性，如下所示:</p><ul class=""><li id="dfb1" class="nk nl it lb b lc ld lf lg li nm lm nn lq no lu np nq nr ns bi translated"><strong class="lb iu"> CatBoost是梯度增强的一种不同实现，它利用了一种称为<em class="nt">有序增强</em>的概念，这在</strong> <a class="ae ky" href="https://arxiv.org/pdf/1706.09516.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu"> CatBoost论文</strong> </a> <strong class="lb iu">中有详细介绍。</strong></li><li id="2329" class="nk nl it lb b lc nu lf nv li nw lm nx lq ny lu np nq nr ns bi translated"><strong class="lb iu">因为CatBoost具有不同的梯度增强实现，所以它有可能在某些任务上胜过其他实现。</strong></li><li id="3d56" class="nk nl it lb b lc nu lf nv li nw lm nx lq ny lu np nq nr ns bi translated"><strong class="lb iu"> CatBoost具有用于交叉验证和网格搜索的可视化小部件，可在Jupyter笔记本中查看。</strong></li><li id="930e" class="nk nl it lb b lc nu lf nv li nw lm nx lq ny lu np nq nr ns bi translated"><strong class="lb iu">CatBoost中的</strong> <a class="ae ky" href="https://catboost.ai/docs/concepts/python-reference_pool.html" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">池模块</strong> </a> <strong class="lb iu">支持分类和文本特征的预处理。</strong></li></ul><p id="05e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要获得完整的特性列表，请务必查看<a class="ae ky" href="https://catboost.ai/docs/concepts/about.html" rel="noopener ugc nofollow" target="_blank"> CatBoost文档页面</a>。虽然CatBoost确实有额外的特性，但是这个实现的主要缺点是它通常比XGBoost慢。但是如果你愿意牺牲速度，这种权衡在某些情况下可能是合理的。</p><h1 id="76a8" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">装置</h1><p id="a15d" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">要安装带有pip的CatBoost，只需运行下面列出的命令。</p><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="37b7" class="oe mo it oa b gy of og l oh oi">pip install catboost</span></pre><p id="fe48" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">或者，您也可以使用以下命令将CatBoost与Conda一起安装。</p><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="0ffc" class="oe mo it oa b gy of og l oh oi">conda config --add channels conda-forge<br/>conda install catboost</span></pre><h1 id="7c53" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">使用CatBoost分类</h1><p id="9c91" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">在本教程中，我将演示如何使用通过Scikit-learn生成的简单数据集，通过CatBoost训练分类模型。你可以在<a class="ae ky" href="https://github.com/AmolMavuduru/CatBoostTutorial" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到这个教程的<a class="ae ky" href="https://github.com/AmolMavuduru/CatBoostTutorial" rel="noopener ugc nofollow" target="_blank">完整代码</a>。</p><h2 id="75ce" class="oe mo it bd mp oj ok dn mt ol om dp mx li on oo mz lm op oq nb lq or os nd ot bi translated">导入库</h2><p id="4a5c" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">在下面的代码中，我从CatBoost导入了Numpy和Pandas等基本库以及一些模块。</p><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="ae9f" class="oe mo it oa b gy of og l oh oi">import numpy as np<br/>import pandas as pd<br/>from catboost import CatBoostClassifier, Pool, cv</span></pre><h2 id="0760" class="oe mo it bd mp oj ok dn mt ol om dp mx li on oo mz lm op oq nb lq or os nd ot bi translated">创建数据集</h2><p id="9ff0" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">在下面的代码中，我使用Scikit-learn中的make_classification函数创建了一个数据集。</p><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="131b" class="oe mo it oa b gy of og l oh oi">from sklearn.datasets import make_classification<br/>X, y = make_classification(n_samples=50000, <br/>                           n_features=20, <br/>                           n_informative=15, <br/>                           n_redundant=5,<br/>                           n_clusters_per_class=5,<br/>                           class_sep=0.7,<br/>                           flip_y=0.03,<br/>                           n_classes=2)</span></pre><p id="29f5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们可以使用下面的代码将数据集分成训练集和测试集。</p><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="afb6" class="oe mo it oa b gy of og l oh oi">from sklearn.model_selection import train_test_split</span><span id="0f6a" class="oe mo it oa b gy ou og l oh oi">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)</span></pre><h2 id="fea9" class="oe mo it bd mp oj ok dn mt ol om dp mx li on oo mz lm op oq nb lq or os nd ot bi translated">训练模型</h2><p id="c2ed" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">CatBoost有一个非常简单的Scikit-learn风格的API用于训练模型。我们可以实例化一个CatBoostClassifier对象，并根据训练数据对其进行训练，如下面的代码所示。请注意，迭代参数对应于提升迭代的次数(或树的数量)。</p><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="d9fa" class="oe mo it oa b gy of og l oh oi">model = CatBoostClassifier(iterations=100,<br/>                           depth=2,<br/>                           learning_rate=1,<br/>                           loss_function='Logloss',<br/>                           verbose=True)</span><span id="6a5d" class="oe mo it oa b gy ou og l oh oi">model.fit(X_train, y_train)</span></pre><p id="a918" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当verbose参数设置为True时，训练模型会将每次迭代中的训练损失写入标准输出。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ov"><img src="../Images/560d0dc8abc5c48aef7392c817204df5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vBEE-p7e5Riy6NH9GoXYKA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">通过训练模型生成的标准输出日志。</p></figure><p id="fecb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意总时间以及剩余时间是如何写入标准输出的。</p><h2 id="cc93" class="oe mo it bd mp oj ok dn mt ol om dp mx li on oo mz lm op oq nb lq or os nd ot bi translated">计算特征统计</h2><p id="b355" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">我们还可以使用calc_feature_statistics函数从训练数据集中计算详细的要素统计数据，如下所示。</p><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="f52a" class="oe mo it oa b gy of og l oh oi">model.calc_feature_statistics(X_train, y_train, feature=1, plot=True)</span></pre><p id="11ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，feature参数指示要为哪个特性计算统计数据。该参数可以是索引的整数、指定功能名称的字符串或指定多个功能的字符串或整数列表。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/5e4c8481b0d8a3cefbc5d95b466b5f86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GFi_Qdo-PNIvZeW69X7N0Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">用CatBoost生成的特征统计图。</p></figure><p id="d8cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上图有助于我们理解模型在根据不同箱中的特征值预测目标时的行为。这些条柱对应于指定特征的不同值范围，并在CatBoost模型中创建决策树时使用。</p><h2 id="a334" class="oe mo it bd mp oj ok dn mt ol om dp mx li on oo mz lm op oq nb lq or os nd ot bi translated">获取功能重要性</h2><p id="55a7" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">我们还可以使用经过训练的CatBoost模型来计算特征重要性。为此，我们首先必须获取训练数据，并使用池模块将其转换为预处理的CatBoost数据集。之后，我们可以简单地使用get_feature_importance函数，如下所示。</p><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="934e" class="oe mo it oa b gy of og l oh oi">train_data = Pool(data=X_train, label=y_train)</span><span id="5831" class="oe mo it oa b gy ou og l oh oi">model.get_feature_importance(train_data)</span></pre><p id="3ab5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该函数返回一个Numpy数组的要素重要性，如下所示。</p><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="907d" class="oe mo it oa b gy of og l oh oi">array([3.01594829, 7.75329451, 5.20064972, 4.43992429, 4.30243392,<br/>       8.32023227, 9.08359773, 2.73403973, 7.11605088, 2.31413571,<br/>       7.76344028, 1.95471762, 6.66177812, 7.78073865, 1.63636954,<br/>       4.66399329, 4.33191962, 1.836554  , 1.96756493, 7.12261691])</span></pre><h2 id="b498" class="oe mo it bd mp oj ok dn mt ol om dp mx li on oo mz lm op oq nb lq or os nd ot bi translated">交叉验证</h2><p id="fc8c" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">为了使用CatBoost执行交叉验证，我们需要完成以下步骤:</p><ol class=""><li id="8b76" class="nk nl it lb b lc ld lf lg li nm lm nn lq no lu ox nq nr ns bi translated">使用池模块创建预处理数据集。</li><li id="3e60" class="nk nl it lb b lc nu lf nv li nw lm nx lq ny lu ox nq nr ns bi translated">为CatBoost模型创建一个参数字典。</li><li id="55c8" class="nk nl it lb b lc nu lf nv li nw lm nx lq ny lu ox nq nr ns bi translated">使用cv函数为模型生成交叉验证分数。</li></ol><p id="4ebf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意，Pool模块还包括用于预处理文本和分类特征的可选参数，但是由于我们数据集中的所有特征都是数值型的，所以在本例中我不需要使用这些参数。</p><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="9577" class="oe mo it oa b gy of og l oh oi">cv_dataset = Pool(data=X_train,<br/>                  label=y_train)</span><span id="c9a9" class="oe mo it oa b gy ou og l oh oi">params = {"iterations": 100,<br/>          "depth": 2,<br/>          "loss_function": "Logloss",<br/>          "verbose": False}</span><span id="ecb9" class="oe mo it oa b gy ou og l oh oi">scores = cv(cv_dataset,<br/>            params,<br/>            fold_count=5, <br/>            plot="True")</span></pre><p id="f25b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在plot参数设置为True的情况下运行上面的代码，可以得到一个很酷的小部件，如下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/3b0a2d438b3a70cfe8c808e4aab3f743.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*rbvz6BIl0vUntJ1Jh6emiQ.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">CatBoost交叉验证小部件。</p></figure><p id="61ad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在左侧，我们可以看到每个折叠的交叉验证结果，在右侧，我们可以看到一个图表，其中包含模型的平均学习曲线以及标准偏差。x轴包含迭代次数，y轴对应于验证损失值。</p><h2 id="43f1" class="oe mo it bd mp oj ok dn mt ol om dp mx li on oo mz lm op oq nb lq or os nd ot bi translated">网格搜索</h2><p id="3dfa" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">我们还可以执行网格搜索，其中库比较不同超参数组合的性能，以找到最佳模型，如下所示。</p><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="567e" class="oe mo it oa b gy of og l oh oi">model = CatBoostClassifier(loss_function='Logloss')</span><span id="c51a" class="oe mo it oa b gy ou og l oh oi">grid = {'learning_rate': [0.03, 0.1],<br/>        'depth': [4, 6, 10]}</span><span id="0483" class="oe mo it oa b gy ou og l oh oi">grid_search_result = model.grid_search(grid,<br/>                                       X=X_train, <br/>                                       y=y_train,<br/>                                       cv=3,<br/>                                       plot=True)</span></pre><p id="f9f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">运行上面的代码会产生下面GIF中演示的小部件。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/5b5118509c117d4d5dd4c6536e2c52b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*ytZvV330anS7YFH6M6qylQ.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">CatBoost网格搜索小部件。</p></figure><p id="a4dc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以通过选择<strong class="lb iu"> params </strong>属性来访问网格搜索中的最佳参数。</p><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="abf9" class="oe mo it oa b gy of og l oh oi">print(grid_search_result['params'])</span></pre><p id="2396" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的print语句为我们提供了网格搜索中的最佳参数，如下所示。</p><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="0f1e" class="oe mo it oa b gy of og l oh oi">{'depth': 10, 'learning_rate': 0.1}</span></pre><h2 id="85aa" class="oe mo it bd mp oj ok dn mt ol om dp mx li on oo mz lm op oq nb lq or os nd ot bi translated">测试模型</h2><p id="cfa7" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">我们可以通过运行predict函数从经过训练的CatBoost模型中生成预测。</p><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="8c18" class="oe mo it oa b gy of og l oh oi">model.predict(X_test)</span></pre><p id="b648" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">运行上面的predict函数会产生一个Numpy类标签数组，如下所示。</p><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="744f" class="oe mo it oa b gy of og l oh oi">array([0, 1, 0, ..., 0, 1, 1])</span></pre><p id="8761" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们想要评估模型在测试数据上的性能，我们可以使用score函数，如下所示。</p><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="81c4" class="oe mo it oa b gy of og l oh oi">model.score(X_test, y_test)</span></pre><p id="41cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">运行上面的函数产生了以下输出。</p><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="d4f1" class="oe mo it oa b gy of og l oh oi">0.906</span></pre><p id="93d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">基于以上结果，我们可以看到该模型达到了90.6%的测试准确率。</p><h2 id="6a2a" class="oe mo it bd mp oj ok dn mt ol om dp mx li on oo mz lm op oq nb lq or os nd ot bi translated">保存模型</h2><p id="d96c" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">您还可以将CatBoost保存为各种格式，例如PMML，如下面的代码所示。</p><pre class="kj kk kl km gt nz oa ob oc aw od bi"><span id="ea3b" class="oe mo it oa b gy of og l oh oi">model.save_model(<br/>    "catboost.pmml",<br/>    format="pmml",<br/>    export_parameters={<br/>        'pmml_copyright': 'my copyright (c)',<br/>        'pmml_description': 'test model for BinaryClassification',<br/>        'pmml_model_version': '1'<br/>    }<br/>)</span></pre><h1 id="0f62" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">摘要</h1><p id="8b9f" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">CatBoost是一个更新的梯度增强框架，具有额外的特性，值得考虑作为XGBoost的潜在替代方案。它可能没有XGBoost那么快，但是它确实有一些有用的特性，并且有可能在某些任务上超过XGBoost，因为它是梯度增强的改进实现。</p><p id="7ac8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">像往常一样，你可以在<a class="ae ky" href="https://github.com/AmolMavuduru/CatBoostTutorial" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上找到本文使用的代码。</p><h1 id="e1d2" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">加入我的邮件列表</h1><p id="2068" class="pw-post-body-paragraph kz la it lb b lc nf ju le lf ng jx lh li nh lk ll lm ni lo lp lq nj ls lt lu im bi translated">你想在数据科学和机器学习方面变得更好吗？您想了解数据科学和机器学习社区的最新图书馆、开发和研究吗？</p><p id="9490" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">加入我的<a class="ae ky" href="https://mailchi.mp/e8dd82679724/amols-data-science-blog" rel="noopener ugc nofollow" target="_blank">邮件列表</a>，获取我的数据科学内容的更新。当你<a class="ae ky" href="https://mailchi.mp/e8dd82679724/amols-data-science-blog" rel="noopener ugc nofollow" target="_blank">注册</a>的时候，你还会得到我免费的<strong class="lb iu">解决机器学习问题的逐步指南</strong>！也可以在<a class="ae ky" href="https://twitter.com/amolmavuduru1" rel="noopener ugc nofollow" target="_blank"> Twitter </a>关注我，了解内容更新。</p><p id="a828" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当你这么做的时候，考虑加入媒体社区，阅读成千上万其他作家的文章。</p><h1 id="4e29" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">来源</h1><ol class=""><li id="1273" class="nk nl it lb b lc nf lf ng li pa lm pb lq pc lu ox nq nr ns bi translated">长度普罗霍伦科娃，古塞夫等。艾尔，<a class="ae ky" href="https://arxiv.org/pdf/1706.09516.pdf" rel="noopener ugc nofollow" target="_blank"> CatBoost:具有分类特征的无偏增强</a>，(2019)，arXiv.org。</li></ol></div></div>    
</body>
</html>