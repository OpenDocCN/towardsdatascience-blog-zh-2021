<html>
<head>
<title>Data lake in S3 from MongoDB</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">来自MongoDB的S3数据湖</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/data-lake-in-s3-from-mongodb-addd0b9f9606?source=collection_archive---------3-----------------------#2021-10-17">https://towardsdatascience.com/data-lake-in-s3-from-mongodb-addd0b9f9606?source=collection_archive---------3-----------------------#2021-10-17</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9d2f" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用Python将MongoDB数据上传到AWS S3构建数据湖</h2></div><p id="b5a6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我非常幸运能够结合我最热衷的话题来写我的学士论文；经济学(以拍卖理论的形式)、软件开发和数据分析。尽管我的论文是关于拍卖和投标策略的实证研究，但它涉及到使用网络爬行收集数据。然而，这是另一个帖子的另一个故事。问题是，在我的论文发表一年后，我仍然每月支付<a class="ae lb" href="https://www.mongodb.com/cloud/atlas" rel="noopener ugc nofollow" target="_blank"> MongoDB Atlas </a>的费用来存储我收集的数据，尽管我并没有使用它。是时候归档数据了。</p><p id="ddb8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文中，我不仅将向您展示我如何将数据从MongoDB移动到AWS S3进行冷存储归档，还将展示您如何将相同的原理用于数据管道来构建数据湖。根据您正在构建的现代数据堆栈的类型，您可能需要创建一个数据湖。一种常见的方法是利用AWS S3的廉价存储，一种常见的用例是在将数据接收到数据仓库进行处理和转换之前对数据进行暂存。例如，您可能有一个运行在Snowflake或Redshift中的数据仓库，这些数据仓库有从AWS S3读取或复制数据的简单方法。</p><h1 id="3435" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">规划</h1><p id="8298" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">一如既往，我喜欢过度工程化的东西，构建抽象和可重用的代码。我没有编写一个从MongoDB读取所有记录、创建一个JSON文件并上传到S3的脚本，而是考虑了以下前提:</p><ul class=""><li id="6555" class="lz ma iq kh b ki kj kl km ko mb ks mc kw md la me mf mg mh bi translated">使用YAML配置文件来确定提取</li><li id="2012" class="lz ma iq kh b ki mi kl mj ko mk ks ml kw mm la me mf mg mh bi translated">遍历一个时间戳字段，比如<code class="fe mn mo mp mq b">createdAt</code></li><li id="3562" class="lz ma iq kh b ki mi kl mj ko mk ks ml kw mm la me mf mg mh bi translated">按一天中的时间进行细分提取，以避免过度消耗RAM</li><li id="f0ec" class="lz ma iq kh b ki mi kl mj ko mk ks ml kw mm la me mf mg mh bi translated">使用带有分页的游标从MongoDB读取，以避免服务器开销</li><li id="9918" class="lz ma iq kh b ki mi kl mj ko mk ks ml kw mm la me mf mg mh bi translated">每个集合应该有自己的配置文件，以便能够定制它</li></ul><h2 id="d300" class="mr ld iq bd le ms mt dn li mu mv dp lm ko mw mx lo ks my mz lq kw na nb ls nc bi translated"><strong class="ak">配置文件</strong></h2><p id="96a0" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">我喜欢YAML的配置文件。它们是终极抽象。你可以运行一个极其复杂的YAML文件，任何人都可以根据自己的喜好对其进行修改，而无需理解其背后的逻辑。它们非常用户友好，易于使用。在这里使用它们的想法是，可以用它们以不同的方式提取不同的MongoDB集合。例如，我的MongoDB实例中的<code class="fe mn mo mp mq b">auctions</code>集合并不是很大，大约有16000条记录。然而，<code class="fe mn mo mp mq b">bids</code>收藏包含了超过36亿条记录。我可以配置这些不同的优化提取过程，避免消耗太多的内存。这也省去了我在脚本中硬编码变量的麻烦，每当我想要改变集合来提取和提高代码的可重用性时，就必须改变它们。</p><p id="8748" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最终结果应该是一个脚本(<code class="fe mn mo mp mq b">main.py</code>)，它接受一个命令行参数作为配置文件的名称，例如<code class="fe mn mo mp mq b">main.py auctions</code>用于运行<code class="fe mn mo mp mq b">auctions.yaml</code>配置文件。该文件将确定MongoDB集合的名称、开始日期、结束日期以及我们希望如何按一天中的小时来分解提取。下面是一个配置文件的示例:</p><figure class="nd ne nf ng gt nh"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="8c68" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这将提取从<code class="fe mn mo mp mq b">2020-03-01</code>到<code class="fe mn mo mp mq b">2020-08-10</code>的带有<code class="fe mn mo mp mq b">createdAt</code>字段的集合<code class="fe mn mo mp mq b">catawiki_auctions</code>中的所有数据，并将在这两个日期之间的每一天使用24小时的时间间隔。</p><p id="6900" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">以下是脚本执行的概述，以及它是如何分解上传数据的:</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi nk"><img src="../Images/36871adfdcd094c08471c9bdc111b861.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r1pwS_25_FTx8yHC7vkF3Q.png"/></div></div><p class="nr ns gj gh gi nt nu bd b be z dk translated">将数据从MongoDB加载到AWS S3数据湖的脚本概述(图片由作者提供)</p></figure><h1 id="c54c" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">履行</h1><h2 id="60f8" class="mr ld iq bd le ms mt dn li mu mv dp lm ko mw mx lo ks my mz lq kw na nb ls nc bi translated"><strong class="ak"> AWS S3斗</strong></h2><p id="9224" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">第一步是创建一个AWS S3存储桶来存储数据，并创建一个IAM用户来使用<a class="ae lb" href="https://boto3.amazonaws.com/v1/documentation/api/latest/index.html" rel="noopener ugc nofollow" target="_blank"> Boto3 </a>连接到AWS。你可以查看这个关于创建S3桶的指南，但是它应该是相当简单的。</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi nv"><img src="../Images/952d14f680d8f2ec4f53a42b41c78946.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RtfLhd7PASGmiT8mV6hqpA.png"/></div></div><p class="nr ns gj gh gi nt nu bd b be z dk translated">创建一个AWS S3桶用作数据湖(图片由作者提供)</p></figure><p id="c6f5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们需要一些凭证，以便能够与AWS S3连接并上传我们的文件。这是使用AWS IAM管理的。在这里，您可以首先创建一个策略，以允许我们将要创建的用户访问我们已经创建的存储桶:</p><figure class="nd ne nf ng gt nh"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="5858" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">然后，您可以创建一个用户，并在选择权限时使用“直接附加现有策略”选项。这将允许您将我们创建的策略附加到用户。之后，您应该能够下载访问密钥ID和秘密访问密钥。这些是您需要使用Boto3调用AWS APIs来上传文件的凭证。</p><h2 id="daef" class="mr ld iq bd le ms mt dn li mu mv dp lm ko mw mx lo ks my mz lq kw na nb ls nc bi translated"><strong class="ak">循环优化RAM </strong></h2><p id="2281" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">现在该看代码了。我把最终的脚本上传到了Github，你可以在这里找到它。出于教学目的，我将该脚本作为一个单一的文件，但在其他情况下，我会将它分成几个实用模块，然后再导入到我们的主文件中。在任何情况下，你都可以看看它，它应该很容易理解，但我会在这篇文章中介绍最重要的部分，同时解释一些定义。</p><p id="6fca" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个脚本最重要的方面是使用的循环和生成器。这样做的目的是优化RAM，当使用MongoDB集合和许多文档在生产中运行这样的过程时，您可能会遇到RAM问题。在生产环境中的小型服务器上运行脚本时尤其如此。出于这个原因，我们经历了3个循环，允许我们划分从MongoDB获得的数据，并避免将其全部加载到内存中。</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi nw"><img src="../Images/251747b162a19a41d1f4c93adc256098.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IHZ2Ew5hPcP3wdr5sy-Lig.png"/></div></div><p class="nr ns gj gh gi nt nu bd b be z dk translated">将数据加载到数据湖时优化RAM的循环(图片由作者提供)</p></figure><p id="ffe6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些循环的另一个极其重要的部分是能够从错误或连接问题中恢复。如果在我们的代码中有一个网络错误或bug，我们可以从它停止的地方重新开始这个过程，而不是必须清除并重新开始。</p><h2 id="fb8e" class="mr ld iq bd le ms mt dn li mu mv dp lm ko mw mx lo ks my mz lq kw na nb ls nc bi translated"><strong class="ak">从开始日期迭代到结束日期</strong></h2><p id="fc21" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">我们首先从配置文件中定义的参数<code class="fe mn mo mp mq b">start_date</code>到<code class="fe mn mo mp mq b">end_date</code>进行迭代。这种迭代一次进行一天，和Python中的惯例一样，端点不是生成序列的一部分。该脚本定义了一个生成器来实现这个调用<code class="fe mn mo mp mq b">date_range</code>。这个想法是你可以通过一个<code class="fe mn mo mp mq b">start_date</code>、一个<code class="fe mn mo mp mq b">end_date</code>和一个天数增量。它的工作方式与<code class="fe mn mo mp mq b">range</code>发电机相同。</p><figure class="nd ne nf ng gt nh"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="3d01" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果这听起来很有趣，你绝对应该看看<a class="ae lb" href="https://wiki.python.org/moin/Generators" rel="noopener ugc nofollow" target="_blank"> Python生成器</a>，它们非常方便。</p><h2 id="2b0a" class="mr ld iq bd le ms mt dn li mu mv dp lm ko mw mx lo ks my mz lq kw na nb ls nc bi translated"><strong class="ak">从00:00迭代到23:59 </strong></h2><p id="cff9" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">现在我们每天都在迭代，我们可以从那天开始迭代不同的小时。例如，我们可以6小时(00:00到06:00，06:00到12:00，12:00到18:00，18:00到00:00)为一批，12小时(00:00到12:00，12:00到00:00)为一批，或者以24小时为间隔一次性完成一整天。对于小的集合，这没有意义，我们可以使用24小时的时间间隔一次性加载一整天。但是，如果我们正在加载大量数据，那么提交数据提取并从可能的错误中恢复是有意义的。同时，这意味着我们可以并行处理我们的任务。例如，如果我们在气流中使用它来生成这样的DAG:</p><figure class="nd ne nf ng gt nh gh gi paragraph-image"><div role="button" tabindex="0" class="nl nm di nn bf no"><div class="gh gi nx"><img src="../Images/0a9c7a465b8d548ecc95c4ac2f1941d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nv-jLlIYGrhCM6Z4lepxAQ.png"/></div></div><p class="nr ns gj gh gi nt nu bd b be z dk translated">将数据上传到S3数据湖的示例气流DAG(图片由作者提供)</p></figure><h2 id="839f" class="mr ld iq bd le ms mt dn li mu mv dp lm ko mw mx lo ks my mz lq kw na nb ls nc bi translated"><strong class="ak">批量接收5K文档的MongoDB数据</strong></h2><p id="57d6" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">最后，我们不希望同时向MongoDB服务器请求数百万个文档。在生产中也是如此，否则，你的工程团队可能会打电话给你，看看你在做什么。同时，正如已经提到的，我们还可以通过请求更少的文档和对结果分页来优化RAM的使用。最后，如果您计划稍后将这些文件接收到一个仓库(如Snowflake)中，那么将记录分解成更小的文件实际上是很有用的。这允许您通过并行读取这些文件来并行接收。有关文件大小和雪花摄取的更多信息，请查看他们文档中的<a class="ae lb" href="https://docs.snowflake.com/en/user-guide/data-load-considerations-prepare.html#general-file-sizing-recommendations" rel="noopener ugc nofollow" target="_blank">一般文件大小建议</a>。</p><p id="3494" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为此，我们再次使用Python生成器并迭代MongoDB游标。这允许我们一次获得5K个记录，而不必将它们存储在内存中，也不必将它们全部请求到服务器。5K是完全可定制的，在这种情况下，我使用5K作为一个完全任意的数字，但你绝对可以增加它。事实上，YAML文件允许使用一个<code class="fe mn mo mp mq b">batch_size</code>属性来精确地配置它(默认为<code class="fe mn mo mp mq b">5000</code>)。</p><h2 id="bc7b" class="mr ld iq bd le ms mt dn li mu mv dp lm ko mw mx lo ks my mz lq kw na nb ls nc bi translated"><strong class="ak">文件上传到S3 </strong></h2><p id="5462" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">之后，过程就简单了，我们只需从MongoDB中获取记录，并将它们存储在一个JSON文件中。注意我们正在执行的所有循环是很重要的，本质上，我们希望所有的循环都以某种方式在文件名中表示出来。如果我们做不到这一点，我们可能会一遍又一遍地覆盖同一个文件。此外，我还包含了数据提取日期的时间戳，这有助于在并行/分布式运行期间出现错误和重叠的情况下进行重新处理。</p><figure class="nd ne nf ng gt nh"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="8b89" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">需要注意的一点是，MongoDB有一些类型有点特殊，没有在JSON标准中定义，所以如果您试图用它们编写JSON字符串，您会得到一个序列化错误(“TypeError:ObjectId类型的对象不是JSON serializable”)。</p><figure class="nd ne nf ng gt nh"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="0c00" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您可以使用PyMongo dumper来解决这个问题:</p><figure class="nd ne nf ng gt nh"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="2299" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">还要注意，我使用了<code class="fe mn mo mp mq b">.gz</code>文件扩展名，这是为了存储压缩的JSON数据:</p><figure class="nd ne nf ng gt nh"><div class="bz fp l di"><div class="ni nj l"/></div></figure><p id="1c2d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最后，为了上传生成的文件，我们只需使用Boto3初始化一个S3客户端，并调用<code class="fe mn mo mp mq b">upload_file</code>方法:</p><figure class="nd ne nf ng gt nh"><div class="bz fp l di"><div class="ni nj l"/></div></figure><h1 id="ac6b" class="lc ld iq bd le lf lg lh li lj lk ll lm jw ln jx lo jz lp ka lq kc lr kd ls lt bi translated">结论</h1><p id="d8fa" class="pw-post-body-paragraph kf kg iq kh b ki lu jr kk kl lv ju kn ko lw kq kr ks lx ku kv kw ly ky kz la ij bi translated">虽然我的主要目标是将我在MongoDB中的数据归档到AWS S3，但我希望我已经展示了如何使用这个相同的脚本来摄取数据以构建数据湖。这种类型的脚本可以很容易地扩展到在<a class="ae lb" href="https://airflow.apache.org/" rel="noopener ugc nofollow" target="_blank"> Airflow </a>、<a class="ae lb" href="https://dagster.io/" rel="noopener ugc nofollow" target="_blank"> Dagster </a>或<a class="ae lb" href="https://airbyte.io/" rel="noopener ugc nofollow" target="_blank"> Airbyte </a>上运行，作为您的数据管道的一部分，从生产系统中提取数据并将其加载到您的数据湖或数据仓库中。事实上，这个脚本非常类似于我们在<a class="ae lb" href="https://sirena.app/" rel="noopener ugc nofollow" target="_blank"> Sirena </a>使用的Airflow DAGs来提取数据并加载到<a class="ae lb" href="http://snowflake.com/" rel="noopener ugc nofollow" target="_blank"> Snowflake </a>中。您甚至可以使用该脚本一次性重新处理数据，或者将所有内容打包成CLI，将其dockerize并在Kubernetes中运行。该脚本非常简单，您甚至可以使用Celery以分布式方式运行它来并行提取。可能性是无限的。</p></div></div>    
</body>
</html>