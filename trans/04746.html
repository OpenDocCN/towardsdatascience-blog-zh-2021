<html>
<head>
<title>K-Means++ Algorithm For High-Dimensional Data Clustering</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">高维数据聚类的K-Means++算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/k-means-algorithm-for-high-dimensional-data-clustering-714c6980daa9?source=collection_archive---------14-----------------------#2021-04-25">https://towardsdatascience.com/k-means-algorithm-for-high-dimensional-data-clustering-714c6980daa9?source=collection_archive---------14-----------------------#2021-04-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/d33e59748be793c9c63d5e06eee7ca9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TZAu5Ch5J7ZW8MIKMjjvyQ.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">Arthur V. Ratz的照片</p></figure><div class=""/><div class=""><h2 id="e0a9" class="pw-subtitle-paragraph kc je jf bd b kd ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt dk translated">利用K-Means++算法进行优化的高维数据集聚类，使用最新的Scikit-Learn 0.20、NumPy 3.20.x Matplotlib 3.4.1、结果可视化等在Anaconda Python 3.8.x中实现。</h2></div><h1 id="6a44" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">介绍</h1><p id="48c3" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">聚类是数据分析中的一个重要步骤，广泛用于分类、收集统计数据和获取特定知识领域的见解。在执行聚类时，它旨在将数据集划分成几个组(即<em class="mi">簇</em>)，将最相似的数据分配给簇<strong class="lo jg">【1，4，6】</strong>。</p><p id="d269" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">数据聚类不仅仅基于一个，而是基于整个类别的<em class="mi">无监督机器学习</em> <em class="mi"> (ML) </em>算法，当组的数量未知时，有效地用于<em class="mi">不确定</em>或<em class="mi">模糊</em>数据聚类。它通过将数据与最初预定义的类相关联，提供了对数据进行分类的能力。</p><p id="72ba" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">然而，现有的大多数基于Lloyd-Forgy方法的算法，在对具有大量特征的数据集进行聚类时，具有巨大的平均复杂度。</p><p id="a57b" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">在实验上，将𝙣=𝟏𝟎实体的三维𝙙=𝟑数据集划分成𝙠=𝟏𝟎簇以进行𝙞=𝟏𝟎迭代可以在超级多项式时间(NP-hard)内完成，这与𝞀=𝟏𝟎成比例。</p><p id="c8aa" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">为了提高Lloyd-Forgy的聚类性能，使用了各种算法级优化。虽然，它们中的许多还没有被很好地研究过，因此是不切实际的。原始Lloyd-Forgy的K-Means聚类至少有两种已知的优化，如<strong class="lo jg">模糊C-Means </strong>和<strong class="lo jg"> K-Means++算法，在<strong class="lo jg">【2，3，6】</strong>中讨论。</strong></p><p id="f3f7" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">为了有效地提高<em class="mi"> NP-hard </em>聚类过程的性能和收敛性，我们将引入K-Means++算法，该算法最初由David Arthur和Sergei Vasilevskii于2007年提出，作为Lloyd-Forgy算法的初始化步骤<strong class="lo jg">【2，5】</strong>。</p><p id="80e7" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">与其他类似的算法不同，K-Means++提供了聚类和质心就地计算的能力，确保聚类以减少的迭代次数执行，等于结果聚类的总数。</p><p id="304a" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">K-Means++算法的平均情况复杂度已经显著降低，非常接近最好情况的Lloyd-Forgy算法的复杂度。K-Means++大约是𝟓.与最初的Lloyd-Forgy算法相比，𝟒𝟗𝙭时间更快，同时用于高维数据集的聚类。</p><h1 id="6166" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">劳埃德-福吉的K-均值聚类</h1><p id="6c03" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">Lloyd-Forgy的K-Means是一种算法，它基于<em class="mi">欧几里德</em> <em class="mi">距离</em>度量，将𝙣- <em class="mi">观测值</em>的数据集𝑿划分为一组𝙠- <em class="mi">聚类</em>的过程，其中每个观测值都是𝒅- <em class="mi">特征</em>的多维向量。每个聚类是到<em class="mi">质心之一的最小距离的一组观察值，</em>被评估为聚类<strong class="lo jg"> [1，4，5，6] </strong>内所有观察值的<em class="mi">最近均值</em>。</p><p id="47a7" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">通常，数据聚类的整个过程可以描述为:</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi mo"><img src="../Images/37794f40f3f97f59b5b8ab7cef5a77ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dhEtqRsHxYVTJ7_ItjGA2g.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">一个数据聚类的例子|图片作者</p></figure><p id="45ea" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">上图展示了将𝒏=𝟏𝟎观测的二维数据集聚类成𝙠=𝟑聚类的过程(从右开始)。</p><p id="cc3a" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">输入数据集𝑿的片段如下所示:</p><pre class="mp mq mr ms gt mt mu mv mw aw mx bi"><span id="ee02" class="my kv jf mu b gy mz na l nb nc">0:     [ x = 0.543974 y = 0.842981 ]<br/>1:     [ x = 0.131690 y = 0.806490 ]<br/>2:     [ x = 0.339777 y = 0.380520 ]<br/>3:     [ x = 0.683979 y = 0.816659 ]<br/>4:     [ x = 0.236921 y = 0.184139 ]<br/>5:     [ x = 0.380008 y = 0.027292 ]<br/>6:     [ x = 0.933727 y = 0.694752 ]<br/>7:     [ x = 0.911393 y = 0.504823 ]<br/>8:     [ x = 0.076103 y = 0.714423 ]<br/>9:     [ x = 0.906728 y = 0.107928 ]<br/>10:    [ x = 0.087780 y = 0.256157 ]</span><span id="578f" class="my kv jf mu b gy nd na l nb nc">              *       *       *</span></pre><p id="d127" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">上面列出的每一个𝒏-observations，都是欧几里得空间ℝ <strong class="lo jg"> ᵈ </strong>中的向量𝒙。因为，𝑿是一个数据集，它的所有向量∀𝒙 ∈ 𝑿排列成一个形状协方差矩阵(𝒏 <strong class="lo jg"> × </strong> 𝒅).</p><p id="865d" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">数据集𝑿的整个聚类分两步进行<strong class="lo jg">【1】</strong>:</p><ol class=""><li id="52f0" class="ne nf jf lo b lp mj ls mk lv ng lz nh md ni mh nj nk nl nm bi translated">计算一组群集𝙎，分配所有观察∀𝒙ₜ ∈ 𝑿，𝙩 =𝟏..𝒏到集群𝒔ᵣ ∈ 𝙎，𝒓=𝟭..具有最近质心𝒄ᵣ ∈ 𝑪的𝙠，其中来自每个观测𝒙ₜ的distance|∀𝒙ₜ𝒄ᵣ|的平方最小。</li><li id="b9d1" class="ne nf jf lo b lp nn ls no lv np lz nq md nr mh nj nk nl nm bi translated">更新质心𝒄ᵣ ∈ 𝑪，𝒓=𝟭..所有星团的𝙠作为<em class="mi">观测值的质心</em>，分配给每个星团𝒔ᵣ ∈ 𝙎.</li></ol><p id="e050" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">继续执行步骤<strong class="lo jg">1–2</strong>，直到𝙠-clusters 𝒔 ∈ 𝙎最终计算完毕。</p><p id="18ba" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">为了执行聚类，我们必须从数据集𝑿中选择𝙠-observations，作为质心𝑪的初始集合，计算来自𝙩=𝟏∀𝒙ₜ∈𝑿的每个观测值的平方距离..𝒏到所有质心∀𝒄ᵣ ∈ 𝑪，𝒓=𝟭..𝙠，将观测值∀𝒙ₜ映射到质心𝒄ᵣ，其中来自这些观测值∀𝒙ₜ的distance|𝒙ₜ𝒄ᵣ|最小，并将它们分配到群集𝒔ᵣ ∈ 𝙎.</p><p id="1799" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">通常，我们在每次𝛃-th迭代𝛃=𝟏时继续计算新的集群𝒔 ∈ 𝙎 <strong class="lo jg"> ⁽ </strong> ᵝ <strong class="lo jg"> ⁾ </strong>..𝙞，重新评估质心并在来自先前(𝛃−𝟏)-th迭代)的每个现有聚类内将观察结果划分成多个新的聚类，直到整个数据集最终被聚类<strong class="lo jg">【1，4，5，6】</strong>。</p><p id="4de8" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">数学上，下面的过程可以表示为等式<strong class="lo jg"> (1.1) </strong>:</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ns"><img src="../Images/2955a788d823a291086b572739f05e6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GHIXrugfVExxUEpF98Tgsw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">Lloyd-Forgy基于距离的聚类过程|图片由作者提供</p></figure><p id="2210" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">反过来，通过使用<em class="mi">质心</em>等式<strong class="lo jg"> (1.2) </strong>，每个聚类的质心被更新为𝒓-th聚类内所有𝒖ᵣ-observations的最近平均值:</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ns"><img src="../Images/a0e943801a6084d8fd0cc9111ebd69e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RSpz4syQJUit-dr_Wnj2sQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">聚类的最近均值公式|作者图片</p></figure><p id="a745" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">在每个聚类的质心∀𝒄ᵣ ∈ 𝑪没有改变∀𝙘ᵣ<strong class="lo jg">⁽</strong>ᵝ⁺<strong class="lo jg">⁾</strong>=∀𝙘ᵣ<strong class="lo jg">⁽</strong>T6】T7的情况下，聚类过程终止，返回聚类的结果集。否则，它继续进行下一次𝛃+𝟭)-th迭代，直到整个数据集已经被聚类，并且聚类过程<strong class="lo jg">【1】</strong>，最终满足下面的条件<strong class="lo jg"> (2.1) </strong>:</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ns"><img src="../Images/96034b6ffd378ad7fa55e3d7580ea686.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J33ZNB-nPKQT0C3ikNrzuw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">K-均值聚类的收敛性|图片作者</p></figure><p id="b25e" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">在执行聚类时，我们的目标是最小化<em class="mi">类间</em>平方距离的总和，以便这些观察值∀𝒙 ∈ 𝒔ᵣ和每个类∀𝒔ᵣ ∈ 𝙎的质心∀𝒄ᵣ之间的距离最小。</p><p id="c355" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">这与方差𝑽𝒂𝒓(𝒔ᵣ的最小化非常相似，方差是𝒖ᵣ-observations在𝒓=𝟭∀𝒔ᵣ∈𝙎每个聚类内的平均协变(成对)方差..𝙠.同时，我们的目标是最大化所有𝙠-clusters ∀𝒔 ∈ 𝙎的质心∀𝒄ᵣ ∈ 𝑪之间的距离，使得所有质心到向量空间ℝ <strong class="lo jg"> ᵈ </strong>的中心𝙘ₒ的平均距离最大。最佳<em class="mi">群集间</em>距离必须始终满足以下条件<strong class="lo jg"> (2.2) </strong>:</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ns"><img src="../Images/cb536bd362e60fc76c81e54185195ece.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2I9gmUJ9em0Q35K4YIjhnw.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">最佳类内距离标准|作者图片</p></figure><p id="d8b4" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">经典的Lloyd-Forgy的K-Means过程是几种聚类算法的基础，包括K-Means++算法、K-Medoids算法、模糊C-Means算法等。尽管，由于潜在的巨大计算复杂性，这些算法中的一些不能有效地用于聚类。</p><h1 id="dfcc" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">为什么K-Means聚类仍然更有效</h1><p id="06b9" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">如前所述，使用Stewart Lloyd和Edward Forgy在1965年提出的K-Means算法以及其他继承方法，在许多情况下，由于超多项式的复杂性，应用于高维数据集的聚类变得低效<strong class="lo jg">【1】</strong>:</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nt"><img src="../Images/a3d683bb3e5d64e91aba7d88fccecc49.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cXxWSCjsI4OkiG9klt9vFQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">K-Means聚类算法的复杂性|图片作者</p></figure><p id="25ff" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">，其中𝒏-大量的观察值，𝒌-总的聚类数，𝒅-大量的特征(即向量空间维度)，𝒊-大量的迭代，𝛔-最小的类内方差。</p><p id="372f" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">Lloyd-Forgy的K-Means算法的最坏情况复杂度成比例地限制为:</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nu"><img src="../Images/2e75c5e62b9f46ec8620ee65f1786f94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZJIsQqg4DJZIcjSKIgIH5w.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">经典的K-均值复杂性渐近边界|图片作者</p></figure><p id="a0e8" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">有几种方法可以解决Lloyd-Forgy的K-Means算法的巨大复杂性，例如降低数据集的维数、进行聚类以及将数据集表示为多维整数格。</p><p id="0b74" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">虽然，在聚类大数据集的情况下，使用这些方法可能仍然是低效的，其中的观测值数量远远超过𝗻 ≫ 𝟏𝟎观测值。</p><p id="de20" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">此外，已知的模糊c-均值算法的复杂度非常接近经典的Lloyd-Forgy算法的平均情况复杂度，并且不同之处在于加权质心计算的额外复杂度𝙊(𝗻:</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nv"><img src="../Images/d34d236f6a3bb2a64a3175a9ba6cf8b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NTyuLF8QtUj4YsLWNRbz-g.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">模糊C均值算法复杂度|图片作者</p></figure><p id="a2cd" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">为了执行高维数据集的聚类，我们需要一种不同的、更有效的算法，具有大规模数据集聚类的降低的复杂性。</p><h1 id="98b3" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">K-Means++算法及其复杂性</h1><p id="0f22" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">由David Arthur和Sergei Vasilevskii在2007年提出的一种优化，被公式化为K-Means++算法，提供了执行高维数据聚类的能力，与先前讨论的最初的Lloyd-Forgy的K-Means和其他方法相比，显著更快。同时，使用优化的K-Means++算法不影响聚类的整体质量，提高了聚类结果的类内和类间距离。与Lloyd-Forgy的方法不同，它主要确保数据集在迭代次数内聚类，迭代次数等于最初给定的聚类数。这反过来对数据聚类的过程产生积极的影响<strong class="lo jg">【2，5】</strong>。</p><p id="c026" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">K-Means++聚类过程可以表述如下<strong class="lo jg">【2，5】</strong>:</p><p id="e7fc" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">让𝙓-𝙠𝙣-observations的数据集-𝘾和𝙎的聚类总数-𝙠质心和聚类的结果集分别为:</p><ol class=""><li id="b291" class="ne nf jf lo b lp mj ls mk lv ng lz nh md ni mh nj nk nl nm bi translated">选择质心𝙘₀作为随机观测∀𝙭 ∈ X:</li></ol><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nt"><img src="../Images/6e5e59c7fba0fa656ae20d9564b3b747.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4fS4myMemigt4ZM4fLQ5tw.png"/></div></div></figure><p id="7ba4" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">2.选择质心𝙘₁作为到质心c0距离最大的观测∀𝙭:</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nt"><img src="../Images/1f6522b74d42f431dd225fd07a0a6c8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Cj87kp_XSvQiILYLjkgXjA.png"/></div></div></figure><p id="51c8" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">3.计算𝛃=𝟏内𝙓的𝙠-clusters ∀𝒔 ∈ 𝙎..迭代的𝙠:</p><p id="324c" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">▪︎为每个观察𝒙ₜ ∈ 𝑿，𝙩=𝟏..𝒏，请执行以下操作:</p><ul class=""><li id="6507" class="ne nf jf lo b lp mj ls mk lv ng lz nh md ni mh nw nk nl nm bi translated">检查观察值∀𝒙ₜ ∈ 𝘾是否已经追加到质心集。如果没有，继续下一步<strong class="lo jg"> 3.2 </strong></li><li id="2609" class="ne nf jf lo b lp nn ls no lv np lz nq md nr mh nw nk nl nm bi translated">计算从当前观测𝒙ₜ到每个现有质心∀𝙘 ∈ 𝘾的|𝒙ₜ - ∀𝙘|距离</li><li id="7a56" class="ne nf jf lo b lp nn ls no lv np lz nq md nr mh nw nk nl nm bi translated">找到质心𝙘ᵣ ∈ 𝘾，𝙧=𝟏..𝛃到𝒙ₜ的距离最短:</li></ul><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nt"><img src="../Images/5e918b4067ab01a900c00aca2d26f9f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4JRDzVhv8-dcdkTxYL-g4w.png"/></div></div></figure><ul class=""><li id="8328" class="ne nf jf lo b lp mj ls mk lv ng lz nh md ni mh nw nk nl nm bi translated">将观测值𝒙ₜ分配给质心为𝙘ᵣ ∈ 𝒔ᵣ的𝙧-th聚类𝒔ᵣ ∈ 𝙎</li></ul><p id="634c" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">4.检查𝙠-centroids ∀𝙘ᵣ ∈ 𝘾，𝙧=𝟏..𝙠已经被最终计算出来，并且所有的观测值都被安排到相应的𝙠-clusters ∀𝒔 ∈ 𝙎.中如果没有，继续执行步骤<strong class="lo jg"> 5。</strong>否则，<strong class="lo jg"> </strong>终止聚类过程。</p><p id="42e2" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">5.在𝒔ᵣ ∈ 𝙎的所有现有集群中找到一个观测𝒙ⱼ，从该处到𝙧=𝟏𝙘ᵣ∈𝘾的质心之一的距离..𝛃是最大的:</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nx"><img src="../Images/48f1c82cdbe0aefb6172b5f9d906370e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KkS5a1PF4KTKP1cjx4X4mg.png"/></div></div></figure><p id="6792" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">6.将观测值𝒙ⱼ附加到集合𝘾，作为新聚类𝒔ᵣ+₁的质心𝙘ᵣ+₁←𝒙ⱼ；</p><p id="bd0e" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">7.继续执行步骤<strong class="lo jg">3–6</strong>，直到以下过程最终收敛，数据集最终聚类；</p><p id="093f" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">上面介绍的算法的主要优点是，它提供了同时计算质心和相应的结果聚类的能力，这极大地影响了算法的复杂性，并因此影响了整个聚类过程的持续时间，使得有可能比以前制定的其他类似算法更快地执行高维数据集的聚类。</p><p id="83eb" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">在每个𝛃=𝟏..𝙠迭代，它计算下一个集群的质心𝙘ᵣ+₁，更新现有的集群𝒔ᵣ ∈ 𝙎，𝙧=𝟏..𝛃通过重新评估所有观测值∀𝒙 ∈ 𝑿到多个新建聚类的分配，使得在其最后的𝙠-th迭代，聚类过程产生结果聚类𝙎.的集合</p><p id="c6e0" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">Python 3.9中的代码示例使用最新的NumPy库实现了优化的K-Means++聚类算法，如下所示:</p><figure class="mp mq mr ms gt is"><div class="bz fp l di"><div class="ny nz l"/></div></figure><p id="555d" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">上面演示的代码片段有一个重要的优化，它允许减少进程内存空间的数量，广泛用于大型数据集的聚类。在聚类过程中，它计算输入数据集𝑿中的观察值的索引，将其附加到𝘾和𝙎集，而不是将相同的高维数据克隆到多个集中。反过来，在处理高维数据集的聚类的情况下，这使得消耗显著更少量的进程存储器成为可能。</p><p id="e5e5" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">最后，您可能已经注意到，与著名的Lloyd-Forgy算法或模糊c均值聚类算法相比，K-Means++的复杂度显著降低，在一般情况下，估计仅为𝙊(𝙠 𝙣𝙙𝙞 +𝙣𝙙。具体地，在迭代总数𝙞等于聚类总数𝙠.的情况下，K-Means++的复杂度从超多项式平滑到象限，由𝙊(𝙠 𝙣𝙙 + 𝙣𝙙限定在这种情况下，K-Means++聚类的复杂性大约比原始Lloyd-Forgy的K-Means或模糊C-Means算法的复杂性低28倍:</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi oa"><img src="../Images/23aa0e739fb2405db769cf5800bac535.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CelmWPLxtSL4gtN_7X8tIQ.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">数据聚类算法复杂性|图片作者</p></figure><p id="3c17" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">此外，下图显示了将具有𝙙=𝟐维度的𝙣=𝟏𝟎观测数据集聚类为一组𝙠=𝟑结果聚类，执行𝙞=𝟏𝟎迭代的估计复杂度:</p><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ob"><img src="../Images/c9f5d75f77cf4d13fe0b3742d2dc058f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S2AsQsdEwn9bU9MFtKFHZg.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">K-Means、模糊C-Means和K-Means算法复杂度|图片作者</p></figure><p id="6615" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">正如你所看到的，在上图中，K-Means++算法有一个复杂度(navy)，由于几个算法级的优化<strong class="lo jg">【1】</strong>，这个复杂度已经大大降低了。</p><h1 id="5b92" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">评估聚类的质量</h1><p id="bfee" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">最后，让我们简短地看一下使用K-Means++算法实现的数据聚类的质量。为了确保K-Means++最有能力提供正确的结果，从而提供适当的聚类质量，我们将进行实验，通过使用<em class="mi">‘scikit-learn’</em>库，执行基于高斯正态分布生成的合成数据集的聚类。</p><p id="0c77" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">使用<em class="mi">‘sci kit-learn’</em>生成各向同性高斯斑点使得创建用于聚类的多维数据集成为可能。该实验的主要目的是确定使用K-Means++算法是否提供聚类的结果，这与使用<em class="mi">【scikit-learn】</em>库<em class="mi"> </em>的情况相同。</p><p id="f3d0" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">这通常是通过生成合成数据集并使用上述K-Means++算法执行相同的聚类来完成的。</p><p id="e42d" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">具体来说，至少有三种主要类型的数据集可用于聚类验证，例如每个数据集分别具有<em class="mi">小</em>、<em class="mi">平均、</em>和<em class="mi">大</em>的类间距离(即<em class="mi">标准差(STDEV) </em>参数)的数据集。</p><p id="c043" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">这是数据集聚类结果的可视化，具有不同的标准差𝛅:</p><h2 id="b6c4" class="my kv jf bd kw oc od dn la oe of dp le lv og oh lg lz oi oj li md ok ol lk om bi translated">案例#1: 𝙣=50，𝙠=3，𝙙=2，𝛅=4.5(小集群间距离)</h2><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi on"><img src="../Images/8f9f0593c33e77c28e92db731dcee3e1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_gQPlrbn4HcmCupTi6iunQ.png"/></div></div></figure><h2 id="e076" class="my kv jf bd kw oc od dn la oe of dp le lv og oh lg lz oi oj li md ok ol lk om bi translated">案例2: 𝙣=50、𝙠=3、𝙙=2、𝛅=1.5(平均集群间距离)</h2><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi on"><img src="../Images/f5f5f6a4d896ff24fe27a9757dd30cf2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oK5rP9xXaRx_ROkAM8aQCQ.png"/></div></div></figure><h2 id="d5f3" class="my kv jf bd kw oc od dn la oe of dp le lv og oh lg lz oi oj li md ok ol lk om bi translated">案例3: 𝙣=100、𝙠=3、𝙙=2、𝛅=0.3(集群间距离大)</h2><figure class="mp mq mr ms gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi on"><img src="../Images/a426cb23d81d56705790b5ace7c31f96.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LGlZ3cAue_wLvaEG1DPN3Q.png"/></div></div></figure><p id="1135" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated">对于正在讨论的使用K-Means++算法的数据聚类的完整结果，请参考相关项目，贡献给Anaconda Cloud:</p><ul class=""><li id="12c8" class="ne nf jf lo b lp mj ls mk lv ng lz nh md ni mh nw nk nl nm bi translated"><a class="ae oo" href="https://anaconda.org/arthurvratz/kmeans_pp_nb/notebook" rel="noopener ugc nofollow" target="_blank">https://anaconda.org/arthurvratz/kmeans_pp_nb/notebook</a></li></ul><h1 id="64df" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">结论</h1><p id="9833" class="pw-post-body-paragraph lm ln jf lo b lp lq kg lr ls lt kj lu lv lw lx ly lz ma mb mc md me mf mg mh ij bi translated">K-Means++算法的一个优点是，它能够同时计算质心并将数据分配给聚类。高维数据集聚类是通过多次迭代实现的，与其他现有方法相比，迭代次数明显减少。K-Means++能够在最初的几次迭代中将数据分配给聚类，而不会影响聚类本身的质量。大多数情况下，它比其他方法更快，提供了早期收敛，从而提高了聚类性能。与劳埃德算法不同，它的复杂度已经大大降低，并且低于NP-hard。最后，K-Means++算法基于使用规则或平方欧几里得距离度量，有效地最小化了类内方差，避免了已知的韦伯问题。类似于K-Medians和K-Medoids算法，它能够在聚类大型高维数据集时提供更好的欧几里德距离解决方案。</p></div><div class="ab cl op oq hu or" role="separator"><span class="os bw bk ot ou ov"/><span class="os bw bk ot ou ov"/><span class="os bw bk ot ou"/></div><div class="ij ik il im in"><p id="2143" class="pw-post-body-paragraph lm ln jf lo b lp mj kg lr ls mk kj lu lv ml lx ly lz mm mb mc md mn mf mg mh ij bi translated"><strong class="lo jg"> <em class="mi"> K-Means++集群Anaconda Python 3.8、NumPy 1.20.x、Scikit-Learn 0.20.x中完整的源代码项目可以从我的GitHub和Anaconda云库下载:</em> </strong></p><ul class=""><li id="a547" class="ne nf jf lo b lp mj ls mk lv ng lz nh md ni mh nw nk nl nm bi translated">【https://github.com/arthurratz/kmean_pp_clu_optimal T4】</li><li id="72d6" class="ne nf jf lo b lp nn ls no lv np lz nq md nr mh nw nk nl nm bi translated"><a class="ae oo" href="https://anaconda.org/arthurvratz/kmeans_pp_nb/notebook" rel="noopener ugc nofollow" target="_blank">https://anaconda.org/arthurvratz/kmeans_pp_nb/notebook</a></li></ul><h1 id="d9e1" class="ku kv jf bd kw kx ky kz la lb lc ld le kl lf km lg ko lh kp li kr lj ks lk ll bi translated">参考</h1><ol class=""><li id="9a20" class="ne nf jf lo b lp lq ls lt lv ow lz ox md oy mh nj nk nl nm bi translated"><a class="ae oo" href="https://en.wikipedia.org/wiki/K-means_clustering" rel="noopener ugc nofollow" target="_blank"><em class="mi">“k-means聚类”——来自维基百科，免费百科</em> </a></li><li id="13ff" class="ne nf jf lo b lp nn ls no lv np lz nq md nr mh nj nk nl nm bi translated"><a class="ae oo" href="https://en.wikipedia.org/wiki/K-means%2B%2B" rel="noopener ugc nofollow" target="_blank"><em class="mi">“k-means++”——来自维基百科，免费百科</em> </a></li><li id="0e3c" class="ne nf jf lo b lp nn ls no lv np lz nq md nr mh nj nk nl nm bi translated"><a class="ae oo" href="https://en.wikipedia.org/wiki/Fuzzy_clustering#Fuzzy_C-means_clustering" rel="noopener ugc nofollow" target="_blank"> <em class="mi">“模糊聚类”——来自维基百科，免费百科</em> </a></li><li id="ec1d" class="ne nf jf lo b lp nn ls no lv np lz nq md nr mh nj nk nl nm bi translated"><a class="ae oo" href="https://www.analyticsvidhya.com/blog/2019/08/comprehensive-guide-k-means-clustering/" rel="noopener ugc nofollow" target="_blank"> <em class="mi">《你需要的最全面的K-Means聚类指南》作者Pulkit Sharma，Analytics Vidhya，2019年8月</em> </a></li><li id="642e" class="ne nf jf lo b lp nn ls no lv np lz nq md nr mh nj nk nl nm bi translated"><a class="ae oo" rel="noopener" target="_blank" href="/understanding-k-means-k-means-and-k-medoids-clustering-algorithms-ad9c9fbf47ca"> <em class="mi">《理解K-Means，K-Means++和，K-Medoids聚类算法》作者Satyam Kumar，走向数据科学，2020年6月</em> </a></li><li id="697d" class="ne nf jf lo b lp nn ls no lv np lz nq md nr mh nj nk nl nm bi translated"><a class="ae oo" href="https://www.springerprofessional.de/advances-in-k-means-clustering/3879526" rel="noopener ugc nofollow" target="_blank"><em class="mi">“K均值聚类的进展。一种数据挖掘思维》，吴俊杰，施普林格柏林海德堡，德国柏林，2012 </em> </a></li></ol></div></div>    
</body>
</html>