<html>
<head>
<title>Do You Use XGBoost? There is a 200x Faster Way</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">你用XGBoost吗？有一种快200倍的方法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/do-you-use-xgboost-heres-how-to-make-it-200x-faster-16cb6039a16e?source=collection_archive---------8-----------------------#2021-07-29">https://towardsdatascience.com/do-you-use-xgboost-heres-how-to-make-it-200x-faster-16cb6039a16e?source=collection_archive---------8-----------------------#2021-07-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="221c" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/tips-and-tricks" rel="noopener" target="_blank">提示和技巧</a></h2><div class=""/><div class=""><h2 id="921f" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">在本文中，我将向您展示四种训练XGBoost的方法。与XGBoost默认设置相比，我们将实现200倍的速度提升。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/e7216e7909810a34ace270f22ab37a86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MlkRsVhx8E7-yMmMkEX1iw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated"><a class="ae lh" href="https://pxhere.com/en/photo/910923" rel="noopener ugc nofollow" target="_blank">信用</a></p></figure><p id="bad0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">作为数据科学家，我们喜欢进行许多耗时的实验。降低我们模型的训练速度意味着我们可以在相同的时间内进行更多的实验。此外，我们还可以通过创建更大的模型集合来利用这种速度，最终获得更高的准确性。</p><h1 id="d528" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">常规XGBoost</h1><p id="77d8" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated"><a class="ae lh" href="https://arxiv.org/abs/1603.02754" rel="noopener ugc nofollow" target="_blank">陈和Guestrin(来自华盛顿大学)</a>发布2016年XGBoost日期。与常规梯度增强相比，它们实现了显著的加速和更高的预测能力(<a class="ae lh" href="https://www.amazon.com/Data-Science-Supply-Chain-Forecast/dp/1730969437" rel="noopener ugc nofollow" target="_blank">参见我的书进行比较</a>，<a class="ae lh" href="https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html" rel="noopener ugc nofollow" target="_blank">参见scikit-learn进行常规梯度增强</a>)。这个新模型很快成为Kaggle上数据科学家的最爱。</p><p id="3fe1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">让我们在一个10M行30列的虚拟数据集上运行XGBoost‘vanilla’版本。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nb nc l"/></div></figure><pre class="ks kt ku kv gt nd ne nf ng aw nh bi"><span id="9e80" class="ni mf it ne b gy nj nk l nl nm">times_cpu = []<br/>for trial in range(trials):<br/> start = time.time()<br/> XGB = XGBRegressor()<br/> XGB = XGB.fit(X,Y)<br/> print(time.time() — start)<br/> times_cpu.append(time.time() — start)</span></pre><p id="4676" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">运行时间(在我的英特尔i7–7700k CPU @ 4.20 GHz计算机上)大约为2000秒。我只尝试了一次这个设置，因为它需要大约半个小时的运行时间。</p><h1 id="7d95" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">使用XGBoost提升直方图</h1><p id="a7d1" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">LGBM由微软团队于2017年<a class="ae lh" href="https://papers.nips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf" rel="noopener ugc nofollow" target="_blank">发布。他们让训练更快的突破性想法是将连续特征(读取训练数据)存储在</a><a class="ae lh" href="https://lightgbm.readthedocs.io/en/latest/Features.html#references" rel="noopener ugc nofollow" target="_blank">离散箱</a>(直方图)中。</p><p id="0b3c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">基于直方图的增强现在被认为是增强树的最佳实践。现在由XGBoost、<a class="ae lh" href="https://catboost.ai/news/catboost-enables-fast-gradient-boosting-on-decision-trees-using-gpus" rel="noopener ugc nofollow" target="_blank"> CatBoost </a>以及<a class="ae lh" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn </a>来实现。</p><p id="40f9" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在训练XGBoost时，可以通过设置<em class="nn"> tree_method = 'hist' </em>来使用这种新的基于直方图的方法。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nb nc l"/></div></figure><pre class="ks kt ku kv gt nd ne nf ng aw nh bi"><span id="3f7f" class="ni mf it ne b gy nj nk l nl nm">times_cpu_hist = []<br/>for trial in range(trials):<br/> start = time.time()<br/> XGB = XGBRegressor(tree_method = “hist”)<br/> XGB = XGB.fit(X,Y)<br/> print(time.time() — start)<br/> times_cpu.append(time.time() — start)</span></pre><p id="a023" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">现在平均跑步时间是57.7秒。我们刚刚实现了35倍的速度提升！</p><h1 id="924c" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">GPU上的XGBoost</h1><p id="70b2" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">XGBoost允许你使用你的GPU来训练模型。这通常比常规CPU运行得更快，并且可以很容易地激活。</p><p id="3f0f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">要使用您的GPU，请将<em class="nn"> tree_method </em>更改为<em class="nn">‘GPU _ hist’</em></p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nb nc l"/></div></figure><pre class="ks kt ku kv gt nd ne nf ng aw nh bi"><span id="53c5" class="ni mf it ne b gy nj nk l nl nm">times_gpu = []<br/>for trial in range(trials):<br/> start = time.time()<br/> XGB = XGBRegressor(tree_method = “gpu_hist”)<br/> XGB = XGB.fit(X,Y)<br/> print(time.time() — start)<br/> times_gpu.append(time.time() — start)</span></pre><p id="19c3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">运行时间现在约为13.1秒(使用Nvidia GeForce GTX 1080)。比CPU快4.4倍。</p></div><div class="ab cl no np hx nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="im in io ip iq"><p id="18e0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">以下是如何使用GPU在windows机器上运行XGBoost的方法。</p><blockquote class="nv nw nx"><p id="885c" class="li lj nn lk b ll lm kd ln lo lp kg lq ny ls lt lu nz lw lx ly oa ma mb mc md im bi translated">如果已经可以在GPU上运行XGBoost，可以跳过这一部分。</p></blockquote><h2 id="8dd3" class="ni mf it bd mg ob oc dn mk od oe dp mo lr of og mq lv oh oi ms lz oj ok mu iz bi translated">步骤1:安装正确版本的XGBoost</h2><p id="e423" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated"><strong class="lk jd">如果你通过conda/anaconda安装了XGBoost，你将无法使用你的GPU </strong>。(如果您不确定如何在您的机器上获得XGBoost，95%的可能性是通过anaconda/conda获得的)。</p><p id="d397" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">相反，我们将使用pip install来安装它。</p><p id="23be" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">打开控制台，键入以下两个提示</p><p id="4bca" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">首先卸载XGBoost。</p><pre class="ks kt ku kv gt nd ne nf ng aw nh bi"><span id="bdca" class="ni mf it ne b gy nj nk l nl nm">pip uninstall xgboost</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ol"><img src="../Images/675b69421058568a7f30a3f3d38593ca.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-8yNkdZjfsKVd85Q-_4PTQ.png"/></div></div></figure><p id="f0e0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">然后，重新安装。</p><pre class="ks kt ku kv gt nd ne nf ng aw nh bi"><span id="2ec1" class="ni mf it ne b gy nj nk l nl nm">pip install xgboost</span></pre><h2 id="bdbd" class="ni mf it bd mg ob oc dn mk od oe dp mo lr of og mq lv oh oi ms lz oj ok mu iz bi translated">步骤2:安装CUDA</h2><p id="779c" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">Cuda需要使用Nvidia GPU进行机器学习和深度学习。</p><p id="63ea" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">在这里下载:<a class="ae lh" href="https://developer.nvidia.com/cuda-downloads" rel="noopener ugc nofollow" target="_blank">https://developer.nvidia.com/cuda-downloads</a>。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi om"><img src="../Images/6718bd6d34e22fb2c76e41d9b086f348.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*k7KLkYIIG706T8snE1Dgiw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">步骤1:下载CUDA</p></figure><p id="6ef3" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您需要使用标准过程(使用快速设置)来安装它—这里没有什么特别要做的。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi on"><img src="../Images/a243d1d1844d437921a487d1a5025470.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*txrSv7xgfOZitIAzB8VvXg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">步骤2:启动安装程序。</p></figure><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oo"><img src="../Images/90bb5d3443bc2167c6fe3bb0097830c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dzT1-iO6mHxu5ckR6RcQ6w.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">第三步:使用快速推荐的设置(原谅我的法语！)</p></figure><p id="d431" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">就是这样！</p></div><div class="ab cl no np hx nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="im in io ip iq"><h1 id="58d5" class="me mf it bd mg mh op mj mk ml oq mn mo ki or kj mq kl os km ms ko ot kp mu mv bi translated">XGBoost采用单精度GPU</h1><p id="a0ae" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">您需要将'<em class="nn">single _ precision _ histogram</em>'设置为True。</p><figure class="ks kt ku kv gt kw"><div class="bz fp l di"><div class="nb nc l"/></div></figure><pre class="ks kt ku kv gt nd ne nf ng aw nh bi"><span id="52ec" class="ni mf it ne b gy nj nk l nl nm">times_gpu_single = []<br/>for trial in range(trials):<br/> start = time.time()<br/> XGB = XGBRegressor(tree_method = “gpu_hist”, single_precision_histogram=True)<br/> XGB = XGB.fit(X,Y)<br/> print(time.time() — start)<br/> times_gpu.append(time.time() — start)</span></pre><p id="0f69" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">跑步时间现在是8.7秒。与标准的GPU方法相比，这减少了大约32%。</p><p id="b3dd" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">⚠️注意到单精度模式并不总是有效的。如果数据超出了单精度浮点数的限制，就会遇到问题。在跟踪预测模型的平方误差时，我遇到了这种情况:<strong class="lk jd">平方</strong>误差超过了单精度浮点数可以处理的最大限制。</p><h1 id="c42d" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">结论</h1><p id="616e" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">使用我们的GPU(而不是CPU)、直方图提升和单精度浮点数，我们为XGBoost实现了228倍的训练时间加速。</p><ul class=""><li id="5d5a" class="ou ov it lk b ll lm lo lp lr ow lv ox lz oy md oz pa pb pc bi translated">CPU: <strong class="lk jd"> 2000 </strong>秒</li><li id="44c2" class="ou ov it lk b ll pd lo pe lr pf lv pg lz ph md oz pa pb pc bi translated">CPU和直方图:<strong class="lk jd"> 57.7 </strong>秒</li><li id="d740" class="ou ov it lk b ll pd lo pe lr pf lv pg lz ph md oz pa pb pc bi translated">图形处理器和直方图:<strong class="lk jd"> 13.1 </strong>秒</li><li id="a819" class="ou ov it lk b ll pd lo pe lr pf lv pg lz ph md oz pa pb pc bi translated">GPU、直方图和单精度:<strong class="lk jd"> 8.7 </strong>秒</li></ul><p id="1064" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">🚀您可以通过在GPU和CPU上并行运行实验来进一步加快实验过程。</p><p id="358f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">⚠️🔬不要忘记本文使用的是虚拟数据集。数据集和机器上的结果可能不同。在选择<strong class="lk jd">您最喜欢的</strong>方法之前，您应该尝试一下GPU、直方图和单精度。</p></div><div class="ab cl no np hx nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="im in io ip iq"><p id="8571" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">您可能也会对本文感兴趣:</p><div class="pi pj gp gr pk pl"><a rel="noopener follow" target="_blank" href="/read-excel-files-with-python-1000x-faster-407d07ad0ed8"><div class="pm ab fo"><div class="pn ab po cl cj pp"><h2 class="bd jd gy z fp pq fr fs pr fu fw jc bi translated">用Python读Excel文件吗？有一个快1000倍的方法。</h2><div class="ps l"><h3 class="bd b gy z fp pq fr fs pr fu fw dk translated">在本文中，我将向您展示用Python加载数据的五种方法。实现了3个数量级的加速。</h3></div><div class="pt l"><p class="bd b dl z fp pq fr fs pr fu fw dk translated">towardsdatascience.com</p></div></div><div class="pu l"><div class="pv l pw px py pu pz lb pl"/></div></div></a></div></div><div class="ab cl no np hx nq" role="separator"><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt nu"/><span class="nr bw bk ns nt"/></div><div class="im in io ip iq"><h1 id="b4b1" class="me mf it bd mg mh op mj mk ml oq mn mo ki or kj mq kl os km ms ko ot kp mu mv bi translated">关于作者</h1><p id="d058" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">Nicolas Vandeput是供应链数据科学家，专门研究需求预测和库存优化。他在2016年创立了自己的咨询公司<a class="ae lh" href="http://www.supchains.com/" rel="noopener ugc nofollow" target="_blank"> SupChains </a>，并在2018年共同创立了<a class="ae lh" href="https://bit.ly/3ozydFN" rel="noopener ugc nofollow" target="_blank"> SKU科学</a>——一个快速、简单、实惠的需求预测平台。尼古拉斯对教育充满热情，他既是一个狂热的学习者，也喜欢在大学教学:自2014年以来，他一直在比利时布鲁塞尔为硕士学生教授预测和库存优化。自2020年以来，他还在法国巴黎的CentraleSupelec教授这两门课程。他于2018年出版了<a class="ae lh" href="https://www.amazon.com/Data-Science-Supply-Chain-Forecasting/dp/3110671107" rel="noopener ugc nofollow" target="_blank"> <em class="nn">供应链预测的数据科学</em></a>(2021年第2版)，2020年出版了<a class="ae lh" href="https://www.amazon.com/Inventory-Optimization-Simulations-Nicolas-Vandeput/dp/3110673916" rel="noopener ugc nofollow" target="_blank"> <em class="nn">库存优化:模型与模拟</em> </a>。</p></div></div>    
</body>
</html>