<html>
<head>
<title>XGBoost explained: DIY XGBoost library in less than 200 lines of python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">XGBoost解释:用不到200行python DIY XGBoost库</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/diy-xgboost-library-in-less-than-200-lines-of-python-69b6bf25e7d9?source=collection_archive---------20-----------------------#2021-04-12">https://towardsdatascience.com/diy-xgboost-library-in-less-than-200-lines-of-python-69b6bf25e7d9?source=collection_archive---------20-----------------------#2021-04-12</a></blockquote><div><div class="fc ij ik il im in"/><div class="io ip iq ir is"><div class=""/><figure class="gl gn jt ju jv jw gh gi paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="gh gi js"><img src="../Images/ea4f654fad88731ec662c672700414a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*v7uP108oJifBZXrj"/></div></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">照片由<a class="ae kh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的<a class="ae kh" href="https://unsplash.com/@madebyjens?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">延斯·勒列</a>拍摄</p></figure><blockquote class="ki kj kk"><p id="05a7" class="kl km kn ko b kp kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg lh li lj io bi translated">更新:发现我关于渐变增强的新书，<a class="ae kh" href="https://amzn.to/3OMgEkR" rel="noopener ugc nofollow" target="_blank">实用渐变增强</a>。这是用python中的许多例子对渐变增强的深入探究。</p></blockquote><div class="lk ll gp gr lm ln"><a href="https://amzn.to/3OMgEkR" rel="noopener  ugc nofollow" target="_blank"><div class="lo ab fo"><div class="lp ab lq cl cj lr"><h2 class="bd iw gy z fp ls fr fs lt fu fw iu bi translated">实用的渐变增强:深入探究Python中的渐变增强</h2><div class="lu l"><h3 class="bd b gy z fp ls fr fs lt fu fw dk translated">这本书的梯度推进方法是为学生，学者，工程师和数据科学家谁希望…</h3></div></div><div class="lv l"><div class="lw l lx ly lz lv ma kb ln"/></div></div></a></div><p id="7091" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">XGBoost可能是数据科学中使用最广泛的库之一。世界各地的许多数据科学家都在使用它。这是一种非常通用算法，可用于执行分类、回归以及置信区间，如本文<a class="ae kh" rel="noopener" target="_blank" href="/confidence-intervals-for-xgboost-cac2955a8fde">中的</a>所示。但是有多少人真正理解它的基本原理呢？</p><p id="9943" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">你可能会认为，一个性能和XGBoost一样好的机器学习算法可能会使用非常复杂和高级的数学。你可能会认为这是软件工程的杰作。</p><p id="3bf9" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">你说对了一部分。XGBoost库非常复杂，但是如果只考虑应用于决策树的梯度推进的数学公式，它并不复杂。</p><p id="b08e" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">您将在下面看到如何使用不到200行代码的梯度推进方法为<strong class="ko iw">回归</strong>训练决策树的详细信息。</p></div><div class="ab cl me mf hz mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="io ip iq ir is"><h1 id="292c" class="ml mm iv bd mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni bi translated">决策图表</h1><p id="c550" class="pw-post-body-paragraph kl km iv ko b kp nj kr ks kt nk kv kw mb nl kz la mc nm ld le md nn lh li lj io bi translated">在进入数学细节之前，让我们回顾一下关于决策树的记忆。原理相当简单:将一个值与遍历二叉树的一组给定特征相关联。二叉树的每个节点都附加一个条件；树叶是有价值的。</p><p id="3c6d" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">如果条件为真，我们使用左边的节点继续遍历，否则，我们使用右边的节点。一旦到达一片叶子，我们就有了我们的预测。</p><p id="5bf0" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">通常，一张图片胜过千言万语:</p><figure class="np nq nr ns gt jw gh gi paragraph-image"><div class="gh gi no"><img src="../Images/c3d266f1a1981545a36ea602d266aea9.png" data-original-src="https://miro.medium.com/v2/resize:fit:940/format:webp/1*6JiNRLLaZiVmCIDwI7b0cQ.png"/></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">三层决策树。图片由作者提供。</p></figure><p id="e32a" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">附加在节点上的条件可以看作是一个决策，因此得名<strong class="ko iw">决策树</strong>。</p><p id="660f" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">这种结构在计算机科学的历史上非常古老，已经成功地使用了几十年。下面几行给出了一个基本实现:</p><figure class="np nq nr ns gt jw"><div class="bz fp l di"><div class="nt nu l"/></div></figure><h1 id="c99b" class="ml mm iv bd mn mo nv mq mr ms nw mu mv mw nx my mz na ny nc nd ne nz ng nh ni bi translated">堆叠多棵树</h1><p id="14bf" class="pw-post-body-paragraph kl km iv ko b kp nj kr ks kt nk kv kw mb nl kz la mc nm ld le md nn lh li lj io bi translated">尽管决策树已经在一些应用中取得了一些成功，如专家系统(在人工智能冬天之前)，但它仍然是一个非常基本的模型，无法处理现实世界数据中通常遇到的复杂性。</p><p id="8101" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">我们通常将这类估计量称为<strong class="ko iw"> <em class="kn">弱模型</em> </strong>。</p><p id="6620" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">为了克服这一限制，九十年代出现了将多个<em class="kn">弱模型</em>组合起来创建一个<em class="kn">强模型:</em> <strong class="ko iw"> <em class="kn">集成学习</em> </strong> <em class="kn">的想法。</em></p><p id="0647" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">这种方法可以应用于任何类型的模型，但由于决策树是简单、快速、通用和易于解释的模型，所以它们被广泛使用。</p><p id="cdc0" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">可以部署各种策略来组合模型。例如，我们可以使用每个模型预测的加权和。或者更好的是，使用贝叶斯方法基于学习将它们结合起来。</p><p id="eded" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">XGBoost和所有的<strong class="ko iw"> boosting </strong>方法使用另一种方法:每个新模型都试图补偿前一个模型的错误。</p><h1 id="7b22" class="ml mm iv bd mn mo nv mq mr ms nw mu mv mw nx my mz na ny nc nd ne nz ng nh ni bi translated">优化决策树</h1><p id="86da" class="pw-post-body-paragraph kl km iv ko b kp nj kr ks kt nk kv kw mb nl kz la mc nm ld le md nn lh li lj io bi translated">正如我们在上面看到的，使用决策树进行预测非常简单。当使用<em class="kn">集成学习</em>时，这项工作并没有变得更加复杂:我们所要做的就是将每个模型的贡献相加。</p><p id="a18f" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">真正复杂的是建造树本身！我们如何找到在训练数据集的每个节点应用的最佳条件？这就是数学帮助我们的地方。完整的推导可以在<a class="ae kh" href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html" rel="noopener ugc nofollow" target="_blank"> XGBoost文档</a>中找到。这里，我们将只关注本文感兴趣的公式。</p><p id="5db7" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">与机器学习一样，我们希望设置我们的模型参数，以便我们的模型对训练集的预测最小化给定的目标:</p><figure class="np nq nr ns gt jw gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/936f0239125e64637b359de271cc1035.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*4RuWqoeljq7jY525oWwVGw.png"/></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">客观公式。作者的公式。</p></figure><p id="de25" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">请注意，该目标由两个术语组成:</p><ul class=""><li id="cea0" class="ob oc iv ko b kp kq kt ku mb od mc oe md of lj og oh oi oj bi translated">一个是测量预测产生的误差。这就是著名的损失函数<em class="kn"> l(y，y_hat) </em>。</li><li id="cbc8" class="ob oc iv ko b kp ok kt ol mb om mc on md oo lj og oh oi oj bi translated">另一个，<em class="kn">ω</em>，控制模型复杂度。</li></ul><p id="cf4a" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">正如XGBoost <a class="ae kh" href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html" rel="noopener ugc nofollow" target="_blank">文档</a>中所述，复杂性是目标的一个非常重要的部分，它允许我们调整偏差/方差权衡。许多不同的函数可以用来定义这个正则项。XGBoost使用:</p><figure class="np nq nr ns gt jw gh gi paragraph-image"><div class="gh gi op"><img src="../Images/eff7109cb2d57e29585476707dea5d05.png" data-original-src="https://miro.medium.com/v2/resize:fit:800/format:webp/1*HIUjyaintVq7KoS8bEV2fA.png"/></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">正则项。作者的公式。</p></figure><p id="ee14" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">这里<em class="kn"> T </em>是叶子的总数，而<em class="kn"> w_j </em>是附在每片叶子上的权重。这意味着大的重量和大量的叶子会被扣分。</p><p id="a65e" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">由于误差通常是复杂的非线性函数，我们使用二阶泰勒展开将其线性化:</p><figure class="np nq nr ns gt jw gh gi paragraph-image"><div role="button" tabindex="0" class="jx jy di jz bf ka"><div class="gh gi oq"><img src="../Images/c002de72f5f69f70945238fb695bdd11.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wKpQ-hXdrizZze9_1Lc3wQ.png"/></div></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">损失的二阶展开。作者的公式。</p></figure><p id="eae9" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">其中:</p><figure class="np nq nr ns gt jw gh gi paragraph-image"><div class="gh gi or"><img src="../Images/e2421fdc7c3cb677043e07bb6e79d487.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*YXfXo5EH5Dy7n4sEgGKdxg.png"/></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">高斯和海森公式。作者的公式。</p></figure><p id="95d0" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">线性化是相对于预测项计算的，因为我们想要估计当预测改变时误差如何改变。线性化是必不可少的，因为它将使误差最小化。</p><p id="02a4" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">我们想要用梯度增强实现的<em class="kn"> </em>是找到将最小化损失函数的最优<em class="kn">【δ_ y _ I】</em>，即，我们想要找到如何修改现有的树，使得修改改进预测。</p><p id="e41b" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">在处理树模型时，有两种参数需要考虑:</p><ul class=""><li id="010b" class="ob oc iv ko b kp kq kt ku mb od mc oe md of lj og oh oi oj bi translated">定义树本身的:每个节点的条件，树的深度</li><li id="653e" class="ob oc iv ko b kp ok kt ol mb om mc on md oo lj og oh oi oj bi translated">附在每片树叶上的值。这些值就是预测本身。</li></ul><p id="2730" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">探索每棵树的配置将会太复杂，因此梯度树提升方法仅考虑将一个节点分成两片叶子。这意味着我们必须优化三个参数:</p><ul class=""><li id="9506" class="ob oc iv ko b kp kq kt ku mb od mc oe md of lj og oh oi oj bi translated">分割值:在什么条件下我们分割数据</li><li id="c65b" class="ob oc iv ko b kp ok kt ol mb om mc on md oo lj og oh oi oj bi translated">附加到左叶的值</li><li id="ea52" class="ob oc iv ko b kp ok kt ol mb om mc on md oo lj og oh oi oj bi translated">附加到右叶的值</li></ul><p id="3acc" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">在XGBoost文档中，树叶<em class="kn"> j </em>相对于目标的最佳值由下式给出:</p><figure class="np nq nr ns gt jw gh gi paragraph-image"><div class="gh gi os"><img src="../Images/27a252f0be789c7f9f3686ad36ef3bde.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*Q7Nbq-3Giw53XfEZEUiQbA.png"/></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">相对于目标的最佳叶值。作者的公式。</p></figure><p id="71d8" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">其中<em class="kn"> G_j </em>是附着在节点<em class="kn"> j </em>上的训练点的梯度之和，<em class="kn"> H_j </em>是附着在节点<em class="kn">j</em>上的训练点的hessian之和</p><p id="45e8" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">利用该最佳参数获得的目标函数的减少量为:</p><figure class="np nq nr ns gt jw gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/850ce48e3317cdffe72c33009503d31d.png" data-original-src="https://miro.medium.com/v2/resize:fit:972/format:webp/1*3YNIy3auM6LYuainjezZMA.png"/></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">使用最佳权重时的客观改进。作者的公式。</p></figure><p id="be92" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">使用brut force选择正确的分割值:我们计算每个分割值的改进，并保留最佳值。</p><p id="de0f" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">现在，我们已经获得了所有必要的数学信息，可以通过添加新叶来提高初始树的性能。</p><p id="312c" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">在具体做之前，我们先花点时间了解一下这些公式是什么意思。</p><h1 id="1d99" class="ml mm iv bd mn mo nv mq mr ms nw mu mv mw nx my mz na ny nc nd ne nz ng nh ni bi translated">爬行梯度推进</h1><p id="5a26" class="pw-post-body-paragraph kl km iv ko b kp nj kr ks kt nk kv kw mb nl kz la mc nm ld le md nn lh li lj io bi translated">让我们试着了解一下权重是如何计算的，以及<em class="kn"> G_j </em>和<em class="kn"> H_i </em>等于什么。因为它们分别是损失函数相对于预测的梯度和hessian，我们必须选择一个损失函数。</p><p id="06e0" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">我们将关注平方误差，这是常用的，也是XGBoost的默认目标:</p><figure class="np nq nr ns gt jw gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/7e0e27eecb88e2c5b5c3ee966205552d.png" data-original-src="https://miro.medium.com/v2/resize:fit:348/format:webp/1*UEPmfAakqbVsnZn7xJpljA.png"/></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">平方误差损失函数。作者的公式</p></figure><p id="7fee" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">这是一个非常简单的公式，它的梯度是:</p><figure class="np nq nr ns gt jw gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/d567e1ffcfc7df1c1f7051d247c4156e.png" data-original-src="https://miro.medium.com/v2/resize:fit:236/format:webp/1*GBVRp186tIT4uuWIRI3e6Q.png"/></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">损失函数的梯度。作者的公式。</p></figure><p id="81fb" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">和黑森:</p><figure class="np nq nr ns gt jw gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/db1a6dcf00529115d3303aabdbe431a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:96/format:webp/1*OI4DnqeHNX6P2nLTijYQ1Q.png"/></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">损失函数的海森。作者的公式。</p></figure><p id="aa19" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">因此，如果我们记住最大限度减少误差的最佳权重公式:</p><figure class="np nq nr ns gt jw gh gi paragraph-image"><div class="gh gi os"><img src="../Images/27a252f0be789c7f9f3686ad36ef3bde.png" data-original-src="https://miro.medium.com/v2/resize:fit:594/format:webp/1*Q7Nbq-3Giw53XfEZEUiQbA.png"/></div><p class="kd ke gj gh gi kf kg bd b be z dk translated">相对于目标的最佳叶值。作者的公式。</p></figure><p id="5298" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">我们认识到最优权重，<em class="kn">即</em>我们添加到先前预测的值是先前预测和真实值之间的平均误差的相反值(当正则化被禁用时，即<em class="kn">λ= 0</em>)。使用平方损失来训练具有梯度增强的决策树归结起来就是用每个新节点中的平均误差来更新预测。</p><p id="fb1c" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">我们还看到<em class="kn">λ</em>具有预期的效果，即确保权重不会太大，因为权重与<em class="kn">λ</em>成反比。</p><h1 id="2030" class="ml mm iv bd mn mo nv mq mr ms nw mu mv mw nx my mz na ny nc nd ne nz ng nh ni bi translated">训练决策树</h1><p id="9fe8" class="pw-post-body-paragraph kl km iv ko b kp nj kr ks kt nk kv kw mb nl kz la mc nm ld le md nn lh li lj io bi translated">现在是容易的部分。假设我们有一个现有的决策树，它可以确保预测有给定的误差。我们希望通过分割一个节点来减少误差并改进附加目标。</p><p id="6af0" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">这样做的算法非常简单:</p><ul class=""><li id="0bdb" class="ob oc iv ko b kp kq kt ku mb od mc oe md of lj og oh oi oj bi translated">选择感兴趣的特征</li><li id="9c0a" class="ob oc iv ko b kp ok kt ol mb om mc on md oo lj og oh oi oj bi translated">使用所选要素的值对附加到当前节点的数据点进行排序</li><li id="63d8" class="ob oc iv ko b kp ok kt ol mb om mc on md oo lj og oh oi oj bi translated">选择一个可能的分割值</li><li id="b954" class="ob oc iv ko b kp ok kt ol mb om mc on md oo lj og oh oi oj bi translated">将此拆分值下方的数据点放在右节点中，将上方的数据点放在左节点中</li><li id="29ea" class="ob oc iv ko b kp ok kt ol mb om mc on md oo lj og oh oi oj bi translated">计算父节点、右节点和左节点的目标约简</li><li id="e25f" class="ob oc iv ko b kp ok kt ol mb om mc on md oo lj og oh oi oj bi translated">如果左右节点的目标约简之和大于父节点的目标约简之和，则保持分割值为最佳值</li><li id="1884" class="ob oc iv ko b kp ok kt ol mb om mc on md oo lj og oh oi oj bi translated">迭代每个分割值</li><li id="df12" class="ob oc iv ko b kp ok kt ol mb om mc on md oo lj og oh oi oj bi translated">使用最佳分割值(如果有)，并添加两个新节点</li><li id="b2f4" class="ob oc iv ko b kp ok kt ol mb om mc on md oo lj og oh oi oj bi translated">如果没有拆分改进了目标，就不要添加子节点。</li></ul><p id="2ed0" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">产生的代码创建一个决策树类，该类由一个目标、多个估计器(即树的数量)和一个最大深度来配置。</p><p id="e511" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">正如承诺的那样，这段代码不到200行:</p><figure class="np nq nr ns gt jw"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="fddf" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">训练的核心编码在函数<em class="kn"> _find_best_split </em>中。它基本上遵循上面详述的步骤。</p><p id="ffa2" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">注意，为了支持任何种类的目标，没有手动计算梯度和hessian的痛苦，我们使用自动微分和<a class="ae kh" href="https://jax.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank"> jax </a>库来自动计算。</p><p id="10a9" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">最初，我们从只有一个节点的树开始，这个节点的叶值leaf是零。因为我们模仿XGBoost，所以我们也使用一个基本分数，我们将它设置为要预测的值的平均值。</p><p id="e519" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">另外，请注意，在第126行，如果我们达到了初始化树时定义的最大深度，我们将停止树的构建。我们可以使用其他条件，如每片叶子的最小样本数或新权重的最小值。</p><p id="3695" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">另一个非常重要的点是用于分割的特征的选择。这里，为了简单起见，特性是随机选择的，但是我们可以使用更聪明的策略，比如使用方差最大的特性。</p></div><div class="ab cl me mf hz mg" role="separator"><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj mk"/><span class="mh bw bk mi mj"/></div><div class="io ip iq ir is"><h1 id="7841" class="ml mm iv bd mn mo mp mq mr ms mt mu mv mw mx my mz na nb nc nd ne nf ng nh ni bi translated">结论</h1><p id="94d5" class="pw-post-body-paragraph kl km iv ko b kp nj kr ks kt nk kv kw mb nl kz la mc nm ld le md nn lh li lj io bi translated">在本文中，我们看到了梯度推进是如何训练决策树的。为了进一步提高我们的理解，我们编写了训练决策树集合并使用它们进行预测所需的最小行集。</p><p id="8cc0" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">深入理解我们用于机器学习的算法绝对至关重要。它不仅帮助我们构建更好的模型，更重要的是允许我们根据自己的需要修改这些模型。</p><p id="90eb" class="pw-post-body-paragraph kl km iv ko b kp kq kr ks kt ku kv kw mb ky kz la mc lc ld le md lg lh li lj io bi translated">例如，在梯度增强的情况下，使用损失函数是提高预测精度的一个很好的方法。</p></div></div>    
</body>
</html>