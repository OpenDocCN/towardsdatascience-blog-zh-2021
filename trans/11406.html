<html>
<head>
<title>Distribution-based loss functions for deep learning models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度学习模型的基于分布的损失函数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/distribution-based-loss-functions-for-deep-learning-models-71fa4b042465?source=collection_archive---------13-----------------------#2021-11-09">https://towardsdatascience.com/distribution-based-loss-functions-for-deep-learning-models-71fa4b042465?source=collection_archive---------13-----------------------#2021-11-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="925a" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="9769" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">交叉熵及其度量分类损失的变体综述——以高度不平衡数据集为例</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/a53f3c43ea8f3a7efcb02fe0380cb5ec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vdWQiGYbhW6suLebCKCz6A.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">由<a class="ae le" href="https://unsplash.com/@wwarby?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">威廉·沃比</a>在<a class="ae le" href="https://unsplash.com/s/photos/measure?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="0b62" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">赋予数据意义</h1><p id="d5bc" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">信息由数据组成。在训练步骤期间，人工神经网络学习将一组输入映射(预测)到来自标记数据集的一组输出。计算最佳权重是一个优化问题，通常通过<a class="ae le" href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent" rel="noopener ugc nofollow" target="_blank"> <strong class="lz ja">随机梯度下降</strong> </a>来解决:使用预测误差的反向传播来更新权重。梯度下降算法更新沿着误差的梯度(或斜率)向下导航的权重，从而可以减少下一次预测的误差。从本质上讲，这就是神经网络的工作方式。</p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><h1 id="5391" class="lf lg iq bd lh li na lk ll lm nb lo lp kf nc kg lr ki nd kj lt kl ne km lv lw bi translated">什么是损失函数？</h1><p id="0824" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">在寻找最佳权重时，学习算法需要一个特殊的对象来评估一组候选权重的适用性:这是<strong class="lz ja">目标函数</strong>。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/59fbe8f63b7128b21773dd0cba10af3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-COR70Dn5rWWxhZeoQnj4w.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@javaistan?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Afif Kusuma </a>在<a class="ae le" href="https://unsplash.com/s/photos/target?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="bc04" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">根据上下文，目标函数可以最大化或最小化。在处理深度学习模型时，专家更喜欢根据误差进行推理，因此他们的目标是最小化目标函数。因此，目标函数称为<strong class="lz ja">损失函数</strong>，其值(即误差)简称为<strong class="lz ja"> <em class="nk">损失</em> </strong>。损失函数对于确保模型响应的适当数学表示至关重要，必须仔细考虑其选择，因为它必须<em class="nk">适合模型域及其分类目标</em>。</p><p id="eb9c" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">损失函数的定义和应用始于标准的机器学习方法。当时，这些函数是基于标签的<strong class="lz ja">分布，正是由于这个原因，下面的函数被称为基于<strong class="lz ja">分布的损失函数</strong>。</strong></p><p id="c835" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">我们的讨论正是从这里开始的，特别是从<strong class="lz ja">交叉熵</strong>的概念开始。</p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><p id="1e63" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">注意:所有用于情节和案例研究的代码都可以在我的个人github上获得:<a class="ae le" href="https://github.com/andrea-ci/misc-stuff/tree/master/nn-metrics" rel="noopener ugc nofollow" target="_blank">https://github . com/Andrea-ci/misc-stuff/tree/master/nn-metrics</a>。</p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><h1 id="fb0e" class="lf lg iq bd lh li na lk ll lm nb lo lp kf nc kg lr ki nd kj lt kl ne km lv lw bi translated">引入交叉熵</h1><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/ec529a0c213d342f61d605605a1b08c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*HvKWfHPByO_2JPi5xgIgtw.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">照片由<a class="ae le" href="https://unsplash.com/@greysonjoralemon?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Greyson Joralemon </a>在<a class="ae le" href="https://unsplash.com/s/photos/balls?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="d9e1" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">根据信息论，给定一个离散随机变量<em class="nk"> x </em>，其<strong class="lz ja">熵</strong> <em class="nk"> H(x) </em>(也被称为<strong class="lz ja">香农熵</strong>，由其创造者<a class="ae le" href="https://en.wikipedia.org/wiki/Claude_Shannon" rel="noopener ugc nofollow" target="_blank">克劳德·香农</a>定义为概率的倒数的对数的期望值:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/a112a783a434ca1cbbeedec1d9678cd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:980/format:webp/1*E5Xe5k5in2JkfPb5jWabRA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">熵—作者提供的图像</p></figure><p id="d735" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">直观地说，<strong class="lz ja">熵</strong>衡量与随机过程的可能结果相关的不确定性:对观察者来说，它们越“令人惊讶”，熵就越高。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/53a706c8af50b9f55aee25159ba3bee6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*oii7oZiaUdKLLzrj4EokLA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者的伯努利变量图像的熵</p></figure><p id="69ee" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">请注意，对数以2为底，因为在信息论的背景下，我们感兴趣的是对随机过程携带的信息进行编码所需的比特数。</p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><p id="5606" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">现在假设我们有两个概率分布，<em class="nk"> p </em>和<em class="nk"> q </em>，定义在同一个随机变量<em class="nk"> x </em>上。如果编码方案是针对<em class="nk"> q </em>而不是<em class="nk"> p </em>优化的(这将是真实的分布)，我们想要测量<strong class="lz ja">从<em class="nk"> x </em>的样本空间中识别一个事件所需的平均比特数</strong>。</p><p id="5ee4" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">在可能的结果只有两个的情况下，如在前面的伯努利变量的例子中，我们对<strong class="lz ja">二元交叉熵损失函数有如下定义:</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/5754122e3eb7a8fdbbc0eba5e51fab38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*qop7e2VkzwK5bj1r06DUhA.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">二元交叉熵—作者图片</p></figure><p id="6b7e" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">具有二元场景允许简化方程，使得我们只有一个自变量，<em class="nk"> pt，</em>，其代表由模型分配给<em class="nk">真实类别</em>(即样本实际所属的类别)的概率值。</p><p id="803f" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">二元交叉熵被广泛用作损失函数，因为它对许多分类任务都很有效。事实上，它是基于分布的损失函数的基本<strong class="lz ja">基线。</strong></p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/e8afe87fb1ec9298b19654d36a5375b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*Gl4lsVsma1lcLidP0Wj2nQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="994e" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">图中，<em class="nk"> yt </em>是二元分类任务中样本的类别标签，<em class="nk"> yp </em>是模型赋予该类别的概率。该函数严重惩罚预测错误，其中以高置信度做出错误决策(即<em class="nk"> yp </em>接近<em class="nk"> 0 </em>)，而当预测接近<em class="nk"> 1 </em>的真值时(即做出正确分类)，该函数变为零。</p><p id="e73e" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">注意:为了简单起见，我们将保留与二元分类任务相关的讨论和示例。然而，到目前为止以及在下文中所做的考虑是有效的，并且自然也适用于多标签分类的情况。</p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><h1 id="bb7c" class="lf lg iq bd lh li na lk ll lm nb lo lp kf nc kg lr ki nd kj lt kl ne km lv lw bi translated">交叉熵的演化</h1><p id="f58e" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">在实践中，我们必须经常处理<strong class="lz ja">非常不平衡的</strong>数据集:例如，如果我们想要训练一个模型来识别海洋中的船只，那么正像素的数量，即那些属于“船”类的像素，相对于所有像素来说将是非常小的百分比。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/a74a343b69b6c4a2560c25bfc92dc00b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6bw8ZczYhcpVoMRNw4-K0g.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">有时阳性样本就像四叶草一样罕见——达斯汀·休姆斯<a class="ae le" href="https://unsplash.com/@dustinhumes_photography?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">在</a><a class="ae le" href="https://unsplash.com/s/photos/four-leaf-clover?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="18b8" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">因此，基于它们在数据集中的丰度，或者换句话说，根据它们的<em class="nk">先验</em>分类概率，具有能够以不同方式处理不同类别标签的损失函数将是有用的。</p><p id="73f9" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">为此开发了两种二进制交叉熵:加权二进制交叉熵和平衡交叉熵。</p><p id="c10e" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated"><strong class="lz ja">加权二进制交叉熵</strong> ( <strong class="lz ja"> WBCE </strong>)使用一个系数对阳性样本进行加权，当数据表现出明显的偏斜时，它通常是首选:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi no"><img src="../Images/e042166efb420d2f035639bd8959faa8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/format:webp/1*0PKF6rcLhSLDN-AcLg8tvw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">加权二进制交叉熵—作者图片</p></figure><p id="7fa8" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated"><strong class="lz ja">平衡二进制交叉熵</strong>(<strong class="lz ja"/>)<strong class="lz ja"/>类似于加权交叉熵，但是在这种情况下，负样本也受到如下权重系数的影响:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi np"><img src="../Images/8521b41ff92d95f5afff5d04dc65056b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1082/format:webp/1*STtLp0YaQwa0uQVay5IxyQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">平衡二进制交叉熵—作者图片</p></figure></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><p id="9035" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">最后，<strong class="lz ja">焦点损失(FL) </strong>也可以看作是二元交叉熵的变化。此函数适用于高度不平衡的数据集，因为它对正确分类的样本的贡献进行加权，并允许更好地学习硬样本(即未检测到的阳性样本):</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/49f292c532e99b49b1ff5804ddade635.png" data-original-src="https://miro.medium.com/v2/resize:fit:976/format:webp/1*k_-ue0Fgfp8VsUMq_pz8tQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">焦点丢失—作者提供的图像</p></figure><p id="35e5" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">这里<em class="nk"> λ </em>是一个超<em class="nk"> - </em>参数，我们可以用它来校准错误分类样本的重要性。直观地说，<em class="nk"> λ </em>通过扩展简单样本接收低值损失的范围来减少简单样本的损失。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/f07ebd129f8955a6baaa7f6223cc4a60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*xyPz5RgSKo9erv0nAUfDOQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">作者图片</p></figure><p id="006e" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">注意，当<em class="nk"> λ=0 </em>时，我们得到标准交叉熵损失。</p><p id="12ea" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">此外，聚焦损失还可以采用另一个参数，通常称为<em class="nk"> α </em>参数，其允许进一步平衡误差项:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/cb8c33a00c60a69ac630aa3cdac09b95.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*l2JhZN6u_xrtjkQdbDJ5Ng.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">焦点丢失的Alpha版本—图片由作者提供</p></figure><h1 id="2fed" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">案例研究:信用卡欺诈的检测</h1><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/e9e0af5acaba163a1fc3bf9ec382e9cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dOpV9Du_9g3w4vmBH2blSw.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">马库斯·温克勒在<a class="ae le" href="https://unsplash.com/s/photos/credit-cards?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><p id="441d" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">让我们举一个例子，其中选择<strong class="lz ja">聚焦损失</strong>允许我们解决一个在极度不平衡的数据集上的二进制分类问题。</p><p id="78a9" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">为此，我们考虑托管在Kaggle上的<a class="ae le" href="https://www.kaggle.com/mlg-ulb/creditcardfraud" rel="noopener ugc nofollow" target="_blank">信用卡欺诈检测</a>数据集。正如他们的网页上所说:</p><blockquote class="ns nt nu"><p id="b2b6" class="lx ly nk lz b ma nf ka mc md ng kd mf nv nh mi mj nw ni mm mn nx nj mq mr ms ij bi translated">“该数据集包含欧洲持卡人在2013年9月的信用卡交易。<br/>该数据集显示了两天内发生的交易，其中284，807笔交易中有492笔欺诈。数据集高度不平衡，正类(欺诈)占所有交易的0.172%。”</p></blockquote></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><p id="69ba" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">不言而喻，这里的目标是检测所有交易中欺诈操作的(非常)小的子集。在下文中，我们给出了以下步骤的简要概述。</p><p id="c527" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">首先，我们加载数据集并执行一些清理，以便为模型训练准备数据。</p><pre class="kp kq kr ks gt ny nz oa ob aw oc bi"><span id="a7b7" class="od lg iq nz b gy oe of l og oh">#<br/># DATA LOADING/CLEANSING<br/>#</span><span id="da9d" class="od lg iq nz b gy oi of l og oh"># Load data from CSV file.<br/>df_raw = pd.read_csv(‘creditcardfraud/creditcard.csv’)<br/>n_samples = len(df_raw)<br/>print(f’Num. of samples: {n_samples}.’)</span><span id="c1de" class="od lg iq nz b gy oi of l og oh"># Check size of samples.<br/>df_pos = df_raw[df_raw[‘Class’] == 1]<br/>n_pos_samples = len(df_pos)<br/>pos_ratio = 100 * n_pos_samples / n_samples<br/>print(f’Num. of positive samples: {n_pos_samples} ({pos_ratio:.2f}% of total).’)</span><span id="6211" class="od lg iq nz b gy oi of l og oh"># Drop useless data and convert amount to log space.<br/>df_cleaned = df_raw.copy()<br/>df_cleaned.pop(‘Time’)<br/>df_cleaned[‘log-amount’] = np.log(df_cleaned.pop(‘Amount’) + 0.001)</span><span id="faf8" class="od lg iq nz b gy oi of l og oh"># Double train/test split for testing and validation data.<br/>df_train, df_test = train_test_split(df_cleaned, test_size = 0.2, shuffle = True)<br/>df_train, df_valid = train_test_split(df_train, test_size = 0.2, shuffle = True)</span><span id="9c2c" class="od lg iq nz b gy oi of l og oh">print(f’Size of training data: {len(df_train)}.’)<br/>print(f’Size of validation data: {len(df_valid)}.’)<br/>print(f’Size of test data: {len(df_test)}.’)</span><span id="9da5" class="od lg iq nz b gy oi of l og oh"># Extract labels and features from data.<br/>labels_train = np.array(df_train.pop(‘Class’))<br/>labels_valid = np.array(df_valid.pop(‘Class’))<br/>labels_test = np.array(df_test.pop(‘Class’))<br/>features_train = np.array(df_train)<br/>features_valid = np.array(df_valid)<br/>features_test = np.array(df_test)</span><span id="c3e1" class="od lg iq nz b gy oi of l og oh"># Normalize data.<br/>scaler = StandardScaler()<br/>features_train = scaler.fit_transform(features_train)<br/>features_valid = scaler.transform(features_valid)<br/>features_test = scaler.transform(features_test)</span><span id="7bcc" class="od lg iq nz b gy oi of l og oh"># Enforce lower/upper bounds.<br/>features_train = np.clip(features_train, -5, 5)<br/>features_valid = np.clip(features_valid, -5, 5)<br/>features_test = np.clip(features_test, -5, 5)<br/>n_features = features_train.shape[-1]</span></pre><p id="dfa2" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">我们可以看到数据集实际上是极不平衡的，阳性样本只占预期总数的<em class="nk"> 0.17% </em>。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/03384568560224e7ef5c1c1aeeaf08d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:766/format:webp/1*tPgJIWq7hbfanlrHOtyxAQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">样本大小—图片由作者提供</p></figure><p id="e7f5" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">使用<strong class="lz ja"> Keras </strong>，我们建立了一个简单的模型，并使用二进制交叉熵作为损失函数来训练它。这是我们的<strong class="lz ja">基线模型</strong>。然后我们采用<strong class="lz ja">焦损</strong>函数来代替，并比较所获得的性能。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/c96da927660c4a51e42093fbbcf4f0a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:974/format:webp/1*MgeYTLBRg37dpUvbwEnIHw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">Keras logo —来源:<a class="ae le" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank"> keras.io主页</a></p></figure><p id="78b3" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">作为评估指标，我们认为<em class="nk">真阳性</em>、<em class="nk">假阴性</em>和<em class="nk">回忆</em>指标、<em class="nk"> </em>是因为我们对衡量模型预测欺诈的能力感兴趣。</p><p id="1f11" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">事实上，在这种情况下，使用准确性并不是一个正确的选择:一个总是预测<em class="nk">假</em>(即没有欺诈)的“模型”将在这个数据集上获得超过99.8%的准确性。这就是为什么很难在如此不平衡的数据上训练一个模型。</p><pre class="kp kq kr ks gt ny nz oa ob aw oc bi"><span id="8fe5" class="od lg iq nz b gy oe of l og oh">#<br/># MODEL TRAINING<br/>#</span><span id="ff88" class="od lg iq nz b gy oi of l og oh"># Model parameters.<br/>opt = Adam(learning_rate = 1e-3)</span><span id="e671" class="od lg iq nz b gy oi of l og oh">metrics = [<br/>    TruePositives(name = 'tp'),<br/>    FalseNegatives(name = 'fn'),<br/>    Recall(name = 'recall')<br/>]</span><span id="a537" class="od lg iq nz b gy oi of l og oh">losses = [<br/>    BinaryCrossentropy(),<br/>    SigmoidFocalCrossEntropy(gamma = 2, alpha = 4)<br/>]</span><span id="0617" class="od lg iq nz b gy oi of l og oh">loss_names = [<br/>    'binary cross-entropy',<br/>    'focal loss'<br/>]</span><span id="fed7" class="od lg iq nz b gy oi of l og oh">logs_loss = []<br/>logs_recall = []</span><span id="4578" class="od lg iq nz b gy oi of l og oh">for loss in losses:</span><span id="6c5d" class="od lg iq nz b gy oi of l og oh">    # Setup/compile the model.<br/>    model = Sequential()<br/>    model.add(Dense(16, input_dim = n_features, activation = 'relu',<br/>        kernel_initializer = 'he_uniform'))<br/>    model.add(Dropout(0.5))<br/>    model.add(Dense(1, activation = 'sigmoid'))<br/>    model.compile(optimizer = opt, loss = loss, metrics = metrics)</span><span id="0632" class="od lg iq nz b gy oi of l og oh">    # Fit the model.<br/>    logs = model.fit(features_train, labels_train, validation_data = (features_valid,<br/>        labels_valid), epochs = EPOCHS, verbose = 0)</span><span id="5a98" class="od lg iq nz b gy oi of l og oh">    logs_loss.append(logs.history['loss'])<br/>    logs_recall.append(logs.history['recall'])</span><span id="3dc7" class="od lg iq nz b gy oi of l og oh">    # Evaluate the model.<br/>    eval_train = model.evaluate(features_train, labels_train, verbose = 0)<br/>    eval_test = model.evaluate(features_valid, labels_valid, verbose = 0)</span><span id="613f" class="od lg iq nz b gy oi of l og oh">table = PrettyTable()<br/>    table.field_names = ['Data', 'Loss', 'TruePositives', 'FalseNegatives', 'Recall']</span><span id="23e7" class="od lg iq nz b gy oi of l og oh">    for stage, eval_info in zip(('training', 'test'), (eval_train, eval_test)):<br/>        row = [stage]<br/>        for ii, lbl in enumerate(model.metrics_names):<br/>            row.append(f'{eval_info[ii]:.3f}')<br/>        table.add_row(row)</span><span id="5d26" class="od lg iq nz b gy oi of l og oh">    print('\n')<br/>    print(table)</span></pre><p id="f655" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">在训练和验证了这两种配置之后，我们可以比较所获得的结果。从基线模型中，我们得到:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/f7455b2e604bd668b09fc031c596a013.png" data-original-src="https://miro.medium.com/v2/resize:fit:1030/format:webp/1*zZXmreZY5ZORvJBWyW1ouw.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">二值交叉熵分类—按作者分类的图像</p></figure><p id="f2e5" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">相反，使用焦点损失时，分类性能会显著提高，并且所有欺诈案例都会被正确检测到:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi om"><img src="../Images/03620af82fe74a2381746da853a3a598.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*0ef7L7osrWGBE5Qa2uEDHQ.png"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">焦点损失分类—按作者分类的图像</p></figure><p id="103d" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">让我们也比较两个损失函数的行为。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/b54e28a73200415715369f49294e6c4e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gJBPaGayWRJq942jdW4a-w.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">损失之间的比较—按作者分类的图像</p></figure><p id="f3ec" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">正如所料，焦点损失值低于交叉熵值。焦点损失会降低被错误分类的阳性样本(欺诈)的权重，从而“鼓励”模型提高对欺诈案例的敏感度。</p><h1 id="9fd8" class="lf lg iq bd lh li lj lk ll lm ln lo lp kf lq kg lr ki ls kj lt kl lu km lv lw bi translated">参考资料:</h1><p id="be11" class="pw-post-body-paragraph lx ly iq lz b ma mb ka mc md me kd mf mg mh mi mj mk ml mm mn mo mp mq mr ms ij bi translated">[1]宗-林逸、普里亚·戈亚尔等人，<a class="ae le" href="https://arxiv.org/abs/1708.02002" rel="noopener ugc nofollow" target="_blank">密集物体探测的焦损失</a></p><p id="3454" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">[2] Hichame Yessou等，<a class="ae le" href="https://arxiv.org/abs/2009.13935" rel="noopener ugc nofollow" target="_blank">多标签遥感影像分类的深度学习损失函数比较研究</a></p><p id="785e" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">[3] <a class="ae le" href="https://www.dlology.com/blog/multi-class-classification-with-focal-loss-for-imbalanced-datasets" rel="noopener ugc nofollow" target="_blank">针对不平衡数据集的具有焦点损失的多类分类</a></p><p id="d6d9" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">[4] <a class="ae le" href="https://www.tensorflow.org/tutorials/structured_data/imbalanced_data" rel="noopener ugc nofollow" target="_blank">不平衡数据的分类</a></p><p id="405a" class="pw-post-body-paragraph lx ly iq lz b ma nf ka mc md ng kd mf mg nh mi mj mk ni mm mn mo nj mq mr ms ij bi translated">[5] <a class="ae le" href="https://www.kaggle.com/mlg-ulb/creditcardfraud" rel="noopener ugc nofollow" target="_blank">关于信用卡欺诈检测的Kaggle数据集</a></p></div></div>    
</body>
</html>