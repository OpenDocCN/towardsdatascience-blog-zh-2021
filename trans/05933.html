<html>
<head>
<title>Piano Instrumental Music Generation using Deep Neural Networks</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">基于深度神经网络的钢琴器乐生成</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pyzzicato-piano-instrumental-music-generation-using-deep-neural-networks-ed9e89320bf6?source=collection_archive---------23-----------------------#2021-05-27">https://towardsdatascience.com/pyzzicato-piano-instrumental-music-generation-using-deep-neural-networks-ed9e89320bf6?source=collection_archive---------23-----------------------#2021-05-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9615" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用扩张卷积或LSTM网络构建一个有趣的制作钢琴器乐的网络应用程序</h2></div><blockquote class="kf kg kh"><p id="6ed3" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">作者:<a class="ae lf" href="https://www.linkedin.com/in/philippe-petit-64896960/" rel="noopener ugc nofollow" target="_blank">菲利普·帕特</a>、<a class="ae lf" href="https://www.linkedin.com/in/gatien-vassor-a5024040/" rel="noopener ugc nofollow" target="_blank">加廷·瓦瑟</a>、<a class="ae lf" href="https://www.linkedin.com/in/emna-bairam-42415852/" rel="noopener ugc nofollow" target="_blank">埃姆纳·拜拉姆</a>和弗拉基米尔·雷蒙——2021年4月</p></blockquote><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi lg"><img src="../Images/def7b18f1d9571ecb4e9076424411bae.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/0*7l-WCUu6QOtZo2MO"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">图片通过CC0许可下的<a class="ae lf" href="https://www.hippopx.com/" rel="noopener ugc nofollow" target="_blank">hippopx.com</a></p></figure><h1 id="5c17" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">1.语境</h1><blockquote class="kf kg kh"><p id="ddd1" class="ki kj kk kl b km kn jr ko kp kq ju kr ks kt ku kv kw kx ky kz la lb lc ld le ij bi translated">Pyzzicato项目是我们为期三个月的数据科学家训练营的最后一部分。</p></blockquote><p id="d624" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">该项目旨在构建一个web应用程序，允许用户根据他们选择的作曲家的风格生成新颖的音乐曲目(使用streamlit)。基本上，这些模型通过使用一套深度学习技术和midi文件数据集，自动学习音乐风格，并基于现有的作品生成新的音乐内容。</p><p id="f8dd" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">在深度学习算法领域，生成模型有一个特殊的目标:它们旨在创建<em class="kk">原始</em>输出，这意味着<em class="kk">“不是一个精确的副本”</em>它们已经学会了复制。从这个意义上说，输出必须是原创的，但不能<em class="kk">过分原创</em>:评委(这里是:我们人类)必须至少在生成的输出中识别出模型训练输入数据的<em class="kk">主要特征</em>。</p><p id="2ff0" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">一言以蔽之:好的生成模型是其输入数据集的“精神孩子”，但不是专业的造假者。完美再现其输入数据的生成模型未能完成其使命，因为生成的内容<em class="kk">已经存在</em>。</p><p id="bee8" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">著名的网站<em class="kk">thispersondoesnotexist.com</em>是这个定义的一个极好的例证，因为它产生了普通人类的图像——而不是现存的人类。</p><p id="db05" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">在音乐生成的背景下，生成模型的一个潜在目标是学习模仿他们已经得到的音乐数据的“风格”。所产生的音乐输出的听众必须感觉它可能是由人类作曲家写的，并且识别一种风格或音乐流派。</p><h1 id="2085" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">2.该项目</h1><h1 id="5f2a" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">数据集和数据探索</h1><p id="2c4b" class="pw-post-body-paragraph ki kj iq kl b km mn jr ko kp mo ju kr mk mp ku kv ml mq ky kz mm mr lc ld le ij bi translated">即使对人类来说，创作音乐也不是一件容易的事情，音乐曲目是一个非常复杂的对象。为了简化模型化，我们选择生成单乐器音乐，因此选择了包含单乐器钢琴演奏记录的数据集。我们使用了作为<a class="ae lf" href="https://magenta.tensorflow.org/" rel="noopener ugc nofollow" target="_blank"> Magenta项目</a>的一部分而创建的<a class="ae lf" href="https://magenta.tensorflow.org/datasets/maestro" rel="noopener ugc nofollow" target="_blank"> MAESTRO </a>数据集(为同步轨道和组织而编辑的MIDI和音频)。</p><p id="5f18" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">该数据集由1276首曲目组成，涵盖了17世纪至20世纪初大多数古典音乐作曲家的200多个小时的钢琴录音。这个数据集相当大，我们必须在合理的时间内选择子数据集来执行深度学习模型的训练。</p><p id="0cb3" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">图1示出了(I)每个作曲家在数据集中演奏和收集的作品的总数，以及(ii)数据集的14个主要作曲家的每个作曲家的独特作品的数量。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi ms"><img src="../Images/d2b4c13fbddefae133fc49fa289e2ac5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*r4KxQk-0pAkK85RH"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated"><em class="mx">图1:每位作曲家的独特曲目和总曲目数(图片由作者提供)</em></p></figure><p id="c381" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">我们已经决定选择表现出非常独特的作曲风格的作曲家，这些作曲家的独特作品的总数足够大，可以在没有太多冗余输入的情况下训练模型。我们还选择了普通观众熟知的作曲家，以方便对结果的评估。基于此，我们选择了三位作曲家——肖邦、巴赫和德彪西——来训练我们的生成模型。</p><h1 id="f5d8" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">数据可视化和预处理</h1><p id="7f57" class="pw-post-body-paragraph ki kj iq kl b km mn jr ko kp mo ju kr mk mp ku kv ml mq ky kz mm mr lc ld le ij bi translated">用python可以很容易地操作MIDI文件，例如使用Colin RAFFEL创建的<a class="ae lf" href="https://craffel.github.io/pretty-midi/" rel="noopener ugc nofollow" target="_blank"> Pretty_MIDI包</a>。[1]</p><p id="46bb" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">pretty_midi对象的类将midi文件的音乐数据编码为音符形式的音符事件列表(start=0.000，end=0.124，pitch=76，velocity=80)。在本例中，音高编号为76的音符在t=0和t=0.124秒之间弹奏，力度(表示弹奏音符的力度)等于80。音高和力度编码在0到127之间。</p><p id="c980" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">这种音乐轨迹的表示不太适合深度学习建模。因此，为了对数据进行编码，<strong class="kl ir">我们使用了一种基于矩阵的方法</strong>，该方法使用了音乐曲目的钢琴卷帘窗表示。</p><p id="f86f" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">钢琴卷帘窗表示将pretty-MIDI对象的音符事件列表转换为[Pitches vs Timestep] 2d矩阵(参见图2)。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi my"><img src="../Images/f1f0a917eebc2016ec98f05db215d45e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1172/0*ndI4_oCh0f8XSECN"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated"><em class="mx">图2:用于生成的输出的数据可视化和视觉评估的音乐轨道(巴赫D小调半音阶幻想曲)的钢琴卷首表示。(图片由作者提供)</em></p></figure><p id="e6b4" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">下面是我们使用Librosa和Pretty-MIDI(在streamlit web应用程序中)绘制钢琴卷帘窗矩阵的代码:</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="19bd" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">钢琴卷帘窗矩阵可以被视为维度为128(MIDI标准中不同音高的最大数量)的音高向量的串联，这些音高向量在表示时间流动的每个不同时间步长(这里是矩阵的一列)被连续播放。</p><p id="6bf4" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">时间步长的持续时间由采样频率fs决定，时间步长= 1/fs。fs=20时，钢琴卷帘窗矩阵的每一列代表1/20秒的音乐。</p><p id="05f9" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">使用这种表示，我们可以定义要给予深度学习模型的特征和目标(参见图3):</p><p id="afe4" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">特征将是长度为n_feat的音调向量序列</p><p id="833c" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">-目标将是n_feat + 1个音调向量，模型将基于特征序列学习预测该向量</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi nb"><img src="../Images/54b624fe49dbe9346cfbccf6d0c45bfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0CWIo0WpPEysuoxf"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated"><em class="mx">图3:左图:音乐输入被编码为特征(基音向量序列)和相应的目标(下一个基音向量)。Nb_note = 88(而不是标准MIDI编码中的128)，因为钢琴板只包含88个键。<br/>右:将输入的MIDI文件分割成特征序列(音高向量序列)和目标序列(后续音高向量)。(图片由作者提供)</em></p></figure><h1 id="14a6" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">3.深度学习模型</h1><p id="ce9b" class="pw-post-body-paragraph ki kj iq kl b km mn jr ko kp mo ju kr mk mp ku kv ml mq ky kz mm mr lc ld le ij bi translated">音乐曲目基本上是音符的时间序列。与噪音相反，音乐曲目中的音符不是随机播放的，而是通过和声、旋律等概念彼此紧密相连的。因此，我们用来生成音乐的深度学习模型必须能够学习<strong class="kl ir">音乐“上下文】</strong> <em class="kk">即</em>来跟踪被相对大量的时间分开的音符。然后，我们使用了两种已知在这项任务中有效的深度学习架构:<strong class="kl ir">扩张卷积网络</strong>和<strong class="kl ir">递归神经网络。</strong></p><p id="b21b" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">我们建立的深度学习模型还必须能够预测包含<strong class="kl ir">零音符(静音)</strong>、<strong class="kl ir">或</strong>、<strong class="kl ir">几个音符(一个<em class="kk">和弦</em>)、</strong>的音高向量，这是音乐曲目中非常常见和重要的特征。为了在我们的建模中以简单的方式允许它，我们在所有模型的输出层中使用了<strong class="kl ir"> sigmoid激活</strong>函数。</p><h1 id="1c0c" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">扩张卷积模型</h1><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/30e90931aeb263372a4b25ad9af9179e.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/0*-EqzJj3aN3rYR9C9"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">【图4】核大小= 3，膨胀大小= 1，2，4的膨胀因果卷积1D的例子(来源:<a class="ae lf" href="https://arxiv.org/pdf/1803.01271.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1803.01271.pdf</a>)</p></figure><p id="6df9" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">图4给出了扩展卷积的原理。扩张卷积模型的主要优点是，通过适当选择层和超参数，它可以在合理的时间内产生像样的结果，可以用作比较的起点。我们首先设计了一个具有几层的简单卷积模型(在Pyzzicato应用程序中称为Conv1D，但不在此讨论)，由于获得了相当好的结果，我们受著名的WaveNet架构(我们称之为“伪wavenet”)的启发，构建了一个更复杂的卷积模型。该模型由五个卷积1D块(膨胀率在2和16之间)和一个分类块组成，如图5所示。注意最后一层的<strong class="kl ir"> sigmoid激活函数</strong>允许在单个音高向量中生成多个一键编码音符事件。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi nd"><img src="../Images/24af1a60c63bbb737f8f60c53b33be23.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*nbtczH1kY1TkETYK"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated"><em class="mx">图5:伪波网架构(图片由作者提供)</em></p></figure><p id="0a02" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">请在此处查看代码:</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="mz na l"/></div></figure><h1 id="cc83" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">RNN (LSTM)模型</h1><p id="1d6d" class="pw-post-body-paragraph ki kj iq kl b km mn jr ko kp mo ju kr mk mp ku kv ml mq ky kz mm mr lc ld le ij bi translated">LSTM是时间序列预测的有效模型，特别适合我们的目的。这是因为它们有选择性地短时间或长时间记忆模式的特性。</p><p id="a25b" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">普通的LSTM单元由一个单元、一个输入门、一个输出门和一个遗忘门组成。该单元记忆任意时间间隔的值，并且三个门按照图6中描述的策略来调节进出该单元的信息流:</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/e10aa6592c52a3264592823e8f7b250d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1284/0*gOn0mxzxoUapCVKi"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">图6:LSTM电池原理(datascientest.com<a class="ae lf" href="https://datascientest.com/blog-data-ia-actualites" rel="noopener ugc nofollow" target="_blank">)</a></p></figure><p id="c08a" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">我们的LSTM模型是一个序列模型，具有2个LSTM层和3个密集层，如图7所示。</p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi nd"><img src="../Images/c3a808e287fc91c663cb74d072d646f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*T0Zxx7bKPfoR8FJH"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">图Pyzzicato中使用的(原始)LSTM模型的架构(图片由作者提供)</p></figure><p id="bea5" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">请在此处查看代码:</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="mz na l"/></div></figure><p id="a28f" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">由于Google Colab的免费GPU功能，模型得到了训练。</p><h1 id="dff3" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">我们如何生成音乐曲目？</h1><p id="e448" class="pw-post-body-paragraph ki kj iq kl b km mn jr ko kp mo ju kr mk mp ku kv ml mq ky kz mm mr lc ld le ij bi translated">为了生成音轨，我们将输入序列(称为“种子”)提供给一个经过训练的模型，并要求它依次预测下一个音高向量，以便模型逐步生成一个新的音乐音轨。还可以调整一些参数，如生成轨道的总时间长度，或sigmoid输出层的阈值。</p><p id="1672" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">下面是我们使用的函数:</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="mz na l"/></div></figure><h1 id="d53f" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">4.结果</h1><h1 id="f735" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">生成模型的分析原理</h1><p id="553c" class="pw-post-body-paragraph ki kj iq kl b km mn jr ko kp mo ju kr mk mp ku kv ml mq ky kz mm mr lc ld le ij bi translated">评估生成模型的性能是一个<strong class="kl ir">未解决的问题</strong>，并且面对创造力的基本概念。总的来说，我们缺乏量化指标来评估产出的质量。的确，当我们听一首由生成模型产生的音乐曲目时，我们如何判断它的质量呢？</p><p id="895f" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">这个问题最常见的答案是主观评估法，基于以下标准:</p><p id="1155" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">- 1/ <strong class="kl ir">音乐性:</strong>音符和和弦的连续是否类似于普遍接受的“音乐”？(<em class="kk">即</em>我们能在其中分辨出一个“旋律”吗？它“听起来像”一首人类创作的音乐吗？)</p><p id="9eb5" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">- 2/ <strong class="kl ir">保真度:</strong>这首音乐是否包含一些特征模式？它属于预期的“音乐流派”(古典音乐、说唱、摇滚等)吗？</p><p id="d3ab" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">- 3/ <strong class="kl ir">表现力:</strong>一个非常主观的标准，对应的问题是“这首音乐是否在听者的头脑中实例化了“情绪？”</p><p id="0d7c" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">尽管评估这个项目成果的最好方法是听音轨(使用pyzzicato应用程序！)，则<strong class="kl ir"> piano_roll </strong>表示允许对前两个定性标准<strong class="kl ir"> <em class="kk">音乐性</em> </strong>和<strong class="kl ir"> <em class="kk">保真度</em> </strong>进行视觉估计。</p><h1 id="946e" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结果概述:</h1><p id="68c4" class="pw-post-body-paragraph ki kj iq kl b km mn jr ko kp mo ju kr mk mp ku kv ml mq ky kz mm mr lc ld le ij bi translated">伪波网和朴素LSTM模型都能够在音乐性和保真度方面产生良好的结果，尽管在这两种模型之间可以发现一些微妙的行为差异。以下是一些生动的例子:</p><p id="1707" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated"><strong class="kl ir">例1: </strong></p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi nf"><img src="../Images/100380b93f6e8f0923cf10e8ad6e0cbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*YWQFZ7E_-jd-CWjS"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">图8:使用从巴赫半音阶幻想曲和D小调赋格中提取的种子序列的输出。(图片由作者提供)</p></figure><p id="3027" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">第一个例子使用10秒钟的巴赫半音阶幻想曲作为种子，说明了我们的模型生成可听音乐的能力:音符间隔相当规则，音高不同，与种子中看到的音符在同一范围内。半音阶的上升和/或下降唤起了种子中所看到的，在两个音轨中都可以看到。然而，伪波网似乎表现出更好的保真度，因为音符持续时间更接近于种子轨道中看到的音符持续时间。</p><p id="2aa9" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">单击此处收听种子序列:</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="ng na l"/></div></figure><p id="b88e" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">以及使用伪波网模型生成的轨迹:</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="ng na l"/></div></figure><p id="849c" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated"><strong class="kl ir">例二:</strong></p><figure class="lh li lj lk gt ll gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi nf"><img src="../Images/5db7b1cdfc05ee775698466837369cb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*LMPWFPw-KR4PInvM"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">图9:使用从肖邦G小调第一叙事曲中提取的种子序列的输出，作品23。<em class="mx">(作者图片)</em></p></figure><p id="ca1a" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">第二个例子使用肖邦的叙事曲n 1的10秒钟作为种子，说明了这样一个事实，即与伪波网相比，天真LSTM更容易适应较慢的节奏和更持续的音符。尽管这两种模式在旋律方面产生了非常相似的输出，但LSTM设法再现了种子开始时可见的长时间持续的音符，而伪波网产生了一种沉默(在音轨的6s和7s之间)。</p><p id="d960" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">单击此处收听种子序列:</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="ng na l"/></div></figure><p id="2c93" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">以及使用伪波网模型生成的轨迹:</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="ng na l"/></div></figure><p id="1ea6" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated"><strong class="kl ir">例3: </strong></p><p id="0b66" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">再来听一个例子，用的是从肖邦的玛祖卡舞曲(作品17)中摘录的一个种子:</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="ng na l"/></div></figure><p id="4cd4" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">最有说服力的轨迹是由天真-LSTM模型生成的:</p><figure class="lh li lj lk gt ll"><div class="bz fp l di"><div class="ng na l"/></div></figure><h1 id="e30d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">限制</h1><p id="dceb" class="pw-post-body-paragraph ki kj iq kl b km mn jr ko kp mo ju kr mk mp ku kv ml mq ky kz mm mr lc ld le ij bi translated">虽然相当令人满意，但上述生成轨迹的示例也凸显了我们的模型的主要局限性:</p><ul class=""><li id="55d6" class="nh ni iq kl b km kn kp kq mk nj ml nk mm nl le nm nn no np bi translated">不太好的概括:输出强烈依赖于选择的种子，有时模型产生很少的音符，甚至完全无声，或有噪音的输出</li><li id="153b" class="nh ni iq kl b km nq kp nr mk ns ml nt mm nu le nm nn no np bi translated">生成长音轨的困难:当我们要求我们的模型生成超过训练序列长度(这里是10s)的音轨时，我们的模型有困难。随着时间的推移，它们有“干涸”的趋势，产生越来越少的音符。</li></ul><h1 id="9cf0" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">5.使用案例和潜在改进</h1><h1 id="d151" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">潜在的使用案例</h1><p id="abb0" class="pw-post-body-paragraph ki kj iq kl b km mn jr ko kp mo ju kr mk mp ku kv ml mq ky kz mm mr lc ld le ij bi translated">属于领域或者创作，为这个项目提出现实的用例不是一件容易的事情。然而，我们可以想象这项工作用于:</p><ul class=""><li id="639f" class="nh ni iq kl b km kn kp kq mk nj ml nk mm nl le nm nn no np bi translated">为呼叫中心或管理部门自动生成等待/等待音乐</li><li id="c47d" class="nh ni iq kl b km nq kp nr mk ns ml nt mm nu le nm nn no np bi translated">音乐制作人/作曲家的旋律建议工具</li></ul><h1 id="3a43" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">潜在的改进</h1><ul class=""><li id="9026" class="nh ni iq kl b km mn kp mo mk nv ml nw mm nx le nm nn no np bi translated">标准化MIDI数据集，以控制种子序列在速度、音高范围等方面的可变性。</li><li id="1b0d" class="nh ni iq kl b km nq kp nr mk ns ml nt mm nu le nm nn no np bi translated">使用最先进的生成对抗模型(gan)</li><li id="373a" class="nh ni iq kl b km nq kp nr mk ns ml nt mm nu le nm nn no np bi translated">通过使用生成器改进训练数据管理，以增加我们的训练数据集的大小(基于矩阵的方法在内存需求方面相当贪婪)</li><li id="de92" class="nh ni iq kl b km nq kp nr mk ns ml nt mm nu le nm nn no np bi translated">使用另一种数据编码模式，如NLP启发的方法，而不是一次性编码的钢琴卷帘窗矩阵</li><li id="125e" class="nh ni iq kl b km nq kp nr mk ns ml nt mm nu le nm nn no np bi translated">实施结果评估的量化指标。我们的方法主要基于主观评价，但文献中的一些参考文献提出了有趣的统计方法[2]。</li></ul><h1 id="b347" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">6.概括起来</h1><p id="5d48" class="pw-post-body-paragraph ki kj iq kl b km mn jr ko kp mo ju kr mk mp ku kv ml mq ky kz mm mr lc ld le ij bi translated">作为结论，我们首先要强调的是，尽管我们的三个模型化架构相对简单，但这里呈现的结果非常令人鼓舞。</p><p id="cd5a" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">在这个项目中，我们设法使用三种不同的生成模型制作音乐，从处理顺序数据的卷积模型到更高级的递归神经网络模型，这些模型考虑了数据结构中更长的路径依赖，如RNN。</p><p id="8915" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">我们已经观察到种子和相应输出之间的真实音乐相干性，特别是对于伪波网和LSTM模型。</p><p id="3010" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">正如文章中所讨论的，更大的数据集肯定会改善我们的结果，首先是通过减少对训练数据的过度拟合。实现量化指标来评估生成轨迹的质量也是非常有趣的。由于时间不够，我们的分析仅限于主观评价方法。</p><p id="fee1" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">使用人工智能深度学习模型的艺术创作是一个非常年轻的研究领域，未来将取得巨大成功。我们很高兴也很自豪有机会一睹这些创新算法的成果！</p><p id="9145" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">对题目感兴趣？以下是令人惊叹的项目的非详尽列表:</p><ul class=""><li id="dec7" class="nh ni iq kl b km kn kp kq mk nj ml nk mm nl le nm nn no np bi translated">谷歌出品的洋红色</li><li id="6ec5" class="nh ni iq kl b km nq kp nr mk ns ml nt mm nu le nm nn no np bi translated">OpenAI的MuseNet</li><li id="b2da" class="nh ni iq kl b km nq kp nr mk ns ml nt mm nu le nm nn no np bi translated"><a class="ae lf" href="https://www.flow-machines.com" rel="noopener ugc nofollow" target="_blank">索尼的流动机器</a></li><li id="91d9" class="nh ni iq kl b km nq kp nr mk ns ml nt mm nu le nm nn no np bi translated"><a class="ae lf" href="https://www.aiva.ai/" rel="noopener ugc nofollow" target="_blank"> AIVA </a>启动</li></ul><p id="c25b" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">还有<a class="ae lf" href="https://medium.com/the-research-nest/ai-that-can-generate-music-fddc5813376a" rel="noopener">更多更多</a>！</p><h1 id="4d49" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">7.下一步是什么？</h1><p id="5a51" class="pw-post-body-paragraph ki kj iq kl b km mn jr ko kp mo ju kr mk mp ku kv ml mq ky kz mm mr lc ld le ij bi translated">首先，我们希望通过向读者提供一种生成他们自己的音乐曲目并评估结果的方式，来扩展这里呈现的结果的主观评估。多亏了我们开发的Pyzzicato网络应用程序，这才成为可能。它可以在datascientest studio上获得！</p><p id="0f06" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">其次，我们对与任何对设计新型深度学习生成模型感兴趣的人合作持开放态度。用于预处理和模型训练的代码可以在github上找到，请随意查看！</p><p id="5a58" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">在这里享受streamlit应用<a class="ae lf" href="https://studio.datascientest.com/project/pyzzicato/" rel="noopener ugc nofollow" target="_blank">带来的乐趣！</a></p><p id="0e37" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated"><em class="kk">非常感谢</em><a class="ae lf" href="https://www.linkedin.com/in/juliette-voyez-81a24583/" rel="noopener ugc nofollow" target="_blank"><em class="kk">Juliette Voyez</em></a><em class="kk">在整个冒险过程中对她的帮助！</em></p><p id="316a" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">[1]科林·拉弗尔和丹尼尔·埃利斯。<a class="ae lf" href="http://colinraffel.com/publications/ismir2014intuitive.pdf" rel="noopener ugc nofollow" target="_blank">使用pretty_midi </a>对MIDI数据进行直观的分析、创建和操作。在2014年第15届音乐信息检索国际会议上的最新突破和演示论文。</p><p id="0b1e" class="pw-post-body-paragraph ki kj iq kl b km kn jr ko kp kq ju kr mk kt ku kv ml kx ky kz mm lb lc ld le ij bi translated">[2]参见杨等<em class="kk">。，对音乐中的生成模型的评估</em>，神经计算和应用，2020年5月，以及其中的参考文献以了解更多细节。</p></div></div>    
</body>
</html>