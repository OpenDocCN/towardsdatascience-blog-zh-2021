<html>
<head>
<title>One common misconception about Random Forest and overfitting</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">关于随机森林和过度适应的一个常见误解</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/one-common-misconception-about-random-forest-and-overfitting-47cae2e2c23b?source=collection_archive---------2-----------------------#2021-03-27">https://towardsdatascience.com/one-common-misconception-about-random-forest-and-overfitting-47cae2e2c23b?source=collection_archive---------2-----------------------#2021-03-27</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="5ebf" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">自举、多数投票规则和100%训练准确率的悖论</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/b44d7781ac076a757182c8f910be8817.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*l9I3VWgvPft8T_rU"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">照片由<a class="ae kv" href="https://unsplash.com/@robertbye?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Robert Bye </a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><p id="dfd2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">100%的训练准确率是否表明过度拟合？有许多调整随机森林中树木深度的建议来防止这种情况发生:见<a class="ae kv" rel="noopener" target="_blank" href="/optimizing-hyperparameters-in-random-forest-classification-ec7741f9d3f6">这里</a>或<a class="ae kv" href="https://medium.com/all-things-ai/in-depth-parameter-tuning-for-random-forest-d67bb7e920d" rel="noopener">这里</a>。这个建议是错误的。这篇文章解释了为什么随机森林的100%训练准确率与过度适应无关。</p><p id="672c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种混乱源于将过度拟合作为一种现象与其指标相混淆。过度拟合的一个简单定义是，当一个模型在我们关心的数据上不再像我们希望的那样准确时。而且我们很少关心训练数据。然而，训练数据上的满分通常是一个很好的指标，表明我们将面临新数据上令人失望的性能下降。通常，但不总是。要理解为什么应该看看随机森林实际上是如何工作的。</p><p id="d125" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">简而言之，随机森林包括1)完全生长的树，2)建立在引导数据上，3)和多数投票规则来进行预测。Leo Breiman和Adele Cutler对前两个概念进行了简洁的解释[1]:</p><blockquote class="ls lt lu"><p id="8d6f" class="kw kx lv ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">如果训练集中的案例数为N，则从原始数据中随机抽取N个案例——但使用替换的<em class="iq">。这个样本将成为种植这棵树的训练集。</em></p><p id="a413" class="kw kx lv ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">每棵树都长到了最大限度。没有修剪。</p></blockquote><p id="1dc0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第三个不言自明。</p><p id="33c6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">记住这3个概念，很容易看出随机森林如何产生100%的训练准确性。考虑一个简单的二进制分类问题。如果每棵树都完全长大，就是这样——每片叶子都是纯叶子，并且每个观察值在引导过程中有62.5%的机会被采样，那么最终集合中超过一半的树“知道”该特定观察值的正确类别。根据多数投票规则，这足以在训练集上给出100%的准确度。</p><p id="7173" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们运行一个简单的模拟。定义一个二元分类问题，并据此模拟一些数据。然后将数据分成两部分:训练和测试。在测试部分，通过向一些特性添加噪声来模拟另一个测试集。在训练集上训练完全成长的简单决策树和随机森林，并对两个测试集进行预测。然后，逐渐减少深度，重复程序。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="lz ma l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mb"><img src="../Images/64b262cbc14daaa96e54ca2619ea4ca7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*g3xIq9KCAXIyjYQxmwk8Ww.png"/></div></div></figure><p id="aa17" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如您所观察到的，更深的决策树倾向于过度拟合数据:在达到最大可能深度的大约35%之后，带有噪声的测试集的准确性下降。兰登森林不会发生这种事。</p><h1 id="771a" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">结论</h1><ol class=""><li id="c587" class="mu mv iq ky b kz mw lc mx lf my lj mz ln na lr nb nc nd ne bi translated">训练数据的100%准确性不一定是问题</li><li id="65d1" class="mu mv iq ky b kz nf lc ng lf nh lj ni ln nj lr nb nc nd ne bi translated">减少随机森林中的最大深度可以节省时间。从网格搜索中排除最大深度可以节省更多时间</li></ol><h1 id="149a" class="mc md iq bd me mf mg mh mi mj mk ml mm jw mn jx mo jz mp ka mq kc mr kd ms mt bi translated">参考</h1><p id="11f3" class="pw-post-body-paragraph kw kx iq ky b kz mw jr lb lc mx ju le lf nk lh li lj nl ll lm ln nm lp lq lr ij bi translated">[1] L. Breiman和A. Cutler，<a class="ae kv" href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm" rel="noopener ugc nofollow" target="_blank">随机森林</a></p></div></div>    
</body>
</html>