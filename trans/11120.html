<html>
<head>
<title>Creating deep neural networks with 3 to 5 lines of code</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用3到5行代码创建深度神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/creating-deep-neural-networks-with-3-to-5-lines-of-code-baa83fa616ed?source=collection_archive---------8-----------------------#2021-10-30">https://towardsdatascience.com/creating-deep-neural-networks-with-3-to-5-lines-of-code-baa83fa616ed?source=collection_archive---------8-----------------------#2021-10-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="d027" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="f29d" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">我们可以通过改变已经提出的模型的很少几行代码来创建新的深度神经网络。</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/f9770367e993f5b06d74b370bb8b0a46.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4Xl7iwMsj2mw3bOAUiv8cQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图片作者。</p></figure><p id="6ac7" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">当在深度学习中处理监督学习时，我们可以说有一些经典的方法可以遵循。第一种解决方案是所谓的“英雄”策略，其中从零开始创建一个全新的深度神经网络(DNN ),并对其进行训练/评估。实际上，这种解决方案可能不是很有趣，因为现在有无数的dnn可用，如许多深度卷积神经网络(CNN)，可以重复使用。第二条路是简单地考虑一个可部署的DNN，为某个环境训练，并在另一个环境中观察它的操作。尽管深度学习取得了诸多进步，但如果环境过于多样化，模型可能会表现不佳。</p><p id="d228" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">今天最著名的方法之一被称为<a class="ae ma" href="https://journalofbigdata.springeropen.com/articles/10.1186/s40537-016-0043-6" rel="noopener ugc nofollow" target="_blank">迁移学习</a>，它用于通过转移相关领域的信息来改进一个领域(上下文)的模型。依赖迁移学习的动机是当我们面临训练数据集中没有那么多样本的情况时。造成这种情况的一些原因是，收集和标记这些数据并不便宜，或者这些数据很少。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mb"><img src="../Images/abb82335d5e747aa6f1f2b414fea9743.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vK3PHmkxImZ0dnuzJU0SdQ.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">照片由<a class="ae ma" href="https://unsplash.com/@itfeelslikefilm" rel="noopener ugc nofollow" target="_blank">扬科·菲利</a> <strong class="bd mc"> </strong>在<a class="ae ma" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄。</p></figure><p id="e6d0" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">但是，迁移学习也有缺点。通常，模型是在大型数据集上训练的，因此这种预训练的模型可以在另一个上下文中重用。因此，我们不是从零开始训练，而是基于预训练模型中嵌入的后天“智能”。然而，即使我们有大量的图像要训练，训练数据集也必须足够通用，以处理不同的上下文。有许多有趣的基准映像集，如ImageNet和COCO，旨在解决这个问题。但是，最终，我们可能会在一个具有挑战性的领域(例如自动驾驶、遥感)工作，其中基于这些经典数据集的迁移学习可能是不够的。</p><p id="65b9" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">即使我们多次尝试增加训练样本，例如数据增加、生成对抗网络(GANs)，我们也可以通过重用和/或进行一些修改和/或组合其他已提出模型的不同特征来创建新模型。这种策略的一个最重要的例子是著名的对象检测DNNs，称为<a class="ae ma" href="https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb" rel="noopener ugc nofollow" target="_blank"> YOLO </a>。这些模型，尤其是版本4，是基于许多其他解决方案开发的。这种网络是一种不同概念的折衷混合，以获得一种新的模型，它们在检测图像和视频中的对象方面非常成功。请注意，<a class="ae ma" href="https://medium.com/@tastekinalperenn/yolox-main-idea-behind-latest-yolo-algorithm-5f8aa930c33c" rel="noopener"> YOLOX </a>是YOLO网络的最新版本。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi md"><img src="../Images/2a14652a073d37eb10a8861fe7861dd8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0YLoVx_GzOY-HRNp75IokA.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">在<a class="ae ma" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上由<a class="ae ma" href="https://unsplash.com/@amayli?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">安梅丽·莫里雄</a> <strong class="bd mc"> </strong>拍摄的照片。</p></figure><p id="76f8" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">这篇文章的方向是通过在以前提出的方法中完成一些变化来创建新的模型。它展示了通过改变先前提出的模型的很少几行代码来创建“新的”dnn是多么容易。请注意，由于我们所做的更改基本上与原始模型的层数有关，因此我们将下面介绍的网络称为“新”网络，这是对它的一点推动。但是，重点是鼓励实践者考虑这一点，并最终在实际环境中使用DNNs时重用/修改以前的想法以产生新的方法。</p><h2 id="9966" class="me mf iq bd mg mh mi dn mj mk ml dp mm ln mn mo mp lr mq mr ms lv mt mu mv iw bi translated"><strong class="ak"> VGG12BN </strong></h2><p id="7d0f" class="pw-post-body-paragraph le lf iq lg b lh mw ka lj lk mx kd lm ln my lp lq lr mz lt lu lv na lx ly lz ij bi translated">在2014年<a class="ae ma" href="https://www.image-net.org/challenges/LSVRC/" rel="noopener ugc nofollow" target="_blank"> ImageNet大规模视觉识别挑战赛</a>(ils vrc)<a class="ae ma" href="https://arxiv.org/abs/1409.1556" rel="noopener ugc nofollow" target="_blank"/>中，VGG 是一个经典的DNN，今天仍有一些用途，尽管一些作者认为它是一个过时的网络。在最初的文章中，作者提出了具有11、13、16和19层的vgg。在这里，我们展示了如何创建一个“新的”VGG组成的12层和批量规格化(BN)只是通过增加/改变5行代码:VGG12BN。</p><p id="aa1c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们依赖PyTorch团队的实现，并修改它来创建我们的模型。VGG修改后的代码是<a class="ae ma" href="https://github.com/vsantjr/DeepLearningMadeEasy/blob/temp_23-09/vggmodified.py" rel="noopener ugc nofollow" target="_blank">这里是</a>，使用它的笔记本是<a class="ae ma" href="https://github.com/vsantjr/DeepLearningMadeEasy/blob/temp_23-09/PyTorch_VGG_ResNet_VGGout.ipynb" rel="noopener ugc nofollow" target="_blank">这里是</a>。此外，我们考虑了fastai的<a class="ae ma" href="https://github.com/fastai/imagenette" rel="noopener ugc nofollow" target="_blank"> imagenette 320 px </a>数据集的一个稍微修改的版本。不同之处在于，原始验证数据集被分成两部分:包含原始验证集的1/3图像的验证数据集，以及组成测试数据集的2/3图像。因此，在训练中有9，469幅图像(70.7%)，在验证中有1，309幅图像(9.78%)，在测试集中有2，616幅图像(19.53%)。这是一个有10个类别的多类别分类问题。我们将数据集称为<a class="ae ma" href="https://www.kaggle.com/valdivinosantiago/imagenettetvt320" rel="noopener ugc nofollow" target="_blank"> imagenettetvt320 </a>。</p><p id="7e56" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">我们现在展示我们在PyTorch团队的VGG实现中所做的修改:</p><ul class=""><li id="a70f" class="nb nc iq lg b lh li lk ll ln nd lr ne lv nf lz ng nh ni nj bi translated">首先，我们注释掉了<code class="fe nk nl nm nn b">from .._internally_replaced_utils import load_state_dict_from_url</code>以避免依赖其他PyTorch模块。此外，为了完全确定我们不会使用预训练模型(默认情况下pretrained = False)，我们注释掉了<code class="fe nk nl nm nn b">model_urls</code>。这只是为了强调这一点，并不是真的有必要这样做；</li><li id="1c6a" class="nb nc iq lg b lh no lk np ln nq lr nr lv ns lz ng nh ni nj bi translated">第一个修改是增加了我们车型的名称:<code class="fe nk nl nm nn b">"vgg12_bn”</code>；</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nt"><img src="../Images/76a9404879958c300b2b8d9074876f43.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bY3g7kolLYOZMBb4RVVCCw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图片作者。</p></figure><ul class=""><li id="8db3" class="nb nc iq lg b lh li lk ll ln nd lr ne lv nf lz ng nh ni nj bi translated">由于我们有10个类，所以我们把<code class="fe nk nl nm nn b">num_classes</code>从1000改成了10；</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nu"><img src="../Images/11a40b6e806f1eeaab9ee2e100907104.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fvtUopq_VFBrWihfjXssQA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图片作者。</p></figure><ul class=""><li id="bfb8" class="nb nc iq lg b lh li lk ll ln nd lr ne lv nf lz ng nh ni nj bi translated">因此，我们创建一个新的配置，<code class="fe nk nl nm nn b">"V"</code>，具有以下卷积层(每层的信道数量按顺序显示):64、64、128、128、256、256、512、512、512。请注意，下面的<code class="fe nk nl nm nn b">"M" </code>表示最大池。因为VGG默认有3个全连接(FC)层，所以我们总共有12个层；</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nv"><img src="../Images/60356e1cf0a08fbec82e44257bdb3b33.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aX3EmJvMKfGlNgDQ4XFR_A.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图片作者。</p></figure><ul class=""><li id="5656" class="nb nc iq lg b lh li lk ll ln nd lr ne lv nf lz ng nh ni nj bi translated">最后，我们创建一个函数<code class="fe nk nl nm nn b">vgg12_bn</code>，其中只有一行代码调用另一个函数。请注意，我们看到参数<code class="fe nk nl nm nn b">"vgg12_bn"</code>(网络名称)、<code class="fe nk nl nm nn b">"V"</code>(配置)和<code class="fe nk nl nm nn b">True</code>的值，其中后者激活批量标准化。</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nw"><img src="../Images/8cbd05e59c3e96746d959e7856da1a86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ptsExAm7tAazSmaEc5WZ9Q.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图片作者。</p></figure><p id="84c7" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">就是这样。通过添加/修改5行代码，我们创建了我们的模型。在笔记本中，我们需要导入<code class="fe nk nl nm nn b">vggmodified</code>文件来使用VGG12BN。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nx"><img src="../Images/1198beaaab723bdf4cfdbfa24c8c8209.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZPhfJeytp5iGG4fk2XgIWw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图片作者。</p></figure><p id="8d03" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">下图(表)显示了基于10个时期训练后的测试数据集的准确度(Acc)结果。我们比较了原始的VGG16BN、VGG19BN和提议的VGG12BN模型。列<strong class="lg ja"> # Train Param (M) </strong>、<strong class="lg ja"> # Param Mem (MB) </strong>、<strong class="lg ja"> Time (s) </strong>表示每个模型的百万个可训练参数的数量，仅由模型参数决定的兆字节大小，以及使用Google Colab执行它们的时间。我们的“新”VGG12BN获得了更高的精度。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ny"><img src="../Images/0006526f786cfa239215f0644752773a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vgYin2pW0qe7AEpYJJ02kA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">VGG:结果。图片作者。</p></figure><h2 id="35bd" class="me mf iq bd mg mh mi dn mj mk ml dp mm ln mn mo mp lr mq mr ms lv mt mu mv iw bi translated">DenseNet-83和ResNet-14 </h2><p id="72be" class="pw-post-body-paragraph le lf iq lg b lh mw ka lj lk mx kd lm ln my lp lq lr mz lt lu lv na lx ly lz ij bi translated">在<a class="ae ma" href="https://arxiv.org/abs/1608.06993" rel="noopener ugc nofollow" target="_blank"> DenseNet </a>模型中，每一层都以前馈方式与其他层相连，旨在最大化层间的信息流。2015年ILSVRC的另一个获奖者，<a class="ae ma" href="https://arxiv.org/abs/1512.03385" rel="noopener ugc nofollow" target="_blank"> ResNet </a>是最受欢迎的CNN之一，其中已经提出了它的几个变体(ResNeXt、Wide ResNet、…)，它也作为其他dnn的一部分得到了重用。这是一种残差学习方法，其中堆叠层拟合残差映射，而不是直接拟合期望的底层映射。我们遵循了与刚才介绍的VGG12BN类似的过程，但现在我们只需更改3行代码，就可以创建DenseNet-83 (83层)和ResNet-14 (14层)。</p><p id="6138" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">访问<a class="ae ma" href="https://github.com/vsantjr/DeepLearningMadeEasy/blob/temp_23-09/densenetmodified.py" rel="noopener ugc nofollow" target="_blank">此处</a>dense net修改后的代码及其对应的笔记本是<a class="ae ma" href="https://github.com/vsantjr/DeepLearningMadeEasy/blob/temp_23-09/PyTorch_DenseNet.ipynb" rel="noopener ugc nofollow" target="_blank">此处</a>。ResNet修改后的代码在这里是<a class="ae ma" href="https://github.com/vsantjr/DeepLearningMadeEasy/blob/temp_23-09/resnetmodified.py" rel="noopener ugc nofollow" target="_blank"/>，笔记本与VGG12BN相同，但现在我们通过运行ResNet-14 来显示<a class="ae ma" href="https://github.com/vsantjr/DeepLearningMadeEasy/blob/temp_23-09/PyTorch_VGG_ResNet_ResNetout.ipynb" rel="noopener ugc nofollow" target="_blank">的输出。由于创建这两个网络的修改是相似的，下面我们将只显示创建DenseNet-83的修改。因此，这就是我们所做的:</a></p><ul class=""><li id="b07f" class="nb nc iq lg b lh li lk ll ln nd lr ne lv nf lz ng nh ni nj bi translated">如前所述，我们注释掉了<code class="fe nk nl nm nn b">from .._internally_replaced_utils import load_state_dict_from_url</code>以避免依赖其他PyTorch模块。但是注意，我们现在没有评论出<code class="fe nk nl nm nn b">models_url</code>；</li><li id="92c0" class="nb nc iq lg b lh no lk np ln nq lr nr lv ns lz ng nh ni nj bi translated">我们添加了我们模型的名称:<code class="fe nk nl nm nn b">"densenet83"</code>；</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nz"><img src="../Images/3413af25551a93dd31a0b48f1fadc307.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZIdjgJO-WziNreOeynmJRw.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图片作者。</p></figure><ul class=""><li id="b795" class="nb nc iq lg b lh li lk ll ln nd lr ne lv nf lz ng nh ni nj bi translated">我们创建一个函数<code class="fe nk nl nm nn b">densenet83</code>，它只有一行代码调用另一个函数。注意，我们看到参数<code class="fe nk nl nm nn b">"densenet83"</code>(网络的名字)，和(3，6，18，12)的值分别是密集块1，2，3，4的重复次数。</li></ul><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oa"><img src="../Images/4e2cff83a15ba2153cdaf512be964cad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0JhFPbPmO65Di6ryn_MWjQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图片作者。</p></figure><p id="9ed5" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">下图显示了ImageNet的DenseNet架构。我们将DenseNet-161作为参照，并将嵌段的重复次数减半以得到DenseNet-83。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ob"><img src="../Images/70a0a240726a19b4c5966402163ea883.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XfFo4rKMlf7G4NzM-Y6AkQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图片(表格)来自<a class="ae ma" href="https://arxiv.org/abs/1608.06993" rel="noopener ugc nofollow" target="_blank"> DenseNet </a>的文章。</p></figure><p id="c77c" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">总共添加/修改了3行代码。对于DenseNet，下图(表)显示了基于10个时期训练后的测试数据集的准确度(Acc)结果。我们比较了最初的DenseNet-121、DenseNet-161和提议的DenseNet-83型号。我们基本上看到了DenseNet-161和我们的“新”DenseNet-83在性能方面的差距，DenseNet-161略胜一筹。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ny"><img src="../Images/1a10339f4a30fe95a9e58ec0d38be2b2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iApDR9wEhR_t3nxBtCpEbA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">DenseNet:结果。图片作者。</p></figure><p id="84b1" class="pw-post-body-paragraph le lf iq lg b lh li ka lj lk ll kd lm ln lo lp lq lr ls lt lu lv lw lx ly lz ij bi translated">关于ResNet，比较ResNet-18、ResNet-34和“新”ResNet-14，ResNet-18最好，ResNet-14次之，如下图(表)所示。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ny"><img src="../Images/686a058c32b2e0ca9ca6f7fdb9ef45e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*43tIfaL9OBdw62F1qAGshQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">ResNet:结果。图片作者。</p></figure><h2 id="f1ba" class="me mf iq bd mg mh mi dn mj mk ml dp mm ln mn mo mp lr mq mr ms lv mt mu mv iw bi translated"><strong class="ak">结论</strong></h2><p id="4018" class="pw-post-body-paragraph le lf iq lg b lh mw ka lj lk mx kd lm ln my lp lq lr mz lt lu lv na lx ly lz ij bi translated">在本帖中，我们展示了通过修改先前提议的网络的几行代码来创建新的dnn是多么容易。我们声称重用以前的想法并通过适当的修改衍生出新的模型是一个很好的途径。</p></div></div>    
</body>
</html>