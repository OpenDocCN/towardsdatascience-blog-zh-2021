<html>
<head>
<title>Efficiently Streaming a Large AWS S3 File via S3 Select</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过S3选择有效地传输大型AWS S3文件</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/efficiently-streaming-a-large-aws-s3-file-via-s3-select-85f7fbe22e46?source=collection_archive---------32-----------------------#2021-04-06">https://towardsdatascience.com/efficiently-streaming-a-large-aws-s3-file-via-s3-select-85f7fbe22e46?source=collection_archive---------32-----------------------#2021-04-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="b0ff" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/making-sense-of-big-data" rel="noopener" target="_blank">理解大数据</a></h2><div class=""/><div class=""><h2 id="31ad" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">使用AWS S3选择将一个大的S3文件流式处理成易于管理的块，而不需要将整个文件下载到本地</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/7d58fe2467eb2417f01101225250e6e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*No9IFtySUvJLQuoA6mG_gQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">在<a class="ae le" href="https://undraw.co/license" rel="noopener ugc nofollow" target="_blank">公共许可</a>下<a class="ae le" href="https://undraw.co/" rel="noopener ugc nofollow" target="_blank">解压缩</a>生成的图像</p></figure><p id="826a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">AWS S3是行业领先的对象存储服务。我们倾向于在S3上存储大量数据文件，有时需要处理这些文件。如果我们正在处理的文件很小，我们基本上可以采用传统的文件处理流程，从S3获取文件，然后逐行处理。但是问题来了，如果文件的大小比。<code class="fe mb mc md me b">&gt; 1GB</code>？😓</p><p id="5965" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">导入(读取)大文件导致<code class="fe mb mc md me b">Out of Memory</code>错误。它还会导致系统崩溃。有图书馆即。<a class="ae le" href="https://pandas.pydata.org/docs/" rel="noopener ugc nofollow" target="_blank">熊猫</a>、<a class="ae le" href="https://docs.dask.org/en/latest/" rel="noopener ugc nofollow" target="_blank">达斯克</a>等。非常擅长处理大文件，但同样的文件是在本地，即我们将不得不从S3进口到我们的本地机器。但是如果我们不想在本地获取和存储整个S3文件呢？🤔</p><p id="c809" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">📜<strong class="lh ja">让我们考虑一些用例:</strong></p><ul class=""><li id="6477" class="mf mg iq lh b li lj ll lm lo mh ls mi lw mj ma mk ml mm mn bi translated">我们希望每天处理一个大的CSV S3文件(~2GB)。它必须在一定的时间范围内处理(如4小时)</li><li id="bfaf" class="mf mg iq lh b li mo ll mp lo mq ls mr lw ms ma mk ml mm mn bi translated">我们需要定期处理来自FTP服务器的大型S3文件。新文件以特定的时间间隔出现，并被顺序处理，即在开始处理新文件之前必须处理旧文件。</li></ul><p id="1820" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">这些是一些非常好的场景，其中本地处理可能会影响系统的整体流程。此外，如果我们在容器中运行这些文件处理单元，那么我们可以使用的磁盘空间是有限的。因此，需要一个云流式传输流(它还可以通过在并行线程/进程中流式传输同一文件的不同块来<code class="fe mb mc md me b">parallelize the processing of multiple chunks</code>同一文件)。这就是我偶然发现<code class="fe mb mc md me b">AWS S3 Select</code>功能的地方。😎</p><p id="ea61" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">📝这篇文章关注的是将一个大文件分成更小的可管理的块(按顺序)。然后，通过在并发线程/进程中运行，这种方法可用于并行处理。查看我在这个上的<a class="ae le" rel="noopener" target="_blank" href="/parallelize-processing-a-large-aws-s3-file-d43a580cea3">下一篇帖子。</a></p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><h1 id="ed71" class="na nb iq bd nc nd ne nf ng nh ni nj nk kf nl kg nm ki nn kj no kl np km nq nr bi translated">S3精选</h1><p id="c16c" class="pw-post-body-paragraph lf lg iq lh b li ns ka lk ll nt kd ln lo nu lq lr ls nv lu lv lw nw ly lz ma ij bi translated">使用<code class="fe mb mc md me b">Amazon S3 Select</code>，您可以使用简单的结构化查询语言(SQL)语句来过滤亚马逊S3对象的内容，并只检索您需要的数据子集。使用<code class="fe mb mc md me b">Amazon S3 Select</code>过滤这些数据，您可以减少亚马逊S3传输的数据量，降低检索这些数据的成本和延迟。</p><p id="bbde" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">Amazon S3 Select适用于以CSV、JSON或Apache Parquet格式存储的对象。它还可以处理用GZIP或BZIP2压缩的对象(仅适用于CSV和JSON对象)和服务器端加密的对象。您可以将结果的格式指定为CSV或JSON，并且可以确定如何分隔结果中的记录。</p><p id="172f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">📝我们将使用Python <code class="fe mb mc md me b">boto3</code>来完成我们的最终目标。</p><h1 id="8532" class="na nb iq bd nc nd nx nf ng nh ny nj nk kf nz kg nm ki oa kj no kl ob km nq nr bi translated">🧱构造SQL表达式</h1><p id="ea7f" class="pw-post-body-paragraph lf lg iq lh b li ns ka lk ll nt kd ln lo nu lq lr ls nv lu lv lw nw ly lz ma ij bi translated">为了配合<code class="fe mb mc md me b">S3 Select</code>，<code class="fe mb mc md me b">boto3</code>提供了<a class="ae le" href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.select_object_content" rel="noopener ugc nofollow" target="_blank">select _ object _ content()</a>函数来查询S3。您在请求中将SQL表达式传递给亚马逊S3。<code class="fe mb mc md me b">Amazon S3 Select</code>支持SQL的子集。<a class="ae le" href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-glacier-select-sql-reference.html" rel="noopener ugc nofollow" target="_blank">查看此链接，了解关于此</a>的更多信息。</p><pre class="kp kq kr ks gt oc me od oe aw of bi"><span id="aa94" class="og nb iq me b gy oh oi l oj ok">response = s3_client.select_object_content(<br/>    Bucket=bucket,<br/>    Key=key,<br/>    ExpressionType='SQL',<br/>    Expression='SELECT * FROM S3Object',<br/>    InputSerialization={<br/>        'CSV': {<br/>            'FileHeaderInfo': 'USE',<br/>            'FieldDelimiter': ',',<br/>            'RecordDelimiter': '\n'<br/>        }<br/>    },<br/>    OutputSerialization={<br/>        'JSON': {<br/>            'RecordDelimiter': ','<br/>        }<br/>    }<br/>)</span></pre><p id="6e71" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在上面的请求中，<code class="fe mb mc md me b">InputSerialization</code>决定了S3文件的类型和相关属性，而<code class="fe mb mc md me b">OutputSerialization</code>决定了我们从这个<code class="fe mb mc md me b">select_object_content()</code>中得到的<code class="fe mb mc md me b">response</code>。</p><h1 id="f0d6" class="na nb iq bd nc nd nx nf ng nh ny nj nk kf nz kg nm ki oa kj no kl ob km nq nr bi translated">🌫️流块</h1><p id="311c" class="pw-post-body-paragraph lf lg iq lh b li ns ka lk ll nt kd ln lo nu lq lr ls nv lu lv lw nw ly lz ma ij bi translated">现在，我们已经对<code class="fe mb mc md me b">S3 Select</code>的工作原理有了一些了解，让我们试着完成一个大文件的流块(子集)用例，就像<code class="fe mb mc md me b">paginated API works</code>一样。😋</p><p id="24dd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><code class="fe mb mc md me b">S3 Select</code>支持<code class="fe mb mc md me b">ScanRange</code>参数，该参数通过指定要查询的字节范围来帮助我们流式传输对象的子集。<code class="fe mb mc md me b">S3 Select</code>请求一系列不重叠的扫描范围。扫描范围不需要与记录边界对齐。查询将处理在指定扫描范围内开始但超出该扫描范围的记录。这意味着将在扫描范围内提取该行，并且可以扩展到提取整行。如果是<code class="fe mb mc md me b">doesn't fetch a subset of a row</code>，要么读取整行，要么跳过整行(在另一个扫描范围内读取)。</p><p id="a721" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">让我们尝试用两个简单的步骤来实现这一点:</p><h2 id="e40b" class="og nb iq bd nc ol om dn ng on oo dp nk lo op oq nm ls or os no lw ot ou nq iw bi translated">1.找出S3文件的总字节数</h2><p id="81a1" class="pw-post-body-paragraph lf lg iq lh b li ns ka lk ll nt kd ln lo nu lq lr ls nv lu lv lw nw ly lz ma ij bi translated">下面的代码片段展示了将对我们的S3文件执行<code class="fe mb mc md me b">HEAD</code>请求并确定文件大小(以字节为单位)的函数。</p><pre class="kp kq kr ks gt oc me od oe aw of bi"><span id="9c1c" class="og nb iq me b gy oh oi l oj ok">def get_s3_file_size(bucket: str, key: str) -&gt; int:<br/>    """Gets the file size of S3 object by a HEAD request<br/><br/>    Args:<br/>        bucket (str): S3 bucket<br/>        key (str): S3 object path<br/><br/>    Returns:<br/>        int: File size in bytes. Defaults to 0 if any error.<br/>    """<br/>    aws_profile = current_app.config.get('AWS_PROFILE_NAME')<br/>    s3_client = boto3.session.Session(profile_name=aws_profile).client('s3')<br/>    file_size = 0<br/>    try:<br/>        response = s3_client.head_object(Bucket=bucket, Key=key)<br/>        if response:<br/>            file_size = int(response.get('ResponseMetadata').get('HTTPHeaders').get('content-length'))<br/>    except ClientError:<br/>        logger.exception(f'Client error reading S3 file {bucket} : {key}')<br/>    return file_size</span></pre><h2 id="7890" class="og nb iq bd nc ol om dn ng on oo dp nk lo op oq nm ls or os no lw ot ou nq iw bi translated">2.创建一个生成器来传输块</h2><p id="e02e" class="pw-post-body-paragraph lf lg iq lh b li ns ka lk ll nt kd ln lo nu lq lr ls nv lu lv lw nw ly lz ma ij bi translated">现在，逻辑是产生S3文件的字节流块，直到我们达到文件大小。请放心，这种连续的扫描范围不会导致响应中的行重叠😉(检查输出图像/ GitHub repo)。很简单，嗯？😝</p><pre class="kp kq kr ks gt oc me od oe aw of bi"><span id="208f" class="og nb iq me b gy oh oi l oj ok">import ast<br/>import boto3<br/>from botocore.exceptions import ClientError<br/><br/>def stream_s3_file(bucket: str, key: str, file_size: int, chunk_bytes=5000) -&gt; tuple[dict]:<br/>    """Streams a S3 file via a generator.<br/><br/>    Args:<br/>        bucket (str): S3 bucket<br/>        key (str): S3 object path<br/>        chunk_bytes (int): Chunk size in bytes. Defaults to 5000<br/>    Returns:<br/>        tuple[dict]: Returns a tuple of dictionary containing rows of file content<br/>    """<br/>    aws_profile = current_app.config.get('AWS_PROFILE_NAME')<br/>    s3_client = boto3.session.Session(profile_name=aws_profile).client('s3')<br/>    expression = 'SELECT * FROM S3Object'<br/>    start_range = 0<br/>    end_range = min(chunk_bytes, file_size)<br/>    while start_range &lt; file_size:<br/>        response = s3_client.select_object_content(<br/>            Bucket=bucket,<br/>            Key=key,<br/>            ExpressionType='SQL',<br/>            Expression=expression,<br/>            InputSerialization={<br/>                'CSV': {<br/>                    'FileHeaderInfo': 'USE',<br/>                    'FieldDelimiter': ',',<br/>                    'RecordDelimiter': '\n'<br/>                }<br/>            },<br/>            OutputSerialization={<br/>                'JSON': {<br/>                    'RecordDelimiter': ','<br/>                }<br/>            },<br/>            ScanRange={<br/>                'Start': start_range,<br/>                'End': end_range<br/>            },<br/>        )<br/><br/>        """<br/>        select_object_content() response is an event stream that can be looped to concatenate the overall result set<br/>        Hence, we are joining the results of the stream in a string before converting it to a tuple of dict<br/>        """<br/>        result_stream = []<br/>        for event in response['Payload']:<br/>            if records := event.get('Records'):<br/>                result_stream.append(records['Payload'].decode('utf-8'))<br/>        yield ast.literal_eval(''.join(result_stream))<br/>        start_range = end_range<br/>        end_range = end_range + min(chunk_bytes, file_size - end_range)<br/><br/><br/>def s3_file_processing():<br/>    bucket = '&lt;s3-bucket&gt;'<br/>    key = '&lt;s3-key&gt;'<br/>    file_size = get_s3_file_size(bucket=bucket, key=key)<br/>    logger.debug(f'Initiating streaming file of {file_size} bytes')<br/>    chunk_size = 524288  # 512KB or 0.5MB<br/>    for file_chunk in stream_s3_file(bucket=bucket, key=key,<br/>                                     file_size=file_size, chunk_bytes=chunk_size):<br/>        logger.info(f'\n{30 * "*"} New chunk {30 * "*"}')<br/>        id_set = set()<br/>        for row in file_chunk:<br/>            # perform any other processing here<br/>            id_set.add(int(row.get('id')))<br/>        logger.info(f'{min(id_set)} --&gt; {max(id_set)}')</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ov"><img src="../Images/2c582298f46f9d643ba51256fb85144c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UTbaGhAuJtcYCaVR.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated"><em class="ow">图片作者</em></p></figure><p id="1986" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">恭喜你！👏我们已经成功地解决了在不使系统崩溃的情况下处理大型S3文件的关键挑战之一。🤘</p><p id="f7cc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">📌您可以<a class="ae le" href="https://github.com/idris-rampurawala/s3-select-demo" rel="noopener ugc nofollow" target="_blank">查看我的GitHub库</a>以获得这种方法的完整工作示例。</p><p id="0241" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">🔖实现更多并发性的下一步是并行处理文件。点击查看这篇文章的续集<a class="ae le" rel="noopener" target="_blank" href="/parallelize-processing-a-large-aws-s3-file-d43a580cea3">。</a></p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><h1 id="0c19" class="na nb iq bd nc nd ne nf ng nh ni nj nk kf nl kg nm ki nn kj no kl np km nq nr bi translated">使用S3-Select的✔️优势</h1><ul class=""><li id="5457" class="mf mg iq lh b li ns ll nt lo ox ls oy lw oz ma mk ml mm mn bi translated">减少IO，从而提高性能</li><li id="5b47" class="mf mg iq lh b li mo ll mp lo mq ls mr lw ms ma mk ml mm mn bi translated">由于数据传输费用减少，成本降低</li><li id="af3a" class="mf mg iq lh b li mo ll mp lo mq ls mr lw ms ma mk ml mm mn bi translated">使用多线程/进程中的<code class="fe mb mc md me b">ScanRange</code>,可以并行运行多个块来加速文件处理</li></ul></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><h1 id="5b3f" class="na nb iq bd nc nd ne nf ng nh ni nj nk kf nl kg nm ki nn kj no kl np km nq nr bi translated">S3选择的❗限制</h1><ul class=""><li id="dee1" class="mf mg iq lh b li ns ll nt lo ox ls oy lw oz ma mk ml mm mn bi translated">输入或结果中记录的最大长度是1 MB</li><li id="5bf0" class="mf mg iq lh b li mo ll mp lo mq ls mr lw ms ma mk ml mm mn bi translated">亚马逊S3选择只能使用JSON输出格式发出嵌套数据</li><li id="89ae" class="mf mg iq lh b li mo ll mp lo mq ls mr lw ms ma mk ml mm mn bi translated">S3选择返回一个编码字节流，所以我们必须循环返回的流并解码输出<code class="fe mb mc md me b">records['Payload'].decode('utf-8')</code></li><li id="d49f" class="mf mg iq lh b li mo ll mp lo mq ls mr lw ms ma mk ml mm mn bi translated">仅适用于以CSV、JSON或Apache Parquet格式存储的对象。为了更多的灵活性/功能，您可以选择<a class="ae le" href="https://docs.aws.amazon.com/athena/latest/ug/what-is.html" rel="noopener ugc nofollow" target="_blank"> AWS Athena </a></li></ul></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><h1 id="c9fc" class="na nb iq bd nc nd ne nf ng nh ni nj nk kf nl kg nm ki nn kj no kl np km nq nr bi translated">📑资源</h1><ul class=""><li id="38cc" class="mf mg iq lh b li ns ll nt lo ox ls oy lw oz ma mk ml mm mn bi translated"><a class="ae le" href="https://github.com/idris-rampurawala/s3-select-demo" rel="noopener ugc nofollow" target="_blank">我的GitHub库演示了上述方法</a></li><li id="a352" class="mf mg iq lh b li mo ll mp lo mq ls mr lw ms ma mk ml mm mn bi translated"><a class="ae le" href="https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.select_object_content" rel="noopener ugc nofollow" target="_blank"> AWS S3选择boto3参考</a></li><li id="82cd" class="mf mg iq lh b li mo ll mp lo mq ls mr lw ms ma mk ml mm mn bi translated"><a class="ae le" href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/selecting-content-from-objects.html" rel="noopener ugc nofollow" target="_blank"> AWS S3选择用户指南</a></li><li id="3aad" class="mf mg iq lh b li mo ll mp lo mq ls mr lw ms ma mk ml mm mn bi translated"><a class="ae le" href="https://aws.amazon.com/blogs/aws/s3-glacier-select/" rel="noopener ugc nofollow" target="_blank"> AWS S3选择示例</a></li><li id="3dcb" class="mf mg iq lh b li mo ll mp lo mq ls mr lw ms ma mk ml mm mn bi translated"><a class="ae le" rel="noopener" target="_blank" href="/parallelize-processing-a-large-aws-s3-file-d43a580cea3">这篇文章的续篇展示了并行文件处理</a></li></ul></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><p id="44b4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="pa">原载于2021年4月6日</em><a class="ae le" href="https://dev.to/idrisrampurawala/efficiently-streaming-a-large-aws-s3-file-via-s3-select-4on" rel="noopener ugc nofollow" target="_blank"><em class="pa">https://dev . to</em></a><em class="pa">。</em></p></div></div>    
</body>
</html>