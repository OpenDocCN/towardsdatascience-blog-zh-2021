<html>
<head>
<title>AI Architects: The New Programmers</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能架构师:新程序员</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ai-architects-the-new-programmers-7ed1963d6166?source=collection_archive---------21-----------------------#2021-09-19">https://towardsdatascience.com/ai-architects-the-new-programmers-7ed1963d6166?source=collection_archive---------21-----------------------#2021-09-19</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ebf5" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">论编程新范式的本质</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/785a31cb8af26b2fa1bdf1101a69c2d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TcMi9WbZHoOnOEh9xhoUew.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片由来自<a class="ae kv" href="https://pixabay.com/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1617969" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>的<a class="ae kv" href="https://pixabay.com/users/ahmadardity-3112014/?utm_source=link-attribution&amp;utm_medium=referral&amp;utm_campaign=image&amp;utm_content=1617969" rel="noopener ugc nofollow" target="_blank"> Ahmad Ardity </a>提供</p></figure><blockquote class="kw kx ky"><p id="e252" class="kz la lb lc b ld le jr lf lg lh ju li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">人工智能的成功故事已经变得如此普遍，似乎每一个软件都将很快以这样或那样的方式融入人工智能。但是人工智能程序是做什么的呢？人工智能程序可以通过经验学习和适应。构建人工智能程序与构建非人工智能程序的不同之处，正在给我们对编程的理解带来范式转变。在这篇文章中，我探索了新范式的本质以及它可能采取的未来路线。</p><p id="f90c" class="kz la lb lc b ld le jr lf lg lh ju li lj lk ll lm ln lo lp lq lr ls lt lu lv ij bi translated">卡帕西在他的文章“<a class="ae kv" href="https://karpathy.medium.com/software-2-0-a64152b37c35" rel="noopener">软件2.0 </a>”中，描述了一个人工智能触发的范式转变。他想象“程序员2.0”更多地参与“编辑和增加数据集”，而不是编写代码，他预测对人类开发人员的需求将会下降。他说，给定正确的数据集，人工智能会自己找到正确的算法。我不这么认为。我相信，软件2.0将更少地关注数据，而更多地关注围绕可能的算法的搜索空间定义适当的约束。我认为对人类开发人员的需求不会减弱，相反，它的性质会发生变化。</p></blockquote><h2 id="d24d" class="lw lx iq bd ly lz ma dn mb mc md dp me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">范式分歧</h2><p id="5df5" class="pw-post-body-paragraph kz la iq lc b ld ms jr lf lg mt ju li mf mu ll lm mj mv lp lq mn mw lt lu lv ij bi translated">算法是机器可解释指令的有限序列。在旧的范例中，程序员的目标是将一系列指令编织在一起，构成一个单一的、定义明确的算法。在新的范式中，程序员只指定一个模板，该模板编码了多个潜在的算法，机器使用训练数据学习最优算法。模板映射到整个算法空间中的某个区域——所有可能算法的空间。</p><h2 id="50bc" class="lw lx iq bd ly lz ma dn mb mc md dp me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">算法模板的分类</h2><p id="8846" class="pw-post-body-paragraph kz la iq lc b ld ms jr lf lg mt ju li mf mu ll lm mj mv lp lq mn mw lt lu lv ij bi translated"><a class="ae kv" href="https://en.wikipedia.org/wiki/Turing_machine" rel="noopener ugc nofollow" target="_blank">图灵机</a> (TM)是一个通用模板。它本身编码了所有可能算法的空间。接下来的是，每一个可以想到的算法，本质上都是一个带有规则表的图灵机。规则表规定了精确的执行路径。一种被称为通用图灵机 (UTM)的特殊图灵机可以模拟任何其他图灵机的执行，给定一个规则表作为输入。</p><p id="0835" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mf lk ll lm mj lo lp lq mn ls lt lu lv ij bi translated">艾伦·图灵的导师阿隆索·丘奇(Alonso Church)开发的Lambda Calculus  (LC)是另一个通用模板，相当于图灵机。LC的美学与TM的美学有很大的不同。TM将计算形式化为根据规则表对一条带上的符号进行操作。LC将其形式化为通过变量绑定和替换的函数及其解析的堆叠。换句话说，TM是计算的可视化表示，而LC是抽象的、代数式的(见图1，2)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mx"><img src="../Images/2f42cf6a9f9a72f873f9e92a12893e02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VS_rypBWCAgE9Yshs59wUg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图一。图灵机。作者Gabriel F-Own作品，CC BY-SA 3.0，【https://commons.wikimedia.org/w/index.php?curid=26270095 T2】</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi my"><img src="../Images/c0582004f428b1ddb1141e2b11b47fcd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xlc8zTS54o00WYHRiFBsKA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图二。通用Lambda解释器的Lambda表达式。它相当于λ微积分中的UTM。取自crypto.stanford.edu/~blynn/lambda/.</p></figure><p id="7819" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mf lk ll lm mj lo lp lq mn ls lt lu lv ij bi translated">TM和LC都跨越了整个算法空间。现在的问题是如何构造只跨越子空间的模板，并在其上进行机器学习。人工神经网络(ANN)利用其可调权重，可以编码和模拟多种算法。人工神经网络并不跨越整个算法空间，除了那些图灵完备的算法空间[1]。因此，他们允许我们通过定制架构创建各种模板。与TM和LC不同，ANN编码的算法空间是连续的，支持<a class="ae kv" href="https://en.wikipedia.org/wiki/Differentiable_programming" rel="noopener ugc nofollow" target="_blank">差分编程</a>，从而更易于机器学习。它也有其局限性，我稍后会谈到这一点。</p><p id="e57e" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mf lk ll lm mj lo lp lq mn ls lt lu lv ij bi translated">算法子空间或模板可以组织成树状结构，如图3所示。树的根是一个通用的模板，不管是TM，LC，还是图灵完全神经网络。随着我们向底部移动，模板变得更加具体，因为它们包含越来越小的算法家族。叶节点对应于单独的算法。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/54bf5f1dd6d98725da4e460929351b00.png" data-original-src="https://miro.medium.com/v2/resize:fit:1202/format:webp/1*OyAxTUUGplZsayRwtaipHA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3。算法之树。叶子对应于特定的算法，而非叶子节点对应于算法族。一个算法家族可以通过一些位于底层的特定算法来模拟。UTM可以模拟任何算法。请注意，该树纯粹是假设的。</p></figure><p id="4e74" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mf lk ll lm mj lo lp lq mn ls lt lu lv ij bi translated">重要的是要注意，有无限多的方式来构建这样一个树，虽然其中一些可能更符合我们对世界的先验知识。</p><h2 id="32d0" class="lw lx iq bd ly lz ma dn mb mc md dp me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">风格的差异</h2><p id="e877" class="pw-post-body-paragraph kz la iq lc b ld ms jr lf lg mt ju li mf mu ll lm mj mv lp lq mn mw lt lu lv ij bi translated">软件2.0的程序员构造模板需要遵循什么策略？鉴于大数据和相对便宜的计算，一个诱人的方法是从通用模板或大搜索空间开始，让机器搜索最佳算法。与包含工作算法的小区域相比，总的算法空间是无限大的，所以用通用模板学习需要大量的数据和计算。例如，<a class="ae kv" href="https://en.wikipedia.org/wiki/GPT-3" rel="noopener ugc nofollow" target="_blank"> GPT-3 </a>，一个产生类似人类文本的语言模型，使用了1750亿个参数，45tb的训练数据。原则上，这种数据密集型方法不需要太多的人类专业知识。</p><p id="dbed" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mf lk ll lm mj lo lp lq mn ls lt lu lv ij bi translated">另一种方法是从一个小的搜索空间或高度专业化的模板开始。在这里，在将球传给机器之前，需要人类的专业知识来适当缩小搜索空间。如果做得好，在更小的搜索空间中学习的成本会变得更低，找到的解决方案更有可能与我们直觉认为的智能一致。一个很好的例子是卷积神经网络(<a class="ae kv" href="https://en.wikipedia.org/wiki/Convolutional_neural_network" rel="noopener ugc nofollow" target="_blank"> CNN </a>)。CNN的想法受到了对大脑视觉皮层进行的神经科学研究的启发。CNN使用<a class="ae kv" href="https://en.wikipedia.org/wiki/Principle_of_locality" rel="noopener ugc nofollow" target="_blank">局部性</a>和t <a class="ae kv" href="https://en.wikipedia.org/wiki/Translational_symmetry" rel="noopener ugc nofollow" target="_blank">平移不变性</a>的原理，产生共享权重，从而比正常情况下需要更少的自由参数。AlexNet [3]是使用CNN的最有影响力的计算机视觉模型之一，需要大约150G的训练数据和6100万个参数。我认为这种方法将在人工智能的未来扮演更重要的角色。为了支持人工智能的这种方法，我将探索它是如何被自然本身用来解决智能和学习问题的。</p><h2 id="1b21" class="lw lx iq bd ly lz ma dn mb mc md dp me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">自然智能的本质</h2><p id="5398" class="pw-post-body-paragraph kz la iq lc b ld ms jr lf lg mt ju li mf mu ll lm mj mv lp lq mn mw lt lu lv ij bi translated">动物和人类不需要像现代人工智能那样需要大量的数据来学习。相反，包括人类在内的幼小动物在与环境互动最少的情况下，学习速度很快。Zador [4]认为这是因为动物天生具有高度结构化的大脑。高度结构化的大脑缩小了可能算法的搜索空间，实现了快速快速的学习。根据上面的图3，这意味着自然大脑编码的模板不是通用的；相反，它们是高度专业化的。</p><p id="1c07" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mf lk ll lm mj lo lp lq mn ls lt lu lv ij bi translated">Zador还引入了“基因组瓶颈”的概念。基因组瓶颈表明，大脑的接线图过于复杂，无法在基因组中明确指定，这暗示着基因组水平上发生了一些压缩。在这一点上，一个有趣的问题出现了:大自然是如何选择高度专业化的大脑结构，同时保持可压缩性的？这个问题的答案将揭示软件2.0的有趣线索。</p><h2 id="9d01" class="lw lx iq bd ly lz ma dn mb mc md dp me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">自然的本质</h2><p id="6ce0" class="pw-post-body-paragraph kz la iq lc b ld ms jr lf lg mt ju li mf mu ll lm mj mv lp lq mn mw lt lu lv ij bi translated">林等人[5]曾无意中回答过这个问题。他们表明，描述宇宙的物理方程需要定义的参数比预期的少得多。他们认为，因为宇宙遵循对称、<a class="ae kv" href="https://en.wikipedia.org/wiki/Principle_of_compositionality" rel="noopener ugc nofollow" target="_blank">组成</a>、<a class="ae kv" href="https://en.wikipedia.org/wiki/Principle_of_locality#:~:text=In%20physics%2C%20the%20principle%20of,%22action%20at%20a%20distance%22." rel="noopener ugc nofollow" target="_blank">位置</a>和等级形式的原则，所以试图模拟其规律的方程也是如此。由此可见，在宇宙中导航的最佳算法也遵循相同的原则，这也解释了深度和廉价学习在图像识别等任务中的成功。</p><p id="1941" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mf lk ll lm mj lo lp lq mn ls lt lu lv ij bi translated">在试图解释深度学习的成功时，作者还回答了关于基因组瓶颈的问题。大自然可能通过选择遵循上述原则的大脑结构来保持可压缩性。换句话说，大脑结构的特殊化不是随机发生的，而是遵循这些高级原则。</p><p id="93f9" class="pw-post-body-paragraph kz la iq lc b ld le jr lf lg lh ju li mf lk ll lm mj lo lp lq mn ls lt lu lv ij bi translated">我预测软件2.0的架构师将利用这些指导原则来构建最佳算法模板。有趣的是，受神经科学启发的卷积神经网络也利用了平移对称性和局部性。请注意，这些原则仅规定了最终解决方案的形式。为了填充适当的内容，我们需要收集更多的知识，并将其转化为软件2.0的细节</p><h2 id="9cb8" class="lw lx iq bd ly lz ma dn mb mc md dp me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">语言的诞生</h2><p id="1370" class="pw-post-body-paragraph kz la iq lc b ld ms jr lf lg mt ju li mf mu ll lm mj mv lp lq mn mw lt lu lv ij bi translated">在使用人工神经网络进行深度学习的情况下，将知识转化为算法模板是具有挑战性的。我们更习惯于使用符号语言来获取、操作和交流知识。所以我们把知识翻译成经典的计算机科学概念更直观，这些概念是<a class="ae kv" href="https://en.wikipedia.org/wiki/Discrete_mathematics" rel="noopener ugc nofollow" target="_blank">离散的</a>符号化的。另一方面，现代深度学习在连续域中运行，由于其可微分性，更适合于机器学习，而不太适合于人类推理其工作方式。因此，经典计算机科学和现代深度学习之间的差距是一个有待解决的挑战[6]。我相信Lambda演算或函数式编程将在缩小差距方面发挥关键作用(克里斯·奥拉赫<a class="ae kv" href="https://colah.github.io/posts/2015-09-NN-Types-FP/" rel="noopener ugc nofollow" target="_blank">认为</a>)。</p><h2 id="7fd2" class="lw lx iq bd ly lz ma dn mb mc md dp me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">结论</h2><p id="9de8" class="pw-post-body-paragraph kz la iq lc b ld ms jr lf lg mt ju li mf mu ll lm mj mv lp lq mn mw lt lu lv ij bi translated">人工智能引发了我们对编程理解的范式转变。我在这篇文章中认为，新范式的艺术不是编辑和增长大型数据集，而是设计最佳算法<strong class="lc ir"> </strong>模板，以最小的训练数据需求促进快速有效的机器学习。《自然》暗示了实现这一目标的可能途径，但还需要更多的工作来释放神经网络的象征力量。</p><h2 id="b81d" class="lw lx iq bd ly lz ma dn mb mc md dp me mf mg mh mi mj mk ml mm mn mo mp mq mr bi translated">参考</h2><ol class=""><li id="7cd2" class="na nb iq lc b ld ms lg mt mf nc mj nd mn ne lv nf ng nh ni bi translated">西格尔曼，H. T .，&amp;桑塔格，E. D. (1992)。神经网络的计算能力。1992年第五届计算学习理论年度研讨会会议录。<a class="ae kv" href="https://doi.org/10.1145/130385.130432" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1145/130385.130432</a></li><li id="a986" class="na nb iq lc b ld nj lg nk mf nl mj nm mn nn lv nf ng nh ni bi translated">胡贝尔博士；威塞尔(1968年3月至2001年1月)。<a class="ae kv" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1557912" rel="noopener ugc nofollow" target="_blank">“猴纹状皮层的感受野和功能构筑”</a>。<em class="lb">生理学杂志</em>。<strong class="lc ir">195</strong>(1):215–243。<a class="ae kv" href="https://en.wikipedia.org/wiki/Doi_(identifier)" rel="noopener ugc nofollow" target="_blank">doi</a>:<a class="ae kv" href="https://doi.org/10.1113%2Fjphysiol.1968.sp008455" rel="noopener ugc nofollow" target="_blank">10.1113/jphysiol . 1968 . sp 008455</a>。<a class="ae kv" href="https://en.wikipedia.org/wiki/ISSN_(identifier)" rel="noopener ugc nofollow" target="_blank">刊号</a><a class="ae kv" href="https://www.worldcat.org/issn/0022-3751" rel="noopener ugc nofollow" target="_blank">0022–3751</a>。PMC  <a class="ae kv" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1557912" rel="noopener ugc nofollow" target="_blank"> 1557912 </a>。<a class="ae kv" href="https://en.wikipedia.org/wiki/PMID_(identifier)" rel="noopener ugc nofollow" target="_blank">PMID</a>T20】4966457。</li><li id="50c4" class="na nb iq lc b ld nj lg nk mf nl mj nm mn nn lv nf ng nh ni bi translated">亚历克斯·克里日夫斯基；苏茨基弗，伊利亚；杰弗里·辛顿(2017年5月24日)。<a class="ae kv" href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" rel="noopener ugc nofollow" target="_blank">“深度卷积神经网络的ImageNet分类”</a> (PDF)。<em class="lb">ACM的通信</em>。<strong class="lc ir">60</strong>(6):84–90。<a class="ae kv" href="https://en.wikipedia.org/wiki/Doi_(identifier)" rel="noopener ugc nofollow" target="_blank">doi</a>:<a class="ae kv" href="https://doi.org/10.1145%2F3065386" rel="noopener ugc nofollow" target="_blank">10.1145/3065386</a>。<a class="ae kv" href="https://en.wikipedia.org/wiki/ISSN_(identifier)" rel="noopener ugc nofollow" target="_blank">刊号</a><a class="ae kv" href="https://www.worldcat.org/issn/0001-0782" rel="noopener ugc nofollow" target="_blank">0001–0782</a>。<a class="ae kv" href="https://en.wikipedia.org/wiki/S2CID_(identifier)" rel="noopener ugc nofollow" target="_blank">S2CID</a><a class="ae kv" href="https://api.semanticscholar.org/CorpusID:195908774" rel="noopener ugc nofollow" target="_blank">195908774</a>。</li><li id="7292" class="na nb iq lc b ld nj lg nk mf nl mj nm mn nn lv nf ng nh ni bi translated">对纯粹学习和人工神经网络能从动物大脑学到什么的评论。<em class="lb"/><strong class="lc ir">10、</strong> 3770 (2019)。<a class="ae kv" href="https://doi.org/10.1038/s41467-019-11786-6" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1038/s41467-019-11786-6</a></li><li id="67a0" class="na nb iq lc b ld nj lg nk mf nl mj nm mn nn lv nf ng nh ni bi translated">林，H. W .，泰格马克，m .，&amp;罗尼克，D. (2017)。为什么深度廉价学习效果这么好？<em class="lb">《统计物理学报》</em>，<em class="lb"> 168 </em> (6)，1223–1247。【https://doi.org/10.1007/s10955-017-1836-5 T4】</li><li id="5948" class="na nb iq lc b ld nj lg nk mf nl mj nm mn nn lv nf ng nh ni bi translated">Velič ković，p .，&amp; Blundell，C. (2021年)。神经算法推理。<em class="lb">图案</em>，<em class="lb"> 2 </em> (7)，100273。<a class="ae kv" href="https://doi.org/10.1016/j.patter.2021.100273" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1016/j.patter.2021.100273</a></li></ol></div></div>    
</body>
</html>