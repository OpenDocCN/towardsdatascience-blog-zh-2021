<html>
<head>
<title>A Primer on Atrous(Dilated) and Depth-wise Separable Convolutions</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">阿特鲁(扩张的)和深度方向可分卷积的初级读本</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-primer-on-atrous-convolutions-and-depth-wise-separable-convolutions-443b106919f5?source=collection_archive---------7-----------------------#2021-09-15">https://towardsdatascience.com/a-primer-on-atrous-convolutions-and-depth-wise-separable-convolutions-443b106919f5?source=collection_archive---------7-----------------------#2021-09-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="5f73" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="9290" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">什么是萎缩/扩张和深度方向可分卷积？与标准卷积有何不同？它们的用途是什么？</h2></div><p id="7abe" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">凭借权重共享和平移不变性等属性，卷积层和CNN在使用深度学习方法的计算机视觉和图像处理任务中变得无处不在。考虑到这一点，本文旨在讨论我们在卷积网络中看到的一些发展。我们特别关注<strong class="kq ja">的两个</strong>发展:阿特鲁(扩张)卷积和深度方向可穿透卷积。我们将看到这两种类型的卷积是如何工作的，它们与普通的卷积有什么不同，以及为什么我们可能要使用它们。</p><h1 id="c745" class="lk ll iq bd lm ln lo lp lq lr ls lt lu kf lv kg lw ki lx kj ly kl lz km ma mb bi translated">卷积层</h1><p id="5531" class="pw-post-body-paragraph ko kp iq kq b kr mc ka kt ku md kd kw kx me kz la lb mf ld le lf mg lh li lj ij bi translated">在进入正题之前，我们先快速提醒一下自己卷积层是如何工作的。从本质上来说，卷积滤波器只是简单的特征提取器。以前手工制作的特征过滤器现在可以通过反向传播的“魔法”来学习。我们有一个在输入特征图上滑动的核(conv层的权重),在每个位置，执行逐元素乘法，然后对乘积求和，以获得标量值。在每个位置执行相同的操作。图1显示了这一过程。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/c260cadb17c5414a964d7597ef0fdfed.png" data-original-src="https://miro.medium.com/v2/resize:fit:488/1*5ks4cemhWo6bO4ATrxkRyg.gif"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图1:一个3×3卷积滤波器的作用[1]。</p></figure><p id="cafc" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">卷积过滤器通过<strong class="kq ja">在输入特征图上滑动</strong>来检测特定的特征，即，它在每个位置寻找该特征。这直观地解释了卷积的平移不变性。</p><h1 id="e26f" class="lk ll iq bd lm ln lo lp lq lr ls lt lu kf lv kg lw ki lx kj ly kl lz km ma mb bi translated">阿特鲁(扩张)卷积</h1><p id="89b7" class="pw-post-body-paragraph ko kp iq kq b kr mc ka kt ku md kd kw kx me kz la lb mf ld le lf mg lh li lj ij bi translated">为了理解atrous卷积与标准卷积有什么不同，我们首先需要知道什么是<strong class="kq ja">感受野</strong>。感受野被定义为产生每个输出元素的输入特征图的区域的大小。在图1的情况下，感受野是3×3，因为输出特征图中的每个元素看到(使用)3×3输入元素。</p><p id="bae8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">深度CNN使用卷积和最大池的组合。这样做的缺点是，在每一步，特征图的空间分辨率减半。将所得的特征图植入到原始图像上导致稀疏特征提取。这种效果可以在图2中看到。conv。滤镜将输入图像缩减采样为原来的两倍。上采样和在图像上施加特征图表明，响应仅对应于1/4的图像位置(稀疏特征提取)。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi gj"><img src="../Images/dd715505b240316cf668dbcd872f68bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dxK0C3WBBqk_eF0k8KRScQ.png"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图DCNN中的稀疏特征提取[2]。</p></figure><p id="de89" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">阿特鲁(扩张)卷积解决了这个问题，并允许密集特征提取。这是通过一个叫做<strong class="kq ja">的新参数(r) </strong>实现的。简而言之，atrous卷积类似于标准卷积，除了atrous卷积核的权重被间隔开<strong class="kq ja">或</strong>个位置，即，扩展卷积层的核是稀疏的。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi mx"><img src="../Images/8ad129d102e57aa362b1f3525a166a3f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/format:webp/1*xBz2R6qoArKjkthYzKLZMQ.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图3:标准与膨胀的内核。</p></figure><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi my"><img src="../Images/4018c56491b3570946f2d70e4c487da7.png" data-original-src="https://miro.medium.com/v2/resize:fit:790/1*SVkgHoFoiMZkjy54zM_SUw.gif"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图4:3×3阿特鲁(扩张)卷积的作用[1]。</p></figure><p id="3572" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">图3(a)示出了标准内核，图3(b)示出了具有<strong class="kq ja">比率r = 2 </strong>的扩大的3×3内核。通过控制速率参数，我们可以任意控制conv的感受野。层。这使得conv。过滤以查看更大的输入区域(感受野),而不降低空间分辨率或增加内核大小。图4示出了运行中的扩展卷积滤波器。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi mz"><img src="../Images/f0d39e79136a965f8a26dea4996fb317.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zKvtCFhcHpMCQhhjanq12w.png"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图5。使用扩张卷积进行密集特征提取[2]。</p></figure><p id="dc1f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">与图2中使用的标准卷积相比，在图5中可以看出，通过使用比率r=2的扩展核来提取密集特征。只需将<strong class="kq ja">膨胀</strong>参数设置为所需的膨胀速率，即可轻松实现膨胀卷积。</p><figure class="mi mj mk ml gt mm"><div class="bz fp l di"><div class="na nb l"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">扩展卷积:pytorch实现</p></figure><h1 id="ad00" class="lk ll iq bd lm ln lo lp lq lr ls lt lu kf lv kg lw ki lx kj ly kl lz km ma mb bi translated">深度方向可分离卷积</h1><p id="e4d3" class="pw-post-body-paragraph ko kp iq kq b kr mc ka kt ku md kd kw kx me kz la lb mf ld le lf mg lh li lj ij bi translated">深度方向可分离卷积是在例外网[3]中引入的。图6示出了标准卷积操作，其中卷积作用于所有通道。对于图6所示的配置，我们有256个5x5x3内核。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi nc"><img src="../Images/1b9454ef2cf57c48e1bdfc3432f46f31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*XHQj7F9r6iynLb7QWSreYw.png"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图6:应用于12x12x3输入的标准5x5卷积。</p></figure><p id="143b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">图7(a)示出了深度方向卷积，其中滤波器被应用于每个通道。这就是深度方向可分离卷积与标准卷积的区别。深度方向卷积的输出具有与输入相同的通道。对于图7(a)所示的配置，我们有3个5x5x1内核，每个通道一个。通道间混合是通过将深度方向卷积的输出与所需数量的输出通道的1×1内核进行卷积来实现的(图7(b))。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mt mu di mv bf mw"><div class="gh gi nd"><img src="../Images/a6ffc8c47de4870015b0f001030ce2a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UXHwtvGpSS34CK6I1Zu3Hg.png"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图7:5×5深度方向的可分离卷积，接着是1×1 conv。</p></figure><h2 id="b4d8" class="ne ll iq bd lm nf ng dn lq nh ni dp lu kx nj nk lw lb nl nm ly lf nn no ma iw bi translated">为什么选择深度可分卷积？</h2><p id="3c7d" class="pw-post-body-paragraph ko kp iq kq b kr mc ka kt ku md kd kw kx me kz la lb mf ld le lf mg lh li lj ij bi translated">为了回答这个问题，我们来看看执行标准卷积和深度可分卷积所需的乘法次数。</p><p id="ee23" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">标准卷积<br/> </strong>对于图6中指定的配置，我们有256个大小为5x5x3的内核。计算卷积所需的总乘法:<br/> <strong class="kq ja"> 256*5*5*3*(8*8个位置)= 1228800 </strong></p><p id="1124" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">深度方向可分离卷积<br/> </strong>对于图7中指定的配置，我们有2个卷积运算:<br/> 1) 3个大小为5×5×1的核。这里，所需的乘法次数是:<strong class="kq ja"> 5*5*3*(8*8个位置)= 4800 <br/> </strong> 2) 256个大小为1x1x3的核用于1x1卷积。所需乘法次数:<strong class="kq ja"> 256*1*1*3*(8*8个位置)= 49152 <br/> </strong>深度可分卷积所需乘法总数:<strong class="kq ja"> 4800 + 49512 = 54312。</strong></p><p id="1c2e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们可以非常清楚地看到<strong class="kq ja">深度方向卷积比标准卷积</strong>需要更少的计算。</p><p id="4a48" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在pytorch中，深度方向可分离卷积可以通过将组参数设置为输入通道的数量来实现。</p><figure class="mi mj mk ml gt mm"><div class="bz fp l di"><div class="na nb l"/></div></figure><p id="e954" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">注意:</strong>py torch中的<strong class="kq ja">组</strong>参数必须是<strong class="kq ja"> in_channels </strong>参数的倍数。这是因为在pytorch中，通过将输入特征分成<strong class="kq ja">组=g </strong>组来应用深度卷积。更多信息<a class="ae np" href="https://pytorch.org/docs/master/generated/torch.nn.Conv2d.html#torch.nn.Conv2d" rel="noopener ugc nofollow" target="_blank">点击这里</a>。</p><h1 id="87f7" class="lk ll iq bd lm ln lo lp lq lr ls lt lu kf lv kg lw ki lx kj ly kl lz km ma mb bi translated">结论</h1><p id="d0e4" class="pw-post-body-paragraph ko kp iq kq b kr mc ka kt ku md kd kw kx me kz la lb mf ld le lf mg lh li lj ij bi translated">这篇文章深入研究了两种流行的卷积类型:atrous(扩张)卷积和深度可分卷积。我们看到了它们是什么，它们与标准卷积运算有何不同，也看到了它们相对于标准卷积运算的优势。最后，我们还看到了如何使用pyTorch实现atrous(扩张)和深度方向可分离卷积。</p></div><div class="ab cl nq nr hu ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="ij ik il im in"><h1 id="87de" class="lk ll iq bd lm ln nx lp lq lr ny lt lu kf nz kg lw ki oa kj ly kl ob km ma mb bi translated"><strong class="ak">参考文献</strong></h1><p id="91d8" class="pw-post-body-paragraph ko kp iq kq b kr mc ka kt ku md kd kw kx me kz la lb mf ld le lf mg lh li lj ij bi translated">[1]卷积运算(<a class="ae np" href="https://github.com/vdumoulin/conv_arithmetic" rel="noopener ugc nofollow" target="_blank">https://github.com/vdumoulin/conv_arithmetic</a>)</p><p id="cf40" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[2]陈，梁杰，等，“Deeplab:基于深度卷积网、atrous卷积和全连通条件随机场的语义图像分割”<em class="oc"> IEEE模式分析与机器智能汇刊</em>40.4(2017):834–848。</p><p id="0078" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[3]弗朗索瓦·乔莱。"例外:具有深度可分卷积的深度学习."<em class="oc">IEEE计算机视觉和模式识别会议论文集</em>。2017.</p></div></div>    
</body>
</html>