<html>
<head>
<title>Batch, Mini-Batch and Stochastic Gradient Descent for Linear Regression</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归的批、小批和随机梯度下降</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/batch-mini-batch-and-stochastic-gradient-descent-for-linear-regression-9fe4eefa637c?source=collection_archive---------10-----------------------#2021-06-15">https://towardsdatascience.com/batch-mini-batch-and-stochastic-gradient-descent-for-linear-regression-9fe4eefa637c?source=collection_archive---------10-----------------------#2021-06-15</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="9100" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">三种基本梯度下降变体的实现和比较</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/5e2ee4dda28d2eb71c116008fe7669f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TgYIHm75rM_j_dpzM9Ds4Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片由来自Pixabay 的<a class="ae kv" href="https://pixabay.com/illustrations/banner-header-mathematics-formula-982162/" rel="noopener ugc nofollow" target="_blank"> geralt提供，由作者修改</a></p></figure></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h2 id="c40e" class="ld le iq bd lf lg lh dn li lj lk dp ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">1.介绍</h2><p id="49e3" class="pw-post-body-paragraph lz ma iq mb b mc md jr me mf mg ju mh lm mi mj mk lq ml mm mn lu mo mp mq mr ij bi translated">梯度下降算法是一种迭代一阶优化方法，用于找到函数的局部最小值(理想情况下是全局最小值)。它的基本实现和行为我已经在我的另一篇文章中描述过了。这一个集中在算法用来计算梯度和制作步骤的数据量的三个主要变量上。</p><p id="64a3" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">这三种变体是:</p><ul class=""><li id="9ea5" class="mx my iq mb b mc ms mf mt lm mz lq na lu nb mr nc nd ne nf bi translated">批量梯度下降(BGD)</li><li id="cc7f" class="mx my iq mb b mc ng mf nh lm ni lq nj lu nk mr nc nd ne nf bi translated">随机梯度下降</li><li id="527c" class="mx my iq mb b mc ng mf nh lm ni lq nj lu nk mr nc nd ne nf bi translated">小批量梯度下降(mBGD)</li></ul><p id="20a9" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">在本文中，我们将在一个简单的线性回归任务中看到它们的性能。</p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><p id="0a3c" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">简单回顾一下——一元线性函数定义为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/be299c0433ea59abcc1a02cb46f5aaba.png" data-original-src="https://miro.medium.com/v2/resize:fit:248/1*Uso3vDiONIkYgIwVcKp2BQ.gif"/></div></figure><p id="021e" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">它由两个系数参数化:</p><ul class=""><li id="34df" class="mx my iq mb b mc ms mf mt lm mz lq na lu nb mr nc nd ne nf bi translated"><em class="nm"> a0 </em> -偏差</li><li id="f9e5" class="mx my iq mb b mc ng mf nh lm ni lq nj lu nk mr nc nd ne nf bi translated"><em class="nm"> a1 </em> -函数的斜率。</li></ul><p id="3873" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">出于演示目的，我们定义以下线性函数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/9479bf853433b6c763def20f78fa04af.png" data-original-src="https://miro.medium.com/v2/resize:fit:284/1*qX50JbWA9J_kIj2oWb5aVQ.gif"/></div></figure><p id="b691" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">其中<strong class="mb ir"> σ </strong>是白(高斯)噪声。下面的代码为我们将要使用的数据集生成了100个点。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="no np l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/9dbd9e306b60176e026ac6346502fde0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1190/format:webp/1*VGIJyVKl_scgKlrVYfmuaA.png"/></div></figure><p id="253e" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">我们希望最小化的成本函数(指标)是<strong class="mb ir">均方误差</strong>，定义为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/ff59140a0f684ca4d5a84b5c9c1dc5dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:366/1*pguJQCtMGIfLp3CKxaR1-g.gif"/></div></div></figure><p id="1678" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">在一元函数的情况下，它可以明确地写成:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/6d71c0bd74579c320176e014595ea7e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:672/1*-sbFxY6mQXni2mhlhKQ3aw.gif"/></div></figure><p id="c088" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">下面的代码计算给定的一组两个参数的MSE成本。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="no np l"/></div></figure><p id="4b3d" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">注意，对于我们的原始系数，由于随机误差(白噪声),最小成本函数不为0(它将在每次运行时变化),此时等于:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/8db70d9439c72f501afdbc741b56a542.png" data-original-src="https://miro.medium.com/v2/resize:fit:210/1*Rnt8Mp9VO5N03lOvKg0J3Q.gif"/></div></figure><p id="3951" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">下图显示了最佳点附近的这个函数。我们可以看到它有一个细长的碗的形状。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/c28d23ddd709c3e6d4065ca9518cdcd3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dBMZz1OR1wipcQws-DN28A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">围绕全局最小值的成本函数；作者图片</p></figure><p id="671f" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">要使用任何梯度下降算法，我们必须计算这个函数的梯度。因为对于一元线性回归，我们的算法最小化2个系数，我们必须分别计算它们的导数。让我们注意到:</p><p id="7830" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">现在，使用<a class="ae kv" href="https://en.wikipedia.org/wiki/Chain_rule" rel="noopener ugc nofollow" target="_blank">链规则</a>我们获得以下结果:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/771a4f0e5bf5289f15bcef36d698a4c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:794/1*fDUUweo-ryoomF_nPpPJOA.gif"/></div></figure><p id="ae21" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">下一节将关注算法本身。使用的代码可以在<a class="ae kv" href="https://github.com/RobKwiatkowski/Gradient_Descent_Visualisations" rel="noopener ugc nofollow" target="_blank">我的GitHub库</a>上找到。</p><h2 id="e9f4" class="ld le iq bd lf lg lh dn li lj lk dp ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">2.批量梯度下降</h2><p id="8b40" class="pw-post-body-paragraph lz ma iq mb b mc md jr me mf mg ju mh lm mi mj mk lq ml mm mn lu mo mp mq mr ij bi translated">在批处理GD中，在每一步都使用整个数据集来计算梯度(记住:我们不计算成本函数本身)。下图显示了它在优化过程中的表现。它需要86次迭代来找到全局最优值(在给定的容差内)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/60aaad29dc14af4f8523dff2270e08b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*10LVAZdtk2928f_WjTnh8A.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">批量梯度下降过程的动画；作者图片</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nx"><img src="../Images/b97e5562c8ac8317fcb6206eded8446e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hLbdNvfS_xYEjXuKZixpxA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">批量梯度下降的轨迹；作者图片</p></figure><p id="3d1c" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">批量梯度下降的轨迹看起来很好——每一步都越来越接近最优，横向振荡随着时间的推移越来越小。这是它具有良好收敛速度的原因。</p><p id="92d5" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">为了准确地找到它的收敛速度，我们必须做一些数学。为了不过分复杂，让我们假设我们的成本函数是强凸的(两次可微的)并且具有<a class="ae kv" href="https://en.wikipedia.org/wiki/Lipschitz_continuity" rel="noopener ugc nofollow" target="_blank">一个Lipschitz连续</a>梯度，其中L &gt; 0定义为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/ab41a7c680d804e33e50f9474f792a39.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/1*c2eXtVCc9aM98Jbeq5mwYw.gif"/></div></figure><p id="de97" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">第二个假设限制了渐变的速度。</p><p id="a7e3" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">如果您可以计算L，那么您可以导出所谓的<strong class="mb ir">“保证进度的界限”</strong>，它是保证收敛的步长(学习速率):</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/bda9cb3a194f01f5ffe64e05b7eded59.png" data-original-src="https://miro.medium.com/v2/resize:fit:146/1*ySFQQ9XrfA5GQxN_k0Rzkw.gif"/></div></figure><p id="3ae1" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">然而，你不应该在实践中使用这个值，因为它真的很小而且收敛很慢。找到最佳学习率是一个巨大的话题，适合单独写一篇文章——只需检查一些东西，例如“<a class="ae kv" href="https://en.wikipedia.org/wiki/Backtracking_line_search" rel="noopener ugc nofollow" target="_blank">回溯线搜索</a>”，“阿米霍条件”或“<a class="ae kv" href="https://en.wikipedia.org/wiki/Wolfe_conditions" rel="noopener ugc nofollow" target="_blank">沃尔夫条件</a>”。</p><p id="bd65" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated"><strong class="mb ir">假设固定步长</strong>收敛速度取决于函数的凸性。</p><p id="6069" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">对于简单(弱)<strong class="mb ir">凸</strong>函数，收敛速度为[1]:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/bcec7a2805973cef4a050b6e6cab0545.png" data-original-src="https://miro.medium.com/v2/resize:fit:110/1*0dm9fP5rN0bkJsPrMPl93g.gif"/></div></figure><p id="4f30" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">其中k是迭代次数。该速率称为“亚线性收敛”，对于给定的容差ε，需要以下迭代次数才能收敛[1]:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/390af5a0d4c6edb19306b8ac9ca0a83c.png" data-original-src="https://miro.medium.com/v2/resize:fit:106/1*lNTRbkbTq3uh30mrI5oIwA.gif"/></div></figure><p id="fb1e" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">对于<strong class="mb ir">强凸</strong>函数，比率为[1]:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/d7848782174bbd44050a51dbb656e2ed.png" data-original-src="https://miro.medium.com/v2/resize:fit:90/1*Osvz9mpkv1nhC02CD4j6_Q.gif"/></div></figure><p id="03a4" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">其中0 </p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi od"><img src="../Images/7fd1f05a645f513d8d4650ecdde702c8.png" data-original-src="https://miro.medium.com/v2/resize:fit:182/1*gCvu-kE7VxgOiEoFvLfv7Q.gif"/></div></figure><p id="5a05" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">Pros and Cons of Batch Gradient Descent:</p><p id="8114" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated"><strong class="mb ir">优点:</strong></p><ul class=""><li id="68d7" class="mx my iq mb b mc ms mf mt lm mz lq na lu nb mr nc nd ne nf bi translated">一个简单的算法，只需要计算一个梯度</li><li id="2706" class="mx my iq mb b mc ng mf nh lm ni lq nj lu nk mr nc nd ne nf bi translated">在训练期间可以使用固定的学习速率，并且可以预期BGD收敛</li><li id="de58" class="mx my iq mb b mc ng mf nh lm ni lq nj lu nk mr nc nd ne nf bi translated">如果损失函数是凸的，非常快地收敛到全局最小值(对于非凸函数，非常快地收敛到局部最小值)</li></ul><p id="2543" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">缺点:</p><ul class=""><li id="fd1f" class="mx my iq mb b mc ms mf mt lm mz lq na lu nb mr nc nd ne nf bi translated">即使使用矢量化实现，当数据集很大时(大数据的情况)，速度也可能很慢</li><li id="ac0f" class="mx my iq mb b mc ng mf nh lm ni lq nj lu nk mr nc nd ne nf bi translated">不是所有的问题都是凸的，所以梯度下降算法不是通用的</li></ul><p id="561d" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">典型使用案例:</p><ul class=""><li id="71e4" class="mx my iq mb b mc ms mf mt lm mz lq na lu nb mr nc nd ne nf bi translated">适合计算机内存的小型数据库</li><li id="5b93" class="mx my iq mb b mc ng mf nh lm ni lq nj lu nk mr nc nd ne nf bi translated">凸成本函数的问题(如OLS，逻辑回归等。)</li></ul><h2 id="6ae4" class="ld le iq bd lf lg lh dn li lj lk dp ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">3.随机梯度下降</h2><p id="62ad" class="pw-post-body-paragraph lz ma iq mb b mc md jr me mf mg ju mh lm mi mj mk lq ml mm mn lu mo mp mq mr ij bi translated">随机梯度下降的思想不是使用整个数据集来计算梯度，而是仅使用单个样本。目标是加快这一进程。就选择样本而言，有两条主要规则:</p><ul class=""><li id="87d0" class="mx my iq mb b mc ms mf mt lm mz lq na lu nb mr nc nd ne nf bi translated">随机规则—随机选择的样本(可能重复)</li><li id="d879" class="mx my iq mb b mc ng mf nh lm ni lq nj lu nk mr nc nd ne nf bi translated">循环规则—每个样本一次(无重复或重复次数最少)</li></ul><p id="0903" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">随机规则更常见。</p><ul class=""><li id="5f31" class="mx my iq mb b mc ms mf mt lm mz lq na lu nb mr nc nd ne nf bi translated">下图显示了SGD如何收敛到最终解(示例性运行)。红点表示为给定步长计算选择的样本。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/cbfd937f9ffd9af020bc46c89cf9abec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/1*rLCX-QeQG6igkX8Zq3sAhw.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">SGS收敛过程的动画；作者图片</p></figure><p id="db27" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">由于其随机性，每次运行需要不同数量的步骤来达到全局最小值。在相同起点(0，0)和相同学习率(0.05)下运行100次所需的迭代直方图下方。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi of"><img src="../Images/6092fcbc6a87fa78b1395372c5a2cd33.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/1*mtx-omdlPTiUlmUkV85KGA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">收敛所需的迭代次数；作者图片</p></figure><p id="7e86" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">与批处理GD相反，它不会直接收敛到解，因为它每次迭代只使用1个样本，这意味着步骤非常嘈杂。但是，它的效率要高得多，CPU/GPU负载更少。这种影响对于小型数据库(像这样)几乎看不到，但在处理大数据时会对性能产生巨大影响。</p><p id="7c20" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">下图显示了上例中SGD步骤的轨迹。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/6c0063571b861a48a9d7ed671061f687.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9GzZbHZR5srOFiXUrCHsVA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">随机梯度下降的轨迹；作者图片</p></figure><p id="a216" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">固定步长随机梯度下降的收敛速度[1]:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/bcec7a2805973cef4a050b6e6cab0545.png" data-original-src="https://miro.medium.com/v2/resize:fit:110/1*0dm9fP5rN0bkJsPrMPl93g.gif"/></div></figure><p id="3539" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">这意味着SGD不像批量梯度下降那样具有线性收敛速度——仅仅意味着它需要更多的迭代(但不一定需要计算时间)。</p><p id="aade" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">随机梯度下降的利与弊:</p><p id="fc13" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated"><strong class="mb ir">优点:</strong></p><ul class=""><li id="70bc" class="mx my iq mb b mc ms mf mt lm mz lq na lu nb mr nc nd ne nf bi translated">对于大型数据集，比批量GD收敛更快(时间更少)</li><li id="1e74" class="mx my iq mb b mc ng mf nh lm ni lq nj lu nk mr nc nd ne nf bi translated">可以逃离局部极小值</li></ul><p id="2db8" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated"><strong class="mb ir">缺点:</strong></p><ul class=""><li id="8ecc" class="mx my iq mb b mc ms mf mt lm mz lq na lu nb mr nc nd ne nf bi translated">步骤更嘈杂— SGD可能需要更多迭代才能收敛到限制值</li><li id="f501" class="mx my iq mb b mc ng mf nh lm ni lq nj lu nk mr nc nd ne nf bi translated">它可以在全局最优值附近“跳跃”——它可能需要比批量GD更大的容差</li></ul><p id="0723" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">典型使用案例:</p><ul class=""><li id="af84" class="mx my iq mb b mc ms mf mt lm mz lq na lu nb mr nc nd ne nf bi translated">是用于训练人工神经网络的更高级随机算法的基础</li></ul><h2 id="8156" class="ld le iq bd lf lg lh dn li lj lk dp ll lm ln lo lp lq lr ls lt lu lv lw lx ly bi translated">4.小批量梯度下降</h2><p id="d74b" class="pw-post-body-paragraph lz ma iq mb b mc md jr me mf mg ju mh lm mi mj mk lq ml mm mn lu mo mp mq mr ij bi translated">小批量梯度下降是一种在纯SGD和批量梯度下降之间找到良好平衡的方法。想法是使用一个观察子集来更新梯度。每个尺寸所用的点数称为批量，一个批量的每次迭代称为一个时期。下面的动画显示了每个步骤中使用的点的收敛过程(批量大小为10)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/244b050f2e752039fe8c166e8a612520.png" data-original-src="https://miro.medium.com/v2/resize:fit:1080/1*l34RbraWYe3Q0K3JLWW2tw.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">小批量梯度下降的收敛过程:作者图片</p></figure><p id="5810" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">轨迹仍然是嘈杂的，但更稳定地走向最小值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/523e6284f419dab3de4dcf4048224ffa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CIGGHrHDun-2vjNBahkxvA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">小批量梯度下降的轨迹；作者图片</p></figure><p id="1953" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">该算法的收敛比介于BGD和mBGD之间，为[1]:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/c0cc742d1233549a189ebbbf533bd0b8.png" data-original-src="https://miro.medium.com/v2/resize:fit:256/1*RGFt8o0GTJ1tbpRz4neD8w.gif"/></div></figure><p id="dd09" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">其中b是批量大小。</p><p id="8e9a" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">小批量梯度下降的利与弊:</p><p id="5f4d" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated"><strong class="mb ir">优点:</strong></p><ul class=""><li id="c724" class="mx my iq mb b mc ms mf mt lm mz lq na lu nb mr nc nd ne nf bi translated">BGD和新加坡元之间在效率方面的良好平衡</li><li id="7b0d" class="mx my iq mb b mc ng mf nh lm ni lq nj lu nk mr nc nd ne nf bi translated">很容易放入计算机内存</li><li id="e68a" class="mx my iq mb b mc ng mf nh lm ni lq nj lu nk mr nc nd ne nf bi translated">可以避开局部最小值</li></ul><p id="ecee" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated"><strong class="mb ir">缺点:</strong></p><ul class=""><li id="008b" class="mx my iq mb b mc ms mf mt lm mz lq na lu nb mr nc nd ne nf bi translated">它仍然可以在全局最优值附近“反弹”——它可能需要比批量GD更大的容差，但小于SGD</li><li id="cf56" class="mx my iq mb b mc ng mf nh lm ni lq nj lu nk mr nc nd ne nf bi translated">另一个需要优化的超参数—批量</li></ul><p id="d89d" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">典型使用案例:</p><ul class=""><li id="d6c8" class="mx my iq mb b mc ms mf mt lm mz lq na lu nb mr nc nd ne nf bi translated">这是深度神经网络训练中非常常见的算法</li></ul><h1 id="ab5e" class="oi le iq bd lf oj ok ol li om on oo ll jw op jx lp jz oq ka lt kc or kd lx os bi translated"><strong class="ak"> 5。总结</strong></h1><p id="8d61" class="pw-post-body-paragraph lz ma iq mb b mc md jr me mf mg ju mh lm mi mj mk lq ml mm mn lu mo mp mq mr ij bi translated">我们经历了梯度下降算法的3个基本变体。在当代的ML中，使用了更先进和更有效的版本，但是仍然使用这里描述的基本思想。进一步的修改包括自适应学习率、各种动量(如内斯特罗夫)、平均等。</p><p id="c939" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">一些非常流行的实现有:</p><ul class=""><li id="23ab" class="mx my iq mb b mc ms mf mt lm mz lq na lu nb mr nc nd ne nf bi translated"><a class="ae kv" href="https://keras.io/api/optimizers/adam/" rel="noopener ugc nofollow" target="_blank">亚当</a></li><li id="8843" class="mx my iq mb b mc ng mf nh lm ni lq nj lu nk mr nc nd ne nf bi translated"><a class="ae kv" href="https://keras.io/api/optimizers/rmsprop/" rel="noopener ugc nofollow" target="_blank"> RMSprop </a></li><li id="46ea" class="mx my iq mb b mc ng mf nh lm ni lq nj lu nk mr nc nd ne nf bi translated">阿达格拉德<a class="ae kv" href="https://keras.io/api/optimizers/adagrad/" rel="noopener ugc nofollow" target="_blank">。</a></li></ul><p id="9b3b" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">有一个正在进行的研究工作，以进一步改善他们的非凸函数(深度神经网络)，其中包括各种想法，每个过程的数据。</p><p id="b439" class="pw-post-body-paragraph lz ma iq mb b mc ms jr me mf mt ju mh lm mu mj mk lq mv mm mn lu mw mp mq mr ij bi translated">如果你想了解更多关于本文主题的细节，我强烈建议你查阅这些阅读材料:</p><ol class=""><li id="7c54" class="mx my iq mb b mc ms mf mt lm mz lq na lu nb mr ot nd ne nf bi translated"><a class="ae kv" href="http://www.stat.cmu.edu/~ryantibs/convexopt/lectures/stochastic-gd.pdf" rel="noopener ugc nofollow" target="_blank">加州大学伯克利分校Ryan Tibshirani的随机梯度下降</a></li><li id="470e" class="mx my iq mb b mc ng mf nh lm ni lq nj lu nk mr ot nd ne nf bi translated"><a class="ae kv" href="https://www.stat.cmu.edu/~ryantibs/convexopt-F18/scribes/Lecture_24.pdf" rel="noopener ugc nofollow" target="_blank">加州大学伯克利分校Ryan Tibshirani的凸优化</a></li><li id="4e05" class="mx my iq mb b mc ng mf nh lm ni lq nj lu nk mr ot nd ne nf bi translated"><a class="ae kv" href="https://www.sciencedirect.com/science/article/abs/pii/S0893608017301399" rel="noopener ugc nofollow" target="_blank">用不一致随机梯度下降加速深度神经网络训练</a></li><li id="4401" class="mx my iq mb b mc ng mf nh lm ni lq nj lu nk mr ot nd ne nf bi translated"><a class="ae kv" href="https://www.sciencedirect.com/science/article/pii/S0885064X20300844" rel="noopener ugc nofollow" target="_blank">深度神经网络训练中随机梯度下降的不收敛性</a></li><li id="81f4" class="mx my iq mb b mc ng mf nh lm ni lq nj lu nk mr ot nd ne nf bi translated"><a class="ae kv" href="https://www.sciencedirect.com/science/article/abs/pii/S0925231219300578" rel="noopener ugc nofollow" target="_blank">带洗牌的分布式随机梯度下降的收敛性分析</a></li></ol></div></div>    
</body>
</html>