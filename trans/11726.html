<html>
<head>
<title>Decision Tree Models in Python — Build, Visualize, Evaluate</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的决策树模型—构建、可视化、评估</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/decision-tree-models-934474910aec?source=collection_archive---------5-----------------------#2021-11-22">https://towardsdatascience.com/decision-tree-models-934474910aec?source=collection_archive---------5-----------------------#2021-11-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="bf4a" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用Python的MITx Analytics Edge指南和示例</h2></div><p id="f228" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">分类和回归树(CART)可以被转换成用于预测分类的图表或规则集。当逻辑回归模型无法提供足够的决策界限来预测标签时，它们会有所帮助。此外，决策树模型更容易理解，因为它们模拟了人类的决策过程。此外，决策树回归可以捕捉非线性关系，从而允许更复杂的模型。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div class="gh gi lb"><img src="../Images/3ab1c11d1aca6442a35f3dd17a377d0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1168/format:webp/1*RcFivanny3i2RnBGPk0y8Q.jpeg"/></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">作者根据<a class="ae ln" href="https://unsplash.com/@emben?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">约翰·西门子</a>在<a class="ae ln" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片修改</p></figure><h2 id="0aac" class="lo lp iq bd lq lr ls dn lt lu lv dp lw ko lx ly lz ks ma mb mc kw md me mf mg bi translated">购物车模型是如何工作的？</h2><p id="b2e8" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">考虑两个独立变量X1和X2的情况。我们想预测结果是红色还是蓝色。CART试图将这些数据分割成子集，使每个子集尽可能地纯净或同质。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi mm"><img src="../Images/953637ea52a6be2c369948292fa21b27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6RMrFGwU-qH4nHW89xc0Xw.jpeg"/></div></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">决策树逻辑和数据分割-作者图片。</p></figure><p id="9197" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">第一次分割(split1)以这样的方式分割数据，如果变量X2小于60将导致蓝色结果，否则将导致查看第二次分割(<em class="mr"> split2 </em>)。<em class="mr"> Split2 </em>导向预测红色当<strong class="kh ir"> X1 &gt; 20 </strong>考虑<strong class="kh ir"> X2 &lt; 60 </strong>时。如果<strong class="kh ir">X2&lt;90</strong>split 3将预测蓝色，否则预测红色。</p><h2 id="c207" class="lo lp iq bd lq lr ls dn lt lu lv dp lw ko lx ly lz ks ma mb mc kw md me mf mg bi translated">如何控制模型性能？</h2><p id="50c2" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">在您通过学科知识或特征选择过程为模型选择了要考虑的变量后，您将需要定义最佳分割数。</p><p id="d096" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">分割的目标是增加每个节点结果的同质性。提高其对数据进行分类的能力。换句话说，每次拆分后增加纯度。如果我们预测蓝色和红色，如果可能的话，选择给出全部蓝色和全部红色的分裂数。选择将生成纯结果的拆分数量。</p><blockquote class="ms"><p id="6bde" class="mt mu iq bd mv mw mx my mz na nb la dk translated">纯节点是导致完美预测的节点。</p></blockquote><p id="deb9" class="pw-post-body-paragraph kf kg iq kh b ki nc jr kk kl nd ju kn ko ne kq kr ks nf ku kv kw ng ky kz la ij bi translated">但是如何量化拆分后的纯度，以确保我们有尽可能多的纯节点。</p><p id="0a07" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的目标是减少每次拆分后的不确定性。一个不好的拆分会让结局50%蓝50%红。例如，完美的分割将给出100%的蓝色。</p><p id="60c1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要衡量拆分在拆分后信息增加方面的表现，我们可以依赖以下指标:</p><p id="463b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">1 — <strong class="kh ir">熵</strong> [ <code class="fe nh ni nj nk b">entropy = -1*sum(p*log(p))</code> ]</p><p id="4c9f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2 — <strong class="kh ir">基尼杂质</strong> [ <code class="fe nh ni nj nk b">Gini = sum(p(1-p)), where p is the proportion of misclassified observation within the sub partition</code> ]</p></div><div class="ab cl nl nm hu nn" role="separator"><span class="no bw bk np nq nr"/><span class="no bw bk np nq nr"/><span class="no bw bk np nq"/></div><div class="ij ik il im in"><h1 id="e6a1" class="ns lp iq bd lq nt nu nv lt nw nx ny lw jw nz jx lz jz oa ka mc kc ob kd mf oc bi translated">例子:预测法官史蒂文斯的决定</h1><p id="f5fb" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">目标是预测史蒂文法官是否投票推翻法院判决，1表示投票推翻判决，0表示他支持法院的判决。</p><p id="4f87" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">代码和数据可在<a class="ae ln" href="https://github.com/muadelm/MITx_Analytics_Edge/tree/main/CART%20model_Supreme%20Court%20Forecasting" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir"> GitHub </strong> </a>获得。</p><p id="5a92" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">数据框显示如下，带有目标变量(反向)。</p><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi od"><img src="../Images/f70c5533e243d0edbbf0bf3a41729db1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sZoxcu7O0ixtlt4ahWNgYA.jpeg"/></div></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">史蒂文法官的法庭判决——图片由作者提供</p></figure><p id="506a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">重要提示</strong>:决策树(DT)既可以处理连续变量，也可以处理数值变量。但是如果您使用Python Scikit Learn，您可能会得到分类的ValueError。</p><p id="5c56" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这些要素具有许多分类值，我们将使用以下函数将其转换为数值:</p><pre class="lc ld le lf gt oe nk of og aw oh bi"><span id="50e0" class="lo lp iq nk b gy oi oj l ok ol"><strong class="nk ir">def</strong> convert_cat(df,col):<br/>    """<br/>    input: dataframe and col list of categorical columns<br/>    output: dataframw with numerical values<br/>    """<br/>    <strong class="nk ir">for</strong> c <strong class="nk ir">in</strong> col:<br/>        item_list <strong class="nk ir">=</strong> df[c]<strong class="nk ir">.</strong>unique()<strong class="nk ir">.</strong>tolist()<br/>        enum<strong class="nk ir">=</strong>enumerate(item_list)<br/>        d <strong class="nk ir">=</strong> dict((j,i) <strong class="nk ir">for</strong> i,j <strong class="nk ir">in</strong> enum)<br/>        print(c)<br/>        print(d)<br/>        <br/>        df[c]<strong class="nk ir">.</strong>replace(d, inplace<strong class="nk ir">=True</strong>)<br/>    <strong class="nk ir">return</strong> df</span><span id="cafc" class="lo lp iq nk b gy om oj l ok ol">convert_cat(df,['Circuit', 'Issue', 'Petitioner', 'Respondent',<br/>       'LowerCourt'])</span></pre><p id="f7c2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">将数据分为训练和测试</p><pre class="lc ld le lf gt oe nk of og aw oh bi"><span id="0d96" class="lo lp iq nk b gy oi oj l ok ol">X_train, X_test, y_train, y_test <strong class="nk ir">=</strong> train_test_split(X, Y, test_size<strong class="nk ir">=</strong>0.2, random_state<strong class="nk ir">=</strong>0)</span></pre><p id="0ed8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">基于训练数据构建决策树模型</p><pre class="lc ld le lf gt oe nk of og aw oh bi"><span id="f776" class="lo lp iq nk b gy oi oj l ok ol">clf <strong class="nk ir">=</strong> tree<strong class="nk ir">.</strong>DecisionTreeClassifier('gini', min_samples_leaf<strong class="nk ir">=</strong>30, random_state<strong class="nk ir">=</strong>0)<br/>clf <strong class="nk ir">=</strong> clf<strong class="nk ir">.</strong>fit(X_train, y_train)</span></pre><p id="0256" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">绘制决策树模型</p><pre class="lc ld le lf gt oe nk of og aw oh bi"><span id="c499" class="lo lp iq nk b gy oi oj l ok ol"><strong class="nk ir">from</strong> sklearn <strong class="nk ir">import</strong> tree <em class="mr"># for decision tree models</em></span><span id="8635" class="lo lp iq nk b gy om oj l ok ol">plt<strong class="nk ir">.</strong>figure(figsize <strong class="nk ir">=</strong> (20,16))<br/>tree<strong class="nk ir">.</strong>plot_tree(clf, fontsize <strong class="nk ir">=</strong> 16,rounded <strong class="nk ir">=</strong> <strong class="nk ir">True</strong> , filled <strong class="nk ir">=</strong> <strong class="nk ir">True</strong>);</span></pre><figure class="lc ld le lf gt lg gh gi paragraph-image"><div role="button" tabindex="0" class="mn mo di mp bf mq"><div class="gh gi on"><img src="../Images/ceb3e41af40bda5467bee48432a353b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sx40GqzerR8JFSRYM59bFw.jpeg"/></div></div><p class="lj lk gj gh gi ll lm bd b be z dk translated">决策树模型—作者图片</p></figure><p id="de81" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">使用分类报告评估模型。</p><pre class="lc ld le lf gt oe nk of og aw oh bi"><span id="3d0d" class="lo lp iq nk b gy oi oj l ok ol">report <strong class="nk ir">=</strong> classification_report(predTree, y_test)<br/>print(report)</span></pre><h2 id="5504" class="lo lp iq bd lq lr ls dn lt lu lv dp lw ko lx ly lz ks ma mb mc kw md me mf mg bi translated">参考</h2><p id="7f62" class="pw-post-body-paragraph kf kg iq kh b ki mh jr kk kl mi ju kn ko mj kq kr ks mk ku kv kw ml ky kz la ij bi translated">关于edX的MITx分析课程</p><div class="oo op gp gr oq or"><a href="https://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/" rel="noopener  ugc nofollow" target="_blank"><div class="os ab fo"><div class="ot ab ou cl cj ov"><h2 class="bd ir gy z fp ow fr fs ox fu fw ip bi translated">机器学习的分类和回归树-机器学习掌握</h2><div class="oy l"><h3 class="bd b gy z fp ow fr fs ox fu fw dk translated">决策树是预测建模机器学习的一种重要算法。经典的决策树…</h3></div><div class="oz l"><p class="bd b dl z fp ow fr fs ox fu fw dk translated">machinelearningmastery.com</p></div></div><div class="pa l"><div class="pb l pc pd pe pa pf lh or"/></div></div></a></div><div class="oo op gp gr oq or"><a href="https://blog.bigml.com/2016/09/28/logistic-regression-versus-decision-trees/" rel="noopener  ugc nofollow" target="_blank"><div class="os ab fo"><div class="ot ab ou cl cj ov"><h2 class="bd ir gy z fp ow fr fs ox fu fw ip bi translated">逻辑回归与决策树</h2><div class="oy l"><h3 class="bd b gy z fp ow fr fs ox fu fw dk translated">给定大量的模型，将哪种模型类型应用于机器学习任务可能是一个令人生畏的问题</h3></div><div class="oz l"><p class="bd b dl z fp ow fr fs ox fu fw dk translated">blog.bigml.com</p></div></div><div class="pa l"><div class="pg l pc pd pe pa pf lh or"/></div></div></a></div></div></div>    
</body>
</html>