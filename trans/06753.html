<html>
<head>
<title>Side-by-side OCR in Python with Google Vision and Tesseract</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的并行OCR与Google Vision和Tesseract</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/side-by-side-ocr-in-python-with-google-vision-and-tesseract-66021d5702a0?source=collection_archive---------9-----------------------#2021-06-18">https://towardsdatascience.com/side-by-side-ocr-in-python-with-google-vision-and-tesseract-66021d5702a0?source=collection_archive---------9-----------------------#2021-06-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="c91d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">关于如何使用一些流行的引擎在Python中执行OCR以及它们的特点和技巧的简短说明</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/e57cab9657b8526702e975d07c65b2d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*K_KgWESylxa2qdv8T0DAyQ.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><h2 id="1010" class="kv kw iq bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">现代光学字符识别系统</h2><p id="0c38" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">OCR(光学字符识别)系统将包含有价值信息(假定为文本格式)的图像转换为机器可读数据。在大多数情况下，通过一些可用的方法执行OCR是从纸质或基于扫描的PDF文档中提取数据的第一步。</p><p id="364d" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">然而，在网上进行短暂的搜索后，你可以找到大量各种开源和商业工具的链接，<a class="ae mk" href="https://cloud.google.com/vision" rel="noopener ugc nofollow" target="_blank">谷歌视觉</a>和<a class="ae mk" href="https://tesseract-ocr.github.io/" rel="noopener ugc nofollow" target="_blank">宇宙魔方</a>作为OCR引擎已经比他们的竞争对手领先了很长时间，尤其是在最近几年。</p><p id="8154" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">Tesseract是一个离线和开源的文本识别引擎，具有全功能的API，可以通过Python的一些包装器模块轻松实现到任何业务项目中，<a class="ae mk" href="https://pypi.org/project/pytesseract/" rel="noopener ugc nofollow" target="_blank"> <em class="mq"> pytesseract </em> </a>就是一个例子。</p><p id="bdfc" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">相反，Google Vision并不在本地运行，而是在远程Google的服务器上运行。要开始在您的项目中使用Google Vision API，您必须进行一些设置步骤，包括根据<a class="ae mk" href="https://cloud.google.com/vision/docs/setup" rel="noopener ugc nofollow" target="_blank">官方指南</a>提供有效凭证。此外，如<a class="ae mk" href="https://cloud.google.com/vision/pricing" rel="noopener ugc nofollow" target="_blank">谷歌的定价政策</a>所述，你可以为超出免费限制的文本识别请求付费。</p><p id="d456" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">尽管这两种工具在用法和选项设置上存在根本差异，但从谷歌趋势来看，它们对网络用户的兴趣几乎是一样的:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="2dfa" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">接下来，我们将尝试使用两个引擎在Python中执行OCR，顺便比较它们在真实图像上的性能(由作者重新创建或扫描以模仿不同初始质量的文档)。</p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><h2 id="98c1" class="kv kw iq bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">OCR比较方法综述</h2><p id="745e" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">为了获得尽可能可比的结果，我们将执行“反转”方法。这意味着，我们将首先对文本图像执行OCR，而不进行任何预处理，尝试从同一图像中反复应用不同的降级过滤器来机器读取字符。在每一步中，我们将评估OCR性能，将正确读取的字符数与两种工具在初始步骤中成功读取的字符数进行比较。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="f968" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">但是作为一个起点，我们将尝试在使用Google Vision和Tesseract在Python中执行OCR时，您应该注意的每个工具的一些奇怪之处。</p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><h2 id="9334" class="kv kw iq bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">宇宙魔方:连接你可能期望分裂的东西</h2><p id="0dd7" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">当您将Tesseract的<em class="mq"> image_to_data </em>方法应用于足够高质量的图像(即足够硬和清晰)时，事情可能看起来很自然。但是对于模糊的图像，这个工具试图不正确地确定文本边界框。这就是实际情况。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="60ba" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">更具体地说(注意一些边界框是如何相互重叠的):</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="75fa" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">为了检索这种奇怪的行为，我们必须手动迭代边界框，找到那些具有显著水平相交区域的边界框，并纠正它们的右边界。在绝大多数情况下，它会使边界框的宽度精确匹配所包含的字符串的长度。这个算法已经包含在Demoocr类的<em class="mq"> get_tes_ocr_data </em>方法中，允许我们在对修改后的图像执行OCR的每个步骤中获取调整后的数据。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div></figure></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><h2 id="9e51" class="kv kw iq bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">Google Vision:拆分你可能期望加入的内容</h2><p id="0e36" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">与Tesseract相反，Google Vision为已识别的文本实体提供了更加分散的边界框。请注意，它将作为标点符号阅读的字符从前面的单词中分离出来是多么有用和隐含。但是在某些情况下，这可能会被认为是不受欢迎的行为！在我们的特殊情况下，从后续数字的边界框中取出一个货币单位，这使得很难直接比较OCR引擎在给定区域内字符数量方面的性能。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div></figure></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><h2 id="8959" class="kv kw iq bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">OCR比较方法:更精细的点</h2><p id="55e2" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">由此可见，我们不仅要考虑在每一步图像修改中识别的字符数量，还要考虑它们的正确位置，即在初始步骤中确定的边界框。由于OCR引擎的输出有很大的不同，我们需要手动校正边界框，这次是通过以下方式合并所有重叠的多边形。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div></figure><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="ca07" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">接下来，我们将在这些基线边界内精确计算识别字符的数量。为了便于说明，正确读取的字符数将显示在图像上相应区域的不透明彩色横条上。堆叠的条越短，相应的OCR引擎在给定区域内成功读取的字符数量就越少。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div></figure></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><h2 id="235a" class="kv kw iq bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">把所有的放在一起</h2><p id="0ced" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">通过比较两种工具在使用不同强度的过滤器修改图像的每一步的OCR结果，我们有望获得谷歌视觉和宇宙魔方特定功能的感觉。下面是一个中间步骤的示例，以及这两种工具的OCR结果。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div></figure><p id="2137" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">下面是我们的比较方法:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mr ms l"/></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/e57cab9657b8526702e975d07c65b2d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*K_KgWESylxa2qdv8T0DAyQ.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="5d22" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">另一个例子是:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/8d608444e551379c10425611d120afb5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*iCHRZdgfflRHoEa0O4J_3w.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="02ba" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">还有一个。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/5ceec69ddfad53fbd26109aad96096ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*BHL9Rvrj7oOTL29uB9D8eQ.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="2c17" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">最后一个了。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/262eff87001d7ce6922567a1e97d1867.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*5rmWxyRm5vk_16noQlpzgQ.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><h2 id="92f6" class="kv kw iq bd kx ky kz dn la lb lc dp ld le lf lg lh li lj lk ll lm ln lo lp lq bi translated">带回家的笔记</h2><p id="a9f8" class="pw-post-body-paragraph lr ls iq lt b lu lv jr lw lx ly ju lz le ma mb mc li md me mf lm mg mh mi mj ij bi translated">如上所述，Google Vision和Tesseract都是成熟的工具，并且历史上已经在许多商业项目中找到了它们的方法。即使没有经过预处理，它们也能对质量尚可的文本图像进行充分的OCR。虽然这样的预处理用<em class="mq"> </em> <a class="ae mk" href="https://opencv.org/" rel="noopener ugc nofollow" target="_blank"> <em class="mq"> OpenCV </em> </a>或<a class="ae mk" href="https://pillow.readthedocs.io/en/stable/" rel="noopener ugc nofollow" target="_blank"> <em class="mq">枕</em> </a>似乎能显著提高对宇宙立方体的OCR结果。此外，您应该已经注意到，这两种工具对带有文本背景的图像的处理是多么不稳定。我们分析的这一发现与另一个比较中<a class="ae mk" href="https://fuzzylabs.ai/blog/the-battle-of-the-ocr-engines/" rel="noopener ugc nofollow" target="_blank">的见解有着特定的对应关系，但这也绝非详尽无遗。总而言之，这两个引擎都应该被认为是易于设置和使用的OCR工具，您的项目的正确选择在很大程度上取决于外部需求和预算。</a></p></div><div class="ab cl mt mu hu mv" role="separator"><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my mz"/><span class="mw bw bk mx my"/></div><div class="ij ik il im in"><p id="51ae" class="pw-post-body-paragraph lr ls iq lt b lu ml jr lw lx mm ju lz le mn mb mc li mo me mf lm mp mh mi mj ij bi translated">在<a class="ae mk" href="https://github.com/woldemarg/google_vision_tesseract_post" rel="noopener ugc nofollow" target="_blank"> Github </a>上查看完整的代码，其中包含一系列方便的OCR和图像预处理方法。</p></div></div>    
</body>
</html>