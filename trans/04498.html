<html>
<head>
<title>Demystifying Gaussian Mixture Models and Expectation Maximization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">揭秘高斯混合模型和期望最大化</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/demystifying-gaussian-mixture-models-and-expectation-maximization-a66575deaea6?source=collection_archive---------17-----------------------#2021-04-17">https://towardsdatascience.com/demystifying-gaussian-mixture-models-and-expectation-maximization-a66575deaea6?source=collection_archive---------17-----------------------#2021-04-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b161" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">以简化的方式解释高斯混合模型及其期望最大化的基本算法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/de40f86475b7efd3f5306f62bf1eafc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4zUYoLyygBgOnsMsY4aOFA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/photos/V6nzK_QYN4g?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditShareLink" rel="noopener ugc nofollow" target="_blank"> Aaron Burden拍摄，Unsplash </a></p></figure><p id="91f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在您学习了如何使用最简单的聚类算法<a class="ae ky" href="https://en.wikipedia.org/wiki/K-means_clustering" rel="noopener ugc nofollow" target="_blank"> k-means </a>对未标记数据点的样本进行聚类之后，我们开始看到k-means在将该技术应用于真实数据集时的一些缺点。ML工程师将采取的下一步是应用更复杂的算法来理解他/她的数据样本中的<a class="ae ky" href="https://en.wikipedia.org/wiki/Cluster_analysis" rel="noopener ugc nofollow" target="_blank">各种分组(集群)</a>，并且该算法很可能是高斯混合建模(GMM)。由于开源软件和多种ML框架的存在，比如<a class="ae ky" href="https://scikit-learn.org/stable/" rel="noopener ugc nofollow" target="_blank"> Scikit-learn </a>，使用这种算法只需要几行代码。然而，对许多人来说，理解这一过程中使用的数学基础是很难的。在本文中，我们将更直观地研究这个算法的内部工作原理。</p><h2 id="ea37" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated"><strong class="ak">GMM相对于k-means的优势</strong></h2><p id="f6c0" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">让我们开始构建高斯混合模型的概念，直接研究k-means的缺点。</p><ul class=""><li id="d9d8" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated"><strong class="lb iu">聚类大小不成比例:</strong> k-means的工作原理是将数据样本分配到最近的假设聚类中心，从而在超平面中形成<a class="ae ky" href="https://en.wikipedia.org/wiki/Voronoi_diagram" rel="noopener ugc nofollow" target="_blank"> Voronoi图</a>。因此，如果一个聚类的分布距离其正确的聚类中心太远，几个数据样本将会被错误分类。使用GMM，数据点在集群中的分布不需要从集群中心开始保持一致。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/d601f6bb3afd763a49e0fe703990f4be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*r2zKSMLWjK5wv0Ig_YSYTQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Voronoi分割重叠的GMM不同分布的集群(图片由作者提供)</p></figure><ul class=""><li id="6359" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated"><strong class="lb iu">不同形状的聚类:</strong>看待上述问题的另一种方式是k-means可能扰乱聚类的形状，尽管数据样本被紧密地打包在聚类内。随着聚类方向的改变，k-means的结果可能会显著改变。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nd"><img src="../Images/118165ef8528059ac6104c7a9832fa0c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cbrhFrELgUCj8da-B4CF7g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">在GMM形成的集群，集群中心和相应的Voronoi分区重叠(图片由作者提供)</p></figure><ul class=""><li id="fa65" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated"><strong class="lb iu">重叠聚类:</strong>有时，属于不同聚类的数据点可以非常紧密地打包，k-means算法会毫不犹豫地在样本分配到其聚类之间进行推测。k-means将在聚类之间绘制硬边界，即使样本可能无限接近该边界。对于GMM，你用一个假设来阐述你的模型，即集群中不存在硬边界，这将我们引向GMM的下一个主要优势。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ne"><img src="../Images/e96d17a0248bcdb37cf203bf86c2c8e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UuBvsou2KxB05ey0R7ZrkA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">GMM的重叠集群(图片由作者提供)</p></figure><ul class=""><li id="91bb" class="mt mu it lb b lc ld lf lg li mv lm mw lq mx lu my mz na nb bi translated"><strong class="lb iu">软分配:</strong>利用k-means的<a class="ae ky" href="https://en.wikipedia.org/wiki/K-means_clustering#Standard_algorithm_(naive_k-means)" rel="noopener ugc nofollow" target="_blank">标准算法</a>，你以100%的概率将一个数据点分配给一个聚类。但是，在GMM中，您可以为数据集中的每个数据样本分配一组可能性，以确定该数据样本属于系统中每个聚类的概率。例如，在下面由4个聚类组成的分布中，如果要求GMM对数据样本进行聚类，则为出现在中心的数据样本分配相等的概率(各25%)。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/7fa4ad96b926adfbc0fdb7b76e1c1292.png" data-original-src="https://miro.medium.com/v2/resize:fit:1222/format:webp/1*WyxqEwd7g-JOVgWI-21wkA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">GMM将做软作业(图片由作者提供)</p></figure><h2 id="fc23" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">正态分布</h2><p id="ea36" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">那么，从数学上来说，我们如何表示这些大小、形状、方向可能不同，并且可能相互重叠的集群的功能呢？正如这项技术的名称所暗示的，我们利用<a class="ae ky" href="https://en.wikipedia.org/wiki/Normal_distribution" rel="noopener ugc nofollow" target="_blank">高斯分布</a>来分别表示系统中的每个集群。在我们研究它是如何成为可能之前，让我们简单回顾一下1D的高斯分布。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ng"><img src="../Images/b011cd1c16f60ceea3c4ca4ed57d6b15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oprMDJmWvUseAW7b9xDonQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">各种高斯分布的PDF(图片来源:<a class="ae ky" href="https://en.wikipedia.org/wiki/Normal_distribution#/media/File:Normal_Distribution_PDF.svg" rel="noopener ugc nofollow" target="_blank">维基百科</a>)</p></figure><p id="aff1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里我们展示了一些高斯分布的概率密度函数(PDF)。该单峰函数的峰代表平均值，并且也出现在该分布的中心。这种分布在平均值周围的分布由标准差来确定。标准差的平方项被称为该分布的方差。</p><p id="0071" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是由于我们的数据集将处于更高维度(比如n)，我们将不得不利用n维高斯分布函数来表示我们的聚类。由于存在多个特征(由图像中的轴表示)对高斯的影响，我们将从简单的方差上升到协方差矩阵，这将有助于我们捕获关于分布(通过方差)和方向(通过相关性)的信息。</p><h2 id="eaf1" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">使用高斯函数创建集群</h2><p id="95b8" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">接下来让我们用高斯分布函数创建不同位置、大小、形状和方向的集群。让我们考虑我们有一个2维特征集，它将产生一个2×2维协方差矩阵。为了创建一个简单的圆形集群，这里是我们的协方差矩阵。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/03dbcb02071b1387fd8a11894f519d89.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*cVtuVBADdbm2qyqSk-74pw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">方差相等的协方差矩阵(图片由作者提供)</p></figure><p id="5e40" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对角线项表示轴的每个方向上高斯分布的方差，它们应该相等以得到圆形。现在，如果我们改变方差的尺寸，圆形将被转换成如图所示的椭圆。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/2ea89777ea40ea160ffeb6d86442a9fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*hOR-Pg5pCqT-7mfa7d5ZQw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">协方差矩阵中方差不相等的椭圆聚类(图片由作者提供)</p></figure><p id="b8f4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">上面的星团是在一个基于笛卡尔坐标的X-Y坐标系中排列的轴。为了改变规则轴对齐的方向，我们在非对角线位置添加相关项。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/c1184a9b79bfbf7cf9d6f08ed14d7ec6.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*K8VhGpD3FwiLHkwGe0GIeg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">带方向的椭圆群集(图片由作者提供)</p></figure><p id="707e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们如何改变这些集群的位置？我们利用n维平均向量(用μ表示)来实现这一点。例如，下面是数据样本的四个高斯模型聚类的集合，每个聚类都有其均值向量和协方差矩阵。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/27687274b63e1bcff759246b10beda7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:222/format:webp/1*_MjgIbWfzOENVY0-BNUJvw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">四个独立的高斯分布</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/d145527f8eece2e07116b59a169136cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1378/format:webp/1*J3zt0NiyfWR6CQ84fXP0wA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">四个分布上的四个独立集群(图片由作者提供)</p></figure><h2 id="5be2" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">我们如何组合这些高斯模型的集群？</h2><p id="dc32" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">虽然我们已经描述了上面的四个集群，但是我们是如何将它们组合在一起的呢？难道不应该有一种关系来捕捉每个集群相对于其他集群的重要性吗？这就引出了另一个由π表示的参数，它量化了系统中每个簇的权重的相对比例。由于这是一个相对标度，我们将确保系统中所有π的总和等于1。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/e426ddfcef41a3b7eff87570c75450e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:258/format:webp/1*uz4OuxdMv_h8kZtcDMCJew.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">比例权重参数</p></figure><p id="2e9c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，为了表示我们的聚类(或混合)分量，我们将不得不使用总共三个参数(均值μ、协方差∑和比例权重π),它们统称为聚类参数。</p><h2 id="c373" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">我们如何跟踪软分配？</h2><p id="56cc" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我们将不得不增加一个术语。我们需要表达将每个数据样本分配给系统中所有现有聚类的概率。我们将通过由rᵢₖ表示的责任向量来捕获该软分配，该责任向量超越了集群k将为数据样本I承担的责任量。由于这些是概率，与单个数据点相关联的责任的总和将等于1。所以，我们有，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/5af727791f283f844afd26d89771c122.png" data-original-src="https://miro.medium.com/v2/resize:fit:276/format:webp/1*aLAIdbEAI59ISaypFD8l4Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">软分配参数</p></figure><h2 id="2d64" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">模型的表述</h2><p id="5abd" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">既然我们已经有了表示集群的高斯分布，让我们从模型中提取一些有用的信息。我们将尝试回答的第一个问题是，如果你随机选择一个数据样本(比如样本I)，它以什么概率属于这些聚类中的每一个(比如总共k个聚类)？答案很简单，就是我们对每个集群的权重比例分量π，因为它转化为每个集群在系统中的重要性/优势。数学上，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/a8aca2b21bbc8eecb1c3bf0a40b98d35.png" data-original-src="https://miro.medium.com/v2/resize:fit:240/format:webp/1*I6SoIoFbfvIQos0pzElTYQ.png"/></div></figure><p id="73ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">换句话说，这个等式对应于将数据样本分配给聚类k的先验概率</p><p id="5a41" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们向前迈出一步。假设我们知道之前看到的iᵗʰ样本(由zᵢ表示)属于聚类k，那么让我们尝试计算数据样本zᵢ中存在的输入向量(由xᵢ表示)的特定配置属于聚类k的概率是多少？如果你想象一下，这个问题的答案等于聚类k的高斯分布函数，因为该分布函数换句话说是存在于其中的样本的个体集合。这里的样本是由输入向量组成的。所以数学上，我们有，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/6df0a81d426686175216439927410e51.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*15VGEAmC5cvdRD3s_fn80Q.png"/></div></figure><p id="83a7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假定zᵢ属于群集k，这个等式对应于xᵢ出现的可能性</p><p id="56f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们已经定义了由聚类参数和软分配组成的模型，我们需要设计一种算法来建立各种聚类参数之间的关系，并最终将软分配分配给所有单个数据点。让我们分两步利用假设的数学能力来解决这个问题。</p><h2 id="2186" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">步骤1:假设，集群参数是已知的</h2><p id="8427" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">尽管我们实际上并不知道，但让我们暂时假设，我们知道所有簇的簇参数{μ，∑和π}的值，因此高斯函数的分布是已知的。我们现在必须计算软分配概率。聚类k中的特定数据样本zᵢ的责任向量内的个体概率值转化为观察到聚类k中的数据样本zᵢ的可能性，该可能性被该聚类k的加权因子(先验概率)放大。因此，在数学上，这被表示为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi no"><img src="../Images/ac2197b98309334a4a53423008bf90e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:562/format:webp/1*PmGmNL1fX2QsYh6PO9aUjg.png"/></div></figure><p id="5b29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因为这是一个概率值，我们必须在责任向量的每个元素的所有可能的集群分配上标准化这个等式。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/3c78bf0b7eb668d73cb35a82eeb02ed6.png" data-original-src="https://miro.medium.com/v2/resize:fit:436/format:webp/1*5D4stQcxQaek0WTfD5G_rA.png"/></div></figure><h2 id="214e" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">步骤2:假设，软分配是已知的</h2><p id="b321" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">以前，我们假设集群参数是已知的，然后，我们找到了一种计算软分配的方法。现在让我们以相反的方式完成这一步。让我们通过假设软分配是已知的来找到计算集群参数的方法。</p><p id="85c9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了简化计算过程，我们将暂时考虑每个数据样本到一个聚类的分配是绝对的(硬的),类似于k-means。现在，我们需要分别使用属于每个聚类的数据样本来计算所有聚类的{μ，∑和π}。这是<a class="ae ky" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation" rel="noopener ugc nofollow" target="_blank">最大似然估计</a>的直接应用，我们试图找到目标函数中参数的最佳值，然后用最佳值拟合函数。</p><p id="8de1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，我们将采取一种捷径，因为我们已经知道高斯分布中的平均向量和协方差矩阵的值，并且我们将不使用带有优化器的MLE的标准过程来导出这些值。因此，均值和协方差的估计值为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/46d1f3cc0259cba7a23fef871449a21b.png" data-original-src="https://miro.medium.com/v2/resize:fit:532/format:webp/1*p206w4PN2q_Ge--rNc_I0g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用硬分配估计聚类参数</p></figure><p id="d433" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">Nₖ表示聚类k中样本的数量，n对应于数据集中样本的总数。π的估计聚类比例参数是属于聚类k的数据样本出现的直接概率值。因此，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/01101dec5c0642a50cd5f2f511af4700.png" data-original-src="https://miro.medium.com/v2/resize:fit:134/format:webp/1*OgQOG9w8r9J4dcJJBa1yjQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用硬分配的估计比例权重</p></figure><p id="5761" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在估计的簇参数方程准备好了，让我们假设簇分配是软的而不是硬的或绝对的来解决原始问题。为此，我们可以使用硬赋值在已经导出的方程的基础上构建。在硬任务中，我们对xᵢ在群集k中的全部观察进行了处理，但现在我们只需缩减其相对责任份额(0.0到1.0)的绝对(100%或0%)贡献。具有软分配的更新的集群参数估计将是:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/d5fe3ce27d19c792b02b4485d6239e5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:598/format:webp/1*uWnyrgn4NDVOFQNsxkc61g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用软分配估计集群参数</p></figure><p id="a637" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">出于好奇，您可以通过将责任向量更改为absolute (1.0或0.0)，将这些等式逆向工程为硬分配的等式。</p><h2 id="53bb" class="lv lw it bd lx ly lz dn ma mb mc dp md li me mf mg lm mh mi mj lq mk ml mm mn bi translated">期望值最大化算法</h2><p id="93b0" class="pw-post-body-paragraph kz la it lb b lc mo ju le lf mp jx lh li mq lk ll lm mr lo lp lq ms ls lt lu im bi translated">我们已经观察到，如果我们将我们的集群参数固定为某些值，我们可以在该时间步长瞬间计算软分配。类似地，当我们固定软分配时，我们可以在该时间步获得集群参数值。这形成了两个基本步骤，我们可以迭代运行有限次来创建期望最大化算法。在这一点上唯一的挫折是我们应该从什么值开始时间步长0。就像在k-means聚类过程的开始，我们可以随机分配一些聚类参数，然后开始EM算法。</p><p id="24c0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们正式定义我们看到的两个步骤。</p><p id="edbf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">电子步骤</strong>:这在我们解释的步骤1中有描述。在这一步中，我们估计在该时间步中给定聚类参数估计的聚类责任向量。</p><p id="c136" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> M步</strong>:这在我们解释的第2步中有描述。在这一步中，我们最大化该时间步中给定责任向量的聚类参数的可能性。</p><p id="b407" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们反复不断地重复上述步骤，直到聚类参数和软分配的似然值收敛到局部模式。更简单地说，这意味着当分别在E-step和M-step之间迭代时，软分配和集群参数值的振荡低于阈值水平。</p><p id="3918" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这篇文章中，我们探讨了无监督学习方法的聚类与高斯混合建模及其潜在的算法的期望最大化。请分享你对这篇文章内容的看法。</p></div></div>    
</body>
</html>