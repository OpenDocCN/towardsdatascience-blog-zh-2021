<html>
<head>
<title>A Hitchhiker’s Guide to Sentiment Analysis using Naive-Bayes Classifier</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用朴素贝叶斯分类器的情感分析指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-hitchhikers-guide-to-sentiment-analysis-using-naive-bayes-classifier-b921c0fb694?source=collection_archive---------19-----------------------#2021-05-14">https://towardsdatascience.com/a-hitchhikers-guide-to-sentiment-analysis-using-naive-bayes-classifier-b921c0fb694?source=collection_archive---------19-----------------------#2021-05-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1abd" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">自然语言处理中最优美的定理之一的教程。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/65111f424af5d94707ebd97030b4373b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Zpl6opHBZPpQu6RHkxvn3Q.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自<a class="ae kv" href="https://unsplash.com/photos/g5gia1p67hE" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="767b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">分类</strong>是机器学习和人类智能的核心。识别声音、面孔或者我们每天看到的图像都属于分类的范畴。现在来看朴素贝叶斯，对于任何开始从事自然语言处理领域的人来说，这是朝着这个目标迈出的第一步；这就是为什么我们了解如何实现这一点以及发动机罩下发生了什么是至关重要的。在这篇文章中，我们将学习如何从头开始实现朴素贝叶斯，并将其用于情感分析。</p><p id="758c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">指数:</strong></p><ol class=""><li id="2983" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated"><strong class="ky ir">情感分析</strong></li><li id="aca5" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">术语</strong></li><li id="2b3b" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">朴素贝叶斯定理</strong></li><li id="225a" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">推导</strong></li><li id="0111" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">训练模型</strong></li><li id="d619" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">已解决的示例</strong></li><li id="7371" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr lx ly lz ma bi translated"><strong class="ky ir">结论</strong></li></ol><h2 id="ca09" class="mg mh iq bd mi mj mk dn ml mm mn dp mo lf mp mq mr lj ms mt mu ln mv mw mx my bi translated"><strong class="ak">情绪分析</strong></h2><p id="86ab" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">现在，情感分析，顾名思义，基本上是我们对陈述的情感进行分类的任务，或者更简单地说，是一个特定陈述试图传达的情感；无论它是正面的还是负面的，悲伤的还是讽刺的；侮辱性的或者有益健康的或者善良的。让我们再举几个例子来详细说明这一点；</p><p id="7d6e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设镇上有一家新餐馆，你和你的朋友决定在那里吃晚餐，并体验这个地方。你真的玩得很开心，当你的朋友问你的意见时，你会说一些类似的话:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ne"><img src="../Images/8f96945cee2e661fd6a08bf7a5b63626.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1F7pWpwSJwHo9VaKKjspyg.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自<a class="ae kv" href="https://unsplash.com/photos/Gg5-K-mJwuQ" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="b778" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="nf"> 1。“这个地方太棒了。食物很美味，气氛也很愉快</em></p><p id="b8d7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在你的朋友有相反的信仰。这个新地方不适合他。所以当你反过来询问他的意见时，他会说:</p><p id="f9d1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2.这个地方真可怜。食物很难吃，环境让我无法忍受，让我很不舒服</p><p id="fccb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我想我们都同意，陈述1表达了一种积极的情绪，而陈述2代表了一种消极的情绪或情感。同样重要的是，我们要注意的是，到底是什么给了我们线索，让我们能够将句子分为肯定和否定。</p><p id="3f56" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">答案？线索…</p><p id="5452" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你仔细观察…在句子中有一些特定的词支配着情感。正面词汇如<strong class="ky ir"> <em class="nf">棒极了，好吃，过瘾</em> </strong> <em class="nf"> </em>等。使句子积极，而像<strong class="ky ir"> <em class="nf">可怜的、可怕的、不舒服的</em> </strong>等词语。把这个句子变成否定句。事实上，如果我们在一个句子中替换这些特殊的提示，它会完全改变它的意思。让我展示给你看:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/edafadf501688229b86da37a4cf14f9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oyHfVbHUey328QXGU2zlfg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="c92f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在每一个例子中，如果你注意到，通过改变一个关键的单词改变了整个句子的味道，而句子的其余部分保持不变。这就是情感分析的本质。此外，我们将把我们的讨论限制在<strong class="ky ir">二元分类</strong>上，将句子分类为肯定或否定。</p><h2 id="9a1b" class="mg mh iq bd mi mj mk dn ml mm mn dp mo lf mp mq mr lj ms mt mu ln mv mw mx my bi translated"><strong class="ak">术语</strong></h2><p id="3c05" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">在深入研究数学之前，我们先来谈谈术语。我们在这里要做的属于<strong class="ky ir">监督机器学习</strong>；这意味着我们将被提供训练输入，并且这些输入中的每一个都将与其正确的输出相关联。现在你的模型的工作是理解这些数据，观察和分析给定的输入和输出之间的关系，并最终以合理的准确度预测输出；给定新的输入。</p><p id="e6e0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">走得更远；我们通常用x表示输入，用y表示输出，其中y ∈[y，yb，yc，…..yn]类，你的模型的任务是预测一个特定的输入x属于哪个输出类。现在，在这种情况下，我们将处理单词和句子，所以在术语上有一点变化。对于由多个特征组成的文档(基本上是句子)，我们的输入将是'<strong class="ky ir"> <em class="nf"> d </em> </strong>'，对于类，我们的输出将是'<strong class="ky ir"> <em class="nf"> c </em> </strong>'在这里，类将代表积极(积极情绪)或消极(消极情绪)。</p><p id="246d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以最后我们会得到一个输入d，我们的模型必须学会预测它属于哪个类，c。</p><p id="732c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">既然我们已经完成了本质和细节，让我们从定理开始。</p><h2 id="90ad" class="mg mh iq bd mi mj mk dn ml mm mn dp mo lf mp mq mr lj ms mt mu ln mv mw mx my bi translated"><strong class="ak">朴素贝叶斯定理</strong></h2><p id="eafa" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">我们先来看看<strong class="ky ir">贝叶斯定理:</strong></p><p id="b2ef" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">P(A | B) = P(B | A) * P(A) / P(B) -&gt;(1)</p><p id="f478" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们来看看这些术语:</p><ul class=""><li id="ea56" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr nh ly lz ma bi translated">P(A | B) =假设事件B发生，事件A发生的概率</li><li id="2095" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr nh ly lz ma bi translated">P(B | A) =假设事件A发生，事件B发生的概率</li><li id="8569" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr nh ly lz ma bi translated">P(A) =事件A发生的概率</li><li id="79af" class="ls lt iq ky b kz mb lc mc lf md lj me ln mf lr nh ly lz ma bi translated">P(B) =事件B发生的概率</li></ul><p id="dc40" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，贝叶斯定理给了我们一种寻找条件概率的方法。贝叶斯定理是朴素贝叶斯定理的核心。</p><p id="36fc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在我们能够描述多项式朴素贝叶斯定理了。顾名思义，这个定理使用了一个贝叶斯分类器，并对特征之间的相互作用进行了简化。</p><p id="16c2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们在朴素贝叶斯中考虑的最重要的假设之一叫做<strong class="ky ir">词袋。</strong>这意味着算法真正关心的是单词及其频率，即该单词在我们的数据中出现了多少次。这个词在我们的句子(文档)中的位置一点也不重要。我们只需要记录特定单词在文档中出现的次数。就是这样。</p><p id="11a3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我举个例子解释一下:</p><p id="5e50" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">”<em class="nf">茶让我快乐。红茶、绿茶、奶茶，不管是哪一种；只要是茶我就满足了。我是喝茶长大的，每喝一口都会让我想起过去的美好时光”</em></p><p id="8d34" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们来进一步分析一下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/067eb6c5be0187e96969e56ebe276ac0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KlZogKOs83E1T-PFGeGmvg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="0ee8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这基本上就是单词袋的概念。哪里有<em class="nf">茶、我、快乐、满足</em>等字眼无关紧要。被用在句子中，重要的是它的频率。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nj"><img src="../Images/41198258312fe091c1523997b6bd84bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FECFHfZVPk0Cb5eYRoGTxA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图片来自<a class="ae kv" href="https://unsplash.com/photos/GkinCd2enIY" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><h2 id="d391" class="mg mh iq bd mi mj mk dn ml mm mn dp mo lf mp mq mr lj ms mt mu ln mv mw mx my bi translated"><strong class="ak">推导</strong></h2><p id="956d" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">现在让我们试着用数学公式来表达。</p><p id="2b76" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果您还记得，我们的主要目标是找到给定特定句子(文档)的类别(积极或消极情绪)。所以我们可以这样解决这个问题:</p><p id="cac0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">1.假设我们有一组可能的类c。</p><p id="beab" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2.我们发现一个文档在一个特定类中的概率。</p><p id="1114" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">本质上是给定一个文档的一个类的条件概率。</p><p id="fb00" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3.我们迭代所有的类，找到哪个类的条件概率最大；给我们答案。</p><p id="4906" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将所有步骤结合在一起，我们得到:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/2b02629c09b3a8780f8f8386c097f0d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y6IhsCE_PNtQU8OeoKJRKg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="1dec" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里，术语ĉ表示具有最大概率的类。</p><p id="3a06" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们已经了解了贝叶斯定理。所以现在我们可以把条件概率的公式代入等式(2)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/cc6141e52118b272ce617c2417e47bd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1360/format:webp/1*qwUpVLGTAu8VG-i8v9f51A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="64e8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以进一步简化这个等式。在对类进行迭代时，我们的文档当然不会改变，只有类会改变；所以我们可以安全地从分母中去掉P(d ),而不会引起任何大问题。因此，我们修改后的方程变成:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/a22505fbd6d8c07d27f6e011812371c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1376/format:webp/1*M2lALOx-mCVKDwnTLdTv3g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="e4e1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">术语P(d|c)称为<strong class="ky ir">似然概率</strong></p><p id="2098" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">第二项P(c)称为<strong class="ky ir">先验概率</strong></p><p id="db8a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以通过将每个文档分成一组特性来进一步简化它<em class="nf"> f1，f2，f3，…..fn。</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nn"><img src="../Images/601b8949e568026f89a15bc0755bb76d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*MvEG8JNxfhSaknHG92U7ew.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="2fc1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我们推导的这一点上，我们将做一个非常重要的假设。我们将假设给定的每个特征f的概率是相互独立的。这是非常关键的一步，它大大降低了我们问题的时间复杂度。让我们多理解一点。</p><p id="9220" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果两个事件X和Y彼此独立，则事件一起发生的概率(P(X和Y))变为:</p><p id="8798" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">P( X ∩ Y) = P(X) * P(Y)</p><p id="a5d5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这意味着:</p><p id="485e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">P( f 1 | c ∩ f 2 | c) = P(f 1 | c) * P(f 2 | c)</p><p id="33d5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以进一步简化方程(5 )!此外，假设事件彼此独立，我们不必考虑每个特征如何与另一个特征相关，或者一个特征在给定另一个特征的情况下发生的概率。这为我们节省了大量的计算能力。</p><p id="5116" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，我们的最终等式变成:</p><p id="1dbf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">P( f 1，f 2，…，f n | c)= P(f 1 | c)P(f 2 | c)…P(f n | c)-&gt;(6)</p><p id="9c8e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">或者</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi no"><img src="../Images/343d3fc6d09321044786d44673310f60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1354/format:webp/1*pV7z10F7i6kt98zfFfT0Mg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="5b8f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当然，一个句子中的特征是它的单词……因此，如果我们将等式中的特征替换为第I个位置的单词的<em class="nf"> wi </em>,我们可以如下重新构建等式:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi np"><img src="../Images/03fca6fc3737693e9a1a2d9f71847aef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uT-XIRDcpknpnfQEyJkbUA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="dfad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">唷！！！！我们终于完成了推导。现在，让我们转到如何在实际问题中应用这个概念。</p><h2 id="1ade" class="mg mh iq bd mi mj mk dn ml mm mn dp mo lf mp mq mr lj ms mt mu ln mv mw mx my bi translated"><strong class="ak">训练模特</strong></h2><p id="63e4" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">1.<strong class="ky ir">计算先验概率</strong>。我们将首先找到属于每个类的文档的数量。找到每一类中文档的百分比将会给我们所需的先验概率。</p><p id="f520" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们假设类<em class="nf"> c </em>中的文档数量是<em class="nf"> Nc。</em></p><p id="c1f5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">假设文档总数为<em class="nf">n。</em></p><p id="b9ff" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">所以，P(c)=<em class="nf">Nc/n total-&gt;</em>(9)</p><p id="1709" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2.<strong class="ky ir">计算</strong> <strong class="ky ir">似然概率</strong>。这是有点棘手的地方。我们的主要目标是找到单词wi在所有c类文档的所有单词中出现的次数。我们首先将所有c类文档连接成一个大的“c类”文本。然后我们使用wi在这个连接文档中的频率来给出似然概率。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/c5e764872006b8d51bda9b3e7a1b4907.png" data-original-src="https://miro.medium.com/v2/resize:fit:1342/format:webp/1*pWo8uekXtbaXvtSgxtkaGQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="b1f3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里V代表词汇表，它是所有文档中所有单词的集合，不考虑类别</p><p id="2791" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，此时我们将面临一个非常独特的问题。假设我们输入的文档是，</p><p id="d415" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我喜欢那部电影</p><p id="9811" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">单词“loved”只出现在积极的类别中，而没有“loved”的例子出现在消极的类别输入中。现在，根据等式(8)，我们必须通过乘以每个类别的似然概率来找到概率。如果我们计算出单词“loved”对于类别“negative”的可能性概率，我们得到:</p><p id="8060" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">p("爱过" | "负")= 0</p><p id="353f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，如果我们把这个值代入方程(8)，我们的类“负”的整个概率变为零；不管其他值是什么？</p><p id="fb94" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了解决这个问题，我们将在分子和分母中引入一个附加项，<strong class="ky ir">拉普拉斯平滑系数</strong>。我们的等式将被修改如下:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ni"><img src="../Images/62d07615768d9faf60d6264ffd189ab9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uh4VJ42XKKA8c31VZHBN2Q.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="9173" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里a是拉普拉斯平滑系数。我们通常认为它的值是1。</p><p id="64fe" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3.<strong class="ky ir">在eqt(8)中插入先验和似然概率。</strong></p><p id="7140" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">既然我们已经计算了先验概率和似然概率，我们就可以简单地继续下去，把它代入。</p><p id="6096" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">不过，我们有几种方法可以优化这一过程:</p><p id="4fbf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">a.<strong class="ky ir">使用对数</strong>:如果我们在方程(8)的两边都应用对数，我们可以将方程转换成特征的线性函数，这将大大提高效率。</p><p id="75e5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是我们的原始方程:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nr"><img src="../Images/c4d7b6350db55abaf1d3f598a9dba52f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nvFQ_Fs6r2vxTwjJWFOoyA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="28f1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，如果我们对两边都应用对数，我们会得到一个线性函数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nk"><img src="../Images/e705a49d7dba26e409afd45958d7e022.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UUBWLmE45TtQftLvTboT8A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="f105" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">b.<strong class="ky ir">停用词</strong>:像<em class="nf">这个、那个、安、曾、当</em>等词。做</p><p id="5898" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通常不会有助于陈述的情绪。我们可以完全去掉它们来简化我们的模型训练。</p><p id="4d04" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">c.<strong class="ky ir">未知单词:</strong>每当你面对一个出现在测试数据集中，但在从训练数据创建的词汇中不存在的单词时，建议完全丢弃这些单词，并且在概率计算中不考虑它们。</p><p id="5a39" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">d.二进制多项式朴素贝叶斯:这是多项式朴素贝叶斯的一个略微修改的版本。在这里，我们将更重视一个词是否存在，而不是它的出现频率。正如我们已经看到的，单个单词可以引起句子情感的巨大变化，因此，忽略该特定单词在句子中出现的次数，而专注于该特定单词在文档中是否存在，这将是一种合乎逻辑的方法。</p><h2 id="2c37" class="mg mh iq bd mi mj mk dn ml mm mn dp mo lf mp mq mr lj ms mt mu ln mv mw mx my bi translated"><strong class="ak">已解决的示例</strong></h2><p id="64cc" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">最后，现在我们已经熟悉了朴素贝叶斯分类器，我们可以在一个例子中实现这些知识。</p><p id="c0f4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">训练数据集:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/772520ab7548bfa6768c0d74e3925158.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*03XeZQLwvssSAXg5XsDf3w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">autor提供的图像</p></figure><p id="428a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">测试数据集:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/3e1373fadd9e04349ba24bc212431c14.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*XLj3HTB-j-blswiquPsLaA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">作者图片</p></figure><p id="3485" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是一个虚构的电影评论数据集。电影评论被分别分为正面和负面两类。</p><p id="1822" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们解决问题:(我们将考虑平滑系数，<em class="nf"> a，</em>为1)</p><p id="d318" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="nf">对于第一个测试用例:</em></p><p id="1a0f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">先验概率:</strong></p><p id="c1a2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">P(c = '正')= 3/6 = 1/2，P(c = '负')= 3/6 = 1/2</p><p id="80d4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">似然概率:</strong></p><p id="fdcd" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">P('Great' | c = '正数')= 1 + 1 / (9 + 19) = 0.0714</p><p id="1255" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">p('电影' | c = '正')= 2 + 1 /(9 + 19) = 0.1071</p><p id="6871" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">P('Great' | c = '负数')= 0 + 1/(10 + 19) = 0.0344</p><p id="99e7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">P('movie' | c = '负数')= 0 + 1/(10 + 19) = 0.0344</p><p id="a180" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我们应用等式(8)中的发现，返回最大概率:</p><p id="66bc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">P(c = '正')* P('伟大' | c = '正')* P('电影' | c = '正')= 0.5 * 0.0714 * 0.1071 = 0.00382</p><p id="6ea3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">P(c = '负')* P('伟大' | c = '负')* P('电影' | c = '负')= 0.5 * 0.0344 * 0.0344 = 0.000591</p><p id="7443" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由此我们可以说这个测试用例属于<strong class="ky ir"> <em class="nf">正向</em> </strong>类。</p><p id="010e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="nf">对于第二个测试用例:</em></p><p id="4534" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">先验概率:</strong></p><p id="b562" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">P(c = '正')= 3/6 = 1/2，P(c = '负')= 3/6 = 1/2</p><p id="6f25" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这个例子中我们遇到了未知单词:<em class="nf"> this，is。我们在计算中不会考虑它们。</em></p><p id="08e0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">似然概率:</strong></p><p id="7ae9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">P('boring' | c = '正数')= 0 + 1 / (9 + 19) = 0.03571</p><p id="5cd1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">p('可悲' | c = '正')= 0 + 1 /(9 + 19) = 0.03571</p><p id="178c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">P('and' | c = '正数')= 0 + 1 /(9 + 19) = 0.03571</p><p id="5894" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">P('boring' | c = '负数')= 1 + 1 / (10+ 19) = 0.0689</p><p id="9ec1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">p('可悲' | c = '负')= 1 + 1 /(10 + 19) = 0.0689</p><p id="2b77" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">P('and' | c = '负数')= 1 + 1 /(10 + 19) = 0.0689</p><p id="a391" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我们应用等式(8)中的发现，返回最大概率:</p><p id="ce5b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">P(c = '正')* P('无聊' | c = '正')* P('可悲' | c = '正')* P('和' | c = '正')= 0.5 * 0.03571 * 0.03571 * 0.03571 = 0.00002277</p><p id="a4b4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">P(c = '负')* P('伟大' | c = '负')* P('电影' | c = '负')= 0.5 * 0.0689 * 0.0689 * 0.0689 = 0.00016354</p><p id="87a4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由此我们可以说这个测试用例属于<strong class="ky ir"> <em class="nf">负</em> </strong>类。</p><h2 id="65d9" class="mg mh iq bd mi mj mk dn ml mm mn dp mo lf mp mq mr lj ms mt mu ln mv mw mx my bi translated"><strong class="ak">结论</strong></h2><p id="c971" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">情感分析广泛应用于社交媒体监控、市场研究等领域。它是自然语言处理最重要的方面之一。朴素贝叶斯分类器是自然语言处理的第一步。希望你喜欢阅读这篇文章。</p><p id="df11" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">实现二进制多项式朴素贝叶斯分类器的python代码可以在我的<a class="ae kv" href="https://github.com/19-ade/Binary_multinomial_naive_bayes" rel="noopener ugc nofollow" target="_blank"> github repo </a>中找到。数据集也包含在回购中。</p></div></div>    
</body>
</html>