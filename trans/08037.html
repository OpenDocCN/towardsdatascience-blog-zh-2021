<html>
<head>
<title>Using a Dataloader in Hugging Face</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在拥抱脸中使用数据加载器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/using-a-dataloader-in-hugging-face-52388f552259?source=collection_archive---------15-----------------------#2021-07-23">https://towardsdatascience.com/using-a-dataloader-in-hugging-face-52388f552259?source=collection_archive---------15-----------------------#2021-07-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="2cbb" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">PyTorch版本</h2></div><p id="1701" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">每一个深入DL世界的人都可能听说，相信，或者是被试图说服的目标，这是一个<a class="ae lb" href="https://arxiv.org/pdf/1706.03762.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="kh ir">变形金刚</strong> </a>的时代。自从第一次出现，<strong class="kh ir">变形金刚</strong>就成了几个方向大量研究的对象:</p><ul class=""><li id="47d7" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la lh li lj lk bi translated">研究人员寻找建筑的改进。</li><li id="4018" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">人们研究管理这个领域的理论。</li><li id="3656" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">搜索可能使用此方法的应用程序。</li></ul><p id="223e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">那些旨在深入研究<strong class="kh ir">变形金刚</strong>的读者可能会找到大量详细讨论<a class="ae lb" href="https://www.coursera.org/lecture/nlp-sequence-models/transformer-network-Kf5Y3" rel="noopener ugc nofollow" target="_blank">变形金刚</a>的资源。简而言之，转换器通常用于为NLP问题开发语言模型。这些模型用于构建句子、Q &amp; A和翻译等任务。在一个非常高级的描述中，转换器可以被认为是复杂的自动编码器，它接收键、值和查询(单词)三元组作为输入，并研究一个语言模型，其中每个单词都有一个依赖于其语义上下文的特定表示。</p><h1 id="f852" class="lq lr iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated">伯特&amp;拥抱脸</h1><p id="cd3d" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">BERT ( <strong class="kh ir">来自变压器</strong>的双向编码器表示)在此<a class="ae lb" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank">引入</a>。随着变形金刚的出现，<strong class="kh ir"> BERT </strong>的想法是采用变形金刚预先训练好的模型，并根据特定任务对这些模型的权重进行微调(<strong class="kh ir">下游任务</strong>)。这种方法产生了一类新的NLP问题，可以通过最初使用转换器来解决，例如分类问题(例如情感分析)。这是通过将网络的上层修改成集群的结构或不同类型的序列来实现的。因此，我们有许多伯特模型。这样一个伟大的“模特银行”就是<a class="ae lb" href="https://huggingface.co/" rel="noopener ugc nofollow" target="_blank">抱脸</a>。该框架提供了一个包含三个基本组件的包:</p><ul class=""><li id="9e06" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la lh li lj lk bi translated">各种预先训练的模型和工具</li><li id="1b17" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">令牌化引擎</li><li id="9f61" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">框架灵活性(例如Torch、Keras)</li></ul><p id="c5a6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个软件包可以处理大量的NLP任务。</p><h1 id="9562" class="lq lr iq bd ls lt lu lv lw lx ly lz ma jw mb jx mc jz md ka me kc mf kd mg mh bi translated">那我为什么要写帖子呢？</h1><p id="1abc" class="pw-post-body-paragraph kf kg iq kh b ki mi jr kk kl mj ju kn ko mk kq kr ks ml ku kv kw mm ky kz la ij bi translated">当我开始使用拥抱脸时，我对它提供的优秀的“端到端”管道以及它提供的数据结构的便利性印象深刻。然而，我觉得他们的教程中有一部分没有很好地涵盖。在我自己设法找到解决方案后，我觉得作为一个“激进的开源”我必须分享它。</p><p id="e846" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">为了说明这个问题，我将简要描述拥抱脸提供的特征提取机制。我们给出的数据很简单:文档和标签。</p><p id="0ea9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">最基本的函数是<strong class="kh ir">记号赋予器:</strong></p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="6950" class="mw lr iq ms b gy mx my l mz na"><strong class="ms ir">from </strong>transformers <strong class="ms ir">import </strong>AutoTokenizer<br/>tokens = tokenizer.batch_encode_plus(documents )</span></pre><p id="a767" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个过程将文档映射成<strong class="kh ir">变形金刚的</strong>标准表示，因此可以直接用于拥抱脸的模型。这里我们提出一个通用的特征提取过程:</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="eec2" class="mw lr iq ms b gy mx my l mz na"><strong class="ms ir">def </strong>regular_procedure(tokenizer, documents , labels ):<br/>    tokens = tokenizer.batch_encode_plus(documents )<br/><br/>    features=[InputFeatures(label=labels[j], **{key: tokens[key][j]   <br/>    <strong class="ms ir">for </strong>key <strong class="ms ir">in </strong>tokens.keys()}) <strong class="ms ir">for </strong>j <strong class="ms ir">in  </strong>range(len(documents ))]<strong class="ms ir"><br/>                </strong><br/>    <strong class="ms ir">return </strong>features</span></pre><p id="e643" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该方法的输出列表:<em class="nb">特性</em>是一个可以用于训练和评估过程的列表。我发现的一个障碍是缺乏使用<strong class="kh ir"> Dataloader的教程。</strong></p><p id="d60c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在所有教程中，假设我们使用训练/评估期间可用的数据。这个假设对于新兵训练营的需求来说是明确的，但是对于现实世界的任务来说是错误的。我们正在处理大数据:</p><p id="b528" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">在大数据中，代码指向数据，而不是数据指向代码</strong></p><p id="932c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我开始尝试。我的目标是创建一个能够用PyTorch <strong class="kh ir">数据加载器访问的特性文件夹。</strong>我的第一次尝试如下:</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="9579" class="mw lr iq ms b gy mx my l mz na"><strong class="ms ir">def </strong>generate_files_no_tensor(tokenizer, documents, labels ):<br/>    tokens  = tokenizer.batch_encode_plus(documents )<br/><br/><br/>    file_pref =<strong class="ms ir">"my_file_"<br/>    for </strong>j <strong class="ms ir">in </strong>range(len(documents) ):<br/>            inputs = {k: tokens[k][j] <strong class="ms ir">for </strong>k <strong class="ms ir">in </strong>tokens}<br/>            feature = InputFeatures(label=labels[j], **inputs)<br/>            file_name = file_pref +<strong class="ms ir">"_"</strong>+str(j)+<strong class="ms ir">".npy"<br/>            </strong>np.save(file_name, np.array(feature))<br/>    <strong class="ms ir">return</strong></span></pre><p id="8456" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这段代码运行得很好，但不是最佳的。它的主要缺点是节省了<strong class="kh ir"> numpy </strong>个对象，而抱紧的模型需要<strong class="kh ir">个张量</strong>。这意味着我的<strong class="kh ir"> __getitem__ </strong>函数将有额外的任务，但上传文件:</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="dc38" class="mw lr iq ms b gy mx my l mz na"><strong class="ms ir">def </strong>__getitemnumpy__(self, idx):<br/>    aa = np.load(self.list_of_files[idx], allow_pickle=<strong class="ms ir">True</strong>)<br/>    cc = aa.data.obj.tolist()<br/>    c1 = cc.input_ids<br/>    c2 = cc.attention_mask<br/>    c3 = cc.label<br/>    <strong class="ms ir">return </strong>torch.tensor(c1), torch.tensor(c2), c3</span></pre><p id="1fb6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在训练过程中，我们需要将数量级的对象转换成张量。</p><p id="a5e2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我决定以一种不同的方式工作:我开发了我的<strong class="kh ir"> __getitem__ </strong>并强迫数据“承认它的规则”。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="ec5a" class="mw lr iq ms b gy mx my l mz na"><strong class="ms ir">def </strong>__getitem__(self, idx):<br/>    aa = torch.load(self.list_of_files[idx])<br/>    <strong class="ms ir">return </strong>aa[0], aa[1], aa[2]</span></pre><p id="a46f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在让我们面对挑战。让我们试试这个:</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="bc11" class="mw lr iq ms b gy mx my l mz na"><strong class="ms ir">def </strong>generate_files_no_tensor(tokenizer, documents, labels ):<br/>    tokens  = tokenizer.batch_encode_plus(documents )<br/><br/><br/>    file_pref =<strong class="ms ir">"my_file_"<br/>    for </strong>j <strong class="ms ir">in </strong>range(len(documents) ):<br/>            inputs = {k: tokens[k][j] <strong class="ms ir">for </strong>k <strong class="ms ir">in </strong>tokens}<br/>            feature = InputFeatures(label=labels[j], **inputs)<br/>            file_name = file_pref +<strong class="ms ir">"_"</strong>+str(j)+<strong class="ms ir">".pt"<br/>            </strong>torch.save(file_name, feature)<br/>    <strong class="ms ir">return</strong></span></pre><p id="2c34" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">有用！我们甚至可以直接接触到张量。但是这个循环非常慢。当我观察这些数据时，我看到了两个“现象”:</p><ul class=""><li id="f235" class="lc ld iq kh b ki kj kl km ko le ks lf kw lg la lh li lj lk bi translated">所有文件都有相同的大小</li><li id="13bf" class="lc ld iq kh b ki ll kl lm ko ln ks lo kw lp la lh li lj lk bi translated">文件很大！！</li></ul><p id="93c2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我花了一段时间才意识到所有的文件都保存了整个张量(它们可能保存了一个指向它的位置的点)。因此，我们必须将它切片。</p><pre class="mn mo mp mq gt mr ms mt mu aw mv bi"><span id="13d5" class="mw lr iq ms b gy mx my l mz na"><strong class="ms ir">def </strong>generate_files_with_tensor(tokenizer, documents, labels ):<br/>    tokens = tokenizer.batch_encode_plus(doc0, return_tensors=<strong class="ms ir">'pt'</strong>)<br/><br/>    file_pref =<strong class="ms ir">"my_file_"<br/>    for </strong>j <strong class="ms ir">in </strong>range(len(documents) ):<br/>        file_name = file_pref +<strong class="ms ir">"_"</strong>+str(j)+<strong class="ms ir">".pt"<br/>        </strong>input_t =  torch.squeeze(torch.index_select(tokens[<strong class="ms ir">"input_ids"</strong>],dim=0,<br/>index=torch.tensor(j)))<br/>        input_m =     torch.squeeze(torch.index_select(tokens[<strong class="ms ir">"attention_mask"</strong>],dim=0,<br/>        index=torch.tensor(j)))<br/>        torch.save([input_t, input_m, labels[j]], file_name)<br/>    <strong class="ms ir">return</strong></span></pre><p id="ab3a" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> index_select </strong>函数对张量进行切片，挤压允许移除尺寸为1的尺寸。它达到了要求。现在我有一个快速的<strong class="kh ir"> __getitem__ </strong>除了上传数据什么也不做。</p><p id="ffbf" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">具有数据结构和<strong class="kh ir">数据加载器</strong>的代码示例存在于<a class="ae lb" href="https://github.com/natanka/Keras_this_and_that/blob/patch-1/huggings_data_loader_examp.py" rel="noopener ugc nofollow" target="_blank">这里</a>。</p><p id="de13" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">希望你会觉得有用。</p></div></div>    
</body>
</html>