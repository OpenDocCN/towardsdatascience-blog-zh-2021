<html>
<head>
<title>Difference between AutoEncoder (AE) and Variational AutoEncoder (VAE)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">自动编码器(AE)和可变自动编码器(VAE)的区别</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/difference-between-autoencoder-ae-and-variational-autoencoder-vae-ed7be1c038f2?source=collection_archive---------0-----------------------#2021-11-03">https://towardsdatascience.com/difference-between-autoencoder-ae-and-variational-autoencoder-vae-ed7be1c038f2?source=collection_archive---------0-----------------------#2021-11-03</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="8e00" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何压缩数据，甚至从随机值生成数据？这就是自动编码器和变型自动编码器。</h2></div><blockquote class="kf"><p id="db38" class="kg kh iq bd ki kj kk kl km kn ko kp dk translated">简化的能力意味着消除不必要的东西，让必要的东西说话——汉斯·霍夫曼</p></blockquote><h1 id="e0c8" class="kq kr iq bd ks kt ku kv kw kx ky kz la jw lb jx lc jz ld ka le kc lf kd lg lh bi translated">上下文—数据压缩</h1><p id="3f8b" class="pw-post-body-paragraph li lj iq lk b ll lm jr ln lo lp ju lq lr ls lt lu lv lw lx ly lz ma mb mc kp ij bi translated">数据压缩是训练网络的重要阶段。这个想法是压缩数据，这样同样数量的信息可以用更少的比特来表示。这也有助于解决维数灾难的问题。具有许多属性的数据集不适合训练，因为它往往会使模型过拟合。因此，在数据集可用于训练之前，需要应用维数减少技术。</p><p id="5fc7" class="pw-post-body-paragraph li lj iq lk b ll md jr ln lo me ju lq lr mf lt lu lv mg lx ly lz mh mb mc kp ij bi translated">这就是自动编码器(AE)和可变自动编码器(VAE)发挥作用的地方。它们是用于压缩输入数据的端到端网络。自动编码器和变分自动编码器都用于将数据从高维空间转换到低维空间，本质上实现压缩。</p><h1 id="2623" class="kq kr iq bd ks kt ku kv kw kx ky kz la jw mi jx lc jz mj ka le kc mk kd lg lh bi translated">自动编码器— AE</h1><h2 id="8d54" class="ml kr iq bd ks mm mn dn kw mo mp dp la lr mq mr lc lv ms mt le lz mu mv lg mw bi translated">这是什么？</h2><p id="0294" class="pw-post-body-paragraph li lj iq lk b ll lm jr ln lo lp ju lq lr ls lt lu lv lw lx ly lz ma mb mc kp ij bi translated">Autoencoder用于学习给定网络配置的未标记数据的有效嵌入。自动编码器由两部分组成，编码器和解码器。编码器将数据从高维空间压缩到低维空间(也称为潜在空间)，而解码器则相反，即将潜在空间转换回高维空间。解码器用于确保潜在空间可以从数据集空间捕获大部分信息，方法是强制它输出作为输入提供给解码器的内容。</p><h2 id="fa85" class="ml kr iq bd ks mm mn dn kw mo mp dp la lr mq mr lc lv ms mt le lz mu mv lg mw bi translated">框图</h2><p id="a030" class="pw-post-body-paragraph li lj iq lk b ll lm jr ln lo lp ju lq lr ls lt lu lv lw lx ly lz ma mb mc kp ij bi translated">框图如下所示。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi mx"><img src="../Images/fb379ba0b245f5544a47d5557754e64f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*qFzKC1GqOR17XaiQBex83w.png"/></div></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">自动编码器—作者图片</p></figure><p id="8053" class="pw-post-body-paragraph li lj iq lk b ll md jr ln lo me ju lq lr mf lt lu lv mg lx ly lz mh mb mc kp ij bi translated">在训练期间，输入数据x被馈送到编码器功能<strong class="lk ir"> <em class="nn"> e_theta(x) </em> </strong>。输入通过一系列层(由变量theta参数化),减少其维度以获得压缩的潜在向量<strong class="lk ir"> <em class="nn"> z </em> </strong>。层数、层的类型和尺寸以及潜在空间尺寸是用户控制的参数。如果潜在空间的维数小于输入空间的维数，就实现了压缩，实质上消除了冗余属性。</p><p id="0baa" class="pw-post-body-paragraph li lj iq lk b ll md jr ln lo me ju lq lr mf lt lu lv mg lx ly lz mh mb mc kp ij bi translated">解码器<strong class="lk ir"> <em class="nn"> d_phi(z) </em> </strong>通常(但不一定)由编码器中使用的层的近似互补层组成，但顺序相反。层的近似补充层是可用于撤销原始层的操作(在某种程度上)的层，诸如将conv层转置到conv层、汇集到取消汇集、完全连接到完全连接等。</p><h2 id="ee66" class="ml kr iq bd ks mm mn dn kw mo mp dp la lr mq mr lc lv ms mt le lz mu mv lg mw bi translated">损失函数</h2><p id="57e7" class="pw-post-body-paragraph li lj iq lk b ll lm jr ln lo lp ju lq lr ls lt lu lv lw lx ly lz ma mb mc kp ij bi translated">整个编码器-解码器架构在损失函数上被集体训练，该损失函数鼓励在输出端重构输入。因此，损失函数是编码器输入和解码器输出之间的均方误差。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi no"><img src="../Images/644e3dcaadf2b396e86fbba162861ae3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FoaRF3WlfvcIqAfxWRypVQ.png"/></div></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">自动编码器损失函数—作者图片</p></figure><p id="2065" class="pw-post-body-paragraph li lj iq lk b ll md jr ln lo me ju lq lr mf lt lu lv mg lx ly lz mh mb mc kp ij bi translated">这个想法是有一个非常低维的潜在空间，以便实现最大的压缩，但同时，误差足够小。将潜在空间的维度减小超过某个值将导致信息的显著损失。</p><p id="010e" class="pw-post-body-paragraph li lj iq lk b ll md jr ln lo me ju lq lr mf lt lu lv mg lx ly lz mh mb mc kp ij bi translated">潜在空间的价值/分布没有限制。它可以是任何东西，只要当解码器功能应用于它时，它可以重构输入。</p><h2 id="bc93" class="ml kr iq bd ks mm mn dn kw mo mp dp la lr mq mr lc lv ms mt le lz mu mv lg mw bi translated">潜在空间可视化</h2><p id="1815" class="pw-post-body-paragraph li lj iq lk b ll lm jr ln lo lp ju lq lr ls lt lu lv lw lx ly lz ma mb mc kp ij bi translated">以下是通过在MNIST数据集上训练网络而生成的潜在空间的示例。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi np"><img src="../Images/6b388666acddf58e91f449e22c152bec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9yGDXANpZ-8j67m22RDDBA.png"/></div></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">自动编码器的潜在空间——作者图片</p></figure><p id="e8f4" class="pw-post-body-paragraph li lj iq lk b ll md jr ln lo me ju lq lr mf lt lu lv mg lx ly lz mh mb mc kp ij bi translated">可以看出，相同的数字倾向于在潜在空间中自我聚集。另一个需要注意的重要事情是，潜在空间的某些部分并不对应于任何数据点。使用这些作为编码器的输入将导致输出看起来不像来自MNIST数据的任何数字。这就是我们所说的潜在空间没有被正则化的意思。这种潜在空间仅具有几个具有生成能力的区域/聚类，这意味着对潜在空间中属于一个聚类的任何点进行采样都将生成该聚类所属的数据的变化。但整个潜在空间不具备再生能力。不属于任何簇的区域将产生垃圾输出。一旦网络被训练，并且训练数据被移除，我们就无法知道解码器从随机采样的潜在向量生成的输出是否有效。因此AE主要用于压缩。</p><blockquote class="kf"><p id="ccdd" class="kg kh iq bd ki kj nq nr ns nt nu kp dk translated">对于有效的输入，AE能够将它们压缩到更少的比特，基本上消除了冗余(编码器)，但是由于非正则化的潜在空间AE，解码器不能用于从潜在空间采样的向量的潜在中生成有效的输入数据。</p></blockquote><h2 id="0c7f" class="ml kr iq bd ks mm nv dn kw mo nw dp la lr nx mr lc lv ny mt le lz nz mv lg mw bi translated">为什么要用AE进行压缩？</h2><p id="36a7" class="pw-post-body-paragraph li lj iq lk b ll lm jr ln lo lp ju lq lr ls lt lu lv lw lx ly lz ma mb mc kp ij bi translated">线性自动编码器的潜在空间非常类似于在数据的主分量分析期间获得的特征空间。输入空间维度<strong class="lk ir"> <em class="nn"> n </em> </strong>和潜在空间维度设置为<strong class="lk ir"> <em class="nn"> m &lt; n </em> </strong>结果的线性自动编码器将跨越与PCA的第一个<strong class="lk ir"> <em class="nn"> m </em> </strong>特征向量所跨越的向量空间相同的向量空间。如果AE和PCA差不多，为什么要用AE？声发射的威力来自它的非线性。添加非线性(例如非线性激活函数和更多的隐藏层)使得AE能够以少得多的信息损失在较低维度中学习输入数据的相当强大的表示。</p><h1 id="f6fe" class="kq kr iq bd ks kt ku kv kw kx ky kz la jw mi jx lc jz mj ka le kc mk kd lg lh bi translated">可变自动编码器</h1><h2 id="3fae" class="ml kr iq bd ks mm mn dn kw mo mp dp la lr mq mr lc lv ms mt le lz mu mv lg mw bi translated">这是什么？</h2><p id="a95a" class="pw-post-body-paragraph li lj iq lk b ll lm jr ln lo lp ju lq lr ls lt lu lv lw lx ly lz ma mb mc kp ij bi translated">变分自动编码器解决了自动编码器中非正则化潜在空间的问题，并为整个空间提供了生成能力。AE中的编码器输出潜在向量。VAE编码器不是输出潜在空间中的向量，而是为每个输入输出潜在空间中预定义分布的参数。然后，VAE对这种潜在分布施加约束，迫使其成为正态分布。这个约束确保了潜在空间的正则化。</p><h2 id="21f7" class="ml kr iq bd ks mm mn dn kw mo mp dp la lr mq mr lc lv ms mt le lz mu mv lg mw bi translated">框图</h2><p id="e0a9" class="pw-post-body-paragraph li lj iq lk b ll lm jr ln lo lp ju lq lr ls lt lu lv lw lx ly lz ma mb mc kp ij bi translated">VAE的框图如下所示。在训练期间，输入数据x被馈送到编码器函数<strong class="lk ir"> <em class="nn"> e_theta(x) </em> </strong>。就像AE一样，输入通过一系列层(由变量theta参数化)来减少其维度，以获得压缩的潜在向量<strong class="lk ir"> <em class="nn"> z </em> </strong>。然而，潜在向量不是编码器的输出。相反，编码器输出每个潜在变量的平均值和标准差。然后，根据该平均值和标准偏差对潜在向量进行采样，然后将其馈送到解码器以重构输入。VAE中的解码器与AE中的解码器工作原理相似。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi oa"><img src="../Images/3b183ca3c6241e58b96b12847873ead8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ET6FM_KEmwa2N4qgW2MglQ.png"/></div></div></figure><h2 id="6505" class="ml kr iq bd ks mm mn dn kw mo mp dp la lr mq mr lc lv ms mt le lz mu mv lg mw bi translated">损失函数</h2><p id="b18e" class="pw-post-body-paragraph li lj iq lk b ll lm jr ln lo lp ju lq lr ls lt lu lv lw lx ly lz ma mb mc kp ij bi translated">损失函数由VAE目标定义。VAE有两个目标</p><ol class=""><li id="4733" class="ob oc iq lk b ll md lo me lr od lv oe lz of kp og oh oi oj bi translated">重建输入</li><li id="7e59" class="ob oc iq lk b ll ok lo ol lr om lv on lz oo kp og oh oi oj bi translated">潜在空间应该是正态分布的</li></ol><p id="8297" class="pw-post-body-paragraph li lj iq lk b ll md jr ln lo me ju lq lr mf lt lu lv mg lx ly lz mh mb mc kp ij bi translated">因此，VAE的训练损失被定义为这些<strong class="lk ir"><em class="nn"/></strong>和<strong class="lk ir"> <em class="nn">相似性损失</em> </strong>的总和。与AE一样，重构误差是输入和重构输出的均方损耗。相似性损失是潜在空间分布和标准高斯(零均值和单位方差)之间的KL散度。那么损失函数就是这两个损失的总和。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi op"><img src="../Images/93d0b606b4acc8c4d0eee9b89d4a0eec.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S1Ez-DYXDSLSZC5wUFGG3g.png"/></div></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">变分自动编码器损失函数—图片由作者提供</p></figure><p id="0432" class="pw-post-body-paragraph li lj iq lk b ll md jr ln lo me ju lq lr mf lt lu lv mg lx ly lz mh mb mc kp ij bi translated">如前所述，在将潜在向量馈送到解码器之前，从编码器生成的分布中对其进行采样。这种随机采样使得编码器很难发生反向传播，因为我们无法追溯这种随机采样导致的错误。因此，我们使用重新参数化技巧来模拟采样过程，这使得误差有可能通过网络传播。潜在向量z被表示为编码器输出的函数。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi oq"><img src="../Images/6e24ac87236f86da8185a510060ad94d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*psQ5oEaQQCRxkgERY58RbQ.png"/></div></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">重新参数化技巧用于将潜在向量z表示为编码器输出的函数。</p></figure><h2 id="3eff" class="ml kr iq bd ks mm mn dn kw mo mp dp la lr mq mr lc lv ms mt le lz mu mv lg mw bi translated">潜在空间可视化</h2><p id="ede1" class="pw-post-body-paragraph li lj iq lk b ll lm jr ln lo lp ju lq lr ls lt lu lv lw lx ly lz ma mb mc kp ij bi translated">训练试图在两个损失之间找到平衡，并以看起来像单位范数的潜在空间分布结束，其中聚类将相似的输入数据点分组。单位范数条件确保潜在空间被均匀地展开，并且在聚类之间没有明显的间隙。事实上，相似数据输入的聚类通常在某些区域重叠。下面是通过在相同的MNIST数据集上训练网络生成的潜在空间的示例，该数据集用于可视化AE的潜在空间。请注意，簇之间没有间隙，空间类似于单位范数的分布。</p><div class="my mz na nb gt ab cb"><figure class="or nc os ot ou ov ow paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><img src="../Images/8b66a4150a4d4b9f11641b8bb1c97a10.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*EdYJUXgXiM4M78-YeNMVwA.png"/></div></figure><figure class="or nc os ot ou ov ow paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><img src="../Images/fb5c3fdbf02a380050e0f713c44fc77a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*yDBdwDZ39E-gLWSRj0m0Bw.png"/></div><p class="nj nk gj gh gi nl nm bd b be z dk ox di oy oz translated">变型自动编码器的潜在空间——作者图像</p></figure></div><p id="5b1d" class="pw-post-body-paragraph li lj iq lk b ll md jr ln lo me ju lq lr mf lt lu lv mg lx ly lz mh mb mc kp ij bi translated">需要注意的重要一点是，当潜在向量从具有重叠聚类的区域中采样时，我们得到变形的数据。当我们对从一个聚类移动到另一个聚类的潜在空间进行采样时，我们得到了解码器输出之间的平滑过渡。</p><figure class="my mz na nb gt nc gh gi paragraph-image"><div role="button" tabindex="0" class="nd ne di nf bf ng"><div class="gh gi pa"><img src="../Images/e97de1f2a09e921460787dc976b0b6cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*p_xiH7i5QDzATqWdjb4a8w.png"/></div></div><p class="nj nk gj gh gi nl nm bd b be z dk translated">VAE的潜在空间被规则化——作者的意象</p></figure><figure class="my mz na nb gt nc"><div class="bz fp l di"><div class="pb pc l"/></div></figure><h1 id="e125" class="kq kr iq bd ks kt ku kv kw kx ky kz la jw mi jx lc jz mj ka le kc mk kd lg lh bi translated">摘要</h1><p id="2519" class="pw-post-body-paragraph li lj iq lk b ll lm jr ln lo lp ju lq lr ls lt lu lv lw lx ly lz ma mb mc kp ij bi translated">本文介绍了自动编码器(AE)和变分自动编码器(VAE)的概念，它们分别用于数据压缩和数据生成。VAE解决了AE的非正则化潜在空间的问题，这使得它能够从潜在空间的随机采样向量中生成数据。AE和VAE的主要总结点如下</p><h2 id="ebe7" class="ml kr iq bd ks mm mn dn kw mo mp dp la lr mq mr lc lv ms mt le lz mu mv lg mw bi translated">自动编码器</h2><p id="f0c8" class="pw-post-body-paragraph li lj iq lk b ll lm jr ln lo lp ju lq lr ls lt lu lv lw lx ly lz ma mb mc kp ij bi translated">用于在潜在空间中生成输入的压缩变换</p><p id="41d4" class="pw-post-body-paragraph li lj iq lk b ll md jr ln lo me ju lq lr mf lt lu lv mg lx ly lz mh mb mc kp ij bi translated">潜在变量没有被正则化</p><p id="701a" class="pw-post-body-paragraph li lj iq lk b ll md jr ln lo me ju lq lr mf lt lu lv mg lx ly lz mh mb mc kp ij bi translated">选择一个随机的潜在变量会产生垃圾输出</p><p id="a173" class="pw-post-body-paragraph li lj iq lk b ll md jr ln lo me ju lq lr mf lt lu lv mg lx ly lz mh mb mc kp ij bi translated">潜在变量具有不连续性</p><p id="d4bd" class="pw-post-body-paragraph li lj iq lk b ll md jr ln lo me ju lq lr mf lt lu lv mg lx ly lz mh mb mc kp ij bi translated">潜在变量是确定性值</p><p id="3d87" class="pw-post-body-paragraph li lj iq lk b ll md jr ln lo me ju lq lr mf lt lu lv mg lx ly lz mh mb mc kp ij bi translated">潜在空间缺乏生成能力</p><h2 id="36af" class="ml kr iq bd ks mm mn dn kw mo mp dp la lr mq mr lc lv ms mt le lz mu mv lg mw bi translated">可变自动编码器(VAE)</h2><p id="2a49" class="pw-post-body-paragraph li lj iq lk b ll lm jr ln lo lp ju lq lr ls lt lu lv lw lx ly lz ma mb mc kp ij bi translated">将潜在变量的条件强制为单位范数</p><p id="ffd4" class="pw-post-body-paragraph li lj iq lk b ll md jr ln lo me ju lq lr mf lt lu lv mg lx ly lz mh mb mc kp ij bi translated">压缩形式的潜在变量是均值和方差</p><p id="84b3" class="pw-post-body-paragraph li lj iq lk b ll md jr ln lo me ju lq lr mf lt lu lv mg lx ly lz mh mb mc kp ij bi translated">潜在变量是平滑连续的</p><p id="c458" class="pw-post-body-paragraph li lj iq lk b ll md jr ln lo me ju lq lr mf lt lu lv mg lx ly lz mh mb mc kp ij bi translated">潜在变量的随机值在解码器处产生有意义的输出</p><p id="e1f7" class="pw-post-body-paragraph li lj iq lk b ll md jr ln lo me ju lq lr mf lt lu lv mg lx ly lz mh mb mc kp ij bi translated">解码器的输入是随机的，并且是从具有编码器输出的均值和方差的高斯样本中采样的。</p><p id="ac55" class="pw-post-body-paragraph li lj iq lk b ll md jr ln lo me ju lq lr mf lt lu lv mg lx ly lz mh mb mc kp ij bi translated">正则化潜在空间</p><p id="991e" class="pw-post-body-paragraph li lj iq lk b ll md jr ln lo me ju lq lr mf lt lu lv mg lx ly lz mh mb mc kp ij bi translated">潜在空间具有生成能力。</p></div><div class="ab cl pd pe hu pf" role="separator"><span class="pg bw bk ph pi pj"/><span class="pg bw bk ph pi pj"/><span class="pg bw bk ph pi"/></div><div class="ij ik il im in"><p id="39ff" class="pw-post-body-paragraph li lj iq lk b ll md jr ln lo me ju lq lr mf lt lu lv mg lx ly lz mh mb mc kp ij bi translated"><strong class="lk ir">如果这篇文章对你有帮助，或者你想了解更多关于机器学习和数据科学的知识，请关注</strong><a class="ae pk" href="https://aqeel-anwar.medium.com/" rel="noopener"><strong class="lk ir">Aqeel an war</strong></a><strong class="lk ir">，或者在</strong><a class="ae pk" href="https://www.linkedin.com/in/aqeelanwarmalik/" rel="noopener ugc nofollow" target="_blank"><strong class="lk ir"><em class="nn">LinkedIn</em></strong></a><strong class="lk ir"><em class="nn">或</em></strong><a class="ae pk" href="https://twitter.com/_aqeelanwar" rel="noopener ugc nofollow" target="_blank"><strong class="lk ir"><em class="nn">Twitter</em></strong></a><strong class="lk ir"><em class="nn">上联系我。</em> </strong></p><div class="pl pm gp gr pn po"><a href="https://aqeel-anwar.medium.com/membership" rel="noopener follow" target="_blank"><div class="pp ab fo"><div class="pq ab pr cl cj ps"><h2 class="bd ir gy z fp pt fr fs pu fu fw ip bi translated">通过我的推荐链接加入媒体- Aqeel Anwar</h2><div class="pv l"><h3 class="bd b gy z fp pt fr fs pu fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="pw l"><p class="bd b dl z fp pt fr fs pu fu fw dk translated">aqeel-anwar.medium.com</p></div></div><div class="px l"><div class="py l pz qa qb px qc nh po"/></div></div></a></div></div></div>    
</body>
</html>