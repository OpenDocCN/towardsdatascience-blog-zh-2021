<html>
<head>
<title>Recurrent Neural Network that impersonates F. Pessoa</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">模拟F. Pessoa的递归神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/generating-text-with-recurrent-neural-networks-based-on-the-work-of-f-pessoa-1e804d88692d?source=collection_archive---------24-----------------------#2021-04-19">https://towardsdatascience.com/generating-text-with-recurrent-neural-networks-based-on-the-work-of-f-pessoa-1e804d88692d?source=collection_archive---------24-----------------------#2021-04-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9c24" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用Tensorflow和Keras的深度学习应用</h2></div><h1 id="98be" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">1.介绍</h1><p id="c6ea" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">离散令牌序列可以在许多应用中找到，即文本中的单词、音乐作品中的音符、图像中的像素、强化学习代理中的动作等<a class="ae lw" href="https://arxiv.org/pdf/1801.00632.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a>。这些序列通常在连续或邻近的记号之间表现出很强的相关性。句子中的单词或单词中的字符的相关性表达了潜在的语义和语言特征。序列<em class="lx"> x_n </em>中的下一个令牌可以建模为:</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="gh gi ly"><img src="../Images/095f78c011145ccc1b36d08d45b16614.png" data-original-src="https://miro.medium.com/v2/resize:fit:388/format:webp/1*KubAHn_t6gs6HtGnQyQk9A.png"/></div></figure><p id="d226" class="pw-post-body-paragraph la lb it lc b ld mg ju lf lg mh jx li lj mi ll lm ln mj lp lq lr mk lt lu lv im bi translated">其中<em class="lx"> x_i </em>表示序列中的第I个<em class="lx">令牌。在自然语言处理(NLP)中，这些被定义为语言模型。通常，每个令牌代表一个单独的单词或字母。生成的输出是一个概率分布，我们可以从中采样以生成序列中的下一个令牌。这些模型也被称为循环的，因为我们可以循环地应用这个生成过程来创建全新的令牌序列。</em></p><p id="5c26" class="pw-post-body-paragraph la lb it lc b ld mg ju lf lg mh jx li lj mi ll lm ln mj lp lq lr mk lt lu lv im bi translated">经常用于处理离散记号序列问题的一种特定类型的生成模型是递归神经网络(RNN)。在更简单的神经网络中，固定维度的特征表示被不同的非线性函数变换几次。在RNN中，这些变换也会在时间上重复，这意味着在每个时间步长，都会处理新的输入，并生成新的输出。它们可以有效地捕获输入序列的语义丰富的表示<a class="ae lw" href="https://icml.cc/Conferences/2011/papers/524_icmlpaper.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a>。RNN在不同的环境中展示了这种能力，例如生成结构化文本、原始图像(基于每像素)，甚至在在线服务上模拟用户行为。</p><p id="2964" class="pw-post-body-paragraph la lb it lc b ld mg ju lf lg mh jx li lj mi ll lm ln mj lp lq lr mk lt lu lv im bi translated">我们的任务是生成类似训练语料库的原始文本。这是一项无人监督的任务，因为我们无法访问任何标记或目标变量。我们首先创建一个单词嵌入，将每个字符映射到一个参数化维度的向量。对于每一个字符，该模型查找其嵌入情况，并将结果输入一个长短期记忆(LSTM)层堆栈，这是一种特殊类型的RNN。这些被开发来扩展RNNs的传统能力，以模拟长期依赖性和对抗消失梯度问题。我们网络的输出是一个密集层，其单元数量等于词汇量。我们没有为这一层定义激活函数；它只是为词汇表中的每个字符输出一个logit。我们使用这些值在以后从分类分布中取样。</p><p id="dd65" class="pw-post-body-paragraph la lb it lc b ld mg ju lf lg mh jx li lj mi ll lm ln mj lp lq lr mk lt lu lv im bi translated">在本文中，我们使用了费尔南多·佩索阿的作品，他是20世纪最重要的文学人物之一，也是葡萄牙语中最伟大的诗人之一。这个数据集现在可以在Kaggle上公开获得，包括4300多首诗歌、散文和其他作品。</p><p id="7417" class="pw-post-body-paragraph la lb it lc b ld mg ju lf lg mh jx li lj mi ll lm ln mj lp lq lr mk lt lu lv im bi translated">在<a class="ae lw" href="https://www.kaggle.com/luisroque/recurrent-neural-network-impersonating-f-pessoa" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>和<a class="ae lw" href="https://github.com/luisroque/deep-learning-articles" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上也有该代码。</p><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ml"><img src="../Images/7a2628447a1531867243df90eb1b96ee.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*fvCINy46IYOJrepi.jpg"/></div></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">图1:佩索阿的肖像，1914年— <a class="ae lw" href="https://commons.wikimedia.org/w/index.php?curid=56775462" rel="noopener ugc nofollow" target="_blank">来源</a></p></figure><p id="5f8c" class="pw-post-body-paragraph la lb it lc b ld mg ju lf lg mh jx li lj mi ll lm ln mj lp lq lr mk lt lu lv im bi translated">本文属于使用TensorFlow进行深度学习的系列文章:</p><ul class=""><li id="bb48" class="mu mv it lc b ld mg lg mh lj mw ln mx lr my lv mz na nb nc bi translated"><a class="ae lw" rel="noopener" target="_blank" href="/transfer-learning-and-data-augmentation-applied-to-the-simpsons-image-dataset-e292716fbd43">应用于辛普森图像数据集的迁移学习和数据增强</a></li><li id="171e" class="mu mv it lc b ld nd lg ne lj nf ln ng lr nh lv mz na nb nc bi translated"><a class="ae lw" rel="noopener" target="_blank" href="/generating-text-with-recurrent-neural-networks-based-on-the-work-of-f-pessoa-1e804d88692d">基于F. Pessoa的工作用递归神经网络生成文本</a></li><li id="3a1a" class="mu mv it lc b ld nd lg ne lj nf ln ng lr nh lv mz na nb nc bi translated"><a class="ae lw" rel="noopener" target="_blank" href="/neural-machine-translation-using-a-seq2seq-architecture-and-attention-eng-to-por-fe3cc4191175">使用Seq2Seq架构和注意力的神经机器翻译(ENG to POR) </a></li><li id="be73" class="mu mv it lc b ld nd lg ne lj nf ln ng lr nh lv mz na nb nc bi translated"><a class="ae lw" rel="noopener" target="_blank" href="/residual-networks-in-computer-vision-ee118d3be68f">残差网络从无到有应用于计算机视觉</a></li></ul><h1 id="0657" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">2.数据预处理</h1><p id="76ba" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">该数据集包括作者用自己的名字写的几篇文章，但也使用不同的异名和假名。每个人都有自己的写作风格，单独学习会很有趣。然而，为了有效地训练深度神经网络(DNN)，我们需要一个大的数据集，这就是建立单一模型的原因。</p><p id="a2c7" class="pw-post-body-paragraph la lb it lc b ld mg ju lf lg mh jx li lj mi ll lm ln mj lp lq lr mk lt lu lv im bi translated">F.佩索阿年轻时曾在南非生活过一段时间，在那里他接触到了英语。这就是为什么他的部分作品是用英语写的。为了避免引入噪声，我们从训练数据集中移除了大部分英语文本。</p><pre class="lz ma mb mc gt ni nj nk nl aw nm bi"><span id="6759" class="nn kj it nj b gy no np l nq nr">import pandas as pd<br/>import numpy as np<br/>from tensorflow.keras.preprocessing.text import Tokenizer<br/>import tensorflow as tf<br/>import ast<br/>import os<br/>import json<br/>import matplotlib.pyplot as plt<br/>from nltk import tokenize<br/>import seaborn as sns</span><span id="b7d1" class="nn kj it nj b gy ns np l nq nr">f_pessoa = pd.read_csv(os.getcwd() + '/f_pessoa_v2.csv')<br/><br/>texts = f_pessoa.copy()<br/><br/># Removing all pseudonyms that wrote in English.<br/><br/>texts = texts[~texts['author'].isin(['Alexander Search', 'David Merrick', 'Charles Robert Anon', 'I. I. Crosse'])]<br/><br/>texts['text'] = texts['text'].apply(lambda t: ast.literal_eval(t))<br/>texts = texts.reset_index().drop('index', axis=1)<br/>texts = texts['text'].tolist()<br/>texts = np.concatenate(texts)<br/><br/>texts = np.asarray(texts)<br/>texts_p = " ".join(texts)<br/><br/># we will be truncating large texts soon, so this code only tries to reduce the <br/># sequence size by splitting the texts that seem to be significantly larger than <br/># the rest. Otherwise, we try to use the structure provided in the data itself<br/><br/>_, ax = plt.subplots(1, 2, figsize=(15, 5))<br/><br/>mylen = np.vectorize(len)<br/><br/>sns.histplot(mylen(texts), bins=50, ax=ax[0])<br/>ax[0].set_title('Histogram of the number of characters in each \nchunk of text BEFORE splitting sentences', fontsize=16)<br/><br/>large_texts = texts[mylen(texts)&gt;350]<br/>large_texts_p = " ".join(large_texts)<br/>large_texts = tokenize.sent_tokenize(large_texts_p)<br/><br/>texts = np.concatenate((texts[~(mylen(texts)&gt;350)], large_texts))<br/><br/>ax[1].set_title('Histogram of the number of characters in each \nchunk of text AFTER splitting sentences', fontsize=16)<br/>sns.histplot(mylen(texts), bins=50, ax=ax[1]);<br/><br/>print(f'Length of texts dataset: {len(texts_p)} characters')</span><span id="0d7b" class="nn kj it nj b gy ns np l nq nr">Length of texts dataset: 5811145 characters</span></pre><figure class="lz ma mb mc gt md gh gi paragraph-image"><div class="ab gu cl nt"><img src="../Images/f5172efa16b501971a4edda4e519f453.png" data-original-src="https://miro.medium.com/v2/format:webp/1*gLwBtHTZwpS1aHIxtvBtRA.png"/></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">图2:分句前(左边)和分句后(右边)每块文本<em class="nu">的字符数直方图。</em></p></figure><p id="4bbd" class="pw-post-body-paragraph la lb it lc b ld mg ju lf lg mh jx li lj mi ll lm ln mj lp lq lr mk lt lu lv im bi translated">清理文本后，我们最终得到了超过580万个字符。请注意，为了避免在规范化序列的文本长度时丢失数据，我们按句子分割了最大的序列。序列长度分布的差异可以在上面的直方图中看到。我们可以预览一些片段。</p><pre class="lz ma mb mc gt ni nj nk nl aw nm bi"><span id="7e92" class="nn kj it nj b gy no np l nq nr">print(texts[97:106])</span><span id="6fca" class="nn kj it nj b gy ns np l nq nr">['O burburinho da água' 'O burburinho da água' 'No regato que se espalha'<br/> 'É como a ilusão que é mágoa' 'Quando a verdade a baralha.'<br/> '— A única vantagem de estudar é gozar o quanto os outros não disseram.'<br/> '— A arte é um isolamento. Todo o artista deve buscar isolar os outros, levar-lhes às almas o desejo de estarem sós. O triunfo supremo de um artista é quando a ler suas obras o leitor prefere tê-las e não as ler. Não é porque isto aconteça aos consagrados; é porque é o maior tributo (...)'<br/> '— Ser lúcido é estar indisposto consigo próprio. O legítimo estado de espírito com respeito a olhar para dentro de si próprio é o estado (...) de quem olha nervos e indecisões.'<br/> 'A única atitude intelectual digna de uma criatura superior é a de uma calma e fria compaixão por tudo quanto não é ele próprio. Não que essa atitude tenha o mínimo cunho de justa e verdadeira; mas é tão invejável que é preciso tê-la.']</span></pre><p id="1550" class="pw-post-body-paragraph la lb it lc b ld mg ju lf lg mh jx li lj mi ll lm ln mj lp lq lr mk lt lu lv im bi translated">更重要的是，我们可以评估独特字符的数量，这是我们的词汇量。</p><pre class="lz ma mb mc gt ni nj nk nl aw nm bi"><span id="b120" class="nn kj it nj b gy no np l nq nr">vocab = sorted(set(texts_p))<br/>print(f'{len(vocab)} unique characters in texts')</span><span id="1a94" class="nn kj it nj b gy ns np l nq nr">156 unique characters in texts</span></pre><p id="3733" class="pw-post-body-paragraph la lb it lc b ld mg ju lf lg mh jx li lj mi ll lm ln mj lp lq lr mk lt lu lv im bi translated">在训练之前，我们需要将字符串转换成一些数字表示。我们从记住一些重要的方面开始对文本进行标记。我们考虑了无限数量的令牌，并在角色级别创建了它们。我们没有过滤任何字符，并保持原来的大写。然后，我们使用标记器将文本映射到编码序列。</p><pre class="lz ma mb mc gt ni nj nk nl aw nm bi"><span id="8cd7" class="nn kj it nj b gy no np l nq nr">def create_character_tokenizer(list_of_strings):<br/>    tokenizer = Tokenizer(filters=None,<br/>                         char_level=True, <br/>                          split=None,<br/>                         lower=False)<br/>    tokenizer.fit_on_texts(list_of_strings)<br/>    return tokenizer<br/><br/>tokenizer = create_character_tokenizer(texts)<br/><br/>tokenizer_config = tokenizer.get_config()<br/><br/>word_counts = json.loads(tokenizer_config['word_counts'])<br/>index_word = json.loads(tokenizer_config['index_word'])<br/>word_index = json.loads(tokenizer_config['word_index'])<br/><br/>def strings_to_sequences(tokenizer, list_of_strings):<br/>    sentence_seq = tokenizer.texts_to_sequences(list_of_strings)<br/>    return sentence_seq<br/><br/>seq_texts = strings_to_sequences(tokenizer, texts)</span></pre><p id="4488" class="pw-post-body-paragraph la lb it lc b ld mg ju lf lg mh jx li lj mi ll lm ln mj lp lq lr mk lt lu lv im bi translated">我们可以看到这种编码的一个例子。</p><pre class="lz ma mb mc gt ni nj nk nl aw nm bi"><span id="00e2" class="nn kj it nj b gy no np l nq nr">print('Original sequence: \n' + texts[0] + '\n')<br/>print('Encoded sequence: ')<br/>print(seq_texts[0])</span><span id="4aa4" class="nn kj it nj b gy ns np l nq nr">Original sequence: <br/>Diana através dos ramos<br/><br/>Encoded sequence: <br/>[46, 6, 3, 8, 3, 1, 3, 9, 7, 3, 19, 26, 5, 1, 10, 4, 5, 1, 7, 3, 11, 4, 5]</span></pre><p id="9474" class="pw-post-body-paragraph la lb it lc b ld mg ju lf lg mh jx li lj mi ll lm ln mj lp lq lr mk lt lu lv im bi translated">我们还需要标准化序列的长度，为此我们定义了300个字符的长度。小于300的序列用零填充，而大于300的序列被截断。</p><pre class="lz ma mb mc gt ni nj nk nl aw nm bi"><span id="c927" class="nn kj it nj b gy no np l nq nr">mylen = np.vectorize(len)<br/><br/>print(max(mylen(texts)))<br/>print(np.round(np.mean(mylen(texts))))</span><span id="d93c" class="nn kj it nj b gy ns np l nq nr">1377<br/>71.0</span><span id="f131" class="nn kj it nj b gy ns np l nq nr">def make_padded_dataset(sequences):<br/>    padded_sequence = tf.keras.preprocessing.sequence.pad_sequences(sequences,<br/>                                                 maxlen=300,<br/>                                                 padding='pre',<br/>                                                 truncating='pre',<br/>                                                 value=0)<br/>    return padded_sequence<br/><br/>padded_sequences = make_padded_dataset(seq_texts)</span></pre><p id="0cd0" class="pw-post-body-paragraph la lb it lc b ld mg ju lf lg mh jx li lj mi ll lm ln mj lp lq lr mk lt lu lv im bi translated">RNN通过接收字符序列并预测序列中的下一个字符来工作。在训练时，模型接收一个输入序列和一个移位一的目标序列。</p><p id="f11e" class="pw-post-body-paragraph la lb it lc b ld mg ju lf lg mh jx li lj mi ll lm ln mj lp lq lr mk lt lu lv im bi translated">例如，表达式<code class="fe nv nw nx nj b">Diana através dos ramos</code>是我们数据集上第一首诗的第一节。这首诗来自Ricardo Reis，他是F. Pessoa的许多别称之一。给定输入，<code class="fe nv nw nx nj b">Diana através dos ramo</code>正确的预测是<code class="fe nv nw nx nj b">iana através dos ramos</code>。请注意，预测与输入的长度相同。</p><p id="a237" class="pw-post-body-paragraph la lb it lc b ld mg ju lf lg mh jx li lj mi ll lm ln mj lp lq lr mk lt lu lv im bi translated">我们采取的另一个决定是将我们的RNN构建为有状态的，这意味着它的内部状态跨批维护。为了提高效率，我们需要确保每个批处理元素都跟随前一个批处理的相应元素。</p><pre class="lz ma mb mc gt ni nj nk nl aw nm bi"><span id="23ea" class="nn kj it nj b gy no np l nq nr">def create_inputs_and_targets(array_of_sequences, batch_size=32):<br/>    input_seq = array_of_sequences[:,:-1]<br/>    target_seq = array_of_sequences[:,1:]<br/>    <br/>    # Prepare the batches and ensure that is ready to be fed to a stateful RNN<br/>    <br/>    num_examples = input_seq.shape[0]<br/><br/>    num_processed_examples = num_examples - (num_examples % batch_size)<br/><br/>    input_seq = input_seq[:num_processed_examples]<br/>    target_seq = target_seq[:num_processed_examples]<br/><br/>    steps = int(num_processed_examples / 32) <br/><br/>    inx = np.empty((0,), dtype=np.int32)<br/>    for i in range(steps):<br/>        inx = np.concatenate((inx, i + np.arange(0, num_processed_examples, steps)))<br/><br/>    input_seq_stateful = input_seq[inx]<br/>    target_seq_stateful = target_seq[inx]<br/>    <br/>    # Split data between training and validation sets<br/>    <br/>    num_train_examples = int(batch_size * ((0.8 * num_processed_examples) // batch_size))<br/><br/>    input_train = input_seq_stateful[:num_train_examples]<br/>    target_train = target_seq_stateful[:num_train_examples]<br/><br/>    input_valid = input_seq_stateful[num_train_examples:]<br/>    target_valid = target_seq_stateful[num_train_examples:]<br/>    <br/>    # Create datasets objects for training and validation data<br/>    <br/>    dataset_train = tf.data.Dataset.from_tensor_slices((input_train, target_train))<br/>    dataset_train = dataset_train.batch(batch_size, drop_remainder=True)<br/><br/>    dataset_valid = tf.data.Dataset.from_tensor_slices((input_valid, target_valid))<br/>    dataset_valid = dataset_valid.batch(batch_size, drop_remainder=True)<br/>    <br/>    return (dataset_train, dataset_valid)<br/>    <br/><br/>train_data, valid_data = create_inputs_and_targets(padded_sequences)</span></pre><h1 id="7ee0" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">3.递归神经网络</h1><p id="e790" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们首先定义了一个嵌入层，将我们的字符索引转换成固定大小的密集向量。值得注意的是，填充值在这一层被屏蔽，这意味着它们被简单地忽略。接下来，我们堆叠了两个单向有状态LSTM层，每个层有512个单元。这些层具有学习长期依赖性的潜力；然而，训练它们在计算上是昂贵的。在它们之间，我们引入了一个辍学层。最后，最后一层为词汇表中的每个字符输出一个logit。这些是根据模型的每个字符的对数似然性。请注意，我们总共有大约4M个参数需要训练。</p><pre class="lz ma mb mc gt ni nj nk nl aw nm bi"><span id="4a7b" class="nn kj it nj b gy no np l nq nr">def get_model(vocab_size, batch_size):<br/>    model = tf.keras.Sequential([<br/>        tf.keras.layers.Embedding(input_dim=vocab_size, output_dim = 256, mask_zero=True, batch_input_shape=(batch_size, None)),<br/>        tf.keras.layers.LSTM(units=512, return_sequences=True,stateful=True),<br/>        tf.keras.layers.Dropout(0.2),<br/>        tf.keras.layers.LSTM(units=512, return_sequences=True,stateful=True),<br/>        tf.keras.layers.Dense(units=vocab_size)<br/>    ])<br/>    return model</span><span id="6ce4" class="nn kj it nj b gy ns np l nq nr">batch_size=32<br/>model = get_model(len(tokenizer.word_index) + 1, batch_size)<br/>model.summary()</span><span id="0446" class="nn kj it nj b gy ns np l nq nr">Model: "sequential"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape              Param #   <br/>=================================================================<br/>embedding (Embedding)        (32, None, 256)           40192     <br/>_________________________________________________________________<br/>lstm (LSTM)                  (32, None, 512)           1574912   <br/>_________________________________________________________________<br/>dropout (Dropout)            (32, None, 512)           0         <br/>_________________________________________________________________<br/>lstm_1 (LSTM)                (32, None, 512)           2099200   <br/>_________________________________________________________________<br/>dense (Dense)                (32, None, 157)           80541     <br/>=================================================================<br/>Total params: 3,794,845<br/>Trainable params: 3,794,845<br/>Non-trainable params: 0<br/>_________________________________________________________________</span><span id="7786" class="nn kj it nj b gy ns np l nq nr">checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath='./models/ckpt',<br/>                                                       save_weights_only=True,<br/>                                                       save_best_only=True)<br/>model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),<br/>              metrics=['sparse_categorical_accuracy'])<br/>history = model.fit(train_data, <br/>                    epochs=30, <br/>                    validation_data=valid_data,<br/>                    callbacks=[checkpoint_callback, <br/>tf.keras.callbacks.EarlyStopping(patience=2)])</span><span id="1a08" class="nn kj it nj b gy ns np l nq nr">Epoch 1/30<br/>2023/2023 [==============================] - 1041s 512ms/step - loss: 0.5216 - sparse_categorical_accuracy: 0.3516 - val_loss: 0.3298 - val_sparse_categorical_accuracy: 0.5669<br/>[...]<br/>Epoch 18/30<br/>2023/2023 [==============================] - 1031s 510ms/step - loss: 0.2495 - sparse_categorical_accuracy: 0.6478 - val_loss: 0.2756 - val_sparse_categorical_accuracy: 0.6268</span><span id="ba8d" class="nn kj it nj b gy ns np l nq nr">def model_history(history):<br/>    history_dict = dict()<br/>    for k, v in history.history.items():<br/>        history_dict[k] = [float(val) for val in history.history[k]]<br/>    return history_dict<br/><br/><br/>history_dict = model_history(history)</span></pre><h1 id="35cc" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">4.结果</h1><p id="b9c2" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">即使使用GPU，训练也非常慢(尽管与CPU相比，训练时间减少了15倍)，回想一下，我们只堆叠了两个单元数量有限的LSTM层。从下图中，我们可以看到训练和验证数据集的准确性快速增加，然后在几个时期内稳定攀升。我们的回调最终被执行(当超过2个时期验证准确性没有增加时)以停止训练过程。没有过度合身的迹象。</p><pre class="lz ma mb mc gt ni nj nk nl aw nm bi"><span id="2818" class="nn kj it nj b gy no np l nq nr">def plot_history(history_dict):<br/>    <br/>    plt.figure(figsize=(15,5))<br/>    plt.subplot(121)<br/>    plt.plot(history_dict['sparse_categorical_accuracy'])<br/>    plt.plot(history_dict['val_sparse_categorical_accuracy'])<br/>    plt.title('Accuracy vs. epochs')<br/>    plt.ylabel('Accuracy')<br/>    plt.xlabel('Epoch')<br/>    plt.xticks(np.arange(len(history_dict['sparse_categorical_accuracy'])))<br/>    ax = plt.gca()<br/>    ax.set_xticklabels(1 + np.arange(len(history_dict['sparse_categorical_accuracy'])))<br/>    plt.legend(['Training', 'Validation'], loc='lower right')<br/><br/>    plt.subplot(122)<br/>    plt.plot(history_dict['loss'])<br/>    plt.plot(history_dict['val_loss'])<br/>    plt.title('Loss vs. epochs')<br/>    plt.ylabel('Loss')<br/>    plt.xlabel('Epoch')<br/>    plt.xticks(np.arange(len(history_dict['sparse_categorical_accuracy'])))<br/>    ax = plt.gca()<br/>    ax.set_xticklabels(1 + np.arange(len(history_dict['sparse_categorical_accuracy'])))<br/>    plt.legend(['Training', 'Validation'], loc='upper right')<br/>    plt.show() <br/>    <br/>plot_history(history_dict)</span></pre><figure class="lz ma mb mc gt md gh gi paragraph-image"><div role="button" tabindex="0" class="mm mn di mo bf mp"><div class="gh gi ny"><img src="../Images/9d2282273d8f0709b53aba5fb75fd8e3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2mKFmqxiwVzijmd-WKFXkQ.png"/></div></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">图3:RNN模型几个时期的精确度和损耗演变。</p></figure><pre class="lz ma mb mc gt ni nj nk nl aw nm bi"><span id="eb81" class="nn kj it nj b gy no np l nq nr">model = get_model(len(tokenizer.word_index) + 1, batch_size=1)<br/>model.load_weights(tf.train.latest_checkpoint('./models/')).expect_partial()<br/><br/>def get_logits(model, token_sequence, initial_state1=None, initial_state2=None, initial_state3=None):<br/>    token_sequence = np.asarray(token_sequence)<br/>    if initial_state1 is not None:<br/>        # set states for all recurrent layers<br/>        model.layers[1].states = initial_state1<br/>        model.layers[3].states = initial_state2<br/>        model.layers[5].states = initial_state3<br/>    else:<br/>        model.layers[1].reset_states()<br/>        model.layers[3].reset_states()<br/>        model.layers[5].reset_states()<br/>    logit = model.predict(token_sequence)<br/>    logit = logit[:,-1,:]<br/>    <br/>    return logit<br/><br/>def sample_token(logits):<br/>    pred = tf.random.categorical(logits, num_samples=1).numpy()[0]<br/>    return pred[0]</span></pre><p id="d965" class="pw-post-body-paragraph la lb it lc b ld mg ju lf lg mh jx li lj mi ll lm ln mj lp lq lr mk lt lu lv im bi translated">为了从我们的模型生成文本，我们需要指定一个种子字符串来启动网络。接下来，我们标记初始字符串并重置网络状态。然后，该字符串被转换为批量大小为1的张量，以提供给我们的模型。我们使用上一时间步的预测来构建分类分布，然后从中进行采样。使用我们的网络的相同状态和先前采样的令牌，我们可以重复预测步骤，直到我们获得具有指定大小的最终序列。</p><p id="87d6" class="pw-post-body-paragraph la lb it lc b ld mg ju lf lg mh jx li lj mi ll lm ln mj lp lq lr mk lt lu lv im bi translated">由此产生的原文分析起来还是挺有意思的。请记住，我们的RNN不得不从很小的数据集开始学习葡萄牙语。除了用葡萄牙语书写的实例之外，没有向模型提供诸如句法或语义的明确信息。对于这项任务来说，数据集也相当小。然而，还是有一些有趣的经验值得注意。比如在标点符号方面，引号的使用是正确的，表现出对要求开、闭的理解。在诸如"<em class="lx">desassessego no poderia！…falências no meu Cora o…</em>或“<em class="lx">As can es…um sono de ouvir…Fico tanto！…" </em>我们几乎可以理解费尔南多·佩索阿的一些无房可住的情况。另一方面，我们看到意义或意图不是RNN可以捕捉到的，我们也可以识别一些拼写错误。</p><pre class="lz ma mb mc gt ni nj nk nl aw nm bi"><span id="6327" class="nn kj it nj b gy no np l nq nr">init_string = 'Desassossego'<br/>num_generation_steps = 500</span><span id="1444" class="nn kj it nj b gy ns np l nq nr">token_sequence = tokenizer.texts_to_sequences([init_string])<br/>initial_state_1, initial_state_2, initial_state_3 = None, None, None<br/>input_sequence = token_sequence<br/><br/>for _ in range(num_generation_steps):<br/>    logits = get_logits(model, <br/>                        input_sequence, <br/>                        initial_state1=initial_state_1,<br/>                        initial_state2=initial_state_2,<br/>                        initial_state3=initial_state_3)<br/>    sampled_token = sample_token(logits)<br/>    token_sequence[0].append(sampled_token)<br/>    input_sequence = [[sampled_token]]<br/>    initial_state_1 = model.layers[1].states<br/>    initial_state_2 = model.layers[3].states<br/>    initial_state_2 = model.layers[5].states<br/>    <br/>print(tokenizer.sequences_to_texts(token_sequence)[0][::2])</span><span id="7e66" class="nn kj it nj b gy ns np l nq nr"><strong class="nj iu">Desassossego não poderia!... Falências no meu coração... Esse reer sobre os braços dos meus caminhos e ignorantes possamos «exensação simbólica» e em Natureza, e a noite nova da ausência de cada? Não pense de bem entendida uma orientada prosa). V. como fui... As canções... é um sono de ouvir... Ficção tanto!... Vejo outro olhar pela Tristeza da cadeira, rainha para a Carta, a noite. Depois no paganismo que se sente no espaço real e de criar uma pedra de tradição sociológica para implicar o de Aristoclator S</strong></span></pre><h1 id="8110" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">5.结论</h1><p id="027f" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">对于这项任务，数据的预处理是具有挑战性的。我们需要确保我们的输入序列以合适的方式编码，以便RNN有效地捕获可用的语义表示。训练rnn在计算上是昂贵的，所以我们决定保持结构尽可能简单。</p><p id="4190" class="pw-post-body-paragraph la lb it lc b ld mg ju lf lg mh jx li lj mi ll lm ln mj lp lq lr mk lt lu lv im bi translated">我们能够生成葡萄牙语文本，除了诗人的作品之外，无需向模型提供任何关于该语言的结构信息。该模型学习了语言的一些基本结构，同时保留了我们可以认为类似于训练语料库的细微差别。</p><p id="44c6" class="pw-post-body-paragraph la lb it lc b ld mg ju lf lg mh jx li lj mi ll lm ln mj lp lq lr mk lt lu lv im bi translated">这种方法可以通过增加模型的深度和每层中单元的数量来扩展。还可以调整批次大小等超参数来提高准确性。我们测试了以写作形式区分的可能性，一个DNN用散文文本训练，另一个用诗歌文本训练。结果并不令人满意，因为DNNs无法生成具有连贯结构的文本。我们把它作为未来的工作。</p><p id="3951" class="pw-post-body-paragraph la lb it lc b ld mg ju lf lg mh jx li lj mi ll lm ln mj lp lq lr mk lt lu lv im bi translated">保持联系:<a class="ae lw" href="https://www.linkedin.com/in/luisbrasroque/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a></p><h1 id="d78e" class="ki kj it bd kk kl km kn ko kp kq kr ks jz kt ka ku kc kv kd kw kf kx kg ky kz bi translated">6.参考</h1><p id="faf4" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated"><a class="ae lw" href="https://arxiv.org/pdf/1801.00632.pdf" rel="noopener ugc nofollow" target="_blank">【1】</a>——【德·布姆等人，2018】德·布姆，c .、德·梅斯特，t .、Dhoedt，B. (2018)。实践中的字符级递归神经网络:训练和采样方案的比较。神经计算与应用，31(8):4001–4017。</p><p id="6a01" class="pw-post-body-paragraph la lb it lc b ld mg ju lf lg mh jx li lj mi ll lm ln mj lp lq lr mk lt lu lv im bi translated"><a class="ae lw" href="https://icml.cc/Conferences/2011/papers/524_icmlpaper.pdf" rel="noopener ugc nofollow" target="_blank">【2】</a>——【苏茨基弗等人，2011年】苏茨基弗本人、马滕斯律师和辛顿律师(2011年)。用递归神经网络生成文本。ICML 11，第1017-1024页，麦迪逊，威斯康星州，美国。全媒体。</p><p id="a5d7" class="pw-post-body-paragraph la lb it lc b ld mg ju lf lg mh jx li lj mi ll lm ln mj lp lq lr mk lt lu lv im bi translated"><a class="ae lw" href="https://www.kaggle.com/luisroque/the-complete-literary-works-of-fernando-pessoa" rel="noopener ugc nofollow" target="_blank">【3】</a>—<a class="ae lw" href="https://www.kaggle.com/luisroque/the-complete-literary-works-of-fernando-pessoa" rel="noopener ugc nofollow" target="_blank">https://www . ka ggle . com/Luis Roque/the-complete-literal-works-of-Fernando-pes SOA</a></p></div></div>    
</body>
</html>