<html>
<head>
<title>What Makes Graph Convolutional Networks Work?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">图卷积网络的工作原理是什么？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-makes-graph-convolutional-networks-work-53badade0ce9?source=collection_archive---------18-----------------------#2021-05-04">https://towardsdatascience.com/what-makes-graph-convolutional-networks-work-53badade0ce9?source=collection_archive---------18-----------------------#2021-05-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="b821" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">GCNs背后的数学直觉的分解。</h2></div><p id="b4b2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">这是机器学习中</em> <a class="ae lc" href="https://medium.com/mlearning-ai/why-graph-theory-is-cooler-than-you-thought-4df73e2a4950" rel="noopener"> <em class="lb">图</em></a><em class="lb"/><a class="ae lc" href="https://medium.datadriveninvestor.com/graph-convolutional-networks-explained-d88566682b8f?source=your_stories_page-------------------------------------" rel="noopener ugc nofollow" target="_blank"><em class="lb">图论</em> </a> <em class="lb">系列的第三部分。</em></p><p id="8722" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">到目前为止，如果您一直在阅读本系列文章，您可能已经了解了一些图论知识，为什么我们在数据科学中关注图结构数据，以及什么是“图卷积网络”。现在，我想向您简要介绍一下<em class="lb">这些东西是如何工作的。</em></p><p id="821f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">对于本领域中可能对这个主题有所了解的朋友，我将谈论</em> <a class="ae lc" href="http://arxiv.org/abs/1609.02907" rel="noopener ugc nofollow" target="_blank"> <em class="lb"> Kipf &amp; Welling的</em> </a> <em class="lb"> 2017年关于半监督学习的图卷积的论文(Thomas Kipf在这里</em>  <em class="lb">写了一个对这个概念更容易理解的解释</em> <a class="ae lc" href="https://tkipf.github.io/graph-convolutional-networks/" rel="noopener ugc nofollow" target="_blank"> <em class="lb">)背后的直觉，尽管自改进这种方法以来已经开发了其他方法。我的目标是介绍数学概念，而不是提供该主题的广泛概述。</em></a></p><p id="b0e2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，如果你不是数学家、数据科学家，甚至不是计算机科学家，先不要离开。目标是将学术论文和概念分解，以便任何人都能理解。请在最后留下回复，让我知道我是否实现了这个目标！</p><h2 id="69c3" class="ld le iq bd lf lg lh dn li lj lk dp ll ko lm ln lo ks lp lq lr kw ls lt lu lv bi translated">我们走吧！— GCNs，目标是什么？</h2><p id="169a" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko ly kq kr ks lz ku kv kw ma ky kz la ij bi translated">在Thomas N. Kipf的博客文章<a class="ae lc" href="https://tkipf.github.io/graph-convolutional-networks/" rel="noopener ugc nofollow" target="_blank"/>中，他说图卷积网络的目标是:</p><blockquote class="mb mc md"><p id="4d8f" class="kf kg lb kh b ki kj jr kk kl km ju kn me kp kq kr mf kt ku kv mg kx ky kz la ij bi translated">“……学习图G =(v，e)上信号/特征的函数”</p></blockquote><p id="5321" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但这到底意味着什么呢？我们来分解一下。</p><p id="1b79" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对<em class="lb">图</em>或<em class="lb">节点进行<em class="lb">某种预测或分类</em>有不同的方法。</em>但我们关注的是图卷积网络，对于<em class="lb">分类关注的是</em> <strong class="kh ir"> <em class="lb">图*。</em> </strong>这意味着，为了提供一个具体的例子，对于类似于<strong class="kh ir">蛋白质的东西，</strong>其具有由整个图形表示的结构:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi mh"><img src="../Images/ef90ef698e5a997262e51a1c7135abb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:822/1*Mi3tQlobhC5r1k6Ima7Z5Q.gif"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">蛋白质的基本结构，其中多个氨基酸通过肽键连接，由<a class="ae lc" href="https://swift.cmbi.umcn.nl/teach/alg/infopages/peptide_bond.gif" rel="noopener ugc nofollow" target="_blank">来源</a>提供。</p></figure><p id="8ad7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们可能需要一种方法来嵌入这个图的<strong class="kh ir">特征</strong>，这样我们就可以<strong class="kh ir">对我们正在查看的蛋白质进行分类<em class="lb">。</em> </strong></p><p id="ad4f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以，让我们回到我们上面的引用。我们想学习图上特征的<em class="lb">函数</em>。换句话说，我们希望了解特征之间的关系——在蛋白质的例子中，这可能是某些原子之间的特定键表示，或者原子表示本身——给定我们图形的特定网络结构。换句话说:<strong class="kh ir"> <em class="lb">节点和键</em> </strong>(边缘)<strong class="kh ir"> <em class="lb"> </em> </strong>的组合如何影响我们正在观察的<strong class="kh ir"> <em class="lb">蛋白质(目标标签)？</em> </strong></p><p id="456c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">免责声明:就像这篇文章中的许多事情一样，这有点过于简单化了。这样做的目的是分享GCNs背后的直觉，并让它变得容易理解。对于完整的解释和数学证明，我鼓励你更深入地研究底部引用的论文。</p><p id="8ce4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">为什么不是普通的卷积神经网络？</strong>我们已经在<a class="ae lc" href="https://medium.datadriveninvestor.com/graph-convolutional-networks-explained-d88566682b8f?source=your_stories_page-------------------------------------&amp;gi=8ec29311e21f" rel="noopener ugc nofollow" target="_blank">之前的文章</a>中谈到了传统机器学习&amp;深度学习模型在面对任意结构化(非欧几里德)数据时面临的困难。GCN允许我们将图表结构嵌入到二维表示中，克服了如果我们试图使用传统CNN对该图表进行分类时会遇到的障碍。</p><p id="5612" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你可以把嵌入想象成一个三维分子模型，像这样:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi mt"><img src="../Images/7299718476658930840e874403812976.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fLwcpnFcUSS97TwlIxrJ2w.jpeg"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">照片由Unsplash上的<a class="ae lc" href="https://unsplash.com/@vlisidis" rel="noopener ugc nofollow" target="_blank">特里·维利迪斯</a>提供</p></figure><p id="b580" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">并把它画在一张纸上，以便我们随身携带和记忆。</p><h2 id="2ccd" class="ld le iq bd lf lg lh dn li lj lk dp ll ko lm ln lo ks lp lq lr kw ls lt lu lv bi translated">为了理解如何实现，让我们看一下正向传播。</h2><p id="561d" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko ly kq kr ks lz ku kv kw ma ky kz la ij bi translated">我可能会把我的学习风格投射到我的读者身上，但是为了真正完全掌握Kipf提出的公式，我必须回到基础，回顾一下向前传播。让我们花一点时间来看看正向传播公式:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi my"><img src="../Images/f1bc18a491e294e80773b4ef53f397de.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aTXzY8HhSGCfloTfcXvopA.png"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">正则神经网络的前向传播方程。图片来自<a class="ae lc" rel="noopener" target="_blank" href="/understanding-graph-convolutional-networks-for-node-classification-a2bfdb7aba7b">来源</a></p></figure><p id="565f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不要让这个公式吓到你。它只是说在下一层的<em class="lb">的特征表示等于在当前层</em>的权重上执行<em class="lb">某种激活函数</em>的结果乘以当前层的<em class="lb">特征表示加上当前层的偏差。</em></p><p id="edb7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">也许那仍然感觉是稠密的，如果是的话，那也没关系。你可以这样想:<em class="lb">这个公式表示的是我们为了得到一个输出而采取的一系列计算步骤，</em>将结果通过我们神经网络中的每一层，直到<em class="lb"> i </em>是我们的最终(输出)层。(如果你知道编程的基础，你可以把它概念化为一个for循环:“对于我们神经网络的每一层，执行这个计算以返回一个最终值。”)</p><p id="3e85" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的<em class="lb">激活函数</em>(表示为sigma) <em class="lb"> </em>可能是类似ReLu的东西，我们的<em class="lb">特征表示</em>可以被认为是以我们的蛋白质为例，其中我们分子中的每个原子都由它的原子序数表示(为了我们的简单例子)。然后，我们希望最后一层的最终输出是我们正在查看的蛋白质的<em class="lb">表示— </em>我们的分类。现在不要太担心偏见，如果你不知道它是什么，我们不会在本文中进一步讨论它。</p><blockquote class="mb mc md"><p id="1037" class="kf kg lb kh b ki kj jr kk kl km ju kn me kp kq kr mf kt ku kv mg kx ky kz la ij bi translated"><strong class="kh ir"> <em class="iq">签到</em> </strong></p><p id="8653" class="kf kg lb kh b ki kj jr kk kl km ju kn me kp kq kr mf kt ku kv mg kx ky kz la ij bi translated">如果你对这一切有点陌生，你的大脑现在可能正在受伤，这没关系。在我们继续之前，让我们花一点时间来回顾和消化我们所学的内容</p><p id="6b59" class="kf kg lb kh b ki kj jr kk kl km ju kn me kp kq kr mf kt ku kv mg kx ky kz la ij bi translated"><em class="iq"> 1。GCN的一个目标是将任意结构的图嵌入到网络的二维表示中。</em></p><p id="838a" class="kf kg lb kh b ki kj jr kk kl km ju kn me kp kq kr mf kt ku kv mg kx ky kz la ij bi translated"><em class="iq"> 2。此外，我们想了解图<em class="iq">上的特征的</em>功能——我们想知道</em>内容<em class="iq">如何影响其他</em>内容<em class="iq">(我们图中的特征如何影响我们的目标)。</em></p><p id="44a1" class="kf kg lb kh b ki kj jr kk kl km ju kn me kp kq kr mf kt ku kv mg kx ky kz la ij bi translated">3.<em class="iq">我们已经回顾了常规神经网络的</em>前向传播<em class="iq">，并且理解了这个等式告诉我们什么。</em></p></blockquote><h2 id="17df" class="ld le iq bd lf lg lh dn li lj lk dp ll ko lm ln lo ks lp lq lr kw ls lt lu lv bi translated">正向传播——这与GCNs有什么关系？</h2><p id="b8a5" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko ly kq kr ks lz ku kv kw ma ky kz la ij bi translated">与任何神经网络类似，图卷积网络需要一种方法来通过层向前传播值以实现其目标。然而，假设我们的数据不是欧几里得的，我们需要对上面讨论的常规正向传播方程做一些小的调整。</p><p id="9060" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，我们需要一种计算方式来表示我们的图形。如果你一直记得我关于图论的第一篇文章，或者如果你记得你的数学(或者，对于房间里的计算机科学家来说，你的数据结构课程)，你会记得表示一个图的一种方式是用一个<strong class="kh ir">邻接矩阵</strong>。让我们看一个例子:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi mz"><img src="../Images/68a548504eb62aceb8d2145db93df68c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wtPaHzg5KskO3EdQt80_ag.png"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">无向图的邻接矩阵，如左图所示。</p></figure><p id="019d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的邻接矩阵实际上是一个<em class="lb">稀疏矩阵</em>，其中我们有代表<strong class="kh ir">节点标签</strong>的行和列，以及任何两个节点是否相互连接的二进制表示。0对应“无连接”，其中1代表连接(一个<strong class="kh ir">边沿</strong>)。</p><p id="2f00" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所以，我们有了GCN难题中第一个缺失的部分:我们的<strong class="kh ir">邻接矩阵</strong>，我们称之为<strong class="kh ir"> A. </strong></p><p id="5291" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们需要确保将<strong class="kh ir"> A </strong>插入到我们的传播方程中。<em class="lb">由于我说过不要担心偏见(并且由于我引用的文章中的图像也省略了图像中的这个值，这样做很好)，您在下面的表示中看不到它。</em></p><p id="b75c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们看看目前为止的等式:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi na"><img src="../Images/17203c5e58b779c71eba89614ee3e01f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F7aj3Y2mpSJ9Wktp2tVaqA.png"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图卷积网络的前向传播方程。图片由<a class="ae lc" rel="noopener" target="_blank" href="/understanding-graph-convolutional-networks-for-node-classification-a2bfdb7aba7b">来源</a>提供，添加了我自己的注释。</p></figure><p id="bf01" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">但是我们还没有完成。有两个问题:</p><ol class=""><li id="ca19" class="nb nc iq kh b ki kj kl km ko nd ks ne kw nf la ng nh ni nj bi translated">如果我们只查看<strong class="kh ir"> A，</strong>我们将查看给定节点的所有邻居，而不是该节点本身。我们一会儿会处理这个问题。</li><li id="36cf" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated">什么是A*？正如我们最初讨论的那样，一个没有被规范化。由于我们必须将我们的权重和特征表示乘以我们的稀疏邻接矩阵<strong class="kh ir"> A，</strong>我们可以预计，特征表示和权重的比例将随着<strong class="kh ir"> A的比例而显著变化。因此，A*是归一化的A。</strong>让我们来讨论这意味着什么:</li></ol><p id="1e5d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">Kipf &amp; Welling引入了一种方法来规范化处理第二个问题的。首先我们需要<a class="ae lc" href="https://en.wikipedia.org/wiki/Degree_matrix" rel="noopener ugc nofollow" target="_blank"> <em class="lb">对角节点度矩阵</em> </a> <em class="lb">，</em> <strong class="kh ir"> <em class="lb"> D. </em> </strong>如果那句话感觉很混乱，可以看看链接的文章！归一化<strong class="kh ir"> A </strong>的方法之一是乘以<strong class="kh ir"> D，</strong>但是基普夫&amp;韦林向我们介绍了另一种叫做对称归一化的方法。这相当于:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi np"><img src="../Images/50e8453e155dca915a40ff73bfedfee1.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/format:webp/1*Ra_5BaRtTMzjvxu3FVauHg.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">邻接矩阵(A)和度矩阵(D)的对称归一化表示，承蒙<a class="ae lc" href="https://tkipf.github.io/graph-convolutional-networks/" rel="noopener ugc nofollow" target="_blank">来源</a>。</p></figure><p id="7aa0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当我们提供最终方程式时，您将看到这一点！现在，我们已经解决了我们的正常化问题，我们就快到了。我们还有一点要补充:自循环。</p><p id="cf8e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们想要学习我们的图中所有节点的表示，这意味着我们需要改变我们的图。因为这已经是一篇很长的文章了，我将再次引用Thomas Kipf在他的<a class="ae lc" href="https://tkipf.github.io/graph-convolutional-networks/" rel="noopener ugc nofollow" target="_blank">博客</a>中的话来解释:</p><blockquote class="mb mc md"><p id="548b" class="kf kg lb kh b ki kj jr kk kl km ju kn me kp kq kr mf kt ku kv mg kx ky kz la ij bi translated">“……与A相乘意味着，对于每个节点，我们对所有相邻节点的所有特征向量求和，而不是对节点本身求和(除非图中存在自循环)。我们可以通过在图中强制执行自循环来“解决”这个问题:我们只需将单位矩阵添加到a中。</p></blockquote><p id="2133" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里，我们到了下一步:加强自我循环。这意味着我们需要我们的节点也连接回自己，看起来像这样:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/65fdbdd3b408d8c6a70c963849dcbdd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:346/format:webp/1*iQ_vEMSVkrM-3q9XEv3yLA.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">单个节点上的自循环，由<a class="ae lc" href="https://commons.wikimedia.org/wiki/File:Self-loop.png" rel="noopener ugc nofollow" target="_blank">源</a>提供。</p></figure><p id="cb79" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果你不能100%确定Kipf在他关于<em class="lb">单位矩阵</em>的引用中所说的话，请记住单位矩阵是一个<em class="lb"> n </em> x <em class="lb"> n </em>矩阵<em class="lb">，1的</em>在主对角线上(左上到右下),其他地方为零:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/e55ce1c58a90f495d61ee5f582ca487a.png" data-original-src="https://miro.medium.com/v2/resize:fit:934/format:webp/1*DX_owLAsSYgqa6gw2nsIOw.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">单位矩阵<em class="ns">n×n，承蒙</em> <a class="ae lc" href="https://en.wikipedia.org/wiki/Identity_matrix" rel="noopener ugc nofollow" target="_blank"> <em class="ns">出处</em> </a> <em class="ns">。</em></p></figure><p id="b55e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">其中<em class="lb"> n </em>在我们的用例中是邻接矩阵<strong class="kh ir"> A. </strong>的维数</p><p id="d104" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，我们已经用A的度矩阵对A进行了归一化，并通过添加我们的单位矩阵来实施自循环，我们得到了图卷积网络中前向传播的最终方程:</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/8704856a331f7b8936303e61b8918de1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1340/format:webp/1*s5M3nMa8M9NUQrlP7WRq9w.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">图卷积网络的前向传播方程——推导邻接矩阵(A)的层(l)特征表示函数的方程，由<a class="ae lc" href="https://tkipf.github.io/graph-convolutional-networks/" rel="noopener ugc nofollow" target="_blank">来源</a>提供，并添加了我自己的注释。</p></figure><p id="e1c0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们分解这个新方程。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/521021f4b44f893d1674c30ca9b49a90.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*y5NRVru7tL-Zmr4WLE6y2g.png"/></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">GCNs前向传播方程的第一条款。</p></figure><p id="f338" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这一块写着:“给定邻接矩阵<strong class="kh ir"> A </strong>，我们的特征对于层<em class="lb"> l </em>的作用”。<em class="lb">这就是我们希望为</em>解决的问题。记住我们最初的目标之一，为一个图寻找特征的函数。</p><figure class="mi mj mk ml gt mm gh gi paragraph-image"><div role="button" tabindex="0" class="mu mv di mw bf mx"><div class="gh gi nv"><img src="../Images/6a218988933d801aa854ddc40f0e1b9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1pFwCZKDTjoWck51j2Qbkw.png"/></div></div><p class="mp mq gj gh gi mr ms bd b be z dk translated">GCNs前向传播方程的第二条款。</p></figure><p id="95b3" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们分解第二部分。请记住，这是等号的右边-这是我们用来寻找我们的功能的功能。</p><p id="4b54" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">记住sigma (𝝈)代表一个激活函数(比如ReLu)。所以这个子句表示的是β的对角度矩阵(<em class="lb"> D̂ </em> )* <em class="lb"> </em>上的激活函数的结果，这是我们添加到单位矩阵的邻接矩阵(a)，我们已经使用“对称归一化”对其进行了归一化。这些然后乘以我们的特征表示和我们当前层的权重，<em class="lb"> l. </em></p><p id="aeb7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">等等——让我们后退一步。</p><p id="70eb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们谈谈拼图的最后一块。还记得我们如何谈论我们的蛋白质例子，以及我们的节点如何可能有一个<em class="lb">标签，</em>像一个原子的名称或缩写(即:碳，或C)，以及一个<em class="lb">特征表示，</em>可能像它们的原子序数？</p><p id="b69d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">本例中的这些表示成为我们的<strong class="kh ir">特征表示向量</strong>，对于上面的等式，表示<em class="lb"> l. </em>的<em class="lb"> H </em></p><h2 id="eeec" class="ld le iq bd lf lg lh dn li lj lk dp ll ko lm ln lo ks lp lq lr kw ls lt lu lv bi translated">用几句话概括一下</h2><p id="bc1e" class="pw-post-body-paragraph kf kg iq kh b ki lw jr kk kl lx ju kn ko ly kq kr ks lz ku kv kw ma ky kz la ij bi translated">你刚刚学习了如何找到给定图的特征的<strong class="kh ir">函数，给定图的邻接矩阵<em class="lb"> A、</em>d、</strong></p><p id="5d01" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">您已经回顾了前向传播，并了解了它与图卷积网络的不同之处。您已经了解了在处理任意结构化数据时需要考虑的一些重要注意事项，比如拥有或创建自循环以及规范化邻接矩阵的重要性。</p><p id="b143" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不错！你已经为下一步做好了充分的准备——这一步很快就会到来——这一步应该会有趣得多:用gcn对东西进行实际分类。</p><p id="de16" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">如果您想在下一次之前了解更多，请查阅所有在本文写作过程中非常有价值的论文和资料:</p><ol class=""><li id="08cc" class="nb nc iq kh b ki kj kl km ko nd ks ne kw nf la ng nh ni nj bi translated"><a class="ae lc" href="https://arxiv.org/abs/1609.02907" rel="noopener ugc nofollow" target="_blank">图卷积网络半监督分类，</a> Kipf &amp; Welling (ICLR 2017)</li><li id="b80d" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated"><a class="ae lc" href="https://tkipf.github.io/graph-convolutional-networks/" rel="noopener ugc nofollow" target="_blank">图卷积网络</a>，托马斯·基普夫</li><li id="1952" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated"><a class="ae lc" rel="noopener" target="_blank" href="/understanding-graph-convolutional-networks-for-node-classification-a2bfdb7aba7b">理解用于节点分类的图卷积网络</a>，Inneke Mayachita</li></ol><p id="c5ae" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">注意事项:</p><ol class=""><li id="eedb" class="nb nc iq kh b ki kj kl km ko nd ks ne kw nf la ng nh ni nj bi translated"><em class="lb"> *GCNs也可以用于节点级分类，但是为了简化示例，我们在这里不关注这一点。</em></li><li id="0ab2" class="nb nc iq kh b ki nk kl nl ko nm ks nn kw no la ng nh ni nj bi translated"><em class="lb">*这代表‘D-hat’，Medium的数学符号支持有点欠缺</em></li></ol></div></div>    
</body>
</html>