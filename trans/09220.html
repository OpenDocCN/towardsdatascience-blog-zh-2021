<html>
<head>
<title>LASSO Increases the Interpretability and Accuracy of Linear Models</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">LASSO增加了线性模型的可解释性和准确性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/lasso-increases-the-interpretability-and-accuracy-of-linear-models-c1b340561c10?source=collection_archive---------15-----------------------#2021-08-26">https://towardsdatascience.com/lasso-increases-the-interpretability-and-accuracy-of-linear-models-c1b340561c10?source=collection_archive---------15-----------------------#2021-08-26</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a800" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解套索的工作原理和原因</h2></div><p id="8e05" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">由<a class="ae le" href="https://www.linkedin.com/in/edkrueger/" rel="noopener ugc nofollow" target="_blank">爱德华·克鲁格</a>、<a class="ae le" href="https://www.linkedin.com/in/erin-oefelein-3105a878/" rel="noopener ugc nofollow" target="_blank">艾琳·欧菲莱因</a>和<a class="ae le" href="https://www.linkedin.com/in/michael-a-strenk-72a6b532/" rel="noopener ugc nofollow" target="_blank">迈克尔·斯特兰克</a></p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi lf"><img src="../Images/55297a7d5c9b5e55e7dd68b427f27398.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*BmjP1jz-eRYxx44wa7FQkQ.jpeg"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">Joshua Sukoff 在<a class="ae le" href="https://unsplash.com/s/photos/rodeo?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="c226" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">为什么是套索？</h1><p id="7d33" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">LASSO或L1正则化是一种可用于改进许多模型的技术，包括广义线性模型(GLMs)和神经网络。LASSO代表“最小绝对收缩和选择操作符”然而，你可能想知道是短语还是缩写先出现的。</p><p id="55e5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">套索执行子集选择</strong></p><p id="07cc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在线性回归、泊松回归和逻辑回归等(GLMs)情况下，LASSO可以选择要素子集。子集选择通常通过消除特征和预测度量以及适用模型的泛化来增强GLMs的可解释性。GLMs中的LASSO功能强大，因为它可以内生地选择子集-无需构建大量不同的模型并与要素子集进行比较。</p><p id="1c9f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">套索提高了可解释性</strong></p><p id="b7bc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">LASSO相对于许多其他子集选择方法的另一个优势是，它倾向于具有较少共线性的要素子集。由于预测指标通常不会受到共线性的影响，依赖于预测指标的子集选择技术通常无法排除高度相关的变量。</p><p id="dc86" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然而，对于解释和因果推断，共线性可能是毁灭性的。</p><p id="0c0e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，考虑在此处找到的<a class="ae le" href="https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset" rel="noopener ugc nofollow" target="_blank">sci kit-Learn糖尿病数据集。目标是衡量糖尿病的进展。标记为“s1”到“s6”的六个特征是每个受试者在不同时间进行的血清测量的记录。</a></p><p id="2fb7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">即使只有一些读数是因果上重要的，线性回归和其他GLMs也会给每个特征一些权重。为什么？</p><p id="957e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">因为非因果读数与因果读数相关，而因果读数与目标相关，<em class="ms">非因果无关读数与目标相关</em>。</p><p id="8e2c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">后果不仅是非因果特征会影响模型中的目标，而且模型会对因果特征产生错误的影响！直观上，通过添加与另一个特征相关的特征，估计过程将在两个特征之间分割原始特征的因果权重。</p><p id="f8e0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当要素之间存在共线性时，LASSO倾向于将这些要素系数中的一部分降低为零。</p><p id="0062" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果您有兴趣查看共线性如何影响推理的模拟，或者如何使用SciKit-Learn将LASSO应用于糖尿病数据集，<a class="ae le" href="https://edkruegerdata.com/subscribe" rel="noopener ugc nofollow" target="_blank">订阅我的邮件列表</a>以获取即将发布的文章。</p><p id="ebdc" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">正则化提高了准确性和泛化能力</strong></p><p id="1d75" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">更少的特征可以导致具有更好预测指标的更精确的模型，这可能是反直觉的。尽管如此，在训练中，尤其是在小型或中型数据集上，该模型将识别无意义特征和目标之间的弱关联。在极端情况下，当对模型评分时，一些观察值将具有无意义特征的相对高或低的值，从而导致极端的预测。</p><p id="cd6a" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">泛化是指模型在训练集和测试集的观察值上的表现。很难量化一个模型概括得有多好。尽管在实践中很少见，但在理想的情况下，如果测试数据来自相同的数据生成过程，测试度量可以评估泛化性能。在任何情况下，稀疏模型都更简单，并且不容易出现不可量化的风险，即留下弱预测特征。</p><p id="d8ec" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在神经网络中，l1正则化不会在模型级别执行子集选择。然而，在神经元层面，它会选择哪些特征是重要的。这种正则化产生了一个更稀疏的神经网络，该网络在测试度量上更有性能，并且泛化得更好。</p><p id="688d" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">让我们回顾一下！</strong></p><p id="0e46" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们深入了解LASSO的工作原理和原因之前，让我们回顾一下。通过排除倾向于相关的特征，LASSO构建了更稀疏、更易解释且通常更具性能的GLMs。一些其他模型也可以受益于预测准确性的提高。</p><h1 id="53a0" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">套索是如何工作的</h1><p id="cfc1" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">了解了它的应用，我们可以看看套索是如何在数学上工作的。LASSO算法是许多正则化技术中的一种，它通常对问题的损失函数应用惩罚项。我们将使用线性回归来演示这项技术。</p><h2 id="48ca" class="mt lw it bd lx mu mv dn mb mw mx dp mf kr my mz mh kv na nb mj kz nc nd ml ne bi translated">线性回归如何工作</h2><p id="11b0" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">在我们将l1惩罚应用于线性回归之前，让我们快速回顾一下线性回归是如何工作的。</p><p id="97f7" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">回想一下，线性回归根据以下形式进行预测。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/66933596479f632fb04e74de56b47897.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*rOk-Hl7mB5rttcHsPpibbw.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">线性形式(来源:作者)</p></figure><p id="0807" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">为了训练模型，我们需要估计系数。为了估计系数，我们通过选择系数值来最小化以下等式给出的平方和。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ng"><img src="../Images/86e5952186ab1dd42df1aa8a3160161e.png" data-original-src="https://miro.medium.com/v2/resize:fit:788/format:webp/1*m-H2-LMzUo2C6JFGwOBaUw.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">平方和(来源:作者)</p></figure><p id="a7f0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">代入线性形式，我们得到下面的损失函数。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi nh"><img src="../Images/e455faa1c7478ecbf2ca470bcf393330.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yN-5Lg2cizecMKY885PZUw.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">线性回归的损失函数(来源:作者)</p></figure><p id="3e08" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">使用线性代数符号编写表达式会产生一个看起来更简洁但等效的函数。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/4b4ea055ec57ccb4d264d71d1715196e.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*2srf3SCosgHLFzZvK7k6TA.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">线性回归的损失函数(来源:作者)</p></figure><p id="d3e5" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，除了系数之外，这个方程中的所有内容我们都知道。我们可以使用几种最小化技术来找到这个问题中系数的最佳值，包括:</p><ul class=""><li id="0afb" class="nj nk it kk b kl km ko kp kr nl kv nm kz nn ld no np nq nr bi translated">解析求解一阶条件</li><li id="8733" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld no np nq nr bi translated">使用QR分解</li><li id="26fc" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld no np nq nr bi translated">使用诸如梯度下降的数值方法</li></ul><p id="23da" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在我们总结线性回归之前，让我们从几何角度来看这个问题。我们想通过一组点找到最佳平面。直觉上，最好的平面是最接近这些点的平面。在下图中，我们有两个维度用于特征，一个维度用于创建3d空间的目标。线性形式是三维平面，因此线性回归会给我们一个最佳拟合平面。黑线表示每个点和平面之间的距离。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/b92a8ddc1bc82f4c50826a994284c3d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*R-8lyq_SBq_bvMGs-fQ8Dw.jpeg"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">线性回归几何学(来源:作者)</p></figure><p id="4c33" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一个合理的目标是找到使黑线的绝对值之和最小的平面，但相反，线性回归使它们的平方和最小。有几个数学和计算上的原因让我们更喜欢用平方和作为损失函数。紫色平面代表最佳拟合平面。</p><h2 id="020e" class="mt lw it bd lx mu mv dn mb mw mx dp mf kr my mz mh kv na nb mj kz nc nd ml ne bi translated">添加惩罚</h2><p id="f0c3" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">要使用LASSO回归，我们只需在损失函数中添加以下惩罚项。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ny"><img src="../Images/7343fe234956065f4e9cb9e6a1cba63e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ySncVWpxtmmCiGT9jbtz6Q.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">L1刑罚(来源:作者)</p></figure><p id="176c" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">该表达式由系数的L1范数和惩罚系数(也称为正则化强度)的乘积组成。惩罚参数是一个固定的外生超参数。这些项的乘积被称为L1罚项，然后应用于损失函数或线性回归，以生成LASSO的损失函数，如下所示。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/38650c4d535e0e5cea7ef9335a12c7eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1296/format:webp/1*3PDoJ76_8o_WJbTCySjsdQ.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">拉索的损失函数(来源:作者)</p></figure><p id="7b5e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这个最小化问题没有封闭形式的解。因此，我们必须使用数值方法，如梯度下降法，来求解最佳系数。</p><p id="554e" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这就是套索回归的全部内容，但是如果不深入研究最小化问题，很难对它的工作原理有任何直觉。</p><h1 id="dd3d" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">套索为什么有效</h1><h2 id="bd83" class="mt lw it bd lx mu mv dn mb mw mx dp mf kr my mz mh kv na nb mj kz nc nd ml ne bi translated">作为约束优化问题的套索</h2><p id="cb4e" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">为了理解套索为什么有效，让我们从另一个角度去发现损失函数。</p><p id="4026" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">假设我们有以下问题，我们必须最小化最小二乘，但在系数的选择上受到限制。特别是，系数的绝对值之和必须小于罚参数。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/a0fbe62a98859a37e4d42cdb8c68c20a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1288/format:webp/1*F5Yj_3fEmBVmg9DSzZ6cgg.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">拉索作为一个约束优化问题(来源:作者)</p></figure><p id="1985" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">事实证明，这个约束优化问题可以通过将其重写为无约束优化问题来解决，从而得到我们针对LASSO的损失函数。</p><p id="bd74" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果你对将约束问题转化为损失函数背后的数学和直觉感到好奇，请查看安德鲁·张伯伦博士的精彩文章！</p><div class="od oe gp gr of og"><a href="https://medium.com/@andrew.chamberlain/a-simple-explanation-of-why-lagrange-multipliers-works-253e2cdcbf74" rel="noopener follow" target="_blank"><div class="oh ab fo"><div class="oi ab oj cl cj ok"><h2 class="bd iu gy z fp ol fr fs om fu fw is bi translated">拉格朗日乘数法工作原理的简单解释</h2><div class="on l"><h3 class="bd b gy z fp ol fr fs om fu fw dk translated">拉格朗日乘数法是经济学家解决最优化问题的常用方法。该技术是一种有效的方法</h3></div><div class="oo l"><p class="bd b dl z fp ol fr fs om fu fw dk translated">medium.com</p></div></div><div class="op l"><div class="oq l or os ot op ou lp og"/></div></div></a></div><h2 id="adba" class="mt lw it bd lx mu mv dn mb mw mx dp mf kr my mz mh kv na nb mj kz nc nd ml ne bi translated">可视化约束区域</h2><p id="8d68" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">既然这两个公式是等价的，那就让我们从几何上更仔细地看一下约束问题。</p><p id="1240" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">但是，首先，让我们快速绕道。另一种正则化罚函数叫做L2罚函数，它的应用方式与L1罚函数完全相同。在线性回归中加入L2惩罚的模型称为岭回归。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi ov"><img src="../Images/f9d8ee7691fd8a9802df242873b7e9b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*5yJOwUm4jed2wErEb2IMbg.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">岭回归惩罚(来源:作者)</p></figure><p id="74f6" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">相关的约束如下。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/99f47120053921ac6c2b942e9120dd93.png" data-original-src="https://miro.medium.com/v2/resize:fit:512/format:webp/1*iHbj4u-t7d4YkoI4MiXyiQ.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">脊约束区域(来源:作者)</p></figure><p id="a6df" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这可能看起来不熟悉，但让我们看看二维空间，设置惩罚参数为1，平方两边，看看等式边界。我们得到下面的等式。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/17bbf93fa41665228270513142902388.png" data-original-src="https://miro.medium.com/v2/resize:fit:402/format:webp/1*_26-cA4DO3_DSbcD5m4KbA.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">单位圆(来源:作者)</p></figure><p id="046f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不认的话就是单位圆！看看二维的套索约束，我们得到类似的东西。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/95b948c4506a880e4d09f961390bfbaf.png" data-original-src="https://miro.medium.com/v2/resize:fit:482/format:webp/1*7OXbRsN8HZ_TfEVPQQiA9w.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">单位钻石(来源:作者)</p></figure><p id="52a0" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这就是单位钻石。让我们想象一下这些约束区域。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/153923efc4278908df0a2909c38113fa.png" data-original-src="https://miro.medium.com/v2/resize:fit:932/format:webp/1*Lj78gwV2BIaAyB_7t_XQAw.png"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">约束区域(来源:作者)</p></figure><p id="d800" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">事实证明，这种菱形约束负责产生稀疏模型。</p><h2 id="81c7" class="mt lw it bd lx mu mv dn mb mw mx dp mf kr my mz mh kv na nb mj kz nc nd ml ne bi translated">可视化约束优化问题</h2><p id="7bfc" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">让我们通过在下图中可视化整个约束优化问题来理解约束区域的形状如何产生稀疏模型。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/73b8574250f0447c96688e6537ddb272.png" data-original-src="https://miro.medium.com/v2/resize:fit:1322/format:webp/1*kL1Cc5pDTJUjUUokjuCrRA.jpeg"/></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">约束最优化问题(来源:作者)</p></figure><p id="15a9" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们看到不同损失函数的套索和脊约束区域和轮廓集(没有惩罚项)。</p><ul class=""><li id="152d" class="nj nk it kk b kl km ko kp kr nl kv nm kz nn ld no np nq nr bi translated">一组环表示每个轮廓集。类似于地形图，每个连续的向外环表示更高的损失函数值。</li><li id="156f" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld no np nq nr bi translated">在每个轮廓集中心找到的每个点代表无约束问题的解。相比之下，轴相交处的菱形和圆形区域代表约束区域。</li><li id="e379" class="nj nk it kk b kl ns ko nt kr nu kv nv kz nw ld no np nq nr bi translated">橙色菱形对应于套索的约束区域，由所有服从L1罚约束的有效候选解组成。相比之下，红色圆圈对应于脊约束区域，由遵守L2约束的所有有效候选解组成。</li></ul><p id="d5ed" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，左上角的轮廓集(带有绿色的内部轮廓)在形状上更像椭圆形，而不是圆形。当等值线集具有相关的要素时，会产生此形状。轮廓组的椭圆形状和约束区域的菱形形状使得轮廓组的最佳点与约束区域的角点重合的机会很高。此时，一些特征可能会被消除，因为一个轴坐标上的值为0。这就是LASSO生成稀疏解的原因。</p><h1 id="9445" class="lv lw it bd lx ly lz ma mb mc md me mf jz mg ka mh kc mi kd mj kf mk kg ml mm bi translated">选择正则化强度</h1><p id="4216" class="pw-post-body-paragraph ki kj it kk b kl mn ju kn ko mo jx kq kr mp kt ku kv mq kx ky kz mr lb lc ld im bi translated">现在我们理解了为什么LASSO产生稀疏解，我们可以检查正则化强度如何产生更多的稀疏解。这由分配给惩罚参数的值控制，该值决定正则化强度。下图显示了系数收缩的路径，即系数向0收缩时的路径。</p><figure class="lg lh li lj gt lk gh gi paragraph-image"><div role="button" tabindex="0" class="ll lm di ln bf lo"><div class="gh gi pb"><img src="../Images/2a0d551db3e990d1db78bd2e28f3bfac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WvdZpVIQzCXcwUWAgH4GEQ.png"/></div></div><p class="lr ls gj gh gi lt lu bd b be z dk translated">正则化强度对系数的影响(来源:作者)</p></figure><p id="5232" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">此图展示了LASSO与岭回归的不同之处，LASSO在应用更多惩罚时会将一些系数设置为零。对于Ridge，这种现象只在罚值非常大的情况下出现，罚值会将所有系数收缩到零。</p><p id="fb36" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">查看套索模型系数的正则化路径可能会很有趣，尤其是当它们将下一个系数设置为0时。路径给出了一系列越来越稀疏的模型。在选择一个时，如果我们主要关心预测，我们可以应用我们的主题专业知识或使用交叉验证。</p><p id="074f" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="kk iu">结论</strong></p><p id="3a22" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是对LASSO回归方法的介绍，我们已经展示了它可以执行正则化和内生变量选择。请继续关注我们的下一篇文章，它将演示如何用Python实现LASSO！</p><p id="4b61" class="pw-post-body-paragraph ki kj it kk b kl km ju kn ko kp jx kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">要了解更多关于数据科学、机器学习和开发的内容，请查看<a class="ae le" href="https://www.youtube.com/channel/UCmvdvjDaSjjMRIAxE5s7EZA" rel="noopener ugc nofollow" target="_blank"> Edward的YouTube频道</a>，并订阅我下面的邮件列表，成为第一个听到新文章的人！</p><div class="od oe gp gr of og"><a href="https://edkruegerdata.com/subscribe" rel="noopener  ugc nofollow" target="_blank"><div class="oh ab fo"><div class="oi ab oj cl cj ok"><h2 class="bd iu gy z fp ol fr fs om fu fw is bi translated">每当爱德华·克鲁格发表文章时，就收到一封电子邮件。</h2><div class="oo l"><p class="bd b dl z fp ol fr fs om fu fw dk translated">edkruegerdata.com</p></div></div><div class="op l"><div class="pc l or os ot op ou lp og"/></div></div></a></div></div></div>    
</body>
</html>