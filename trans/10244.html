<html>
<head>
<title>Discover the Soul of a Movie Like a Data Scientist</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">像数据科学家一样发现电影的灵魂</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/discover-the-soul-of-a-movie-like-a-data-scientist-3753cd96e231?source=collection_archive---------30-----------------------#2021-09-28">https://towardsdatascience.com/discover-the-soul-of-a-movie-like-a-data-scientist-3753cd96e231?source=collection_archive---------30-----------------------#2021-09-28</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="fbb4" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">从描述中提炼出这部电影固有的缺点</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/bc95b0e117084611f2f7c8819cdd824c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1HwYdXIGEex7NQzzVJgYsw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Ahmet Yal nkaya在<a class="ae ky" href="https://unsplash.com/s/photos/movie?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="26ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个故事中，我们有一个特定的目标:给定一部电影的情节，我们如何发现它的灵魂？怎样才能定位到最有意义的词，让我们想投入时间去看一部电影？哪些概念容易引起我们的共鸣，哪些符号让我们对图片的波长产生共鸣？</p><p id="db23" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我们探索如何使用非负矩阵分解(NMF)来模拟一组电影的类型或主题。此外，我们为每种类型确定最重要的词，并提出描述每部电影的潜在特征。我们可以稍后通过<em class="lv">迁移学习</em>使用这些特性来解决其他问题。</p><blockquote class="lw lx ly"><p id="a9e1" class="kz la lv lb b lc ld ju le lf lg jx lh lz lj lk ll ma ln lo lp mb lr ls lt lu im bi translated"><a class="ae ky" href="https://www.dimpo.me/newsletter?utm_source=medium&amp;utm_medium=article&amp;utm_campaign=nmf" rel="noopener ugc nofollow" target="_blank">学习率</a>是为那些对AI和MLOps的世界感到好奇的人准备的时事通讯。你会在每周五收到我关于最新人工智能新闻和文章的更新和想法。在这里订阅<a class="ae ky" href="https://www.dimpo.me/newsletter?utm_source=medium&amp;utm_medium=article&amp;utm_campaign=nmf" rel="noopener ugc nofollow" target="_blank"/>！</p></blockquote></div><div class="ab cl mc md hx me" role="separator"><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh mi"/><span class="mf bw bk mg mh"/></div><div class="im in io ip iq"><h1 id="04c1" class="mj mk it bd ml mm mn mo mp mq mr ms mt jz mu ka mv kc mw kd mx kf my kg mz na bi translated">什么是NMF？</h1><p id="f42d" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">NMF是一种矩阵分解技术，很像奇异值分解(SVD)，但我们将结果矩阵约束为非负的，而不是正交的。</p><p id="d640" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">给定一个矩阵<code class="fe ng nh ni nj b">X</code>，我们想把它分解成两个矩阵<code class="fe ng nh ni nj b">W</code>和<code class="fe ng nh ni nj b">H</code>，这样<code class="fe ng nh ni nj b">X ≈ W x H</code>。我们使用近似等号<code class="fe ng nh ni nj b">≈</code>,因为与奇异值分解不同，NMF产生原始矩阵的近似值。而且<code class="fe ng nh ni nj b">W</code>和<code class="fe ng nh ni nj b">H</code>的每个元素都是非负的。</p><blockquote class="nk"><p id="9ead" class="nl nm it bd nn no np nq nr ns nt lu dk translated"><em class="nu">如果矩阵</em> <code class="fe ng nh ni nj b"><em class="nu">X</em></code> <em class="nu">保存人脸图像，</em> <code class="fe ng nh ni nj b"><em class="nu">W</em></code> <em class="nu">将捕捉这些人脸的面部特征以及</em> <code class="fe ng nh ni nj b"><em class="nu">H</em></code> <em class="nu">这些特征在每幅图像中的相对重要性。</em></p></blockquote><p id="b2d9" class="pw-post-body-paragraph kz la it lb b lc nv ju le lf nw jx lh li nx lk ll lm ny lo lp lq nz ls lt lu im bi translated">直观地说，我们可以把这两个矩阵想成这样:假设我们有一个矩阵<code class="fe ng nh ni nj b">X</code>，其中每一列都是一张人脸的矢量化图像。<code class="fe ng nh ni nj b">W</code>表示面部特征(如鼻子、眉毛、胡须等。)和<code class="fe ng nh ni nj b">H</code>捕捉每个图像中特征的相对重要性。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/43445e293da751eb7048455f0c33b2c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1366/format:webp/0*i3i677_vOgdg174y.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="ab93" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">既然我们对NMF的成就有了一个展望，我们就准备动手了。但是首先，我们再简单地转到单词规范化和TF-idf。</p><h1 id="a0fc" class="mj mk it bd ml mm ob mo mp mq oc ms mt jz od ka mv kc oe kd mx kf of kg mz na bi translated">密度与重要性</h1><p id="d31a" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated"><em class="lv">词频——逆文档频率</em> (TF-idf) <em class="lv"> </em>是信息检索中常用的一种权重度量。它的作用是衡量一个词对语料库中的文档有多重要。</p><p id="09cd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">TF-idf由两个术语组成。第一个是术语频率，计算一个单词在文档中出现的归一化频率。因此，单词在文档中出现的次数除以该文档中的总单词数。</p><blockquote class="nk"><p id="6fba" class="nl nm it bd nn no np nq nr ns nt lu dk translated">TF衡量一个单词在文档中出现的频率，而idf计算一个单词的重要性。</p></blockquote><p id="f889" class="pw-post-body-paragraph kz la it lb b lc nv ju le lf nw jx lh li nx lk ll lm ny lo lp lq nz ls lt lu im bi translated">第二个术语是反向文档频率(idf)。这是通过语料库中文档数量的对数除以出现特定术语的文档数量来计算的。因此，从第一项中，我们得到了单词的频率，而第二项通过对频繁出现的单词进行加权并对罕见的单词进行放大，为我们提供了每个单词的重要性。</p><h1 id="3fa8" class="mj mk it bd ml mm ob mo mp mq oc ms mt jz od ka mv kc oe kd mx kf of kg mz na bi translated">回到我们的任务</h1><p id="c1d1" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">现在我们对NMF和TF-idf有了一个很好的了解，我们已经准备好着手解决眼前的问题；我们如何从一个电影情节中发掘更多？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/b7f4b82b501501097385b70ee7d32429.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*rTgfuwk3u3yhMP1P.jpeg"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@waldemarbrandt67w?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">瓦尔德马尔·布兰德</a>在<a class="ae ky" href="https://unsplash.com/s/photos/screenplay?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="a25d" class="mj mk it bd ml mm ob mo mp mq oc ms mt jz od ka mv kc oe kd mx kf of kg mz na bi translated">数据集</h1><p id="0f1f" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">对于这个实验，我们使用由<a class="ae ky" href="https://www.kaggle.com/jrobischon" rel="noopener ugc nofollow" target="_blank"> JustinR </a>在<a class="ae ky" href="https://creativecommons.org/licenses/by-sa/4.0/" rel="noopener ugc nofollow" target="_blank">CC BY-SA 4.0</a>creative commons许可下创建的<a class="ae ky" href="https://www.kaggle.com/jrobischon/wikipedia-movie-plots/metadata" rel="noopener ugc nofollow" target="_blank">维基百科电影情节</a>数据集。该数据集包含来自世界各地的34，886部电影的信息，如<em class="lv">标题、类型、上映年份、导演、情节、</em>等。不过我们将要使用的尺寸只是标题和T21情节。</p><p id="e1ee" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">此外，我们还使用了Philippe Rémy的英语常用名数据集，原因我们将在下面进一步解释。</p><h1 id="e2c5" class="mj mk it bd ml mm ob mo mp mq oc ms mt jz od ka mv kc oe kd mx kf of kg mz na bi translated">用NMF进行主题建模</h1><p id="b252" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">现在让我们深入研究代码，并从我们的工作中获益。首先，我们需要加载数据。我们将维基百科电影情节数据加载到内存中，对50%的电影进行采样以避免内存问题，并只保留我们关心的列(即电影<em class="lv">标题</em>和<em class="lv">情节</em>)。我们还加载英语名字数据集。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="f51c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们准备电影情节，将它们存储在一个列表中，使用TF-idf对它们进行规范化和矢量化。目的是建立一个矩阵<code class="fe ng nh ni nj b">X</code>，其中行类似于电影，列是在所有情节组合中出现的唯一单词。因此，矩阵<code class="fe ng nh ni nj b">X</code>中的单元格<code class="fe ng nh ni nj b">ij</code>捕获了<code class="fe ng nh ni nj b">j</code>这个词在电影<code class="fe ng nh ni nj b">i</code>的情节中出现了多少次。例如，在下面的矩阵中，我们可以看到单词<code class="fe ng nh ni nj b">2</code>在电影<code class="fe ng nh ni nj b">1</code>的情节中出现了<code class="fe ng nh ni nj b">7</code>次。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/5282ea2e044330660302b5ebb6b5d6b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:764/format:webp/0*hpM06bVeA4R63cWf.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="fcad" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们看看python代码。我们使用方便的scikit-learn的<code class="fe ng nh ni nj b">TfidfVectorizer</code>类来实现我们的目的。这个类将一个叫做<code class="fe ng nh ni nj b">stop_words</code>的东西作为参数。这些都是常用词，如“<em class="lv">一”、“是”、“该”、“T22”等。，没有任何实际意义，也没有任何价值。</em></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="0830" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">除了这些单词之外，我们还会传递英文名字，因此我们也可以将它们排除在流程之外。在矢量器完成对单词的处理后，我们可以得到最终的词汇表，供以后使用。</p><p id="0148" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在我们有了矩阵<code class="fe ng nh ni nj b">X</code>，我们想像之前看到的那样，把它分解成两个矩阵<code class="fe ng nh ni nj b">W</code>和<code class="fe ng nh ni nj b">H</code>。为此，我们可以使用scikit-learn提供的<code class="fe ng nh ni nj b">NMF</code>因子分解器。分解需要一个名为<code class="fe ng nh ni nj b">n_components</code>的参数。这些是主题的数量，或者在我们的例子中是电影主题，我们希望返回。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="09ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们现在准备好研究每一个流派的灵魂。我们想找到每个主题最重要的词是什么。这个过程从情节中提取主题，并对电影主题进行建模。除了两个辅助函数之外，我们已经具备了实现这一目标所需的一切。让我们在下面定义它们。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oh oi l"/></div></figure><p id="a50e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">结果是每个<code class="fe ng nh ni nj b">18</code>主题的前<code class="fe ng nh ni nj b">10</code>个单词，因为我们将<code class="fe ng nh ni nj b">n_components</code>设置为<code class="fe ng nh ni nj b">18</code>。那么，它们是什么？</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/075cc3c74438d2b5f3aa62a7cc857546.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*a6cxGro24jFPZ7lV.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">18个主题中每个主题的前10个单词—图片由作者提供</p></figure><p id="e705" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有些主题乍一看没有意义，但它们中的大多数都是特定流派的代表。例如，倒数第二个是关于动画和卡通的，第二个是关于犯罪的，我们在倒数第三排有一个西部主题，在中间的某个地方有一个医学主题。对于10到15行代码来说，这是一个令人印象深刻的结果！</p><h1 id="ad88" class="mj mk it bd ml mm ob mo mp mq oc ms mt jz od ka mv kc oe kd mx kf of kg mz na bi translated">结论</h1><p id="dd10" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">在这个故事中，我们探讨了非负矩阵分解(NMF)以及如何使用它进行主题建模。NMF在<a class="ae ky" href="https://scikit-learn.org/stable/auto_examples/decomposition/plot_faces_decomposition.html#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py" rel="noopener ugc nofollow" target="_blank">人脸分解</a>、<a class="ae ky" href="http://www.quuxlabs.com/blog/2010/09/matrix-factorization-a-simple-tutorial-and-implementation-in-python/" rel="noopener ugc nofollow" target="_blank">协同过滤</a>、<a class="ae ky" href="https://ieeexplore.ieee.org/document/1532909" rel="noopener ugc nofollow" target="_blank">化学</a>、<a class="ae ky" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2623306/" rel="noopener ugc nofollow" target="_blank">基因表达</a>等等方面都有各种应用。您现在已经有了进一步检查它并在项目中使用它所需的东西。</p><p id="ddf5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个故事的灵感来自于雷切尔·托马斯关于计算线性算法的课程。</p><h1 id="f70f" class="mj mk it bd ml mm ob mo mp mq oc ms mt jz od ka mv kc oe kd mx kf of kg mz na bi translated">关于作者</h1><p id="31a4" class="pw-post-body-paragraph kz la it lb b lc nb ju le lf nc jx lh li nd lk ll lm ne lo lp lq nf ls lt lu im bi translated">我的名字是<a class="ae ky" href="https://www.dimpo.me/?utm_source=medium&amp;utm_medium=article&amp;utm_campaign=nmf" rel="noopener ugc nofollow" target="_blank"> Dimitris Poulopoulos </a>，我是一名为<a class="ae ky" href="https://www.arrikto.com/" rel="noopener ugc nofollow" target="_blank"> Arrikto </a>工作的机器学习工程师。我曾为欧洲委员会、欧盟统计局、国际货币基金组织、欧洲央行、经合组织和宜家等主要客户设计和实施过人工智能和软件解决方案。</p><p id="22ed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你有兴趣阅读更多关于机器学习、深度学习、数据科学和数据操作的帖子，请关注我的<a class="ae ky" href="https://towardsdatascience.com/medium.com/@dpoulopoulos/follow" rel="noopener" target="_blank">媒体</a>、<a class="ae ky" href="https://www.linkedin.com/in/dpoulopoulos/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>或Twitter上的<a class="ae ky" href="https://twitter.com/james2pl" rel="noopener ugc nofollow" target="_blank"> @james2pl </a>。</p><p id="378e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所表达的观点仅代表我个人，并不代表我的雇主的观点或意见。</p></div></div>    
</body>
</html>