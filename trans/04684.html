<html>
<head>
<title>17 Clustering Algorithms Used In Data Science and Mining</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据科学和挖掘中使用的17种聚类算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/17-clustering-algorithms-used-in-data-science-mining-49dbfa5bf69a?source=collection_archive---------1-----------------------#2021-04-23">https://towardsdatascience.com/17-clustering-algorithms-used-in-data-science-mining-49dbfa5bf69a?source=collection_archive---------1-----------------------#2021-04-23</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="c49e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">聚类算法的概述、它们的用例以及它们的优缺点</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/33decab686988e21fc12bd08c36bacc6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*S_Pkf-cK1gUE0OPbDM81ww.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">各种聚类算法。</p></figure><blockquote class="kv"><p id="0221" class="kw kx iq bd ky kz la lb lc ld le lf dk translated">“要走得快，就一个人走；想走远，就一起走。”——非洲谚语。</p></blockquote><blockquote class="lg lh li"><p id="847e" class="lj lk ll lm b ln lo jr lp lq lr ju ls lt lu lv lw lx ly lz ma mb mc md me lf ij bi translated"><strong class="lm ir">快速提示:</strong>如果您通过基于chromium的浏览器(例如，Google Chrome、Chromium、Brave)阅读本文，下面的目录就可以了。然而，像Firefox这样的其他浏览器就不一样了，在Firefox中，你需要点击每个链接两次才能到达指定的部分。尽情享受吧！</p><p id="5e17" class="lj lk ll lm b ln mf jr lp lq mg ju ls lt mh lv lw lx mi lz ma mb mj md me lf ij bi translated">一如既往，除非特别说明，所有文字和图像都是作者创作的。</p></blockquote><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="04f2" class="mp mq iq ml b gy mr ms l mt mu"><strong class="ml ir">Table of Contents(TOC)</strong><br/><strong class="ml ir">🄰.</strong><a class="ae mv" href="#36be" rel="noopener ugc nofollow"><strong class="ml ir">Introduction</strong></a><br/>   ► <a class="ae mv" href="#94e3" rel="noopener ugc nofollow">Machine learning</a><br/>   ► <a class="ae mv" href="#c789" rel="noopener ugc nofollow">Cluster analysis</a><br/>   ► <a class="ae mv" href="#4cd4" rel="noopener ugc nofollow">Types of Clustering</a><br/><strong class="ml ir">🄱.</strong><a class="ae mv" href="#a176" rel="noopener ugc nofollow"><strong class="ml ir">Clustering Algorithms</strong></a><br/>   ► <a class="ae mv" href="#7b7b" rel="noopener ugc nofollow"><strong class="ml ir">Centroid-based clustering</strong></a><br/>    <strong class="ml ir"> ➤</strong> <a class="ae mv" href="#9136" rel="noopener ugc nofollow">k-means</a><br/>     ➤ <a class="ae mv" href="#51f6" rel="noopener ugc nofollow">k-means++</a><br/>     ➤ <a class="ae mv" href="#9062" rel="noopener ugc nofollow">k-means||</a><br/>     ➤ <a class="ae mv" href="#7e1d" rel="noopener ugc nofollow">Fuzzy C-means</a><br/>     ➤ <a class="ae mv" href="#a536" rel="noopener ugc nofollow">k-medoids, PAM</a><br/>     ➤ <a class="ae mv" href="#0133" rel="noopener ugc nofollow">k-Medians</a><br/>     ➤ <a class="ae mv" href="#eb7d" rel="noopener ugc nofollow">k-Modes</a><br/>     ➤ <a class="ae mv" href="#87c6" rel="noopener ugc nofollow">k-prototypes</a><br/>     ➤ <a class="ae mv" href="#5190" rel="noopener ugc nofollow">CLARA</a><br/>     ➤ <a class="ae mv" href="#a329" rel="noopener ugc nofollow">CLARANS</a><br/>   ► <a class="ae mv" href="#5c4a" rel="noopener ugc nofollow"><strong class="ml ir">Distribution-based clustering</strong></a><br/>    <strong class="ml ir"> ➤</strong> <a class="ae mv" href="#8925" rel="noopener ugc nofollow">GMM</a><br/>     <strong class="ml ir">➤</strong> <a class="ae mv" href="#ffbb" rel="noopener ugc nofollow">EM</a><br/>     ➤ <a class="ae mv" href="#14a7" rel="noopener ugc nofollow">DMM</a><br/>   ►<strong class="ml ir"> </strong><a class="ae mv" href="#6178" rel="noopener ugc nofollow"><strong class="ml ir">Density-based clustering</strong></a><br/>    <strong class="ml ir"> ➤</strong> <a class="ae mv" href="#5588" rel="noopener ugc nofollow">DBSCAN</a><br/>     ➤ <a class="ae mv" href="#b750" rel="noopener ugc nofollow">ADBSCAN</a><br/>     ➤ <a class="ae mv" href="#eeb3" rel="noopener ugc nofollow">DENCLUE</a><br/>     ➤ <a class="ae mv" href="#d22d" rel="noopener ugc nofollow">OPTICS</a><br/><strong class="ml ir">🄲.</strong><a class="ae mv" href="#9b71" rel="noopener ugc nofollow"><strong class="ml ir">Conclusion</strong></a><strong class="ml ir"><br/>🄳.</strong><a class="ae mv" href="#61ab" rel="noopener ugc nofollow"><strong class="ml ir">Useful Resources</strong></a></span></pre><h2 id="36be" class="mp mq iq bd mw mx my dn mz na nb dp nc nd ne nf ng nh ni nj nk nl nm nn no np bi translated">🄰.介绍</h2><p id="565a" class="pw-post-body-paragraph lj lk iq lm b ln nq jr lp lq nr ju ls nd ns lv lw nh nt lz ma nl nu md me lf ij bi nv translated"><span class="l nw nx ny bm nz oa ob oc od di">随着信息变得越来越重要，全球各地的人们都可以获取信息，越来越多的数据科学和机器学习方法被开发出来。聚类分析模型乍看起来可能很简单，但理解如何处理大量数据是至关重要的。然而，在大量的聚类算法中做出合理的选择有时会令人望而生畏，并且需要对各种算法有相当多的了解。因此，本文汇编了17种聚类算法，以向读者提供关于其中大多数算法的大量信息。</span></p><p id="94e3" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨机器学习</strong></p><p id="fdb4" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">机器学习是人工智能的一个子领域，其最简单而直接的定义是如何通过发现统计模式从数据(例如，从传感器、实验收集的数据……)中教会机器做出决策并自行完成任务(自动化数据驱动模型)。就这么简单。然而，困难来自于缩小的细节和应用。一切都是为了分析数据并从中学习。此外，机器学习在其核心部分为数据科学提供了基础，正如Drew Conway ven图所示。</p><p id="afa2" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">从历史上来说，机器学习源于人工智能中的<a class="ae mv" href="https://plato.stanford.edu/entries/connectionism/" rel="noopener ugc nofollow" target="_blank">连接主义者</a>，其中一组个体想要复制具有相似特征的人脑机制。此外，它主要受益于心理学和其他领域(如统计学)的思想。).此外，统计学和机器学习是根本不同的领域，前者旨在为人类提供正确的工具来分析和理解数据。后者专注于自动化人类对数据分析的干预(AI <a class="ae mv" href="https://en.wikipedia.org/wiki/Technological_singularity" rel="noopener ugc nofollow" target="_blank">奇点</a>)。</p><p id="c789" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨聚类分析</strong></p><p id="a606" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">聚类分析、聚类或数据分割可以定义为一种无监督(无标签数据)的机器学习技术，旨在发现模式(例如，许多子组、每个组的大小、共同特征、数据内聚性……)，同时收集数据样本，并使用预定义的距离度量(如欧几里德距离等)将它们分组到相似的记录中。共享相似特征的数据对象或观察结果被分组到一个由保存这些数据样本的距离(例如，椭圆的长轴)描述的聚类中。).</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/6d4b85d1c9224702e271dd5ab7beb4d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:450/format:webp/1*RK8PZSbnhKKZ5p5QacgBVw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">椭圆的轴。</p></figure><p id="fc88" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">聚类分析被各种应用广泛采用，例如图像处理、神经科学、经济学、网络通信、医学、推荐系统、客户细分等。此外，在处理新数据集以提取见解和了解数据分布时，可以将聚类视为第一步。聚类分析也可以用于执行维数减少(例如，PCA)。它还可以作为其他算法(如分类、预测和其他数据挖掘应用程序)的预处理或中间步骤。</p><p id="4cd4" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨聚类的类型</strong></p><p id="c35f" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">有许多方法可以将聚类方法分成不同的类别。例如，基于重叠区域，存在两种类型的聚类:</p><p id="be16" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>硬聚类:聚类不重叠:k-means，k-means++。一个数据点只属于一个集群。它要么属于某个集群，要么不属于。</p><p id="27c3" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>软聚类:聚类可以重叠:模糊c均值，EM。一个数据对象可以以一定的概率或隶属度存在于多个集群中。</p><p id="d37f" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">此外，聚类算法可以根据它们试图达到的目的进行分类。因此，存在两种基于该标准的聚类技术:</p><p id="d214" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>单一:在聚类成员之间存在一些共同的属性(例如，25%的患者表现出疫苗a的副作用):数据根据单个特征生成的值进行划分。</p><p id="9aed" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>多面体:聚类成员之间存在某种程度的相似性，但没有共同的属性(例如，相异度):数据根据所有特征生成的值进行划分。</p><p id="f1c5" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">基于所使用的聚类分析技术，每个聚类表示一个质心、代表数据样本中心的单个观察值和一个边界界限。下图显示了一些常见的聚类分析算法类别。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi of"><img src="../Images/572daacc5a54972b720740c6e7395f8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JJW1805_hzUk_n4XNB-HvA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae mv" href="https://link.springer.com/article/10.1007/s40745-015-0040-1" rel="noopener ugc nofollow" target="_blank">聚类算法综述</a>。</p></figure><h2 id="a176" class="mp mq iq bd mw mx my dn mz na nb dp nc nd ne nf ng nh ni nj nk nl nm nn no np bi translated"><strong class="ak"> 🄱.聚类算法。</strong></h2><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="1d8a" class="mp mq iq ml b gy mr ms l mt mu">► <a class="ae mv" href="#04f2" rel="noopener ugc nofollow"><strong class="ml ir">Go To TOC</strong></a><strong class="ml ir"> </strong>◀</span></pre><h2 id="7b7b" class="mp mq iq bd mw mx my dn mz na nb dp nc nd ne nf ng nh ni nj nk nl nm nn no np bi translated"><strong class="ak"> ⓪.基于质心的聚类。</strong></h2><p id="23b6" class="pw-post-body-paragraph lj lk iq lm b ln nq jr lp lq nr ju ls nd ns lv lw nh nt lz ma nl nu md me lf ij bi translated">该方法的主要步骤之一是初始化聚类k的数量，这是一个在模型训练阶段保持不变的<a class="ae mv" href="https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)" rel="noopener ugc nofollow" target="_blank">超参数</a>。</p><h2 id="9136" class="mp mq iq bd mw mx my dn mz na nb dp nc nd ne nf ng nh ni nj nk nl nm nn no np bi translated"><strong class="ak"> ⓐ.k-means </strong>或<strong class="ak">劳埃德算法</strong></h2><p id="7759" class="pw-post-body-paragraph lj lk iq lm b ln nq jr lp lq nr ju ls nd ns lv lw nh nt lz ma nl nu md me lf ij bi translated">最流行的划分算法之一(在<a class="ae mv" href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=k-means&amp;btnG=" rel="noopener ugc nofollow" target="_blank">谷歌学者</a>上有超过100万的引用)用于对数字数据属性进行聚类。使用这种多面体硬聚类技术，n个数据对象被分成k个分区(k &lt; &lt; n)，其中每个分区代表一个聚类。每个群集必须至少包含一个数据点。此外，每个数据对象必须只属于一个组。此外，同一聚类的观察结果应该相似或相近。相反，不同组的对象必须相距很远或彼此不同。换句话说，k-means算法的目标是最大化每对聚类中心之间的距离，最小化每个聚类内观测值之间的距离(例如，最小化一个聚类内的平方误差之和，SSE)。).</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi og"><img src="../Images/37653b1a376f3ff93cdfbcefd1b1c9c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-XHGTWyUruEhLuiz-uBp9g.png"/></div></div></figure><p id="95ae" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">如果满足以下条件，k- means聚类可以很好地工作:</p><p id="7480" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>各属性的分布方差呈球形。</p><p id="c5cd" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>集群是线性可分的。</p><p id="110a" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒉ </strong>星团具有相似数量的观测值(更接近大小。).</p><p id="b9c2" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒊ </strong>变量呈现相同的方差。</p><p id="5f69" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">然而，如果其中一个假设被打破，并不一定意味着k- means将无法聚类观察结果，因为算法的唯一目的是最小化平方误差之和(SSE)。这里有一个很好的<a class="ae mv" href="https://stats.stackexchange.com/questions/133656/how-to-understand-the-drawbacks-of-k-means" rel="noopener ugc nofollow" target="_blank">讨论</a>说明，如果前面的假设之一不满足，k-means会很好地工作。</p><p id="eaa3" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">为了更好地理解数据(例如，提取信息和查找聚类)，经验法则是在二维空间中绘制数据。例如，要找到虹膜数据集中有多少聚类，一个基本的相关矩阵就可以说明很多。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/1ad171df018676e759de51a1b0dff92d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ef5eJt9Ss1WIQSN-RmAtVA.png"/></div></div></figure><p id="e5f6" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">如图所示，该数据集中有三个主要的集群。因此，为了进一步的培训，k应该等于3。然而，这不是选择k值的最佳方法。</p><p id="0b35" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">在实践中，标准方法是从肘方法开始，该算法针对不同的k值(例如，k= 1、2、3、4……)运行，并使用一种称为WCSS(类内平方和)的稳健方法，该方法计算每个类成员与其质心之间的距离之和，以使其最小化，从而达到k的最佳值</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/8f1b5101d247edc5e49c4662bd478128.png" data-original-src="https://miro.medium.com/v2/resize:fit:1258/format:webp/1*4j0CXLy1xdYHSuXgU2sGOw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">k的最佳值是3。</p></figure><p id="ff94" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">还有另一种通过计算每个聚类的轮廓系数来选择正确的k值的方法:相同聚类的点之间的平均距离。它根据数据对象的分类给出了数据对象相似程度的指标。为了说明这一点，在iris数据集上执行了侧影绘图，其中每个聚类都有一个侧影系数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oj"><img src="../Images/397fe2c74c8e4e66bac0b9f8ea570e95.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fN_2GQi-gk2DG84qkGqAXw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">k = [2，3，…，7]的轮廓分数</p></figure><p id="3009" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">使用这种方法，系数越接近1，k值就越适合模型。因此，k的最佳值是2和3，因为它们为每个聚类提供了比其他值更高的轮廓系数。</p><p id="8025" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">k也可以使用shoulder方法初始化，该方法显示平方和百分比(BSS/TSS)与聚类数的关系图。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/2be54ce6dd0761be8517da9a64f547a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1242/format:webp/1*47KVPbXBAZnAAcEMrL48Bw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">k的最佳值是3。</p></figure><p id="3141" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">如图所示，最佳聚类数是一个肩(跃)开始形成的地方。因此，k等于3。</p><p id="932a" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">此外，还有很多其他方法可以用来估计k的最佳值，例如<a class="ae mv" href="https://www.investopedia.com/terms/r/r-squared.asp" rel="noopener ugc nofollow" target="_blank"> R平方</a>度量。然而，剪影评分<a class="ae mv" href="https://www.sciencedirect.com/science/article/abs/pii/S003132031200338X" rel="noopener ugc nofollow" target="_blank">已经被证明</a>是找到k的最好方法</p><p id="7a24" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨的解释。</strong></p><p id="740c" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">这一切都是从在特征空间中随机放置k个点开始的，其中每个点代表一个唯一聚类的质心。使用某个相异度度量，迭代计算数据集的每个观察值与每个聚类中心之间的距离。此外，将每个观察值分配给最近质心的聚类。之后，对于每个聚类，计算每个聚类的点的平均值(数字属性),并将质心重新分配给结果平均值。该过程将一直重复，直到满足预定的收敛条件(例如，达到最大迭代次数，意味着差异变得不变，BSS变得低于给定的最小值，SSE的最小值，最小化目标函数，失真…)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ol"><img src="../Images/b8b06e2c1f1ddac8943bd995a000a250.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SYE0yTyZ_bbWMvn9hMTdHA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">k均值的目标函数。</p></figure><p id="4aef" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨算法。</strong></p><p id="b456" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">🄀从数据集中挑选k个随机质心。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi om"><img src="../Images/14c468d1393f6313df0cbc780f91857c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vvRAy2bR-ZYD6RDtniKy3Q.png"/></div></div></figure><p id="00ea" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>使用适当的相异度度量(例如欧几里德距离)计算每个数据点w.r.t聚类的质心之间的距离。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi on"><img src="../Images/441244397d6ea9b6e96405d03bd8602a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hG3Qk5eu2RH4bPZDtUkdhg.png"/></div></div></figure><p id="3118" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒉ </strong>根据计算的距离将每个数据点分配到最近的聚类。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oo"><img src="../Images/a574b9d2211d32c474dd8c3869ad18be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OCsaY7ql7wJUGO6eFI1H7w.png"/></div></div></figure><p id="dca1" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒊ </strong>通过计算数据点的平均值来重新定位质心。因此k-means只对数值数据有效！</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi op"><img src="../Images/50c00b1183f0124d4935ba7adf5a0aa3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EkK817VFdxydAM8lWaVnCg.png"/></div></div></figure><p id="18c0" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒋ </strong>重复⒈，直到聚类变得稳定或者目标函数j达到其最小值。</p><p id="7404" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨的优势。</strong></p><p id="89a1" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">🄀的学习曲线相对陡峭。</p><p id="1b04" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>被各种包广泛实现(r中的<a class="ae mv" href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/00Index.html" rel="noopener ugc nofollow" target="_blank"> Stats </a> package，python中的<a class="ae mv" href="https://scikit-learn.org/" rel="noopener ugc nofollow" target="_blank">scikit-learn</a>…)</p><p id="28ce" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒉ </strong>聚类小数据集的快速收敛。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/601586f1e75e265c0390e632146be3f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1234/format:webp/1*-FI7rX-Lxn9B07rIVZT20g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">20次迭代来聚类虹膜数据集。</p></figure><p id="2a68" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒊ </strong>容易实现。</p><p id="14e0" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨无往不利。</strong></p><p id="0b10" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>对于大型数据集，计算开销很大(k变大。).</p><p id="b893" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">⒈:有时，很难为聚类数(k)选择一个初始值。</p><p id="e46c" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">⒉不能保证收敛到全局最小值。它对质心的初始化很敏感。不同的设置可能导致不同的结果。</p><p id="e725" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒊ </strong>对异常值的强烈敏感性。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi or"><img src="../Images/46856dad3863820ce88915a37aaa4707.png" data-original-src="https://miro.medium.com/v2/resize:fit:968/format:webp/1*ofV9SaXqf-eyekcrZtz8MQ.png"/></div></figure><p id="ed75" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒋ </strong>只对数值数据有效。</p><p id="bea6" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒌ </strong>未能为一组具有非凸形状的点提供良好的聚类质量。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi os"><img src="../Images/319fc4603eeddd27a7c27a07d490779f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1210/format:webp/1*pqrvJgGG6c-0aPMYStr6mw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">k-means无法分离月亮形状的数据点。</p></figure><p id="7500" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">然而，一些缺点可以通过使用肘方法来初始化聚类数，使用k-means++来克服参数初始化中的敏感性，以及使用类似于<a class="ae mv" href="https://en.wikipedia.org/wiki/Genetic_algorithm" rel="noopener ugc nofollow" target="_blank">遗传算法</a>的技术来寻找全局最优解来解决。</p><p id="64a6" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">⇨应用程序。</p><p id="757a" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">k-means聚类被各种现实世界的企业所采用，例如搜索引擎(例如，文档聚类、相似文章聚类)、客户细分、垃圾邮件/ham检测系统、学术表现、故障诊断系统、无线通信等等。</p><p id="0707" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨目标函数最小化。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ot"><img src="../Images/b6a36b1e535c810f6f1d8f0da16cc8cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wz1z2IittQyPAgjzrhEl9A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">k均值的代价函数。</p></figure><p id="a240" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">为了找到k个集群的最优解，成本函数<strong class="lm ir"> J </strong> w.r.t <strong class="lm ir"> μ </strong>的导数必须等于零<strong class="lm ir">。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/37170d7ce1694e101a07d9e5cd743b8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*5PPHrgKn2k9RudaczERiTg.png"/></div></figure><p id="92f0" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">对于每个集群J，前面的等式将导致:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/797fa304efa04afef95c53a7a5751372.png" data-original-src="https://miro.medium.com/v2/resize:fit:1204/format:webp/1*EUWE7NgM7nDLDzNSGoOwjg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">每个聚类质心的梯度w.r.t欧几里德距离。</p></figure><p id="4854" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">每次迭代后，每个聚类的质心被更新为该聚类内所有数据点的经验平均值。</p><p id="9162" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">注意，最小化每个聚类内的欧几里德距离的问题被称为<a class="ae mv" href="https://en.wikipedia.org/wiki/Weber_problem" rel="noopener ugc nofollow" target="_blank">韦伯问题</a>。而且，从几何学上讲，<em class="ll">均值</em>并不是最优解。因此，需要复杂的几何中心，如中位数，medoid，以尽量减少欧几里德距离。</p><h2 id="51f6" class="mp mq iq bd mw mx my dn mz na nb dp nc nd ne nf ng nh ni nj nk nl nm nn no np bi translated"><strong class="ak"> ⓑ k-means++ </strong></h2><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="f631" class="mp mq iq ml b gy mr ms l mt mu">► <a class="ae mv" href="#04f2" rel="noopener ugc nofollow"><strong class="ml ir">Go To TOC</strong></a><strong class="ml ir"> </strong>◀</span></pre><p id="f11e" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">k-means++背后的思想是，它试图在每次迭代分配一个新中心的同时分散中心。因此，该算法从随机(统一)从数据集中选取一个初始中心开始，这意味着所有点被选中的概率相等。然后计算从每个数据点到先前选择的中心的距离平方。之后，它通过简单地将距离除以总距离来计算每个数据点的概率。此外，将新的聚类中心分配给具有最高概率或最高距离的点。换句话说，数据对象成为新聚类中心的可能性与距离的平方成正比。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/5df809abd00f55a411ecc41f441f49ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1134/1*o6TPTo1duXtGfKwb8UBj-Q.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">k_means++采样k个质心。</p></figure><p id="950b" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">一旦分配了中心，k-means算法将与这些聚类的中心一起运行，并且它将收敛得更快，因为质心已经被仔细选择并且彼此远离。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/48e45a5733ba121e22a1eee359c8e556.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/1*8C-Zg0WIJ6AoshCQv_J6dg.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">采样质心上的k_means。大约12次迭代</p></figure><p id="2358" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨算法</strong></p><p id="f563" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">初始化步骤</strong></p><p id="3d4a" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">以均匀的方式独立地对每个质心进行采样，其概率与每个数据点到每个质心的距离的平方成比例。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oy"><img src="../Images/48c6e358ed4daa5f36feffefb476481c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UkaCAVVVLq7dLHNZBFEj4A.png"/></div></div></figure><p id="4614" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">聚类步骤</strong></p><p id="40d2" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">一旦K个质心被均匀采样，K-means算法将使用这些质心运行。</p><p id="43f8" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨的优势</strong></p><p id="0c3b" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>与K-means具有相同的优势。</p><p id="26fe" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>比K-means收敛更快，迭代次数更少。</p><p id="6b6c" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨无往不利。</strong></p><p id="3b3c" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">与K-means有相同的缺点。</p><p id="b321" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">初始化步骤(为K选择一个初始值)可以被认为是kmeans++的主要缺点之一，就像K-means算法的其他风格一样。但是，它比单独运行K-means更容易收敛，速度更快。此外，该算法仍然对异常值敏感，这些异常值可以使用<a class="ae mv" href="https://en.wikipedia.org/wiki/Local_outlier_factor" rel="noopener ugc nofollow" target="_blank"> LOF </a>、<a class="ae mv" href="https://en.wikipedia.org/wiki/Random_sample_consensus#:~:text=Random%20sample%20consensus%20(RANSAC)%20is,the%20values%20of%20the%20estimates." rel="noopener ugc nofollow" target="_blank">兰萨克</a>和其他方法来解决。</p><h2 id="9062" class="mp mq iq bd mw mx my dn mz na nb dp nc nd ne nf ng nh ni nj nk nl nm nn no np bi translated"><strong class="ak"> ⓒ K-means||，可伸缩</strong> K <strong class="ak"> -means++ </strong></h2><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="d62b" class="mp mq iq ml b gy mr ms l mt mu">► <a class="ae mv" href="#04f2" rel="noopener ugc nofollow"><strong class="ml ir">Go To TOC</strong></a><strong class="ml ir"> </strong>◀</span></pre><p id="77ae" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">K-means并行是另一种充分的技术，它在每次迭代后更新样本分布的频率较低。它在k-means算法中引入了一个过采样因子(L ~ k阶，例如k，k/2，…)。利用这个因素，对于更大的数据集，它将使算法收敛得更快。</p><p id="9ed5" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨ </strong> <strong class="lm ir">算法。</strong></p><p id="d27e" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">初始化步骤</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/379b79c9ede5a1110d1f991be9106e5f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/1*UQi9YcC1-35c2p4J4Stc6Q.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">k-means||初始化步骤。</p></figure><p id="de7b" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>初始化过采样因子l的值</p><p id="9164" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>对于一定数量的迭代(0 ≤ nb_iter ≤ k)，以与每个数据点到每个质心的距离的平方成比例的概率(比kmeans++算法中的概率大l倍)均匀随机采样l个质心。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oz"><img src="../Images/83a92dfe5caeea91aab0e73b93e4e6e6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R_iAeoBWHwoAQl2RSuKIzA.png"/></div></div></figure><blockquote class="lg lh li"><p id="64bc" class="lj lk ll lm b ln mf jr lp lq mg ju ls lt mh lv lw lx mi lz ma mb mj md me lf ij bi translated">nb_iter = 0 <strong class="lm ir"> ⇨ </strong> k均值聚类。</p><p id="b39a" class="lj lk ll lm b ln mf jr lp lq mg ju ls lt mh lv lw lx mi lz ma mb mj md me lf ij bi translated">nb_iter = k，而L = 1 <strong class="lm ir"> ⇨ </strong> k-means++聚类。</p></blockquote><p id="25ef" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">聚类步骤</strong></p><p id="805f" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">一旦K个质心被均匀采样，K-means算法将使用这些质心运行。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/9bbf91863303fe8da2e23207063c6577.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/1*IAnw42bS9u5IA7APW8JUdw.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">k _ means次迭代收敛！</p></figure><p id="f034" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨ </strong> <strong class="lm ir">优点。</strong></p><p id="5130" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">🄀很适合大型数据集。运行时间~日志(k)</p><p id="1bd8" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>比kmeans++快，因为它每次迭代采样l个质心。</p><p id="9799" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">⇨</strong>t30】无往不利。</p><p id="e6a8" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>它会导致基于l值的过采样或欠采样</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/4a6187bcb5fa0ca4dfb081bcf3b2047f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/1*BmMYeKdTsARzxT1Xjrlr1g.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">使用k均值进行过采样(L = 20) ~ 13次迭代！</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/e130f51afab697601230df4230c437ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/1*NCcCPcKG5FeiERLkc-ZpFA.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">欠采样(L = 0) ~ 14次k均值迭代！</p></figure><h2 id="7e1d" class="mp mq iq bd mw mx my dn mz na nb dp nc nd ne nf ng nh ni nj nk nl nm nn no np bi translated"><strong class="ak"> ⓓ.模糊C均值:FCM </strong></h2><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="7980" class="mp mq iq ml b gy mr ms l mt mu">► <a class="ae mv" href="#04f2" rel="noopener ugc nofollow"><strong class="ml ir">Go To TOC</strong></a><strong class="ml ir"> </strong>◀</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/592218506098eb848870f7868257b5b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/1*Q5ZTC6jfFh2MMAYJiRCkXg.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">FCM。</p></figure><p id="e337" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">术语<a class="ae mv" href="https://en.wikipedia.org/wiki/Fuzzy_logic" rel="noopener ugc nofollow" target="_blank">模糊</a>用于强调这样一个事实，即一个数据点可以存在于一个或多个聚类中，允许形成各种不同的聚类阴影(例如，不相交、非分离……)。例如，橙色是红色和黄色的混合，这意味着它在某种程度上属于每个颜色组。</p><p id="f195" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">隶属度函数用于测量数据点属于每个聚类的程度。它描述了一个数据点属于某个聚类的概率。</p><p id="3d73" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">该算法旨在最小化以下成本函数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pa"><img src="../Images/b6109c69e72755a9a14186f47768859d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QpBWLW5P7D_xY7Mbj89GiQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">FCM的目标函数。</p></figure><p id="4821" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨算法</strong></p><p id="99f8" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>基于预定义的权重aij^p和p的初始值选择k个初始模糊伪质心</p><p id="ef79" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>使用模糊划分更新聚类中心。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pb"><img src="../Images/7352e9286a4624a099f02dbac92c31c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OPM7ht03f-o3vjItQmXZLg.png"/></div></div></figure><p id="0618" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒉ </strong>使用以下公式更新权重。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pc"><img src="../Images/4e0e476bc6fae3a3257f847d485f55be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8wDJduQWgSQx2CNRzShrPQ.png"/></div></div></figure><p id="2b17" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">⒊计算目标函数j</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pd"><img src="../Images/d71aeda1b2e493976534ee5ece889480.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Lx3jrBxlXdwsp8WK9PLSfA.png"/></div></div></figure><p id="b6a8" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒋ </strong>重复<strong class="lm ir"> ⒈ </strong>直到质心稳定或者满足以下标准:新计算的代价函数和旧的代价函数之差小于某个值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/7b14aa663febb573b6922c946247cfd6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1362/format:webp/1*R9yAu9HfQzfyCvXKECSY-Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">FCM的收敛条件。</p></figure><p id="d51e" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">模糊k-means提供了大量真实世界的用例，如图像分割、异常检测。与边缘和对象检测等其他图像处理技术相比，它的计算量较小。</p><p id="7f4f" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨的优势。</strong></p><p id="4d0b" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>与k-means相比，重叠数据的结果更好。</p><p id="05a5" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>时间复杂度低。</p><p id="4460" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒉ </strong>收敛有保证。</p><p id="8b71" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨无往不利。</strong></p><p id="d6ab" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>对k和p的初始值敏感</p><p id="db11" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>对异常值敏感。</p><p id="17b5" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨目标函数最小化。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pf"><img src="../Images/3a91cd97ae0833b22258ac92b758871c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uNVLU5z2zqcK6zSbcxRgyA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">FCM的成本函数。</p></figure><p id="a16d" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">为了找到k个集群的最优解，成本函数<strong class="lm ir"> J </strong> w.r.t <strong class="lm ir"> μ </strong>的导数必须等于零<strong class="lm ir">。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/37170d7ce1694e101a07d9e5cd743b8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1148/format:webp/1*5PPHrgKn2k9RudaczERiTg.png"/></div></figure><p id="bd94" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">对于每个集群J，前面的等式将导致:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pg"><img src="../Images/ef10282d60426aeacab6f43d095bf500.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uy8Z2MD68Az3R8NGdnIcSw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">成本函数w.r.t形心j的梯度</p></figure><p id="3eee" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">已知对于数据集中的每个观察值，所有聚类的成员总数等于1；因此，在每次迭代之后，每个聚类的质心被更新为其经验平均值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ph"><img src="../Images/edb20a9e3a7e6cb28510ad416e8313c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CVTrc4pDLulGi9b8R-3ypA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">每次迭代后，每个聚类的质心被更新为该聚类内所有数据点的平均值。</p></figure><h2 id="a536" class="mp mq iq bd mw mx my dn mz na nb dp nc nd ne nf ng nh ni nj nk nl nm nn no np bi translated"><strong class="ak"> ⓔ.k-水母，PAM(水母周围的分区)</strong></h2><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="1715" class="mp mq iq ml b gy mr ms l mt mu">► <a class="ae mv" href="#04f2" rel="noopener ugc nofollow"><strong class="ml ir">Go To TOC</strong></a><strong class="ml ir"> </strong>◀</span></pre><p id="6e85" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">k-means算法的修改版本，其中medoid表示一个数据点，该数据点在一个聚类内的所有点中具有最低的平均相异度。目的是最小化每个集群的总成本。与k-means不同，它使用一个<a class="ae mv" href="https://en.wikipedia.org/wiki/Medoid" rel="noopener ugc nofollow" target="_blank"> medoid </a>作为度量来重新分配每个聚类的质心。水母对异常值不太敏感。这些中值线是来自数据集的实际观测值，而不是像k-means那样的计算点(平均值)。最好使用曼哈顿距离作为度量，因为它对异常值不太敏感。</p><p id="ca8e" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">⇨</strong>t32】算法。</p><p id="b9dc" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>随机选取k个观测值作为初始均值。</p><p id="7775" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">⒈计算观测值和流星体之间的距离。</p><p id="4362" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒉ </strong>将每个点分配到最近的中点。</p><p id="5ae4" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒊ </strong>在每个集群中选择一个新的观测值(非medoid)并与相应的medoid交换。</p><p id="da0b" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒋ </strong>计算每个簇内每个medoid和新数据点的交换成本。</p><p id="26ed" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒌ </strong>选择具有最低成本(例如，最小相异度之和)的观察值作为新的medoid。</p><p id="ab22" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒍ </strong>重复步骤<strong class="lm ir"> ⒈ </strong>直到满足收敛条件(例如，最小化成本函数、误差平方和(PAM中的SSE))。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pi"><img src="../Images/e86496515a629c73cac22f9127f40f83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RIeowK06hou3W4dDk0fs9A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">k-medoids代价函数。</p></figure><p id="a5b0" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨的优势。</strong></p><p id="e12e" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>在存在异常值的情况下比k-means更稳健(受异常值影响较小。)</p><p id="5f6f" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">⒈:这很容易实现。</p><p id="da31" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒉ </strong>它以固定的迭代次数收敛。</p><p id="1da3" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒊ </strong>它可以有效地处理小数据集。</p><p id="03fe" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨无往不利。</strong></p><p id="8ef1" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>它不适合大型数据集。</p><p id="2bb0" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">⒈的计算复杂度相当昂贵。</p><p id="c656" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒉ </strong>参数k需要初始化为某个值。</p><p id="4c0b" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">⒊不能保证收敛到全局最小值。它对质心的初始化很敏感。不同的设置导致不同的结果。</p><p id="785c" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">⒋只对数字数据有效。</p><p id="6a58" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">为了提高PAM的效率，使用了CLARA算法。</p><h2 id="0133" class="mp mq iq bd mw mx my dn mz na nb dp nc nd ne nf ng nh ni nj nk nl nm nn no np bi translated"><strong class="ak"> ⓕ.k线中位数</strong></h2><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="0316" class="mp mq iq ml b gy mr ms l mt mu">► <a class="ae mv" href="#04f2" rel="noopener ugc nofollow"><strong class="ml ir">Go To TOC</strong></a><strong class="ml ir"> </strong>◀</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/0505c2012bcfc663969e58c231183c50.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/1*9gnUPb_1Lu2_J444fN5DrA.gif"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">k线中线。</p></figure><p id="6ae2" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">k-means算法的一个修改版本使用<a class="ae mv" href="https://en.wikipedia.org/wiki/Median" rel="noopener ugc nofollow" target="_blank">中值</a>，它代表其他观察值均匀分布在其周围的中间点。中位数对异常值的敏感度低于平均值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/9c151807d25df3c5bd10a5a1b8463d22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1186/format:webp/1*9XhIhibE58HkTYQxBNH_jA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">中位数对异常值的敏感度低于平均值。</p></figure><p id="65a1" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">此外，它使用曼哈顿距离作为计算观测值之间距离的度量。此外，该算法旨在最小化以下成本函数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pi"><img src="../Images/e86496515a629c73cac22f9127f40f83.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RIeowK06hou3W4dDk0fs9A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">k-中位数代价函数。</p></figure><p id="8bf8" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨算法。</strong></p><p id="599d" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>随机选取k个观测值作为初始中位数。</p><p id="1dbb" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">⒈计算观察值和中间值之间的距离。</p><p id="475c" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒉ </strong>将每个点分配到最近的中间值。</p><p id="a376" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒊ </strong>计算每个聚类的中值，并将其指定为该聚类的新质心</p><p id="c33f" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒋ </strong>重复步骤<strong class="lm ir"> ⒈ </strong>直到满足收敛条件(例如最小化类似SSE的成本函数)。</p><h2 id="eb7d" class="mp mq iq bd mw mx my dn mz na nb dp nc nd ne nf ng nh ni nj nk nl nm nn no np bi translated"><strong class="ak"> ⓕ.k模式</strong></h2><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="255d" class="mp mq iq ml b gy mr ms l mt mu">► <a class="ae mv" href="#04f2" rel="noopener ugc nofollow"><strong class="ml ir">Go To TOC</strong></a><strong class="ml ir"> </strong>◀</span></pre><p id="84ae" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">由于k-means只处理数字数据属性，因此开发了一种改进的K-means算法来对分类数据进行聚类。<a class="ae mv" href="https://en.wikipedia.org/wiki/Mode_(statistics)" rel="noopener ugc nofollow" target="_blank">模式</a>替换每个集群中的平均值。然而，有人可能会想到在分类属性和数字属性之间进行映射，然后使用k-means进行聚类。这有时可以在小维度数据集上工作。但是，两种不同类型的属性之间的映射不能保证高维数据的高质量聚类。因此，建议在对分类数据属性进行聚类时使用k-modes。</p><p id="5bd5" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">k-modes中使用的相异度度量之一是余弦相异度度量，这是一种基于频率的方法，用于计算两个观察值之间的距离(例如，两个句子或两个文档之间的距离)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pk"><img src="../Images/e1e2dc41b8a952b7d21db246193edc9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AZG50Y3YvFHeHzioRJv_jA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">k模式的成本函数。</p></figure><p id="250f" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨算法。</strong></p><p id="8f00" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">K-Modes聚类过程由以下步骤组成:</p><p id="9576" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>随机选取k个观测值作为初始中心(模态)。</p><p id="5fd8" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>计算每个数据点和聚类中心(模式)之间的相异度</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/60a497f1dbd3a471d17761699cbc0d8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1246/format:webp/1*UhsxCEMPCf7g1dP4brnGKQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">分类数据的相异度(如余弦…)</p></figure><p id="49e5" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒉ </strong>根据相异度(如余弦相异度函数)将每个观察值分配到最近的聚类中心。</p><p id="6cb1" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒊ </strong>根据每个集群中计算的模式值重新定位每个质心。</p><p id="9fb4" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒋ </strong>重复步骤2，直到满足收敛条件(例如，最小化像SSE这样的成本函数)。</p><p id="bbfc" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨的优势。</strong></p><p id="ae72" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>能够对<strong class="lm ir"> </strong>分类数据属性进行聚类。</p><p id="d15a" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">⒈</strong>it<strong class="lm ir">t23】比K-prototypes收敛的更快。</strong></p><p id="0685" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨无往不利。</strong></p><p id="39f6" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>对于大型数据集，计算开销很大(k变大。).</p><p id="392a" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>有时，很难为聚类数(k)选择正确的初始值。</p><p id="0ad9" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">⒉不能保证收敛到全局最小值。它对质心的初始化很敏感。不同的设置可能导致不同的结果。</p><p id="b98b" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒊ </strong>效率取决于算法使用的相异度(例如，斯皮尔曼相关、余弦距离……)。</p><p id="1fc5" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒋ </strong>额外的<strong class="lm ir"> </strong>变量被添加到algorithm(𝛾中，该变量控制从每个观测值到它们的聚类中心的距离的权重。</p><p id="7e9c" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">局部最优问题可以使用全局优化算法来解决，例如<a class="ae mv" href="https://en.wikipedia.org/wiki/Cuckoo_search" rel="noopener ugc nofollow" target="_blank">布谷鸟搜索算法</a>。</p><p id="d5dd" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨应用。</strong></p><p id="cd93" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">k-modes通常用于文本挖掘，如文档聚类、主题建模(其中每个聚类组代表一个给定的主题(相似的词…))、欺诈检测系统、营销(如客户细分。)、网页聚类等等。</p><h2 id="87c6" class="mp mq iq bd mw mx my dn mz na nb dp nc nd ne nf ng nh ni nj nk nl nm nn no np bi translated"><strong class="ak"> ⓖ.k-原型</strong></h2><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="b10c" class="mp mq iq ml b gy mr ms l mt mu">► <a class="ae mv" href="#04f2" rel="noopener ugc nofollow"><strong class="ml ir">Go To TOC</strong></a><strong class="ml ir"> </strong>◀</span></pre><p id="1602" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">这种方法适用于数值和分类数据属性的混合。该算法可以被认为是k-means和k-modes算法之间的组合。</p><p id="1bb1" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">使用该算法，每个数据点都有一个权重，该权重是数字和分类聚类的一部分。此外，每种类型的观察可以以单独的方式处理，其中质心在每种类型的聚类中扮演吸引子的角色。可以使用模糊隶属函数<strong class="lm ir"> aij </strong>来控制给定数据点的隶属度，就像在FCM中一样。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pm"><img src="../Images/e443b23eb8bb59b79c454cdbe5c7afc5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GfPQSbrGEMB-SaBqOb0jmw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">K原型的成本函数</p></figure><p id="3e51" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 𝛾 </strong>用于平衡分类数据属性和数字数据属性之间的影响。</p><p id="3129" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨算法。</strong></p><p id="9a1d" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">K原型聚类过程包括以下步骤:</p><p id="561c" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>随机选取k个代表作为k个聚类的初始原型。</p><p id="3059" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>计算距离(如欧几里德)和相异度(如余弦)。)在每个数据点和相应的聚类中心(原型)之间。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/f7ff55aa4c069c9f2b1ea99cdc88bb14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*FS_M7b3L0B2wmQApLs8SRA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">欧几里德距离和相异度。</p></figure><p id="0a0f" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒉ </strong>根据距离公式将每个观察值分配给最近的集群原型。</p><p id="a94f" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒊ </strong>根据以下公式重新定位每个聚类中心。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi po"><img src="../Images/940ca89c422af16d8244c7681adb788c.png" data-original-src="https://miro.medium.com/v2/resize:fit:988/format:webp/1*54lk6uvHTbvrf7PyKFlk5Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">每种属性的聚类中心。</p></figure><p id="1cf8" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒋ </strong>重复步骤<strong class="lm ir"> ⒈ </strong>直到满足收敛条件(例如，成本函数的最小值)。</p><p id="c4d9" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨的优势</strong></p><p id="5a8d" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>能够聚类混合类型的属性。</p><p id="bc5d" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">⒈在合理的迭代次数内收敛。</p><p id="c8b5" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨无往不利。</strong></p><p id="6a1c" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">🄀不同的相异措施会导致不同的结果。</p><p id="283a" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>对k和𝛾.的初始值敏感</p><p id="953e" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">⒉不能保证收敛到全局最小值。它对水母的初始化很敏感。不同的设置可能导致不同的结果。</p><p id="1adc" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒊ </strong>效率取决于算法使用的相异度(例如，斯皮尔曼相关、余弦距离……)。</p><p id="0aa5" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">在聚类分类数据的情况下，⒋ </strong>比k-modes慢。</p><h2 id="5190" class="mp mq iq bd mw mx my dn mz na nb dp nc nd ne nf ng nh ni nj nk nl nm nn no np bi translated"><strong class="ak"> ⓗ.CLARA(集群大型应用。)</strong></h2><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="bd6e" class="mp mq iq ml b gy mr ms l mt mu">► <a class="ae mv" href="#04f2" rel="noopener ugc nofollow"><strong class="ml ir">Go To TOC</strong></a><strong class="ml ir"> </strong>◀</span></pre><p id="07a6" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">它是一种基于样本的方法，随机选择数据点的一个小子集，而不是考虑整个观察值，这意味着它在大型数据集上工作得很好。此外，从先前选择的样本中选择k个medoids。这将有助于提高PAM的可伸缩性(减少计算时间和内存分配问题)。它按顺序处理不同批次的数据集，以找到最佳结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/0b593cc969e49f99efe0e4228008fcd7.png" data-original-src="https://miro.medium.com/v2/resize:fit:762/format:webp/1*gcPFv9F5GtqPDvQhtiWlfg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">克拉拉</p></figure><p id="6dd9" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">该算法的结果是具有最小成本的一组medoids。</p><p id="8e95" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨算法</strong></p><p id="f1ff" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>从固定大小(大小s)的数据中随机选择多个子集。</p><p id="90ec" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>在一个数据块上计算k-medoid算法，并选择相应的k个medoid。</p><p id="5c2f" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒉ </strong>将原始数据集的每个观测值分配给最近的medoid。</p><p id="e97e" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒊ </strong>计算观测值与其最近的中值的差异的平均值。</p><p id="80b3" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒋ </strong>保留均值最小的数据子集。</p><p id="5b08" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒌ </strong>重复，直到找到最优的medoids。</p><p id="dcc6" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨的优势。</strong></p><p id="beb8" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>能够处理大型数据集。</p><p id="109a" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>在处理大型数据集时减少计算时间。</p><p id="9518" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">⒉处理异常值的能力。</p><p id="4f45" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨无往不利。</strong></p><p id="9e52" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>效率受到k值和样本大小的影响。</p><p id="f691" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">⒈聚类的质量取决于所使用的抽样方法的质量。</p><p id="fdad" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒉ </strong>难以实现。</p><h2 id="a329" class="mp mq iq bd mw mx my dn mz na nb dp nc nd ne nf ng nh ni nj nk nl nm nn no np bi translated">ⓘ.基于随机搜索的大型应用集群。)</h2><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="8644" class="mp mq iq ml b gy mr ms l mt mu">► <a class="ae mv" href="#04f2" rel="noopener ugc nofollow"><strong class="ml ir">Go To TOC</strong></a><strong class="ml ir"> </strong>◀</span></pre><p id="7978" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">它是k-medoid的扩展，用于数据挖掘中对大型数据集进行聚类。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/6c2383c61ff7479ae6c358dc85f7bfdd.png" data-original-src="https://miro.medium.com/v2/resize:fit:846/format:webp/1*4b_qMRDo2QMX-KcQDGQuLA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">克拉伦斯</p></figure><p id="de9e" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">该算法的结果是具有最小成本的一组medoids。</p><p id="920e" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨算法。</strong></p><p id="4278" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>从数据集中随机选择k-medoids。</p><p id="aa1b" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">⒈ 从先前选择的观察和医疗器械中挑选一个。</p><p id="b717" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒉ </strong>计算两点和数据集中所有其他数据点之间的距离。</p><p id="a03d" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒊ </strong>计算交换两个数据点的成本，选择成本最低的一个作为medoid。</p><p id="80d0" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒋ </strong>重复步骤<strong class="lm ir"> ⒈ </strong>直到收敛(找到k-medoids的最优选择)。</p><p id="1961" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨的优势。</strong></p><p id="e1e0" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">在大型数据集上，🄀比帕姆和克拉拉更有效。</p><p id="42a2" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>处理异常值的能力。</p><p id="d77a" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨无往不利。</strong></p><p id="7f6c" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>难以实现。</p><p id="86b8" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>聚类的质量取决于所用抽样方法的质量。</p><h2 id="5c4a" class="mp mq iq bd mw mx my dn mz na nb dp nc nd ne nf ng nh ni nj nk nl nm nn no np bi translated">➀.基于模型/分布的聚类。</h2><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="d16b" class="mp mq iq ml b gy mr ms l mt mu">► <a class="ae mv" href="#04f2" rel="noopener ugc nofollow"><strong class="ml ir">Go To TOC</strong></a><strong class="ml ir"> </strong>◀</span></pre><p id="7f2d" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨概率建模。</strong></p><p id="7426" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">概率模型是由数据变量上的联合分布参数化的生成数据模型:P(x1，x2，…，xn，y1，y2，…，yn|θ)其中X是观察数据，y:潜在变量，θ是参数。</p><blockquote class="lg lh li"><p id="1775" class="lj lk ll lm b ln mf jr lp lq mg ju ls lt mh lv lw lx mi lz ma mb mj md me lf ij bi translated">P(y1，…，yn|x1，…，xn，θ) = P(x1，…，xn，y1，…，yn|θ)(联合)/ P(x1，…，xn|θ)(边际概率)</p></blockquote><p id="56f5" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">学习。</strong></p><p id="d00d" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">使用最大似然法执行学习阶段:</p><blockquote class="lg lh li"><p id="688b" class="lj lk ll lm b ln mf jr lp lq mg ju ls lt mh lv lw lx mi lz ma mb mj md me lf ij bi translated">θML = argmax θ P(x1，…，xn|θ)</p></blockquote><p id="ed78" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">目的是找到一个参数θ，使观测数据的概率最大化。</p><p id="304c" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">预测。</strong></p><blockquote class="lg lh li"><p id="bd04" class="lj lk ll lm b ln mf jr lp lq mg ju ls lt mh lv lw lx mi lz ma mb mj md me lf ij bi translated">P(xn+1，yn+1|x1，…，xn，θ)</p></blockquote><p id="6c59" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">目标是在给定观察数据集的情况下计算潜在属性的条件分布。</p><p id="889d" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">分类:</strong></p><p id="afb3" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">目标是找到一个类，在给定学习参数θ的情况下，使未来数据的概率最大化:</p><blockquote class="lg lh li"><p id="3398" class="lj lk ll lm b ln mf jr lp lq mg ju ls lt mh lv lw lx mi lz ma mb mj md me lf ij bi translated">argmax c P(xn+1|θc)</p></blockquote><p id="ba80" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">概率建模中使用的一些标准算法是EM算法、MCMC采样、连接树等。</p><h2 id="8925" class="mp mq iq bd mw mx my dn mz na nb dp nc nd ne nf ng nh ni nj nk nl nm nn no np bi translated"><strong class="ak"> ⓐ.GMM:高斯混合模型</strong></h2><p id="9065" class="pw-post-body-paragraph lj lk iq lm b ln nq jr lp lq nr ju ls nd ns lv lw nh nt lz ma nl nu md me lf ij bi translated">在2-d变量空间中，高斯分布是使用具有正态分布的两个随机变量构建的二元正态分布，每个正态分布由其平均值和标准偏差参数化。</p><blockquote class="lg lh li"><p id="5343" class="lj lk ll lm b ln mf jr lp lq mg ju ls lt mh lv lw lx mi lz ma mb mj md me lf ij bi translated">在我看来，高斯分布如此重要是因为它使计算(如线性代数计算。)毫不费力地做。然而，它并不是现实世界应用程序的完美模型。</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/6320c10fda8f255f1d074d946f4c589b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1280/format:webp/1*X7hL3-Jbx-w_J5rB5iWnqw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">三维空间中的高斯及其投影。</p></figure><p id="644a" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">高斯混合模型是半参数模型(随着数据增加的有限数量的参数。)用作软聚类算法，其中每个聚类对应于一个生成模型，该生成模型旨在发现给定聚类的概率分布参数(例如，均值、协方差、密度函数……)(它自己的概率分布控制每个聚类)。学习的过程是用高斯模型来拟合数据点。高斯混合模型假设聚类在n维空间中以正态分布分布。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ps"><img src="../Images/50c4bc4b6bd5e89bd0c46b46e9edc038.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*i_jGHp5SKbDKKYywfiKsQQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">协方差矩阵和一维空间中的高斯公式。</p></figure><p id="cd55" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">为了说明一维空间中的混合模型，假设有两个正态分布的信息源，其中从每个信息源收集了n个样本。要估计每个高斯分布的平均值，取观察值的总和，并除以收集的样本数(经验平均值。)，同样用于估计其他参数。</p><p id="b76c" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">当有k个高斯模型，并且没有给出关于观测值来自哪里的信息时，问题就出现了；不容易搞清楚怎么把点分成k簇。因此，估计每个高斯参数几乎是不可能的。然而，如果高斯参数(均值、方差)是预定义的，这个问题就可以解决。</p><p id="456f" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">这就是EM方法试图解决的问题。</p><h2 id="ffbb" class="mp mq iq bd mw mx my dn mz na nb dp nc nd ne nf ng nh ni nj nk nl nm nn no np bi translated"><strong class="ak"> ⓑ.EM:集群环境下的期望最大化。</strong></h2><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="87d8" class="mp mq iq ml b gy mr ms l mt mu">► <a class="ae mv" href="#04f2" rel="noopener ugc nofollow"><strong class="ml ir">Go To TOC</strong></a><strong class="ml ir"> </strong>◀</span></pre><p id="bb79" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">这是一种众所周知的用于拟合混合分布的算法，旨在当一些数据点不可用时(例如，未知参数、潜在值等)，使用最大似然原理(寻找最优值)来估计给定分布的参数。在GMM的背景下，直觉是在空间中随机放置k个高斯模型，并计算每个数据点对某个高斯模型的隶属度。与硬聚类(例如，k-means)不同，该方法计算每个点成为某个聚类的成员的概率。此外，这些值用于重新估计聚类参数(例如，平均值、协方差)以拟合为每个聚类分配的点。</p><p id="2873" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">EM广泛用于解决诸如“隐藏数据”问题、隐藏马尔可夫模型之类的问题，其中存在依赖于先前隐藏变量的状态的潜在变量序列。此外，每个观察值取决于相应隐藏变量的状态。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pt"><img src="../Images/7c5fd4854a3de214642f2fcdd6ece024.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IxRsrDiGNIrKHfFrnG6hRA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">隐马尔可夫模型。</p></figure><p id="4813" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> πₖ </strong>是给定前一状态k的转移概率。箭头描述变量之间的相关性。</p><p id="ca57" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">算法。</strong></p><p id="1622" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">EM算法包括两个步骤，期望步骤和最大化步骤。</p><p id="b839" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">步骤-0: </strong>参数θs的初始化。</p><p id="875d" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> E步骤:</strong>在该步骤中，通过计算每个潜在数据点的归一化期望值<strong class="lm ir"> Wij </strong>(数据点在每个分布中的权重)来估计观察值来自哪个分布，假设给定聚类J的质心<strong class="lm ir"> μj </strong>和协方差矩阵<strong class="lm ir">σJ</strong>当前假设成立:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi pu"><img src="../Images/b14fcae68d3adaa17070b9d560837a67.png" data-original-src="https://miro.medium.com/v2/resize:fit:1298/format:webp/1*KmqL9c0w-KnMvTm4g6qULw.png"/></div></figure><p id="d3bd" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> P(xi|K=j，θ) </strong>是多元正态分布<strong class="lm ir"> Xi~N(μi，σI)的条件概率。</strong></p><p id="c719" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">每个聚类具有可以基于训练数据集估计的概率<strong class="lm ir"> 𝜋( </strong>先于<strong class="lm ir"> ) </strong>。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pv"><img src="../Images/ff5ed4ebfe6f3e28df39b0664531a146.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dHB4Y-oh4g0VQ129kIOv_w.png"/></div></div></figure><p id="5f1f" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> M步:</strong>使用前一步获得的信息，m步将使用新的最大似然假设(假设每个隐藏变量的值为期望值)更新均值<strong class="lm ir"> μj </strong>和协方差<strong class="lm ir">σj</strong>(或方差<strong class="lm ir"> 𝜎 </strong>)的估计值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pw"><img src="../Images/f55083349c10b8b92a6f971b505eb834.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3VESHGsdlhikTly7LAyP8w.png"/></div></div></figure><p id="9e67" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">重复E和M步骤，直到对数似然函数收敛。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi oh"><img src="../Images/8271cea753ed0b34fcd05f3123079cdc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fF4scs3p5UsDuE309ePBOg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">对数似然函数</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi px"><img src="../Images/82f15818a988dcaf8bbd7c05615f0eaa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vu-iMwslRVCmPky05LIhKw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">对数似然图。</p></figure><p id="ef8d" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">利用每次迭代后似然性单调增加的事实，该算法更有可能收敛到最优。</p><p id="4482" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">为了演示EM算法，让我们考虑从三个高斯模型(a、b、c)生成的观察值。由于每个样本都是未标记的，所以目标是估计这三个高斯模型的参数，以将每个点标记为某个高斯分布。为了估计这些参数，将三个高斯模型随机放置在1-d数据集空间中。</p><p id="6a2e" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>计算从具有以下密度函数的三个高斯模型生成的每个数据点的可能性。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi py"><img src="../Images/28093cb933b492b859a4f3cad4fd4862.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*P-KT5CpwaM2db8ytRN54Rg.png"/></div></div></figure><p id="6637" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ E步:</strong>对于每个数据点，计算其权重wi(ai，bi，ci)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi pz"><img src="../Images/088c9b6974e46764aa6442906a233c3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Jkud8lHZBHO2phtA93LClw.png"/></div></div></figure><p id="a19e" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒉ </strong> <strong class="lm ir"> M步:</strong>此时，可以估计每个模型的均值和方差。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qa"><img src="../Images/9e1753cb75cb56f2652abd4a58892223.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*73sBM6podkEBawVy25ZKGQ.png"/></div></div></figure><p id="ea7e" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">⒊估计概率。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qb"><img src="../Images/3b0a96363bc8de85f878b4fc1fcdd60d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*QkU7qScWGHnQpdxVF0qxzg.png"/></div></div></figure><p id="d971" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒋ </strong>重复e和m步骤，直到对数似然函数收敛。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qc"><img src="../Images/e6a055fef5a4554891fcd45ed2bbc29c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*hgKk54jxfSJP6i4Pbn4Eyg.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">具有3个集群的1d GMM。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qc"><img src="../Images/e02f02efe7a8a09f33d6dc5d7cc7a40c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*Jd4VzUNUdC_UJ_aydSa0YQ.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">具有3个集群的2d GMM。</p></figure><p id="d8ba" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨优势。</strong></p><p id="c7d6" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>它产生混合分布参数的有效估计。</p><p id="7870" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">⒈实现起来非常简单。</p><p id="203c" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨无往不利。</strong></p><p id="57c4" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>为k(混合模型的数量)选择一个初始值，就像在k-means中一样。</p><p id="d43b" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>对初始值敏感，从而导致不同的结果。</p><p id="5d82" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">⒉:它可能会收敛到一个局部最优解。</p><p id="0aac" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">⒊的收敛速度可能会很慢。</p><h2 id="14a7" class="mp mq iq bd mw mx my dn mz na nb dp nc nd ne nf ng nh ni nj nk nl nm nn no np bi translated">ⓒ.狄利克雷混合模型。</h2><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="5291" class="mp mq iq ml b gy mr ms l mt mu">► <a class="ae mv" href="#04f2" rel="noopener ugc nofollow"><strong class="ml ir">Go To TOC</strong></a><strong class="ml ir"> </strong>◀</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qc"><img src="../Images/bae4845ed69ad7b2c12ebb6f20238fbc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*hatVz-KZlLvIJ0NE1ED6XA.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">狄利克雷过程。</p></figure><p id="eb09" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">狄利克雷过程是一种随机过程，它在用于定义贝叶斯非参数(非固定参数集)的离散分布(概率度量)上产生分布。例如~无限数量的参数。)模特。狄利克雷分布是由浓度/精度参数/向量(α₁，…，αₖ)参数化的连续多元密度函数，具有正分量和基本分布H: DP(α，h)。它类似于两个以上结果的Beta分布(如Coinflip)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qd"><img src="../Images/5ddb4fdd1080f8e2a48a854bbf51d503.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*KHvWov2pdL7qDW-tY2IkGw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">联合分布的图形模型。</p></figure><p id="26e2" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">k维狄利克雷:<strong class="lm ir"> (π₁，π₂，…，πₖ) ~Dirichlet(α₁，α₂,…，αₖ) </strong></p><p id="dfe9" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">θ是独立的参数，并且在H上同分布，目标是在给定观测值xi的情况下推断参数θ和潜在变量。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qc"><img src="../Images/4aa32bb6706f0e0c95a66ef6816f008d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iHaBTA6Iase8cwOmbbjrYw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd qe"> α &gt; 1的不同值的狄利克雷分布和样本。</strong></p></figure><ul class=""><li id="55b2" class="qf qg iq lm b ln mf lq mg nd qh nh qi nl qj lf qk ql qm qn bi translated"><strong class="lm ir"> α = (1，1，1) </strong>，该图代表均匀分布。</li><li id="347e" class="qf qg iq lm b ln qo lq qp nd qq nh qr nl qs lf qk ql qm qn bi translated"><strong class="lm ir"> α &gt; (1，1，1) </strong>，该图代表单峰分布。</li><li id="4118" class="qf qg iq lm b ln qo lq qp nd qq nh qr nl qs lf qk ql qm qn bi translated"><strong class="lm ir"> 0 &lt; α &lt; (1，1，1) </strong>，该图代表多峰分布。</li></ul><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qc"><img src="../Images/66e439bdc478cec6e5fdabfcf721073b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IBGJngpqvaXY3A-wkvbYrA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd qe"> α &lt; 1不同值的狄利克雷分布和样本。</strong></p></figure><p id="292e" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">狄利克雷分布的一个很大的性质是，当合并两个不同的分量(πi，πj)时，会产生一个边际分布，这个边际分布是由参数(αi，αj)相加而参数化的狄利克雷分布。它类似于降维的思想。这种特性被称为塌陷。另一个性质是，可以证明具有伽马分布的随机变量遵循狄利克雷分布。</p><p id="e75e" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> π </strong>是经常用著名的断棒例子描述的概率。为了解释这些值，一根长度为一个单位的棍子被用来随机产生一个介于0和1(棍子的最大长度)之间的数，在该数处棍子将被折断。一旦生成，木棒可以在长度<strong class="lm ir"> π </strong>处断裂，该长度代表来自以1和α为参数的β分布的随机值:<strong class="lm ir">π∾β(1，α) </strong>。通过折断这根棍子，它将生成一个概率质量函数(PMF)，其中两个结果的概率分别为<strong class="lm ir"> π </strong>和<strong class="lm ir">1π</strong>。两根木棒可以类似地进一步折断，这样所有木棒的长度之和必须等于1。并且该过程可以无限重复。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qt"><img src="../Images/6f8cc5ed3e79cb846014b9b44cf643f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1146/format:webp/1*GRpI8eLc3RDrPi0CFsow7Q.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">棍子折断的例子。</p></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qu"><img src="../Images/190f89e296e211cab4cf002edbff3c93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H9m2LvgYLHI3ySz_qXY8Yw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">y轴代表后验的期望混合权重(<strong class="bd qe"> πi </strong>)。x轴代表组件的数量。</p></figure><p id="5ac8" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">狄利克雷分布经常在主题建模和LDA(潜在{隐藏主题}狄利克雷{狄利克雷分布}分配)的上下文中解释。LDA的工作原理是将许多文档聚类成包含相似单词的主题，而无需事先了解这些主题。LDA通过从两个分布中采样来构建文档(按文档的主题分布，按主题的单词分布)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qd"><img src="../Images/5ddb4fdd1080f8e2a48a854bbf51d503.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*KHvWov2pdL7qDW-tY2IkGw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">联合分布的图形模型。</p></figure><p id="0264" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">在LDA中，每个主题在单词上具有多项式分布(H)，每个文档从由<strong class="lm ir"> α </strong>参数化的狄利克雷分布(<strong class="lm ir"> π </strong>中采样，并且每个单词(xi)从具有由<strong class="lm ir"> π参数化的多项式分布的隐藏主题(Zi)中采样。</strong></p><p id="e0de" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">通过对每个文档进行分类，LDA倾向于通过最大化其概率来使每个文档有意义，如下所示:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qv"><img src="../Images/b07c3593ae503a7e27202a52ef4cbce9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t3BFe9-HAicMQ6uCopDguQ.png"/></div></div></figure><p id="7d25" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">然而，最大化这个公式是相当昂贵的。因此<a class="ae mv" href="https://en.wikipedia.org/wiki/Gibbs_sampling#:~:text=In%20statistics%2C%20Gibbs%20sampling%20or,when%20direct%20sampling%20is%20difficult." rel="noopener ugc nofollow" target="_blank">吉布斯采样</a>用于最大化方程的每个参数(单词:x，题目:z…)。</p><p id="8861" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨ LDA算法。</strong></p><p id="3831" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>初始化话题数量k</p><p id="eedb" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>将每个文档中的每个单词随机分类到一个主题中。</p><p id="2c5c" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒉ </strong>对每个文档进行迭代，并计算以下概率:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qw"><img src="../Images/b92d79a8078611f6921699ec65c180ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aL4pGx4GPpKhosq2LnscVQ.png"/></div></div></figure><p id="9c31" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">⒊把每个单词重新归类到给定的主题中。</p><p id="e00c" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒋ </strong>重复直到前一个公式达到最大值。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qv"><img src="../Images/b07c3593ae503a7e27202a52ef4cbce9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*t3BFe9-HAicMQ6uCopDguQ.png"/></div></div></figure><p id="01af" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨的优势。</strong></p><p id="9cdf" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>对于大型数据集非常高效和灵活。</p><p id="ef27" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>算法的工作流程独立于其他任务。</p><p id="ca9b" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨无往不利。</strong></p><p id="c1ba" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>题目的数量k必须事先定义。</p><p id="8aea" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>不相关的话题。</p><h2 id="6178" class="mp mq iq bd mw mx my dn mz na nb dp nc nd ne nf ng nh ni nj nk nl nm nn no np bi translated">➁.基于密度的聚类。</h2><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="d7d0" class="mp mq iq ml b gy mr ms l mt mu">► <a class="ae mv" href="#04f2" rel="noopener ugc nofollow"><strong class="ml ir">Go To TOC</strong></a><strong class="ml ir"> </strong>◀</span></pre><p id="ef7a" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">在基于密度的聚类中，数据空间中的密集区域与密度较低的区域是分开的。如果某个位置的密度大于预定义的阈值，则将观测值分配给给定的聚类。</p><p id="d644" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">对于一个集群中的给定观测，该点周围的局部密度必须超过某个阈值。局部密度由两个参数定义:包含围绕给定点的一定数量的邻居的圆的半径<strong class="lm ir"> ε </strong>和围绕该半径的最小数量的点:<strong class="lm ir"> minPts </strong>。</p><p id="6af0" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨定义。</strong></p><p id="3c8e" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">🄀</strong>t34】Eps-邻域:半径为EPS的圆对于给定点的面积。</p><p id="c94a" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈Density可达:</strong>点p被描述为从点q相对于Eps和MinPoints可达的密度当且仅当存在一组点(p1，p2，…，pi，…，pn)使得pi+1从pi直接可达。</p><p id="b1d2" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒉ </strong></p><p id="1bc9" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒊密度连通:</strong>一个点p被描述为关于Eps和MinPoints连通到点q的密度当且仅当存在一个点w是从p和q可达的密度<strong class="lm ir"> </strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qx"><img src="../Images/6a00466841c194432dbb89ed0390330d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AbS0auZ2UPfoYQuyDHn1kQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">可达性类型。</p></figure><p id="3d15" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨的优势。</strong></p><p id="f89b" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>不需要簇数k</p><p id="4f55" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>发现更复杂形状的星团(例如月亮形状的星团。).</p><p id="f9ed" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒉ </strong>离群点检测。</p><p id="9641" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨无往不利。</strong></p><p id="b880" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>对拓扑连接的对象进行分类在计算上是不可行的。</p><p id="b9ab" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>不像K-means那样保持可扩展性。</p><p id="5220" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒉ </strong>对Eps、MinPts敏感</p><p id="4283" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒊ </strong>密度测量受采样数据点的影响。</p><h2 id="5588" class="mp mq iq bd mw mx my dn mz na nb dp nc nd ne nf ng nh ni nj nk nl nm nn no np bi translated"><strong class="ak"> ⓐ.DBS can :D</strong>en sity-<strong class="ak">B</strong>S<strong class="ak">S</strong>S<strong class="ak">C</strong>说明<strong class="ak">A</strong>A<strong class="ak">N</strong>oise应用</h2><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="f03a" class="mp mq iq ml b gy mr ms l mt mu">► <a class="ae mv" href="#04f2" rel="noopener ugc nofollow"><strong class="ml ir">Go To TOC</strong></a><strong class="ml ir"> </strong>◀</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qc"><img src="../Images/7a3cb85954e113fd44946a34e6e661b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*3_WQJ9ljj60KnedgkqNikw.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">DBSCAN发现4个集群。</p></figure><p id="bfee" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">这是迄今为止最流行的基于密度的聚类算法，在<a class="ae mv" href="https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C5&amp;q=DBSCAN&amp;btnG=" rel="noopener ugc nofollow" target="_blank">谷歌学术</a>上被引用超过41k次。中心思想是将观测结果分成3种类型的点组:</p><p id="0b4c" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀核心</strong>点:在ε-邻域中有超过<strong class="lm ir">个minPts </strong>点。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qx"><img src="../Images/bb9ab1cdf8cc212bfef3b009617d60df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MtP7ga3id49j5aT86741bw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><strong class="bd qe"> minPts = 5 </strong></p></figure><p id="9905" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈边界</strong>点:少于<strong class="lm ir"> minPts </strong>在<strong class="lm ir"> ε </strong>内但在一个核心点附近。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qy"><img src="../Images/96d54f74ceb242079af9191b09ff856a.png" data-original-src="https://miro.medium.com/v2/resize:fit:946/format:webp/1*FRAXUMjKJ9Pc03N91LW50g.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">从核心点a可以到达点B。</p></figure><p id="483b" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒉ </strong> <strong class="lm ir">噪声</strong>或<strong class="lm ir">异常值</strong>点:所有剩余的点:不是核心点，并且没有接近到足以从核心点到达。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qz"><img src="../Images/05c1af5402a5f0a11d262165d73dc7a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*u_QQnPt2XtbXxD130K8p9A.png"/></div></figure><p id="83d7" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨的解释。</strong></p><p id="f20f" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">它从随机选择一个还没有被分配给一个簇的点开始。然后，该算法确定它是核心点还是离群点。一旦找到一个核心点，它的所有密度可达观测值将被添加到一个聚类中。之后，该算法将对每个可直接到达的点执行邻居跳转，并将它们添加到集群中。如果添加了异常值，它将被标记为边界点。然后，该算法选取另一个核心点，并重复前面的步骤，直到所有点都被分配到聚类或被标记为异常值。</p><p id="2e67" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨算法。</strong></p><p id="85c3" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>随机挑选一个点p</p><p id="2898" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>找出由p给定<strong class="lm ir"> eps </strong>和<strong class="lm ir"> minPts </strong>密度可达的所有点。</p><p id="7cdc" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒉ </strong>检验p是否为核心点。一个集群将由至少一个核心点、可到达的核心点以及它们的所有边界构成。</p><p id="a2ac" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒊ </strong>重复前面的步骤，直到遍历完所有的点。</p><p id="77dc" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨的优势。</strong></p><p id="fee1" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>能够确定任意形状的星团。</p><p id="e1a5" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>对异常值不太敏感。</p><p id="b94e" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒉ </strong>可以作为离群点检测。</p><p id="7b30" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">⒊可以有效地处理任何规模的数据集。</p><p id="a99b" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨无往不利。</strong></p><p id="9e12" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>不适用于高维数据集。</p><p id="96cd" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>取决于几个超参数。</p><p id="f3f7" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒉ </strong>发现密度变化的星团的问题。</p><p id="47ea" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒊ </strong>只对数字数据有效。</p><p id="99f7" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">⇨应用程序。</p><p id="db4f" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">它广泛用于异常检测、科学文献和其他应用。</p><h2 id="b750" class="mp mq iq bd mw mx my dn mz na nb dp nc nd ne nf ng nh ni nj nk nl nm nn no np bi translated"><strong class="ak"> ⓑ.ADBSCAN: A </strong>自适应<strong class="ak"> DBSCAN </strong></h2><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="5f73" class="mp mq iq ml b gy mr ms l mt mu">► <a class="ae mv" href="#04f2" rel="noopener ugc nofollow"><strong class="ml ir">Go To TOC</strong></a><strong class="ml ir"> </strong>◀</span></pre><p id="a995" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">顾名思义，该算法与之前的算法不同，它采用代表每个聚类密度分布的<strong class="lm ir"> Eps </strong>和<strong class="lm ir"> MinPts </strong>的值。它会自动找到合适的Eps和MinPts值。</p><p id="1302" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">它首先为<strong class="lm ir"> Eps </strong>随机选择一个值。然后，它对数据集运行DBSCAN，如果它未能找到一个集群，它会将<strong class="lm ir"> Eps </strong>的值增加0.5。当算法找到一个聚类(10%的相似数据)时，它会将该聚类从数据集中排除。并且，该算法不断增加eps的值以找到下一个聚类。一旦算法成功地扫描完大约95%的数据，剩余的数据点将被声明为异常值。</p><p id="c578" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">但是，ADBSCAN需要数据集中聚类数的初始值。有关更多信息，请考虑阅读<a class="ae mv" href="https://arxiv.org/pdf/1809.06189v3.pdf" rel="noopener ugc nofollow" target="_blank">这篇文章</a>。</p><h2 id="eeb3" class="mp mq iq bd mw mx my dn mz na nb dp nc nd ne nf ng nh ni nj nk nl nm nn no np bi translated">ⓒ.登线索:登基CLU圣<strong class="ak"> E </strong>环</h2><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="2773" class="mp mq iq ml b gy mr ms l mt mu">► <a class="ae mv" href="#04f2" rel="noopener ugc nofollow"><strong class="ml ir">Go To TOC</strong></a><strong class="ml ir"> </strong>◀</span></pre><p id="2bcd" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">DENCLUE应用<a class="ae mv" href="https://en.wikipedia.org/wiki/Kernel_density_estimation" rel="noopener ugc nofollow" target="_blank">核密度估计</a>方法来估计产生数据样本的随机变量的未定义概率密度函数。该估计基于核密度函数(例如，高斯密度函数。)表示每个数据点的分布。然后，通过求和(或积分)来计算所有先前函数的核密度估计。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qc"><img src="../Images/aa95fe6404abd8fe4d82fa0c29724eb0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*hax1nCWf2w16s57iqjv1Ow.gif"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">基于样本分布的核密度估计(KDE)。</p></figure><p id="7fe3" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">核是一种数学函数，用于模拟数据点及其邻居之间的影响。此外，核密度函数具有以下特性:</p><p id="d2bc" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>非负性:K(x) ≥ 0</p><p id="eaed" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>对称:K(x) = K(-x)</p><p id="c78c" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">⒉内核下的面积必须等于一个单位。</p><p id="f816" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒊ </strong>递减:K'(x) ≤ 0</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi qc"><img src="../Images/298fa2c5fae75b58b6caf3689839d2a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xhOJQRVsII5Uc8oVI9Zt4A.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">不同类型的一维核。</p></figure><p id="474c" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">DENCLUE使用了密度吸引子的概念，这些吸引子代表了周围形成集群的观察结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi qz"><img src="../Images/a2fb8e9b44f402421df412bbe137d1d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/format:webp/1*PpA8QyEYTwElx-7XqxuqzQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">带有两个密度吸引子的二维内核示例。</p></figure><p id="22c2" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">有两种类型的集群:</p><p id="8a40" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>中心定义簇:它是通过分配被吸引到给定密度吸引子的点的密度而形成的。</p><p id="45f2" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>任意形状的簇:它由具有高密度的密度吸引子(&gt;给定阈值)合并而成</p><p id="a384" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨算法。</strong></p><p id="c10b" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>通过将所有数据点的密度函数相加来估计数据空间的整体核密度函数。</p><p id="8be5" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>聚类通过识别构成估计密度函数的局部最大值的密度吸引子来形成。</p><p id="8f36" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒉ </strong>使用带有估计密度函数梯度的<a class="ae mv" href="https://en.wikipedia.org/wiki/Hill_climbing" rel="noopener ugc nofollow" target="_blank">爬山</a>算法计算局部最大值。</p><p id="bd28" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨的优势。</strong></p><p id="1985" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>明显比DBSCAN快。</p><p id="d7fa" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>灵活适用于任何任意形状的集群。</p><p id="a0a0" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">⒉可以有效地处理任何规模的数据集。</p><p id="482a" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨无往不利。</strong></p><p id="c635" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>不适合高维数据集。</p><p id="e040" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>取决于几个超参数。</p><p id="0ca5" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒉ </strong>只对数值数据有效。</p><h2 id="d22d" class="mp mq iq bd mw mx my dn mz na nb dp nc nd ne nf ng nh ni nj nk nl nm nn no np bi translated"><strong class="ak"> ⓓ.光学:</strong>排序点识别聚类结构。</h2><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="94dc" class="mp mq iq ml b gy mr ms l mt mu">► <a class="ae mv" href="#04f2" rel="noopener ugc nofollow"><strong class="ml ir">Go To TOC</strong></a><strong class="ml ir"> </strong>◀</span></pre><p id="ecc5" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">由于DBSCAN的性能取决于其参数设置，Optics扩展了DBSCAN，使其对参数设置不太敏感，并在簇之间查找结构。直觉上，基于两个参数，较高密度的区域将在较低密度的区域之前首先被处理:</p><p id="74e2" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀核心距离:</strong>包含至少MinPts个观测值的最小半径eps。</p><p id="8a81" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈可达距离:</strong>使两个观测值密度可达的最小距离。</p><p id="74ae" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">也就是说，光学根据观测值的密度结构形成有序的集群。此外，它使用所有点的可达性距离的计算值作为阈值，以分离数据和异常值(位于红线以上的点)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ra"><img src="../Images/021fc4681300fb49aab9e00c0817c649.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Wq1ycxIsjc6NkSJ7fI-ALQ.png"/></div></div></figure><p id="b242" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨算法。</strong></p><p id="b495" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>从数据集中随机选取一个数据点。</p><p id="07b4" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>通过计算eps邻域内的核心距离来确定所选点是否为核心点。</p><p id="df5d" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒉ </strong>如果所选择的点是核心点，那么对于每一个其他的观察值，更新从先前选择的点的可达性距离。此外，将新的观察结果插入到OrderSeeds中，该order seeds包含按可达性距离排序的点。</p><p id="8907" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒊ </strong>如果所选点不是核心点，则移动到OrderSeeds中的下一个观察点，或者如果OrderSeeds为空，则移动到初始数据点中的下一个观察点。</p><p id="8206" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒋ </strong>重复，直到遍历完所有观测值。</p><p id="e15f" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨的优势。</strong></p><p id="fcdf" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>能够发现内在的、分层嵌套的聚类结构。</p><p id="b129" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>需要与DBSCAN相同数量的参数(eps和<strong class="lm ir"> minPts </strong>),但不需要eps，这降低了算法的运行时复杂性。</p><p id="4c19" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒉ </strong>能够发现具有不同密度的星团。</p><p id="9c7a" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨无往不利。</strong></p><p id="924f" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> 🄀 </strong>没有密度下降的集群的问题。</p><p id="ecf5" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⒈ </strong>仍然对参数<strong class="lm ir"> minPts </strong>敏感。</p><p id="5692" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⇨应用。</strong></p><p id="6901" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">光学可用于异常检测(发现异常值)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi rb"><img src="../Images/990711b12c397889cc86b26acdd3ca98.png" data-original-src="https://miro.medium.com/v2/resize:fit:1240/format:webp/1*HBlYVyFN2tbz-YEtlGHRLg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">异常检测。</p></figure><h2 id="9b71" class="mp mq iq bd mw mx my dn mz na nb dp nc nd ne nf ng nh ni nj nk nl nm nn no np bi translated">🄲.<strong class="ak">结论</strong></h2><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="e627" class="mp mq iq ml b gy mr ms l mt mu">► <a class="ae mv" href="#04f2" rel="noopener ugc nofollow"><strong class="ml ir">Go To TOC</strong></a><strong class="ml ir"> </strong>◀</span></pre><p id="be1f" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">通过本文，您已经了解了如何使用聚类分析作为一种强大的技术来发现模式并从数据中提取洞察力。然而，决定是否选择给定的聚类算法取决于几个标准，例如聚类应用的目标(例如，主题建模、推荐系统……)、数据类型等。此外，数据挖掘团队有责任决定选择最适合他们需要的方法。</p><p id="502b" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">哇呜！你已经到了今天博客的结尾，这有点让人不知所措，不骗你。然而，你可以通过粉碎👏按钮，直到你感到宽慰😌。</p><p id="fde0" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">我希望你喜欢这篇花了我很长时间(大约一个月)让它尽可能简洁明了的文章。我将感谢您的支持，请关注我，关注即将到来的工作和/或分享这篇文章，以便其他人可以找到它。</p><p id="d80a" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">正如你可以从插图中看出的，我已经设法实现并可视化了大多数算法。当它完成后，我将在GitHub上发布第一个版本。</p><p id="43fc" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">和往常一样，你可以引用这篇文章中的任何插图和其他信息:</p><blockquote class="lg lh li"><p id="d254" class="lj lk ll lm b ln mf jr lp lq mg ju ls lt mh lv lw lx mi lz ma mb mj md me lf ij bi translated">Mahmoud Harmouch，17数据科学和挖掘中使用的聚类算法，走向数据科学，2021年4月23日。</p></blockquote><p id="8f57" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">如果你有任何需要传授的智慧之言，我很高兴在评论区看到你的想法。</p><p id="56af" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">如果您在这篇文章中遇到了任何误传或错误，为了改进内容，请不要忘记提及它们。</p><p id="49cb" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">下一篇文章再见。</p><p id="2c3a" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">Peace✌️</p><h2 id="61ab" class="mp mq iq bd mw mx my dn mz na nb dp nc nd ne nf ng nh ni nj nk nl nm nn no np bi translated">🄳.有用的资源</h2><pre class="kg kh ki kj gt mk ml mm mn aw mo bi"><span id="4853" class="mp mq iq ml b gy mr ms l mt mu">► <a class="ae mv" href="#04f2" rel="noopener ugc nofollow"><strong class="ml ir">Go To TOC</strong></a><strong class="ml ir"> </strong>◀</span></pre><p id="7ec8" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⓐ.k-means </strong></p><p id="7ea1" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">【0】</strong>笔记本:<a class="ae mv" href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.11-K-Means.ipynb#scrollTo=hIA9Xj4ds_KV" rel="noopener ugc nofollow" target="_blank"> 05.11-K-Means.ipynb </a></p><p id="90d5" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">【1】</strong>钱丹·k·雷迪，巴努基兰·温扎莫里；<a class="ae mv" href="https://dmkd.cs.vt.edu/papers/kmeans13.pdf" rel="noopener ugc nofollow" target="_blank">分区和层次聚类算法综述</a></p><p id="c96a" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">【2】</strong>jeffp；<a class="ae mv" href="https://www.cs.utah.edu/~jeffp/teaching/cs5955/L10-kmeans.pdf" rel="noopener ugc nofollow" target="_blank"> L10: k均值聚类</a></p><p id="31aa" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">【3】</strong>克里斯皮赫；<strong class="lm ir"> </strong> <a class="ae mv" href="https://stanford.edu/~cpiech/cs221/handouts/kmeans.html" rel="noopener ugc nofollow" target="_blank"> K表示</a></p><p id="429e" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⓑ.k-means++ </strong></p><p id="6c6f" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">【0】</strong>大卫·亚瑟，谢尔盖·瓦西里耶维奇；<a class="ae mv" href="http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf" rel="noopener ugc nofollow" target="_blank"> k-means++:小心播种的好处。</a></p><p id="7cf6" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⓒ.k-means|| </strong></p><p id="5578" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">【0】</strong>巴赫曼·巴赫马尼、本杰明·莫塞利、安德烈·瓦塔尼、拉维·库马尔、谢尔盖·瓦西里维茨基；<a class="ae mv" href="https://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf" rel="noopener ugc nofollow" target="_blank">可扩展K-Means++ </a></p><p id="c9e0" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⓓ.FCM </strong></p><p id="39a5" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">马杜库马尔，桑提亚库马里，N..(2015).<a class="ae mv" href="https://www.researchgate.net/publication/274096080_Evaluation_of_k-Means_and_fuzzy_C-Means_segmentation_of_MR_images_of_brain" rel="noopener ugc nofollow" target="_blank">脑部MR图像k均值和模糊C均值分割的评价。埃及放射学和核医学杂志。1.2015年2月10日</a></p><p id="895b" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⓔ.k-medoids </strong></p><p id="4037" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">【0】</strong>维基百科关于K-medoids的文章:【https://en.wikipedia.org/wiki/K-medoids】T42</p><p id="990e" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">【1】</strong>Tri Nguyen的K-medoids实现:<a class="ae mv" href="https://laptrinhx.com/link/?l=https%3A%2F%2Ftowardsdatascience.com%2Fk-medoids-clustering-on-iris-data-set-1931bf781e05" rel="noopener ugc nofollow" target="_blank">https://towardsdatascience . com/K-me doids-clustering-on-iris-data-set-1931 BF 781 e 05</a></p><p id="5c6e" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">【2】</strong>Github库的scikit learn _ extra:<a class="ae mv" href="https://laptrinhx.com/link/?l=https%3A%2F%2Fgithub.com%2Fscikit-learn-contrib%2Fscikit-learn-extra%2Ftree%2Fmaster%2Fsklearn_extra%2Fcluster" rel="noopener ugc nofollow" target="_blank">https://Github . com/scikit-learn-contrib/scikit-learn-extra/tree/master/sk learn _ extra/cluster</a></p><p id="bc75" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⓕ.k线中线</strong></p><p id="ac8a" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">【0】</strong>桑乔伊·达斯古普塔、纳韦·弗罗斯特、米哈尔·莫什科维茨、赛勒斯·拉什奇安；<a class="ae mv" href="https://arxiv.org/abs/2002.12538" rel="noopener ugc nofollow" target="_blank">可解释的k-均值和k-中位数聚类</a></p><p id="7812" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">【1】</strong>大卫·多汉、斯特凡尼·卡普、布莱恩·马泰杰克；k-中值算法:<a class="ae mv" href="https://www.cs.princeton.edu/courses/archive/fall14/cos521/projects/kmedian.pdf" rel="noopener ugc nofollow" target="_blank">理论实践</a></p><p id="2542" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">【2】</strong>李，金华&amp;宋，史记&amp;张，郁莉&amp;周，甄。(2016).<a class="ae mv" href="https://www.researchgate.net/publication/311423522_Robust_K-Median_and_K-Means_Clustering_Algorithms_for_Incomplete_Data" rel="noopener ugc nofollow" target="_blank">针对不完整数据的鲁棒K-中值和K-均值聚类算法。</a>工程中的数学问题。2016.1–8.10.1155/2016/4321928.</p><p id="2f98" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⓖ.k模式</strong></p><p id="b4d9" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">【0】</strong>曾永贺。<a class="ae mv" href="https://arxiv.org/pdf/cs/0603120.pdf" rel="noopener ugc nofollow" target="_blank">K-模式聚类的近似算法</a></p><p id="df36" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">米盖尔·Á。魏冉·王·卡雷拉-佩皮尼昂。<a class="ae mv" href="https://arxiv.org/abs/1304.6478" rel="noopener ugc nofollow" target="_blank">聚类的K-modes算法</a></p><p id="9526" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">【2】</strong>黄哲雪。<a class="ae mv" href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.15.4028&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">k-Means算法的扩展，用于聚类具有分类值的大型数据集</a></p><p id="c8a7" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">【3】</strong>夏尔马，n .和n .高德。<a class="ae mv" href="https://www.semanticscholar.org/paper/K-modes-Clustering-Algorithm-for-Categorical-Data-Sharma-Gaud/289a15c11b28347665baea1261bd530a804bca4d" rel="noopener ugc nofollow" target="_blank">分类数据的K-modes聚类算法。</a><em class="ll">国际计算机应用杂志</em>127(2015):1–6。</p><p id="a475" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⓗ.</strong><strong class="lm ir">k-原型</strong></p><p id="5bfd" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">【0】【贾】、。(2020).<a class="ae mv" href="https://doi.org/10.1155/2020/5143797" rel="noopener ugc nofollow" target="_blank">基于混合相异系数的加权k原型聚类算法</a>。工程中的数学问题。欣达维。</p><p id="9e78" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">【1】</strong>黄哲雪。(1998).<a class="ae mv" href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.15.4028&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">对k-Means算法的扩展，用于聚类具有分类值的大型数据集</a>。数学和信息科学，邮政信箱664号，堪培拉，法案2601，澳大利亚。</p><p id="e309" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">【2】</strong>黄，哲人。"<a class="ae mv" href="https://www.semanticscholar.org/paper/CLUSTERING-LARGE-DATA-SETS-WITH-MIXED-NUMERIC-AND-Huang/d42bb5ad2d03be6d8fefa63d25d02c0711d19728" rel="noopener ugc nofollow" target="_blank">对具有混合数值和分类值的大型数据集进行聚类。</a>(1997)。</p><p id="1255" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">【3】</strong>Byoungwook Kim。(2017).使用部分距离计算的快速K原型算法。</p><p id="71e9" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⓘ.克拉拉和克拉拉恩斯</strong></p><p id="04b0" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">【0】</strong>Erich Schubert，Peter J. Rousseeuw:更快的k-Medoids聚类:改进PAM、CLARA和CLARANS算法。相似性搜索及其应用。SISAP 2019:171–187<a class="ae mv" href="https://doi.org/" rel="noopener ugc nofollow" target="_blank">https://doi.org/</a>10.1007/978–3–030–32047–8 _ 16</p><p id="5cec" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">【1】</strong>Ng、Raymond &amp;韩、佳伟。(2002).<a class="ae mv" href="https://www.researchgate.net/publication/3297085_CLARANS_A_method_for_clustering_objects_for_spatial_data_mining" rel="noopener ugc nofollow" target="_blank"> CLARANS:一种用于空间数据挖掘的对象聚类方法</a>。知识与数据工程。14.1003- 1016.10.1109/TKDE.2002</p><p id="b7c6" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">【2】</strong>维贾亚·萨格维卡尔，维迪亚·萨格维卡尔，卡尔帕娜·德鲁克卡尔。(2013).<a class="ae mv" href="https://www.longdom.org/articles/performance-assessment-of-clarans-a-method-for-clustering-objects-for-spatial-data-mining.pdf" rel="noopener ugc nofollow" target="_blank">CLARANS的性能评估:一种用于<br/>空间数据挖掘</a>的对象聚类方法。《普通高等教育发展条约》，第2卷第6期:第1-8页</p><p id="0d21" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">ⓙ GMM。</p><p id="53e5" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">【0】</strong>雷诺德(2009)高斯混合模型。载于:李世泽，贾恩(编)《生物识别百科全书》。马萨诸塞州波士顿斯普林格。<a class="ae mv" href="https://doi.org/10.1007/978-0-387-73003-5_196" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1007/978-0-387-73003-5_196</a></p><p id="7744" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">【1】</strong>笔记本:<a class="ae mv" href="http://ethen8181.github.io/machine-learning/clustering/GMM/GMM.html#Gaussian-Mixture-Model" rel="noopener ugc nofollow" target="_blank"> 1高斯混合模型</a></p><p id="6041" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">【2】</strong>笔记本:<a class="ae mv" href="https://gitlab.com/deep.TEACHING/educational-materials/blob/master/notebooks/graphical-models/directed/exercise-1d-gmm-em.ipynb" rel="noopener ugc nofollow" target="_blank">习题——1D高斯混合模型与期望最大化</a></p><p id="2c0c" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⓚ.数字万用表</strong></p><p id="5188" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">【0】</strong>博客:<a class="ae mv" href="https://luiarthur.github.io/TuringBnpBenchmarks/dpsbgmm" rel="noopener ugc nofollow" target="_blank">狄利克雷过程高斯混合模型在各种ppl中的破棒构造</a></p><p id="5656" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">【1】</strong>幻灯片:<a class="ae mv" href="https://www.slideserve.com/prescott-ellison/memoized-online-variational-inference-for-dirichlet-process-mixture-models" rel="noopener ugc nofollow" target="_blank">狄利克雷过程混合模型的记忆化在线变分推断</a></p><p id="124d" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">【2】</strong>博客:<a class="ae mv" href="http://blog.bogatron.net/blog/2014/02/02/visualizing-dirichlet-distributions/" rel="noopener ugc nofollow" target="_blank">用Matplotlib可视化狄利克雷分布</a></p><p id="71fc" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">【3】</strong>博客:Ritchie Vink，<a class="ae mv" href="https://www.ritchievink.com/blog/2018/06/05/clustering-data-with-dirichlet-mixtures-in-edward-and-pymc3/" rel="noopener ugc nofollow" target="_blank">用Edward和Pymc3中的Dirichlet混合对数据进行聚类</a>。</p><p id="9f82" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⓛ.数据库扫描</strong></p><p id="f7c3" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">迈克尔·哈斯勒，马修·皮肯布洛克，德里克·多兰。<a class="ae mv" href="https://cran.r-project.org/web/packages/dbscan/vignettes/dbscan.pdf" rel="noopener ugc nofollow" target="_blank"> dbscan:基于密度的快速R聚类</a></p><p id="02ac" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">埃里希·舒伯特、约格·桑德、马丁·伊斯特、汉斯-彼得·克里格尔和徐小薇。2017.DBSCAN重温，重温:为什么和如何你应该(仍然)使用DBSCAN。ACM Trans数据库系统。42、3、第十九条(2017年7月)，21页。<a class="ae mv" href="https://doi.org/10.1145/3068335" rel="noopener ugc nofollow" target="_blank">https://doi.org/10.1145/3068335</a></p><p id="2758" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"> ⓜ.</strong> <strong class="lm ir"> DENCLUE </strong></p><p id="7928" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">欣内堡，a .和h .加布里埃尔。"基于核密度估计的快速聚类."国际开发协会(IDA)(2007)。</p><p id="0e93" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated">ⓝ.光学</p><p id="f47b" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir">【0】</strong>安克斯特，米哈尔&amp;布留尼格，马库斯&amp;克里格尔，汉斯-彼得&amp;桑德，约尔格。(1999).<a class="ae mv" href="https://www.researchgate.net/publication/221214752_OPTICS_Ordering_Points_to_Identify_the_Clustering_Structure" rel="noopener ugc nofollow" target="_blank">光学:对点进行排序，以识别聚类结构。</a>西格蒙德记录。28.49–60.10.1145/304182.304187.</p><p id="749d" class="pw-post-body-paragraph lj lk iq lm b ln mf jr lp lq mg ju ls nd mh lv lw nh mi lz ma nl mj md me lf ij bi translated"><strong class="lm ir"/>幻灯片:<a class="ae mv" href="https://www.slideshare.net/rpiryani/optics-ordering-points-to-identify-the-clustering-structure" rel="noopener ugc nofollow" target="_blank">光学排序点识别聚类结构</a></p></div></div>    
</body>
</html>