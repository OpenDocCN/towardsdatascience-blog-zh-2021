<html>
<head>
<title>How-to Build a Transformer Tokenizer</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何构建一个Transformer标记器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/transformers-from-scratch-creating-a-tokenizer-7d7418adb403?source=collection_archive---------8-----------------------#2021-06-24">https://towardsdatascience.com/transformers-from-scratch-creating-a-tokenizer-7d7418adb403?source=collection_archive---------8-----------------------#2021-06-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="2b03" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">你所需要的就是使用高频变压器创建一个定制的记号赋予器</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d8a1cecde4e332d51663701154cf56cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bOSSuJoKB5DU578tVIOU4g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="8c51" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi lu translated"><span class="l lv lw lx bm ly lz ma mb mc di">一个</span>几乎每个自然语言处理(NLP)项目都是这样开始的:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="md me l"/></div></figure><p id="d098" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当然，总会有一些复杂的情况。也许数据具有您(或其他任何人)以前从未见过的奇怪属性，并且使数据成为预处理的噩梦——但是对于模型设置，我们通常可以使用现有的预训练模型。</p><p id="f19d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这很好，但是如果没有符合我们特定要求的预训练模型呢？</p><p id="e004" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">也许我们希望我们的模型能够理解一种不太常见的语言，例如，有多少transformer模型已经接受了皮埃蒙特语或纳瓦特尔语的训练？</p><p id="02c2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="mf">*零点* </em></p><p id="a56d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在这种情况下，我们需要做一些不同的事情。我们需要从头开始构建自己的模型。</p><p id="40a0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，构建新的transformer模型背后的很大一部分工作是创建新的模型标记器。</p><p id="4dd2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">记号赋予器是我们从人类可读文本到转换器可读记号的翻译器。在本文中，我们将学习如何构建我们自己的transformer tokenizer。</p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><h1 id="b880" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">获取数据</h1><p id="767a" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">像往常一样，我们从数据开始。关于transformer的最大好处之一是我们的选择太多了——我们构建我们的标记器和预训练transformer模型所需要的只是非结构化的文本数据。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nk me l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">多语言数据的HuggingFace数据集的视频漫游</p></figure><p id="4e02" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">最大的非结构化文本数据集之一是OSCAR，碰巧这是一个巨大的多语言数据集。现在，我们可以通过HF <code class="fe nl nm nn no b">datasets</code>包直接访问这个数据集，让我们来看看。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="md me l"/></div></figure><p id="a40a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，我们只能通过Python列出数据集的名称——这不是很多信息。为了获得更多细节，我们可以前往HF <a class="ae np" href="https://huggingface.co/datasets/viewer/" rel="noopener ugc nofollow" target="_blank">数据集查看器</a>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nq"><img src="../Images/8e259097544ba0c9a40bb2aed77f7389.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*G1NwHu_j40T_-ldjGD_n7g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">左侧窗口允许我们使用过滤器和/或通过指定确切的数据集进行搜索。</p></figure><p id="9556" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">选择<strong class="la iu"> oscar </strong>后，我们将看到另一个下拉选项— <strong class="la iu">子集</strong> —它列出了oscar中所有可用的不同子集(如语言)。</p><p id="bb27" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以通过选择<strong class="la iu">unshuffled _ duplicated _ PMS</strong>子集来预览OSCAR Piemontese数据。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/c6acc52ea1d2764cc2c0b6e05a93768a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zDIZ3HCiIOZBw4mExYaBPQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">奥斯卡皮埃蒙特子集预览。</p></figure><p id="ad92" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将继续使用<code class="fe nl nm nn no b">datasets.load_dataset(<em class="mf">'dataset'</em>, <em class="mf">'subset'</em>)</code>用Python下载这个数据集。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="md me l"/></div></figure><p id="4402" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从这里我们可以看到拉丁子集包含18.8K个样本，其中每个样本都是一个字典，包含一个<em class="mf"> id </em>和<em class="mf">文本</em>。</p><p id="a896" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在训练我们的标记器时，我们需要从文件中读取数据——我们将在纯文本文件中存储所有样本，用换行符分隔每个样本。</p><p id="4ba8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们将把每个文本文件分成5K个样本的块(尽管对于这种大小的数据集来说没有必要，但对于大型数据集来说是必需的)，并将它们保存到一个新的<em class="mf"> oscar_la </em>目录中。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="md me l"/></div></figure></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><h1 id="ec83" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">训练分词器</h1><p id="230e" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">现在我们的数据已经准备好了，我们可以开始训练标记器了。我们将使用字节级字节对编码(BPE)记号赋予器。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nk me l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">标记化器构建的视频演练</p></figure><p id="e196" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">字节级编码意味着我们将从字节字母表中构建我们的记号化器词汇表。由于这个原因，所有的单词都可以分解成记号——甚至是新单词——所以我们不需要特殊的未知记号。</p><p id="a655" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们需要一个文件列表来输入我们的分词器的训练过程，我们将列出所有的<em class="mf">。我们的<em class="mf"> oscar_la </em>目录中的txt </em>文件。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="md me l"/></div></figure><p id="cc3b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在我们初始化并训练我们的记号赋予器。我们将使用roBERTa特殊标记，一个词汇大小为<code class="fe nl nm nn no b">30522</code>的标记，以及一个最小频率(一个标记在数据中出现的次数，以引起我们的注意)<code class="fe nl nm nn no b">2</code>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="md me l"/></div></figure><p id="d63e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们现在已经构建了我们的拉丁文roBERTa tokenizer。让我们给它起一个听起来像拉丁语的名字——Bert ius——并保存到文件中。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="md me l"/></div></figure><h2 id="698a" class="ns mo it bd mp nt nu dn mt nv nw dp mx lh nx ny mz ll nz oa nb lp ob oc nd od bi translated">记号化器文件</h2><p id="459a" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">保存我们的记号赋予器创建了两个文件，一个<code class="fe nl nm nn no b">merges.txt</code>和<code class="fe nl nm nn no b">vocab.json</code>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/04e466c8072bacce524a97e2562d3af9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OQcKRz0V46SBCa3ELHtEKw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">两个标记器文件— <strong class="bd oe"> merges.txt </strong>和<strong class="bd oe"> vocab.json </strong>。</p></figure><p id="9aa2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当我们的记号赋予器编码文本时，它将首先使用<code class="fe nl nm nn no b">merges.txt</code>将文本映射到记号，然后使用<code class="fe nl nm nn no b">vocab.json</code>将记号映射到记号id。</p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><h1 id="b41d" class="mn mo it bd mp mq mr ms mt mu mv mw mx jz my ka mz kc na kd nb kf nc kg nd ne bi translated">使用分词器</h1><p id="fd22" class="pw-post-body-paragraph ky kz it la b lb nf ju ld le ng jx lg lh nh lj lk ll ni ln lo lp nj lr ls lt im bi translated">我们已经构建并保存了我们的标记器——但是我们如何使用它呢？嗯，我们可以像加载其他标记器一样加载我们的标记器——使用<code class="fe nl nm nn no b">from_pretrained</code>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="md me l"/></div></figure><p id="37a3" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在让我们试着编码每个人最喜欢的拉丁短语:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="md me l"/></div></figure><p id="b4ca" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里可以看到我们的两个张量，<code class="fe nl nm nn no b">input_ids</code>和<code class="fe nl nm nn no b">attention_mask</code>。在<code class="fe nl nm nn no b">input_ids</code>中，我们可以看到由<strong class="la iu"> 0 </strong>表示的序列令牌<strong class="la iu">s&gt;T21，由<strong class="la iu"> 2 </strong>表示的序列令牌<strong class="la iu">s \&gt;</strong>结束，以及由<strong class="la iu"> 1 </strong>表示的填充令牌<strong class="la iu"> &lt; pad &gt; </strong>。</strong></p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><p id="ecdb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">太好了！我们在巨大的OSCAR数据集的拉丁子集上组装了一个定制的标记器。本文到此为止！</p><p id="8c6b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我希望你喜欢这篇文章！如果你有任何问题，请通过<a class="ae np" href="https://twitter.com/jamescalam" rel="noopener ugc nofollow" target="_blank"> Twitter </a>或在下面的评论中告诉我。如果你想要更多这样的内容，我也会在<a class="ae np" href="https://www.youtube.com/c/jamesbriggs" rel="noopener ugc nofollow" target="_blank"> YouTube </a>上发布。</p><p id="387c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢阅读！</p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><p id="89c7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae np" href="https://bit.ly/nlp-transformers" rel="noopener ugc nofollow" target="_blank">🤖《变形金刚》NLP课程70%的折扣</a></p></div><div class="ab cl mg mh hx mi" role="separator"><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml mm"/><span class="mj bw bk mk ml"/></div><div class="im in io ip iq"><p id="9cbb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="mf">*除另有说明外，所有图片均出自作者之手</em></p></div></div>    
</body>
</html>