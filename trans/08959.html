<html>
<head>
<title>An Overview of Boosting Methods: CatBoost, XGBoost, AdaBoost, LightBoost, Histogram-Based Gradient Boost</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">增强方法概述:CatBoost、XGBoost、AdaBoost、LightBoost、基于直方图的梯度增强</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-overview-of-boosting-methods-catboost-xgboost-adaboost-lightboost-histogram-based-gradient-407447633ac1?source=collection_archive---------22-----------------------#2021-08-18">https://towardsdatascience.com/an-overview-of-boosting-methods-catboost-xgboost-adaboost-lightboost-histogram-based-gradient-407447633ac1?source=collection_archive---------22-----------------------#2021-08-18</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="e024" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">用python实现在一个视图中编译所有boosting方法</h2></div><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="961a" class="kr ks it kn b gy kt ku l kv kw"><strong class="kn iu"><em class="kx">Table of Contents</em></strong><em class="kx"> <br/></em><strong class="kn iu">1. Introduction<br/>2. AdaBoost<br/>3. Gradient Boost<br/>3.1. XGBoost<br/>3.2. Histogram-Based Gradient Boost<br/>3.3. LightBoost<br/>3.4. CatBoost<br/>4. Summary</strong></span></pre><figure class="ki kj kk kl gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi ky"><img src="../Images/d1ba294376789490c9ee619bfc252849.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*SNtI3V1jsk-w-GYn"/></div></div><p class="lg lh gj gh gi li lj bd b be z dk translated">扎克·赖纳在<a class="ae lk" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="ed43" class="ll ks it bd lm ln lo lp lq lr ls lt lu jz lv ka lw kc lx kd ly kf lz kg ma mb bi translated">1.介绍</h1><p id="e7cf" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw mx im bi translated">在集成学习中，目标是用多种学习算法最成功地训练模型。在集成学习之一Bagging方法中，多个模型被并行应用于同一数据集的不同子样本。Boosting是另一种方法，在实践中经常使用，它按顺序而不是并行建立，旨在训练算法和训练模型。弱算法训练模型，然后根据训练结果重新组织模型，使模型更容易学习。然后，这个修改后的模型被发送到下一个算法，第二个算法比第一个算法更容易学习。本文包含不同的增强方法，从不同的角度解释了这种顺序方法。</p><h1 id="e9e1" class="ll ks it bd lm ln lo lp lq lr ls lt lu jz lv ka lw kc lx kd ly kf lz kg ma mb bi translated"><strong class="ak"> 2。AdaBoost </strong></h1><p id="2524" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw mx im bi translated">自适应增强(Adaboost)是一种广泛使用的基于决策树桩<em class="kx">的增强方法(决策树桩:分配阈值，根据阈值进行预测。)</em>。但是这种方法在Adaboost中并不是盲目重复的。构建了多种算法，这些算法依次更新它们的权重，并在做出最准确的估计时扮演单独的角色。在每个算法中计算错误率。权重被更新，因此被引用到第二算法。第二种算法对模型进行分类，像第一种模型一样更新权重，并将其转移到第三种算法。这些过程一直持续到n个估计量或达到误差=0。在此过程中，该算法使分类更容易和更成功，因为权重由前一算法更新并发送到另一算法。让我们用一个例子来解释这个复杂的顺序算法过程:</p><p id="cb19" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mx im bi translated">假设有两个标签，<em class="kx">红色</em>和<em class="kx">蓝色</em>。第一种算法(弱分类器1)分离标签，结果是2个蓝色样本和1个红色样本被错误分类。<strong class="me iu">增加这些错误分类的权重</strong>，之后降低正确分类的权重，发送到下一个模型学习。与以前的算法相比，新模型学习起来更容易，在新模型中，以前的模型错误分类的样本具有增加的偏差，而正确分类的样本具有减少的偏差。在接下来的步骤中重复相同的过程。总之，强分类是在弱分类的配合下发生的。因为它用于分类，所以也可以通过导入<em class="kx"> AdaBoostRegressor </em>用于回归。</p><p id="f210" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mx im bi translated"><strong class="me iu">超参数</strong></p><p id="88e7" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mx im bi translated">所有超参数均可在sklearn <a class="ae lk" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html" rel="noopener ugc nofollow" target="_blank">网站</a>上获得。总结一下:</p><ul class=""><li id="d321" class="nd ne it me b mf my mi mz ml nf mp ng mt nh mx ni nj nk nl bi translated"><strong class="me iu"><em class="kx">base _ estimators:</em></strong>一种顺序改进的算法(默认为决策树分类器)</li><li id="25e2" class="nd ne it me b mf nm mi nn ml no mp np mt nq mx ni nj nk nl bi translated"><strong class="me iu"><em class="kx">n _ estimators:</em></strong>确定上述过程将采取的最大步骤数。(默认值=50)</li><li id="c475" class="nd ne it me b mf nm mi nn ml no mp np mt nq mx ni nj nk nl bi translated"><strong class="me iu"> <em class="kx">学习_速率:</em> </strong>决定了权重的变化量。如果选择过小，n_estimators值必须很高。如果选择得太大，它可能永远达不到最佳值。(默认值=1)</li></ul><figure class="ki kj kk kl gt kz"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="73cd" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mx im bi translated">导入了将用于所有方法的数据集，现在让我们实现Adaboost:</p><figure class="ki kj kk kl gt kz"><div class="bz fp l di"><div class="nr ns l"/></div></figure><h1 id="3c74" class="ll ks it bd lm ln lo lp lq lr ls lt lu jz lv ka lw kc lx kd ly kf lz kg ma mb bi translated">3.梯度增强</h1><p id="5434" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw mx im bi translated">Adaboost通过用决策树桩(1个节点分成2片叶子)更新权重来改进自己。梯度增强，另一种顺序方法，通过创建8到32片叶子来优化损失，这意味着树在梯度增强中更大<em class="kx">(损失:记住线性模型中的残差。(y_test-y_prediction)给出残差，每个数据点的平方和给出损失。为什么应用正方形？因为我们寻找的值是预测与实际结果的偏差。负值被平方，因为它们将使损失值变小，即使它偏离)</em>。简而言之，通过使残差值更接近0，残差值被转移到下一个算法，从而最小化损失值。</p><figure class="ki kj kk kl gt kz"><div class="bz fp l di"><div class="nr ns l"/></div></figure><p id="01d8" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mx im bi translated">与Adaboost一样，梯度增强可通过导入<em class="kx"> GradientBoostRegressor </em>用于回归。</p><figure class="ki kj kk kl gt kz"><div class="bz fp l di"><div class="nr ns l"/></div></figure><pre class="ki kj kk kl gt km kn ko kp aw kq bi"><span id="c448" class="kr ks it kn b gy kt ku l kv kw">OUT</span><span id="ce75" class="kr ks it kn b gy nt ku l kv kw">[0.22686585533221332,0.20713350861706786,0.1900682640534445,<br/> 0.1761959477525979,0.16430532532798403,0.1540494010479854,<br/> 0.14517117541343785,0.1375952312491854,0.130929810958826,<br/> 0.12499605002264891,0.1193395594019215,0.11477096339545599,<br/> 0.11067921343289967,0.10692446632551068,...................<br/> ...........................................................<br/> 0.05488031632425609,0.05484366975329703,0.05480676108875857,<br/> 0.054808073418709524,0.054740333154284,0.05460221966859833,<br/> 0.05456647041868937,0.054489873127848434,0.054376259548495065,<br/> 0.0542407250628274]</span></pre><p id="3df1" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mx im bi translated">查看Error_list，可以看到每一步损失值都在变小。[从0.22开始，到0.05结束]</p><h2 id="bca6" class="kr ks it bd lm nu nv dn lq nw nx dp lu ml ny nz lw mp oa ob ly mt oc od ma oe bi translated">3.1.XGBoost</h2><p id="f48e" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw mx im bi translated">XGBoost(代表极端梯度增强)最初是由陈天琦在2014年开发的，在梯度增强之前速度快得多，因此它是首选的增强方法。由于它包含超参数，可以进行许多调整，如正则化超参数，以防止过度拟合。</p><p id="4830" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mx im bi translated"><strong class="me iu">超参数</strong></p><ul class=""><li id="8bbc" class="nd ne it me b mf my mi mz ml nf mp ng mt nh mx ni nj nk nl bi translated"><strong class="me iu"><em class="kx">learning _ rate&amp;eta:</em></strong>乘以权重值使用。其目的是通过安排特征权重使过程更保守。默认值=0.3，通常使用0.01–0.3。</li><li id="b97f" class="nd ne it me b mf nm mi nn ml no mp np mt nq mx ni nj nk nl bi translated"><strong class="me iu"> <em class="kx"> max_depth: </em> </strong>树的最大深度。如果该值增加，模型可能会过度拟合。默认值=6且仅当<code class="fe of og oh kn b">growing_policy=lossguide</code>、<code class="fe of og oh kn b">max_value=0</code>时。请查看增长政策。</li><li id="2ff5" class="nd ne it me b mf nm mi nn ml no mp np mt nq mx ni nj nk nl bi translated"><strong class="me iu"><em class="kx">n _ estimators:</em></strong>集合树的数量</li><li id="681b" class="nd ne it me b mf nm mi nn ml no mp np mt nq mx ni nj nk nl bi translated"><strong class="me iu"> <em class="kx"> alpha: </em> </strong> L1正则化在权重上。鼓励小重量。默认值=1。</li><li id="c1d2" class="nd ne it me b mf nm mi nn ml no mp np mt nq mx ni nj nk nl bi translated"><strong class="me iu"> <em class="kx"> gamma: </em> </strong>正则化复杂度的超参数之一，在一片叶子中发生分裂所必需的最小损失。默认值=0，较大的灰度系数使模型更加保守。</li><li id="d22b" class="nd ne it me b mf nm mi nn ml no mp np mt nq mx ni nj nk nl bi translated"><strong class="me iu"><em class="kx">λ:</em></strong>L2正则化在权数上。鼓励小重量。默认值=1。</li><li id="ee20" class="nd ne it me b mf nm mi nn ml no mp np mt nq mx ni nj nk nl bi translated"><strong class="me iu"> <em class="kx">子样本:</em> </strong>将训练多少个数据样本？默认值=1表示100%百分比，如果设置为0.5，则随机选择50%的数据。</li><li id="12a1" class="nd ne it me b mf nm mi nn ml no mp np mt nq mx ni nj nk nl bi translated"><strong class="me iu"><em class="kx">col sample _ bytree:</em></strong>构造每棵树时列的子采样比。默认值=1。对于每个构建的树，进行一次子采样。</li><li id="865c" class="nd ne it me b mf nm mi nn ml no mp np mt nq mx ni nj nk nl bi translated"><strong class="me iu"><em class="kx">col sample _ by level:</em></strong>每个级别的列的子样本比率。默认值=1。对于树中达到的每个新的深度级别，进行一次子采样。从为当前树选择的列集中对列进行子采样。</li><li id="1323" class="nd ne it me b mf nm mi nn ml no mp np mt nq mx ni nj nk nl bi translated"><strong class="me iu"><em class="kx">col sample _ node:</em></strong>每个节点列的子样比率。每次评估新的分割时，进行一次子采样。从为当前级别选择的列集中对列进行子采样。</li><li id="900d" class="nd ne it me b mf nm mi nn ml no mp np mt nq mx ni nj nk nl bi translated"><strong class="me iu"><em class="kx">min _ child _ weight:</em></strong>用于控制过拟合。过高的值会导致拟合不足。默认值=1。</li><li id="c1b2" class="nd ne it me b mf nm mi nn ml no mp np mt nq mx ni nj nk nl bi translated"><strong class="me iu"> <em class="kx"> grow_policy: </em> </strong>控制新节点添加到树中的方式。在LGBM发布后，它被添加到XGBoost中。由于LGBM的高速度(由于wise-leaf)，它被添加到XGBoost与wise-leaf的工作中。为了激活它，<code class="fe of og oh kn b">grow_policy=lossguide</code>，默认=depthwise</li><li id="376d" class="nd ne it me b mf nm mi nn ml no mp np mt nq mx ni nj nk nl bi translated"><strong class="me iu"> <em class="kx">目标:</em> </strong>指定学习任务。<em class="kx">‘regsquarederror’</em>:平方损失回归；<em class="kx">‘reg logist’</em>:logistic regression等。</li><li id="cda3" class="nd ne it me b mf nm mi nn ml no mp np mt nq mx ni nj nk nl bi translated"><strong class="me iu"><em class="kx">eval _ metric:</em></strong>test _ data(validation _ data)的评估度量。<em class="kx">‘RMSE’</em>:均方根误差；<em class="kx">‘Mae’</em>:平均绝对误差，<em class="kx">‘mape’</em>:平均绝对百分比误差。</li></ul><p id="0fbe" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mx im bi translated">更可在此<a class="ae lk" href="https://xgboost.readthedocs.io/en/latest/parameter.html" rel="noopener ugc nofollow" target="_blank"> <strong class="me iu">链接</strong> </a> <strong class="me iu">。</strong></p><figure class="ki kj kk kl gt kz"><div class="bz fp l di"><div class="nr ns l"/></div></figure><h2 id="6377" class="kr ks it bd lm nu nv dn lq nw nx dp lu ml ny nz lw mp oa ob ly mt oc od ma oe bi translated">3.2.基于直方图的梯度增强</h2><p id="c5b6" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw mx im bi translated">用宁滨(离散化)对数据进行分组，这是一种数据预处理方法，这里<a class="ae lk" rel="noopener" target="_blank" href="/an-overview-of-data-preprocessing-features-enrichment-automatic-feature-selection-60b0c12d75ad"><strong class="me iu"/></a><strong class="me iu">已经解释过了。</strong>例如，当给定‘年龄’列时，将这些数据分为30–40、40–50、50–60三组，然后将其转换为数值数据，这是一种非常有效的方法。当这种宁滨方法适用于决策树时，通过减少特征的数量，它加快了算法的速度。这种方法也可以通过用直方图对树进行分组而在每个树的构造中用作集成。scikit学习库提供的方法:</p><figure class="ki kj kk kl gt kz"><div class="bz fp l di"><div class="nr ns l"/></div></figure><h2 id="9d11" class="kr ks it bd lm nu nv dn lq nw nx dp lu ml ny nz lw mp oa ob ly mt oc od ma oe bi translated">3.3.光增强</h2><p id="d723" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw mx im bi translated">LGBM(代表轻度梯度增强机器)最初由微软在2017年发布，是用户首选的另一种梯度增强方法，并且是基于决策树的。与其他方法的关键区别在于，它基于树叶对树进行拆分，即可以检测和停止点拍所需的单元(记住:其他的是基于深度或基于级别的)。由于LGBM是基于叶子的，如图2所示，LGBM是一种非常有效的方法，可以减少误差，从而提高精度和速度。您可以使用特殊算法拆分分类数据，但是必须输入一个整数值，如index，而不是列的字符串名称。</p><figure class="ki kj kk kl gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi oi"><img src="../Images/b5f3da052093371df91d0ed7a6c4fbb4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3JP7lDKx4mE5ZUUd9nIR6g.jpeg"/></div></div><p class="lg lh gj gh gi li lj bd b be z dk translated">图二。基于层与基于叶，按作者排序的图像</p></figure><figure class="ki kj kk kl gt kz"><div class="bz fp l di"><div class="nr ns l"/></div></figure><h2 id="531c" class="kr ks it bd lm nu nv dn lq nw nx dp lu ml ny nz lw mp oa ob ly mt oc od ma oe bi translated">3.4.CatBoost</h2><p id="90be" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw mx im bi translated">CatBoost是Yandex在2017年开发的。由于它通过一次热编码将所有分类特征转换为数字，因此其名称来自于<strong class="me iu">Cat</strong>egorical<strong class="me iu">Boost</strong>ing。应该输入索引值，而不是but列的字符串名称。它还处理缺失的数值。它也比XGBoost快得多。与其他增强方法不同，Catboost与<strong class="me iu">对称树</strong>不同，后者在每一层的节点中使用相同的分裂。</p><p id="f24f" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mx im bi translated">XGBoost和LGBM计算每个数据点的残差，并将模型训练为具有残差目标值。它重复这个迭代次数，从而训练和降低残余误差，达到目标。由于这种方法应用于每个数据点，它可能在泛化方面较弱，并导致过拟合。</p><p id="0d24" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mx im bi translated">Catboost还计算每个数据点的残差，并对用其他数据训练的模型执行此操作。以这种方式，为每个数据点获得不同的残差数据。将这些数据作为目标进行评估，并根据迭代次数训练通用模型。由于许多模型将通过定义来实现，这种计算复杂性看起来真的很昂贵，并且花费太多时间。但是在有序升压的情况下，它在更短的时间内完成。<strong class="me iu">有序提升</strong>，而不是从第(n+1)个数据点计算的残差开始。就是指数据点的残差。为了计算第(n+2)个数据点，应用第(n+1)个数据点，等等。</p><p id="2d8d" class="pw-post-body-paragraph mc md it me b mf my ju mh mi mz jx mk ml na mn mo mp nb mr ms mt nc mv mw mx im bi translated"><strong class="me iu">超参数</strong></p><ul class=""><li id="8fde" class="nd ne it me b mf my mi mz ml nf mp ng mt nh mx ni nj nk nl bi translated"><strong class="me iu"> <em class="kx"> l2_leaf_reg: </em> </strong> L2代价函数的正则化项。</li><li id="504e" class="nd ne it me b mf nm mi nn ml no mp np mt nq mx ni nj nk nl bi translated"><strong class="me iu"> <em class="kx">学习_速率:</em> </strong>渐变步长。在过度拟合的情况下降低学习率。</li><li id="ee80" class="nd ne it me b mf nm mi nn ml no mp np mt nq mx ni nj nk nl bi translated"><strong class="me iu"> <em class="kx">深度:</em> </strong>树的深度，大多用在6-10之间。</li><li id="1a0f" class="nd ne it me b mf nm mi nn ml no mp np mt nq mx ni nj nk nl bi translated"><strong class="me iu"><em class="kx">one _ hot _ max _ size:</em></strong>用小于或等于给定参数值的几个不同值对所有分类特征进行one-hot编码</li><li id="c1c2" class="nd ne it me b mf nm mi nn ml no mp np mt nq mx ni nj nk nl bi translated"><strong class="me iu"> <em class="kx"> grow_policy: </em> </strong>决定建筑类型的树。可以使用SymmetricTree、Depthwise或Lossguide。</li></ul><figure class="ki kj kk kl gt kz"><div class="bz fp l di"><div class="nr ns l"/></div></figure><h1 id="05f3" class="ll ks it bd lm ln lo lp lq lr ls lt lu jz lv ka lw kc lx kd ly kf lz kg ma mb bi translated">4.摘要</h1><p id="89ee" class="pw-post-body-paragraph mc md it me b mf mg ju mh mi mj jx mk ml mm mn mo mp mq mr ms mt mu mv mw mx im bi translated">在这篇文章中，boosting方法是通过决策树来实现的，但是通过改变相关的超参数，可以很容易地实现其他机器学习模型。此外，所有boosting方法都应用了基本版本(未调整任何超参数)来比较boosting方法的性能，上面应用的代码列表如下:</p><figure class="ki kj kk kl gt kz gh gi paragraph-image"><div role="button" tabindex="0" class="la lb di lc bf ld"><div class="gh gi oj"><img src="../Images/de634d674753f0c185658ab36171fb7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AYuBghholOUCnhyhIZIwVw.png"/></div></div><p class="lg lh gj gh gi li lj bd b be z dk translated">增强方法的比较，图片由作者提供</p></figure><blockquote class="ok ol om"><p id="6527" class="mc md kx me b mf my ju mh mi mz jx mk on na mn mo oo nb mr ms op nc mv mw mx im bi translated">在以后的文章中，我们将通过优化超参数来更深入地探讨每种提升方法。</p></blockquote><h1 id="49e3" class="ll ks it bd lm ln lo lp lq lr ls lt lu jz lv ka lw kc lx kd ly kf lz kg ma mb bi translated">参考</h1><ul class=""><li id="f828" class="nd ne it me b mf mg mi mj ml oq mp or mt os mx ni nj nk nl bi translated"><a class="ae lk" href="https://stackoverflow.com/questions/50087526/what-is-the-intuition-behind-symmetric-trees-in-catboost-algorithm" rel="noopener ugc nofollow" target="_blank">https://stack overflow . com/questions/50087526/catboost算法中对称树背后的直觉是什么</a></li><li id="5b8c" class="nd ne it me b mf nm mi nn ml no mp np mt nq mx ni nj nk nl bi translated"><a class="ae lk" href="https://catboost.ai/docs/concepts/python-reference_parameters-list.html" rel="noopener ugc nofollow" target="_blank">https://catboost . ai/docs/concepts/python-reference _ parameters-list . html</a></li><li id="6ab3" class="nd ne it me b mf nm mi nn ml no mp np mt nq mx ni nj nk nl bi translated"><a class="ae lk" href="https://csiss.org/SPACE/workshops/2004/SAC/files/fisher.pdf" rel="noopener ugc nofollow" target="_blank">https://csiss.org/SPACE/workshops/2004/SAC/files/fisher.pdf</a></li><li id="1c32" class="nd ne it me b mf nm mi nn ml no mp np mt nq mx ni nj nk nl bi translated">【https://xgboost.readthedocs.io/en/latest/parameter.html T2】号</li></ul></div><div class="ab cl ot ou hx ov" role="separator"><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy oz"/><span class="ow bw bk ox oy"/></div><div class="im in io ip iq"><h2 id="3f8d" class="kr ks it bd lm nu nv dn lq nw nx dp lu ml ny nz lw mp oa ob ly mt oc od ma oe bi translated">回到指引点击<a class="ae lk" href="https://ibrahimkovan.medium.com/machine-learning-guideline-959da5c6f73d" rel="noopener">这里</a>。</h2><div class="pa pb gp gr pc pd"><a href="https://ibrahimkovan.medium.com/machine-learning-guideline-959da5c6f73d" rel="noopener follow" target="_blank"><div class="pe ab fo"><div class="pf ab pg cl cj ph"><h2 class="bd iu gy z fp pi fr fs pj fu fw is bi translated">机器学习指南</h2><div class="pk l"><h3 class="bd b gy z fp pi fr fs pj fu fw dk translated">本文旨在准备一个机器学习数据库，以便在一个视图中显示所有的机器学习标题。这个…</h3></div><div class="pl l"><p class="bd b dl z fp pi fr fs pj fu fw dk translated">ibrahimkovan.medium.com</p></div></div><div class="pm l"><div class="pn l po pp pq pm pr le pd"/></div></div></a></div></div></div>    
</body>
</html>