<html>
<head>
<title>Hands-On Reinforcement Learning Course: Part 3</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">实践强化学习课程:第 3 部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hands-on-reinforcement-learning-course-part-3-5db40e7938d4?source=collection_archive---------9-----------------------#2021-12-17">https://towardsdatascience.com/hands-on-reinforcement-learning-course-part-3-5db40e7938d4?source=collection_archive---------9-----------------------#2021-12-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="b608" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">表格 SARSA</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5033b9e6aca6d475094ebed424d8d8a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8YWaRRQLC3jGe-2sEZ-GFg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">贝尔格莱德萨瓦马拉(图片由作者提供)</p></figure><h1 id="53d8" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">欢迎来到我的强化学习课程❤️</h1><p id="6bf8" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">这是我强化学习实践课程的第三部分，带你从零到英雄🦸‍♂️.</p><p id="f959" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">我们仍处于旅程的起点，解决相对容易的问题。</p><p id="bfc2" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">在<a class="ae mr" rel="noopener" target="_blank" href="/hands-on-reinforcement-learning-course-part-2-1b0828a1046b">第 2 部分</a>中，我们实现了离散 Q 学习来训练<code class="fe ms mt mu mv b">Taxi-v3</code>环境中的代理。</p><p id="9241" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">今天，我们将进一步解决<code class="fe ms mt mu mv b">MountainCar</code>环境问题🚃使用 SARSA 算法。</p><p id="50ea" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">让我们帮助这辆可怜的车赢得对抗地心引力的战斗！</p><p id="5ec9" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">本课所有代码都在<a class="ae mr" href="https://github.com/Paulescu/hands-on-rl" rel="noopener ugc nofollow" target="_blank"> <strong class="ls iu">本 Github repo </strong> </a> <strong class="ls iu">中。</strong> Git 克隆它，跟随今天的问题。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><a href="https://github.com/Paulescu/hands-on-rl"><div class="gh gi mw"><img src="../Images/4d79edee15b30416e90a9e266fe6847e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*9x8msVghom_em5vn.jpeg"/></div></a></figure><h1 id="04a4" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">第三部分</h1><h1 id="fc37" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">内容</h1><ol class=""><li id="005b" class="mx my it ls b lt lu lw lx lz mz md na mh nb ml nc nd ne nf bi translated">山地汽车问题🚃</li><li id="0f9d" class="mx my it ls b lt ng lw nh lz ni md nj mh nk ml nc nd ne nf bi translated">环境、行动、状态、奖励</li><li id="b7e8" class="mx my it ls b lt ng lw nh lz ni md nj mh nk ml nc nd ne nf bi translated">随机代理基线🚃🍷</li><li id="5c7a" class="mx my it ls b lt ng lw nh lz ni md nj mh nk ml nc nd ne nf bi translated">萨尔萨特工🚃🧠</li><li id="1cd3" class="mx my it ls b lt ng lw nh lz ni md nj mh nk ml nc nd ne nf bi translated">停下来喘口气，⏸🧘</li><li id="0525" class="mx my it ls b lt ng lw nh lz ni md nj mh nk ml nc nd ne nf bi translated">重述✨</li><li id="5021" class="mx my it ls b lt ng lw nh lz ni md nj mh nk ml nc nd ne nf bi translated">家庭作业📚</li><li id="77b9" class="mx my it ls b lt ng lw nh lz ni md nj mh nk ml nc nd ne nf bi translated">下一步是什么？❤️</li></ol><h1 id="3b63" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">1.山地汽车问题🚃</h1><p id="5ee8" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">山地汽车问题是一个重力存在的环境(多么令人惊讶)，目标是帮助一辆可怜的汽车赢得与它的战斗。</p><p id="f6fc" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">汽车需要逃离它被困的山谷。这辆车的引擎不够强大，无法单程爬上山，所以唯一的办法就是来回行驶，建立足够的动力。</p><p id="4c03" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">让我们来看看它的实际应用:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="bd6b" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">你刚刚看到的视频对应于我们今天要建造的<code class="fe ms mt mu mv b">SarsaAgent</code>。</p><p id="a35c" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">很有趣，不是吗？</p><p id="2eb8" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">你可能想知道。<em class="nn">这个看起来很酷，但是当初为什么会选择这个问题呢？</em></p><h2 id="28ab" class="no kz it bd la np nq dn le nr ns dp li lz nt nu lk md nv nw lm mh nx ny lo nz bi translated">为什么会有这个问题？</h2><p id="add9" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">本课程的理念是逐步增加复杂性。循序渐进。</p><p id="0db4" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">与第 2 部分中的<code class="fe ms mt mu mv b">Taxi-v3</code>环境相比，今天的环境在复杂性方面有了微小但相关的增加。</p><p id="d156" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">但是，<em class="nn">这里到底什么更难？</em></p><p id="7b68" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">正如我们在<a class="ae mr" rel="noopener" target="_blank" href="/hands-on-reinforcement-learning-course-part-2-1b0828a1046b"> <strong class="ls iu">第二部分</strong> </a>中看到的，一个强化学习问题的难度与问题的大小直接相关</p><ul class=""><li id="9c71" class="mx my it ls b lt mm lw mn lz oa md ob mh oc ml od nd ne nf bi translated">行动空间:<em class="nn">代理在每一步可以选择多少个行动？</em></li><li id="197b" class="mx my it ls b lt ng lw nh lz ni md nj mh nk ml od nd ne nf bi translated">状态空间:<em class="nn">代理可以在多少种不同的环境配置中找到自己？</em></li></ul><p id="e4e2" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">对于动作和状态数量有限的小环境，我们有强有力的保证像 Q-learning 这样的算法能够很好地工作。这些被称为<strong class="ls iu">表格或离散环境</strong>。</p><p id="39d1" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">q 函数本质上是一个矩阵，其行数与状态数相同，列数与动作数相同。在这些<em class="nn">小</em>的世界里，我们的代理人可以轻松地探索各州并制定有效的政策。随着状态空间和(尤其是)动作空间变得更大，RL 问题变得更难解决。</p><p id="8601" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">今天的环境是<strong class="ls iu">不是</strong>表格化的。但是，我们将使用一个离散化“技巧”将其转换为表格形式，然后求解。</p><p id="7dd9" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">我们先熟悉一下环境吧！</p><h1 id="5875" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">2.环境、行动、状态、奖励</h1><p id="2e1c" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><a class="ae mr" href="https://github.com/Paulescu/hands-on-rl/blob/main/02_mountain_car/notebooks/00_environment.ipynb" rel="noopener ugc nofollow" target="_blank">👉🏽notebooks/00 _ environment . ipynb</a></p><p id="7504" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">让我们加载环境:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/7da35feedc76c2f8c137503b2000ded9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1tuqWxs7U8C31iw18ICa-A.png"/></div></div></figure><p id="6e82" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">并绘制一帧:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/b275c69ac864d4d3817ad3fb441afebd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X-xICVY6MWLIb6Y9fxdpUA.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/71efbabebe891d7367022448cded3b07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-TrXOvIG8Lj7XaANk0O6bQ.png"/></div></div></figure><p id="22d5" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">两个数字决定了汽车的<strong class="ls iu">状态</strong>:</p><ul class=""><li id="9c24" class="mx my it ls b lt mm lw mn lz oa md ob mh oc ml od nd ne nf bi translated">其位置范围从<strong class="ls iu"> -1.2 </strong>到<strong class="ls iu"> 0.6 </strong></li><li id="d460" class="mx my it ls b lt ng lw nh lz ni md nj mh nk ml od nd ne nf bi translated">其速度范围从<strong class="ls iu"> -0.07 </strong>到<strong class="ls iu"> 0.07 </strong>。</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/49c5f3709c966ee0934495d1c3d5c957.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZBFyzjhlkMr_eZw9h-veDw.png"/></div></div></figure><p id="e8fa" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">状态由两个连续的数字给出。这是<code class="fe ms mt mu mv b">Taxi-v3</code>环境与<a class="ae mr" rel="noopener" target="_blank" href="/hands-on-reinforcement-learning-course-part-2-1b0828a1046b">第二部分</a>的显著区别。我们将在后面看到如何处理这个问题。</p><p id="d0bf" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">有哪些<strong class="ls iu">动作</strong>？</p><p id="fc3b" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">有 3 种可能的操作:</p><ul class=""><li id="9fa7" class="mx my it ls b lt mm lw mn lz oa md ob mh oc ml od nd ne nf bi translated"><code class="fe ms mt mu mv b">0</code>向左加速</li><li id="e3d7" class="mx my it ls b lt ng lw nh lz ni md nj mh nk ml od nd ne nf bi translated"><code class="fe ms mt mu mv b">1</code>无所事事</li><li id="19ba" class="mx my it ls b lt ng lw nh lz ni md nj mh nk ml od nd ne nf bi translated"><code class="fe ms mt mu mv b">2</code>向右加速</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/d1b95de5e5ddfef7310f0eeda2a28d93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*b8lH0_VujZ0Svs1LqxRXhw.png"/></div></div></figure><p id="3c0b" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">而<strong class="ls iu">奖励</strong>？</p><ul class=""><li id="a985" class="mx my it ls b lt mm lw mn lz oa md ob mh oc ml od nd ne nf bi translated">如果汽车位置小于 0.5，则奖励-1。</li><li id="41be" class="mx my it ls b lt ng lw nh lz ni md nj mh nk ml od nd ne nf bi translated">一旦汽车的位置高于 0.5，或者达到最大步数，本集就结束:<code class="fe ms mt mu mv b">n_steps &gt;= env._max_episode_steps</code></li></ul><p id="e6b3" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">默认的负奖励-1 鼓励汽车尽快逃离山谷。</p><p id="7d43" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">总的来说，我建议你直接在 Github 中检查<a class="ae mr" href="https://github.com/openai/gym/tree/master/gym/envs" rel="noopener ugc nofollow" target="_blank">开放 AI 健身房环境的</a>实现，以了解状态、动作和奖励。</p><p id="d505" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">该代码有很好的文档记录，可以帮助您快速了解开始 RL 代理工作所需的一切。<code class="fe ms mt mu mv b">MountainCar</code>的实现是<a class="ae mr" href="https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py" rel="noopener ugc nofollow" target="_blank">这里以</a>为例。</p><p id="4ab0" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">很好。我们熟悉了环境。</p><p id="f934" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">让我们为这个问题建立一个基线代理！</p><h1 id="f929" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">3.随机代理基线🚃🍷</h1><p id="0f88" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><a class="ae mr" href="https://github.com/Paulescu/hands-on-rl/blob/main/02_mountain_car/notebooks/01_random_agent_baseline.ipynb" rel="noopener ugc nofollow" target="_blank">👉🏽notebooks/01 _ random _ agent _ baseline . ipynb</a></p><p id="9d0b" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">强化学习问题很容易变得复杂。结构良好的代码是控制复杂性的最佳盟友。</p><p id="f833" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">今天我们将提升我们的 Python 技能，并为我们所有的代理使用一个<code class="fe ms mt mu mv b">BaseAgent</code>类。从这个<code class="fe ms mt mu mv b">BaseAgent</code>类，我们将派生出我们的<code class="fe ms mt mu mv b">RandomAgent</code>和<code class="fe ms mt mu mv b">SarsaAgent</code>类。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/e1837196a3033d1b2be8bb4d44a7886a.png" data-original-src="https://miro.medium.com/v2/resize:fit:722/format:webp/1*7cjWWHpyER3WHBzhmoT24Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">父类和子类(图片由作者提供)</p></figure><p id="fbb7" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated"><code class="fe ms mt mu mv b">BaseAgent</code>是我们在<code class="fe ms mt mu mv b"><a class="ae mr" href="https://github.com/Paulescu/hands-on-rl/blob/main/02_mountain_car/src/base_agent.py" rel="noopener ugc nofollow" target="_blank">src/base_agent.py</a></code>中定义的一个<strong class="ls iu">抽象类</strong></p><p id="96f6" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">它有 4 种方法。</p><p id="ab1c" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">它的两个方法是抽象的，这意味着当我们从<code class="fe ms mt mu mv b">BaseAgent:</code>派生出<code class="fe ms mt mu mv b">RandomAgent</code>和<code class="fe ms mt mu mv b">SarsaAgent</code>时，我们必须实现它们</p><ul class=""><li id="4be8" class="mx my it ls b lt mm lw mn lz oa md ob mh oc ml od nd ne nf bi translated"><code class="fe ms mt mu mv b">get_action(self, state)</code> →根据状态返回要执行的动作。</li><li id="d7a6" class="mx my it ls b lt ng lw nh lz ni md nj mh nk ml od nd ne nf bi translated"><code class="fe ms mt mu mv b">update_parameters(self, state, action, reward, next_state)</code> →根据经验调整代理参数。这里我们将实现 SARSA 公式。</li></ul><p id="1ebc" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">其他两种方法允许我们将经过训练的代理保存到磁盘或从磁盘加载。</p><ul class=""><li id="a411" class="mx my it ls b lt mm lw mn lz oa md ob mh oc ml od nd ne nf bi translated"><code class="fe ms mt mu mv b">save_to_disk(self, path)</code></li><li id="a78f" class="mx my it ls b lt ng lw nh lz ni md nj mh nk ml od nd ne nf bi translated"><code class="fe ms mt mu mv b">load_from_disk(cls, path)</code></li></ul><p id="4bdb" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">随着我们开始实现更复杂的模型和训练时间的增加，在训练期间保存检查点将是一个很好的主意。</p><p id="186d" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">下面是我们的<code class="fe ms mt mu mv b">BaseAgent</code>类的完整代码:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/108c21d0746e04852d247d3f9c219958.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W6OLswy8iqO29x8hgIlNWQ.png"/></div></div></figure><p id="000f" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">从这个<code class="fe ms mt mu mv b">BaseAgent</code>类，我们可以将<code class="fe ms mt mu mv b">RandomAgent</code>定义如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/633deb4ebeae0a7ad7b49806ed47cb63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NqjFJieCnYBOTPakBKMu3Q.png"/></div></div></figure><p id="9333" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">让我们评估一下这个<code class="fe ms mt mu mv b">RandomAgent</code>而不是<code class="fe ms mt mu mv b">n_episodes = 100</code>，看看它的表现如何:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/a9499024d339bb02ded6ccec743187b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7ZNfh5L5qkU-VxNA_wog6w.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/778b892e530459526678ed0608281f5e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uQh_RUWWPqzQjA3B5CAQfQ.png"/></div></div></figure><p id="fa41" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">而我们<code class="fe ms mt mu mv b">RandomAgent</code>的成功率是…</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/baae1694d5e0205d973c625337aaf131.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yrNQ9bjgci0hxQHrWJAHBg.png"/></div></div></figure><p id="c19f" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi">0% 🤭…</p><p id="16b7" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">我们可以用下面的直方图来看代理在每集中走了多远:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/af2bf9bda1ee329979d30b00293e3009.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*iOJFFQVHZgeb8BKAVtsqGQ.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oo"><img src="../Images/d3f5ca94b26f15f04b21e653c81e4a80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9m2_9ismBnGmOKHqJUIgwQ.png"/></div></div></figure><p id="8cb1" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">在这些<code class="fe ms mt mu mv b">100</code>运行中，我们的<code class="fe ms mt mu mv b">RandomAgent</code>没有越过<strong class="ls iu"> 0.5 </strong>标记。一次都没有。</p><p id="d8ad" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">当你在本地机器上运行这段代码时，你会得到稍微不同的结果，但是在任何情况下，0.5 以上的完成剧集的百分比都不会是 100%。</p><p id="a72c" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">你可以使用<code class="fe ms mt mu mv b"><a class="ae mr" href="https://github.com/Paulescu/hands-on-rl/blob/37fbac23d580a44d46d4187525191b324afa5722/02_mountain_car/src/viz.py#L52-L61" rel="noopener ugc nofollow" target="_blank">src/viz.py</a></code>中的<code class="fe ms mt mu mv b">show_video</code>功能观看我们悲惨的<code class="fe ms mt mu mv b">RandomAgent</code>行动</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/f8b58ba0dc8b9b574b18b1a0908ed2d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PtgQ4CoSxkwAM9KdJHe9wA.png"/></div></div></figure><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="e8ba" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">一个随机的代理不足以解决这种环境。</p><p id="08ce" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">让我们试试更聪明的方法😎…</p><h1 id="0534" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">4.萨尔萨特工🚃🧠</h1><p id="7f13" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><a class="ae mr" href="https://github.com/Paulescu/hands-on-rl/blob/main/02_mountain_car/notebooks/02_sarsa_agent.ipynb" rel="noopener ugc nofollow" target="_blank">👉🏽notebooks/02 _ sarsa _ agent . ipynb</a></p><p id="eac0" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated"><a class="ae mr" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.17.2539&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank"> SARSA </a>(由 Rummery 和 Niranjan 提出)是一种通过学习最优 q 值函数来训练强化学习代理的算法。</p><p id="b446" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">它于 1994 年出版，是 Q-learning(由克里斯·沃金斯和彼得·达扬出版)的两年后。</p><p id="bed8" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">SARSA 代表<strong class="ls iu"> S </strong>状态<strong class="ls iu"> A </strong>动作<strong class="ls iu">R</strong>e 前进<strong class="ls iu"> S </strong>状态<strong class="ls iu"> A </strong>动作。</p><p id="5fe1" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">SARSA 和 Q-learning 都利用贝尔曼方程来迭代地寻找最佳 Q 值函数的更好的近似<strong class="ls iu"> Q*(s，a) </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/f0b59d13eaf1d21327365f9379b8f374.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*my9zeq4ne0RkBZDy.png"/></div></div></figure><p id="615d" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">但是他们的做法略有不同。</p><p id="9d6a" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">如果您还记得第 2 部分，Q-learning 的更新公式是</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/7685aadd34934cb8328fd926bc2f2d0e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*VmNvb0OaMSsBhykj.png"/></div></div></figure><p id="fbec" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这个公式是一种计算 q 值的新估计值的方法</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi or"><img src="../Images/a396ade980b188267f4b6afb3dc8a0ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MlcDcVs5vRN1uM09F1EM9A.png"/></div></div></figure><p id="dd61" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这个数量是一个<em class="nn">目标</em>🎯我们想把原来的估计修正为。这是我们应该瞄准的最佳 q 值的<em class="nn">估计</em>，它随着我们训练代理和我们的 q 值矩阵的更新而改变。</p><blockquote class="os ot ou"><p id="fb30" class="lq lr nn ls b lt mm ju lv lw mn jx ly ov mo mb mc ow mp mf mg ox mq mj mk ml im bi translated">强化学习问题通常看起来像有监督的 ML 问题，但是带有移动的目标🏃 🎯</p></blockquote><p id="dae5" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">SARSA 有一个类似的更新公式，但有一个不同的<em class="nn">目标</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oy"><img src="../Images/a6c2642c7cc16f874d3b7cdbb5941ed0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y-vKWZbWFOIpQNBDHlu-Aw.png"/></div></div></figure><p id="2fd4" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">SARSA 的目标</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oz"><img src="../Images/35c812c36f3839155ed5875056785dce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*n9seK9X4q4KBNE7qpyidTA.png"/></div></div></figure><p id="ab6d" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">也取决于代理将在下一个状态<strong class="ls iu">s’采取的动作<strong class="ls iu">a’</strong>。</strong>这是非典<strong class="ls iu">A’</strong>s 中最后一个<strong class="ls iu"> A </strong>的名字。</p><p id="cf93" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">如果你探索足够的状态空间并用 SARSA 更新你的 q 矩阵，你将得到一个最优策略。太好了！</p><p id="7278" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">你可能会想…</p><p id="3f69" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">Q-learning 和 SARSA 在我看来几乎一模一样。有什么区别？🤔</p><p id="19bd" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">有一个关键区别:</p><ul class=""><li id="4e94" class="mx my it ls b lt mm lw mn lz oa md ob mh oc ml od nd ne nf bi translated">SARSA 的更新取决于下一个动作<strong class="ls iu">a’，</strong>并因此取决于当前策略。随着您的训练和 q 值(以及相关策略)的更新，新策略可能会针对相同的状态<strong class="ls iu">s’</strong>产生不同的下一个动作<strong class="ls iu">a’</strong>。你不能用过去的经验<strong class="ls iu"> (s，a，r，s’，a’)</strong>来提高你的估计。取而代之的是，你用一次经历来更新 q 值，然后把它扔掉。因此，SARSA 被称为<strong class="ls iu">基于策略的</strong>方法。</li><li id="3137" class="mx my it ls b lt ng lw nh lz ni md nj mh nk ml od nd ne nf bi translated">在 Q-learning 中，更新公式不依赖于下一个动作<strong class="ls iu">a’，</strong>，而只依赖于<strong class="ls iu"> (s，a，r，s’)。</strong>您可以重用使用旧版本策略收集的过去经验<strong class="ls iu"> (s，a，r，s’)，</strong>，以提高当前策略的 q 值。Q-learning 是一种<strong class="ls iu">非政策</strong>方法。</li></ul><p id="665e" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">策略外的方法比策略内的方法需要更少的经验来学习，因为您可以多次重用过去的经验来改进您的估计。他们更<strong class="ls iu">样高效</strong>。</p><p id="d3e5" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">然而，当状态、动作空间增长时，非策略方法具有收敛到最优 Q 值函数 Q*(s，a)的问题。他们可能很狡猾，而且不稳定。</p><p id="86fa" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">当我们进入深度 RL 领域时，我们将在课程的后面遇到这些权衡🤓。</p></div><div class="ab cl pa pb hx pc" role="separator"><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf"/></div><div class="im in io ip iq"><p id="1b73" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">回到我们的问题…</p><p id="f4c5" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">在<code class="fe ms mt mu mv b">MountainCar</code>环境中，状态不是离散的，而是一对连续的值(位置<code class="fe ms mt mu mv b">s1</code>，速度<code class="fe ms mt mu mv b">s2</code>)。</p><p id="7a1f" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated"><em class="nn">连续</em>在这个上下文中实质上是指<em class="nn">无限可能的值</em>。如果有无限个可能的状态，不可能把它们都访问一遍来保证 SARSA 收敛。</p><p id="55c2" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">为了解决这个问题，我们可以使用一个技巧。</p><p id="3589" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">让我们把状态向量离散成一组有限的值。本质上，我们并没有改变环境，而是改变了代理用来选择其动作的状态的表示。</p><p id="2711" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">我们的<code class="fe ms mt mu mv b">SarsaAgent</code>通过将位置<code class="fe ms mt mu mv b">[-1.2 … 0.6]</code>四舍五入到最近的<code class="fe ms mt mu mv b">0.1</code>标记，将速度<code class="fe ms mt mu mv b">[-0.07 ...0.07]</code>四舍五入到最近的<code class="fe ms mt mu mv b">0.01</code>标记，将状态<code class="fe ms mt mu mv b">(s1, s2)</code>从连续离散化。</p><p id="8ed6" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">该函数正是这样做的，将连续状态转化为离散状态:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ph"><img src="../Images/3147d5d395aa5ba36a0586a206200447.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VFlTdjKroJ6Qt-douDurcw.png"/></div></div></figure><p id="9060" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">一旦代理使用离散化状态，我们可以使用上面的 SARSA 更新公式，随着我们不断迭代，我们将更接近最佳 q 值。</p><p id="e7c9" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这是<code class="fe ms mt mu mv b">SarsaAgent</code>的全部实现</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pi"><img src="../Images/9f4857a08e1173fcd3baef34a1a88118.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PF3p91x8tGMO4dGphz9qvQ.png"/></div></div></figure><p id="9e2e" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">注意👆q 值函数是一个 3 维矩阵:2 维表示状态(位置、速度)，1 维表示动作。</p><p id="8999" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">让我们选择合理的超参数，并为<code class="fe ms mt mu mv b">n_episodes = 10,000</code>训练这个<code class="fe ms mt mu mv b">SarsaAgent</code></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pj"><img src="../Images/2c6732348865d94c1d0c1a6d3f556d14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hZeundcHQjXNC9zRVL8Qew.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pk"><img src="../Images/0cbeedc3acc92f887ebe9752b6bcdea2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YX_S88jawVJLzMQKU447uA.png"/></div></div></figure><p id="f861" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">让我们用它们的 50 集移动平均线(橙色线)来绘制<code class="fe ms mt mu mv b">rewards</code>和<code class="fe ms mt mu mv b">max_positions</code>(蓝色线)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pl"><img src="../Images/97e59c913aa67e7b89eb8d8504e5c6d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CPTL-g8uY3AePOHsQmzz0A.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pm"><img src="../Images/ff1167d5b8c7524211f4ab5475176e72.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8SCFSfDwUJ8V4jSEN370Ng.png"/></div></div></figure><p id="b956" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">超级！看起来我们的<code class="fe ms mt mu mv b">SarsaAgent</code>正在学习。</p><p id="64b2" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">在这里你可以看到它的作用:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl nm l"/></div></figure><p id="5469" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">如果你观察上面的<code class="fe ms mt mu mv b">max_position</code>图表，你会意识到汽车偶尔无法爬山。</p><p id="22f6" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这种情况多长时间发生一次？让我们评价一下<code class="fe ms mt mu mv b">1,000</code>随机集上的代理:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pn"><img src="../Images/cab197523e01c5e6e1d3e292c59f5d9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9mOT1QGH8NAz6EHMLQqZrA.png"/></div></div></figure><p id="7af4" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">并计算成功率:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi po"><img src="../Images/b0e7b05a16497020693421715c812059.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EH--CxYcZTcvatgXx3t1oA.png"/></div></div></figure><p id="08c4" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated"><strong class="ls iu"> 95.2% </strong>还算不错。尽管如此，并不完美。请记住这一点，我们将在本课程的稍后部分回来。</p><p id="ce05" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated"><strong class="ls iu">注意:</strong>当您在您的终端上运行这段代码时，您会得到稍微不同的结果，但是我敢打赌您不会得到 100%的性能。</p><p id="2981" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">干得好！我们实现了一个<code class="fe ms mt mu mv b">SarsaAgent</code>来学习🤟</p><p id="131b" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这是一个暂停的好时机…</p><h1 id="35e8" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">5.停下来喘口气，⏸🧘</h1><p id="5c24" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><a class="ae mr" href="https://github.com/Paulescu/hands-on-rl/blob/main/02_mountain_car/notebooks/03_momentum_agent_baseline.ipynb" rel="noopener ugc nofollow" target="_blank">👉🏽notebooks/03 _ momentum _ agent _ baseline . ipynb</a></p><p id="7270" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">如果我告诉您,<code class="fe ms mt mu mv b">MountainCar</code>环境有一个更简单的解决方案会怎么样…</p><p id="fddb" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">100%的时间都有效？😅</p><p id="f45d" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">要遵循的最佳政策很简单。</p><p id="955e" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated"><em class="nn">跟着势头走</em>:</p><ul class=""><li id="d38f" class="mx my it ls b lt mm lw mn lz oa md ob mh oc ml od nd ne nf bi translated">当汽车向右移动时向右加速<code class="fe ms mt mu mv b">velocity &gt; 0</code></li><li id="af61" class="mx my it ls b lt ng lw nh lz ni md nj mh nk ml od nd ne nf bi translated">当汽车向左移动时，向左加速<code class="fe ms mt mu mv b">velocity &lt;= 0</code></li></ul><p id="730c" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这个策略看起来像这样:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pp"><img src="../Images/4012cabf567e3a9f2d13e57dd003b369.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IBxzJEqZJLRR2Bts5syeEg.png"/></div></div></figure><p id="48ab" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这是你如何用 Python 写这个<code class="fe ms mt mu mv b">MomentumAgent</code>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pq"><img src="../Images/74ed40dbf718576f627033ca08448754.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*oq-v4VUolKEM4Cx5kUeCSQ.png"/></div></div></figure><p id="90e2" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">你可以仔细检查它是否完成了每一集。百分之百的成功率。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pr"><img src="../Images/14b4810c2813c63d25c88867abcc8f1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Brb755zRjnvSDq90D93GoQ.png"/></div></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pq"><img src="../Images/ea03557160962785a1944ed38962d043.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JQkdwAwr09MCHOyAX4ExOQ.png"/></div></div></figure><p id="28b4" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">另一方面，如果你画出受过训练的<code class="fe ms mt mu mv b">SarsaAgent</code>的政策，你会看到这样的东西:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ps"><img src="../Images/8d07b90efa77e94c4727943fad141766.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zfrKsKAXTc27LLz18orfQw.png"/></div></div></figure><p id="5edf" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这与完美的<code class="fe ms mt mu mv b">MomentumAgent</code>策略有 50%的重叠</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pt"><img src="../Images/620a577f6e2b1c1677aa4d93674bd2c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Ebc3DA3id2RpPSVUj7chQ.png"/></div></div></figure><p id="5282" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这意味着我们的<code class="fe ms mt mu mv b">SarsaAgent</code>只有 50%的时间是正确的<em class="nn">。</em></p><p id="b0d3" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这很有意思……</p><p id="ff20" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated"><strong class="ls iu">为什么</strong> <code class="fe ms mt mu mv b"><strong class="ls iu">SarsaAgent</strong></code> <strong class="ls iu">经常出错却仍能取得好成绩？</strong></p><p id="46ce" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这是因为<code class="fe ms mt mu mv b">MountainCar</code>仍然是一个小环境，所以 50%的时候做出错误的决定并不那么关键。对于更大的问题，经常犯错不足以构建智能代理。</p><blockquote class="os ot ou"><p id="82b9" class="lq lr nn ls b lt mm ju lv lw mn jx ly ov mo mb mc ow mp mf mg ox mq mj mk ml im bi translated"><em class="it">你会买一辆 95%都正确的自动驾驶汽车吗？😱</em></p></blockquote><p id="7efd" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">另外，你还记得我们用来应用 SARSA 的<em class="nn">离散化技巧</em>吗？这是一个对我们帮助很大的技巧，但也给我们的解决方案带来了错误/偏见。</p><p id="d08b" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated"><strong class="ls iu">我们为什么不提高对状态和速度离散化的分辨率，以获得更好的解呢？</strong></p><p id="3c50" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这样做的问题是状态数量的指数增长，也称为<strong class="ls iu">维数灾难</strong>。随着每个状态分量分辨率的增加，状态总数呈指数增长。状态空间增长太快，SARSA 代理无法在合理的时间内收敛到最优策略。</p><p id="3f30" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated"><strong class="ls iu">好吧，但是有没有其他的 RL 算法可以完美解决这个问题？</strong></p><p id="4781" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">是的，有。我们将在接下来的课程中讨论它们。总的来说，对于 RL 算法，没有一种方法是万能的，因此您需要针对您的问题尝试几种算法，看看哪种效果最好。</p><p id="c165" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">在<code class="fe ms mt mu mv b">MountainCar</code>环境中，完美的策略看起来如此简单，以至于我们可以尝试直接学习它，而不需要计算复杂的 q 值矩阵。一种<strong class="ls iu">策略优化</strong>方法可能效果最好。</p><p id="bc13" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">但是我们今天不打算这样做。如果您想使用 RL 完美地解决这种环境，请按照课程进行操作。</p><p id="f16f" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">享受你今天的成就。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pu"><img src="../Images/de308bac849da4d211f4b59f3de4768c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Sax1jIXL1NQefK7h42Riew.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">谁在找乐子？(图片由作者提供)</p></figure><h1 id="50c2" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">6.重述✨</h1><p id="eb2a" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">哇！我们今天讨论了很多事情。</p><p id="77d2" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这是 5 个要点:</p><ul class=""><li id="5ee6" class="mx my it ls b lt mm lw mn lz oa md ob mh oc ml od nd ne nf bi translated">SARSA 是一种基于策略的算法，可以在表格环境中使用。</li><li id="3f05" class="mx my it ls b lt ng lw nh lz ni md nj mh nk ml od nd ne nf bi translated">使用状态的离散化，可以将小型连续环境视为表格，然后使用表格 SARSA 或表格 Q-learning 进行求解。</li><li id="4f38" class="mx my it ls b lt ng lw nh lz ni md nj mh nk ml od nd ne nf bi translated">由于维数灾难，更大的环境不能被离散化和求解。</li><li id="840a" class="mx my it ls b lt ng lw nh lz ni md nj mh nk ml od nd ne nf bi translated">对于比<code class="fe ms mt mu mv b">MountainCar</code>更复杂的环境，我们将需要更先进的 RL 解决方案。</li><li id="a93f" class="mx my it ls b lt ng lw nh lz ni md nj mh nk ml od nd ne nf bi translated"><strong class="ls iu">有时候 RL 并不是最佳方案</strong>。当你试图解决你关心的问题时，请记住这一点。不要和你的工具结婚(在这种情况下是 RL)，而是专注于找到一个好的解决方案。不要只见树木不见森林🌲🌲🌲。</li></ul><h1 id="109a" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">7.家庭作业📚</h1><p id="a537" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated"><a class="ae mr" href="https://github.com/Paulescu/hands-on-rl/blob/main/02_mountain_car/notebooks/04_homework.ipynb" rel="noopener ugc nofollow" target="_blank">👉🏽notebooks/04 _ homework . ipynb</a></p><p id="f0f4" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">这是我要你做的:</p><ol class=""><li id="805f" class="mx my it ls b lt mm lw mn lz oa md ob mh oc ml nc nd ne nf bi translated"><a class="ae mr" href="https://github.com/Paulescu/hands-on-rl" rel="noopener ugc nofollow" target="_blank"> <strong class="ls iu"> Git 克隆</strong></a>repo 到你的本地机器。</li><li id="c9bd" class="mx my it ls b lt ng lw nh lz ni md nj mh nk ml nc nd ne nf bi translated"><a class="ae mr" href="https://github.com/Paulescu/hands-on-rl/tree/main/02_mountain_car#quick-setup" rel="noopener ugc nofollow" target="_blank"> <strong class="ls iu">设置</strong> </a>本课的环境<code class="fe ms mt mu mv b">02_mountain_car</code></li><li id="8cb5" class="mx my it ls b lt ng lw nh lz ni md nj mh nk ml nc nd ne nf bi translated">打开<code class="fe ms mt mu mv b"><a class="ae mr" href="http://02_mountain_car/notebooks/04_homework.ipynb" rel="noopener ugc nofollow" target="_blank">02_mountain_car/notebooks/04_homework.ipynb</a></code>并尝试完成 2 个挑战。</li></ol><p id="86a7" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">在第一个挑战中，我要求你调整 SARSA 超参数<code class="fe ms mt mu mv b">alpha</code>(学习率)和<code class="fe ms mt mu mv b">gamma</code>(折扣系数)以加速训练。可以从<a class="ae mr" rel="noopener" target="_blank" href="/hands-on-reinforcement-learning-course-part-2-1b0828a1046b">第二部</a>中得到启发。</p><p id="fb27" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">在第二个挑战中，尝试提高离散化的分辨率，并使用表格 SARSA 学习 q 值函数。就像我们今天做的一样。</p><p id="a5af" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">让我知道，如果你建立一个代理，实现 99%的性能。</p><h1 id="6259" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">8.下一步是什么？❤️</h1><p id="5f05" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">在下一课中，我们将进入强化学习和监督机器学习交叉的领域🤯。</p><p id="b59d" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">我保证会很酷的。</p><p id="0782" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">在那之前，</p><p id="c0b6" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">在这个叫做地球的神奇星球上多享受一天吧🌎</p><p id="3915" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">爱❤️</p><p id="35b9" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">不断学习📖</p><p id="9d74" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">如果你喜欢这门课程，请分享给你的朋友和同事。</p><p id="b99f" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">你可以通过<code class="fe ms mt mu mv b">plabartabajo@gmail.com</code>找到我。我很乐意联系。</p><p id="a081" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">回头见！</p></div><div class="ab cl pa pb hx pc" role="separator"><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf pg"/><span class="pd bw bk pe pf"/></div><div class="im in io ip iq"><p id="a196" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated"><em class="nn">你想成为(甚至)更好的数据科学家，接触关于机器学习和数据科学的顶级课程吗？</em></p><p id="11d7" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">👉🏽订阅<a class="ae mr" href="https://datamachines.xyz/subscribe/" rel="noopener ugc nofollow" target="_blank"> <strong class="ls iu"> <em class="nn"> datamachines </em>简讯</strong> </a> <strong class="ls iu">。</strong></p><p id="f8c5" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">👉🏽<a class="ae mr" href="https://medium.com/@paulba-93177" rel="noopener"> <strong class="ls iu">跟着我</strong> </a>上媒。</p><p id="8f75" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">祝你愉快，🧡❤️💙</p><p id="e017" class="pw-post-body-paragraph lq lr it ls b lt mm ju lv lw mn jx ly lz mo mb mc md mp mf mg mh mq mj mk ml im bi translated">避寒胜地</p></div></div>    
</body>
</html>