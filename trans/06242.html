<html>
<head>
<title>What Is the Minimum Sample Size Required to Perform a Meaningful Linear Regression?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">进行有意义的线性回归所需的最小样本量是多少？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/what-is-the-minimum-sample-size-required-to-perform-a-meaningful-linear-regression-945c0edf1d0?source=collection_archive---------20-----------------------#2021-06-04">https://towardsdatascience.com/what-is-the-minimum-sample-size-required-to-perform-a-meaningful-linear-regression-945c0edf1d0?source=collection_archive---------20-----------------------#2021-06-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="5dd7" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a>，<a class="ae ep" href="https://medium.com/analytics-vidhya/machine-learning/home" rel="noopener">机器学习</a></h2><div class=""/><div class=""><h2 id="0001" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">一种基于估计精度的定量方法</h2></div><h1 id="c17b" class="ko kp iq bd kq kr ks kt ku kv kw kx ky kf kz kg la ki lb kj lc kl ld km le lf bi translated">介绍</h1><p id="de06" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><em class="mc">进行可靠的线性回归所需的最小样本量是多少？更准确地说，估计的斜率只偏离“真实”一个百分点的概率是多少？</em></p><p id="b5b8" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">在这篇文章中，我试图提供一个定量的答案。更准确地说，我推断在Y=αX+β+ϵ的一般情况下，在高斯-马尔可夫假设下，对于“足够有代表性”的样本:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi mi"><img src="../Images/2ce8e30f846b4a3ea5cd6d69d98aea56.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/format:webp/1*ux7C_dmOVt6sgqRGa4-O-w.png"/></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">OLS回归的简化公式</p></figure><p id="42d7" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated"><em class="mc">其中</em>α’<em class="mc">是斜率估计值，η是相对误差，ρ是X和Y之间的相关性，m是样本大小。</em></p><p id="8898" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated"><em class="mc">估算误差表示为“真实值”的一部分，而不是原始值，这一事实对于给出有用的实际答案至关重要，因为如果α=100，则像</em> α' <em class="mc"> =α 1这样的估算可能被认为是可接受的，如果α=0.1，则被认为是不可接受的。在我看来，使用与</em>α’<em class="mc">相关的置信区间，似乎与告知信任线性回归结果所需的最小样本量无关。</em></p></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><h1 id="a00a" class="ko kp iq bd kq kr nb kt ku kv nc kx ky kf nd kg la ki ne kj lc kl nf km le lf bi translated"><strong class="ak">频率主义者的方法</strong></h1><p id="6efe" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">在下面，如果α是随机变量x的一个矩，I用α'表示α的任何无偏估计量(以及这个估计量对给定样本取的值)，用α̃表示样本矩。</p><h2 id="70d2" class="ng kp iq bd kq nh ni dn ku nj nk dp ky lp nl nm la lt nn no lc lx np nq le iw bi translated">一般结果的证明</h2><p id="b3bc" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">考虑简单的线性回归:我们有m个观测值的Y=αX+β+ϵ，并作出以下假设(高斯-马尔可夫假设):</p><ul class=""><li id="7344" class="nr ns iq li b lj md lm me lp nt lt nu lx nv mb nw nx ny nz bi translated">严格的外来性:E[ϵ|X]=0</li><li id="6aa1" class="nr ns iq li b lj oa lm ob lp oc lt od lx oe mb nw nx ny nz bi translated">同质性:Var(ϵ|X)=σ</li><li id="504d" class="nr ns iq li b lj oa lm ob lp oc lt od lx oe mb nw nx ny nz bi translated">单独的误差项不相关</li></ul><p id="008a" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">那么普通的最小二乘估计量α’是这样的:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/f68db93227f275b76474126130ea28c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:300/format:webp/1*IhgQ7qbwViFxoJYKXKMUkw.png"/></div></figure><p id="903d" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">并且表示ρX和Y之间的相关性:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/6b6e55c0bc3290a7f7800cee3ade0556.png" data-original-src="https://miro.medium.com/v2/resize:fit:730/format:webp/1*UHfjmJMcRwO4fmmWzrulxw.png"/></div></figure><p id="b80a" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">这意味着(使用Bienaymé–Chebyshev不等式):</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/ce93142717d788a7590087d6502d92b9.png" data-original-src="https://miro.medium.com/v2/resize:fit:646/format:webp/1*2Rj1oermUrkE-v4OVOfeHw.png"/></div></figure><p id="3b13" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">结果是:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/9b0a7b5036b223a5190f15ab5ea287ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/format:webp/1*xtHx8KC5SHHtINFXHYr-rg.png"/></div></figure><p id="8dd7" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">两边除以|α|:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/2ab51ecc7911afcdec9584a28405ee28.png" data-original-src="https://miro.medium.com/v2/resize:fit:714/format:webp/1*Zj5xYrxDP03QQieJmQgh-Q.png"/></div></figure><p id="32c9" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">这种不平等就变成了:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/d16702e82b4fca705db25439943b9e07.png" data-original-src="https://miro.medium.com/v2/resize:fit:698/format:webp/1*Fp5vJstwb1bWsVvpatrxgw.png"/></div></figure><p id="8edd" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">最后:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/aa2121a9501ce8e8029f37c38552ff8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:770/format:webp/1*_vJBe0BxSOvPgUy8fhtVcg.png"/></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">经典OLS回归的PAC-贝叶斯公式</p></figure><p id="94d1" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">换句话说:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/bc587e6fbf629b0f6139e898060e2bf7.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*E-XmDQAZdPouKmm5ceurmg.png"/></div></figure><p id="eda2" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">其中:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/3aed1d4dd54a6a8f4c0725fd11b9eeef.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/format:webp/1*NEFOodRnym3aB1qzsxwlEA.png"/></div></figure><h2 id="11d5" class="ng kp iq bd kq nh ni dn ku nj nk dp ky lp nl nm la lt nn no lc lx np nq le iw bi translated">关于不等式的评论</h2><p id="a12c" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated"><em class="mc">数据量</em></p><p id="a79d" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">误差的概率随着样本量的增加而降低。</p><p id="1412" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated"><em class="mc">样本代表性</em></p><p id="1286" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">误差低于η的概率受X的样本方差除以X的方差的限制(绝对值)。换句话说，X的样本越能代表X可以取的值，误差的概率就越低。</p><p id="defe" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated"><em class="mc">X和Y的相关性</em></p><p id="7c2d" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">当X和Y之间的相关性很高时，错误的概率就更低了。在ρ=1的边界情况下，两个不同的点(x，y)足以完全知道α。</p><p id="1053" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated"><em class="mc">误差幅度</em></p><p id="d773" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">可接受的误差范围越小，误差超出该范围的可能性就越大。</p><h2 id="41ae" class="ng kp iq bd kq nh ni dn ku nj nk dp ky lp nl nm la lt nn no lc lx np nq le iw bi translated">数字示例</h2><p id="d695" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">对于一个“足够有代表性”的样本，标准偏差之比等于1，容许误差η= 10%，相关性ρ=0.8，至少需要m=188个数据，才能有(1-δ)=70%以上的机会估计系数α(在η之内)。</p><p id="7230" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">如果η=100%，则δ是α’和α符号相反的概率的上限。在这种情况下，对于其他参数使用与之前相同的值，m=6个样本数据足以将该概率降低到δ &lt;10%.</p><p id="447c" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">For a ratio of standard deviations equal to 1 (“representative sample”) and for an upper bound of the relative error η=10%, the graph below shows the evolution of the lower bound of the probability of the relative error with respect to the sample size m for different values of ρ, the correlation between X and Y.</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="op oq di or bf os"><div class="gh gi oo"><img src="../Images/50bbd2c812bb72bf0caec01aabfbde5d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OaW8xl09EYbOr9Yjll4HGQ.png"/></div></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">Minimum sample size to perform an OLS regression (relative error η = 10%) — Image by author</p></figure><h2 id="d11c" class="ng kp iq bd kq nh ni dn ku nj nk dp ky lp nl nm la lt nn no lc lx np nq le iw bi translated">Normality assumption for the residuals</h2><p id="6d1d" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">If we assume that: ϵ ~ N(0,σ² ) then:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/eb9ef5332e0ebaac11c83ecd6b6a3830.png" data-original-src="https://miro.medium.com/v2/resize:fit:284/format:webp/1*qzgwtRX3XlfaQRzJUQM1tQ.png"/></div></figure><p id="70cf" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">And we can use the following inequality:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/c8dcdd6f34275e2177d03483a5271043.png" data-original-src="https://miro.medium.com/v2/resize:fit:600/format:webp/1*GVU94H43Xe6qhlDZycEmgw.png"/></div></figure><p id="ed75" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">where Φ is the distribution function of N(0,1) and:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/50282af4e2e5014a0d0d696f3ad28834.png" data-original-src="https://miro.medium.com/v2/resize:fit:218/format:webp/1*7_nGtWDX0kCPEdhsCtrhIw.png"/></div></figure><p id="cd2b" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">Therefore:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/7540692821407f6bb998a59d7dd6fdb7.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*54an4azvp5aeU6wwe8NnLQ.png"/></div></figure><p id="69bd" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">Consequently:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/5a678d3733231af5c1ff5bfe731fd21e.png" data-original-src="https://miro.medium.com/v2/resize:fit:832/format:webp/1*72olJG95GmN0yTw7e-b1KQ.png"/></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">PAC-Bayesian formula for the OLS regression with normality assumption</p></figure><p id="1f24" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">Or alternatively:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/1286241459283f1dccd619d2fcd492a2.png" data-original-src="https://miro.medium.com/v2/resize:fit:522/format:webp/1*u7pGYwgMPtAw44NxfX3Zmg.png"/></div></figure><p id="7215" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">with:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/f50875a3dd7f38e6748da488497bae2c.png" data-original-src="https://miro.medium.com/v2/resize:fit:486/format:webp/1*8Sz2ohUZpprhzmxUXdNmWw.png"/></div></figure><p id="6ab7" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">This framing establishes a somewhat peculiar confidence interval of width η⋅α (unknown) around α’, with a confidence level of (1-δ).</p></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><h1 id="8942" class="ko kp iq bd kq kr nb kt ku kv nc kx ky kf nd kg la ki ne kj lc kl nf km le lf bi translated"><strong class="ak">贝叶斯方法</strong></h1><h2 id="090f" class="ng kp iq bd kq nh ni dn ku nj nk dp ky lp nl nm la lt nn no lc lx np nq le iw bi translated">频率主义者和贝叶斯方法的区别</h2><p id="5cdd" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">到目前为止，这些计算都是基于频率主义方法。在这个框架中，α'是一个随机变量，α是一个未知但固定的系数:研究一个人必须进行线性回归的最小样本量只对一般的方法考虑有效。一旦在实践中进行线性回归，人们就不再研究随机变量α’,而是研究它可以取的一个值:α’和α是两个数字，或多或少彼此接近，但与任何概率计算无关。这就是frequentist置信区间的问题:所寻求的值可能属于也可能不属于它，而不可能说出它的概率(区间的置信水平与这个特定区间包含该值的概率无关)。</p><p id="f5f1" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">也可以使用一种方法上非常不同的方法，贝叶斯分析。在这种情况下，它不再是一个关于一般情况的推理问题，想象一个人重复执行线性回归，而是同时包含所有特殊情况。那么α’是一个数，α是一个随机变量，就有可能知道对于每一个具体的线性回归，α接近α’的概率。可信区间是frequentist置信区间的贝叶斯框架中的镜像，但在这种情况下，相关的置信水平可以解释为α属于该区间的概率。</p><h2 id="fbfd" class="ng kp iq bd kq nh ni dn ku nj nk dp ky lp nl nm la lt nn no lc lx np nq le iw bi translated">可信区间</h2><p id="de20" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">这里考虑一个简单的贝叶斯线性回归的例子:Y=αX+β+ϵ</p><p id="886f" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">如果我们做如下假设:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/e571cbfad61fb1926356ccbb7c6e47b6.png" data-original-src="https://miro.medium.com/v2/resize:fit:338/format:webp/1*K-aZeop68Cy5f-IuHqrxlA.png"/></div></figure><p id="8c3e" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">然后，α遵循学生的t分布，具有ν=(m-2)个自由度、位置参数α’和比例参数σ(关于此设置的更多细节，请参见Merlise Clyde、Mine etinkaya-Rundel、Colin Rundel、、Christine Chai和Lizzy Huang撰写的<a class="ae pb" href="https://statswithr.github.io/book/introduction-to-bayesian-regression.html#bayesian-simple-linear-regression-using-the-reference-prior" rel="noopener ugc nofollow" target="_blank">贝叶斯思维简介</a>):</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div role="button" tabindex="0" class="op oq di or bf os"><div class="gh gi pc"><img src="../Images/d0bb66a56b6c09759207d1a908ce3bce.png" data-original-src="https://miro.medium.com/v2/resize:fit:318/format:webp/1*v1e2Qgm9K5vkffgqHsmZLQ.png"/></div></div></figure><p id="5e2b" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">对于ν&gt;2(因此m&gt;4)，该分布的平均值为α'，其方差为:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/1cccf5c3c4eea59039b462435fa36b8b.png" data-original-src="https://miro.medium.com/v2/resize:fit:182/format:webp/1*RLgDsFJ4WHIk6Bzp7pdEgA.png"/></div></figure><p id="3d28" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">所以用我们正在研究的广义学生t分布的分布函数来表示F:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/c46a8262c0dbdb64114ea538d2869b72.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*G2WdPrcu3G21-SQfiMnMJA.png"/></div></figure><p id="457c" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">最后:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/5a682ed4ba50fc41c395209fcdf89979.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/format:webp/1*ga3Ox8nRH1pcTwpLVEgNDA.png"/></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">贝叶斯线性回归的可信区间</p></figure><p id="ef47" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">这是标准可信区间。必须注意，对m的依赖性并没有消失:它包含在函数F中，当m趋于无穷大时，函数F的方差趋近于0。</p></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><h1 id="1d29" class="ko kp iq bd kq kr nb kt ku kv nc kx ky kf nd kg la ki ne kj lc kl nf km le lf bi translated">结论</h1><p id="9ef7" class="pw-post-body-paragraph lg lh iq li b lj lk ka ll lm ln kd lo lp lq lr ls lt lu lv lw lx ly lz ma mb ij bi translated">对于进行线性回归所需的最小样本量问题，给出一个详细的定量答案是可以想象的。实际上，使用高斯-马尔可夫假设，根据以下公式，<em class="mc"> α </em>上的相对误差低于某个阈值的概率取决于X和Y之间的实际相关性<em class="mc"> ρ </em>，样本大小m和样本的代表性:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/ec36a5180f7647749810441745b7244a.png" data-original-src="https://miro.medium.com/v2/resize:fit:708/format:webp/1*cG8GNnT8Sy_92JoIEs2fXA.png"/></div><p class="mq mr gj gh gi ms mt bd b be z dk translated">OLS回归的PAC-贝叶斯公式</p></figure><p id="6b25" class="pw-post-body-paragraph lg lh iq li b lj md ka ll lm me kd lo lp mf lr ls lt mg lv lw lx mh lz ma mb ij bi translated">其中:</p><figure class="mj mk ml mm gt mn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/a42b1a9ca0cdcec155441903b5092d6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:354/format:webp/1*1P4_ARRU1VgrUmNhXKgjkg.png"/></div></figure></div></div>    
</body>
</html>