<html>
<head>
<title>A Quick Guide to Cross-Entropy Loss Function</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">交叉熵损失函数快速指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-quick-guide-to-cross-entropy-loss-function-8f3410ec6ab1?source=collection_archive---------35-----------------------#2021-06-07">https://towardsdatascience.com/a-quick-guide-to-cross-entropy-loss-function-8f3410ec6ab1?source=collection_archive---------35-----------------------#2021-06-07</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="cb12" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="0dc0" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">多类别分类是机器学习中的常见问题。让我们深入研究一下最常用的损失函数的定义。</h2></div><p id="f20b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">机器学习历来擅长的最突出的任务之一是将项目(<em class="ln">例如</em>图像、<a class="ae lo" rel="noopener" target="_blank" href="/classifying-scientific-papers-with-universal-sentence-embeddings-4e0695b70c44">文档</a>，声音)分类成不同的类别。特别是近年来，能够执行数学模型的硬件的进步，如<a class="ae lo" href="https://en.wikipedia.org/wiki/Convolutional_neural_network" rel="noopener ugc nofollow" target="_blank">卷积</a>神经网络(CNNs)和<a class="ae lo" href="https://en.wikipedia.org/wiki/Recurrent_neural_network" rel="noopener ugc nofollow" target="_blank">递归神经网络</a> (RNNs，LSTMs)，使得在性能上实现量子飞跃(<a class="ae lo" href="https://www.tensorflow.org/quantum/tutorials/qcnn" rel="noopener ugc nofollow" target="_blank">有时</a>字面意思)成为可能。然而，定义模型只是故事的一半。为了找到执行这一任务的最佳参数，还需要定义一个成本或损失函数，该函数捕捉我们想要优化的本质，并执行某种形式的梯度下降以达到一组合适的参数。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi lp"><img src="../Images/804967fcc41d241d166e246515b10151.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hcKZe4hkZyfRbDcQlEgogw.jpeg"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">Jan Antonin Kolar通过Unsplash.com拍摄的照片。</p></figure><h2 id="d3cf" class="mf mg it bd mh mi mj dn mk ml mm dp mn la mo mp mq le mr ms mt li mu mv mw iz bi translated">从多项式概率分布到交叉熵</h2><p id="b8eb" class="pw-post-body-paragraph kr ks it kt b ku mx kd kw kx my kg kz la mz lc ld le na lg lh li nb lk ll lm im bi translated">首先，我们需要定义一个描述我们问题的统计框架。通常情况下，属于一个系统的每个项目(<em class="ln">例如</em>一只宠物的图片)可以被唯一地分配到<em class="ln"> C </em> ≥ 2个可能的离散类别(<em class="ln">例如</em>猫、狗、金鱼)中的一个。对于每个训练示例<em class="ln"> i </em>，我们都有一个基础事实标签(<em class="ln"> t_i </em>)。此外，我们的模型输出一个由<em class="ln"> C </em>个数字组成的向量，这个向量必须被解释为j ∈ [1，…，<em class="ln"> C </em>是正确的类别的概率。总结一下:</p><ol class=""><li id="732c" class="nc nd it kt b ku kv kx ky la ne le nf li ng lm nh ni nj nk bi translated">每个观察都有离散的可能结果；</li><li id="1ed8" class="nc nd it kt b ku nl kx nm la nn le no li np lm nh ni nj nk bi translated">对于每个观察值，每个类别都有一个给定的发生概率(<em class="ln"> p </em> _1、…、<em class="ln"> p </em> _ <em class="ln"> C </em>)，使得<em class="ln">p</em>_ 1+…+<em class="ln">p</em>_<em class="ln">C</em>= 1，并且对于所有<em class="ln">j</em><em class="ln">p _ j</em>≥0；</li><li id="4234" class="nc nd it kt b ku nl kx nm la nn le no li np lm nh ni nj nk bi translated">有<em class="ln"> N个</em>观测值，每个观测值都是相互独立的；</li></ol><p id="5cc0" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果是这样的话，那么，你很幸运！每个可能的结果都由所谓的<a class="ae lo" href="https://en.wikipedia.org/wiki/Binomial_distribution" rel="noopener ugc nofollow" target="_blank">二项式</a>概率分布来描述，每个独立试验类别<em class="ln"> j </em>中的<em class="ln">即</em>是具有概率<em class="ln"> p_j </em>的正确结果。当我们将试验扩展到<em class="ln">C</em>T24】2类时，相应的概率分布称为<a class="ae lo" href="https://en.wikipedia.org/wiki/Multinomial_distribution" rel="noopener ugc nofollow" target="_blank">多项式</a>。为了简单起见，忽略归一化因子，它看起来是这样的:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi nq"><img src="../Images/2eb0b81bf59a97254c21c03928dc6871.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YUq0WOUBAbgPP6yaf6HGwQ.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">多项式概率分布的定义，直到一个归一化因子。图片作者。</p></figure><p id="fc8e" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">正如优化问题中经常出现的情况，当算法在计算机中实现时，处理乘积的最大值可能会引起数值问题。相反，最好使用前面带负号的同一个函数的对数，在参数空间中搜索它的<em class="ln">最小值</em>。这通常被称为<a class="ae lo" href="https://www.statlect.com/glossary/log-likelihood" rel="noopener ugc nofollow" target="_blank">对数可能性</a>。记住log 函数的<a class="ae lo" href="http://dl.uncw.edu/digilib/Mathematics/Algebra/mat111hb/EandL/logprop/logprop.html" rel="noopener ugc nofollow" target="_blank">属性，该公式转化为:</a></p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi nr"><img src="../Images/2488ef34412034e85029e8f0adffb6a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nYJ-T7V9QBMoRJPVBaY5_A.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">多项式概率分布的对数似然，直到一个常数，它可以被重新吸收到优化问题的定义中。图片作者。</p></figure><p id="aa87" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">一些读者可能已经认识到这个功能:它通常被称为<a class="ae lo" href="https://en.wikipedia.org/wiki/Cross_entropy" rel="noopener ugc nofollow" target="_blank">交叉熵</a>，因为它与从<a class="ae lo" href="https://en.wikipedia.org/wiki/Entropy_(information_theory)" rel="noopener ugc nofollow" target="_blank">信息论</a>中已知的一个量相关，以编码一个人需要多少内存<em class="ln">例如</em>来传输一条消息。</p><p id="a5a1" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">简而言之，我们将优化我们的模型的参数，以最小化上面定义的交叉熵函数，其中输出对应于<em class="ln"> p_j </em>，真实标签对应于<em class="ln"> n_j </em>。值得注意的是，真正的标签通常由一个<a class="ae lo" href="https://en.wikipedia.org/wiki/One-hot" rel="noopener ugc nofollow" target="_blank">独热编码</a>、<em class="ln">即</em>向量来表示，该向量的元素除了在对应于正类的索引处的元素之外都是0。在我们的例子中，<code class="fe ns nt nu nv b">cat=(1,0,0)</code>、<code class="fe ns nt nu nv b">dog=(0,1,0)</code>和<code class="fe ns nt nu nv b">goldfish=(0,0,1)</code>。这意味着总和<code class="fe ns nt nu nv b">logP</code>减少到单个元素<code class="fe ns nt nu nv b">n_m log p_m</code>，其中<em class="ln"> m </em>是正类的索引。</p><h2 id="7a56" class="mf mg it bd mh mi mj dn mk ml mm dp mn la mo mp mq le mr ms mt li mu mv mw iz bi translated">将模型输出转换成概率向量</h2><p id="d009" class="pw-post-body-paragraph kr ks it kt b ku mx kd kw kx my kg kz la mz lc ld le na lg lh li nb lk ll lm im bi translated">一般来说，ML模型的输出(<em class="ln"> logits </em>)不能被解释为不同结果的<em class="ln">概率</em>，而是一组浮点数。如何解读这个元组就看我们自己了。如上所述，为了成为概率向量，<em class="ln">p</em>_ 1+…+<em class="ln">p</em>_<em class="ln">C</em>= 1且<em class="ln"> p_j </em> ≥ 0对于所有<em class="ln"> j </em>。</p><p id="0d71" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在<strong class="kt jd">二元分类的情况下，</strong>用于将<em class="ln">唯一输出数</em>转换成概率的最常用函数是<a class="ae lo" href="https://en.wikipedia.org/wiki/Sigmoid_function" rel="noopener ugc nofollow" target="_blank"> sigmoid函数</a>，也称为<em class="ln">逻辑曲线</em>:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi nw"><img src="../Images/20f39c641e8543b1f94ef692cd02b30e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gXMom_LVP1HsyWfKD7Uy8w.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">s形函数的定义。图片作者。</p></figure><p id="deb2" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated"><em class="ln">s形管</em>“挤压”接近0的负值和+1的正值:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/3240ba3d486c0fced2d57bb29660f09f.png" data-original-src="https://miro.medium.com/v2/resize:fit:746/format:webp/1*MQSUfcToeSq-PS4tNX0ahw.png"/></div><p class="mb mc gj gh gi md me bd b be z dk translated">范围从-10到10的Sigmoid函数。图片由作者提供，用<a class="ae lo" href="https://www.wolframalpha.com/input/?i=plot+1%2F%281%2Be%5E-x%29+from+-10+to+10" rel="noopener ugc nofollow" target="_blank"> Wolfram:Alpha </a>制作。</p></figure><p id="e63b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">在<strong class="kt jd">多类</strong>的情况下，需要一个扩展sigmoid的函数，以便处理代表跨越<em class="ln"> C </em>可能类的概率的向量。最常用的函数叫做<em class="ln"> Softmax: </em></p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi ny"><img src="../Images/a34f2fc0e09d86d5206f105e73cd16ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1116/format:webp/1*RD9Q4rBh5ft7aRjTveq1RQ.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">Softmax函数的定义。图片作者。</p></figure><p id="aa52" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">可以看到，每个元素依赖于所有剩余的<em class="ln"> N_C-1 </em>元素的值。这很好，因为我们希望概率被标准化。事实上，由于指数函数的<a class="ae lo" href="https://byjus.com/maths/exponential-functions/" rel="noopener ugc nofollow" target="_blank">属性，<em class="ln"> softmax </em>强制所有元素为<em class="ln">x _ I</em>T24】0，并且所有元素的总和为1。这样，应用于<em class="ln">逻辑</em>的<em class="ln"> softmax </em>的输出可以被解释为跨可能类别的概率向量。</a></p><p id="e474" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">然而，需要注意的一点是，指数函数可能会使大数字变得更大，以至于它们可能不再适合计算机的内存。幸运的是，有一个常用的技巧来避免这种情况。在我们只有三个类别的情况下，更容易看到它的作用。</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi nz"><img src="../Images/e9d7e3ade5e573130fbea86728082248.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zDmm8Lka50o2S5EuNuuVQQ.png"/></div></div><p class="mb mc gj gh gi md me bd b be z dk translated">Softmax函数应用于具有三个元素的logit向量。</p></figure><p id="41e9" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">首先，我们确定向量的最大元素。假设是<em class="ln"> x_3 </em>。然后，我们将logits向量转换如下:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi oa"><img src="../Images/6a8067d87d9e65a65d15c2ec7f544a86.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*68ZAq2dYQfSmOTXa3aXCXQ.png"/></div></div></figure><p id="094b" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">记住e⁰=1，我们现在应用<em class="ln"> softmax </em>:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi ob"><img src="../Images/2217bbafb7539888a3ebcdb52fd0765e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wz2MuUmgt8d4uVmPNndO9A.png"/></div></div></figure><p id="f660" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">如果你不相信这个向量与没有减去<em class="ln"> x_3 </em>的向量相同，看看第三个元素:</p><figure class="lq lr ls lt gt lu gh gi paragraph-image"><div role="button" tabindex="0" class="lv lw di lx bf ly"><div class="gh gi oc"><img src="../Images/4ad36245c37fee5e13e052665da0bf87.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hit93VZbk4sXkx1uR6c6ZQ.png"/></div></div></figure><p id="dc2a" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">因此，由于指数函数的一个属性，我们可以从<em class="ln"> softmax </em>的计算中“移除”最大的元素，并可能避免内存问题。我觉得这个解决方案至少非常优雅！</p><p id="8947" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">这里有一个利用<a class="ae lo" href="https://numpy.org/" rel="noopener ugc nofollow" target="_blank"> NumPy </a>的快速python实现:</p><figure class="lq lr ls lt gt lu"><div class="bz fp l di"><div class="od oe l"/></div><p class="mb mc gj gh gi md me bd b be z dk translated">softmax的Python实现，使用NumPy计算指数。</p></figure><h2 id="5532" class="mf mg it bd mh mi mj dn mk ml mm dp mn la mo mp mq le mr ms mt li mu mv mw iz bi translated"><strong class="ak">结论</strong></h2><p id="1bfe" class="pw-post-body-paragraph kr ks it kt b ku mx kd kw kx my kg kz la mz lc ld le na lg lh li nb lk ll lm im bi translated">我经常发现交叉熵的大多数解释令人困惑，因为他们把车放在了马的前面。事实上，我们可以从函数定义开始，然后让读者相信这是正确的，但是我更喜欢从头开始。</p><p id="ad69" class="pw-post-body-paragraph kr ks it kt b ku kv kd kw kx ky kg kz la lb lc ld le lf lg lh li lj lk ll lm im bi translated">我希望这篇帖子澄清为什么我们在多类分类任务中需要这样的函数，为什么我们不能只在损失函数中使用模型输出。</p></div></div>    
</body>
</html>