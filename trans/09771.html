<html>
<head>
<title>Regression for Classification | Hands on Experience</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分类回归|实践经验</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/regression-for-classification-hands-on-experience-8754a909a298?source=collection_archive---------10-----------------------#2021-09-13">https://towardsdatascience.com/regression-for-classification-hands-on-experience-8754a909a298?source=collection_archive---------10-----------------------#2021-09-13</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0b17" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">具有核心概念的逻辑回归和<em class="ki"> Softmax </em>回归</h2></div><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi kj"><img src="../Images/2cb61711246abff4e75b839bf8b33598.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OxZ9VTsqvNYko58C4_axYA.jpeg"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated"><a class="ae kz" href="https://unsplash.com/@lanceanderson?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">兰斯·安德森</a>在<a class="ae kz" href="https://unsplash.com/s/photos/architecture?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍照</p></figure><p id="0bcc" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在我们的生活中，我们都开发了许多回归模型。但是只有少数人熟悉使用回归模型进行分类。所以我的意图是揭示这个隐藏世界的美丽。</p><p id="e1a5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">众所周知，当我们想要从多个自变量中预测一个连续的因变量时，我们使用线性/多项式回归。但是到了分类的时候，就不能再用那个了。</p><blockquote class="lw"><p id="a5b1" class="lx ly it bd lz ma mb mc md me mf lv dk translated">基本上，分类是预测一个标签，回归是预测一个数量。</p></blockquote><p id="b164" class="pw-post-body-paragraph la lb it lc b ld mg ju lf lg mh jx li lj mi ll lm ln mj lp lq lr mk lt lu lv im bi translated">为什么线性回归不能用于分类？主要原因是预测值是连续的，而不是概率性的。所以我们无法得到一个确切的类来完成分类。浏览下面的预测，你会进一步理解。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/4b74cf58d4c6657124404c53eab08170.png" data-original-src="https://miro.medium.com/v2/resize:fit:1100/format:webp/1*Ju6gDSXoqQlubcQHprgBoQ.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">作者图片</p></figure><p id="07f9" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">概率介于0和1之间。但是在线性回归中，我们预测的是一个绝对数字，其范围可以在0和1之外。</p><p id="0d85" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">是的，您仍然可以将该值标准化到0–1的范围内，但是结果可能会更糟。这是因为线性回归拟合受包含异常值的影响很大。即使一个小的异常值也会破坏你的分类。</p><p id="a501" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">另一方面，使用线性回归进行多类预测是没有意义的。线性回归假设顺序在0、1和2之间，而在分类体系中，这些数字仅仅是分类占位符。</p><p id="2df5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">为了克服上述问题，有两个很好的解决方案。</p><ol class=""><li id="621e" class="mm mn it lc b ld le lg lh lj mo ln mp lr mq lv mr ms mt mu bi translated">逻辑回归—用于二元分类</li><li id="d29d" class="mm mn it lc b ld mv lg mw lj mx ln my lr mz lv mr ms mt mu bi translated">Softmax回归-用于多类分类</li></ol><p id="f8d0" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我用红酒质量数据集(在Kaggle中)向你展示这一点。</p><div class="na nb gp gr nc nd"><a href="https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009" rel="noopener  ugc nofollow" target="_blank"><div class="ne ab fo"><div class="nf ab ng cl cj nh"><h2 class="bd iu gy z fp ni fr fs nj fu fw is bi translated">红酒质量</h2><div class="nk l"><h3 class="bd b gy z fp ni fr fs nj fu fw dk translated">用于回归或分类建模的简单明了的实践数据集</h3></div><div class="nl l"><p class="bd b dl z fp ni fr fs nj fu fw dk translated">www.kaggle.com</p></div></div><div class="nm l"><div class="nn l no np nq nm nr kt nd"/></div></div></a></div><p id="31bc" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">原始数据集在<a class="ae kz" href="https://archive.ics.uci.edu/ml/datasets/wine+quality" rel="noopener ugc nofollow" target="_blank"> UCI机器学习知识库</a>中公开。</p><blockquote class="ns nt nu"><p id="2be5" class="la lb nv lc b ld le ju lf lg lh jx li nw lk ll lm nx lo lp lq ny ls lt lu lv im bi translated">注意:我今天不会执行任何详细的预处理或降维技术，因为我的意图是主要在分类模型上向您介绍。</p></blockquote><p id="4f82" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">好吧！首先，我们将看到如何使用逻辑回归实现二元分类。</p><h1 id="68c9" class="nz oa it bd ob oc od oe of og oh oi oj jz ok ka ol kc om kd on kf oo kg op oq bi translated">逻辑回归</h1><p id="1c94" class="pw-post-body-paragraph la lb it lc b ld or ju lf lg os jx li lj ot ll lm ln ou lp lq lr ov lt lu lv im bi translated">在应用模型之前，让我们先了解一下逻辑回归中的一些核心概念。</p><p id="8861" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我将通过一个例子向您展示逻辑回归是如何工作的。考虑投掷硬币的两种结果概率。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ow"><img src="../Images/625b2384ee0c2101e96ee2d2f08bd409.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MAWqZCghkXS5K_z3vXYb1Q.jpeg"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">作者图片</p></figure><p id="5445" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">让我们把显示头部的概率作为p，显示尾部的概率作为q，然后我们定义一个新概念叫做<strong class="lc iu">赔率</strong>作为<strong class="lc iu"> p/q. </strong></p><p id="df27" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">在这种情况下，我们只有两种可能。那不是头就是尾。所以我们可以把q写成1-p，现在我们将看到p的变化，它可以从0变化到1。我们取3个公平点，用赔率来分析。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ox"><img src="../Images/d0eda7cb6addf701a88da66f4b078fd0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4NKG9jEPmxA9Brq6D6pDrw.jpeg"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">作者图片</p></figure><p id="b7ca" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">现在我们可以得到几率的对数。那么它将被称为<strong class="lc iu"> Log Odds </strong>或<strong class="lc iu">Logit【Log(p/q)】</strong>。这些对数优势仅适用于2个类别的值条件。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi oy"><img src="../Images/5546de07994abb55342fe5b15fafd88f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*FLJl3xA8tneatbzR3dLXJw.jpeg"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">作者图片</p></figure><p id="a9ba" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">如你所见，它呈现出对称分布。当p=0.5时，Logit函数不偏向任何人。当p=1时，受到+inf的青睐，同时q (p-1)受到-inf的青睐。这个特征是逻辑回归的直觉，因为它可以用它的性质对二元问题进行分类。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi oz"><img src="../Images/c618d54b54167969753d28098baf5eb6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*A1ys5AXhS93pMxpGnp1AWQ.jpeg"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">作者图片</p></figure><p id="81fb" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">然后我们可以从这个Logit函数推导出逻辑函数。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi pa"><img src="../Images/95c1eb2ef8703853808dc655e508da52.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zkk6jGpHWacRN_RjSZkKOQ.jpeg"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">作者图片</p></figure><p id="db5d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">如果我们绘制图表，它将被视为如下。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi pb"><img src="../Images/90dd98a8d101c14e7a99c8606e36e313.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_9meYARfvhn5kbgjJj-1OA.jpeg"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">作者图片</p></figure><p id="55e1" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">下图简要显示了逻辑回归的工作原理。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi pc"><img src="../Images/b837ded4ab7a7b70be97d6e14327c58f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*vDPKFL7jR3E5_M2mPuDhmg.jpeg"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">作者图片</p></figure><p id="e492" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">您可以更深入地挖掘片段，如分析损失函数(二元交叉熵)和制定决策边界。这超出了本文的范围，因为需要更多的时间来解释。让我们看看实现，没有任何进一步的原因。</p><p id="7e31" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">首先，让我们将CSV文件加载到熊猫数据框架中。</p><pre class="kk kl km kn gt pd pe pf pg aw ph bi"><span id="3dc7" class="pi oa it pe b gy pj pk l pl pm">wineData = pd.read_csv('winequality-red.csv')<br/>wineData.head()</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi pn"><img src="../Images/c706f724ba748f81cfd8e416235a045e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cPevR3AabXdHzpRzvXuguw.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">作者图片</p></figure><p id="4dd4" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这里我们将质量作为目标列。</p><pre class="kk kl km kn gt pd pe pf pg aw ph bi"><span id="535e" class="pi oa it pe b gy pj pk l pl pm">wineData.quality.unique()</span></pre><p id="039c" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们可以把葡萄酒的质量分为3到8级</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi po"><img src="../Images/400dc26744abf213dc77d070d225b0dd.png" data-original-src="https://miro.medium.com/v2/resize:fit:496/format:webp/1*dXtgzAOAvnIzZs3ZGe_zKA.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">作者图片</p></figure><p id="1c50" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这其实是一个多类分类问题。(有6节课)我将在softmax回归上解释这个场景。为了解释逻辑回归，我添加了另一个具有以下条件的列。</p><ul class=""><li id="2eea" class="mm mn it lc b ld le lg lh lj mo ln mp lr mq lv pp ms mt mu bi translated">如果葡萄酒质量大于或等于6 = &gt;“好”(编码为1)</li><li id="050d" class="mm mn it lc b ld mv lg mw lj mx ln my lr mz lv pp ms mt mu bi translated">否则= &gt;“坏的”(编码为0)</li></ul><p id="78de" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">添加类别列后，我删除了质量列。所以现在我们的目标将是类别列。</p><pre class="kk kl km kn gt pd pe pf pg aw ph bi"><span id="928e" class="pi oa it pe b gy pj pk l pl pm">wineData['category'] = np.where(wineData['quality'] &gt;= 6, 1, 0)<br/>wineData = wineData.drop(['quality'], axis = 1)<br/>wineData.head()</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi pq"><img src="../Images/e6f13e33dcab4c303c729449d9f9334e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*TRc7-mJIKeAcetdVYKfjEA.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">作者图片</p></figure><p id="fe34" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">让我们来看看数据分布，以观察任何阶级的不平衡。如果您为类别列绘制计数图，您将看到如下图。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/a86b796e4dd9c076c5a05bf12c2555db.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*J-TroDR75t7wbpjnT4u8Iw.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">作者图片</p></figure><p id="0961" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们可以看到阶级不平衡。你可以丢弃一些数据点，或者使用<strong class="lc iu">重采样</strong> <strong class="lc iu">技术</strong>如欠采样(例如:接近缺失)或过采样(例如:SMOTE)来克服这个问题。至于简单性，我放弃了数据点。之后，您可以观察到一个平衡的数据集，如下所示。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/73a191eea5e70379c41a49dc22ecd942.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*iQjsl7JinD8ltuUOfIB-og.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">作者图片</p></figure><p id="38ae" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">您可以进一步分析特征之间的任何相关性。这是所有列的相关矩阵。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi ps"><img src="../Images/d1f0b09c98cebcf4c9b8b746e24f04bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xvuy0jk3r8NdTkGzS_ibXg.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">作者图片</p></figure><p id="4e16" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">让我们对此做一些小的预处理。首先，我将分离数据集进行训练和测试，因为我们需要查看准确性度量。</p><pre class="kk kl km kn gt pd pe pf pg aw ph bi"><span id="1114" class="pi oa it pe b gy pj pk l pl pm">target = wineData['category'].copy()<br/>features = wineData.drop('category', 1)</span><span id="2ece" class="pi oa it pe b gy pt pk l pl pm">X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.2, random_state = 101)</span></pre><p id="856e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">数据集没有正确标准化。在应用模型之前，让我们对数据应用标准的缩放函数。</p><pre class="kk kl km kn gt pd pe pf pg aw ph bi"><span id="5d01" class="pi oa it pe b gy pj pk l pl pm">scaler = StandardScaler()</span><span id="799f" class="pi oa it pe b gy pt pk l pl pm"># we only fit the training data to scaler<br/>scaler.fit(X_train)</span><span id="e6b2" class="pi oa it pe b gy pt pk l pl pm">train_scaled = scaler.transform(X_train)<br/>test_scaled = scaler.transform(X_test)</span><span id="b155" class="pi oa it pe b gy pt pk l pl pm">X_train = pd.DataFrame(train_scaled, columns = X_train.columns)<br/>X_test = pd.DataFrame(test_scaled, columns = X_test.columns)</span></pre><p id="f5a1" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">标准化后，训练集如下。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi pu"><img src="../Images/00d8653be4d05fad772432608935df38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sVs6JTgwAj3wAJdI2pa9wA.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">作者图片</p></figure><p id="1041" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">现在我们可以应用逻辑回归模型。Scikit-learn库为逻辑回归提供了一个简单的实现。</p><pre class="kk kl km kn gt pd pe pf pg aw ph bi"><span id="d824" class="pi oa it pe b gy pj pk l pl pm">from sklearn.linear_model import LogisticRegression<br/> <br/>logReg = LogisticRegression()<br/>logReg.fit(X_train,y_train)</span></pre><p id="012e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">太容易了吧！我们可以通过调用<code class="fe pv pw px pe b">model.predict</code>函数来获得测试集的模型预测。</p><pre class="kk kl km kn gt pd pe pf pg aw ph bi"><span id="cdc3" class="pi oa it pe b gy pj pk l pl pm">predictions = logReg.predict(X_test)<br/>y_hat = pd.DataFrame(predictions, columns=["predicted"])</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi py"><img src="../Images/113c41ebf8308426a8caf05f9f1a1f80.png" data-original-src="https://miro.medium.com/v2/resize:fit:268/format:webp/1*NshHpB9rI0iWoSLSkExGeA.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">作者图片</p></figure><p id="b0fd" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">嘣！没有任何连续值。如你所见，我们的预测以类的形式出现。进一步的精度测量可以计算如下。</p><pre class="kk kl km kn gt pd pe pf pg aw ph bi"><span id="b2d6" class="pi oa it pe b gy pj pk l pl pm">from sklearn.metrics import classification_report<br/>print(classification_report(y_test,y_hat))</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi pz"><img src="../Images/d058a10eaaca70499a543f48e9d1b725.png" data-original-src="https://miro.medium.com/v2/resize:fit:1002/format:webp/1*8VO2gnfN08KmKtM9onkehg.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">作者图片</p></figure><p id="0ac0" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这里我们只做了标准化就达到了76%的准确率。此外，您可以使用异常值处理、转换和离散化技术来改进这个模型。</p><p id="e2e9" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">让我们也画出混淆矩阵。</p><pre class="kk kl km kn gt pd pe pf pg aw ph bi"><span id="ac2e" class="pi oa it pe b gy pj pk l pl pm">from sklearn.metrics import confusion_matrix</span><span id="39b2" class="pi oa it pe b gy pt pk l pl pm">cf_matrix=confusion_matrix(y_test,y_hat)<br/>ax= plt.subplot()<br/>sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True,fmt='.2%', cmap='Blues', ax=ax);  <br/>ax.set_xlabel('Predicted labels');<br/>ax.set_ylabel('True labels'); <br/>ax.set_title('Confusion Matrix');</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi qa"><img src="../Images/61947ec4214637a35ea7a76ec19c39a4.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*csGne8gU6yyeAs0xQRAktQ.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">作者图片</p></figure><p id="6601" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">从混淆矩阵的结果中，我们可以看到我们的模型表现得相当好。</p><p id="de96" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">太好了！我们成功地实现了逻辑回归模型。现在让我们看看softmax回归是如何工作的。</p><h1 id="4f83" class="nz oa it bd ob oc od oe of og oh oi oj jz ok ka ol kc om kd on kf oo kg op oq bi translated">Softmax回归</h1><p id="f086" class="pw-post-body-paragraph la lb it lc b ld or ju lf lg os jx li lj ot ll lm ln ou lp lq lr ov lt lu lv im bi translated">正如我之前提到的，Softmax回归用于多类分类。希望大家还记得逻辑回归的最后一张图。Softmax的工作方式与此类似，但这里我们有多个类。请参见下图。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi qb"><img src="../Images/e808e7b8e0f62f1fe2dafe474d2aa2ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J0eJoYeRLnANtZ-A3Wjq7g.jpeg"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">作者图片</p></figure><p id="5d15" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">对于一个给定的数据点，我们逐个计算那个数据点属于每一类的概率，然后就可以在其中找到最大的概率。它将是该数据点的正确类别。</p><p id="f7a0" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">让我们看看计算是如何进行的。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi qc"><img src="../Images/9b32be8021b1c386e19465a4109a7e17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7kwolZHvD8MvZp-QJRud_w.jpeg"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">作者图片</p></figure><p id="d489" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这被称为Softmax函数。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi qd"><img src="../Images/16896264fa53f0ff76010ac86adeeafc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hdSI6wANuIc4IObLQe-ySw.jpeg"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">作者图片</p></figure><p id="3048" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">为了估计w权重，我们需要最小化损失函数。它被称为范畴交叉熵。</p><p id="93cc" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">最终，我们可以找到max(softmax)来预测正确的类。</p><p id="0ddb" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">好吧！让我们亲身体验一下。之前我已经将葡萄酒质量分为好的和坏的。这里，我将原始列离散化为</p><ul class=""><li id="0b32" class="mm mn it lc b ld le lg lh lj mo ln mp lr mq lv pp ms mt mu bi translated">如果葡萄酒质量大于或等于7 = &gt;“好”(编码为2)</li><li id="cb4e" class="mm mn it lc b ld mv lg mw lj mx ln my lr mz lv pp ms mt mu bi translated">如果葡萄酒质量等于6 = &gt;“好”(编码为1)</li><li id="34a9" class="mm mn it lc b ld mv lg mw lj mx ln my lr mz lv pp ms mt mu bi translated">否则= &gt;“坏的”(编码为0)</li></ul><p id="013c" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">所以这将把我们引向一个多类分类问题。(有3个类)让我们检查数据分布。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/f2f788090ffecb88397edc650cb6b389.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*NVzx6p_OmbRTSRPlQC0egA.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">作者图片</p></figure><p id="7eef" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这里我们也可以看到阶级的不平衡。让我们去掉一些数据点来消除不平衡。完整的实现可以在我的colab笔记本上看到。删除后，数据将如下所示。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/533dd0efcbf71e68d045128c320a177e.png" data-original-src="https://miro.medium.com/v2/resize:fit:778/format:webp/1*itHTvCA-7eyNfJ_PJeps0w.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">作者图片</p></figure><blockquote class="ns nt nu"><p id="2585" class="la lb nv lc b ld le ju lf lg lh jx li nw lk ll lm nx lo lp lq ny ls lt lu lv im bi translated">注意:如果要应用重采样技术，应该仅将其应用于训练数据。(分成训练和测试后)</p></blockquote><p id="6a08" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">好吧！让我们做一个训练测试分割，并应用一些预处理技术。拆分和标准化与之前相同。标准化之后，你的训练集如下。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div role="button" tabindex="0" class="kp kq di kr bf ks"><div class="gh gi qe"><img src="../Images/7ae86860ac560d978cc231fe7ea57ace.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PpiaikJOPGRvci_v3vix3Q.png"/></div></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">作者图片</p></figure><p id="722c" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">现在让我们应用Softmax回归。</p><p id="cb42" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">我们将再次使用scikit-learn中的逻辑回归，但是我们需要将multi_class参数设置为<code class="fe pv pw px pe b">multinomial</code>,以便该函数执行softmax回归。我们还需要一个支持softmax回归的解算器，比如solver='lbfgs '。</p><p id="a336" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这些求解器用于查找最小化成本函数的参数权重。</p><pre class="kk kl km kn gt pd pe pf pg aw ph bi"><span id="3667" class="pi oa it pe b gy pj pk l pl pm">softReg = LogisticRegression(multi_class = 'multinomial', solver = 'lbfgs')<br/>softReg.fit(X_train,y_train)</span></pre><p id="03d5" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">测试数据的模型预测如下。</p><pre class="kk kl km kn gt pd pe pf pg aw ph bi"><span id="a200" class="pi oa it pe b gy pj pk l pl pm">predictions = softReg.predict(X_test)<br/>y_hat = pd.DataFrame(predictions, columns=["predicted"])<br/>print(y_hat.head())</span></pre><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi qf"><img src="../Images/6490069f722459433cd4801bd9dad183.png" data-original-src="https://miro.medium.com/v2/resize:fit:292/format:webp/1*5RKruVG5ASQAAkt2fJdUqw.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">作者图片</p></figure><p id="3ded" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">很简单，对吗？因此，我们的softmax回归模型能够对多类问题进行分类。让我们看看评价。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi qg"><img src="../Images/543d66c7d15ba3a91419a6b049ba41c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*l_2JSpG88KuCo-Iet8CBpA.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">作者图片</p></figure><p id="b41e" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">从分类报告中，我们可以看到70%的准确率。这实际上并不坏，因为我们没有去深度预处理阶段。正如我前面说过的，可以通过适当的预处理技术和重采样技术来进一步增强这一点。</p><figure class="kk kl km kn gt ko gh gi paragraph-image"><div class="gh gi qh"><img src="../Images/89a3d3fb775cee9c4446f817695367c1.png" data-original-src="https://miro.medium.com/v2/resize:fit:758/format:webp/1*Xtgq5azQWqLwEpNmEBhIDA.png"/></div><p class="kv kw gj gh gi kx ky bd b be z dk translated">作者图片</p></figure><p id="f6ed" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">如混淆矩阵所示，我们的模型很好地分类了0和2类，但在识别1类时有一些问题。你可以试试上面建议的方法，然后告诉我结果。我的笔记本会支持你的实现。</p><p id="2978" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">太好了！现在你知道如何使用回归进行分类了。</p><h1 id="9b19" class="nz oa it bd ob oc od oe of og oh oi oj jz ok ka ol kc om kd on kf oo kg op oq bi translated">资源</h1><ul class=""><li id="4746" class="mm mn it lc b ld or lg os lj qi ln qj lr qk lv pp ms mt mu bi translated">完整的合作实验室Python笔记本。</li></ul><div class="na nb gp gr nc nd"><a href="https://colab.research.google.com/drive/1QyVpFToKva_NceVKUzQ1cbYmKEbTSFsr?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="ne ab fo"><div class="nf ab ng cl cj nh"><h2 class="bd iu gy z fp ni fr fs nj fu fw is bi translated">分类回归</h2><div class="nk l"><h3 class="bd b gy z fp ni fr fs nj fu fw dk translated">由Yasas Sandeepa创作</h3></div><div class="nl l"><p class="bd b dl z fp ni fr fs nj fu fw dk translated">colab.research.google.com</p></div></div><div class="nm l"><div class="ql l no np nq nm nr kt nd"/></div></div></a></div><ul class=""><li id="88a0" class="mm mn it lc b ld le lg lh lj mo ln mp lr mq lv pp ms mt mu bi translated">红酒质量数据集。</li></ul><p id="4f03" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">这个数据集在数据库内容许可(DbCL)1.0版下，在<a class="ae kz" href="https://archive.ics.uci.edu/ml/datasets/wine+quality" rel="noopener ugc nofollow" target="_blank"> UCI机器学习库</a>和Kaggle中公开提供。</p><div class="na nb gp gr nc nd"><a href="https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009" rel="noopener  ugc nofollow" target="_blank"><div class="ne ab fo"><div class="nf ab ng cl cj nh"><h2 class="bd iu gy z fp ni fr fs nj fu fw is bi translated">红酒质量</h2><div class="nk l"><h3 class="bd b gy z fp ni fr fs nj fu fw dk translated">用于回归或分类建模的简单明了的实践数据集</h3></div><div class="nl l"><p class="bd b dl z fp ni fr fs nj fu fw dk translated">www.kaggle.com</p></div></div><div class="nm l"><div class="qm l no np nq nm nr kt nd"/></div></div></a></div><p id="0a7c" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">[1] Paulo Cortez，葡萄牙吉马雷斯米尼奥大学，<a class="ae kz" href="http://www3.dsi.uminho.pt/pcortez" rel="noopener ugc nofollow" target="_blank">http://www3.dsi.uminho.pt/pcortez</a>a . Cerdeira，F. Almeida，T. Matos和J. Reis，葡萄牙波尔图Vinho Verde地区葡萄栽培委员会，2009年</p><h1 id="37a2" class="nz oa it bd ob oc od oe of og oh oi oj jz ok ka ol kc om kd on kf oo kg op oq bi translated">结论</h1><p id="020c" class="pw-post-body-paragraph la lb it lc b ld or ju lf lg os jx li lj ot ll lm ln ou lp lq lr ov lt lu lv im bi translated">今天我们学习了如何使用回归技术来解决分类问题。具体来说，我们学习了逻辑回归和softmax回归中的主要直觉和核心概念。此外，我们已经从头开始实现了这两个模型。</p><p id="517d" class="pw-post-body-paragraph la lb it lc b ld le ju lf lg lh jx li lj lk ll lm ln lo lp lq lr ls lt lu lv im bi translated">感谢您通读这篇文章。希望对你有帮助。欢迎留言，提出您的宝贵意见和建议。关注我获取<a class="ae kz" href="https://medium.com/@yasassandeepa007" rel="noopener">介质</a>最新文章。注意安全！快乐学习！❤️</p></div></div>    
</body>
</html>