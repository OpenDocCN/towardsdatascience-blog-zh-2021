<html>
<head>
<title>pytorch-widedeep, deep learning for tabular data IV: Deep Learning vs LightGBM</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">pytorch-widedeep，用于表格数据的深度学习IV:深度学习与LightGBM</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pytorch-widedeep-deep-learning-for-tabular-data-iv-deep-learning-vs-lightgbm-cadcbf571eaf?source=collection_archive---------19-----------------------#2021-06-13">https://towardsdatascience.com/pytorch-widedeep-deep-learning-for-tabular-data-iv-deep-learning-vs-lightgbm-cadcbf571eaf?source=collection_archive---------19-----------------------#2021-06-13</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="7309" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/making-sense-of-big-data" rel="noopener" target="_blank">理解大数据</a></h2><div class=""/><div class=""><h2 id="95c1" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">对于分类和回归问题的表格数据，DL算法和LightGBM之间的彻底比较</h2></div><p id="8f93" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在这里，我们继续这个系列的另一篇文章。这是<a class="ae lk" href="https://jrzaurin.github.io/infinitoml/" rel="noopener ugc nofollow" target="_blank">系列</a>的第四部。之前的三个帖子，以及<a class="ae lk" href="https://jrzaurin.github.io/infinitoml/2021/02/18/pytorch-widedeep_iii.html" rel="noopener ugc nofollow" target="_blank">这个帖子</a>的原始版本都托管在我自己的<a class="ae lk" href="https://jrzaurin.github.io/infinitoml/" rel="noopener ugc nofollow" target="_blank">博客</a>里，以防万一。</p><p id="26d4" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">几个月前，当我发布了库<a class="ae lk" href="https://github.com/jrzaurin/pytorch-widedeep" rel="noopener ugc nofollow" target="_blank"> pytorch-widedeep </a>的最后一个测试版本(<code class="fe ll lm ln lo b">0.4.8</code>)时，我就开始计划这篇文章了。然而，从那时起，一些事情变得优先，这意味着运行我运行的数百个实验(可能超过1500个)，花费了我比我预期多得多的时间。尽管如此，我们还是来了。</p><p id="c9cb" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这个项目的所有深度学习模型都在一个<code class="fe ll lm ln lo b">p2.xlarge</code>实例上运行，所有的<code class="fe ll lm ln lo b">LightGBM</code>实验都在我的Mac <code class="fe ll lm ln lo b">Mid 2015</code>上运行。</p><p id="cdd4" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">首先，我要感谢AWS社区建设者们，尤其是卡梅隆，让我在AWS周围的生活变得更加轻松。</p><p id="98ab" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这个项目的所有深度学习模型都在一个<code class="fe ll lm ln lo b">p2.xlarge</code>实例上运行，所有的<code class="fe ll lm ln lo b">LightGBM</code>实验都在我的Mac <code class="fe ll lm ln lo b">Mid 2015</code>上运行。</p><p id="277e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">一旦得到了适当的认可，让我告诉你一些关于所有这些实验和这篇文章的背景。</p><h2 id="5f5d" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">1.简介:为什么会这样？</h2><p id="57d0" class="pw-post-body-paragraph ko kp iq kq b kr mh ka kt ku mi kd kw kx mj kz la lb mk ld le lf ml lh li lj ij bi translated">嗯，在过去的几年里，特别是去年，我一直在努力改进<a class="ae lk" href="https://github.com/jrzaurin/pytorch-widedeep" rel="noopener ugc nofollow" target="_blank"> pytorch-widedeep </a>。这真的很有趣，我学到了很多。然而，当我向库中添加模型时，尤其是表格组件(见<a class="ae lk" href="https://pytorch-widedeep.readthedocs.io/en/latest/model_components.html" rel="noopener ugc nofollow" target="_blank">这里的</a>)，我想知道除了学习这些模型本身之外，它是否还有其他用途。你看，我是一名教育科学家，我在学术界呆了十多年。在那里，我们曾经做了很多不太有用的事情，很酷(有时)，但不是很有用。几年前，驱使我进入私营部门的一个原因是寻找一种“有用”的感觉，在那里我可以建造一些既有科学意义又有用的东西。考虑到这一点，原谅我的多余，我希望这个库是有用的。形容词“有用的”在这里可以指许多事情。这可能意味着直接使用库，或者派生回购并使用代码，或者只是复制并粘贴给定项目的部分代码。然而，最终，我想回答的一个问题是:<em class="mm">这些模型比其他更“标准”的模型(如GBMs)的性能更好，甚至更好吗？</em>。注意，我写的是“<em class="mm">一个问题</em>”，而不是“<em class="mm">问题</em>”。稍后会有更多关于这个的报道。</p><p id="25ec" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">当然，我不是第一个将深度学习(以下简称DL)方法与GBMs进行表格数据比较的人，也不会是最后一个。事实上，在我写这几行字的时候，一篇新论文:<a class="ae lk" href="https://arxiv.org/pdf/2106.03253.pdf" rel="noopener ugc nofollow" target="_blank">表格数据:深度学习不是你需要的全部</a> [1]发表了。这篇文章和那篇文章非常相似，结论也完全一致。但是，也有一些不同之处。将DL算法与<code class="fe ll lm ln lo b">XGBoost</code>【2】和<code class="fe ll lm ln lo b">CatBoost</code>【3】进行比较，而我使用<code class="fe ll lm ln lo b">LightGBM</code>【4】(参见第2.3节对该算法使用的解释)。此外，我要说，我在这里使用的四个数据集中的三个比他们论文中的数据集更具挑战性，但这可能只是我的看法。最后，除了<code class="fe ll lm ln lo b">TabNet</code>之外，我这里用的DL模型和那篇论文里的是不一样的。尽管如此，在结论部分，我将写一些关于如何处理这个基准/测试练习的想法。</p><p id="ff69" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">除了那篇论文，在<em class="mm">发布新模型的所有</em>论文中，经常有DL架构和GBM之间的综合比较。我对其中一些出版物的主要警告如下:我经常无法在论文中重现结果(这当然可能是我的错)，我有时发现优化DL模型的努力比优化GBM的努力要多一些。最后但并非最不重要的一点是，有些论文中的结果表格缺乏一致性，有时令人困惑。例如，论文A将使用DL模型A来查找性能优于所有GBM的，通常为<code class="fe ll lm ln lo b">XGBoost</code>、<code class="fe ll lm ln lo b">Catboost</code>和<code class="fe ll lm ln lo b">LightGBM</code>。然后，论文B将推出新的DL模型B，其性能也将优于所有GBM，但在他们的论文中，结果表明模型A不再优于GBM。</p><p id="0abb" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">考虑到所有这些，我决定使用py torch-wide deep并运行一组相当大的实验，包括针对表格数据和<code class="fe ll lm ln lo b"><a class="ae lk" href="https://lightgbm.readthedocs.io/en/latest/#" rel="noopener ugc nofollow" target="_blank">LightGBM</a></code>的不同DL模型。</p><p id="72f7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在我继续之前，让我评论一下回购中的代码“质量”。人们必须记住，这里的目标是以严格的方式测试算法，而不是编写生产代码。如果你想看更好的代码，你可以去pytorch-widedeep 或者我的其他一些回复。只是说以防一些“纯粹主义者”试图浪费宇宙的时间。</p><h2 id="70b1" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">2.数据集和模型</h2><p id="8961" class="pw-post-body-paragraph ko kp iq kq b kr mh ka kt ku mi kd kw kx mj kz la lb mk ld le lf ml lh li lj ij bi translated">对于这里的实验，我使用了四个数据集和四个DL模型。</p><h2 id="0cd9" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">2.1数据集</h2><ol class=""><li id="c787" class="mn mo iq kq b kr mh ku mi kx mp lb mq lf mr lj ms mt mu mv bi translated"><a class="ae lk" href="https://archive.ics.uci.edu/ml/datasets/adult" rel="noopener ugc nofollow" target="_blank">成人普查</a>(二元分类)</li><li id="41c6" class="mn mo iq kq b kr mw ku mx kx my lb mz lf na lj ms mt mu mv bi translated"><a class="ae lk" href="https://archive.ics.uci.edu/ml/datasets/Bank+Marketing" rel="noopener ugc nofollow" target="_blank">银行营销</a>(二元分类)</li><li id="59f7" class="mn mo iq kq b kr mw ku mx kx my lb mz lf na lj ms mt mu mv bi translated"><a class="ae lk" href="https://www.kaggle.com/neomatrix369/nyc-taxi-trip-duration-extended" rel="noopener ugc nofollow" target="_blank">纽约市出租车乘坐时长</a>(回归)</li><li id="91f1" class="mn mo iq kq b kr mw ku mx kx my lb mz lf na lj ms mt mu mv bi translated"><a class="ae lk" href="https://archive.ics.uci.edu/ml/datasets/Facebook+Comment+Volume+Dataset" rel="noopener ugc nofollow" target="_blank">脸书评论卷</a>(回归)</li></ol><p id="3ff7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在<a class="ae lk" href="https://github.com/jrzaurin/tabulardl-benchmark" rel="noopener ugc nofollow" target="_blank"> repo </a>中的bash脚本<code class="fe ll lm ln lo b">get_data.sh</code>有你需要的所有信息来获取这些数据集，以防你想自己探索它们。当然，用于运行实验和再现结果的所有代码也可以在该报告中获得。</p><p id="536f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">以下是关于数据集的一些基本信息:</p><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="8f85" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">表1 </strong>。本帖中使用的数据集的基本信息</p><p id="4e83" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我选择这些数据集是有原因的。</p><p id="900b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">总的来说，我寻找的是二元、多类和回归数据集，这些数据集如果不是由分类特征主导，也有很多。这是因为根据我的经验，表格数据的DL模型在存在分类特征的大型数据集中变得更有用和更有竞争力(尽管[5]表明编码数字特征也能获得更好的结果),而且如果这些分类特征有很多类别。这是因为嵌入获得了更重要的价值，即我们学习那些分类特征的表示，这些分类特征编码了与所有其他特征以及特定数据集的目标的关系。请注意，使用GBMs时不会发生这种情况。即使使用了<a class="ae lk" href="https://maxhalford.github.io/blog/target-encoding/" rel="noopener ugc nofollow" target="_blank">目标编码</a>，实际上也没有太多的学习元素(当然仍然有用)。</p><p id="5461" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">当然，我们可以获取以数字特征为主的数据集，并以某种方式将它们绑定起来，使其成为分类数据集。然而，这对我来说似乎有点太“强迫”了。本着让这篇文章的内容尽可能接近真实用例的想法，我很难想到许多“真实世界”的场景，在这些场景中，我们被提供以数字特征为主的数据集，然后这些数据集在被馈送给算法之前被转换/分类。换句话说，我不想考虑那些为了比较GBM和DL模型而必须将数字特征绑定到分类中的数据集。</p><p id="d916" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">另一方面，我还寻找我已经熟悉的数据集，或者不需要太多功能工程就可以将数据传递给模型的数据集。这样，我也许可以在这方面节省一些时间，把更多的精力放在实验上，因为我打算进行大量的实验。最后，我寻找在某种程度上尽可能与“真实世界”中的数据集相似的数据集，但是具有易于处理的大小，以便我可以在合理的时间框架内进行实验。</p><p id="d497" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">虽然我确实找到了适合二元分类和回归的数据集，但我没有找到我特别喜欢的多类分类数据集(如果有人有任何建议，请在下面发表评论，我很高兴尝试一下)。也许将来我会包括<a class="ae lk" href="https://archive.ics.uci.edu/ml/datasets/covertype" rel="noopener ugc nofollow" target="_blank"> CoverType </a>数据集，但是是在UCI ML存储库中的那个，而不是Kaggle的平衡版本。现在，我将继续上面列举的四个。让我简单评论一下每个数据集。</p><p id="3398" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我会将<em class="mm">成人人口普查数据集</em>称为“<em class="mm">最简单的数据集</em>”，在这个意义上，简单的模型(即朴素贝叶斯分类器)已经可以在没有任何特征工程的情况下达到大约84%的准确率。就个人而言，我通常在现实世界中找不到这些好的数据集。然而，它是ML教程、帖子等最受欢迎和最著名的数据集之一，我最终决定将它包括在内。</p><p id="036c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="mm">银行营销</em>数据集也广为人知。这些数据与基于电话的直接营销活动相关，试图预测客户是否会订购产品。在这种情况下，提及几个相关方面是很重要的。首先我使用了<a class="ae lk" href="https://archive.ics.uci.edu/ml/datasets/Bank+Marketing" rel="noopener ugc nofollow" target="_blank">原始数据集</a>，这个数据集有点不平衡(正负类比是0.127)。第二，你可能会四处看看，发现有些人获得了比我稍后将展示的更好的结果。我发现的所有这样的案例要么使用Kaggle的平衡数据集，一个叫做<code class="fe ll lm ln lo b">duration</code>的特性，要么两者都用。<code class="fe ll lm ln lo b">duration</code>特性指的是通话的持续时间，是在通话后<strong class="kq ja">才知道的，对目标影响很大。因此，我没有在实验中使用它。这个数据集比成人数据集更像一个真实的用例，因为数据是不平衡的，预测根本不是一件容易的事情。尽管如此，数据量还是很小，也没有那么不平衡。</strong></p><p id="be7c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><em class="mm">纽约市出租车乘车时长</em>数据集也是众所周知的，是我使用的所有数据集中最大的。我们的目标是预测纽约市出租车出行的总时长。我没有从<a class="ae lk" href="https://www.kaggle.com/c/nyc-taxi-trip-duration" rel="noopener ugc nofollow" target="_blank"> Kaggle网站</a>获取数据集，而是从<a class="ae lk" href="https://www.kaggle.com/neomatrix369/nyc-taxi-trip-duration-extended" rel="noopener ugc nofollow" target="_blank">这里</a>手动下载了一个扩展版本，那里所有的功能工程都已经完成。</p><p id="c146" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">最后，<em class="mm">脸书评论卷</em>数据集是另一个理想的候选，因为它有一个很好的大小，并且所有的特征工程都是为我做的。我们的目标是预测帖子将收到的评论量。事实上，这个数据集最初是用来比较决策树和神经网络的。数据集和预处理的详细描述可以在<a class="ae lk" href="https://uksim.info/uksim2015/data/8713a015.pdf" rel="noopener ugc nofollow" target="_blank">原始出版物</a>【6】中找到。特别是，我在本文的实验中使用了他们的训练变量— 5数据集，它有199029行和54列。</p><p id="4a1b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在数据输入算法之前，数据准备步骤的所有代码可以在<a class="ae lk" href="https://github.com/jrzaurin/tabulardl-benchmark/tree/master/prepare_datasets" rel="noopener ugc nofollow" target="_blank">这里</a>找到</p><h2 id="5919" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">2.2.DL模型</h2><p id="4c20" class="pw-post-body-paragraph ko kp iq kq b kr mh ka kt ku mi kd kw kx mj kz la lb mk ld le lf ml lh li lj ij bi translated">正如我在帖子前面提到的，所有DL模型都是通过<a class="ae lk" href="https://github.com/jrzaurin/pytorch-widedeep" rel="noopener ugc nofollow" target="_blank"> pytorch-widedeep </a>运行的。该库提供了四个宽深模型<a class="ae lk" href="https://pytorch-widedeep.readthedocs.io/en/latest/model_components.html" rel="noopener ugc nofollow" target="_blank">组件</a> : <code class="fe ll lm ln lo b">wide</code>、<code class="fe ll lm ln lo b">deeptabular</code>、<code class="fe ll lm ln lo b">deeptext</code>、<code class="fe ll lm ln lo b">deepimage</code>。让我简要地评论一下其中的每一个。更多详情，请参见<a class="ae lk" href="https://jrzaurin.github.io/infinitoml/" rel="noopener ugc nofollow" target="_blank">伴随帖子</a>、<a class="ae lk" href="https://pytorch-widedeep.readthedocs.io/en/latest/model_components.html" rel="noopener ugc nofollow" target="_blank">文档</a>或<a class="ae lk" href="https://github.com/jrzaurin/pytorch-widedeep/tree/tabnet/pytorch_widedeep/models" rel="noopener ugc nofollow" target="_blank">源代码</a>本身。</p><ol class=""><li id="4692" class="mn mo iq kq b kr ks ku kv kx ni lb nj lf nk lj ms mt mu mv bi translated"><code class="fe ll lm ln lo b">wide</code>:这只是一个通过<code class="fe ll lm ln lo b">Embedding</code>层实现的线性模型</li><li id="3732" class="mn mo iq kq b kr mw ku mx kx my lb mz lf na lj ms mt mu mv bi translated"><code class="fe ll lm ln lo b">deeptabular</code>:该组件将处理“标准”表格数据(即分类和数字列)，有4个选项:</li></ol><p id="a8d5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">2.1 <code class="fe ll lm ln lo b">TabMlp</code>:简单的标准MLP。例如，非常类似于fastai库中的<a class="ae lk" href="https://docs.fast.ai/tabular.learner.html" rel="noopener ugc nofollow" target="_blank">表格api </a>实现。</p><p id="0bf8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">2.2 <code class="fe ll lm ln lo b">TabResnet</code>:类似于MLP，但是我没有使用密集层，而是使用Resnet块。</p><p id="4605" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">2.3 <code class="fe ll lm ln lo b">Tabnet</code> [7]:这是一个非常有趣的实现。很难用几句话来解释，因此我强烈建议阅读<a class="ae lk" href="https://arxiv.org/abs/1908.07442" rel="noopener ugc nofollow" target="_blank">论文</a>。<code class="fe ll lm ln lo b">Tabnet</code>旨在与GBMs竞争，并通过特性重要性提供模型可解释性。<code class="fe ll lm ln lo b">pytorch-widedeep</code>对<code class="fe ll lm ln lo b">Tabnet</code>的实现完全是基于dreamquark-ai的家伙们的奇妙<a class="ae lk" href="https://github.com/dreamquark-ai/Tabnet" rel="noopener ugc nofollow" target="_blank">实现</a>，因此，<strong class="kq ja">所有</strong>的功劳都归于他们。简单地说，我对它进行了修改，使其能够在一个宽而深的框架内工作，并添加了几个额外的功能，比如GLU块中的内部丢失和不使用ghost批处理规范化的可能性[8]。</p><p id="3002" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">注意，最初的实现允许分两个阶段进行训练。首先通过标准编码器-解码器方法进行自我监督训练，然后仅使用编码器进行监督训练或微调。在<code class="fe ll lm ln lo b">pytorch-widedeep</code>只实施监督培训(即编码器)。作者表明，无监督预训练主要在低数据量区域或当未标记数据集比标记数据集大得多时提高性能。因此，如果你处于其中一种场景中(或者简单地作为一个通用语句)，你最好使用dreamquark-ai的实现。</p><p id="e169" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">2.4.<code class="fe ll lm ln lo b">TabTransformer</code>【9】:这与<code class="fe ll lm ln lo b">TabResnet</code>类似，但是作者使用了Transformer [10]块，而不是Resnet块。与<code class="fe ll lm ln lo b">Tabnet</code>的情况类似，<code class="fe ll lm ln lo b">TabTransformer</code>允许两个阶段的训练过程，无监督的预训练，然后是有监督的训练或微调。<code class="fe ll lm ln lo b">pytorch-widedeep</code>对<code class="fe ll lm ln lo b">TabTransformer</code>的实施旨在以“标准”方式使用，即监督培训。注意与塞尔詹的结果一致。ark，Tomas Pfister针对<code class="fe ll lm ln lo b">Tabnet</code>，作者发现，无监督预训练主要在低数据量范围内或当未标记数据集远大于标记数据集时提高性能。<code class="fe ll lm ln lo b">pytorch-widedeep</code>中可用的<code class="fe ll lm ln lo b">TabTransformer</code>实现部分基于<a class="ae lk" href="https://github.com/awslabs/autogluon/tree/058398b61d1b2011f56a9dce149b0989adbbb04a/tabular/src/autogluon/tabular/models/tab_transformer" rel="noopener ugc nofollow" target="_blank">自动引导</a>库中的实现和王飞<a class="ae lk" href="https://github.com/lucidrains/tab-transformer-pytorch" rel="noopener ugc nofollow" target="_blank">这里</a>的实现。</p><p id="b386" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">3.<code class="fe ll lm ln lo b">deeptext</code>:由一堆rnn(lstm或gru)组成的标准文本分类器/回归器。此外，还可以选择在rnn堆栈之上添加一组密集层，以及其他一些额外功能。</p><p id="0709" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">4.<code class="fe ll lm ln lo b">deepimage</code>:使用预训练网络(特别是ResNets)或4个卷积层序列的标准图像分类器/回归器。此外，还可以选择在CNN和其他一些额外功能的基础上添加一组密集层。</p><h2 id="f749" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">2.3.为什么是<code class="fe ll lm ln lo b">LightGBM</code>？</h2><p id="057f" class="pw-post-body-paragraph ko kp iq kq b kr mh ka kt ku mi kd kw kx mj kz la lb mk ld le lf ml lh li lj ij bi translated">如果你和我一起工作过，或者甚至和我聊过一些ML项目，你会知道我最喜欢的算法之一是<code class="fe ll lm ln lo b"><a class="ae lk" href="https://lightgbm.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">LightGBM</a></code>。我广泛使用的是。事实上，我最近生产的3毫升系统都依赖于<code class="fe ll lm ln lo b">LightGBM</code>。即使不比它的兄弟姐妹(如<code class="fe ll lm ln lo b"><a class="ae lk" href="https://xgboost.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">XGBoost</a></code>或<code class="fe ll lm ln lo b"><a class="ae lk" href="https://catboost.ai/" rel="noopener ugc nofollow" target="_blank">CatBoost</a></code>)更好，它的表现也类似，速度明显更快，并支持分类特征(见<a class="ae lk" href="https://www.tandfonline.com/doi/abs/10.1080/01621459.1958.10501479" rel="noopener ugc nofollow" target="_blank">此处</a>)。尽管在支持分类特性方面<code class="fe ll lm ln lo b">CatBoost</code>可能是更好的解决方案)。此外，还提供了GBM通常的灵活性和性能。</p><h2 id="9cca" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">2.4.实验设置和其他注意事项</h2><p id="c2de" class="pw-post-body-paragraph ko kp iq kq b kr mh ka kt ku mi kd kw kx mj kz la lb mk ld le lf ml lh li lj ij bi translated">正如我在帖子前面提到的，我针对四个数据集运行了许多实验(并非所有实验都被记录和/或发布),重点关注可用于<code class="fe ll lm ln lo b">deeptabular</code>组件的不同模型。所有的实验运行都可以在<a class="ae lk" href="https://github.com/jrzaurin/tabulardl-benchmark/tree/master/run_experiments" rel="noopener ugc nofollow" target="_blank">这里</a>的repo中找到。</p><p id="1836" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">实验不仅考虑了模型的不同参数(即单元数量、层数等..)还包括不同的优化器、学习率调度器和训练过程。例如，所有实验都提前停止，大多数情况下有30个周期的<code class="fe ll lm ln lo b">patience</code>。我使用了三种不同的优化器(<code class="fe ll lm ln lo b">Adam</code>【11】、<code class="fe ll lm ln lo b">AdamW</code>【12】和<code class="fe ll lm ln lo b">RAdam</code>【13】)和三种不同的学习率调度器(<code class="fe ll lm ln lo b">ReduceLROnPlateau</code>、<code class="fe ll lm ln lo b">OneCycleLR</code>【14】、<code class="fe ll lm ln lo b">CyclicLR</code>【15】)。以下命令对应于一个实验运行:</p><pre class="nb nc nd ne gt nl lo nm nn aw no bi"><span id="c235" class="lp lq iq lo b gy np nq l nr ns">python adult/adult_tabmlp.py --mlp_hidden_dims [100,50] --mlp_dropout 0.2 --optimizer Adam --early_stop_patience 30 --lr_scheduler CyclicLR --base_lr 5e-4 --max_lr 0.01 --n_cycles 10 --n_epochs 100 --save_results</span></pre><p id="1cd2" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">上面的命令将为成人数据集运行一个<code class="fe ll lm ln lo b">TabMlp</code>模型。大多数<code class="fe ll lm ln lo b">args</code>都很容易理解。也许唯一值得一提的是，这个特定的实验是用一个<code class="fe ll lm ln lo b">CyclicLR</code>调度程序运行的，其中学习率在0.0005到0.01之间振荡，在100个时期内振荡10次(即每10个时期一个周期)。</p><p id="39ec" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">值得一提的是，在运行实验时，我假设在DL模型参数和训练设置中有一个固有的层次结构。因此，我没有一次优化所有的参数，而是选择了那些我认为更相关的参数，并运行了重现该层次结构的实验。例如，当运行一个简单的<code class="fe ll lm ln lo b">MLP</code>时，我假设层中神经元的数量是一个比我是否在最后一层使用<code class="fe ll lm ln lo b">BatchNorm</code>更重要的参数。这可能是，或者肯定是，最好的办法是一次优化所有参数，但遵循这种“分层”方法也让我感觉到改变一些单独的参数如何影响模型的性能。尽管如此，每个模型和每个数据集平均运行了大约100次实验，所以探索是相对详尽的(只是相对而言)。</p><p id="0a7d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">另一方面，<code class="fe ll lm ln lo b">LightGBM</code>通过使用<code class="fe ll lm ln lo b"><a class="ae lk" href="https://optuna.org/" rel="noopener ugc nofollow" target="_blank">Optuna</a></code>【16】、<code class="fe ll lm ln lo b"><a class="ae lk" href="https://github.com/hyperopt/hyperopt" rel="noopener ugc nofollow" target="_blank">Hyperopt</a></code>【17】或两者，并选择导致最佳度量的参数来优化。所有的代码都可以在<a class="ae lk" href="https://github.com/jrzaurin/tabulardl-benchmark" rel="noopener ugc nofollow" target="_blank">这里</a>找到。注意，实验和repo中的代码代表了一个关于如何使用<code class="fe ll lm ln lo b">pytorch-widedeep</code>(如果你想使用库)的非常详细和全面的教程。</p><p id="968c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">值得一提的是，在进行实验时，DL模型和<code class="fe ll lm ln lo b">LightGBM</code>的早期停止标准都是基于验证损失。或者，可以监控一个指标，例如f1分数的准确性。请注意，精确度(或f1)和损失并不一定完全成反比。可能存在边缘情况，其中算法确实不确定某些预测(即，预测接近度量阈值，导致高损失值)，但最终做出正确的预测(更高的准确度)。当然，在理想情况下，我们希望算法是确定的，并做出正确的预测，但你知道，现实世界是混乱和嘈杂的。尽管如此，出于好奇，我尝试在一些实验中监控指标。总的来说，我确实发现结果与那些监控损失值一致，尽管在某些情况下可以实现稍微好一点的指标。</p><p id="612a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">另一条相关信息与用于表示分类特征的嵌入数量有关。可以想象，这里的可能性是无限的，我必须找到一种方法，在所有实验中始终自动化这个过程。为此，我决定使用fastai的经验法则。对于给定的分类特征，嵌入的数量将是:</p><figure class="nb nc nd ne gt nf gh gi paragraph-image"><div role="button" tabindex="0" class="nu nv di nw bf nx"><div class="gh gi nt"><img src="../Images/2eb6e1e4e52db81624141e2f1144b63b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bIPpee2zc6eq91Dyz5TkWQ@2x.png"/></div></div><p class="oa ob gj gh gi oc od bd b be z dk translated">Eq 1。Fastai的嵌入数经验法则</p></figure><p id="c9eb" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">例外的是<code class="fe ll lm ln lo b">TabTransformer</code>。<code class="fe ll lm ln lo b">TabTransformer</code>将分类特征视为序列的一部分(即上下文相关),其中序列顺序无关紧要，即不需要位置编码。因此，它们不是“一个挨着一个”堆叠，而是“一个在另一个上面”堆叠。这意味着所有分类特征必须具有相同的维度。请注意，当数据集中的分类要素有大量类别时，这有点不方便。</p><p id="d421" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">例如，假设我们有一个只有2个分类特征的数据集，分别有50个和3个不同的类别。例如，虽然使用16维的嵌入对于前者来说似乎是合适的，但是对于后者来说，它确实看起来像是一种“过度表示”。人们仍然可以使用fastai的经验法则，用更低的维度填充嵌入，但这将意味着一些注意力头在整个训练过程中会关注零/无，这对我来说似乎是一种浪费。尽管有这种潜在的“浪费”，我正在考虑将它作为<code class="fe ll lm ln lo b">pytorch-widedeep</code>的<code class="fe ll lm ln lo b">TabTransformer</code>实施的一个选项。与此同时，" T8 "所有的 " <code class="fe ll lm ln lo b">TabTransformer</code>实验都使用一个附加的设置来运行，其中带有少量类别的分类特征通过<code class="fe ll lm ln lo b">wide</code>组件。</p><p id="2797" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">最后，对于所有实验，我使用80%的数据进行训练，10%用于验证/参数调整。然后，在最后一次训练运行中将这两个数据集合并，并在剩余的10%的数据上测试该算法。除非有时间成分，否则数据集是随机分割的。在这些情况下，我使用了按时间顺序排列的训练/测试分割(注意，在<em class="mm">脸书评论卷</em>数据集的情况下，我没有使用论文中使用的测试集。所有训练、验证和测试数据集都是本文中描述的Variant-5数据集的拆分。</p><p id="5d55" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">就这些了，事不宜迟，我们来看结果。</p><h2 id="1ace" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">3.结果</h2><p id="5ca1" class="pw-post-body-paragraph ko kp iq kq b kr mh ka kt ku mi kd kw kx mj kz la lb mk ld le lf ml lh li lj ij bi translated">前面几节提供了这个“项目”的背景和我所做的实验的细节。在本节中，我将简单地展示所有数据和模型组合的前5个结果，并在我认为必要时加上一些注释。所有实验结果的完整表格可以在<a class="ae lk" href="https://github.com/jrzaurin/tabulardl-benchmark/tree/master/analyze_experiments/leaderboards" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p><h2 id="9d09" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">3.1成人人口普查数据集</h2><h2 id="c654" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">3.1.1 <code class="fe ll lm ln lo b">TabMlp</code></h2><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="4017" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">表二</strong>。使用<code class="fe ll lm ln lo b">TabMlp</code>获得的成人人口普查数据集的结果。</p><p id="16c6" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">也许要做的第一个评论与列/参数有关。很容易理解，并非所有的参数/列都适用于每个实验/行。例如，<code class="fe ll lm ln lo b">base_lr</code>、<code class="fe ll lm ln lo b">max_lr</code>、<code class="fe ll lm ln lo b">div_factor</code>或<code class="fe ll lm ln lo b">final_div_factor</code>等参数/栏仅在学习率调度器为<code class="fe ll lm ln lo b">CyclicLR</code>或<code class="fe ll lm ln lo b">OneCycleLR</code>时适用。</p><p id="1931" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">另一方面，MLP的密集图层是使用与<code class="fe ll lm ln lo b">fastai</code>库中非常相似的方法构建的。这种方法在MLP的每个密集层内的操作方面提供了灵活性(详见<a class="ae lk" href="https://pytorch-widedeep.readthedocs.io/en/latest/model_components.html#pytorch_widedeep.models.tab_mlp.TabMlp" rel="noopener ugc nofollow" target="_blank">。在该上下文中，三列<code class="fe ll lm ln lo b">mlp_batchnorm_last</code>和<code class="fe ll lm ln lo b">mlp_linear_first</code>设置这些操作发生的顺序。例如，如果对于一个给定的密集层，我们设置了<code class="fe ll lm ln lo b">mlp_linear_first = True</code>，那么实现的密集层将如下所示:<code class="fe ll lm ln lo b">[LIN -&gt; ACT -&gt; DP]</code>。另一方面，如果<code class="fe ll lm ln lo b">mlp_linear_first = False</code>，则密集层将按照以下顺序执行操作:<code class="fe ll lm ln lo b">[DP -&gt; LIN -&gt; ACT]</code>。</a></p><p id="ae85" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在成人普查数据集的情况下，循环学习率调度器产生非常好的结果。事实上，具有足够参数的一个周期的学习率将在仅仅一个时期内导致可接受的验证损失(假设批量足够小)，这可能说明该数据集不是特别困难。尽管如此，最好的结果(可以忽略不计)是用<code class="fe ll lm ln lo b">ReduceLROnPlateau</code>学习速率调度程序获得的。这实际上在不同数据集的所有实验中都很常见，也与我在许多不同场景中运行DL模型的经验相一致，无论是表格数据还是文本。使用10个时期的<code class="fe ll lm ln lo b">patience</code>运行<code class="fe ll lm ln lo b">ReduceLROnPlateau</code>学习率调度程序。这与30个周期的<code class="fe ll lm ln lo b">EarlyStopping</code>耐心一起意味着，当使用<code class="fe ll lm ln lo b">ReduceLROnPlateau</code>时，在实验被迫停止之前，学习率将降低3倍。</p><p id="6bf9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">关于实验设置、模型实现和每个参数/列背后含义的完整细节，请查看两个<code class="fe ll lm ln lo b">pytorch-widedeep</code>的<a class="ae lk" href="https://pytorch-widedeep.readthedocs.io/en/latest/index.html" rel="noopener ugc nofollow" target="_blank">文档</a>和实验<a class="ae lk" href="https://github.com/jrzaurin/tabulardl-benchmark" rel="noopener ugc nofollow" target="_blank">报告</a>。</p><h2 id="0677" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">3.1.2 <code class="fe ll lm ln lo b">TabResnet</code></h2><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="66f9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">表3 </strong>。使用<code class="fe ll lm ln lo b">TabResnet</code>获得的成人数据集的结果。</p><p id="2e98" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">表3中的<code class="fe ll lm ln lo b">block_dim = same</code>表示由密集层组成的Resnet块具有与传入嵌入相同的维度(参见此处的<a class="ae lk" href="https://github.com/jrzaurin/pytorch-widedeep/blob/tabnet/pytorch_widedeep/models/tab_resnet.py" rel="noopener ugc nofollow" target="_blank">了解实施细节)。</a></p><p id="4498" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">另一方面，<code class="fe ll lm ln lo b">TabResnet</code>模型提供了在Resnet块“之上”使用MLP的可能性。当<code class="fe ll lm ln lo b">mlp_hidden_dims = None</code>指示没有使用MLP，并且最后一个Resnet块的输出被直接“插入”到输出神经元中时。因此，如表3所示，使用<code class="fe ll lm ln lo b">TabResnet</code>获得的前5个结果对应于没有MLP的架构。因此，所有与MLP相关的参数/列对于这些实验都是多余的。</p><p id="8bab" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我发现有趣的是，无论是<code class="fe ll lm ln lo b">Adam</code>还是<code class="fe ll lm ln lo b">AdamW</code>，最好的结果都是使用<code class="fe ll lm ln lo b">OneCycleLR</code>获得的。当使用这个调度程序时，我通常将epochs的数量设置在1到10之间。通常情况下，我会在少量的时段($\leq 5$)和小批量的情况下获得最佳结果，这意味着与使用大批量的情况相反，学习率的增加/减少会更加缓慢(即，分布在更多的步骤上)。最后注意，参数/列<code class="fe ll lm ln lo b">n_cycles</code>仅适用于<code class="fe ll lm ln lo b">CyclicLR</code>调度程序。因为它没有在任何一个前5名的实验中使用，所以在表3中可以忽略。</p><h2 id="67d9" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">3.1.3 <code class="fe ll lm ln lo b">Tabnet</code></h2><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="07bb" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">表4 </strong>。使用<code class="fe ll lm ln lo b">Tabnet</code>获得的成人数据集的结果。</p><p id="09bb" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><code class="fe ll lm ln lo b">Tabnet</code>最近受到了一些关注，因为它与GBM竞争，甚至超过了GBM。此外，它是一个非常优雅的实现，通过使用注意力机制获得的特征重要性来提供模型可解释性。</p><p id="994a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">事实是，对于成人人口普查数据集，我在验证集上获得了最差的损失值(但正如我们将在后面看到的，不是最差的度量)。也许我只是错过了能够带来更好结果的精确的参数集。然而，值得强调的是，我对<code class="fe ll lm ln lo b">Tabnet</code>的研究与其他3种模型的研究具有相同的详细程度。</p><p id="6928" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">另一方面，有趣的是，在所有运行的实验中，在没有重影批次标准化的情况下始终获得最佳结果。因此，表4中的参数/列<code class="fe ll lm ln lo b">virtual_batch_size</code>可以忽略。类似地，由于最佳结果都是使用<code class="fe ll lm ln lo b">ReduceLROnPlateau</code>获得的，所以在表4中可以忽略与循环学习率调度器相关的所有参数。</p><p id="1a39" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">最后，与我过去运行的一些实验一致，使用<code class="fe ll lm ln lo b">RAdam</code>获得的最佳结果通常涉及相对较高的学习率。</p><h2 id="5917" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">3.1.4 <code class="fe ll lm ln lo b">TabTransformer</code></h2><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="360c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">表5 </strong>。使用<code class="fe ll lm ln lo b">TabTransformer</code>获得的成人人口普查数据集的结果。</p><p id="9b52" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">与所有以前的模型一样，如果您想要了解每个参数/列的详细含义，请查看[源代码]本身的[文档]。</p><p id="e73c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">或许值得一提的是，当前馈隐藏尺寸(<code class="fe ll lm ln lo b">ff_hidden_dim</code>)被设置为<code class="fe ll lm ln lo b">NaN</code>时，模型将默认为等于4倍输入嵌入尺寸的<code class="fe ll lm ln lo b">ff_hidden_dim</code>值(在表中所示的所有实验/行中为16)。这将产生一个尺寸为<code class="fe ll lm ln lo b">[ff_input_dim -&gt; 4 * ff_input_dim -&gt; ff_input_dim]</code>的前馈层。类似地，当<code class="fe ll lm ln lo b">mlp_hidden_dims = None</code>时，模型将默认为输入尺寸的4倍，导致尺寸<code class="fe ll lm ln lo b">[mlp_input_dim -&gt; 4 * mlp_input_dim -&gt; 2* mlp_input_dim -&gt; output_dim]</code>的MLP。</p><p id="b179" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">此外，如前所述，<code class="fe ll lm ln lo b">TabTransformer</code>也是用包括<code class="fe ll lm ln lo b">wide</code>组件的设置运行的。这由<code class="fe ll lm ln lo b">with_wide</code>参数指定。</p><p id="d468" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">值得注意的是，最佳损失值与其他DL模型的损失值相似，通常使用<code class="fe ll lm ln lo b">RAdam</code>优化器获得。</p><h2 id="7f95" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">3.1.5 DL与<code class="fe ll lm ln lo b">LightGBM</code></h2><p id="24c0" class="pw-post-body-paragraph ko kp iq kq b kr mh ka kt ku mi kd kw kx mj kz la lb mk ld le lf ml lh li lj ij bi translated">看完每个DL模型获得的结果后，这是关键时刻，让我们看看DL结果与使用<code class="fe ll lm ln lo b">LightGBM</code>获得的结果相比如何。</p><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="a3af" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">表六</strong>。使用四个DL模型和<code class="fe ll lm ln lo b">LightGBM</code>获得的成人人口普查数据集的结果。<code class="fe ll lm ln lo b">runtime</code>单位是秒。</p><p id="6d47" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">让我再次强调，表6中显示的指标都是针对测试数据集获得的。<code class="fe ll lm ln lo b">runtime</code>列显示了使用验证期间获得的最佳参数的最终训练数据集(即包含90%数据的数据集)的训练时间，单位为秒。DL模型在AWS上的<code class="fe ll lm ln lo b">p2.xlarge</code>实例上运行，所有的<code class="fe ll lm ln lo b">LightGBM</code>实验都在我的Mac上运行。</p><p id="67c9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">它们是值得评论的几个方面。首先，所有DL模型获得的结果与<code class="fe ll lm ln lo b">LightGBM</code>的结果相当，但并不比后者更好。第二，表现最好的DL模型(很少)是最简单的模型<code class="fe ll lm ln lo b">TabMlp</code>。最后，使用<code class="fe ll lm ln lo b">LightGBM</code>时的训练时间比使用任何DL型号都要短得多。</p><h2 id="bf21" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">3.2银行营销数据集</h2><p id="b33c" class="pw-post-body-paragraph ko kp iq kq b kr mh ka kt ku mi kd kw kx mj kz la lb mk ld le lf ml lh li lj ij bi translated">上一节中的大多数注释适用于本节中显示的表。</p><p id="de60" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">请注意，正如我在帖子前面提到的，银行营销数据集有点不平衡。因此，我还使用<a class="ae lk" href="https://arxiv.org/abs/1708.02002?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank">焦点损失</a>【18】(可通过参数或损失函数输入在<code class="fe ll lm ln lo b">pytorch_widedeep</code>中访问)进行了一些实验。见<a class="ae lk" href="https://pytorch-widedeep.readthedocs.io/en/latest/trainer.html" rel="noopener ugc nofollow" target="_blank">此处</a>。总的来说，所获得的结果与没有焦点损失的结果相似，但并不比没有焦点损失的结果好。这与我在其他数据集上的经验一致，我发现当数据集高度不平衡时(例如，大约2%的正类与负类比率)，焦点损失会导致明显更好的结果。</p><h2 id="6ff0" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">3.2.1 <code class="fe ll lm ln lo b">TabMlp</code></h2><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="65f5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">表7 </strong>。使用<code class="fe ll lm ln lo b">TabMlp</code>获得的银行营销数据集的结果。</p><h2 id="c205" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">3.2.2 <code class="fe ll lm ln lo b">TabResnet</code></h2><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="1ac8" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">表8 </strong>。使用<code class="fe ll lm ln lo b">TabResnet</code>获得的银行营销数据集的结果。</p><p id="9fc3" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">同样，非常有趣的是，<code class="fe ll lm ln lo b">RAdam</code> optimizer和<code class="fe ll lm ln lo b">OneCycleLR</code>为这个DL模型带来了一些最好的结果。</p><h2 id="c8bf" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">3.2.3 <code class="fe ll lm ln lo b">Tabnet</code></h2><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="2cd5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">表9 </strong>。使用<code class="fe ll lm ln lo b">Tabnet</code>获得的银行营销数据集的结果。</p><p id="feac" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">注意，在这种情况下，使用<code class="fe ll lm ln lo b">Tabnet</code>获得的前5个结果都具有相对较高的学习率值(<code class="fe ll lm ln lo b">lr = 0.03</code>)。此外，与成人人口普查数据集的情况类似，<code class="fe ll lm ln lo b">Tabnet</code>产生最差的验证损失值。</p><h2 id="8def" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">3.2.4 <code class="fe ll lm ln lo b">TabTransformer</code></h2><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="d48b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">表10 </strong>。使用<code class="fe ll lm ln lo b">TabTransformer</code>获得的银行营销数据集的结果。</p><p id="98f7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">或许值得注意的是，与之前的一些结果一致，这里使用<code class="fe ll lm ln lo b">RAdam</code>获得的最佳结果涉及相对较高的学习率(与使用<code class="fe ll lm ln lo b">Adam</code>或<code class="fe ll lm ln lo b">AdamW</code>获得的结果相比是10倍)。)</p><h2 id="2659" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">3.2.5 DL与<code class="fe ll lm ln lo b">LightGBM</code></h2><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="a6f9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">表11 </strong>。使用四个DL模型和LightGBM获得的银行营销数据集的结果。</p><p id="5a0d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我必须承认，表11中显示的结果起初让我感到惊讶(至少可以这么说)。我再次运行了几个DL模型，并多次运行<code class="fe ll lm ln lo b">LightGBM</code>进行双重检查，最终得出结论(剧透警告)这将是我在本文中运行的所有实验中唯一一个DL模型表现优于<code class="fe ll lm ln lo b">LightGBM</code>的情况。事实上，如果我们以我的工作经验加入这里的实验，这是我第二次发现DL模型比<code class="fe ll lm ln lo b">LightGBM</code>表现得更好(稍后会有更多)。此外，使用<code class="fe ll lm ln lo b">TabResnet</code>或<code class="fe ll lm ln lo b">TabTransformer</code>获得的改进非常显著，如果这是一个“真实世界”的例子，人们可能会考虑使用DL模型，并接受运行时间和成功指标之间的折衷。</p><p id="0cf1" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">当然，您可以更深入地研究<code class="fe ll lm ln lo b">LightGBM</code>，设置样品重量，甚至使用自定义损耗，但DL型号也是如此。因此，总的来说，我认为这种比较是公平的。然而，我感到非常惊讶，我认为我可能在代码中有一个我没有发现的错误。因此，如果有人在某个时候检查代码，发现确实有bug，请告诉我🙂。</p><p id="ad62" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">最后，有人可能会对<code class="fe ll lm ln lo b">Tabnet</code>的表现感到失望，就像我一样。有可能我没有正确地实现它，尽管代码完全基于dreamquark-ai的实现(<strong class="kq ja">全部</strong>归功于他们)，并且当用更简单的数据集测试时，我获得了与GBM类似的结果。我发现<code class="fe ll lm ln lo b">Tabnet</code>是一个非常优雅的实现，不知何故我相信它应该表现得更好。我将在结论部分回到这一点。</p><h2 id="9068" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">3.3纽约市出租车行程持续时间</h2><p id="46e5" class="pw-post-body-paragraph ko kp iq kq b kr mh ka kt ku mi kd kw kx mj kz la lb mk ld le lf ml lh li lj ij bi translated">正如我前面提到的，这是最大的数据集，因此，我试验了更大的批量。虽然这可能会稍微改变一些个别的结果，但我相信这不会改变本节的整体结论。</p><h2 id="d67c" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">3.3.1 <code class="fe ll lm ln lo b">TabMlp</code></h2><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="6364" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">表12 </strong>。使用<code class="fe ll lm ln lo b">TabMlp</code>获得的纽约市出租车行程持续时间数据集的结果。</p><p id="47da" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这种情况下的验证损失是<code class="fe ll lm ln lo b">MSE</code>。验证集中目标变量的标准偏差(<code class="fe ll lm ln lo b">std</code>下文)约为599。假设std是我们在预测期望值时获得的<code class="fe ll lm ln lo b">RMSE</code>,我们可以看到这不是一个非常强大的模型，即预测出租车行程持续时间的任务确实相对具有挑战性。</p><p id="3334" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">再来看看其他DL车型表现如何。</p><h2 id="42dc" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">3.3.2 <code class="fe ll lm ln lo b">TabResnet</code></h2><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="5e51" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">表13 </strong>。使用<code class="fe ll lm ln lo b">TabResnet</code>获得的纽约市出租车行程持续时间数据集的结果。</p><h2 id="9b46" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">3.3.3 <code class="fe ll lm ln lo b">Tabnet</code></h2><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="76d1" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">表14 </strong>。使用<code class="fe ll lm ln lo b">Tabnet</code>获得的纽约市出租车行程持续时间数据集的结果。</p><h2 id="4c0a" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">3.3.4 <code class="fe ll lm ln lo b">TabTransformer</code></h2><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="9f69" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">表15 </strong>。使用<code class="fe ll lm ln lo b">TabTransformer</code>获得的纽约市出租车行程持续时间数据集的结果。</p><h2 id="d411" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">3 . 3 . 5 D1 vs<code class="fe ll lm ln lo b">LightGBM</code></h2><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="ab9d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">表16 </strong>。使用四个DL模型和LightGBM获得的纽约市出租车出行持续时间数据集的结果。</p><p id="97d5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在这种情况下，<code class="fe ll lm ln lo b">TabTransformer</code>和<code class="fe ll lm ln lo b">Tabnet</code>是性能最差的型号。正如我前面提到的，我将在结论部分思考潜在的原因。</p><h2 id="9065" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">3.4脸书评论卷</h2><p id="85b2" class="pw-post-body-paragraph ko kp iq kq b kr mh ka kt ku mi kd kw kx mj kz la lb mk ld le lf ml lh li lj ij bi translated">这是我们将在本帖中讨论的四个数据集的最后一个，第二个回归问题。</p><h2 id="572a" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">3.4.1 <code class="fe ll lm ln lo b">TabMlp</code></h2><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="4302" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">表17 </strong>。使用<code class="fe ll lm ln lo b">TabMlp</code>获得的脸书评论量数据集的结果。</p><p id="ba53" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">与纽约市出租车行程持续时间的情况一样，验证损失是<code class="fe ll lm ln lo b">MSE</code>损失。在脸书评论量数据集的情况下，目标变量的<code class="fe ll lm ln lo b">std</code>约为13。因此，按照同样的推理，我们可以看到，使用这个特定的数据集来预测脸书评论量的任务是具有挑战性的</p><p id="9593" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">再来看看其他DL车型表现如何。</p><h2 id="0c43" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">3.4.2 <code class="fe ll lm ln lo b">TabResnet</code></h2><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="1742" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">表18 </strong>。使用<code class="fe ll lm ln lo b">TabResnet</code>获得的脸书评论量数据集的结果。</p><h2 id="a3b4" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">3.4.3 <code class="fe ll lm ln lo b">Tabnet</code></h2><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="b428" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">表19 </strong>。使用<code class="fe ll lm ln lo b">Tabnet</code>获得的脸书评论量数据集的结果。</p><h2 id="da12" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">3.4.4 <code class="fe ll lm ln lo b">TabTransformer</code></h2><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="8bc0" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">表20 </strong>。使用<code class="fe ll lm ln lo b">TabTransformer</code>获得的脸书评论量数据集的结果。</p><h2 id="a81f" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">3.4.5 DL与<code class="fe ll lm ln lo b">LightGBM</code></h2><figure class="nb nc nd ne gt nf"><div class="bz fp l di"><div class="ng nh l"/></div></figure><p id="1030" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">表21 </strong>。使用四个DL模型和LightGBM获得的脸书评论量数据集的结果。</p><h2 id="147c" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">4.摘要</h2><p id="0c16" class="pw-post-body-paragraph ko kp iq kq b kr mh ka kt ku mi kd kw kx mj kz la lb mk ld le lf ml lh li lj ij bi translated">我已经使用了四个数据集，运行了超过1500次实验(意味着在参数设置下运行)，将四个DL模型与<code class="fe ll lm ln lo b">LightGBM</code>进行了比较。这是一些结果的总结。</p><ul class=""><li id="e4c5" class="mn mo iq kq b kr ks ku kv kx ni lb nj lf nk lj oe mt mu mv bi translated">赢了，而且从来没有打架</li></ul><p id="dc32" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">除了一个例外，<code class="fe ll lm ln lo b">LightGBM</code>比DL模型表现得更好，而这个例外恰恰就是例外。除了这里运行和讨论的实验之外，我还可以添加两个在我工作的公司中使用DL处理表格数据的场合。具体来说，这里称为<code class="fe ll lm ln lo b">TabMlp</code>的型号在一种情况下带有<code class="fe ll lm ln lo b">wide</code>组件，而在另一种情况下单独存在。</p><p id="2657" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在2016年发表广受欢迎的<a class="ae lk" href="https://arxiv.org/abs/1606.07792" rel="noopener ugc nofollow" target="_blank">Wide and Deep</a>【19】论文后不久，Wide &amp; Deep模型被用于推荐算法的上下文中。当时，我使用XGBoost来预测一个兴趣度，并根据这个兴趣度对报价进行排名。然后用<code class="fe ll lm ln lo b"><a class="ae lk" href="https://keras.io/" rel="noopener ugc nofollow" target="_blank">Keras</a></code>实现的宽深度模型获得了比XGBoost略好的MAP和NDCG(几乎相同的度量，尽管略低，当仅使用深度组件时获得)。考虑到在生产过程中需要考虑的其他因素，我们最终使用了XGBoost。</p><p id="07ca" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在第二种情况下，一个更近的项目，<code class="fe ll lm ln lo b">TabMlp</code>本身获得了非常相似的，但仍然比使用<code class="fe ll lm ln lo b">LightGBM</code>获得的RMSE和R2值低。尽管<code class="fe ll lm ln lo b">TabMLP</code>的预测没有被直接使用，但我们发现这些嵌入对一些额外的项目很有用，我们围绕<code class="fe ll lm ln lo b">TabMlp</code>构建了一个生产系统。</p><p id="ca45" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">到目前为止，我关注的是通过成功指标衡量的绩效。然而，当涉及到训练(和预测)时间时，差异是如此之大，以至于在这个阶段，这些算法中的一些只是用于研究目的和/或kaggle比赛。不要误会我的意思，你只是通过挑战当前的解决方案和既定的概念来推动一个行业的技术发展。我只是说，在现阶段，在生产环境中，很难想象围绕这些算法构建一个健壮的系统。这就是我写“<em class="mm">从来没有打架</em>”的原因。当您上线时，通常不仅仅是成功指标，还有速度和弹性。总的来说，在我看来，表格数据的DL模型离正常插入生产系统还有点远(但请阅读下文)。</p><p id="f16b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">最后，你可能会在这里或那里读到，通过适当的功能工程，噪声消除，平衡和“谁知道还有什么”DL模型优于GBM。事实是，在我的经历中实际上是相反的。当一个人设法设计出好的、强大的特性时，GBM甚至比DL模型表现得更好。这也和最近一些比赛的结果一致。例如，在2020年的<a class="ae lk" href="https://recsys-twitter.com/" rel="noopener ugc nofollow" target="_blank"> RecSys挑战赛</a>中，NVIDIA的家伙使用<a class="ae lk" href="https://medium.com/rapids-ai/winning-solution-of-recsys2020-challenge-gpu-accelerated-feature-engineering-and-training-for-cd67c5a87b1f" rel="noopener">聪明的工程</a>(例如，面向目标的编码)“插入”到类固醇(或者更好，GPU)的XGBoost中而获胜。我不确定使用这些特性和DL模型是否真的会改善他们的结果。</p><p id="02d7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">总的来说，如果我加入这篇文章中的结果，加上我发现在行业中真实数据集的表格数据上尝试DL模型，我只能得出结论，就整体性能而言，表格数据的DL模型“还不太好”。</p><ul class=""><li id="b560" class="mn mo iq kq b kr ks ku kv kx ni lb nj lf nk lj oe mt mu mv bi translated"><code class="fe ll lm ln lo b">TabNet</code>和<code class="fe ll lm ln lo b">TabTransformer</code></li></ul><p id="7051" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">一个相当令人惊讶的结果是<code class="fe ll lm ln lo b">Tabnet</code>的糟糕表现，也许在较小的程度上，<code class="fe ll lm ln lo b">TabTransformer</code>也是如此。</p><p id="c461" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">一种可能是，我还没有找到导致良好度量的正确的参数集。事实上，使用<code class="fe ll lm ln lo b">Tabnet</code>和<code class="fe ll lm ln lo b">TabTransformer</code>时的过拟合量非常显著，高于使用<code class="fe ll lm ln lo b">TabResnet</code>和<code class="fe ll lm ln lo b">TabMlp</code>时的过拟合量。这使我相信，如果我找到一组更好的正则化参数，或者简单地对每个分类特征使用不同数量的嵌入，我可能能够改进上表中显示的结果。然而，我也应该说，考虑到这些算法的良好接收和我获得的差的结果，我在尝试一些额外的参数时更加强调了一点。不幸的是，我的尝试都没有带来显著的改善。</p><p id="076b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">当然，第二种可能性是在<code class="fe ll lm ln lo b">pytorch-widedeep</code>的实现是错误的。我想随着我不断发布版本和使用这个包，我会发现这一点的。</p><p id="a6b7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">总的来说，我发现<code class="fe ll lm ln lo b">Tabnet</code>是表现最差的(也是最慢的),我肯定会在接下来的几周里投入一些额外的时间来看看这是否与输入参数有关。</p><ul class=""><li id="bf5c" class="mn mo iq kq b kr ks ku kv kx ni lb nj lf nk lj oe mt mu mv bi translated">简单胜过复杂。</li></ul><p id="a0a6" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">有趣的是，总的来说，实现与<code class="fe ll lm ln lo b">LightGBM</code>相似性能的DL算法是一个简单的MLP。在我写这篇文章的时候，我想知道这是否与正在兴起的将MLP带回来的趋势有关(例如，[20]、[21]或[22])，更复杂模型的出现只是炒作的结果，而不是对当前解决方案的适当探索。</p><p id="176b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">当然，对于更复杂的模型，探索和超参数优化的空间更大。虽然这是我打算继续探索的东西，但在空间和时间中有一个时刻，人们会怀疑"<em class="mm">这真的值得吗？</em>”。</p><p id="6b6e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">让我们看看我能否在下一节回答这个问题</p><h2 id="022d" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">5结论</h2><p id="aba4" class="pw-post-body-paragraph ko kp iq kq b kr mh ka kt ku mi kd kw kx mj kz la lb mk ld le lf ml lh li lj ij bi translated">当我开始考虑这篇文章时，我已经知道DL模型对<code class="fe ll lm ln lo b">LightGBM</code>来说不是真正的挑战。如果我们只关注性能指标和运行时间，唯一可能的结论是，在现实环境中，用于表格数据的DL模型仍然不是GBM的竞争对手。然而，在行业/市场的现阶段，这真的是<em class="mm">要回答的问题</em>吗？我不这么认为。</p><p id="af14" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这不是竞争，也不应该是，这应该是一个联盟。要回答的问题是:“表格数据的DL模型如何在行业中提供帮助并补充当前的系统”。让我们思考一下这个问题。</p><p id="6fcf" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">根据我的经验，表格数据上的DL模型在涉及许多分类特征的大型数据集上表现最佳，这些数据集本身有许多类别。在这些情况下，人们可以尝试DL模型，最初的目标是直接使用预测。然而，即使预测最终没有被使用，嵌入也包含了大量有用的信息。关于每个分类特征如何相互作用的信息以及关于每个分类特征如何与目标变量相关的信息。这些嵌入可用于许多附加产品。</p><p id="9fe7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">例如，假设您有一个数据集，其中包含数千种品牌及其相应产品的元数据和价格。你的任务是预测价格如何随时间变化(即预测价格)。分类特性<code class="fe ll lm ln lo b">brand</code>的嵌入将为您提供特定品牌如何与数据集中的其余列和目标(价格)相关的信息。换句话说，如果给定一个品牌，你找到了由嵌入邻近度定义的最接近的品牌，你将“自然地”直接找到给定空间内的竞争对手(假设数据集代表市场)。</p><p id="6eb6" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">此外，GBM不允许迁移学习，但DL模型允许。此外，正如在<code class="fe ll lm ln lo b">TabNet</code>和<code class="fe ll lm ln lo b">TabTransformer</code>论文中提到的，自我监督训练在数据量低或未标记数据集比标记数据集大得多的情况下会产生更好的性能。因此，在一些场景中，DL模型非常有用。</p><p id="2869" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">例如，让我们假设你在一个国家有一个关于给定问题的大型数据集，但是在另一个国家有一个关于完全相同问题的小得多的数据集。我们还假设数据集在列方面非常相似。人们可以使用大数据集训练DL模型，并将学习“转移”到第二个更小的数据集，希望获得比仅使用小数据集高得多的性能。</p><p id="ac03" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">还有一些我能想到的其他场景，但我会把它留在这里。总的来说，我只是想说明，如果你来这里是为了享受GBM比DL模型表现更好的事实，我希望你喜欢这一过程(并开始在一个好的治疗师那里思考)，但在我看来，这不是重点。</p><p id="af86" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><strong class="kq ja">就指标而言，GBM比DL模型表现更好，这是正确的，但后者带来了GBM所没有的一些功能，因此是对它们的完美补充。</strong></p><h2 id="1ad6" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">6.未来的工作</h2><p id="5e0c" class="pw-post-body-paragraph ko kp iq kq b kr mh ka kt ku mi kd kw kx mj kz la lb mk ld le lf ml lh li lj ij bi translated">几个月前我就开始考虑这个帖子了。然后，一些其他的事情在我的生活中占据了优先地位(加上大量的工作)，这变成了一个有点漫长的旅程。我现在希望能从我团队中非常聪明的人那里得到一点帮助，并改进repo中的表格vs DL代码，也许自动化一些过程，这样我就可以在未来轻松地添加更多数据集。</p><p id="0225" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这也是对<code class="fe ll lm ln lo b"><a class="ae lk" href="https://github.com/jrzaurin/pytorch-widedeep" rel="noopener ugc nofollow" target="_blank">pytorch-widedeep</a></code>图书馆的一个很好的测试(如果你喜欢它，或者觉得它有用，请给它一颗星😊).本帖所有链接都指向回购中的<code class="fe ll lm ln lo b">tabnet</code>分支，是最新的。在接下来的几天里，我将合并并发布v1版本的包，然后更新链接和帖子。从那里，有一系列的算法，我们想带来(如圣)，也增加了一些不同形式的训练。</p><p id="1fb0" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">除了向库中添加更多的算法或改进基准代码之外，我想用最后一个想法来结束本文。正如我在文章开头提到的，论文之间有不一致的地方。不同的论文会发现不同的结果为所有算法考虑，GBMs或基于DL。当你阅读它们的时候，你会感觉到有一种急切的心情，想要发表一些获得成功的东西。对于像我这样的背景与计算机科学不同的人来说，这在某种意义上让我想起了我作为天文学家的日子。几年后，我发现我所在领域的大多数出版物都不是很好，但是因为评判你的标准只是出版物和引用，所以你可以发表任何东西，而且越快越好。</p><p id="b614" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在这个阶段，抛开出版物和引用不谈，我认为我们中的一些人以及一些公司(以便我们可以使用实际的真实数据集)有机会合作，并为表格数据正确地基准测试DL算法。我相信这些算法在行业中的潜力是巨大的，通过适当的基准测试，我们不仅可以了解它们在哪里表现得更好，还可以了解如何更有效地使用它们。</p><p id="a884" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">就是这样！如果你是在这里做的，我希望你喜欢和/或觉得这有用。</p><h2 id="c2f9" class="lp lq iq bd lr ls lt dn lu lv lw dp lx kx ly lz ma lb mb mc md lf me mf mg iw bi translated">参考</h2><p id="9849" class="pw-post-body-paragraph ko kp iq kq b kr mh ka kt ku mi kd kw kx mj kz la lb mk ld le lf ml lh li lj ij bi translated">[1]表格数据:深度学习不是你需要的全部:拉维德·施瓦兹-齐夫，阿米泰·艾蒙，2021，<a class="ae lk" href="https://%5Barxiv.org/pdf/2106.03253.pdf" rel="noopener ugc nofollow" target="_blank"> arxiv:2106.03253 </a></p><p id="cc93" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[2] XGBoost:一个可扩展的树提升系统。陈天琦，卡洛斯·盖斯特林2016，<a class="ae lk" href="https://arxiv.org/abs/1603.02754" rel="noopener ugc nofollow" target="_blank"> arXiv:1603.02754 </a></p><p id="d433" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[3] CatBoost:具有分类特征的无偏增强。柳德米拉·普罗霍伦科娃，格莱布·古塞夫，亚历山大·沃罗贝夫，安娜·维罗妮卡·多罗古什，安德烈·谷林，<a class="ae lk" href="https://arxiv.org/abs/1706.09516" rel="noopener ugc nofollow" target="_blank"> arXiv:1706.09516 </a></p><p id="9529" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[4] LightGBM:一种高效的梯度推进决策树。柯，，托马斯·芬利，王泰峰，2017，<a class="ae lk" href="https://papers.nips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf" rel="noopener ugc nofollow" target="_blank">第31届神经信息处理系统会议</a></p><p id="e14c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[5] SAINT:通过行注意和对比预训练改进用于表格数据的神经网络。Gowthami Somepalli，Micah Goldblum，Avi Schwarzschild，C. Bayan Bruss，Tom Goldstein，2021，<a class="ae lk" href="https://arxiv.org/abs/2106.01342" rel="noopener ugc nofollow" target="_blank"> arXiv:2106.01342 </a></p><p id="7de0" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[6]使用神经网络和决策树进行评论量预测，Kamaljot Singh，Ranjeet Kaur，2015年第17届UKSIM-AMSS建模与仿真国际会议。</p><p id="1a34" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[7] TabNet:专注的可解释表格学习，塞尔詹·奥·阿里克，托马斯·普菲斯特，<a class="ae lk" href="https://arxiv.org/abs/1908.07442" rel="noopener ugc nofollow" target="_blank"> arXiv:1908.07442v5 </a></p><p id="6081" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[8]训练更长，推广更好:缩小神经网络大批量训练中的推广差距。Elad Hoffer，Itay Hubara和Daniel Soudry，2017年，<a class="ae lk" href="https://arxiv.org/abs/1705.08741" rel="noopener ugc nofollow" target="_blank"> arXiv:1705.08741 </a></p><p id="b71c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[9] TabTransformer:使用上下文嵌入的表格数据建模。黄鑫，阿什什·赫坦，米兰·茨维特科维奇，佐哈尔·卡尔宁，2020。<a class="ae lk" href="https://arxiv.org/abs/2012.06678" rel="noopener ugc nofollow" target="_blank"> arXiv:2012.06678v1 </a></p><p id="5236" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[10]关注是你所需要的一切，Ashish Vaswani，Noam Shazeer，Niki Parmar等人，2017年。<a class="ae lk" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> arXiv:1706.03762v5 </a></p><p id="39d7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[11] Adam:一种随机优化方法，Diederik P. Kingma，Jimmy Ba，2014，<a class="ae lk" href="https://arxiv.org/abs/1412.6980" rel="noopener ugc nofollow" target="_blank"> arXiv:1412.6980 </a></p><p id="f199" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[12]解耦权重衰减正则化，Ilya Loshchilov，Frank Hutter，2017年。<a class="ae lk" href="https://arxiv.org/abs/1711.05101" rel="noopener ugc nofollow" target="_blank"> arXiv:1711.05101 </a></p><p id="e13b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[13]关于自适应学习率的方差及其超越，，，蒋鹏程，，陈，，高剑锋，韩家伟，2019，<a class="ae lk" href="https://arxiv.org/abs/1908.03265" rel="noopener ugc nofollow" target="_blank"> arxiv.org:1908.03265 </a></p><p id="8786" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[14]训练神经网络的循环学习率，Leslie N. Smith，2017，<a class="ae lk" href="https://arxiv.org/abs/1506.01186" rel="noopener ugc nofollow" target="_blank"> arxiv.org:1506.01186 </a></p><p id="e22f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[15]超收敛:使用大学习率的非常快速的神经网络训练，Leslie N. Smith，Nicholay Topin，2017，<a class="ae lk" href="https://arxiv.org/abs/1708.07120" rel="noopener ugc nofollow" target="_blank"> arxiv.org:1708.0712 </a></p><p id="d41c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[16] Optuna:下一代超参数优化框架。秋叶拓哉，佐野正太郎，柳濑俊彦，太田赳，小山正典，2019，<a class="ae lk" href="https://arxiv.org/abs/1907.10902" rel="noopener ugc nofollow" target="_blank"> arXiv:1907.10902 </a></p><p id="e3ac" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[17]超参数优化算法，James Bergstra，Rémi Bardenet，Yoshua Bengio，Balázs Kégl，2011年，<a class="ae lk" href="https://papers.nips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf" rel="noopener ugc nofollow" target="_blank">第25届神经信息处理系统会议</a></p><p id="966d" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[18]密集物体探测的焦点损失，宗-林逸，普里亚·戈亚尔，罗斯·吉斯克，明凯·何，彼得·多拉尔，2017年，<a class="ae lk" href="https://arxiv.org/abs/1708.02002?source=post_page---------------------------" rel="noopener ugc nofollow" target="_blank"> arxiv.org:1708.02002 </a></p><p id="9ae5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[19]推荐系统的广度和深度学习，Heng-Tze Cheng，Levent Koc，Jeremiah Harmsen等人，2016，<a class="ae lk" href="https://arxiv.org/abs/1606.07792" rel="noopener ugc nofollow" target="_blank"> arxiv.org:1606.07792 </a></p><p id="3a5b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[20] FNet:用傅立叶变换混合令牌，李中清-索普，约书亚·安斯利，伊利亚·埃克斯坦，圣地亚哥·翁塔农，2021，<a class="ae lk" href="https://arxiv.org/abs/2105.03824" rel="noopener ugc nofollow" target="_blank"> arxiv.org:2105.03824 </a></p><p id="4d9f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[21]关注MLPs，，戴子航，David R. So，Quoc诉乐，2021，【arxiv.org:2105.08050 </p><p id="d3f7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[22] ResMLP:利用数据有效训练的图像分类前馈网络，Hugo Touvron，Piotr Bojanowski，Mathilde Caron等人，2021，<a class="ae lk" href="https://arxiv.org/abs/2105.03404" rel="noopener ugc nofollow" target="_blank"> arxiv.org:2105.03404 </a></p></div></div>    
</body>
</html>