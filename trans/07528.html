<html>
<head>
<title>Assyrian or Babylonian? Language Identification in Cuneiform Texts</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">亚述人还是巴比伦人？楔形文字中的语言识别</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/assyrian-or-babylonian-language-identification-in-cuneiform-texts-4f15a14a5d70?source=collection_archive---------21-----------------------#2021-07-09">https://towardsdatascience.com/assyrian-or-babylonian-language-identification-in-cuneiform-texts-4f15a14a5d70?source=collection_archive---------21-----------------------#2021-07-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="4310" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="eed8" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">古代美索不达米亚方言的语言模型</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/58f546f6390d7e44d5edb303d0435c2e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LfPq8VGwuPMA4e5vxaxvOQ.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">楔形文字泥板。来源:<a class="ae le" href="https://commons.wikimedia.org/wiki/File:Cuneiform_tablet-MAHG_15856-IMG_9472-black.jpg" rel="noopener ugc nofollow" target="_blank"> Rama </a>，维基共享</p></figure><p id="0304" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><a class="ae le" href="https://en.wikipedia.org/wiki/Mesopotamia" rel="noopener ugc nofollow" target="_blank">美索不达米亚</a>是幼发拉底河和底格里斯河之间地区的一个古老名称，位于今天的伊拉克和土耳其、叙利亚和伊朗的部分地区。现代社会的“现代性”很大程度上归功于美索不达米亚人在农业、数学或冶金等领域的发明。在这些发明中，古代美索不达米亚人最伟大的成就之一是发展了<a class="ae le" href="https://en.wikipedia.org/wiki/Cuneiform" rel="noopener ugc nofollow" target="_blank"> <em class="mb">楔形文字</em>手稿</a>，并由此发展出最早的书写系统之一。这是公元前第三个千年，历史才刚刚开始。</p><p id="07c8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">楔形文字由印在新鲜泥板上的楔形符号组成。粘土是一种可以抵抗时间流逝而不会显著退化的材料。因此，考古学家设法找到了大量保存完好的石碑。该地区的粘土也非常便宜和丰富。这使得这种形式的经文非常受欢迎，不同的民族在很长一段时间里根据他们的语言改编了这种文字。因此，我们对美索不达米亚几千年来(书面)语言的演变有一个很好的记录。事实上，我们有这么多的记录跨越了这么长的时间，它们几乎是无法管理的。这就是数据科学的用武之地。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi mc"><img src="../Images/bd9747ee78b384b193c8a172a7935698.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7zoH-Rz1jwrm1cbGoNsD8w.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">古代美索不达米亚地图。来源:<a class="ae le" href="https://commons.wikimedia.org/wiki/File:N-Mesopotamia_and_Syria_english.svg" rel="noopener ugc nofollow" target="_blank"> Goran tek-en </a>，维基共享</p></figure><p id="229a" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">本文的目的是展示如何编写一个简单的定制语言模型来识别楔形文字文本中的美索不达米亚语言和方言。为此，我们使用来自Kaggle data challenge的数据集<a class="ae le" href="https://www.kaggle.com/wilstrup/cuneiform-language-identification" rel="noopener ugc nofollow" target="_blank"><em class="mb"/></a><em class="mb">识别楔形文字。</em>该数据集包含大约139，000个楔形文字片段，还提供了语言或方言标签。有七个标签，包括<a class="ae le" href="https://en.wikipedia.org/wiki/Sumerian_language" rel="noopener ugc nofollow" target="_blank">夏季语</a> (SUX)，一种孤立的语言，以及<a class="ae le" href="https://en.wikipedia.org/wiki/Akkadian_language" rel="noopener ugc nofollow" target="_blank">阿卡德语</a>的六种方言:新亚述语(NEA)，标准巴比伦语(STB)，晚期巴比伦语(LTB)，新巴比伦语(NEB)，中巴比伦外围语(MPB)，以及旧巴比伦语(OLB)。</p><p id="1ace" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如果你想知道这些字符的样子，下面的图片显示了我们的数据集的一个样本。很漂亮，对吧？</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi md"><img src="../Images/8a439ba325c81109d0e6c5190f8849c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Rq9ZXuEU0S5UaVQ2Ufo-3w.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">我们数据的子集。来源:作者对Kaggle <a class="ae le" href="https://www.kaggle.com/wilstrup/cuneiform-language-identification" rel="noopener ugc nofollow" target="_blank">数据集</a>的分析</p></figure><p id="b14c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">接下来，如果您想在本地机器上复制这个分析，我建议您下载这些字体中的一种<a class="ae le" href="http://oracc.museum.upenn.edu/doc/help/visitingoracc/fonts/" rel="noopener ugc nofollow" target="_blank"/>。虽然这不会影响您复制分析的能力，但否则文本将无法正确呈现。</p><h1 id="f2d9" class="me mf iq bd mg mh mi mj mk ml mm mn mo kf mp kg mq ki mr kj ms kl mt km mu mv bi translated">语言模型</h1><p id="65cb" class="pw-post-body-paragraph lf lg iq lh b li mw ka lk ll mx kd ln lo my lq lr ls mz lu lv lw na ly lz ma ij bi translated">一般来说，<a class="ae le" href="https://en.wikipedia.org/wiki/Language_model" rel="noopener ugc nofollow" target="_blank">语言模型</a>是单词序列的概率分布。在这个宽泛的定义中，我们可以找到不同类型的模型。一些最常见的使用<em class="mb"> n-gram </em>的概念，即n个连续字符(单词)的序列，我们在其中分割文本。N-gram模型在NLP文献中相当常见。它们的应用包括语音和手写识别以及机器翻译。因此，Python中有一些众所周知的包，比如<a class="ae le" href="http://www.nltk.org/" rel="noopener ugc nofollow" target="_blank"> NLTK </a>，用它们我们可以很容易地构建一个N元模型。</p><p id="f0fb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">对于这项任务，我使用一个N-gram模型来预测楔形文字N-gram序列属于特定的古代美索不达米亚语言或方言的概率。在这个实现中，我们假设每个字符是一个单字。从技术上讲，楔形文字可以是一个完整的单词，也可以只是一个声音，但是我们会忽略这个事实。现在，你可能已经猜到了，如果我遵循这个挑战的简单路径，我就不会写这篇文章。相反，在这里我将向您展示如何从零开始构建一个N元模型<em class="mb"/>。事实上，我在代码中使用的唯一Python库是<code class="fe nb nc nd ne b">numpy</code>和<code class="fe nb nc nd ne b">pandas</code>。我决定这样做有两个原因:首先，标准的NLP库是为处理拉丁字符而设计的，所以让它们适应楔形文字有点痛苦。第二，好玩！我坚信测试你在一个主题上的知识的最好方法是从基础开始，看看你是否能用尽可能少的专业库来编码它。</p><p id="f509" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我提出的N-gram模型实现受到了使用<a class="ae le" href="https://en.wikipedia.org/wiki/Chain_rule_(probability)" rel="noopener ugc nofollow" target="_blank">概率链规则</a>和<a class="ae le" href="https://en.wikipedia.org/wiki/Markov_property" rel="noopener ugc nofollow" target="_blank">马尔可夫假设</a>的<a class="ae le" rel="noopener" target="_blank" href="/the-beginners-guide-to-language-models-aa47165b57f9">经典N-gram模型</a>的启发。简单来说，这个假设说明了一个n元文法在位置<em class="mb"> t </em>的概率只取决于前面的<em class="mb"> k </em> n元文法(其中<em class="mb"> k </em>通常为1)。因此，可以通过取其n元文法的条件概率的乘积来计算序列的概率，直到<em class="mb"> t-k </em>。</p><p id="93a2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">嗯，我的代码并没有完全做到这一点，而是更接近和更简单。它实际上是计算每种语言的训练集中所有n元语法的无条件概率。然后，它通过乘以序列中出现的所有n元语法的无条件概率来估计观察到的序列属于每种语言的概率。选择概率最高的语言作为预测标签。</p><p id="d4b8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">显然，可以开发更复杂的方法。然而，我发现这个非常简单的模型在测试集上已经表现得非常好了。</p><h1 id="db2c" class="me mf iq bd mg mh mi mj mk ml mm mn mo kf mp kg mq ki mr kj ms kl mt km mu mv bi translated">履行</h1><p id="375e" class="pw-post-body-paragraph lf lg iq lh b li mw ka lk ll mx kd ln lo my lq lr ls mz lu lv lw na ly lz ma ij bi translated">现在我们对语言模型有了更好的理解，让我们看看如何为楔形文字编写自定义实现。就代码而言，这一部分有点重，但是请相信我:付出的努力是值得的！或者，您可以转到结果部分，了解故事的结局。</p><p id="9853" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们从一些辅助函数开始。第一个，我们称之为<code class="fe nb nc nd ne b">preprocess</code>，将楔形文字的字符串转换成对我们来说更方便的格式。它执行两个操作:首先，它添加一个句子开头和结尾的标记(我们分别称之为B和E)，然后它在字符之间添加一个空格。b和E是有用的标记:它们帮助我们计算一个句子以给定字符开始和结束的概率。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nf ng l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">预处理功能。来源:作者</p></figure><p id="6c61" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">现在，我们需要计算n-grams！下面的两个函数是我们实现的核心，因为它们将文本分割成n个字母，并计算每个字母在我们的示例中出现的次数。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nf ng l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">N-gram计数函数。来源:作者</p></figure><p id="2b5e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们的最后一个帮助函数<code class="fe nb nc nd ne b">logprob</code>，将用于计算n元文法的整个序列的对数概率。在这种类型的问题中，我们通常处理非常小的概率，所以取对数通常是跟踪我们的数字的好主意。这里我们有另一个关于经典语言模型的简化。通常，N-gram模型会在其词汇表中包含一个“未知”单词标记，以估计找到真正不常见单词的概率。然而，在我们的例子中，我们将简单地假设找到这样的字符的概率是0.00001(比我们最稀有的一些字符低大约10倍)。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nf ng l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">计算一个序列的对数概率。来源:作者</p></figure><p id="3950" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">就是这样！这些都是我们需要的辅助函数。现在轮到定义一个类来包装我们的模型，您可以在下面的代码片段中看到。因为没有更好的名字，就叫它<code class="fe nb nc nd ne b">CuneiPy</code>吧。我们的包装类遵循经典的Scikit-Learn结构，包含两个方法:<code class="fe nb nc nd ne b">.fit()</code>和<code class="fe nb nc nd ne b">.predict()</code>。<code class="fe nb nc nd ne b">.fit()</code>方法计算每个n元语法和语言的无条件概率(记住，我们有七个)。这就是n元语法在n元语法总数中出现的次数。<code class="fe nb nc nd ne b">.predict()</code>方法获取楔形文字字符序列，并计算该序列属于我们的一种语言的概率。然后，它取概率最高的语言，选择它作为预测标签。</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nf ng l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">CuneiPy —模型的包装类。来源:作者</p></figure><h1 id="fcd7" class="me mf iq bd mg mh mi mj mk ml mm mn mo kf mp kg mq ki mr kj ms kl mt km mu mv bi translated">结果</h1><p id="8cb0" class="pw-post-body-paragraph lf lg iq lh b li mw ka lk ll mx kd ln lo my lq lr ls mz lu lv lw na ly lz ma ij bi translated">如果你现在还和我在一起，你可能会想这个东西到底有没有用。你可能已经猜到了答案:是的！虽然肯定有改进的空间，但我们的简单模型在1000个观察值的测试集上表现得非常好，加权f 1分数约为0.8(取决于训练-测试划分)。</p><p id="7dd6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">测试<code class="fe nb nc nd ne b">CuneiPy</code>就像这样简单:</p><figure class="kp kq kr ks gt kt"><div class="bz fp l di"><div class="nf ng l"/></div><p class="la lb gj gh gi lc ld bd b be z dk translated">测试CuneiPy。来源:作者</p></figure><p id="6dca" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">正如我们在<code class="fe nb nc nd ne b">.fit()</code>部分看到的，我们使用的是二元模型(因此<em class="mb"> n = 2 </em>)。列“楔形文字”是我们语言模型的输入，我们的目标列由数据集中的“lang”定义。下面的分类报告为我们提供了一个模型性能最佳的细分。也许不足为奇的是，该模型更善于识别新亚述语(NEA)和索姆利语(SUX)，这两种最常见的语言。该模型也很好地识别了晚期巴比伦语(LTB ),尽管只有11%的文本属于这种阿卡德方言。中巴比伦周边语(MPB)是我们的模型最难对付的方言。这与我们在样本中对这种语言的表述是如此之少(占所有观察结果的3.9%)是一致的。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi md"><img src="../Images/88b0a56d2a0705345cf809e82815b5c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JcyEQCOEtXceWunkBvtkig.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">分类报告。来源:作者对Kaggle <a class="ae le" href="https://www.kaggle.com/wilstrup/cuneiform-language-identification" rel="noopener ugc nofollow" target="_blank">数据集</a>的分析</p></figure><h1 id="484a" class="me mf iq bd mg mh mi mj mk ml mm mn mo kf mp kg mq ki mr kj ms kl mt km mu mv bi translated">主要外卖</h1><p id="caac" class="pw-post-body-paragraph lf lg iq lh b li mw ka lk ll mx kd ln lo my lq lr ls mz lu lv lw na ly lz ma ij bi translated">在本文中，我们看到了如何创建一个定制的N-gram语言模型来预测古代美索不达米亚楔形文字的方言或语言。该模型是经典N元模型的一个版本，利用了概率链规则和马尔可夫假设。虽然简单，但该模型实现了0.8的F1分数。</p><p id="71bb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我希望你喜欢这篇文章。如果你喜欢，你可以访问这个<a class="ae le" href="https://github.com/AlvaroCorrales/Cuneiform-classification" rel="noopener ugc nofollow" target="_blank"> Github库</a>中的代码。</p><p id="4548" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">如有任何反馈，请通过LinkedIn<a class="ae le" href="https://www.linkedin.com/in/alvaro-corrales-cano/" rel="noopener ugc nofollow" target="_blank">或电子邮件联系我！</a></p></div></div>    
</body>
</html>