<html>
<head>
<title>For Better-Performing Models, Don’t Assume Data Is I.I.D. without Checking</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">对于性能更好的模型，不要未经检查就假设数据是独立的</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/want-to-ruin-your-model-assume-data-is-i-i-d-78c61a0b2076?source=collection_archive---------40-----------------------#2021-12-14">https://towardsdatascience.com/want-to-ruin-your-model-assume-data-is-i-i-d-78c61a0b2076?source=collection_archive---------40-----------------------#2021-12-14</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="c148" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">关注数据中的自相关可以帮助您构建更好的预测模型</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/fa42b5a4e6cb158098c2a5178dd08dc9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*J5-sJSNrhikkcA6S"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated"><a class="ae kv" href="https://unsplash.com/@theeastlondonphotographer?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Ehimetalor Akhere Unuabona </a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><p id="5836" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，一个小测验。</p><p id="64a4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在下面的两个例子中，报告的准确性可信吗？</p><ol class=""><li id="116e" class="ls lt iq ky b kz la lc ld lf lu lj lv ln lw lr lx ly lz ma bi translated">Shivram 想到了一个绝妙的主意，仅仅通过 iPhone 的移动来预测心率。他从数千名同意的用户那里收集 iPhone 动作的时间同步数据和 Apple watch 的心率数据。然后，他将数据<em class="mb">随机</em>一秒一秒地分成训练集、验证集和测试集。在他对自己的模型感到满意之后，他报告说，他能够从 iPhone 的运动中预测心率，在测试集上的准确率高达 98%!</li><li id="e171" class="ls lt iq ky b kz mc lc md lf me lj mf ln mg lr lx ly lz ma bi translated">Abhilash 希望利用卫星图像来寻找森林的位置。他获得了一些卫星图像和人类绘制的森林地理位置图的训练数据。然后，他将像素随机分成训练集、验证集和测试集。在他对他的模型满意之后，他报告他的测试准确率为 99%！</li></ol><p id="eed9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在上述两种情况下，报告的准确性可信吗？</p><p id="3439" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">不要！</p><p id="2f8c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这篇文章中，我们将了解为什么他们不是。我们还将学习一些基本的预处理原则，人们可以遵循这些原则来避免将来出现这样的陷阱。</p><h1 id="7925" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">为什么要关心数据是否是 id？</h1><p id="f911" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">独立同分布数据(i.i.d .)在预测设置中有许多好的特性<a class="ae kv" rel="noopener" target="_blank" href="/independent-and-identically-distributed-ce250ad1bfa8">在预测设置中，知道一个数据点并不能告诉你关于另一个数据点的任何事情。当我们为模型训练拆分数据时，必须知道数据是否是独立的。</a></p><p id="7bd1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将数据分为训练集、验证集和测试集是在监督学习设置中测试模型性能的最标准方法之一。即使在我们进入建模之前(这在机器学习中几乎受到了所有的关注)，不关心上游过程，如数据来自哪里，它们是否真的相同，以及我们如何分割它们，都会对预测的质量产生影响。</p><p id="961d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当数据具有高自相关性时，这尤其重要。点之间的自相关仅仅意味着一个点上的值与其周围的值相似。以温度为例。预计任何时刻的温度都与前一分钟的温度相似。因此，如果我们希望预测温度，我们需要特别注意分割数据。具体来说，我们需要确保在训练、验证和测试集之间没有可能夸大模型性能的数据泄漏。</p><h1 id="9bc3" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">模型性能会因信息泄露而被夸大到什么程度？</h1><p id="4150" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">看完以上，很自然的会问，这是一个足够重要的问题，值得我去关心吗？通过一个高度自相关数据的例子，我们会看到答案当然是肯定的！我们将把这个例子分成两部分。首先，我们将数据随机分为训练集和验证集，并在验证集上实现非常高的准确性。然后，我们将使用分层随机抽样来分割数据，从而减少信息泄漏。然后，我们将看到同样的模型如何具有几乎为零的精度。</p><h1 id="5725" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">交互式示例</h1><p id="10c6" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">如果您希望以交互方式了解这个示例，您可以使用<a class="ae kv" href="https://colab.research.google.com/drive/1EJFCIX4enOvwLby_0DPFX4-KwG-S7YTF#scrollTo=lgm0yAIwN6O9" rel="noopener ugc nofollow" target="_blank">这个 colab 笔记本</a>。</p><p id="a59b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们先导入相关的包。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="c5ea" class="nj mi iq nf b gy nk nl l nm nn">import matplotlib.pyplot as plt<br/>import numpy as np<br/>import pandas as pd<br/>import seaborn as sns<br/>import sklearn.model_selection<br/>import sklearn.linear_model<br/>import sklearn.ensemble</span></pre><p id="c325" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们来做一些响应变量自相关性高的合成数据。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="b99e" class="nj mi iq nf b gy nk nl l nm nn"># number of examples in our data<br/>n = int(100*2*np.pi)<br/># Seed for reproducebility<br/>np.random.seed(4)<br/># make one feature (predictor)<br/>x = np.arange(n)<br/># make one response (variable to predict) which has high autocorrelation. Use a<br/># sine wave.<br/>y =  np.sin(x/n*7.1*np.pi)+np.random.normal(scale = 0.1, size = n)<br/># merge them into a dataframe to allow easy manipulation later<br/>df = pd.DataFrame({"x":np.array(x), "y":np.array(y), "y_pred":np.nan})<br/># visualize the response versus feature<br/>sns.set(style = "ticks", font_scale = 1.5)<br/>sns.regplot(x="x",y="y",data=df)</span></pre><h2 id="b2af" class="nj mi iq bd mj no np dn mn nq nr dp mr lf ns nt mt lj nu nv mv ln nw nx mx ny bi translated">数据的随机分割</h2><p id="087f" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">让我们将数据随机分为训练集和验证集，看看模型的表现如何。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="ad37" class="nj mi iq nf b gy nk nl l nm nn"># Use a helper to split data randomly into 5 folds. i.e., 4/5ths of the data is chosen *randomly* and put into the train set, while the rest is put into</span><span id="4174" class="nj mi iq nf b gy nz nl l nm nn"># is chosen *randomly* and put into the train set, while the rest is put into</span><span id="c59c" class="nj mi iq nf b gy nz nl l nm nn"># the validation set.</span><span id="0a49" class="nj mi iq nf b gy nz nl l nm nn">kf = sklearn.model_selection.KFold(n_splits=5, shuffle=True, random_state=42)</span><span id="2bd4" class="nj mi iq nf b gy nz nl l nm nn"># Use a random forest model with default parameters.</span><span id="622c" class="nj mi iq nf b gy nz nl l nm nn"># The hyperaparameter of the model are not important for this example because we</span><span id="d2dd" class="nj mi iq nf b gy nz nl l nm nn"># will use the same model twice- once with data split randomly and (later) with</span><span id="27b2" class="nj mi iq nf b gy nz nl l nm nn"># data split with stratification</span><span id="b0ee" class="nj mi iq nf b gy nz nl l nm nn">reg = sklearn.ensemble.RandomForestRegressor()</span><span id="a90f" class="nj mi iq nf b gy nz nl l nm nn"># use k-1 folds to train. Predict on the kth fold and store in the dataframe</span><span id="7abd" class="nj mi iq nf b gy nz nl l nm nn">for fold, (train_index, test_index) in enumerate(kf.split(df)):</span><span id="8822" class="nj mi iq nf b gy nz nl l nm nn">reg.fit(df.loc[train_index, "x"].values.reshape(-1, 1), df.loc[train_index, "y"])</span><span id="ef5d" class="nj mi iq nf b gy nz nl l nm nn">df.loc[test_index, "y_pred"] = reg.predict(df.loc[test_index, "x"].values.reshape(-1, 1))</span><span id="baf3" class="nj mi iq nf b gy nz nl l nm nn"># visualize true y versus predicted y</span><span id="6550" class="nj mi iq nf b gy nz nl l nm nn">fig, ax = plt.subplots(figsize = (5,5))</span><span id="e721" class="nj mi iq nf b gy nz nl l nm nn">sns.kdeplot(</span><span id="5a1f" class="nj mi iq nf b gy nz nl l nm nn">data=df, x="y_pred", y="y",</span><span id="f869" class="nj mi iq nf b gy nz nl l nm nn">fill=True, thresh=0.3, levels=100, cmap="mako_r",ax=ax</span><span id="fb22" class="nj mi iq nf b gy nz nl l nm nn">)</span><span id="74fd" class="nj mi iq nf b gy nz nl l nm nn">ax.set_xlim(-2,2)</span><span id="c6a2" class="nj mi iq nf b gy nz nl l nm nn">ax.set_ylim(-2,2)</span><span id="3ac5" class="nj mi iq nf b gy nz nl l nm nn">ax.set_xlabel(r"y$_{\rm predicted}$")</span><span id="9f20" class="nj mi iq nf b gy nz nl l nm nn">ax.set_title("Exaggerated predictive ability\nassuming data is i.i.d.")</span><span id="7fab" class="nj mi iq nf b gy nz nl l nm nn">r2 = sklearn.metrics.r2_score(df.y, df.y_pred)</span><span id="e501" class="nj mi iq nf b gy nz nl l nm nn">ax.annotate(f"R$^2$ = {r2:0.2f}", xy =(0.95,0.95), ha = "right", va = "top", xycoords = "axes fraction")</span><span id="038d" class="nj mi iq nf b gy nz nl l nm nn">print(f"[INFO] Coefficient of determination of the model is {r2:0.2f}.")</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/86a7a4ab38b0c6f769e473e33c1de5b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*G-pGUekwNIPbVDwYMa-5sw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">假设数据是独立的，夸大了预测能力(但实际上不是)。</p></figure><p id="15eb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">哇哦。！我们实现了 97%的 R2！似乎我们的模型在模拟正弦响应函数方面做得非常出色。</p><p id="7da6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">但是……模型<em class="mb">真的</em>能够理解 x 和 y 之间的响应函数吗？或者它只是作为一个最近邻插值？换句话说，模型只是通过记忆训练数据，输出最接近训练样本的 y 值来作弊吗？让我们通过使模型难以作弊来找出答案。</p><h2 id="a2c0" class="nj mi iq bd mj no np dn mn nq nr dp mr lf ns nt mt lj nu nv mv ln nw nx mx ny bi translated">数据的分层分割</h2><p id="3944" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">现在，我们将沿着 x(特征)轴将数据分成 5 个块，而不是随机分割数据。然后，我们将把 4 个数据块放入训练数据，1 个数据块放入验证集。</p><blockquote class="ob oc od"><p id="3717" class="kw kx mb ky b kz la jr lb lc ld ju le oe lg lh li of lk ll lm og lo lp lq lr ij bi translated">通过沿着自相关特征对数据进行分层，我们尊重了数据的非独立身份性质。</p></blockquote><p id="ae68" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们看看模型是否有同样的精度。</p><pre class="kg kh ki kj gt ne nf ng nh aw ni bi"><span id="e7cc" class="nj mi iq nf b gy nk nl l nm nn"># How many chunks to split data in? <br/>nbins = 5<br/>df["fold"] = pd.cut(df.x, bins = nbins, labels = range(nbins))</span><span id="ae26" class="nj mi iq nf b gy nz nl l nm nn"># Split the data into training and validation data based on the chunks.<br/># Train on 4 chunks, predict on the remaining chunk.<br/>for fold in sorted(df.fold.unique()):<br/>  train_index = df.loc[df.fold!=fold].index<br/>  test_index = df.loc[df.fold==fold].index<br/>  reg.fit(df.loc[train_index, "x"].values.reshape(-1, 1), df.loc[train_index, "y"])<br/>  df.loc[test_index, "y_pred"] = reg.predict(df.loc[test_index, "x"].values.reshape(-1, 1))<br/># Visualize true y versus precited y.<br/>fig, ax = plt.subplots(figsize = (5,5))<br/>sns.kdeplot(<br/>    data=df, x="y_pred", y="y",<br/>    fill=True, thresh=0.3, levels=100, cmap="mako_r",ax=ax<br/>)<br/>ax.set_xlim(-2,2)<br/>ax.set_ylim(-2,2)<br/>ax.set_xlabel(r"y$_{\rm predicted}$")<br/>ax.set_title("True predictive ability")<br/>r2 = sklearn.metrics.r2_score(df.y, df.y_pred)<br/>ax.annotate(f"R$^2$ = {r2:0.2f}", xy =(0.95,0.95), ha = "right", va = "top", xycoords = "axes fraction")<br/>print(f"[INFO] Coefficient of determination of the model is {r2:0.2f}.")</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/1bb3ea0357433b92fc42aadb29b6ba2f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1156/format:webp/1*qPvSU-lfqUj6bOSJHbmtfg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">同一模型的真实预测能力</p></figure><p id="f442" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们看到我们的模型具有低于随机的性能(旁注:不知道决定系数怎么会是负的？<a class="ae kv" rel="noopener" target="_blank" href="/r²-or-r²-when-to-use-what-4968eee68ed3">在这里阅读更多</a>)！这表明，我们的初始模型并没有真正使用 x 作为 y 的信息预测器，而只是从训练集中找到最接近的 x，并吐出相应的 y。因此，如果我们不小心数据中的自相关，我们可能会夸大模型性能。</p><p id="800e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">更糟糕的是，我们可能错误地推断出 x 的重要性，并进而得出几个科学结论。然而，我们的模型仅使用 x 来插入/记忆响应。不幸的是，这不是一个虚构的例子。<a class="ae kv" href="https://www.nature.com/articles/s41467-020-18321-y" rel="noopener ugc nofollow" target="_blank">这篇论文</a>表明，地球科学中试图预测植被生物量的几篇论文(类似于本文开头 Abhilash 的例子)都被这个问题弄得千疮百孔。</p><h1 id="b98a" class="mh mi iq bd mj mk ml mm mn mo mp mq mr jw ms jx mt jz mu ka mv kc mw kd mx my bi translated">结论</h1><p id="ea21" class="pw-post-body-paragraph kw kx iq ky b kz mz jr lb lc na ju le lf nb lh li lj nc ll lm ln nd lp lq lr ij bi translated">拆分数据可能会产生巨大的后果。如果有任何证据表明数据是自相关的，或者更一般地说是非同分布的，则分层分裂或使用信号分解来解相关数据的其他技术可能是有用的。至少，在开始建模之前将数据可视化是非常有益的。因此，下次你遇到 Shivram、Abhilash 或其他任何声称在随机拆分他们的数据后实现了非常高的建模性能的人时，你已经准备好帮助他们提出更好的预测模型，而不会夸大模型性能。</p></div></div>    
</body>
</html>