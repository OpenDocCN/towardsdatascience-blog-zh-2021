<html>
<head>
<title>Deep Split Q-learning and Ms. Pacman</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">深度分裂Q-learning和Pacman女士</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/deep-split-q-learning-and-ms-pacman-5749791d55c8?source=collection_archive---------21-----------------------#2021-05-09">https://towardsdatascience.com/deep-split-q-learning-and-ms-pacman-5749791d55c8?source=collection_archive---------21-----------------------#2021-05-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="e38e" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="bb9b" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">通过修改更新规则在深度RL环境中重新创建分裂Q学习的尝试</h2></div><p id="b81f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">2013年，谷歌<a class="ae lk" href="https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learning" rel="noopener ugc nofollow" target="_blank"> DeepMind </a>的一篇论文引发了深度强化学习(RL)，特别是深度Q学习网络(DQN)的爆炸。2015年，这是在基础上改进的<a class="ae lk" href="https://arxiv.org/abs/1509.06461" rel="noopener ugc nofollow" target="_blank">，具有双重深度Q学习网络。这个网站和其他网站上有许多帖子详细介绍了如何构建一个在那篇论文中非常成功的网络版本；这篇文章是关于扩展这个框架到最近的一个想法:分裂Q学习。分裂Q学习已经在一些不同的环境中被提出，但是林等人的论文。2020年自治主体和多主体系统国际会议(AAMAS)接受的人工智能是本文的重点。</a></p><p id="7f14" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在这里，我们将看看这种范式是什么，并尝试在《吃豆人MDP》上实现它的深度Q学习网络，这是范·哈瑟尔特等人在2015年的论文中唯一表现不佳的游戏之一。艾尔。代码可以在<a class="ae lk" href="https://github.com/Kickflip89/DQN_project" rel="noopener ugc nofollow" target="_blank">这个链接</a>上公开获得，它源于我在研究生院做的一个项目。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi ll"><img src="../Images/9322620a8605f297cd5ae2417eec8c98.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*uWwdcxZTIOOKxVvrMxxt1g.png"/></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">OpenAI Gym的MsPacman环境的开始状态</p></figure></div><div class="ab cl lx ly hu lz" role="separator"><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc md"/><span class="ma bw bk mb mc"/></div><div class="ij ik il im in"><h2 id="0e19" class="me mf iq bd mg mh mi dn mj mk ml dp mm kx mn mo mp lb mq mr ms lf mt mu mv iw bi translated">背景</h2><p id="875d" class="pw-post-body-paragraph ko kp iq kq b kr mw ka kt ku mx kd kw kx my kz la lb mz ld le lf na lh li lj ij bi translated">DeepMind架构已经在很多地方解释过了，在<a class="ae lk" href="https://gym.openai.com/envs/#atari" rel="noopener ugc nofollow" target="_blank">健身房</a>的突破环境的模型和教程可以在<a class="ae lk" href="https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26" rel="noopener ugc nofollow" target="_blank">成为人类</a>找到。我们的版本将使用PyTorch vice TensorFlow，所以如果你没有看过他们的基本深度Q-learning教程<a class="ae lk" href="https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html" rel="noopener ugc nofollow" target="_blank">你可能也想看看，我假设读者至少对Q-learning有点熟悉。</a></p><p id="7cf0" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">作为概述，2015年的论文以RGB Atari图像作为游戏状态，以分数作为主要的奖励系统。最高分范·哈塞尔特等人。al调优的双深度Q-Learning网络在50M训练迭代后，在特定的评测实验中能够达到was 3210。</p><p id="4d0e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">林等人提出的分裂Q学习。人工智能是一种将奖励分为积极和消极两种的想法。在正常的Q学习中，对模型或表的更新是基于预期奖励(Q值)和实际奖励(奖励加上下一个状态中的最大Q值)之间的差异。如何应用这种差异可以有所不同，我们可以使用L1，L2类型的更新，但我们通常也会将学习率α应用于我们决定使用的任何更新。数学上，对于Q表、状态s、行动a、奖励r和新状态s’，这看起来像:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nb"><img src="../Images/34aba621a51e2aed94281b36297b21a9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_1WbBUx26b4LVwupMzRNRg@2x.png"/></div></div></figure><p id="3c5e" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这里，γ是预计未来回报的折现率。等式1是来自贝尔曼等式的Q值的定义，而等式2来自时间差分(TD)型训练。然后基于学习率α: Q(s，a) ← Q(s，a) + α*diff更新Q表。在林等人提出的分裂Q学习中。al，有一个单独的奖励和惩罚表(或模型),它遵循的思想是，生物制剂倾向于用单独的而不一定是排他的机制来处理奖励和惩罚。除了分流，林等人。al还建议改变如何通过两个超参数来更新Q值。其中一个，W，在每个状态转换时缩放奖励。另一个λ缩放更新前的旧Q值:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi ng"><img src="../Images/e6aec61c12e88cd39302df8954b1960a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rgfNgR9UqeCLsXThQr-Bsw@2x.png"/></div></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">注意:方程式编号可能有问题</p></figure><p id="5560" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这两个表是奖励(+)和惩罚(-)的。深度Q-learning的这次更新特别困难的是，目标不仅仅是基于贝尔曼方程(方程1)，它是λ的目标的折扣版本，好像深度Q-learning在移动目标和收敛方面没有足够的问题！值得注意的是，αt(学习率)和γ(折扣)在正向和负向流之间共享。对于Q表的更新规则，林等。所有人都能够在代理人身上表现出与某些奖赏处理障碍的临床行为相匹配的紧急行为。最值得注意的是，他们的“慢性疼痛”模型将两个奖励超参数都设置为0.5，显示出与实际慢性疼痛患者的临床行为相似的紧急行为。我最初的目标是用深度分裂Q-learning框架复制这种行为，并可能尝试改进Hasselt et。al基线，虽然这两个都还在工作。</p><h2 id="2a03" class="me mf iq bd mg mh mi dn mj mk ml dp mm kx mn mo mp lb mq mr ms lf mt mu mv iw bi translated">预备:预处理、重放缓冲、深度Q学习</h2><p id="fcd2" class="pw-post-body-paragraph ko kp iq kq b kr mw ka kt ku mx kd kw kx my kz la lb mz ld le lf na lh li lj ij bi translated">让深度Q学习网络(DQN)和双重深度Q学习网络(DDQN)收敛有几个重要的技巧，因此我们将详细介绍它们，以及它们是如何在这个项目的PyTorch代码库中实现的。第一个是我们的状态表示和重放缓冲区。MsPacmanDeterministic-v4(我们将使用的健身房环境)中的状态表示为大小为(3x210x160)的RGB图像，并且是0–255范围内的整数元素。</p><p id="ff0c" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">为了节省内存，我们将这些帧存储为向下采样的灰度图像，并将它们转换为uint8 (8位与64位相比，存储的100000多个帧有很大的不同)。根据DeepMind的论文，我们还希望包含最后几帧，以给当前图像一些上下文，并防止代理在每次帧更新时计算动作。这个预处理步骤被封装在下面的代码片段中:</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="d5eb" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated"><code class="fe nj nk nl nm b">.unsqueeze(0)</code>是因为我们将把4个帧连接成一个状态供网络处理，所以一个图像的输出大小是(1x105x80)，但是一个状态最终将是(1x4x105x80)。暂时忽略self参数，它最终将成为DQN包装类的一部分。</p><p id="d306" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">重放缓冲区只是一个存储大量MDP转换的对象。您可能还记得，MDP转换采用一个状态-动作对，并返回新状态s’和采取该动作的即时回报r。在我们的实现中，我们将在更高的级别收集这些不同的元素，并将我们需要的所有内容的元组传递到缓冲区。这个缓冲区被实现为一个环形缓冲区，这意味着一旦我们超过了允许的最大元素数，它就开始覆盖它的元素。重放缓冲区的目的是能够从缓冲区中对成批的随机元素进行采样:这打破了状态之间的相关性，并允许我们使用深度神经网络来估计Q值。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="1a72" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">实际的代理类需要用超参数(现在是gamma，和某种探索/开发方案)初始化，初始化游戏状态，初始化模型，并运行下面的训练循环:</p><ol class=""><li id="1566" class="nn no iq kq b kr ks ku kv kx np lb nq lf nr lj ns nt nu nv bi translated">决定一项行动(勘探/开发)</li><li id="9f2c" class="nn no iq kq b kr nw ku nx kx ny lb nz lf oa lj ns nt nu nv bi translated">采取行动，记录到重放缓冲区的转换</li><li id="8f80" class="nn no iq kq b kr nw ku nx kx ny lb nz lf oa lj ns nt nu nv bi translated">从重放缓冲区取样并训练策略网络</li></ol><p id="bdad" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">在本文中，我将抽象出DQN类(实际的火炬网络)的细节，因为它对于本次讨论并不重要。只要说它有三个卷积层、一个展平层和所有动作的估计Q值的输出就足够了。正如以上教程中所解释的，输出完整的Q值集并使用一键编码来处理反向传播和单动作选择会更快。</p><p id="ec37" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我将简要提及，由于DeepMind论文等论文中的一些结果，我们正在使用Huber Loss(smoothl Loss)和RMSprop优化器进行训练，但当然还有其他选项(例如MSE)。下面是我们的初始化和内务功能(环境、epsilon缩放、超参数):</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="edc1" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如果你对这个循环还不太熟悉，我建议你去读一读更深入的Q-learning或者深度Q-learning教程，比如上面提到的那些。第二项是大多数环境争论发生的地方。因为我们的状态代表四帧，所以我们需要聚集奖励，并在环境中的一些时间步长上连接处理过的图像。这是由下面的代码块完成的:</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="nh ni l"/></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">一个“动作”时间步由多个帧组成。生命['ale.lives']在下面进一步解释</p></figure><p id="528f" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">然后，我们将一组<code class="fe nj nk nl nm b"> old_state, action, reward, new_state, is_done</code>传递给重放缓冲区，并使用重放缓冲区的<code class="fe nj nk nl nm b">sample()</code>方法拟合来自样本的模型。</p><p id="594a" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们需要讨论的最后一个初步主题是深度Q学习的训练方法。您可能已经注意到，在<code class="fe nj nk nl nm b">__init__()</code>方法中，我们创建了两个DQN模型:一个策略模型和一个目标模型。您可能还记得，在Q表中，我们通过Q(s，a)←Q(s，a) + α*L(diff)来更新表。在一个模型中，我们不能更新<em class="ob">值</em>；我们必须通过某种形式的梯度下降来更新模型的<em class="ob">参数</em>。但是更新规则应该是什么，它转移到split-Q学习吗？一种方法是根据对Q表的<a class="ae lk" rel="noopener" target="_blank" href="/temporal-difference-learning-47b4a7205ca8">时间差异</a>更新来查看更新规则，并将其推广到深度Q学习框架。TD本质上是上面的等式2，其中r+γmax(Q(s’，a’))项是损失函数的目标，而策略网络对Q(s，a)的输出被调整:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi oc"><img src="../Images/dceb1ac9f94473a2b24a2f87ab2a67cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8WYz-PBqpRLyjov5RcUbYw@2x.png"/></div></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">范.哈瑟尔特等人的形式主义。阿尔，2015</p></figure><p id="c294" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这里θ’是目标网络，θ是策略网络。对于完整的训练循环，我们需要做的最后一件事是确定ϵ缩放和目标网络更新规则。这些最好从文献和做一些实验开始。MsPacman上的典型DQN示例如下所示，其中τ(目标网络更新周期)和ϵ(勘探/开采政策)具有不同的超参数。</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi od"><img src="../Images/f6882752de6be7d02cf49f9f44c7ada4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1078/format:webp/1*g27t3BoGy83Im09b7seapw.png"/></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">上图:慢速ϵ斜坡和300个时期的目标网络更新。Bot:快速ϵ斜坡与500个时代的目标网络更新。</p></figure><p id="5bad" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">完整训练循环的最后一个修改是增加了对失去一条生命(影响一个幽灵)的惩罚的能力，这不是内置在MsPacman健身房环境中的，但在Lin et中有很大的特点。艾尔关于分裂Q学习的论文。为了做到这一点，我们可以利用gym在python字典中输出剩余生命数的事实(<code class="fe nj nk nl nm b">lives = lives['ale.lives']</code>)。我们现在可以建立一个完整的Q迭代:</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="nh ni l"/></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">深度Q学习网络的Q迭代。第26行允许我们修改撞击幽灵的惩罚</p></figure><p id="5295" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">拥有一个<code class="fe nj nk nl nm b">total_reward</code>和一个<code class="fe nj nk nl nm b">total_score</code>变量的原因是，尽管它们在这个实现中做同样的事情，但是在分离Q-learning实现中它们是不同的。基本的训练循环是在指定数量的迭代或时期内保持运行这个Q-学习函数，在达到终端状态的任何时候重置环境。<code class="fe nj nk nl nm b">fit_buffer()</code>函数本质上实现了等式7和8，但是我们将把它留给分裂Q学习实现。</p><h2 id="634c" class="me mf iq bd mg mh mi dn mj mk ml dp mm kx mn mo mp lb mq mr ms lf mt mu mv iw bi translated">深度分裂Q学习网络和双重深度Q学习</h2><p id="45a6" class="pw-post-body-paragraph ko kp iq kq b kr mw ka kt ku mx kd kw kx my kz la lb mz ld le lf na lh li lj ij bi translated">为了尝试将分裂Q学习方法移动到深度学习框架中，我们必须改变更新规则，以尝试匹配等式4和5。从方程5开始，如果我们去掉学习率，我们得到一个有趣的方程，它成为贝尔曼对Q(s，a)的定义，如果λ=1。暂且不考虑奖励权重，这看起来像是:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi oe"><img src="../Images/05edb90864a4cf2d807c5455f5524fd4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6zoHuB9NNtf525p9irIbWA@2x.png"/></div></div></figure><p id="8d22" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">等式10本质上只是从等式5中移除α，但它对于Lin等人的Q(s，a)的不同定义也有一定意义。艾尔的分裂Q学习范式。因为他们对历史进行贴现，所以新的Q值有两个目标:λQ(s，a)和r + γmax(Q(s '，a ')。通过<em class="ob">从通常的更新中减去</em> (1-λ)Q(s，a ),这些方程本质上是制定一个新的更新规则，该规则逐渐将Q值向两个目标移动。我们现在可以将TD逻辑应用于上述内容，并将其转移到深度Q学习范式，其中(λ-1)Q(s，a)项由目标网络计算，并添加到正常的r + γ(π(s))项。右手边的Q(s，a)项由策略网络计算，现在我们有损失函数的两个项:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi of"><img src="../Images/2648edc157c3c35450b3fdfeb1d7bcf1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*B9mucOCQbN-fYxk4y4jAaw@2x.png"/></div></div></figure><p id="b7f9" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">换句话说，7–8和12–13的唯一区别是等式12中的新项:(1-λ)Q(s，a；θ),它将目标移向贴现Q值。如果λ=1，这就简化为正常的Q更新规则。当我们加入即时奖励的权重时，完全分割Q学习算法是:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div class="gh gi og"><img src="../Images/5a1ecf3487c00245be8025fd97a79483.png" data-original-src="https://miro.medium.com/v2/resize:fit:1174/format:webp/1*6pO-7NtGqWkFapbNK9NkHA.png"/></div><p class="lt lu gj gh gi lv lw bd b be z dk translated">深度分裂Q学习算法，基于等式10-13，L代表损失函数</p></figure><p id="3000" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">我们还可以实现一个版本的深度分裂Q学习，它遵循双重深度Q学习方法。双重深度Q学习和深度Q学习之间的区别在于，在计算未来折扣奖励时，策略网络用于<em class="ob">选择</em>行动，而目标网络用于<em class="ob">评估</em>这些行动。数学上，Y的定义变成:</p><figure class="lm ln lo lp gt lq gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi oh"><img src="../Images/6f6f37fbdaf63b9ddedc382494b47ccb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ajkJfokqcizBZemC56VQxw@2x.png"/></div></div></figure><p id="b6ba" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">这是在等式12中用于分裂Q学习更新规则的一个相当容易的替代，我们将把这个想法用于<code class="fe nj nk nl nm b">fit_buffer()</code>实现。这种两个流方法的“最佳动作”是两个流的Q值之和的最大可能动作。但是，在培训期间，我们仍然使用每个流来计算基于该流的最佳行为的未来折扣奖励。我们只对Q值求和，以实际选择给定状态下的最佳行动。下面是代码中初始化和最佳操作选择的样子:</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="1a14" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">最后要实现的是<code class="fe nj nk nl nm b">fit_buffer()</code>函数，它处理模型的训练更新。该功能必须完成以下步骤:</p><ol class=""><li id="25bf" class="nn no iq kq b kr ks ku kv kx np lb nq lf nr lj ns nt nu nv bi translated">将批处理解包为状态、动作、奖励、惩罚、下一个状态、完成</li><li id="3535" class="nn no iq kq b kr nw ku nx kx ny lb nz lf oa lj ns nt nu nv bi translated">使用策略网络来选择下一个状态中的最佳行为，以进行奖励和惩罚</li><li id="3952" class="nn no iq kq b kr nw ku nx kx ny lb nz lf oa lj ns nt nu nv bi translated">使用目标网络评估未来的贴现报酬，以及目标中(1-λ)Q(s，a)项的Q(s，a)</li><li id="7f90" class="nn no iq kq b kr nw ku nx kx ny lb nz lf oa lj ns nt nu nv bi translated">计算策略网络对每个流的Q值的估计</li><li id="12b2" class="nn no iq kq b kr nw ku nx kx ny lb nz lf oa lj ns nt nu nv bi translated">计算每个流的损失和反向传播</li></ol><p id="6e57" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">下面是我对奖励流(惩罚流实现完全一样)的这个函数的实现。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="d7be" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">如果不知道DQN是如何工作的，这个函数的一些细节可能很难理解，但它需要一个动作和状态输入的掩码，这就是为什么有很多一个热向量和一个向量飞来飞去。随意浏览<a class="ae lk" href="https://github.com/Kickflip89/DQN_project" rel="noopener ugc nofollow" target="_blank">项目</a>，看看DQN类是如何工作的，以及Q-learning和双Q-learning的实现。</p><h2 id="5e17" class="me mf iq bd mg mh mi dn mj mk ml dp mm kx mn mo mp lb mq mr ms lf mt mu mv iw bi translated">结果和结论</h2><p id="496c" class="pw-post-body-paragraph ko kp iq kq b kr mw ka kt ku mx kd kw kx my kz la lb mz ld le lf na lh li lj ij bi translated">延伸林等人。al从Q表到深度Q学习网络的分割Q学习不是一个简单的任务，但是我相信这代表了这样做的逻辑框架。目前，我必须提供的唯一评估是代理在用类似于Lin et的参数初始化时的行为。艾尔的作品。在这个实验中，我修改了流，在每个时间步长提供-1的惩罚，在每次失去一个生命时提供-500的惩罚。然后，我用. 5初始化了一个代理，奖励权重W和过去的折扣λ(他们的慢性疼痛模型)。在林等人看来。艾尔的论文，这导致了一个代理人，不参与奖励寻求行为，但有能力避免鬼，如果他们太接近。避免MsPacman中的幽灵似乎是这些CNN风格的Q值估计网络的一个主要挑战(可能是因为幽灵倾向于在存在中闪现和消失)，所以我们将比较双重深度Q学习网络和慢性疼痛(CP)代理。每个代理被训练10，000集，在500，000次迭代中epsilon从1到0.1渐变。</p><p id="b45b" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">DQN的代理人在下面，但还没有真正学会避开幽灵。相反，它的策略是吃下强力药丸，以防止鬼魂在游戏的开始阶段成为一个因素。</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="oi ni l"/></div></figure><p id="c2e5" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">慢性疼痛剂训练了相同的时间，并开始表现出一些行为。艾尔观察到了，但仍偶尔从事寻求奖励的行为。可能10，000个训练集不足以使Q值收敛到Lin et的行为。艾尔在他们的模型中看到了，但令人鼓舞的是看到了冷漠和回避风险的开端！</p><figure class="lm ln lo lp gt lq"><div class="bz fp l di"><div class="oi ni l"/></div></figure><h2 id="4843" class="me mf iq bd mg mh mi dn mj mk ml dp mm kx mn mo mp lb mq mr ms lf mt mu mv iw bi translated">参考</h2><p id="40cc" class="pw-post-body-paragraph ko kp iq kq b kr mw ka kt ku mx kd kw kx my kz la lb mz ld le lf na lh li lj ij bi translated">[1]柏寒·林、吉列尔莫·切奇、贾雷尔·布内夫、詹娜·赖宁和伊琳娜·里什。2020.<em class="ob">两股潮流的故事:来自人类行为和神经精神病学的强化学习模型</em>。arXiv:1906.11286 [cs。LG]</p><p id="bef7" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[2] Hado van Hasselt、Arthur Guez和David Silver。2015.<em class="ob">采用双Q学习的深度强化学习</em>。arXiv:1509.06461 [cs。LG]</p><p id="25ea" class="pw-post-body-paragraph ko kp iq kq b kr ks ka kt ku kv kd kw kx ky kz la lb lc ld le lf lg lh li lj ij bi translated">[3] V. Mnih、K. Kavukcuoglu、D. Silver、A. A .鲁苏、J. Veness、M. G .贝勒马尔、A. Graves、M. Riedmiller、A. K. Fidjeland、G. Ostrovski、S. Petersen、C. Beattie、A. Sadik、I. Antonoglou、H. King、D. Kumaran、D. Wierstra、S. Legg和D. Hassabis。<em class="ob">通过深度强化学习的人类水平控制</em>。自然，518(7540):529–533，2015。</p></div></div>    
</body>
</html>