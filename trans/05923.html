<html>
<head>
<title>How to Fine-Tune BERT With NSP</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用NSP微调伯特</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-fine-tune-bert-with-nsp-8b5615468e12?source=collection_archive---------13-----------------------#2021-05-27">https://towardsdatascience.com/how-to-fine-tune-bert-with-nsp-8b5615468e12?source=collection_archive---------13-----------------------#2021-05-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="9e33" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">使用变压器和PyTorch轻松微调</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/29e02790127127fa914a93b8cccb22f9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-9IEzrq66vuBNUzAmu6RRw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">硬拉，伯特的最爱——作者图片</p></figure><p id="6f70" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi lu translated">下一句预测(NSP)是BERT模型(另一个是掩蔽语言建模——MLM)后面的训练过程的一半。</p><p id="fc3c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">尽管NSP(和MLM)被用来预先训练BERT模型，我们可以使用这些精确的方法来微调我们的模型，以更好地理解我们自己的用例中特定的语言风格。</p><p id="43d4" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，在本文中，我们将详细介绍如何获取非结构化的文本，并使用NSP对BERT模型进行微调。</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h1 id="274d" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">培养</h1><p id="f7b8" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">那么，我们如何使用NSP对模型进行微调呢？</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nh ni l"/></div></figure><p id="309c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，我们需要数据。因为我们本质上只是在连续句和随机句之间切换——我们可以使用几乎任何文本。我们不需要有标签的或特殊的数据。</p><p id="276c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这里，我们将使用<em class="nj">马库斯·奥勒留</em>的<em class="nj">冥想</em>，来源于<a class="ae nk" href="http://classics.mit.edu/Antoninus/meditations.html" rel="noopener ugc nofollow" target="_blank">这里</a>并稍加预处理(<a class="ae nk" href="https://github.com/jamescalam/transformers/blob/main/data/text/meditations/clean.txt" rel="noopener ugc nofollow" target="_blank">干净版</a>)。</p><p id="7891" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">首先，我们将导入/初始化并加载我们的文本数据。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl ni l"/></div></figure><p id="635c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们需要在标记化之前对数据进行预处理，这样我们就可以得到一组随机和非随机的句子对。</p><h2 id="b635" class="nm ml it bd mm nn no dn mq np nq dp mu lh nr ns mw ll nt nu my lp nv nw na nx bi translated">NSP预处理</h2><p id="45b4" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">首先，我们将创建一个随机句子的<code class="fe ny nz oa ob b">bag</code>——我们将根据句点字符对其进行拆分。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl ni l"/></div></figure><p id="5c66" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这就给我们留下了1372个句子样本。从这里开始，我们需要创建句子对，并标记它们是连续的句子对，还是随机的句子对。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl ni l"/></div></figure><p id="6d9c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们可以在最后一个细胞的输出中看到我们创建的两对细胞。第一个标记为<em class="nj"> 1 </em>并显示一个<code class="fe ny nz oa ob b">NotNextSentence</code>对，第二个标记为<em class="nj"> 0 </em>并显示一个<code class="fe ny nz oa ob b">IsNextSentence</code>对。</p><h2 id="95f3" class="nm ml it bd mm nn no dn mq np nq dp mu lh nr ns mw ll nt nu my lp nv nw na nx bi translated">标记化</h2><p id="43f2" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">我们的数据现在可以进行标记化了，这次我们将每个序列截断/填充到512个标记的相同长度。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl ni l"/></div></figure><p id="597c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在<em class="nj"> token_type_ids </em>张量中，我们可以看到同样的<em class="nj"> 0 </em>后面跟着<em class="nj"> 1 </em>记号(分别是句子A和B)，后面跟着更多的<em class="nj"> 0 </em>记号。这些用于指示填充标记。</p><p id="d0c2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们还需要一个<em class="nj">标签</em>张量，它将对应于包含在<code class="fe ny nz oa ob b">label</code>变量中的值。我们的<em class="nj">标签</em>张量——和以前一样——必须是一个<code class="fe ny nz oa ob b">torch.LongTensor</code>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl ni l"/></div></figure><h2 id="f89f" class="nm ml it bd mm nn no dn mq np nq dp mu lh nr ns mw ll nt nu my lp nv nw na nx bi translated">输入管道</h2><p id="e6c2" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">现在我们的输入张量已经准备好了，我们可以开始为训练构建模型输入管道。我们首先从数据中创建PyTorch数据集。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl ni l"/></div></figure><p id="e06c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">并初始化数据加载器，这是我们在训练中将用来将数据加载到模型中的。</p><h2 id="1fff" class="nm ml it bd mm nn no dn mq np nq dp mu lh nr ns mw ll nt nu my lp nv nw na nx bi translated">培训设置</h2><p id="817a" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">现在，在开始训练之前，我们设置最后几件事情。我们将模型移动到GPU(如果可用)，激活训练模式，并初始化我们的优化器。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl ni l"/></div></figure><p id="fed6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们使用Adam with weighted decay作为我们的优化器，这是训练变压器模型的常见选择，因为它减少了过度拟合的可能性(由于大多数变压器的巨大尺寸，这是一个常见的问题)。</p><h2 id="ae46" class="nm ml it bd mm nn no dn mq np nq dp mu lh nr ns mw ll nt nu my lp nv nw na nx bi translated">训练循环</h2><p id="6d8c" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">最后，我们可以进入训练循环。我们将训练几个时期来展示损失随时间的变化。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nl ni l"/></div></figure><p id="30ef" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">就这样，我们用NSP微调了我们的伯特模型！</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><p id="7b9f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这就是这篇关于用NSP微调伯特的文章。</p><p id="d193" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">NSP有很多，但是概念和实现并不太复杂——但是功能非常强大。</p><p id="f3fa" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">使用我们在这里学到的知识，我们可以采用NLP中最好的模型，并对它们进行微调以适应我们更特定于领域的语言用例——只需要未标记的文本——通常是很容易找到的数据源。</p><p id="20e5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我希望你喜欢这篇文章！如果你有任何问题，请通过<a class="ae nk" href="https://twitter.com/jamescalam" rel="noopener ugc nofollow" target="_blank"> Twitter </a>或在下面的评论中告诉我。如果你想要更多这样的内容，我也会在<a class="ae nk" href="https://www.youtube.com/c/jamesbriggs" rel="noopener ugc nofollow" target="_blank"> YouTube </a>上发布。</p><p id="434f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢阅读！</p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><h1 id="1480" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">参考</h1><p id="d6d0" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">[1] J .德夫林等人。艾尔。，<a class="ae nk" href="https://arxiv.org/pdf/1810.04805.pdf" rel="noopener ugc nofollow" target="_blank"> BERT:语言理解深度双向转换器预训练</a> (2019)，NAACL</p><p id="d0c1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae nk" href="https://bit.ly/nlp-transformers" rel="noopener ugc nofollow" target="_blank">🤖《变形金刚》NLP课程70%的折扣</a></p></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><p id="ac96" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">如果你有兴趣了解更多关于NSP到底是如何运作的——看看这篇文章:</p><div class="oc od gp gr oe of"><a rel="noopener follow" target="_blank" href="/bert-for-next-sentence-prediction-466b67f8226f"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd iu gy z fp ok fr fs ol fu fw is bi translated">用于下一句预测的BERT</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">另一半用来训练伯特</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">towardsdatascience.com</p></div></div><div class="oo l"><div class="op l oq or os oo ot ks of"/></div></div></a></div></div><div class="ab cl md me hx mf" role="separator"><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi mj"/><span class="mg bw bk mh mi"/></div><div class="im in io ip iq"><p id="f8bb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="nj">*所有图片均由作者提供，除非另有说明</em></p></div></div>    
</body>
</html>