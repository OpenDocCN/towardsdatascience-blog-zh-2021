<html>
<head>
<title>Upgrade Your Beginner NLP Project with BERT</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用BERT升级你的初学者NLP项目</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/text-classification-with-bert-2e0297ea188a?source=collection_archive---------11-----------------------#2021-03-29">https://towardsdatascience.com/text-classification-with-bert-2e0297ea188a?source=collection_archive---------11-----------------------#2021-03-29</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="bf40" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">深度学习不一定要复杂</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/cf7ee006139dcd9be8676cdc9321f06b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DSr9mHoBZkByt_8n"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">布雷特·乔丹在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h1 id="5369" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">介绍</h1><p id="eeee" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">当我刚开始学习数据科学和看项目时，我认为你可以做深度学习或常规项目。事实并非如此。</p><p id="25a3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">随着强大的模型变得越来越容易获得，我们可以轻松地利用深度学习的一些功能，而不必优化神经网络或使用GPU。</p><p id="33d1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在这篇文章中，我们将看看<strong class="lt iu">嵌入</strong>。这是深度学习模型将单词表示为向量的方式。我们可以将模型的一部分生成嵌入，并在上面安装一个常规的(<em class="ms"> scikit-learn </em>)模型，以获得一些令人难以置信的结果！</p><p id="eb17" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我将分别解释每种方法，用图表来表示它的工作原理，并展示如何在Python中实现这些技术。</p><h1 id="7cbc" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">目录</h1><ul class=""><li id="3c1b" class="mt mu it lt b lu lv lx ly ma mv me mw mi mx mm my mz na nb bi translated"><a class="ae ky" href="#b225" rel="noopener ugc nofollow">先决条件</a></li><li id="a076" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm my mz na nb bi translated"><a class="ae ky" href="#ca97" rel="noopener ugc nofollow">将单词表示为向量</a></li><li id="3386" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm my mz na nb bi translated"><a class="ae ky" href="#9bce" rel="noopener ugc nofollow">单词包方法</a> <br/> - <a class="ae ky" href="#924f" rel="noopener ugc nofollow">计数向量器</a>-<br/>-<a class="ae ky" href="#2880" rel="noopener ugc nofollow">TF-IDF</a></li><li id="91d6" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm my mz na nb bi translated"><a class="ae ky" href="#b612" rel="noopener ugc nofollow">嵌入文字作为向量</a><br/>-<a class="ae ky" href="#b525" rel="noopener ugc nofollow">word 2 vec</a><br/>-<a class="ae ky" href="#3f73" rel="noopener ugc nofollow">手套</a> <br/> - <a class="ae ky" href="#1bdd" rel="noopener ugc nofollow"> Doc2Vec </a></li><li id="ee3a" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm my mz na nb bi translated"><a class="ae ky" href="#cc62" rel="noopener ugc nofollow">基于变压器的型号</a> <br/> - <a class="ae ky" href="#51c1" rel="noopener ugc nofollow">通用语句编码器</a>-<br/>BERT</li><li id="1a67" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm my mz na nb bi translated"><a class="ae ky" href="#f3cb" rel="noopener ugc nofollow">结论</a></li></ul><h1 id="b225" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">先决条件</h1><ul class=""><li id="2c99" class="mt mu it lt b lu lv lx ly ma mv me mw mi mx mm my mz na nb bi translated">你应该了解<strong class="lt iu">机器学习</strong>的基础知识。</li><li id="7c8f" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm my mz na nb bi translated">为了充分利用这一点，您应该知道如何在<em class="ms"> scikit-learn </em>中拟合模型，并且已经有了一个适合于<strong class="lt iu"> NLP </strong>的数据集。</li><li id="527c" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm my mz na nb bi translated">本教程对于已经有一个<strong class="lt iu"> NLP </strong>项目并且希望升级它并尝试深度学习的人来说非常理想。</li><li id="c12a" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm my mz na nb bi translated">本文中的每个模型都增加了复杂性。本文将解释基本原理以及如何使用该技术，但是您可能希望访问一些提供的链接来完全理解这些概念。</li></ul><h1 id="5614" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">资料组</h1><p id="6cb3" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">为了说明每个模型，我们将使用Kaggle <em class="ms"> NLP和灾难推特</em>数据集。这是大约10，000条推文，这些推文是根据关键词(例如<em class="ms">着火</em>)挑选出来的，然后标记它们是否是关于一场真正的灾难。</p><p id="4d28" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">您可以在此阅读比赛内容并查看结果:</p><div class="nh ni gp gr nj nk"><a href="https://www.kaggle.com/c/nlp-getting-started" rel="noopener  ugc nofollow" target="_blank"><div class="nl ab fo"><div class="nm ab nn cl cj no"><h2 class="bd iu gy z fp np fr fs nq fu fw is bi translated">灾难微博的自然语言处理</h2><div class="nr l"><h3 class="bd b gy z fp np fr fs nq fu fw dk translated">预测哪些推文是关于真正的灾难，哪些不是</h3></div><div class="ns l"><p class="bd b dl z fp np fr fs nq fu fw dk translated">www.kaggle.com</p></div></div><div class="nt l"><div class="nu l nv nw nx nt ny ks nk"/></div></div></a></div><p id="d8b2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">您可以在这里查看或克隆所有代码:</strong></p><div class="nh ni gp gr nj nk"><a href="https://github.com/AdamShafi92/Exploring-Embeddings" rel="noopener  ugc nofollow" target="_blank"><div class="nl ab fo"><div class="nm ab nn cl cj no"><h2 class="bd iu gy z fp np fr fs nq fu fw is bi translated">adamshafi 92/探索-嵌入</h2><div class="nr l"><h3 class="bd b gy z fp np fr fs nq fu fw dk translated">看看不同的方法来嵌入文本使用的数据从ka ggle:https://www.kaggle.com/c/nlp-getting-started运行…</h3></div><div class="ns l"><p class="bd b dl z fp np fr fs nq fu fw dk translated">github.com</p></div></div><div class="nt l"><div class="nz l nv nw nx nt ny ks nk"/></div></div></a></div><h1 id="c5c2" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">可视化</h1><p id="9e45" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们将使用2个<strong class="lt iu">可视化</strong>来探索每个模型。我在下面列举了一些例子。</p><p id="1553" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">将文字形象化……</strong></p><p id="fca6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">一个<a class="ae ky" href="https://pair-code.github.io/understanding-umap/" rel="noopener ugc nofollow" target="_blank"> UMAP </a>表示所有的句子。UMAP是一种降维方法，它允许我们仅在二维空间中查看高维度的单词表示。</p><blockquote class="oa"><p id="22e4" class="ob oc it bd od oe of og oh oi oj mm dk translated"><strong class="ak">降维</strong>是将数据从高<strong class="ak">维</strong>空间转换到低<strong class="ak">维</strong>空间，以便低<strong class="ak">维</strong>表示保留原始数据的一些有意义的属性，理想情况下接近其固有<strong class="ak">维</strong>。</p></blockquote><p id="d3f4" class="pw-post-body-paragraph lr ls it lt b lu ok ju lw lx ol jx lz ma om mc md me on mg mh mi oo mk ml mm im bi translated">这对于可视化主题集群非常有用，但是如果你以前没有遇到过降维，这可能会令人困惑。我们本质上只是在寻找我们的词被分成集群，其中具有相似主题的<strong class="lt iu">推文在空间上彼此接近。蓝色(非灾难)和橙色(灾难)文本之间的清晰区分也是很好的，因为这表明我们的模型能够很好地对这些数据进行分类。</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="op oq l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图表。关于灾难的推文的UMAP表现。鼠标悬停时打开原件显示推文。</p></figure><p id="408e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">评估模型性能… </strong></p><p id="169e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">一组5张图表。从左至右:</p><ol class=""><li id="d762" class="mt mu it lt b lu mn lx mo ma or me os mi ot mm ou mz na nb bi translated"><strong class="lt iu"> ROC AUC。这是一个典型的评分系统，允许我们比较模型。它考虑了预测的概率</strong></li><li id="389d" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm ou mz na nb bi translated"><strong class="lt iu">精度/召回</strong>。另一个典型指标是，我们正在寻找一个大而平滑的AUC。</li><li id="7f98" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm ou mz na nb bi translated"><strong class="lt iu">特征重要性</strong>。这样我们就可以比较每种方法的效果。对伯特来说，这不会显示太多，但有助于说明可解释性</li><li id="9dc5" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm ou mz na nb bi translated"><strong class="lt iu">预测概率</strong>。这使我们能够直观地看到模型如何区分这两个类别。理想情况下，我们希望看到0和1的集群只有很少的50%左右。</li><li id="527b" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm ou mz na nb bi translated"><strong class="lt iu">混淆矩阵</strong>。我们可以想象假阳性对假阴性。</li></ol><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="op oq l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图表。评估模型性能的5幅图。</p></figure><h1 id="25ba" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">定义</h1><ul class=""><li id="53c7" class="mt mu it lt b lu lv lx ly ma mv me mw mi mx mm my mz na nb bi translated"><strong class="lt iu">矢量:</strong>矢量的经典描述是一个既有大小又有方向的量(例如向西5英里)。在机器学习中，我们经常使用高维向量。</li><li id="96ab" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm my mz na nb bi translated"><strong class="lt iu">嵌入:</strong>将一个词(或句子)表示为向量的一种方式。</li><li id="0660" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm my mz na nb bi translated"><strong class="lt iu">文档:</strong>一个单独的文本。</li><li id="f281" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm my mz na nb bi translated"><strong class="lt iu">语料库:</strong>一组文本。</li></ul><h1 id="ca97" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">将单词表示为向量</h1><p id="7411" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">为了创建基于单词的模型，我们必须将这些单词转换成数字。最简单的方法是对每个单词进行热编码，并告诉我们的模型:</p><ul class=""><li id="b3f0" class="mt mu it lt b lu mn lx mo ma or me os mi ot mm my mz na nb bi translated"><strong class="lt iu">句子#1 </strong>有单词#1，单词#12，单词#13。</li><li id="df99" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm my mz na nb bi translated"><strong class="lt iu">句子#2 </strong>有单词#6，单词#24，单词#35。</li></ul><p id="b9b0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">单词袋和TDF-IDF以这种方式表示单词，并在此基础上增加了一些单词出现频率的度量。</p><blockquote class="oa"><p id="7def" class="ob oc it bd od oe of og oh oi oj mm dk translated">单词包方法通过简单地为每个单词创建一个列并用一个数字表示该单词出现的位置来将单词表示为向量。向量的大小与语料库中唯一单词的数量相同。</p></blockquote><p id="e4cf" class="pw-post-body-paragraph lr ls it lt b lu ok ju lw lx ol jx lz ma om mc md me on mg mh mi oo mk ml mm im bi translated">这对于某些方法来说很好，但是我们丢失了关于在同一个句子中有不同意思的单词的信息，或者上下文如何改变一个单词的意思。</p><blockquote class="oa"><p id="e2f3" class="ob oc it bd od oe of og oh oi oj mm dk translated">将单词转换成数字或向量，称为嵌入单词。我们可以把一组变成向量的单词描述为嵌入。</p></blockquote><p id="4136" class="pw-post-body-paragraph lr ls it lt b lu ok ju lw lx ol jx lz ma om mc md me on mg mh mi oo mk ml mm im bi translated">我们对单词进行矢量化的目的是以一种尽可能获取更多信息的方式来表示单词…</p><blockquote class="ov ow ox"><p id="ddd5" class="lr ls ms lt b lu mn ju lw lx mo jx lz oy mp mc md oz mq mg mh pa mr mk ml mm im bi translated">我们如何告诉模型一个词和另一个词相似？它是怎么知道完全不同的单词意思是一样的呢？或者另一个单词如何改变它后面的单词的意思呢？或者甚至当一个单词在同一个句子中有多个意思的时候？(<a class="ae ky" href="https://en.wikipedia.org/wiki/Buffalo_buffalo_Buffalo_buffalo_buffalo_buffalo_Buffalo_buffalo" rel="noopener ugc nofollow" target="_blank">水牛水牛水牛水牛水牛水牛水牛水牛水牛</a> —我在看你)</p></blockquote><p id="7a7a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">深度学习已经允许开发各种技术，这些技术在回答大多数这些问题方面有很大的帮助。</p><h1 id="9bce" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">单词袋方法</h1><p id="a3bc" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">这是最简单的表示单词的方式。我们将每个句子表示为一个向量，取语料库中的所有单词，根据每个单词是否出现在句子中，给每个单词一个1或0。</p><p id="dc2c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">你可以看到随着<strong class="lt iu">字数的增加</strong>，它会变得非常大<strong class="lt iu"/>。这个问题是我们的矢量开始变得<strong class="lt iu">稀疏</strong>。如果我们有很多包含各种单词的短句，我们的数据集中就会有很多0。稀疏会成倍增加我们的计算时间。</p><p id="7a9f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们可以通过对每个单词进行<strong class="lt iu">计数</strong>，而不仅仅是1或0，来“升级”一包单词的表示。当我们进行计数时，我们也可以删除在语料库中不常出现的单词，例如，我们可以删除出现次数少于5次的每个单词。</p><p id="0866" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">另一种提高单词量的方法是使用n-grams。这只是用了n个单词，而不是1个。这有助于捕捉句子中更多的上下文。</p><h2 id="924f" class="pb la it bd lb pc pd dn lf pe pf dp lj ma pg ph ll me pi pj ln mi pk pl lp pm bi translated">计数矢量器</h2><p id="3b34" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">直觉</strong></p><p id="3eb8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这是矢量语言最简单的方法。我们简单地计算句子中的每个单词。在大多数情况下，建议删除非常常见的单词和非常罕见的单词。</p><p id="6ef4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">实施</strong></p><pre class="kj kk kl km gt pn po pp pq aw pr bi"><span id="206c" class="pb la it po b gy ps pt l pu pv">from sklearn.feature_extraction.text import CountVectorizer</span><span id="94a8" class="pb la it po b gy pw pt l pu pv">bow = CountVectorizer(min_df=5,max_df=.99, ngram_range=(1, 2)) #remove rare and common words with df parameter<br/>#include single and 2 word pairs</span><span id="a863" class="pb la it po b gy pw pt l pu pv">X_train_vec = bow.fit_transform(X_train[‘text’])<br/>X_test_vec = bow.transform(X_test[‘text’])</span><span id="ff62" class="pb la it po b gy pw pt l pu pv">cols = bow.get_feature_names() #if you need feature names</span><span id="9d75" class="pb la it po b gy pw pt l pu pv">model = RandomForestClassifier(n_estimators=500, n_jobs=8)<br/>model.fit(X_train_vec, y_train)<br/>model.score(X_test_vec, y_test)</span></pre><p id="e859" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">可视化</strong></p><p id="f60d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这个4000维向量的2d表示并不好。我们在中间有一个斑点和许多不同的点。我们的模型没有明确的方法来聚集或分离数据。</p><p id="0e46" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">您可以打开图，将鼠标悬停在上面，查看每个点是什么。</strong></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="px oq l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图表。按“查看原文”打开互动版本，查看原始推文。</p></figure><p id="7007" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">无论如何，我们的模型表现得相当好，它能够区分一些相当数量的推文。然而，从特性的重要性我们可以看出，它主要是通过使用<strong class="lt iu">URL</strong>来做到这一点的。这是发现灾难微博的有效方法吗？</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="px oq l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图表。评估模型性能的5幅图。</p></figure><h2 id="2880" class="pb la it bd lb pc pd dn lf pe pf dp lj ma pg ph ll me pi pj ln mi pk pl lp pm bi translated">TF-IDF</h2><p id="30ca" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">直觉</strong></p><p id="5dff" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">使用单词包和计数的一个问题是，频繁出现的单词，如<em class="ms">和</em>，开始主导特征空间，而不提供任何附加信息。可能有更重要的特定领域的单词，但是由于它们不经常出现而被模型丢失或忽略。</p><p id="35dd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">TF-IDF代表<strong class="lt iu">词频—逆文档频率</strong></p><ul class=""><li id="0364" class="mt mu it lt b lu mn lx mo ma or me os mi ot mm my mz na nb bi translated"><strong class="lt iu">词频</strong>:该词在当前文档中的频率得分。</li><li id="f2f4" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm my mz na nb bi translated"><strong class="lt iu">逆文档频率</strong>:对单词在语料库中的稀有程度进行评分。</li></ul><p id="80b4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在TF-IDF中，我们像在单词包中一样，使用单词的频率对单词进行评分。然后<strong class="lt iu">惩罚</strong>在所有文档中<strong class="lt iu">频繁出现的</strong>单词(比如the、and、or)。</p><p id="5432" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们也可以将n-grams与TF-IDF一起使用。</p><p id="2625" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">实施</strong></p><pre class="kj kk kl km gt pn po pp pq aw pr bi"><span id="936f" class="pb la it po b gy ps pt l pu pv">from sklearn.feature_extraction.text import TfidfVectorizer</span><span id="e4ca" class="pb la it po b gy pw pt l pu pv">tfidf= TfidfVectorizer(min_df=5,max_df=.99, ngram_range=(1, 2)) #remove rare and common words with df parameter<br/>#include single and 2 word pairs</span><span id="cba3" class="pb la it po b gy pw pt l pu pv">X_train_vec = tfidf.fit_transform(X_train[‘text’])<br/>X_test_vec = tfidf.transform(X_test[‘text’])</span><span id="0148" class="pb la it po b gy pw pt l pu pv">cols = tfidf.get_feature_names() #if you need feature names</span><span id="b561" class="pb la it po b gy pw pt l pu pv">model = RandomForestClassifier(n_estimators=500, n_jobs=8)<br/>model.fit(X_train_vec, y_train)<br/>model.score(X_test_vec, y_test)</span></pre><p id="17f5" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">可视化</strong></p><p id="753b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">TF-IDF与该数据集上的计数矢量器没有太大区别。灾难和非灾难推文之间仍然有很多重叠。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="op oq l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图表。按“查看原文”打开互动版本，查看原始推文。</p></figure><p id="314e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">通过使用TF-IDF，我们看到了模型性能的小幅提升。一般来说，这确实表现得更好，因为我们降低了通常不会给模型增加任何东西的常见单词的权重。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="op oq l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图表。评估模型性能的5幅图。</p></figure><h1 id="b612" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">将单词作为向量嵌入</h1><p id="d732" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">单词袋模型有3个关键问题:</p><ol class=""><li id="e2dc" class="mt mu it lt b lu mn lx mo ma or me os mi ot mm ou mz na nb bi translated"><strong class="lt iu">相似的词互不相关。</strong>模型不知道<em class="ms">不好</em>和<em class="ms">可怕</em>这两个词是相似的，只知道这两个都和负面情绪有关。</li><li id="16be" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm ou mz na nb bi translated">单词不在上下文中。讽刺甚至<em class="ms">还不错</em>可能都没捕捉好。具有双重含义的单词不会被捕获。</li><li id="b6e6" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm ou mz na nb bi translated">使用大型语料库会产生非常大的稀疏向量。这使得大规模计算变得困难。</li></ol><blockquote class="ov ow ox"><p id="966d" class="lr ls ms lt b lu mn ju lw lx mo jx lz oy mp mc md oz mq mg mh pa mr mk ml mm im bi translated">通过深度学习，我们从简单的表示转移到嵌入。与之前的方法不同，深度学习模型通常<strong class="lt iu">输出固定长度的向量</strong>，该向量<strong class="lt iu">不必与语料库</strong>中的字数相同。我们现在为数据集中的每个单词或句子创建一个唯一的向量表示。</p></blockquote><h2 id="b525" class="pb la it bd lb pc pd dn lf pe pf dp lj ma pg ph ll me pi pj ln mi pk pl lp pm bi translated">Word2Vec</h2><p id="316c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">Word2Vec是一种生成嵌入的深度学习方法，发表于2013年。它可以相对容易地在你的语料库上进行训练，但本教程的目的是使用预训练的方法。我将简要地解释模型是如何被训练的。</p><p id="e3d7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这个模型有两种训练方式。</p><ul class=""><li id="333d" class="mt mu it lt b lu mn lx mo ma or me os mi ot mm my mz na nb bi translated"><strong class="lt iu"> Skip-gram: </strong>模型循环遍历句子中的每个单词，并尝试预测相邻的单词。</li><li id="dd05" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm my mz na nb bi translated"><strong class="lt iu">连续单词包:</strong>模型循环遍历每个单词，并使用周围的<em class="ms"> n个</em>单词来预测它。</li></ul><p id="bcd9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">要深入了解这一模式，只需看看杰伊·阿拉姆的这篇精彩文章就够了。</p><p id="717c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">实施</strong></p><p id="cf0e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了实现Word2Vec，我们将使用一个在Gensim的<strong class="lt iu"> Google News </strong>数据集上训练的版本。该模型为每个单词输出大小为300的向量。理论上，相似的单词应该有相似的向量表示。</p><p id="f332" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">Word2Vec和GLoVe的一个问题是我们不能轻易生成一个<strong class="lt iu">句子</strong>嵌入。</p><p id="37bf" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">要生成嵌入Word2Vec或GLoVe的句子，我们必须为每个单词生成一个300大小的向量，然后<strong class="lt iu">对它们进行平均</strong>。这样做的问题是，尽管相似的句子应该有相似的句子向量，但我们丢失了任何关于单词的<strong class="lt iu">顺序的信息。</strong></p><pre class="kj kk kl km gt pn po pp pq aw pr bi"><span id="d37c" class="pb la it po b gy ps pt l pu pv">import gensim<br/>import gensim.models as g<br/>import gensim.downloader<br/>from spacy.tokenizer import Tokenizer<br/>from spacy.lang.en import English</span><span id="69d1" class="pb la it po b gy pw pt l pu pv"><br/>def vectorize_sentence(sentence,model):<br/>    nlp = English()<br/>    tokenizer = Tokenizer(nlp.vocab)<br/>    a = []<br/>    for i in tokenizer(sentence):<br/>        try:<br/>            a.append(model.get_vector(str(i)))<br/>        except:<br/>            pass<br/>        <br/>    a=np.array(a).mean(axis=0)<br/>    a = np.zeros(300) if np.all(a!=a) else a<br/>    return a</span><span id="b1e4" class="pb la it po b gy pw pt l pu pv">word2vec = gensim.downloader.load('word2vec-google-news-300') #1.66 gb</span><span id="430c" class="pb la it po b gy pw pt l pu pv"># vectorize the data</span><span id="c024" class="pb la it po b gy pw pt l pu pv">X_train_vec = pd.DataFrame(np.vstack(X_train['text'].apply(vectorize_sentence, model=word2vec)))<br/>X_test_vec = pd.DataFrame(np.vstack(X_test['text'].apply(vectorize_sentence, model=word2vec)))</span><span id="073f" class="pb la it po b gy pw pt l pu pv"># Word2Vec doesn't have feature names</span><span id="13e3" class="pb la it po b gy pw pt l pu pv">model = RandomForestClassifier(n_estimators=500, n_jobs=8)<br/>model.fit(X_train_vec, y_train)<br/>model.score(X_test_vec, y_test)</span></pre><p id="c377" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">可视化</strong></p><p id="d9dc" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">乍一看，Word2Vec似乎比以前的方法更好地表示了我们的数据。有清晰的蓝色区域和单独的橙色区域。左上角的群集似乎主要是大写字母的单词，在其他地区有关于天气的推文。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="op oq l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图表。按“查看原文”打开互动版本，查看原始推文。</p></figure><p id="474f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">不幸的是，乍一看，这与模型性能无关。精度得分比TF-IDF稍差。然而，如果我们看看混淆矩阵，我们可以看到这个模型在识别灾难微博方面做得更好。</p><p id="b482" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这里的一个大问题是，我们现在不知道是什么推动了这些更好的预测。有一个特性很明显被模型使用的比其他的多，但是不做额外的工作我们无法发现这代表了什么。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="op oq l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图表。评估模型性能的5幅图。</p></figure><h2 id="3f73" class="pb la it bd lb pc pd dn lf pe pf dp lj ma pg ph ll me pi pj ln mi pk pl lp pm bi translated">手套</h2><p id="a469" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">直觉</strong></p><p id="36ff" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">手套代表Glo bal。</p><p id="8527" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">GloVe类似于Word2Vec，因为它是一种早期的嵌入方法，已于2014年发布。然而，GloVe的关键区别在于，GloVe不仅仅依赖于附近的单词，而是结合了<strong class="lt iu">全局统计</strong> — <strong class="lt iu">跨语料库</strong>的单词出现，以获得单词向量。</p><p id="e35f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">训练GloVe的方式是通过计算语料库中每个单词的共现矩阵。然后，对这个矩阵进行某种类型的降维，将其缩减到一个固定的大小，为每个句子留下一个向量。我们可以很容易地访问这个模型的预训练版本。如果你想知道更多关于它是如何工作的，请看这里。</p><p id="d68e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">实施</strong></p><p id="5604" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们使用的是GloVe ' <em class="ms"> Gigaword </em>'模型，它是在维基百科<strong class="lt iu">语料库上训练的。您会注意到它的大小比Word2Vec模型小得多，这表明它可能只训练了较少的单词。这是一个问题，因为GLoVe不能识别我们数据集中的一个单词，它将返回一个<strong class="lt iu">错误</strong>(我们用0代替……)。</strong></p><pre class="kj kk kl km gt pn po pp pq aw pr bi"><span id="5215" class="pb la it po b gy ps pt l pu pv">import gensim<br/>import gensim.models as g<br/>import gensim.downloader<br/>from spacy.tokenizer import Tokenizer<br/>from spacy.lang.en import English</span><span id="2d6e" class="pb la it po b gy pw pt l pu pv">def vectorize_sentence(sentence,model):<br/>    nlp = English()<br/>    tokenizer = Tokenizer(nlp.vocab)<br/>    a = []<br/>    for i in tokenizer(sentence):<br/>        try:<br/>            a.append(model.get_vector(str(i)))<br/>        except:<br/>            pass<br/>        <br/>    a=np.array(a).mean(axis=0)<br/>    a = np.zeros(300) if np.all(a!=a) else a<br/>    return a</span><span id="9d6c" class="pb la it po b gy pw pt l pu pv">gv = gensim.downloader.load('glove-wiki-gigaword-300') #376mb</span><span id="e806" class="pb la it po b gy pw pt l pu pv"># vectorize the data</span><span id="0676" class="pb la it po b gy pw pt l pu pv">X_train_vec = pd.DataFrame(np.vstack(X_train['text'].apply(vectorize_sentence, model=gv)))<br/>X_test_vec = pd.DataFrame(np.vstack(X_test['text'].apply(vectorize_sentence, model=gv)))</span><span id="c967" class="pb la it po b gy pw pt l pu pv"># GloVe doesn't have feature names</span><span id="2fd6" class="pb la it po b gy pw pt l pu pv">model = RandomForestClassifier(n_estimators=500, n_jobs=8)<br/>model.fit(X_train_vec, y_train)<br/>model.score(X_test_vec, y_test)</span></pre><p id="827c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">可视化</strong></p><p id="6b5b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">手套向量很有趣，右上角的区域是每个单词首字母大写的推文。这不是我们有兴趣区分的东西。否则蓝色和橙色会有很多重叠。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="op oq l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图表。按“查看原文”打开互动版本，查看原始推文。</p></figure><p id="78b6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">到目前为止，我们的手套模型表现明显比其他模型差。最可能的原因是这个模型不理解我们语料库中的许多单词。为了解决这个问题，你必须自己在语料库(或一些Twitter数据)上训练这个模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="op oq l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图表。评估模型性能的5幅图。</p></figure><h2 id="1bdd" class="pb la it bd lb pc pd dn lf pe pf dp lj ma pg ph ll me pi pj ln mi pk pl lp pm bi translated"><strong class="ak"> Doc2Vec </strong></h2><p id="ac78" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated"><strong class="lt iu">直觉</strong></p><p id="06e4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">GLoVe和Word2Vec的关键问题在于<strong class="lt iu">我们只是对整个句子</strong>进行平均。Doc2Vec针对句子进行了预训练，应该可以更好地表示我们的句子。</p><p id="411e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">实现</strong></p><p id="697e" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">Doc2Vec不是Gensim库的一部分，所以我在网上找到了一个已经过预训练的版本，但是我不确定是什么版本。</p><pre class="kj kk kl km gt pn po pp pq aw pr bi"><span id="1b98" class="pb la it po b gy ps pt l pu pv"># Model downloaded from <a class="ae ky" href="https://ai.intelligentonlinetools.com/ml/text-clustering-doc2vec-word-embedding-machine-learning/" rel="noopener ugc nofollow" target="_blank">https://ai.intelligentonlinetools.com/ml/text-clustering-doc2vec-word-embedding-machine-learning/</a><br/>#<a class="ae ky" href="https://ibm.ent.box.com/s/3f160t4xpuya9an935k84ig465gvymm2" rel="noopener ugc nofollow" target="_blank">https://ibm.ent.box.com/s/3f160t4xpuya9an935k84ig465gvymm2</a></span><span id="b19e" class="pb la it po b gy pw pt l pu pv"># Load unzipped model, saved locally<br/>model="../doc2vec/doc2vec.bin"<br/>m = g.Doc2Vec.load(model)</span><span id="f61f" class="pb la it po b gy pw pt l pu pv"># Instantiate SpaCy Tokenizer<br/>nlp = English()<br/>tokenizer = Tokenizer(nlp.vocab)</span><span id="3b99" class="pb la it po b gy pw pt l pu pv"># Loop Through texts and create vectors</span><span id="b5d7" class="pb la it po b gy pw pt l pu pv">a=[]<br/>for text in tqdm(X_train['text']):<br/>    a.append(m.infer_vector([str(word) for word in tokenizer(text)]))<br/>    <br/>X_train_vec = pd.DataFrame(np.array(a))<br/>    <br/>a=[]<br/>for text in tqdm(X_test['text']):<br/>    a.append(m.infer_vector([str(word) for word in tokenizer(text)]))<br/>    <br/>X_test_vec = pd.DataFrame(np.array(a))</span><span id="8256" class="pb la it po b gy pw pt l pu pv"># Doc2Vec doesn't have feature names</span><span id="dac8" class="pb la it po b gy pw pt l pu pv">model = RandomForestClassifier(n_estimators=500, n_jobs=8)<br/>model.fit(X_train_vec, y_train)<br/>model.score(X_test_vec, y_test)</span></pre><p id="b1dd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">可视化</strong></p><p id="128c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我曾期待这款产品能带来巨大的成功，但它并没有实现。最左边的区域是带有@的推文，而最右边的区域主要是URL。这个模型很好地处理了这些问题(尽管对完整的句子进行了编码)，但我们要寻找的是比这更细微的差别。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="op oq l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图表。按“查看原文”打开互动版本，查看原始推文。</p></figure><p id="8733" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我的上述意见反映在模型中，这表现得像手套一样糟糕。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="op oq l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图表。评估模型性能的5幅图。</p></figure><h1 id="cc62" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">基于变压器的模型</h1><p id="1242" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我不会在这里谈论太多细节，但理解基于transformer的模型是值得的，因为自2017年谷歌论文发布以来，这种模型架构已经导致了我们在过去几年中看到的最先进的NLP模型的爆炸。</p><p id="afd0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">即使这些模型是最近才发布的，并且是在大型数据集上训练的，我们仍然可以使用高级python库来访问它们。是的，我们可以利用最先进的深度学习模型，只需几行代码。</p><h2 id="51c1" class="pb la it bd lb pc pd dn lf pe pf dp lj ma pg ph ll me pi pj ln mi pk pl lp pm bi translated">通用句子编码器</h2><div class="nh ni gp gr nj nk"><a href="https://amitness.com/2020/06/universal-sentence-encoder/" rel="noopener  ugc nofollow" target="_blank"><div class="nl ab fo"><div class="nm ab nn cl cj no"><h2 class="bd iu gy z fp np fr fs nq fu fw is bi translated">通用语句编码器直观解释</h2><div class="nr l"><h3 class="bd b gy z fp np fr fs nq fu fw dk translated">随着像BERT和他的朋友们这样的变形模型在NLP研究团体中掀起风暴，人们可能会很想…</h3></div><div class="ns l"><p class="bd b dl z fp np fr fs nq fu fw dk translated">amitness.com</p></div></div><div class="nt l"><div class="py l nv nw nx nt ny ks nk"/></div></div></a></div><p id="29e8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">谷歌的<strong class="lt iu">通用句子编码器</strong>包括一个变压器架构和深度平均网络。当发布时，它实现了最先进的结果，因为传统上，句子嵌入是对整个句子进行平均的。在通用句子编码器中，每个单词都有影响。</p><p id="91e2" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">与Word2Vec相比，使用它的主要好处是:</p><ol class=""><li id="4c5d" class="mt mu it lt b lu mn lx mo ma or me os mi ot mm ou mz na nb bi translated">使用Tensorflow Hub非常容易。该模型自动为整个句子生成一个嵌入。</li><li id="f170" class="mt mu it lt b lu nc lx nd ma ne me nf mi ng mm ou mz na nb bi translated">该模型比Word2Vec更好地捕捉了<strong class="lt iu">词序</strong>和<strong class="lt iu">上下文</strong>。</li></ol><p id="55b3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">更多信息，请看t <a class="ae ky" href="https://amitness.com/2020/06/universal-sentence-encoder/" rel="noopener ugc nofollow" target="_blank">他的解释</a>和<a class="ae ky" href="https://tfhub.dev/google/universal-sentence-encoder/4" rel="noopener ugc nofollow" target="_blank">谷歌的下载页面</a>。</p><p id="06cd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">实施</strong></p><p id="9a49" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这是最容易实现的模型之一。</p><pre class="kj kk kl km gt pn po pp pq aw pr bi"><span id="d300" class="pb la it po b gy ps pt l pu pv">import tensorflow_hub as hub</span><span id="09f3" class="pb la it po b gy pw pt l pu pv">def embed_document(data):<br/>    model = hub.load("../USE/")<br/>    embeddings = np.array([np.array(model([i])) for i in data])<br/>    return pd.DataFrame(np.vstack(embeddings))</span><span id="d41d" class="pb la it po b gy pw pt l pu pv"># vectorize the data</span><span id="6e6a" class="pb la it po b gy pw pt l pu pv">X_train_vec = embed_document(X_train['text'])<br/>X_test_vec = embed_document(X_test['text'])</span><span id="e43c" class="pb la it po b gy pw pt l pu pv"># USE doesn't have feature names</span><span id="2300" class="pb la it po b gy pw pt l pu pv">model = RandomForestClassifier(n_estimators=500, n_jobs=8)<br/>model.fit(X_train_vec, y_train)<br/>model.score(X_test_vec, y_test)</span></pre><p id="4d57" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">可视化</strong></p><p id="ee2b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在这个<strong class="lt iu">就是</strong>有意思。橙色和蓝色很容易区分。悬停在推文上，很明显语义相似的推文彼此接近。</p><p id="eb95" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果您运行了代码，您还会注意到这个模型嵌入句子的速度非常快，这是一个很大的好处，因为NLP工作会因为数据量大而变慢。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="op oq l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图表。按“查看原文”打开互动版本，查看原始推文。</p></figure><p id="5eb7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">不出所料，这款机型表现非常好。尽管精度仅比TF-IDF略高，但我所观察的每一项指标都有所提高。</p><p id="4fda" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">可解释性仍然是一个问题。一个特性似乎比其他的更重要，但是它对应的是什么呢？</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="op oq l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图表。评估模型性能的5幅图。</p></figure><h2 id="b2b6" class="pb la it bd lb pc pd dn lf pe pf dp lj ma pg ph ll me pi pj ln mi pk pl lp pm bi translated">伯特</h2><div class="nh ni gp gr nj nk"><a href="https://github.com/UKPLab/sentence-transformers" rel="noopener  ugc nofollow" target="_blank"><div class="nl ab fo"><div class="nm ab nn cl cj no"><h2 class="bd iu gy z fp np fr fs nq fu fw is bi translated">uk plab/句子-变形金刚</h2><div class="nr l"><h3 class="bd b gy z fp np fr fs nq fu fw dk translated">这个框架提供了一个简单的方法来计算句子和段落的密集向量表示(也称为…</h3></div><div class="ns l"><p class="bd b dl z fp np fr fs nq fu fw dk translated">github.com</p></div></div><div class="nt l"><div class="pz l nv nw nx nt ny ks nk"/></div></div></a></div><p id="960a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">BERT代表来自变压器的双向编码器表示。它是一个深度学习模型，具有<strong class="lt iu"> transformer </strong>架构。该模型以类似于Word2Vec的方式进行训练，即在句子中间屏蔽一个单词，并让模型填充空白。给定一个输入句子，它还被训练预测下一个句子。</p><p id="38f9" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">伯特接受了来自英语维基百科和图书语料库数据集的超过300万个单词的训练。</p><p id="856a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在引擎盖下，有两个关键概念:</p><p id="dec8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">嵌入:</strong>单词的向量表示，其中相似的单词彼此“接近”。BERT使用“单词块”嵌入(30k单词)加上句子嵌入来显示单词在哪个句子中，位置嵌入代表每个单词在句子中的位置。然后可以将文本输入到BERT中。</p><p id="eee7" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">注意:</strong>核心思想是每次模型预测一个输出单词时，它只使用输入中最相关信息集中的部分，而不是整个序列。用更简单的话来说，它只关注一些输入词。</p><p id="9732" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然而，我们真的不需要担心这一点，因为我们有办法用几行代码生成嵌入。</p><p id="47db" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">实施</strong></p><p id="8280" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">伯特的言辞非常有力。当进行微调时，该模型能够很好地捕捉语义差异和词序。</p><p id="6a1d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><a class="ae ky" href="https://medium.com/r?url=https%3A%2F%2Fgithub.com%2FUKPLab%2Fsentence-transformers" rel="noopener">句子转换包</a>允许我们利用预训练的BERT模型，这些模型已经过特定任务的训练，如语义相似性或问题回答。这意味着我们的嵌入是针对特定任务的。这也使得生成完整句子的嵌入非常容易。</p><p id="95c0" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在这个例子中，我使用了RoBERTa，它是脸书的BERT的优化版本。</p><pre class="kj kk kl km gt pn po pp pq aw pr bi"><span id="9def" class="pb la it po b gy ps pt l pu pv">from sentence_transformers import SentenceTransformer</span><span id="b85d" class="pb la it po b gy pw pt l pu pv">bert = SentenceTransformer('stsb-roberta-large') #1.3 gb</span><span id="0812" class="pb la it po b gy pw pt l pu pv"># vectorize the data</span><span id="02e6" class="pb la it po b gy pw pt l pu pv">X_train_vec = pd.DataFrame(np.vstack(X_train['text'].apply(bert.encode)))<br/>X_test_vec = pd.DataFrame(np.vstack(X_test['text'].apply(bert.encode)))</span><span id="33df" class="pb la it po b gy pw pt l pu pv"># BERT doesn't have feature names</span><span id="b167" class="pb la it po b gy pw pt l pu pv">model = RandomForestClassifier(n_estimators=500, n_jobs=8)<br/>model.fit(X_train_vec, y_train)<br/>model.score(X_test_vec, y_test)</span></pre><p id="34fb" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><strong class="lt iu">观想</strong></p><p id="6d9a" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">很难说这是不是比通用的句子编码器版本更好。我的直觉是，这个模型在区分灾难和非灾难推文方面做得更差，但在聚类类似主题方面可能做得更好。可惜我目前只能衡量前者！</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="op oq l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图表。按“查看原文”打开互动版本，查看原始推文。</p></figure><p id="dc51" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这个模型客观上比通用的句子编码器差。一个特性比其他的更重要，我希望这对应于URL，也许模型对这些的权重太大了，但是不能从其他1023个向量中提取细节。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="op oq l"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图表。评估模型性能的5幅图。</p></figure><h1 id="f3cb" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="3e95" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">我们探索了多种将单词转化为数字的方法。在这个数据集上，谷歌的通用句子编码器表现最好。对于大多数应用程序来说，这是值得一试的，因为它们的性能非常好。我觉得Word2Vec现在有点过时了，USE之类的方法这么快，这么厉害。</p><p id="d3bd" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">我们很多人第一次学习NLP的方法是通过做一个<strong class="lt iu">情感分析</strong>项目，用一个<strong class="lt iu">单词包</strong>表示文本。这是一种很好的学习方式，但我觉得它带走了NLP的很多乐趣。一袋单词和一个热编码数据没有太大区别。产生的模型不是特别有效，而且很少能捕捉到文本中的任何细微差别。<strong class="lt iu">我们可以轻松采用BERT嵌入，这通常会带来巨大的性能提升。</strong></p><p id="2147" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">作为最后一点，总是值得考虑模型<strong class="lt iu">可解释性</strong>和<strong class="lt iu">可解释性</strong>。使用单词袋方法，我们可以清楚地说出哪些单词影响了模型。在伯特模型中，我们可以很容易地说出向量中的哪个位置影响了模型，但要说出每个向量的确切含义却需要相当大的努力(而且几乎是不可能的)。一个悬而未决的问题是——<strong class="lt iu">伯特使用URL预测灾难了吗？还是它对语言理解得更好？</strong></p><h2 id="ce56" class="pb la it bd lb pc pd dn lf pe pf dp lj ma pg ph ll me pi pj ln mi pk pl lp pm bi translated">了解更多信息</h2><div class="nh ni gp gr nj nk"><a rel="noopener follow" target="_blank" href="/simple-logistic-regression-a9735ed23abd"><div class="nl ab fo"><div class="nm ab nn cl cj no"><h2 class="bd iu gy z fp np fr fs nq fu fw is bi translated">清理逻辑回归</h2><div class="nr l"><h3 class="bd b gy z fp np fr fs nq fu fw dk translated">非数学家指南</h3></div><div class="ns l"><p class="bd b dl z fp np fr fs nq fu fw dk translated">towardsdatascience.com</p></div></div><div class="nt l"><div class="qa l nv nw nx nt ny ks nk"/></div></div></a></div><div class="nh ni gp gr nj nk"><a href="https://adam-shafi.medium.com/tech-skills-2021-5805848851c6" rel="noopener follow" target="_blank"><div class="nl ab fo"><div class="nm ab nn cl cj no"><h2 class="bd iu gy z fp np fr fs nq fu fw is bi translated">2021年，数据科学家和分析师必须学习技术技能</h2><div class="nr l"><h3 class="bd b gy z fp np fr fs nq fu fw dk translated">以数据驱动的方法理解英国市场的技术技能。</h3></div><div class="ns l"><p class="bd b dl z fp np fr fs nq fu fw dk translated">adam-shafi.medium.com</p></div></div><div class="nt l"><div class="qb l nv nw nx nt ny ks nk"/></div></div></a></div><h2 id="05d9" class="pb la it bd lb pc pd dn lf pe pf dp lj ma pg ph ll me pi pj ln mi pk pl lp pm bi translated">联系我</h2><div class="nh ni gp gr nj nk"><a href="https://www.linkedin.com/in/adamshafi/" rel="noopener  ugc nofollow" target="_blank"><div class="nl ab fo"><div class="nm ab nn cl cj no"><h2 class="bd iu gy z fp np fr fs nq fu fw is bi translated">亚当·沙菲-数据科学家-凯捷| LinkedIn</h2><div class="nr l"><h3 class="bd b gy z fp np fr fs nq fu fw dk translated">拥有4年以上分析经验的数据科学家，包括管理团队、交付项目和转变…</h3></div><div class="ns l"><p class="bd b dl z fp np fr fs nq fu fw dk translated">www.linkedin.com</p></div></div><div class="nt l"><div class="qc l nv nw nx nt ny ks nk"/></div></div></a></div></div></div>    
</body>
</html>