<html>
<head>
<title>Identifying Global Feature Relationships with SHAP Values</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">确定全球特征与SHAP值的关系</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/identifying-global-feature-relationships-with-shap-values-f9e8b2b4121c?source=collection_archive---------8-----------------------#2021-12-07">https://towardsdatascience.com/identifying-global-feature-relationships-with-shap-values-f9e8b2b4121c?source=collection_archive---------8-----------------------#2021-12-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="879e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何计算要素的全局信息，并将其用于不可知的要素选择过程x</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/ae146886eca54b2b7c59dc1445f0b29d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gddcH--jkWm4jjic"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">沙哈达特·拉赫曼在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="ea9e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，使用SHAP值是解释机器学习模型和理解数据特征与输出之间关系的最常用方法之一。</p><p id="6bf5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">SHAP值的最大优势之一是它们提供了局部可解释性:我们可以看到每个特性如何影响每个实例的结果。</p><p id="f8d9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，这种能力是有代价的:使用SHAP值进行特征选择并不那么简单。通常根据要素的平均SHAP值或最大SHAP值来截断要素，但是，这些方法并不能保证不会从数据集中移除某些重要的要素。直到今天，对于如何使用它进行特征选择还没有达成共识。</p><p id="5a56" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另一种方法是根据对数据集的百分比影响来定义要素重要性。然而，这需要大量的微调，而且不够标准化，不能以不可知的方式使用。</p><p id="9892" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在本帖中，我们将研究一篇<a class="ae kv" href="https://arxiv.org/abs/2107.12436" rel="noopener ugc nofollow" target="_blank">论文</a> [1】，该论文提出了一种使用SHAP值寻找全局解释的方法，这种方法使我们能够识别我们的要素之间的收敛程度，从而更好地理解数据集并提供更好的要素选择方法。</p><p id="8c97" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这篇文章的所有代码都可以在<a class="ae kv" href="https://www.kaggle.com/tiagotoledojr/shap-s-i-r-global-explanations" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上的笔记本中找到，也可以在我的<a class="ae kv" href="https://github.com/TNanukem/paper_implementations/blob/main/SHAP%20S-R-I.ipynb" rel="noopener ugc nofollow" target="_blank"> Github </a>上找到。</p><h1 id="0388" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">SHAP快速概览</h1><p id="7f13" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">首先，让我们回顾一下SHAP价值观和SHAP图书馆。这个想法不是解释方法的内部工作，而是回顾对库中结果的解释，并理解如何获得它们。</p><p id="1508" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当我们从一对模型+数据集获取SHAP值时，我们会得到一个NxM矩阵，其中N是数据集实例的数量，M是要素的数量。我们拥有与原始数据集相同的形状。</p><p id="750e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该矩阵中的每个I，j条目表示特征j对实例I的预测的影响。这允许我们在局部水平上解释我们的模型如何基于特征进行预测。</p><p id="2639" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果获取模型的平均预测值，并将其与SHAP值矩阵中每一行的总和相加，您将获得数据集每个实例的准确预测值。</p><p id="87e7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是我们将要使用的SHAP的基本结构。</p><p id="ff7d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另一个非常重要的结构是SHAP相互作用矢量。</p><h2 id="c56a" class="mp lt iq bd lu mq mr dn ly ms mt dp mc lf mu mv me lj mw mx mg ln my mz mi na bi translated">SHAP相互作用矢量</h2><p id="bcc7" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">两个要素之间的SHAP交互矢量定义了预测中这些要素之间的交互。为了计算它，当j存在和j不存在时，计算特征I的SHAP值。对所有可能性进行置换会生成交互向量。</p><p id="bffa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果我们将特征I的SHAP向量定义为包含每个样本的特征I的SHAP值的向量，则我们知道:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/ad7f093f0646706f4a5b41252788841a.png" data-original-src="https://miro.medium.com/v2/resize:fit:296/format:webp/1*5mywhsqu4nMxp68YQCK4XA.png"/></div></figure><p id="9d3b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中pij是特征I和j之间的SHAP相互作用矢量</p><p id="00a9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如我们将在下一节中看到的，这些概念很重要，因为所描述的方法有一个真正强大的几何解释，这将帮助我们理解正在发生的事情。</p><p id="ae25" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，让我们在空间上绘制这两个向量，并从那里开始构建:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/9747c8cce15878280e41a1218aa2f9e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*QeQbK3t28CfAXAiR02ldJw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">SHAP矢量和SHAP相互作用矢量。图片由作者根据图片[1]提供</p></figure><h1 id="e7f8" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">协同作用</h1><p id="755f" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">协同具有测量一个特征如何受益于数据集中另一个特征的直觉。如果两个要素高度协同，那么它们都应该出现在数据集上，因为它们彼此帮助很大。</p><p id="740c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对此的几何解释是，我们可以利用pi向量在pij向量上的投影来创建协同向量，该向量可以被翻译为以下等式:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/aee0d79975a1c3d3820852d3166955a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/format:webp/1*HrEfFM7IVAWSJTy49PHC0A.png"/></div></figure><p id="ce63" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">视觉上我们得到:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/9daa0e504c7e8d4629e5312b5569b123.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*vKSWWJy-Uw0lzE2Ki13wYw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">协同矢量。图片由作者根据图片[1]提供</p></figure><p id="3b55" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，我们可以通过计算该投影的长度来生成这些特征之间的协同值:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/f449b30dd0f2fc1437d7a54f20f61c45.png" data-original-src="https://miro.medium.com/v2/resize:fit:346/format:webp/1*VrrZrGOTJJ03RcwjTBid6A.png"/></div></figure><p id="33f8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">既然我们定义了这个向量，我们基本上是在说:给定我的特征I的预测能力，它有多少来自与特征j的相互作用？如果我们回答了这个问题，那么我们就知道有多少预测能力不是来自于此。</p><p id="a5b9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">考虑到这一点，我们可以将这两个特征之间的自主向量定义为向量减法:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/4cb32e6347d4d980d858ae092454c10f.png" data-original-src="https://miro.medium.com/v2/resize:fit:334/format:webp/1*W8tc0J5vdPdCqTDWMUuRJw.png"/></div></figure><p id="dd0b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">从几何学上讲，我们可以看到，协同(来自交互的内容)和自主(不来自交互的内容)的总和将合计为原始特征向量:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/114c943901e3bb2a487bd779845a26ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1138/format:webp/1*hJclzIvrBrGTbDrXxjFkpA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">自主向量。图片由作者根据图片[1]提供</p></figure><h1 id="1f85" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">裁员</h1><p id="a606" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">冗余的直觉是来自特征I的信息量被复制到特征j上。两个完全冗余的特征将具有完全相同的信息。一个例子是开尔文和摄氏度的温度。</p><p id="bc5c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们可以通过将从I到j的自治向量投影到从j到I的自治向量来测量共享了多少信息。这在代数上转化为:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/bb776881529db3e78e0863f1601bbaa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:434/format:webp/1*mS0yOETKZHyUlTvPNPR5kg.png"/></div></figure><p id="34b2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">视觉上，我们有下面的图像。请注意，相对于aij和pij向量，aji向量属于另一个平面:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/c1836b3829b7fedebd7de84d7d9035d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/1*7DmI5i23PA5XtsQkCBqlLA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">冗余向量。图片由作者根据图片从[1]</p></figure><p id="f7a5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，正如我们对synergy所做的那样，我们可以通过计算投影的长度，从这个向量生成一个值:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/d0c0d97ab3971c540fd7c6556da47612.png" data-original-src="https://miro.medium.com/v2/resize:fit:356/format:webp/1*GHQqA4A6WFxPKAkF6KsuXA.png"/></div></figure><h1 id="e5fe" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">独立性ˌ自立性</h1><p id="96bb" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">最后，我们将定义最后一条信息:给定I中与特征j中的信息不协同或不冗余的信息，我们独立于特征I。</p><p id="cb97" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这可以计算为特征之间的自主性和冗余性之间的差异:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/adf2306252f34dcbe4b37497d33790cf.png" data-original-src="https://miro.medium.com/v2/resize:fit:344/format:webp/1*iyB4R0czsURA8WS1-JA2oA.png"/></div></figure><p id="cf28" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">正如我们对其他特征所做的那样，我们可以通过计算这个向量在π上的投影长度来计算标量值:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/7f5439df1e674450c5fad9a4aebf40d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:328/format:webp/1*hH1rfZiUB5SNLd3fhBImTg.png"/></div></figure><h1 id="0cae" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">特征编码</h1><p id="1442" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">现在，让我们深入一些编码，看看我们如何使用来自<a class="ae kv" href="https://github.com/slundberg/shap" rel="noopener ugc nofollow" target="_blank"> SHAP库</a>的SHAP值结果来生成这些值。</p><p id="7578" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于这个例子，我们将使用来自UCI知识库的葡萄酒数据集，它是免费使用的，是从sklearn包的一个函数中获取的。然后，我们将应用随机森林分类器。</p><p id="432e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，让我们导入所需的库:</p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="0530" class="mp lt iq nm b gy nq nr l ns nt">import shap<br/>import numpy as np<br/>import pandas as pd</span><span id="bef6" class="mp lt iq nm b gy nu nr l ns nt">from sklearn.datasets import load_wine<br/>from sklearn.ensemble import RandomForestClassifier</span></pre><p id="39b7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们在数据集上安装分类器:</p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="635e" class="mp lt iq nm b gy nq nr l ns nt"># Get the dataset and fit a Random Forest on it<br/>X, y = load_wine(return_X_y=True, as_frame=True)</span><span id="11c6" class="mp lt iq nm b gy nu nr l ns nt">rf = RandomForestClassifier()<br/>rf.fit(X, y)</span></pre><p id="01a9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，我们可以使用SHAP库来生成SHAP值:</p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="90c7" class="mp lt iq nm b gy nq nr l ns nt"># Runs the explainer on the model and the dataset to grab the Shap Values<br/>explainer = shap.Explainer(rf)<br/>shap_values = explainer(X)</span></pre><p id="f521" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当我们执行上面的代码时，SHAP库返回三个矩阵，所以我们将选择SHAP矩阵:</p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="9eb2" class="mp lt iq nm b gy nq nr l ns nt"># The return of the explainer has three matrices, we will get the shap values one<br/>shap_values = shap_values.values[:, :, 0]</span></pre><p id="1434" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，让我们生成相互作用值，以生成SHAP相互作用矢量:</p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="d322" class="mp lt iq nm b gy nq nr l ns nt">shap_interaction_values = explainer.shap_interaction_values(X)[0]</span></pre><p id="24da" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们现在将定义一些零矩阵来填充我们的计算。这不是最快的方法，但是，这样做是为了更有启发性:</p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="fd7a" class="mp lt iq nm b gy nq nr l ns nt"># Define matrices to be filled<br/>    s = np.zeros((shap_values.shape[1], shap_values.shape[1], shap_values.shape[0]))<br/>    a = np.zeros((shap_values.shape[1], shap_values.shape[1], shap_values.shape[0]))<br/>    r = np.zeros((shap_values.shape[1], shap_values.shape[1], shap_values.shape[0]))<br/>    i_ = np.zeros((shap_values.shape[1], shap_values.shape[1], shap_values.shape[0]))</span><span id="6949" class="mp lt iq nm b gy nu nr l ns nt">S = np.zeros((shap_values.shape[1], shap_values.shape[1]))<br/>R = np.zeros((shap_values.shape[1], shap_values.shape[1]))<br/>I = np.zeros((shap_values.shape[1], shap_values.shape[1]))</span></pre><p id="c454" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们为每个向量定义了一个矩阵，为每个标量值定义了一个矩阵。现在，让我们迭代每个SHAP值(想象I和j上的双for循环),并选择我们要使用的向量:</p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="de31" class="mp lt iq nm b gy nq nr l ns nt"># Selects the p_i vector -&gt; Shap Values vector for feature i<br/>pi = shap_values[:, i]</span><span id="7cbe" class="mp lt iq nm b gy nu nr l ns nt"># Selects pij -&gt; SHAP interaction vector between features i and j<br/>pij = shap_interaction_values[:, i, j]<br/>            <br/># Other required vectors<br/>pji = shap_interaction_values[:, j, i]<br/>pj = shap_values[:, j]</span></pre><p id="5ba0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有了这些，就可以很容易地根据上面提供的等式计算出以下向量:</p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="84c8" class="mp lt iq nm b gy nq nr l ns nt"># Synergy vector<br/>s[i, j] = (np.inner(pi, pij) / np.linalg.norm(pij)**2) * pij<br/>s[j, i] = (np.inner(pj, pji) / np.linalg.norm(pji)**2) * pji</span><span id="6b8e" class="mp lt iq nm b gy nu nr l ns nt"># Autonomy vector<br/>a[i,j] = pi - s[i, j]<br/>a[j,i] = pj - s[j, i]</span><span id="f077" class="mp lt iq nm b gy nu nr l ns nt"># Redundancy vector<br/>r[i,j] = (np.inner(a[i, j], a[j, i]) / np.linalg.norm(a[j, i])**2) * a[j, i]<br/>r[j,i] = (np.inner(a[j, i], a[i, j]) / np.linalg.norm(a[i, j])**2) * a[i, j]</span><span id="80e2" class="mp lt iq nm b gy nu nr l ns nt"># Independece vector<br/>i_[i, j] = a[i, j] - r[i, j]<br/>i_[j, i] = a[j, i] - r[j, i]</span></pre><p id="6675" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然后，使用长度计算公式，我们得到最终的标量值:</p><pre class="kg kh ki kj gt nl nm nn no aw np bi"><span id="4ab7" class="mp lt iq nm b gy nq nr l ns nt"># Synergy value<br/>S[i, j] = np.linalg.norm(s[i, j])**2 / np.linalg.norm(pi)**2</span><span id="d661" class="mp lt iq nm b gy nu nr l ns nt"># Redundancy value<br/>R[i, j] = np.linalg.norm(r[i, j])**2 / np.linalg.norm(pi)**2</span><span id="1bc2" class="mp lt iq nm b gy nu nr l ns nt"># Independence value<br/>I[i, j] = np.linalg.norm(i_[i, j])**2 / np.linalg.norm(pi)**2</span></pre><p id="bece" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，论文作者在<a class="ae kv" href="https://github.com/BCG-Gamma/facet" rel="noopener ugc nofollow" target="_blank">刻面库</a>上提供了这些方法的开源实现。</p><h1 id="097a" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">特征选择模型建议</h1><p id="2724" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">既然我们已经了解了这个方法是如何工作的，我们可以开始考虑如何使用它来生成一个特征选择方法。</p><p id="9574" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">方法的提议可以是:</p><ul class=""><li id="29dd" class="nv nw iq ky b kz la lc ld lf nx lj ny ln nz lr oa ob oc od bi translated">给定数据集上的训练模型，从中获取SHAP信息</li><li id="767e" class="nv nw iq ky b kz oe lc of lf og lj oh ln oi lr oa ob oc od bi translated">运行信噪比计算</li><li id="854f" class="nv nw iq ky b kz oe lc of lf og lj oh ln oi lr oa ob oc od bi translated">如果一对特征的冗余度大于阈值，则将它们标记为移除</li><li id="3f0c" class="nv nw iq ky b kz oe lc of lf og lj oh ln oi lr oa ob oc od bi translated">从与数据集其余部分协同性最小的要素对中获取要素，并将其移除。为此，您可以使用平均协同效应或其他指标。</li></ul><p id="028b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这只是如何使用这些值来改进您的特征选择方法的基本想法。我希望在未来能产生更多关于这个主题的想法。</p><p id="a1f9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[1] Ittner等，利用SHAP向量分解的全局模型解释中的协同性、冗余性和独立性(2021)，<a class="ae kv" href="https://arxiv.org/abs/2107.12436" rel="noopener ugc nofollow" target="_blank"><br/>arXiv:2107.12436</a><strong class="ky ir">【cs .LG] </strong></p></div></div>    
</body>
</html>