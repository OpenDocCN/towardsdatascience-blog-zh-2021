<html>
<head>
<title>Logistic regression explained</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">逻辑回归解释</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/logistic-regression-explained-7695f15d1b8b?source=collection_archive---------19-----------------------#2021-10-11">https://towardsdatascience.com/logistic-regression-explained-7695f15d1b8b?source=collection_archive---------19-----------------------#2021-10-11</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/a587164a30e0847342bfa043d6833008.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Qc8wm38BKRmPOJ6i"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">Pawel Czerwinski 在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><h2 id="885a" class="jh ji jj bd b dl jk jl jm jn jo jp dk jq translated" aria-label="kicker paragraph">数据科学基础</h2><div class=""/><div class=""><h2 id="87ed" class="pw-subtitle-paragraph kp js jj bd b kq kr ks kt ku kv kw kx ky kz la lb lc ld le lf lg dk translated">了解这种受监督的机器学习算法是如何工作的</h2></div><p id="366c" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><em class="md">逻辑回归</em>由于其简单性和可解释性，是一种流行的分类算法。如果你正在学习或实践数据科学，很可能你已经听说过这个算法，甚至使用过它。如果你想加深对逻辑回归的理解，学习它背后的数学知识，这篇文章提供了对逻辑回归的简单介绍。</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi me"><img src="../Images/71f7c33aee4aa15dc00737d2835c44ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*v5TCi4DCmGrzZeBC"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">Pawel Czerwinski 在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="3632" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><em class="md">本帖假设读者对线性回归有所了解。如果你对线性回归不太适应，</em> <a class="ae jg" rel="noopener" target="_blank" href="/linear-regression-explained-89cc3886ab48"> <em class="md">这篇文章</em> </a> <em class="md">可能有助于获得线性回归的基础知识。我们将看到一些用Python编写的代码示例，然而，读者可以在不了解任何Python的情况下吸收概念性知识。</em></p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="86d7" class="mq mr jj bd ms mt mu mv mw mx my mz na ky nb kz nc lb nd lc ne le nf lf ng nh bi translated">概观📜</h1><p id="a1c4" class="pw-post-body-paragraph lh li jj lj b lk ni kt lm ln nj kw lp lq nk ls lt lu nl lw lx ly nm ma mb mc im bi translated">理解<em class="md">逻辑函数</em>是理解<em class="md">逻辑回归</em>的重要前提。所以让我们从理解什么是逻辑函数开始。</p><h2 id="186e" class="nn mr jj bd ms no np dn mw nq nr dp na lq ns nt nc lu nu nv ne ly nw nx ng jp bi translated">📍物流功能</h2><blockquote class="ny nz oa"><p id="113a" class="lh li md lj b lk ll kt lm ln lo kw lp ob lr ls lt oc lv lw lx od lz ma mb mc im bi translated">逻辑函数是一种类型的<a class="ae jg" href="https://deepai.org/machine-learning-glossary-and-terms/sigmoid-function" rel="noopener ugc nofollow" target="_blank"> sigmoid函数</a>,用于压缩0和1之间的值。</p></blockquote><p id="79ce" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">虽然<em class="md">乙状结肠功能</em>是<em class="md">物流和其他功能</em>的总称，但该术语通常用于指代<em class="md">物流功能</em>。例如，在神经网络的上下文中，逻辑函数通常被称为<em class="md"> sigmoid激活函数</em>。数学上，该函数如下所示:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/bdddf07cdcbb107c2a31f3cc136faf76.png" data-original-src="https://miro.medium.com/v2/resize:fit:560/format:webp/1*0u3Dz9JQqVw2wCrRJa5ddg.png"/></div></figure><p id="0ce9" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">函数通常用这两种形式中的一种来表示。这些形式是等价的，因为我们可以重新排列一种形式来得到另一种形式:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi of"><img src="../Images/5536e1226819e0d3e3bfe797b99bea41.png" data-original-src="https://miro.medium.com/v2/resize:fit:826/format:webp/1*wazIdKgFKRdWVYoBPOHcJQ.png"/></div></figure><p id="1d38" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们将继续使用第一种形式，因为它更简洁。逻辑函数的一个重要特征是它在0和1处有两条水平渐近线:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi og"><img src="../Images/d2621a141bf8d4e0c719f2c246aed184.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*a4OJ4SmFomVFrQOFDnYXxQ.png"/></div></figure><p id="800f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果我们把逻辑函数形象化，这个特征会变得更清楚:</p><pre class="mf mg mh mi gt oh oi oj ok aw ol bi"><span id="21c5" class="nn mr jj oi b gy om on l oo op">import numpy as np<br/>import pandas as pd<br/>pd.options.display.precision = 4</span><span id="1cd4" class="nn mr jj oi b gy oq on l oo op">import matplotlib.pyplot as plt<br/>import seaborn as sns<br/>sns.set(style='darkgrid', context='talk', palette='rainbow')</span><span id="299a" class="nn mr jj oi b gy oq on l oo op">from sklearn.linear_model import LogisticRegression</span><span id="325e" class="nn mr jj oi b gy oq on l oo op"># Create integers between -10 to 10 (both inclusive)<br/>z = np.arange(-10,11)</span><span id="518a" class="nn mr jj oi b gy oq on l oo op"># Create logistic function<br/>def logistic(x):<br/>    return 1 / (1+np.exp(-x))</span><span id="5c0e" class="nn mr jj oi b gy oq on l oo op"># Plot x: z and y: logistic(z)<br/>plt.figure(figsize=(10,4))<br/>sns.lineplot(x=z, y=logistic(z))<br/>plt.xlabel('z')<br/>plt.ylabel('logistic(z)')<br/>plt.title('Logistic function');</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi or"><img src="../Images/2808d769c38eaea8e0a4fb1fdfab4d42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1244/format:webp/1*PTlp8WylwElSnjH4NWy1cg.png"/></div></figure><p id="2900" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们看到一条类似拉伸S的曲线，垂直轴上的函数输出范围从0到1。当<code class="fe os ot ou oi b">z=0</code>时，逻辑函数返回0.5。这意味着<code class="fe os ot ou oi b">logistic(z)&gt;0.5</code>意味着<code class="fe os ot ou oi b">z</code>是正的，而<code class="fe os ot ou oi b">logistic(z)&lt;0.5</code>意味着<code class="fe os ot ou oi b">z</code>是负的。</p><p id="2aa3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因为逻辑函数将任何实数转换为0到1之间的值，所以当我们想将数值转换为概率时，它非常有用。</p><h2 id="da28" class="nn mr jj bd ms no np dn mw nq nr dp na lq ns nt nc lu nu nv ne ly nw nx ng jp bi translated">📍逻辑回归</h2><p id="f192" class="pw-post-body-paragraph lh li jj lj b lk ni kt lm ln nj kw lp lq nk ls lt lu nl lw lx ly nm ma mb mc im bi translated">逻辑回归是一种二元分类算法，尽管其名称包含“回归”一词。对于二元分类，我们有两个想要预测的目标类。让我们称它们为<em class="md">正</em> (y=1)和<em class="md">负</em> (y=0)类。当我们将<a class="ae jg" rel="noopener" target="_blank" href="/linear-regression-explained-89cc3886ab48"> <em class="md">线性回归</em> </a>和<em class="md">逻辑函数</em>结合起来，我们得到逻辑回归方程:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/2b9548b3dddba4c733bc4888bc1cd577.png" data-original-src="https://miro.medium.com/v2/resize:fit:1104/format:webp/1*zfJa_9VX9-4KnPLfbuWu5A.png"/></div></figure><p id="3ea3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这与以下内容相同:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ow"><img src="../Images/9a6833987d42433dc279e0f45833841a.png" data-original-src="https://miro.medium.com/v2/resize:fit:660/format:webp/1*2VxqfkFfjYbWXIGgk-P0Rg.png"/></div></div></figure><blockquote class="ny nz oa"><p id="f8b2" class="lh li md lj b lk ll kt lm ln lo kw lp ob lr ls lt oc lv lw lx od lz ma mb mc im bi translated">逻辑回归预测记录属于给定特征的正类的概率。</p></blockquote><p id="7e27" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因为我们有两个类，所以找到属于负类的概率很简单:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/3b477f9b7b70fb5d41eff2ebf0d77614.png" data-original-src="https://miro.medium.com/v2/resize:fit:414/format:webp/1*cnqrcQ33G5OeeK1H-dQlzw.png"/></div></figure><p id="bcb1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">一旦我们有了概率值，就很容易将它们转换成预测类。使用阈值0.5(即概率为50%)，我们可以将概率值转换为类别:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/7297870b4c5c11ba3877fbff315d89da.png" data-original-src="https://miro.medium.com/v2/resize:fit:488/format:webp/1*snzV8jbnir797hLAQ8Hqyg.png"/></div></figure><p id="d137" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这相当于(如果不清楚为什么它们是相同的，再看一下上面的<em class="md">物流功能部分</em>👀):</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/909fcd1892aa9a54c846fd48755f90aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:780/format:webp/1*HkgzMqd96xpzIg1K6-iaXw.png"/></div></figure><p id="2705" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果我们使用术语<code class="fe os ot ou oi b">z</code>来表示系数和特征的截距和点积之和:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/facd248178e055f88f5861211351bed2.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/format:webp/1*lC-bp63lurglnfQkkFlIJA.png"/></div></figure><p id="5d2a" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">那么我们可以说对于<code class="fe os ot ou oi b">z</code>的负值，预测类为0，否则，预测类为1。下面是一个小玩具数据集的简单示例，它有一个二元目标和两个特征:</p><pre class="mf mg mh mi gt oh oi oj ok aw ol bi"><span id="b8b8" class="nn mr jj oi b gy om on l oo op"># Create sample data<br/>train = pd.DataFrame({'x1': [1, 2, 2, 3, 2, 4, 3, 4, 3, 4], <br/>                      'x2': [2, 3, 4, 2, 1, 3, 5, 2, 3, 6], <br/>                      'y': [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]})</span><span id="4fcb" class="nn mr jj oi b gy oq on l oo op"># Train a logistic regression<br/>target = 'y'<br/>features = ['x1', 'x2']<br/>model = LogisticRegression()<br/>model.fit(train[features], train[target])</span><span id="f9d9" class="nn mr jj oi b gy oq on l oo op"># Predict<br/>for i, row in train.iterrows():<br/>    train.loc[i, 'z'] = model.intercept_[0] + np.dot(model.coef_[0], <br/>                                                  row[:2])<br/>train['p_hat'] = model.predict_proba(train[features])[:,1]<br/>train['y_hat'] = model.predict(train[features]) # default threshold is 0.5</span><span id="d55a" class="nn mr jj oi b gy oq on l oo op">def highlight(data):<br/>    n = len(data)<br/>    if data['z']&lt;0:<br/>        return n*['background-color: #FF9A98']<br/>    else:<br/>        return n*['background-color: lightgreen']<br/>(train.style.apply(highlight, axis=1)<br/>  .format('{:.2%}', subset='p_hat').hide_index())</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/d5ff8a9c1df3abc0df93454b8bc2eca6.png" data-original-src="https://miro.medium.com/v2/resize:fit:548/format:webp/1*eM85_7t9adGtZ90XpU80SA.png"/></div></figure><p id="3633" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果你想学着像这样美化熊猫的数据帧，你可能会喜欢阅读这个帖子。</p><p id="88b1" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们可以看到，对于负值的<code class="fe os ot ou oi b">z</code>，概率值小于0.5，对于正值的<code class="fe os ot ou oi b">z</code>，概率值大于0.5。现在是学习逻辑回归的决策边界的好时机，它基本上包含了我们在本节中讨论的内容。</p><h2 id="7f16" class="nn mr jj bd ms no np dn mw nq nr dp na lq ns nt nc lu nu nv ne ly nw nx ng jp bi translated">📍判别边界</h2><p id="16fe" class="pw-post-body-paragraph lh li jj lj b lk ni kt lm ln nj kw lp lq nk ls lt lu nl lw lx ly nm ma mb mc im bi translated"><a class="ae jg" href="https://www.cs.princeton.edu/courses/archive/fall08/cos436/Duda/PR_simp/bndrys.htm" rel="noopener ugc nofollow" target="_blank">逻辑回归的决策边界</a>由下式给出:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/26c2cc2932470b915b4eb4488457bd71.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*xJs2_OTQNG7i8wZ5qA6KNw.png"/></div></figure><p id="2fed" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在我们的例子中，由于我们只有两个特征，等式变成:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/080f0c6659afec0d6c02979bc14fd87b.png" data-original-src="https://miro.medium.com/v2/resize:fit:388/format:webp/1*eQH8IvqnG-0spz-ulrKKdQ.png"/></div></figure><p id="c283" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们沿着决策边界在横轴上绘制<code class="fe os ot ou oi b">x1</code>，在纵轴上绘制<code class="fe os ot ou oi b">x2</code>。如果我们重新排列上述等式，使<code class="fe os ot ou oi b">x2</code>用<code class="fe os ot ou oi b">x1</code>表示，绘制决策边界变得更容易:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/f87ee47ac0fca5b669ac0aa02624bc96.png" data-original-src="https://miro.medium.com/v2/resize:fit:374/format:webp/1*Cpe07KswXqHe8NU_LTEgug.png"/></div></figure><pre class="mf mg mh mi gt oh oi oj ok aw ol bi"><span id="795f" class="nn mr jj oi b gy om on l oo op">def decision_boundary(x1):<br/>    intercept = model.intercept_[0]<br/>    coefficients = model.coef_[0]<br/>    x2 = -(intercept+coefficients[0]*x1)/coefficients[1]<br/>    return x2</span><span id="12b0" class="nn mr jj oi b gy oq on l oo op"># Plot decision boundary<br/>plt.figure(figsize=(6,4))<br/>sns.scatterplot(data=train, x='x1', y='x2', hue='y', <br/>                palette=['red', 'green'])<br/>sns.lineplot(x=train['x1'], y=train['x1'].apply(decision_boundary), <br/>             color='red')<br/>plt.fill_between(train['x1'], train['x1'].apply(decision_boundary), <br/>                 color='red', alpha=0.4)<br/>plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1))<br/>plt.title('Decision boundary');</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/f89d7a9c19cbebb21e75106efc1e757e.png" data-original-src="https://miro.medium.com/v2/resize:fit:958/format:webp/1*IWztDRqAGdLQ5Mfh1tG31w.png"/></div></figure><p id="e591" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">从这里，我们可以很容易地看到，逻辑回归的决策边界是线性的。使用这一行，我们可以确定给定记录的预测类:如果一个记录落在左下角阴影三角形中(即<code class="fe os ot ou oi b">z&lt;0</code>，那么预测类是0，否则(即<code class="fe os ot ou oi b">z≥0</code>)，预测类是1。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="e337" class="mq mr jj bd ms mt mu mv mw mx my mz na ky nb kz nc lb nd lc ne le nf lf ng nh bi translated">训练逻辑回归🔧</h1><p id="9b9c" class="pw-post-body-paragraph lh li jj lj b lk ni kt lm ln nj kw lp lq nk ls lt lu nl lw lx ly nm ma mb mc im bi translated">熟悉了逻辑回归背后的直觉之后，现在让我们学习模型如何学习最优的<a class="ae jg" href="https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/" rel="noopener ugc nofollow" target="_blank">模型参数</a>(即截距和系数)。在训练逻辑回归时，我们希望找到最佳的参数组合，以便与任何其他组合相比，它们在所有训练示例中产生最低的误差。对于给定的一组参数，我们可以使用<em class="md">对数损失(又名交叉熵损失)函数</em>来测量误差，作为训练示例:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/547b4b9e10f78a9a9b95b1fc78801299.png" data-original-src="https://miro.medium.com/v2/resize:fit:888/format:webp/1*oEfcvgSLjkFNGho5AT2Blw.png"/></div></figure><p id="8746" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">虽然<a class="ae jg" href="https://datascience.stackexchange.com/questions/57009/why-doesnt-the-binary-classification-log-loss-formula-make-it-explicit-that-nat" rel="noopener ugc nofollow" target="_blank">对数的底数并不重要</a>，但使用自然对数是很常见的:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/ee429c422ecbc50bfacf999eaaf7d98b.png" data-original-src="https://miro.medium.com/v2/resize:fit:812/format:webp/1*K0Ax_4QKOFCIODF3xCHKJw.png"/></div></figure><p id="a559" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这可以简化为:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/046f9563cf248bb1008e53caa3f77feb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1048/format:webp/1*RuA6JPSNHxxiftlAD2Z0ow.png"/></div></figure><p id="144b" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">你会注意到，当<code class="fe os ot ou oi b">y=0</code>时，等式的前半部分为0，而当<code class="fe os ot ou oi b">y=1</code>时，等式的后半部分为0。这个函数一开始可能看起来不直观，但是在一个小例子中应用它可以阐明函数背后的直观性。让我们来看看两个类别不同概率的对数损失:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/6a347706e9c8d8897bef267764a330ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*NmsZ7weRifOBO3NBN3cJjg.png"/></div></figure><p id="814f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们可以看到，对于负记录，误差随着概率越来越接近0而减小，对于正记录，误差随着概率越来越接近1而减小。换句话说，正面记录的概率越高或负面记录的概率越低，误差就越小。现在，这是直觉！</p><p id="7a10" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因为我们希望最小化所有训练样本的误差，所以集体误差由平均测井损失给出:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pk"><img src="../Images/0e74acf513f04247d39029fa34c2afd9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1144/format:webp/1*FwafEMDrRVNPdlGN6-ya6w.png"/></div></figure><p id="a1b5" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在我们知道了如何测量一组给定参数的误差，下一个问题是如何找到使误差最小的最佳参数。这就是优化算法如<a class="ae jg" href="https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html" rel="noopener ugc nofollow" target="_blank">梯度下降</a>发挥作用的地方。优化算法求解参数的最佳组合，使测井曲线损失最小化。实际上，优化算法的选择取决于实现。例如，默认情况下，Scikit-learn对<code class="fe os ot ou oi b"><a class="ae jg" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" rel="noopener ugc nofollow" target="_blank">LogisticRegression()</a></code>使用<a class="ae jg" href="https://stats.stackexchange.com/a/285106/249142" rel="noopener ugc nofollow" target="_blank"> LBFGS </a>优化算法。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="6efa" class="mq mr jj bd ms mt mu mv mw mx my mz na ky nb kz nc lb nd lc ne le nf lf ng nh bi translated">解释逻辑回归参数🔎</h1><p id="0f1a" class="pw-post-body-paragraph lh li jj lj b lk ni kt lm ln nj kw lp lq nk ls lt lu nl lw lx ly nm ma mb mc im bi translated">为了解释逻辑回归参数，我们首先需要熟悉优势、优势比和logit函数。</p><h2 id="6e96" class="nn mr jj bd ms no np dn mw nq nr dp na lq ns nt nc lu nu nv ne ly nw nx ng jp bi translated">📍可能性</h2><p id="ffed" class="pw-post-body-paragraph lh li jj lj b lk ni kt lm ln nj kw lp lq nk ls lt lu nl lw lx ly nm ma mb mc im bi translated">赔率是事件的概率与其互补事件的概率(即事件不发生的概率)之比:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pl"><img src="../Images/0018761b930c85ece0e64647d0e94051.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/1*jmUuXJnepnpv8llQUhIbMA.png"/></div></figure><p id="9394" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">例如，如果我们掷一枚公平硬币，得到正面的几率是1 (1:1):</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/d4cc8372e8eb7eb30c5e97b0813c93dc.png" data-original-src="https://miro.medium.com/v2/resize:fit:378/format:webp/1*yYhrQZ-3-fpj2Wb5_SwsKA.png"/></div></figure><p id="2348" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">挑一张黑桃牌的赔率是0.33 (1:3):</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/25b48c88ecc0f3bdbc0351cc376111c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*Y4JTf2x7filnSJi6PHZsOg.png"/></div></figure><p id="d178" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">黑桃牌的胜算不大。换句话说，拿到黑牌的可能性比拿到非黑牌的可能性低三倍。</p><p id="d0c3" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">赔率不能是负数，因为它是概率的比率，并具有以下含义:<br/> ◼️赔率低于1意味着赔率不利于该事件(例如0.25或20:80或1:4) <br/> ◼️赔率为1意味着赔率相等(即50:50或1:1)……(<em class="md">赔率</em>等于<em class="md">偶数</em>😅)<br/> ◼赔率高于1表示赔率对赛事有利(例如4或80:20或4:1)</p><h2 id="633c" class="nn mr jj bd ms no np dn mw nq nr dp na lq ns nt nc lu nu nv ne ly nw nx ng jp bi translated">📍让步比</h2><p id="3475" class="pw-post-body-paragraph lh li jj lj b lk ni kt lm ln nj kw lp lq nk ls lt lu nl lw lx ly nm ma mb mc im bi translated">比值比是两个比值之间的比值。</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi po"><img src="../Images/a392c7fcd172a1bd2bd1ae3dbedbcd51.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/format:webp/1*RgB65WUwt-MryoutMzXfUg.png"/></div></figure><p id="5de6" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">假设一个事件最初的赔率是0.25 (1:4)，但现在新的赔率是1.5 (3:2)。</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/6be765d9cc2e955246138e8986ee17a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:666/format:webp/1*aZM0aslDj2WKEQ0FJQ_0mw.png"/></div></figure><p id="7fe8" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">赔率是6。这意味着当前赔率比初始赔率大6倍。</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/42bac83fae59ef693355e030919d2d2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*CyZ_EhI6E_XvyV3kqvDhaA.png"/></div></figure><p id="7089" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">比值比也不能为负，因为它是两个比值之间的比值。赔率告诉我们赔率相对于基础赔率的变化:<br/> ◼️赔率低于1意味着赔率下降(赔率比以前低)<br/> ◼️赔率为1意味着赔率没有变化(赔率与以前相同)<br/> ◼️赔率高于1意味着赔率增加(赔率比以前高)</p><p id="f566" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">优势比对优势的大小不敏感。例如，从0.01到0.04和从2到8的比值都将导致比值比为4。</p><h2 id="7aa3" class="nn mr jj bd ms no np dn mw nq nr dp na lq ns nt nc lu nu nv ne ly nw nx ng jp bi translated">📍Logit函数</h2><p id="fae9" class="pw-post-body-paragraph lh li jj lj b lk ni kt lm ln nj kw lp lq nk ls lt lu nl lw lx ly nm ma mb mc im bi translated">现在，让我们熟悉一下<em class="md"> logit函数</em>。</p><blockquote class="ny nz oa"><p id="07de" class="lh li md lj b lk ll kt lm ln lo kw lp ob lr ls lt oc lv lw lx od lz ma mb mc im bi translated">Logit函数的作用与logistic函数相反:它将概率转换为实数。</p></blockquote><p id="8732" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果我们重新排列逻辑回归方程，使得右边类似于线性回归方程，我们在左边得到logit函数:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pr"><img src="../Images/da5366c259fb261d52e2137372877e26.png" data-original-src="https://miro.medium.com/v2/resize:fit:960/format:webp/1*55dwu62wlM0luH76wlHb4A.png"/></div></figure><p id="2c35" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这种到logit函数的转换有助于解释模型参数。我们可以用类似于<a class="ae jg" rel="noopener" target="_blank" href="/linear-regression-explained-89cc3886ab48">线性回归</a>的方式解释参数，但是我们相对于对数概率或属于正类的概率而不是连续的目标变量来解释它。</p><h2 id="ae33" class="nn mr jj bd ms no np dn mw nq nr dp na lq ns nt nc lu nu nv ne ly nw nx ng jp bi translated">📍解释逻辑回归参数</h2><p id="57cd" class="pw-post-body-paragraph lh li jj lj b lk ni kt lm ln nj kw lp lq nk ls lt lu nl lw lx ly nm ma mb mc im bi translated">对于大多数人来说，开始时解释逻辑回归参数并不直观。如果你第一次就明白了，干得好！否则，不要担心，随着练习，它会变得更加直观，所以要花时间来吸收这些知识。</p><p id="8538" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们看看小模型的参数汇总:</p><pre class="mf mg mh mi gt oh oi oj ok aw ol bi"><span id="76a2" class="nn mr jj oi b gy om on l oo op">summary = pd.DataFrame(np.concatenate([model.intercept_,<br/>                                       model.coef_[0]]), <br/>                       index=[f'θ_{i}' for i in range(3)], <br/>                       columns=['values'])<br/>summary['exp(values)'] = np.exp(summary['values'])<br/>summary</span></pre><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/822a83d5df054ebdd7a7e4dabeb76cb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:412/format:webp/1*TAekiFoRJgGGDLp5tcWq0A.png"/></div></figure><p id="cfa6" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们可以用以下方式解释这些参数:</p><p id="dead" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">◼️:如果我们使用<code class="fe os ot ou oi b">exp(values)</code>，我们相对于<em class="md"/><em class="md">几率</em>来解释它，希望这样更容易理解。我建议使用这种方法:</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pt"><img src="../Images/7c015b5cb8842b8e3d7d2014afacba1d.png" data-original-src="https://miro.medium.com/v2/resize:fit:612/format:webp/1*H-H6eLbigTdPBcteN4V2VA.png"/></div></figure><p id="033d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">◼️如果我们使用<code class="fe os ot ou oi b">values</code>，我们将需要相对于<em class="md">来解释它的对数几率:</em></p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/1f938edf9961a65108488e72dcea2214.png" data-original-src="https://miro.medium.com/v2/resize:fit:840/format:webp/1*jr5hNRzXKKOQMqsCqnFJ7w.png"/></div></figure><p id="67ae" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们看看我们构建的样本模型的一些示例解释:</p><p id="3781" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">截距:</strong>当所有特征都为0时，<em class="md">成为正类的几率</em>预计为0.0045。</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pu"><img src="../Images/28b75b870d0dd7d133cd5e1994e0b574.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*SPHZi13OZq6DZ3rHtViVAw.png"/></div></figure><p id="7f01" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">或者，当所有特征取值为0时，<em class="md">正类的对数概率</em>预计为-5.3994。</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pv"><img src="../Images/64494919a52ed1eeec82f3d29cc7ccc4.png" data-original-src="https://miro.medium.com/v2/resize:fit:818/format:webp/1*bRDt1uz8wzK7Z5RYbCTtWg.png"/></div></figure><p id="3cb0" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jt">系数:</strong>保持其他变量不变，如果<code class="fe os ot ou oi b">x1</code>增加一个单位，<em class="md">属于正类的几率</em>预计变化3.6982。</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi pw"><img src="../Images/fff7874bbb12e9a179fe774977ea7b19.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/format:webp/1*GWsKJMLeUSdYl-XdmeWZBA.png"/></div></figure><p id="7317" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">或者，保持<code class="fe os ot ou oi b">x2</code>不变，如果<code class="fe os ot ou oi b">x1</code>增加一个单位，<em class="md">属于阳性类别的对数概率</em>预计将改变1.3078。</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div class="gh gi px"><img src="../Images/b7d70d470a795b33e32097468123a0cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*TsC9od-P3rWskdTRvg_Hzw.png"/></div></figure><p id="5b0d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我们现在可以看到，逻辑回归系数显示对数优势比。</p><figure class="mf mg mh mi gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi py"><img src="../Images/2bd0a2d5a53d8d318033cf797253cc9f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gcw7A9ypoQtplHaQ"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">丹-克里斯蒂安·pădureț在<a class="ae jg" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="42fd" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这就是这篇文章的全部内容！希望你喜欢学习逻辑回归背后的直觉和数学。一路上，我们还学习了其他相关的概念，如赔率、赔率比和多个有用的函数。如果你也想学习神经网络，我们今天学习的一些概念如逻辑函数和对数损失函数将会派上用场。最后，当向涉众解释模型的驱动因素时，能够解释如何解释逻辑回归结果是有帮助的。如果你渴望了解更多关于逻辑回归的知识，请查看这个资源。</p><p id="e9ca" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><em class="md">您想访问更多这样的内容吗？媒体会员可以无限制地访问媒体上的任何文章。如果您使用</em> <a class="ae jg" href="https://zluvsand.medium.com/membership" rel="noopener"> <em class="md">我的推荐链接</em></a><em class="md">成为会员，您的一部分会费将直接用于支持我。</em></p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="9080" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">感谢您阅读我的文章。如果你感兴趣，这里有我其他一些帖子的链接:</p><p id="4e1f" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">◼️️ <a class="ae jg" rel="noopener" target="_blank" href="/k-nearest-neighbours-explained-52c910c035c5"> K近邻讲解</a> <br/> ◼️️ <a class="ae jg" rel="noopener" target="_blank" href="/comparing-random-forest-and-gradient-boosting-d7236b429c15">比较随机森林和梯度推进</a> <br/> ◼️️ <a class="ae jg" rel="noopener" target="_blank" href="/how-are-decision-trees-built-a8e5af57ce8?source=your_stories_page-------------------------------------">决策树是如何建立的？</a> <br/> ◼️️ <a class="ae jg" rel="noopener" target="_blank" href="/pipeline-columntransformer-and-featureunion-explained-f5491f815f?source=your_stories_page-------------------------------------">管道、ColumnTransformer和FeatureUnion说明</a>t21】◼️️<a class="ae jg" rel="noopener" target="_blank" href="/featureunion-columntransformer-pipeline-for-preprocessing-text-data-9dcb233dbcb6">feature union、ColumnTransformer &amp;管道用于预处理文本数据</a></p><p id="434d" class="pw-post-body-paragraph lh li jj lj b lk ll kt lm ln lo kw lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">再见🏃 💨</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="a39e" class="mq mr jj bd ms mt mu mv mw mx my mz na ky nb kz nc lb nd lc ne le nf lf ng nh bi translated">参考📁</h1><ul class=""><li id="1ee3" class="pz qa jj lj b lk ni ln nj lq qb lu qc ly qd mc qe qf qg qh bi translated">Aurelien Geron，<em class="md">使用Scikit-Learn、Keras和TensorFlow进行动手机器学习，2017年</em> —第4章</li></ul></div></div>    
</body>
</html>