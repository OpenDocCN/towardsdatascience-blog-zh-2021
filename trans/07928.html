<html>
<head>
<title>Understand your Algorithm with Grad-CAM</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Grad-CAM了解您的算法</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understand-your-algorithm-with-grad-cam-d3b62fce353?source=collection_archive---------0-----------------------#2021-07-21">https://towardsdatascience.com/understand-your-algorithm-with-grad-cam-d3b62fce353?source=collection_archive---------0-----------------------#2021-07-21</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1224" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated"><em class="kf">当它是一个黑匣子时，我们为什么要足够信任AI来驾驶汽车，检测疾病，识别嫌疑人？</em></h2></div><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/25adb65b9e9481cbbe6fe09cf12c33d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*KoHwRNZGrVrhdbye3BDEew.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">Grad-CAM解读猫狗(图片由作者提供)</p></figure></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="77af" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">介绍</h1><p id="3f3f" class="pw-post-body-paragraph lv lw iq lx b ly lz jr ma mb mc ju md me mf mg mh mi mj mk ml mm mn mo mp mq ij bi translated"><em class="mr">人工智能仅仅是一个黑匣子，我们开始信任它足以驾驶汽车，检测疾病，识别嫌疑人，只是因为大肆宣传吗？</em></p><p id="1464" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">你可能听说过网飞的纪录片，<a class="ae mx" href="https://www.codedbias.com/" rel="noopener ugc nofollow" target="_blank"> <em class="mr">编码偏差</em> </a> <em class="mr"> </em>(这里可以看电影<a class="ae mx" href="https://www.netflix.com/watch/81328723?trackId=13752289&amp;tctx=0%2C0%2Cc77bcc95096eda2155aea4834c5e05d7e93362a6%3A32c7d55a6b15cbadcb275c7f53e59f1978b6351f%2Cc77bcc95096eda2155aea4834c5e05d7e93362a6%3A32c7d55a6b15cbadcb275c7f53e59f1978b6351f%2C%2C" rel="noopener ugc nofollow" target="_blank">)。这部电影批评了深度学习算法固有的偏见；特别是他们没有发现深色皮肤和女性的脸。这部电影表明解决问题的办法在政府。“推动美国有史以来第一部针对影响我们所有人的算法偏见的立法。”是深度学习算法本身存在固有的偏见，还是训练数据中的偏见问题？虽然对人工智能监管的需求存在，但解决方案的很大一部分取决于机器学习工程师。我们需要让我们的深度学习算法更具可解释性，找到弱点和偏见的领域，并改进我们的算法。</a></p><p id="b144" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated"><strong class="lx ir">如何才能理解我们的卷积神经网络(黑盒)在做决策时看到和理解的东西？</strong></p><p id="58c0" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">这篇博客文章完全是关于制作视觉解释热图，这将帮助我们理解深度学习算法如何做出决策。在专注于建筑安全和安保的建筑技术初创公司<a class="ae mx" href="https://forsight.ai/" rel="noopener ugc nofollow" target="_blank"> Forsight </a>，我们的计算机视觉团队使用视觉解释来改进我们的数据集和模型。点击阅读更多关于我们工作的信息<a class="ae mx" href="https://medium.com/swlh/construction-feat-tf2-object-detection-api-4465a3937c87#" rel="noopener">。现在，我们要探讨的是视觉解释算法:<strong class="lx ir"> Grad-CAM </strong>。</a></p><p id="b83f" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">在本文中，我们将向您展示如何使用Grad-CAM解读一组猫狗图像。您可以轻松地扩展这个示例，并使用它来调试您自己的模型。我们将向您展示如何通过查看模型中早期图层的Grad-CAM热图来提高现有文献的准确性和精确度。最后，我们将研究模型的错误，看看Grad-CAM如何帮助我们改进模型。让我们跳进来。</p><p id="7d5c" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated"><em class="mr">补充</em><a class="ae mx" href="https://colab.research.google.com/drive/1rxmXus_nrGEhxlQK_By38AjwDxwmLn9S?usp=sharing" rel="noopener ugc nofollow" target="_blank"><em class="mr">Google Colab</em></a><em class="mr">笔记本会帮你重现这里制作的热图。请随意复制代码并将其用于您自己的项目！</em></p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="7349" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">数据集和模型</h1><p id="b9d6" class="pw-post-body-paragraph lv lw iq lx b ly lz jr ma mb mc ju md me mf mg mh mi mj mk ml mm mn mo mp mq ij bi translated">在将Grad-CAM解释应用于复杂的数据集和任务之前，让我们用一个经典的图像分类问题来简化它。我们将使用来自<a class="ae mx" href="https://www.kaggle.com/c/dogs-vs-cats/data" rel="noopener ugc nofollow" target="_blank"> kaggle </a>的高质量数据集对猫&amp;狗进行分类。这里我们有一个包含37，500张图像的大型数据集(25，000次训练&amp; 12，500次测试)。数据包含两类:猫&amp;狗。这些数据包含了各种背景下的猫和狗，不同的亮度水平，以及参与不同的活动，这将考验我们的视觉解释！</p><p id="e5f2" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">在<a class="ae mx" href="https://colab.research.google.com/drive/1rxmXus_nrGEhxlQK_By38AjwDxwmLn9S?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Google Colab </a>笔记本上，你将加载一个预先训练好的模型，它已经可以对猫&amp;狗进行分类。对于那些对模型架构感兴趣的人来说:它由标准化的重新缩放层组成，随后是先前在COCO数据集上训练的<a class="ae mx" href="https://arxiv.org/abs/1801.04381" rel="noopener ugc nofollow" target="_blank"> <strong class="lx ir"> MobileNetV2 </strong> </a>层。在基本模型之后，我们包括了一个全局平均池层、一个10%漏失的漏失层和一个密集预测层。在下图中，您可以看到培训后的所有评估集指标。提醒误报表明猫被错误地归类为狗。同样，假阴性表示狗被错误地归类为猫。一个经过训练的模型已经准备好了，我们可以开始研究这个问题:<em class="mr">它是如何做出决策的？</em></p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi my"><img src="../Images/fe86cbfd3e595521a34a8fb06af91eac.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*Z4byarUTUjlbvTOJUNqgWw.png"/></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">模型指标(作者图片)</p></figure></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="abe6" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">摄像机</h1><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi mz"><img src="../Images/85ae491221e228200a1b339cf5d497a3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zq1lp5Bg8sdflG_ngrezzg.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">Grad-CAM由Ramprasaath R. Selvaraju等人对arxiv.org进行概述</p></figure><p id="740c" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated"><strong class="lx ir"> <em class="mr">警告，Grad-CAM可能很难把你的头缠在</em>上。</strong></p><blockquote class="na nb nc"><p id="58dc" class="lv lw mr lx b ly ms jr ma mb mt ju md nd mu mg mh ne mv mk ml nf mw mo mp mq ij bi translated">梯度加权类激活映射(Grad-CAM)使用流入最终卷积层的任何目标概念(比如分类网络中的“狗”或字幕网络中的单词序列)的梯度来产生粗略的定位图，该定位图突出显示图像中的重要区域以预测概念。</p></blockquote><p id="7988" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated"><a class="ae mx" href="https://arxiv.org/abs/1610.02391" rel="noopener ugc nofollow" target="_blank">该技术</a>在通用性和准确性方面比以前的方法有所改进。这很复杂，但幸运的是，输出很直观。从高层次来看，我们将一幅图像作为输入，并创建一个模型，该模型在我们想要创建Grad-CAM热图的层被切断。我们附加完全连接的层用于预测。然后，我们通过模型运行输入，获取图层输出和损失。接下来，我们找到我们期望的模型层的输出相对于模型损耗的梯度。从那里，我们采取有助于预测的梯度部分，减少，调整大小，并重新缩放，以便热图可以与原始图像重叠。你可以遵循下面代码中的具体步骤，并在这里查看概述完整数学的学术论文<a class="ae mx" href="https://arxiv.org/pdf/1610.02391.pdf" rel="noopener ugc nofollow" target="_blank"/>。</p><pre class="kh ki kj kk gt ng nh ni nj aw nk bi"><span id="5978" class="nl le iq nh b gy nm nn l no np">gradModel = Model(</span><span id="af8e" class="nl le iq nh b gy nq nn l no np">inputs=[model.inputs],</span><span id="c6b0" class="nl le iq nh b gy nq nn l no np">outputs=[model.get_layer(layer_name).output,</span><span id="e34b" class="nl le iq nh b gy nq nn l no np">model.output])</span><span id="27a8" class="nl le iq nh b gy nq nn l no np">with tf.GradientTape() as tape:</span><span id="edf7" class="nl le iq nh b gy nq nn l no np">   # cast the image tensor to a float-32 data type, pass the</span><span id="fbf4" class="nl le iq nh b gy nq nn l no np">   # forward propagate the image through the gradient model, and grab the loss</span><span id="7a25" class="nl le iq nh b gy nq nn l no np">   # associated with the specific class index</span><span id="1c54" class="nl le iq nh b gy nq nn l no np">   inputs = tf.cast(img_array, tf.float32)</span><span id="6de8" class="nl le iq nh b gy nq nn l no np">   (convOutputs, predictions) = gradModel(inputs)</span><span id="1234" class="nl le iq nh b gy nq nn l no np">   loss = predictions[:, 0]</span><span id="8efd" class="nl le iq nh b gy nq nn l no np"># use automatic differentiation to compute the gradients</span><span id="850e" class="nl le iq nh b gy nq nn l no np">grads = tape.gradient(loss, convOutputs)</span><span id="258d" class="nl le iq nh b gy nq nn l no np"># compute the guided gradients</span><span id="0596" class="nl le iq nh b gy nq nn l no np">castConvOutputs = tf.cast(convOutputs &gt; 0, "float32")</span><span id="61db" class="nl le iq nh b gy nq nn l no np">castGrads = tf.cast(grads &gt; 0, "float32")</span><span id="ad8e" class="nl le iq nh b gy nq nn l no np">guidedGrads = castConvOutputs * castGrads * grads</span><span id="b934" class="nl le iq nh b gy nq nn l no np"># the convolution and guided gradients have a batch dimension</span><span id="ffaf" class="nl le iq nh b gy nq nn l no np"># (which we don't need) so let's grab the volume itself and</span><span id="f64a" class="nl le iq nh b gy nq nn l no np"># discard the batch</span><span id="6245" class="nl le iq nh b gy nq nn l no np">convOutputs = convOutputs[0]</span><span id="9bba" class="nl le iq nh b gy nq nn l no np">guidedGrads = guidedGrads[0]</span><span id="1f27" class="nl le iq nh b gy nq nn l no np"># compute the average of the gradient values, and using them</span><span id="71c8" class="nl le iq nh b gy nq nn l no np"># as weights, compute the ponderation of the filters with</span><span id="5d0f" class="nl le iq nh b gy nq nn l no np"># respect to the weights</span><span id="ca66" class="nl le iq nh b gy nq nn l no np">weights = tf.reduce_mean(guidedGrads, axis=(0, 1))</span><span id="0cd7" class="nl le iq nh b gy nq nn l no np">cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)</span><span id="490a" class="nl le iq nh b gy nq nn l no np"># grab the spatial dimensions of the input image and resize</span><span id="930a" class="nl le iq nh b gy nq nn l no np"># the output class activation map to match the input image</span><span id="58dd" class="nl le iq nh b gy nq nn l no np"># dimensions</span><span id="ce36" class="nl le iq nh b gy nq nn l no np">(w, h) = (img_array.shape[2], img_array.shape[1])</span><span id="3315" class="nl le iq nh b gy nq nn l no np">heatmap = cv2.resize(cam.numpy(), (w, h))</span><span id="8c78" class="nl le iq nh b gy nq nn l no np"># normalize the heatmap such that all values lie in the range</span><span id="e41d" class="nl le iq nh b gy nq nn l no np"># [0, 1], scale the resulting values to the range [0, 255],</span><span id="b344" class="nl le iq nh b gy nq nn l no np"># and then convert to an unsigned 8-bit integer</span><span id="5c72" class="nl le iq nh b gy nq nn l no np">numer = heatmap - np.min(heatmap)</span><span id="04a5" class="nl le iq nh b gy nq nn l no np">denom = (heatmap.max() - heatmap.min()) + eps</span><span id="54b4" class="nl le iq nh b gy nq nn l no np">heatmap = numer / denom</span></pre><p id="8b27" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">配备了Grad-CAM生成代码，我们带着预先训练好的模型和验证集去看看<em class="mr">我们的模型对猫&amp;狗</em>了解多少。</p><p id="066e" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">我们开始为模型中的最后一个卷积层conv 1创建Grad-CAM热图。理论上，该层的热图应该显示被模型分类的对象的最准确的视觉解释。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nr"><img src="../Images/3ffab993351aae07955d46d6fdc343d5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_L2hUgxVmj_EZRZ7Dyxgvw.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">grad-CAM conv 1层热图(图片由作者提供)</p></figure><p id="b8f1" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">在梯度数学中，我们捕捉所有通向最后一个卷积层的连续特征图的重要性。我们注意到，虽然热图强调了分类对象，在本例中是一只猫，但它并不十分精确。被强调的区域(红色)包围了猫的区域，但是不太精确地适合猫。该区域包括人的衬衫、人的手和背景的部分。我们知道模型看到了一只猫，但是我们不太确定是这只猫的什么让模型相信这确实是一只猫。我们怎样才能使Grad-CAM热图更精确？</p><p id="04e5" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">你会注意到上面的代码已经被包装在Google Colab笔记本的一个函数中，以允许Grad-CAM在不同的模型层中重现。让我们使用这个函数来探索早期的模型层。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/1326e34e0009c6a20b2e217d29a2a49c.png" data-original-src="https://miro.medium.com/v2/resize:fit:700/1*I3Yc0KHi9ONp_-vbHXVbuw.gif"/></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">所有模型层的Grad-CAM热图的GIF(GIF由作者提供)</p></figure><p id="9807" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">这里有很多东西需要消化。当我们研究热图时，模型学习的逻辑开始出现。前10层(块1到块3)检测图像中的轮廓和边界。深度方向层不强调对象，而投影和扩展层不强调轮廓。接下来的大约20层(块4到11)是检测图像中的概念。Block 4 expand relu就是一个很好的例子。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nr"><img src="../Images/15f4a201fe2c07f7cd3ddc18790e1598.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*15BDTROG9jEGHaYlyswUxg.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">block_4_expand_relu图层的Grad-CAM热图(图片由作者提供)</p></figure><p id="6126" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">在模型架构的这一点上，定义猫的特征的形状开始从图像中的一般轮廓中脱颖而出。你可以看到猫的胡须、耳朵和眼睛是如何用红色和黄色的阴影强调的，而图像的其余部分是蓝色和绿色的。当我们深入模型的各层时，你可以看到模型是如何推理猫的定义特征如何区分这张图像的。最后的~16层(块12到16)尝试使用来自最早层的空间信息和最近开发的概念来识别图像中的对象。模型中的最后一层，Conv_1，确实正确地识别了物体的大致区域，但是它没有拾取存在于早期Grad-CAM热图中的物体的细微差别。我们用人类的直觉代替Grad-CAM的数学，它改进了Grad-CAM的结果。</p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><p id="4374" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">为了合并早期的图层，我们将来自<strong class="lx ir">所有模型图层</strong>的Grad-CAM热图平均在一起。你可以在下面看到结果。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi kg"><img src="../Images/2e4c42f1afecb23f73877b106bbdbe6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*IhUduiz2V5MmPWBpFFIM2w.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">Grad-CAM热图改进(图片由作者提供)</p></figure><p id="ed09" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">Grad-CAM热图现在强调猫的脸、眼睛和爪子，不强调人的手臂。总的来说，我们有一个更精确的重点区域来定位猫。我们知道，该模型根据其内在特征将该图像分类为猫，而不是图像中的一般区域。</p><p id="373c" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">模型给人印象特别深刻，因为这是一个很难的图像！这个人穿着一件黑白相间的衬衫，和猫的颜色一样。然而，该模型可以区分黑猫脸和黑色人类衬衫。通过全层Grad-CAM，我们了解了模型的优势。<strong class="lx ir">我们如何使用Grad-CAM来了解我们模型的弱点？</strong></p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><p id="b365" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">让我们仔细看看下面7张被错误归类为狗的猫的图片。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nt"><img src="../Images/d1dddb24adb1cf5ca0113e43b50bd1ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pUhz_IiffaSB2sXOBvdheQ.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">模型误报(图片由作者提供)</p></figure><p id="fdbe" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">我们可以使用Grad-CAM热图来为我们提供线索，了解为什么模型在进行正确分类时遇到了困难。当一只猫坐在笼子后面时，这个模型似乎在区分它时遇到了一些困难。我们可以从Grad-CAM中看到，模型强调笼子的栅栏，很难找到猫的特征。在5和6中，猫尾巴是图像中的显著特征。但是从Grad-CAM中，我们可以看到模型很难识别这个特征，因为它带有绿色和蓝色的阴影。Grad-CAM热图所能提供的最明显的例子在# 1中。模特被猫玩的玩具弄糊涂了。从热图中我们可以看到，模型认为玩具是猫的一部分。这种混乱造成了模型的错误分类。使用这些知识，我们可以找到更多的猫玩玩具的例子，并将其包含在我们的数据集中，这将有助于我们的模型学习和改进。</p><p id="a859" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated"><strong class="lx ir">改进模型</strong></p><p id="d721" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">注意到我们的模型在笼子方面有问题，我们从google images中拿出了一些更具体的例子；特别是运输板条箱。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nu"><img src="../Images/79b91c8142506bf727e24987421bd2b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o6mEHI8krZSR37VFDugyZQ.png"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">这个模型在运输工具上有问题(图片由作者提供)</p></figure><p id="2a5e" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">从grad-CAM热图上的红色区域可以看出，模型关注的是猫周围的运输箱，而不是猫本身，这导致了错误的分类。当猫在运输板条箱中时，我们如何帮助模型区分它们？</p><p id="a95d" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">我们从谷歌图片中创建了一个补充数据集，包括运输板条箱中的猫和狗。使用<a class="ae mx" href="https://albumentations.ai/" rel="noopener ugc nofollow" target="_blank">相册</a> <em class="mr">，</em>我们用额外的图像增强来增强数据集。有了我们需要的数据，我们开始实验。我们用原始数据集创建了一系列模型，并添加了25%、50%、75%和100%的补充运输板条箱数据集。在我们实验的每一次迭代中，我们都回到Grad-CAM，看看模型是否在原始图像上调整了它的重点区域。</p><p id="adbb" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">为了查看模型真正锁定的位置，我们创建了一个遮罩，使用阈值来捕捉Grad-CAM中最强烈的(红色)区域。然后，我们使用蒙版分割出原始图像中最重要的像素。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nv"><img src="../Images/43ebf95a42836ca9fb013fd10c563a84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*uFcITXCWbXMFXhT9XPOOCg.gif"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">随着我们从扩充的集合中添加数据，模型得到了改进</p></figure><p id="10b4" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">正如您从上面的过滤图像中看到的，随着我们从补充数据集向训练数据集添加更多图像，模型的焦点从板条箱转移到猫的面部和特征。而且不需要很长时间，模型就开始得到正确的分类！当模型在原始数据集加上仅25%的补充运输器板条箱数据集上训练时，模型指定上面的图像是猫的概率为90%。</p><p id="67bb" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">使用同样的实验，我们观察了笼子里的猫的其他图像，看看我们是否可以在模型焦点中找到相同的进展。下面是一些突出的例子。</p><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nu"><img src="../Images/41f3fc0a983e098044cd050dd62aa299.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*P8IeH5ua-FAH8BD1nc6BjQ.gif"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">作者GIF</p></figure><figure class="kh ki kj kk gt kl gh gi paragraph-image"><div role="button" tabindex="0" class="km kn di ko bf kp"><div class="gh gi nu"><img src="../Images/69e5e1b33f04085013aeb43143f21e0d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*5kYMghiwbQjWkjFR8Bwvuw.gif"/></div></div><p class="ks kt gj gh gi ku kv bd b be z dk translated">作者GIF</p></figure><h1 id="b247" class="ld le iq bd lf lg nw li lj lk nx lm ln jw ny jx lp jz nz ka lr kc oa kd lt lu bi translated">结论</h1><p id="9d69" class="pw-post-body-paragraph lv lw iq lx b ly lz jr ma mb mc ju md me mf mg mh mi mj mk ml mm mn mo mp mq ij bi translated">在这篇博客文章中，我们希望使用Grad-CAM为卷积神经网络可视化解释提供一些有用的见解和工具。在猫和狗的帮助下，我们探索了一个模型如何区分类别。通过考虑模型所有层的热图，我们改进了最终卷积层的Grad-CAM结果。我们用Grad-CAM瞄准了模型中的偏差和弱点。最重要的是，我们使用Grad-CAM来改进我们的算法。机器学习是一个迭代过程，我们的模型永远不够好。我们希望这里介绍的技术将有助于在整个改进过程中提供更多的透明度，并在模型中建立更多的信任。在Forsight，我们使用Grad-CAM热图来识别模型中的错误重点区域。</p><p id="eda9" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated">为了帮助ML/AI爱好者，以及任何有兴趣帮助解决这个问题的人，我们创建并分享了一个<a class="ae mx" href="https://colab.research.google.com/drive/1rxmXus_nrGEhxlQK_By38AjwDxwmLn9S?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Google Colab </a>笔记本，它让你能够摆弄和制作我们在这篇博客中展示的Grad-CAM热图。</p><p id="eede" class="pw-post-body-paragraph lv lw iq lx b ly ms jr ma mb mt ju md me mu mg mh mi mv mk ml mm mw mo mp mq ij bi translated"><strong class="lx ir">如果你对这个话题感兴趣，并且你愿意致力于深度学习算法的解释，请</strong> <a class="ae mx" href="https://forsight.ai/contact/" rel="noopener ugc nofollow" target="_blank"> <strong class="lx ir">联系我们</strong> </a> <strong class="lx ir">。</strong></p></div><div class="ab cl kw kx hu ky" role="separator"><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb lc"/><span class="kz bw bk la lb"/></div><div class="ij ik il im in"><h1 id="dcd0" class="ld le iq bd lf lg lh li lj lk ll lm ln jw lo jx lp jz lq ka lr kc ls kd lt lu bi translated">参考</h1><ol class=""><li id="eb58" class="ob oc iq lx b ly lz mb mc me od mi oe mm of mq og oh oi oj bi translated">Ramprasaath R. Selvaraju等人，2019年，“Grad-CAM:通过基于梯度的定位从深度网络进行视觉解释”，<a class="ae mx" href="https://arxiv.org/pdf/1610.02391.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/pdf/1610.02391.pdf</a></li><li id="7b2d" class="ob oc iq lx b ly ok mb ol me om mi on mm oo mq og oh oi oj bi translated">狗对猫，<a class="ae mx" href="https://www.kaggle.com/c/dogs-vs-cats/data" rel="noopener ugc nofollow" target="_blank">https://www.kaggle.com/c/dogs-vs-cats/data</a></li><li id="0269" class="ob oc iq lx b ly ok mb ol me om mi on mm oo mq og oh oi oj bi translated">马克·桑德勒等人，2019，“MobileNetV2:反向残差和线性瓶颈”，<a class="ae mx" href="https://arxiv.org/pdf/1801.04381.pdf" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1801.04381</a></li><li id="f490" class="ob oc iq lx b ly ok mb ol me om mi on mm oo mq og oh oi oj bi translated"><em class="mr">编码偏差</em>，2020，<a class="ae mx" href="https://www.codedbias.com/" rel="noopener ugc nofollow" target="_blank">https://www.codedbias.com/</a></li></ol></div></div>    
</body>
</html>