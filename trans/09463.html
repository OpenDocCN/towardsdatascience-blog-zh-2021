<html>
<head>
<title>An Introduction to Reinforcement Learning with OpenAI Gym, RLlib, and Google Colab</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用OpenAI Gym、RLlib和Google Colab进行强化学习的介绍</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/an-introduction-to-reinforcement-learning-with-openai-gym-rllib-and-google-colab-48fc1ddfb889?source=collection_archive---------15-----------------------#2021-09-02">https://towardsdatascience.com/an-introduction-to-reinforcement-learning-with-openai-gym-rllib-and-google-colab-48fc1ddfb889?source=collection_archive---------15-----------------------#2021-09-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f6ae" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">使用OpenAI Gym、RLlib和Google Colab进行强化学习的入门教程</h2></div><figure class="kf kg kh ki gt kj"><div class="bz fp l di"><div class="kk kl l"/></div><p class="km kn gj gh gi ko kp bd b be z dk translated">本教程将使用强化学习(RL)来帮助平衡一个虚拟的横竿。上面来自PilcoLearner的<a class="ae kq" href="https://www.youtube.com/watch?v=XiigTGKZfks" rel="noopener ugc nofollow" target="_blank">视频</a>展示了在现实生活中使用RL的结果。</p></figure><p id="b921" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><strong class="kt ir">作者</strong> : <a class="ae kq" href="https://twitter.com/GalarnykMichael" rel="noopener ugc nofollow" target="_blank">迈克尔·加拉尼克</a>和<a class="ae kq" href="https://twitter.com/sven_mika" rel="noopener ugc nofollow" target="_blank">斯文·米卡</a></p><p id="7b06" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">强化学习(RL)的一个可能的定义是学习如何在与环境交互时最大化总回报的计算方法。虽然定义是有用的，但本教程旨在通过图像、代码和视频示例来说明什么是强化学习，同时介绍代理和环境等强化学习术语。</p><p id="80c2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">特别是，本教程探索了:</p><ul class=""><li id="f7b0" class="ln lo iq kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated">什么是强化学习</li><li id="363a" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">开放的健身房钢管环境</li><li id="348a" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">代理在强化学习中的作用</li><li id="5487" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">如何使用Python库RLlib训练代理</li><li id="ab90" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">如何使用GPU加速训练</li><li id="90a3" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">使用光线调节进行超参数调节</li></ul><h1 id="00f2" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">什么是强化学习</h1><p id="e499" class="pw-post-body-paragraph kr ks iq kt b ku mt jr kw kx mu ju kz la mv lc ld le mw lg lh li mx lk ll lm ij bi translated">正如<a class="ae kq" href="https://www.anyscale.com/blog/reinforcement-learning-with-rllib-in-the-unity-game-engine" rel="noopener ugc nofollow" target="_blank">之前的一篇帖子提到的</a>，人工智能的一个子领域机器学习(ML)使用神经网络或其他类型的数学模型来学习如何解释复杂的模式。ML的两个领域由于其高度的成熟度而最近变得非常受欢迎，这两个领域是监督学习(SL)和强化学习(RL)，在监督学习中，神经网络学习基于大量数据进行预测，在强化学习中，网络使用模拟器以试错的方式学习做出良好的行动决策。</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi my"><img src="../Images/95a29e4f9e5eb60e575827487ef26172.png" data-original-src="https://miro.medium.com/v2/resize:fit:1272/0*OQOxucT7BIV2xN7r"/></div><p class="km kn gj gh gi ko kp bd b be z dk translated">作者图片</p></figure><p id="76b6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">RL是令人难以置信的成功背后的技术，例如DeepMind的AlphaGo Zero和《星际争霸2》AI(alpha star)或OpenAI的DOTA 2 AI(“open AI Five”)。请注意<a class="ae kq" href="https://www.anyscale.com/blog/best-reinforcement-learning-talks-from-ray-summit-2021" rel="noopener ugc nofollow" target="_blank">强化学习有许多令人印象深刻的用途</a>，它在现实生活决策问题中如此强大和有前途的原因是因为RL能够持续学习——有时甚至在不断变化的环境中——从不知道要做出什么决定(随机行为)开始。</p><h1 id="c82b" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">代理和环境</h1><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nb"><img src="../Images/f81244b996c6fa22457b83ca6c3cb623.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X9YBdLB2NecK6m43MsvMMA.png"/></div></div><p class="km kn gj gh gi ko kp bd b be z dk translated">作者图片</p></figure><p id="c841" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">上图显示了代理和环境之间的交互和通信。在强化学习中，一个或多个智能体在一个环境中相互作用，这个环境可以是本教程中类似CartPole的模拟，也可以是与真实世界传感器和执行器的连接。在每一步，代理接收一个观察(即环境的状态)，采取一个行动，并且通常接收一个奖励(代理接收奖励的频率取决于给定的任务或问题)。代理人从重复的试验中学习，这些试验的序列被称为一个事件——从最初的观察到导致环境达到其“完成”状态的“成功”或“失败”的动作序列。RL框架的学习部分训练关于哪些动作(即，顺序决策)导致代理最大化他们的长期累积回报的策略。</p><h1 id="b62d" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">开放的健身房钢管环境</h1><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi ng"><img src="../Images/1599f8a2e21762a0d360667703f04d27.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kndmeJCWFaNNAJEEVaxK1Q.png"/></div></div><p class="km kn gj gh gi ko kp bd b be z dk translated">作者图片</p></figure><p id="c653" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">我们试图解决的问题是试图保持一根柱子直立。具体来说，该杆通过一个非驱动关节连接到一个小车上，小车沿无摩擦轨道移动。钟摆开始直立，目标是通过增加和减少小车的速度来防止它倒下。</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nh"><img src="../Images/d428b992b3aa2d67fdcd45ac8e41db79.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*vikll2QyrqJwiD9W"/></div></div><p class="km kn gj gh gi ko kp bd b be z dk translated">作者图片</p></figure><p id="e2ce" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">本教程将使用<a class="ae kq" href="http://gym.openai.com/" rel="noopener ugc nofollow" target="_blank"> OpenAI Gym </a>而不是从头开始编写这个环境，OpenAI Gym 是一个工具包，它提供了各种各样的模拟环境(雅达利游戏、棋盘游戏、2D和3D物理模拟，等等)。Gym对您的代理的结构没有任何假设(在这个cartpole示例中，是什么推动小车向左或向右)，并且与任何数值计算库兼容，比如numpy。</p><p id="2906" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">下面的代码加载CartPole环境。</p><pre class="kf kg kh ki gt ni nj nk nl aw nm bi"><span id="827d" class="nn mc iq nj b gy no np l nq nr">import gym<br/>env = gym.make("CartPole-v0")</span></pre><p id="f986" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在让我们通过观察动作空间来开始理解这个环境。</p><pre class="kf kg kh ki gt ni nj nk nl aw nm bi"><span id="1f2e" class="nn mc iq nj b gy no np l nq nr">env.action_space</span></pre><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/ae9e0139012356998f7e83e959b7eab8.png" data-original-src="https://miro.medium.com/v2/resize:fit:580/0*M9O-lv9RJVb7NL_x"/></div></figure><p id="db60" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">输出离散(2)意味着有两个动作。在CartPole中，0对应“向左推车”，1对应“向右推车”。请注意，在这个特定的例子中，静止不动不是一个选项。在强化学习中，代理产生一个动作输出，这个动作被发送到一个环境中，然后环境做出反应。环境会产生一个观察结果(以及奖励信号，此处未显示)，我们可以在下面看到:</p><pre class="kf kg kh ki gt ni nj nk nl aw nm bi"><span id="179f" class="nn mc iq nj b gy no np l nq nr">env.reset()</span></pre><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nh"><img src="../Images/b7801db2383abde29f3751321a3a8760.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*sd1Ks3Ku6L2QTBY9"/></div></div></figure><p id="3298" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">观察值是dim=4的向量，包含小车的x位置、小车的x速度、以弧度表示的极角(1弧度= 57.295度)以及极的角速度。上面显示的数字是开始新一集(` env.reset()`)后的初步观察。随着每个时间步长(和动作)，观察值将会改变，这取决于小车和杆子的状态。</p><h1 id="aab1" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">培训代理</h1><p id="4c23" class="pw-post-body-paragraph kr ks iq kt b ku mt jr kw kx mu ju kz la mv lc ld le mw lg lh li mx lk ll lm ij bi translated">在强化学习中，代理的目标是随着时间的推移产生越来越聪明的行为。它通过一项政策做到了这一点。在深度强化学习中，这种策略用神经网络来表示。让我们首先在没有任何神经网络或机器学习算法的情况下与健身房环境进行交互。相反，我们将从随机运动(向左或向右)开始。这只是为了理解机制。</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nh"><img src="../Images/35de6819a049c1c5f85e1bdeb9206e14.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*dbfXd_pgWn2ZK8xB"/></div></div><p class="km kn gj gh gi ko kp bd b be z dk translated">作者图片</p></figure><p id="dc79" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">下面的代码重置环境，需要20个步骤(20个周期)，总是采取随机操作并打印结果。</p><pre class="kf kg kh ki gt ni nj nk nl aw nm bi"><span id="e510" class="nn mc iq nj b gy no np l nq nr"># returns an initial observation<br/>env.reset()</span><span id="dc3c" class="nn mc iq nj b gy nt np l nq nr">for i in range(20):</span><span id="336d" class="nn mc iq nj b gy nt np l nq nr">   # env.action_space.sample() produces either 0 (left) or 1 (right).<br/>   observation,reward,done,info =env.step(env.action_space.sample())</span><span id="1134" class="nn mc iq nj b gy nt np l nq nr">   print("step", i, observation, reward, done, info)</span><span id="bdc2" class="nn mc iq nj b gy nt np l nq nr">env.close()</span></pre><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nu"><img src="../Images/7fdae1c47f93e39815ebb38fa133b6c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*m8DBHmGzZz5a_b8r"/></div></div><p class="km kn gj gh gi ko kp bd b be z dk translated">样本输出。在钢管舞中有多种终止情节的条件。在图像中，该集因超过12度(0.20944 rad)而终止。情节终止的其他条件是推车位置大于2.4(推车的中心到达显示器的边缘)，情节长度大于200，或者当100次连续试验的平均回报大于或等于195.0时的解决要求。</p></figure><p id="2e1a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">上面的打印输出显示了以下内容:</p><ul class=""><li id="6ee0" class="ln lo iq kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated">步骤(它在环境中循环了多少次)。在每个时间步中，代理选择一个动作，环境返回一个观察和一个奖励</li><li id="999b" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">环境观察[x推车位置、x推车速度、杆角度(rad)、杆角速度]</li><li id="2686" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">前一次行动获得的奖励。规模因环境而异，但目标始终是增加你的总回报。包括终止步骤在内，每一步的奖励是1。之后为0(图中的步骤18和19)。</li><li id="093f" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">done是一个布尔值。它指示是否该再次重置环境。大多数任务被划分为定义明确的情节，如果done为True，则表明该情节已经终止。在推车杆中，可能是杆倾斜过远(超过12度/0.20944弧度)，位置超过2.4表示推车的中心到达显示器的边缘，情节长度大于200，或者解决的要求是在100次连续试验中平均回报大于或等于195.0。</li><li id="c8e7" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">info是对调试有用的诊断信息。对于这种横向环境，它是空的。</li></ul><p id="78e3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">虽然这些数字是有用的输出，但视频可能会更清晰。如果您在Google Colab中运行这段代码，需要注意的是，没有可用于生成视频的显示驱动程序。但是，可以安装一个虚拟显示驱动程序来让它工作。</p><pre class="kf kg kh ki gt ni nj nk nl aw nm bi"><span id="4941" class="nn mc iq nj b gy no np l nq nr"># install dependencies needed for recording videos<br/>!apt-get install -y xvfb x11-utils<br/>!pip install pyvirtualdisplay==0.2.*</span></pre><p id="ae73" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">下一步是启动虚拟显示器的实例。</p><pre class="kf kg kh ki gt ni nj nk nl aw nm bi"><span id="eb61" class="nn mc iq nj b gy no np l nq nr">from pyvirtualdisplay import Display<br/>display = Display(visible=False, size=(1400, 900))<br/>_ = display.start()</span></pre><p id="d496" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">OpenAI gym有一个VideoRecorder包装器，可以录制MP4格式的跑步环境视频。下面的代码和前面的一样，除了它是200步，并且是录音。</p><pre class="kf kg kh ki gt ni nj nk nl aw nm bi"><span id="90b4" class="nn mc iq nj b gy no np l nq nr">from gym.wrappers.monitoring.video_recorder import VideoRecorder</span><span id="c242" class="nn mc iq nj b gy nt np l nq nr">before_training = "before_training.mp4"</span><span id="1c8d" class="nn mc iq nj b gy nt np l nq nr">video = VideoRecorder(env, before_training)</span><span id="25cf" class="nn mc iq nj b gy nt np l nq nr"># returns an initial observation<br/>env.reset()</span><span id="033f" class="nn mc iq nj b gy nt np l nq nr">for i in range(200):<br/>   env.render()<br/>   video.capture_frame()</span><span id="a3f5" class="nn mc iq nj b gy nt np l nq nr">   # env.action_space.sample() produces either 0 (left) or 1 (right).<br/>   observation, reward, done, info = env.step(env.action_space.sample())</span><span id="f5f3" class="nn mc iq nj b gy nt np l nq nr">   # Not printing this time<br/>   # print("step", i, observation, reward, done, info)</span><span id="f91e" class="nn mc iq nj b gy nt np l nq nr">video.close()<br/>env.close()</span></pre><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nh"><img src="../Images/7b9da25a9b7a4c42634bd82bd44bf45b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*PYGxPOcUGBe5JAuF"/></div></div><p class="km kn gj gh gi ko kp bd b be z dk translated">通常，当done为1(真)时，您将结束模拟。上面的代码让环境在达到终止条件后继续运行。例如，在CartPole中，这可能是杆翻倒、杆脱离屏幕或达到其他终止条件的时候。</p></figure><p id="f9f6" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">上面的代码将视频文件保存到Colab磁盘中。为了在笔记本中显示它，您需要一个助手函数。</p><pre class="kf kg kh ki gt ni nj nk nl aw nm bi"><span id="0263" class="nn mc iq nj b gy no np l nq nr"><strong class="nj ir">from</strong> base64 <strong class="nj ir">import</strong> b64encode<br/><strong class="nj ir">def</strong> <strong class="nj ir">render_mp4</strong>(videopath: str) -&gt; str:<br/>  """<br/>  Gets a string containing a b4-encoded version of the MP4 video<br/>  at the specified path.<br/>  """<br/>  mp4 = open(videopath, 'rb').read()<br/>  base64_encoded_mp4 = b64encode(mp4).decode()<br/>  <strong class="nj ir">return</strong> f'&lt;video width=400 controls&gt;&lt;source src="data:video/mp4;' \<br/>         f'base64,{base64_encoded_mp4}" type="video/mp4"&gt;&lt;/video&gt;'</span></pre><p id="3ced" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">下面的代码呈现了结果。你应该得到一个类似于下面的视频。</p><pre class="kf kg kh ki gt ni nj nk nl aw nm bi"><span id="733d" class="nn mc iq nj b gy no np l nq nr"><strong class="nj ir">from</strong> IPython.display <strong class="nj ir">import</strong> HTML<br/>html = render_mp4(before_training)<br/>HTML(html)</span></pre><figure class="kf kg kh ki gt kj"><div class="bz fp l di"><div class="nv kl l"/></div></figure><p id="6085" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">播放视频表明，随机选择一个动作并不是保持侧手翻直立的好策略。</p><h1 id="89fd" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">如何使用Ray的RLlib训练代理</h1><p id="d598" class="pw-post-body-paragraph kr ks iq kt b ku mt jr kw kx mu ju kz la mv lc ld le mw lg lh li mx lk ll lm ij bi translated">教程的前一部分让我们的代理做出随机的行为，而不考虑环境的观察和奖励。拥有一个代理的目标是随着时间的推移产生越来越聪明的行为，而随机的行为并不能实现这个目标。为了让代理人随着时间的推移做出更明智的行动，itl需要一个更好的策略。在深度强化学习中，策略用神经网络来表示。</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nw"><img src="../Images/065801460603cebc85eeb6ca442d0fc2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*zf-_RtflOSSk3Mq2"/></div></div><p class="km kn gj gh gi ko kp bd b be z dk translated">作者图片</p></figure><p id="710a" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">本教程将使用<a class="ae kq" href="https://docs.ray.io/en/master/rllib.html" rel="noopener ugc nofollow" target="_blank"> RLlib库</a>来训练一个更聪明的代理。RLlib有许多优点，例如:</p><ul class=""><li id="2b7c" class="ln lo iq kt b ku kv kx ky la lp le lq li lr lm ls lt lu lv bi translated">极度的灵活性。它允许您定制RL周期的每个方面。例如，本节教程将使用PyTorch创建一个定制的神经网络策略(RLlib也有对TensorFlow的本地支持)。</li><li id="b97f" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">可扩展性。强化学习应用程序可能是计算密集型的，并且通常需要扩展到一个集群以实现更快的训练。RLlib不仅对GPU有一流的支持，而且它还建立在<a class="ae kq" href="https://ray.io/" rel="noopener ugc nofollow" target="_blank"> Ray </a>之上，后者是<a class="ae kq" href="https://www.anyscale.com/blog/parallelizing-python-code" rel="noopener ugc nofollow" target="_blank">并行</a>和<a class="ae kq" href="https://www.anyscale.com/blog/writing-your-first-distributed-python-application-with-ray" rel="noopener ugc nofollow" target="_blank">分布式</a> Python的开源库。这使得将Python程序从笔记本电脑扩展到集群变得容易。</li><li id="523f" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">统一的API和对离线、基于模型、无模型、多代理算法等的支持(本教程不探讨这些算法)。</li><li id="60f9" class="ln lo iq kt b ku lw kx lx la ly le lz li ma lm ls lt lu lv bi translated">成为<a class="ae kq" href="https://github.com/ray-project/ray" rel="noopener ugc nofollow" target="_blank"> Ray项目生态系统</a>的一部分。这样做的一个优点是，RLlib可以与生态系统中的其他库一起运行，如<a class="ae kq" href="https://docs.ray.io/en/master/tune/index.html" rel="noopener ugc nofollow" target="_blank">Ray tuning</a>，这是一个用于实验执行和任何规模的超参数调整的库(稍后将详细介绍)。</li></ul><p id="359f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">虽然有些功能在这篇文章中不会被完全利用，但是当你想做一些更复杂的事情和解决现实世界的问题时，它们是非常有用的。你可以在这里了解一些令人印象深刻的RLlib <a class="ae kq" href="https://www.anyscale.com/blog/best-reinforcement-learning-talks-from-ray-summit-2021" rel="noopener ugc nofollow" target="_blank">用例。</a></p><p id="6766" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">要开始使用RLlib，您需要首先安装它。</p><pre class="kf kg kh ki gt ni nj nk nl aw nm bi"><span id="210e" class="nn mc iq nj b gy no np l nq nr">!pip install 'ray[rllib]'==1.6</span></pre><p id="4f1f" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在，您可以使用近似策略优化(PPO)算法来训练PyTorch模型。这是一个非常全面的，适合所有类型的算法，你可以在这里了解更多关于<a class="ae kq" href="https://docs.ray.io/en/master/rllib-algorithms.html#proximal-policy-optimization-ppo" rel="noopener ugc nofollow" target="_blank">的信息。下面的代码使用了一个由32个神经元和线性激活函数组成的单一隐藏层的神经网络。</a></p><pre class="kf kg kh ki gt ni nj nk nl aw nm bi"><span id="7459" class="nn mc iq nj b gy no np l nq nr">import ray<br/>from ray.rllib.agents.ppo import PPOTrainer</span><span id="6894" class="nn mc iq nj b gy nt np l nq nr">config = {</span><span id="0114" class="nn mc iq nj b gy nt np l nq nr">   "env": "CartPole-v0",<br/>   <br/>   # Change the following line to `“framework”: “tf”` to use tensorflow<br/>   "framework": "torch",<br/>   "model": {<br/>      "fcnet_hiddens": [32],<br/>      "fcnet_activation": "linear",<br/>   },<br/>}</span><span id="5503" class="nn mc iq nj b gy nt np l nq nr">stop = {"episode_reward_mean": 195}</span><span id="b2cf" class="nn mc iq nj b gy nt np l nq nr">ray.shutdown()</span><span id="c111" class="nn mc iq nj b gy nt np l nq nr">ray.init(<br/>   num_cpus=3,<br/>   include_dashboard=False,<br/>   ignore_reinit_error=True,<br/>   log_to_driver=False,<br/>)</span><span id="d4a7" class="nn mc iq nj b gy nt np l nq nr"># execute training<br/>analysis = ray.tune.run(<br/>   "PPO",<br/>   config=config,<br/>   stop=stop,<br/>   checkpoint_at_end=True,<br/>)</span></pre><p id="ee29" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这段代码应该会产生相当多的输出。最后一个条目应该是这样的:</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nh"><img src="../Images/976f81f344553bf403264cedcae43f42.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*NvggdP554R21VMqt"/></div></div></figure><p id="e5b4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">条目显示，解决环境问题需要35次迭代，运行时间超过258秒。每次都不一样，但是每次迭代大概需要7秒(258 / 35 = 7.3)。注意，如果你想学习Ray API，看看像ray.shutdown和ray.init这样的命令是做什么的，你可以<a class="ae kq" href="https://www.anyscale.com/blog/writing-your-first-distributed-python-application-with-ray" rel="noopener ugc nofollow" target="_blank">看看这个教程</a>。</p><h1 id="f03b" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">如何使用GPU加速训练</h1><p id="0004" class="pw-post-body-paragraph kr ks iq kt b ku mt jr kw kx mu ju kz la mv lc ld le mw lg lh li mx lk ll lm ij bi translated">虽然本教程的其余部分使用了CPU，但需要注意的是，您可以通过在Google Colab中使用GPU来加速模型训练。这可以通过选择<strong class="kt ir">运行时&gt;更改运行时类型</strong>并将硬件加速器设置为<strong class="kt ir"> GPU </strong>来完成。然后选择<strong class="kt ir">运行时&gt;重启并运行所有</strong>。</p><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nx"><img src="../Images/d63512e962d1ccf0e32c2164668fa6fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*WbPkL30nXm5RWxxX"/></div></div></figure><p id="f320" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">请注意，虽然训练迭代的次数可能大致相同，但是每次迭代的时间已经显著减少(从7秒减少到5.5秒)。</p><h1 id="d067" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">创建训练模型的视频</h1><p id="0a89" class="pw-post-body-paragraph kr ks iq kt b ku mt jr kw kx mu ju kz la mv lc ld le mw lg lh li mx lk ll lm ij bi translated">RLlib提供了一个Trainer类，它保存了一个环境交互策略。通过训练器界面，可以对策略进行训练、操作计算和检查。虽然之前从<code class="fe ny nz oa nj b">ray.tune.run</code>返回的分析对象不包含任何训练器实例，但是它拥有从保存的检查点重建一个所需的所有信息，因为<code class="fe ny nz oa nj b">checkpoint_at_end=True</code>是作为参数传递的。下面的代码显示了这一点。</p><pre class="kf kg kh ki gt ni nj nk nl aw nm bi"><span id="b10a" class="nn mc iq nj b gy no np l nq nr"># restore a trainer from the last checkpoint<br/>trial = analysis.get_best_logdir("episode_reward_mean", "max")<br/>checkpoint = analysis.get_best_checkpoint(trial,<br/>   "training_iteration",<br/>   "max",<br/>)</span><span id="ee1e" class="nn mc iq nj b gy nt np l nq nr">trainer = PPOTrainer(config=config)<br/>trainer.restore(checkpoint)</span></pre><p id="4414" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在让我们创建另一个视频，但这一次选择由经过训练的模型推荐的动作，而不是随机动作。</p><pre class="kf kg kh ki gt ni nj nk nl aw nm bi"><span id="f505" class="nn mc iq nj b gy no np l nq nr">after_training = "after_training.mp4"<br/>after_video = VideoRecorder(env, after_training)</span><span id="20d8" class="nn mc iq nj b gy nt np l nq nr">observation = env.reset()<br/>done = False<br/>while not done:<br/>   env.render()<br/>   after_video.capture_frame()<br/>   action = trainer.compute_action(observation)<br/>   observation, reward, done, info = env.step(action)<br/>after_video.close()<br/>env.close()</span><span id="a9b1" class="nn mc iq nj b gy nt np l nq nr"># You should get a video similar to the one below.<br/>html = render_mp4(after_training)<br/>HTML(html)</span></pre><figure class="kf kg kh ki gt kj"><div class="bz fp l di"><div class="kk kl l"/></div></figure><p id="8b95" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">这一次，杆子平衡得很好，这意味着代理已经解决了横竿环境！</p><h1 id="8820" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">使用光线调节进行超参数调节</h1><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nh"><img src="../Images/5b6d950e8a8c47c5907f88084da985d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*35nJ-t49baaIkIRH"/></div></div><p class="km kn gj gh gi ko kp bd b be z dk translated">射线生态系统(图片由作者提供)</p></figure><p id="fcf2" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">RLlib是一个强化学习库，是Ray生态系统的一部分。Ray是一个高度可扩展的并行和分布式python通用框架。它非常通用，这种通用性对于支持其图书馆生态系统非常重要。这个生态系统涵盖了从<a class="ae kq" href="https://docs.ray.io/en/latest/tune/index.html" rel="noopener ugc nofollow" target="_blank">培训</a>，到<a class="ae kq" href="https://docs.ray.io/en/master/serve/index.html" rel="noopener ugc nofollow" target="_blank">生产服务</a>，到<a class="ae kq" href="https://www.anyscale.com/blog/data-processing-support-in-ray" rel="noopener ugc nofollow" target="_blank">数据处理</a>等等。您可以一起使用多个库，并构建完成所有这些事情的应用程序。</p><p id="8b14" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">教程的这一部分利用了<a class="ae kq" href="https://docs.ray.io/en/latest/tune/index.html" rel="noopener ugc nofollow" target="_blank"> Ray Tune </a>，这是Ray生态系统中的另一个库。这是一个用于任何规模的实验执行和超参数调整的库。虽然本教程将只使用网格搜索，但请注意，光线调整还可以让您访问更有效的超参数调整算法，如基于群体的训练，BayesOptSearch和HyperBand/ASHA。</p><p id="77bf" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">现在让我们试着找出能够在最少的时间内解决CartPole环境的超参数。</p><p id="0dff" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">输入以下代码，并准备好它需要一段时间才能运行:</p><pre class="kf kg kh ki gt ni nj nk nl aw nm bi"><span id="5244" class="nn mc iq nj b gy no np l nq nr">parameter_search_config = {<br/>   "env": "CartPole-v0",<br/>   "framework": "torch",</span><span id="55b5" class="nn mc iq nj b gy nt np l nq nr"># Hyperparameter tuning<br/>"model": {<br/>   "fcnet_hiddens": ray.tune.grid_search([[32], [64]]),<br/>   "fcnet_activation": ray.tune.grid_search(["linear", "relu"]),<br/>   },<br/>   "lr": ray.tune.uniform(1e-7, 1e-2)<br/>}</span><span id="9cde" class="nn mc iq nj b gy nt np l nq nr"># To explicitly stop or restart Ray, use the shutdown API.<br/>ray.shutdown()</span><span id="d081" class="nn mc iq nj b gy nt np l nq nr">ray.init(<br/>   num_cpus=12,<br/>   include_dashboard=False,<br/>   ignore_reinit_error=True,<br/>   log_to_driver=False,<br/>)</span><span id="33c7" class="nn mc iq nj b gy nt np l nq nr">parameter_search_analysis = ray.tune.run(<br/>   "PPO",<br/>   config=parameter_search_config,<br/>   stop=stop,<br/>   num_samples=5,<br/>   metric="timesteps_total",<br/>   mode="min",<br/>)</span><span id="19c2" class="nn mc iq nj b gy nt np l nq nr">print(<br/>   "Best hyperparameters found:",<br/>   parameter_search_analysis.best_config,<br/>)</span></pre><p id="d0dc" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">通过向ray.init传递num_cpus=12来请求12个CPU内核，可以在三个CPU上并行运行四次试验。如果这不起作用，也许谷歌已经改变了Colab上可用的虚拟机。任何大于或等于3的值都可以。如果Colab因内存不足而出错，你可能需要做<strong class="kt ir">运行时&gt;工厂重置运行时</strong>，紧接着<strong class="kt ir">运行时&gt;运行所有</strong>。请注意，在Colab笔记本的右上角有一个区域显示RAM和磁盘的使用情况。</p><p id="bbc7" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">指定num_samples=5意味着您将获得学习率的五个随机样本。对于其中的每一个，隐藏层的大小有两个值，激活函数有两个值。因此，将有5 * 2 * 2 = 20次试验，在计算运行时，在单元的输出中显示它们的状态。</p><p id="3ff4" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">注意，Ray会打印当前的最佳配置。这包括所有已经设置的默认值，这是找到其他可以调整的参数的好地方。</p><p id="529c" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">运行此命令后，最终输出可能类似于以下输出:</p><p id="05a3" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><em class="ob"> INFO tune.py:549 —总运行时间:3658.24秒(调优循环为3657.45秒)。</em></p><p id="7d52" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><em class="ob">找到的最佳超参数:{'env': 'CartPole-v0 '，' framework': 'torch '，' model': {'fcnet_hiddens': [64]，' fcnet_activation': 'relu'}，' lr ':0.006733929096170726 }；' ' ' '</em></p><p id="0fd0" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">因此，在二十组超参数中，具有64个神经元、ReLU激活函数和大约6.7e-3的学习率的超参数表现最好。</p><h1 id="c817" class="mb mc iq bd md me mf mg mh mi mj mk ml jw mm jx mn jz mo ka mp kc mq kd mr ms bi translated">结论</h1><figure class="kf kg kh ki gt kj gh gi paragraph-image"><div role="button" tabindex="0" class="nc nd di ne bf nf"><div class="gh gi nh"><img src="../Images/637103ec2bf9b2faaf56e14aaade74bc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*IMhmPR1OD7vlkiG5"/></div></div><p class="km kn gj gh gi ko kp bd b be z dk translated">神经MMO是一个模拟大型多人在线游戏的环境——一种支持数百到数千个并发玩家的类型。你可以在这里了解Ray和RLlib如何帮助实现这个项目和其他项目的一些关键特性<a class="ae kq" href="https://www.anyscale.com/blog/best-reinforcement-learning-talks-from-ray-summit-2021" rel="noopener ugc nofollow" target="_blank">(图片由</a><a class="ae kq" href="https://twitter.com/jsuarez5341?lang=en" rel="noopener ugc nofollow" target="_blank"> Joseph Suarez </a>提供，经许可使用)</p></figure><p id="6d39" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated">本教程通过介绍强化学习术语，展示代理和环境如何交互，并通过代码和视频示例演示这些概念，说明了什么是强化学习。如果你想了解更多关于强化学习的知识，请查看斯文·米卡的<a class="ae kq" href="https://www.anyscale.com/events/2021/06/24/hands-on-reinforcement-learning-with-rays-rllib" rel="noopener ugc nofollow" target="_blank"> RLlib教程</a>。这是了解RLlib的最佳实践、多代理算法以及更多内容的好方法。如果你想了解所有关于RLlib和Ray的最新消息，可以考虑<a class="ae kq" href="https://twitter.com/raydistributed" rel="noopener ugc nofollow" target="_blank">关注twitter上的@ Ray distributed</a>和<a class="ae kq" href="https://anyscale.us5.list-manage.com/subscribe?u=524b25758d03ad7ec4f64105f&amp;id=d94e960a03" rel="noopener ugc nofollow" target="_blank">注册Ray简讯</a>。</p></div><div class="ab cl oc od hu oe" role="separator"><span class="of bw bk og oh oi"/><span class="of bw bk og oh oi"/><span class="of bw bk og oh"/></div><div class="ij ik il im in"><p id="7526" class="pw-post-body-paragraph kr ks iq kt b ku kv jr kw kx ky ju kz la lb lc ld le lf lg lh li lj lk ll lm ij bi translated"><em class="ob">原载于</em><a class="ae kq" href="https://www.anyscale.com/blog/an-introduction-to-reinforcement-learning-with-openai-gym-rllib-and-google" rel="noopener ugc nofollow" target="_blank"><em class="ob">https://www.anyscale.com</em></a><em class="ob">。</em></p></div></div>    
</body>
</html>