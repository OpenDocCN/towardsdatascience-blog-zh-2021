<html>
<head>
<title>Speed up Hugging Face Training Jobs on AWS by Up to 50% with SageMaker Training Compiler</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用SageMaker Training编译器，将AWS上的拥抱面部训练工作的速度提高50%</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/speed-up-hugging-face-training-jobs-on-aws-by-up-to-50-with-sagemaker-training-compiler-9ad2ac5b0eb?source=collection_archive---------21-----------------------#2021-12-02">https://towardsdatascience.com/speed-up-hugging-face-training-jobs-on-aws-by-up-to-50-with-sagemaker-training-compiler-9ad2ac5b0eb?source=collection_archive---------21-----------------------#2021-12-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="4e17" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">SageMaker培训编译器概述和入门指南</h2></div><p id="8ad9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">简介</strong></p><p id="d0e4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">深度神经网络每年都在稳步变大，因为硬件和算法的进步已经允许神经网络由数千亿个参数组成。尤其是通常用于自然语言处理(NLP)任务的Transformer模型，近年来使用的参数数量激增。例如，2018年提出的变压器双向编码器表示(BERT) large有超过3.4亿个参数，2021年提出的开关变压器有1.6万亿个参数。</p><p id="01ee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">尽管最近的技术进步，训练这些模型的成本和时间变得令人望而却步。在AWS，我们有代表客户进行发明的记录，以减少为最常见的深度学习框架(TensorFlow和PyTorch)运行培训工作的成本和时间。其中两项创新包括用于数据并行和模型并行的SageMaker库，它通过分布式培训技术的进步减少了特定任务的培训时间。如果你想更深入地了解我们的分布式培训技术，我强烈推荐阅读<a class="ae lb" href="https://www.amazon.science/publications/herring-rethinking-the-parameter-server-at-scale-for-the-cloud" rel="noopener ugc nofollow" target="_blank">我们关于SageMaker数据并行的出版物</a>。</p><p id="4d49" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">2021年，我们推出了深度学习模型训练的最新创新产品:<a class="ae lb" href="https://aws.amazon.com/blogs/aws/new-introducing-sagemaker-training-compiler/" rel="noopener ugc nofollow" target="_blank"> SageMaker训练编译器</a>。SageMaker Training编译器作为SageMaker SDK的一部分提供，只需对利用TensorFlow或PyTorch的现有代码进行最少的更改，就可以优化您的训练代码，以提高运行效率，消耗更少的计算和内存，将GPU驱动的训练作业速度提高50%。这一进步将使最先进的深度学习模型向更广泛的受众大众化，并为我们在AWS上使用这些框架的客户提供性能改进。</p><p id="37b6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我将带您了解SageMaker训练编译器的高级设计，并向您展示如何将SageMaker训练编译器与拥抱脸变形金刚库集成在一起。拥抱脸是一家提供易于使用的工具和库的公司，用于开发和部署最先进的开源NLP模型。</p><p id="70c1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> SageMaker培训编译器</strong></p><p id="c0f1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">编译器负责将您在高级编程语言(如Python或Java)中指定的代码转换成在硬件上执行的机器代码。在将高级编程语言翻译成机器代码的过程中，编译器设计者必须做出与这种翻译相关的决定，这种决定对所执行的编译代码的性能有影响。在AWS，我们的编译器设计人员致力于创建一个专门针对大规模深度学习训练优化的编译器，结果是SageMaker Training编译器。<br/> <br/>用TensorFlow和PyTorch构建的神经网络，以图形的形式直观、逻辑地表示出来。SageMaker Training编译器使用图形级优化，如运算符融合、内存规划和代数简化，来优化图形，以便在底层硬件上执行。此外，SageMaker Training编译器提供后端优化(内存延迟隐藏、面向循环的优化)和数据流优化(布局转换、公共子表达式消除)来有效地协调训练执行。</p><p id="705d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">实时运算符融合是SageMaker Training编译器提供的节省时间的主要技术之一，因此值得深入研究。训练深度学习模型的主要瓶颈之一是往返内存以检索和执行操作符的延迟成本。通过将多个运算符融合在一起，SageMaker训练编译器要求训练作业更少地访问内存，并且可以直接在后端设备的硬件寄存器中执行更多的运算符。与传统的操作员执行方法相比，这可以提高性能。</p><p id="101e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">高层建筑</strong></p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi lc"><img src="../Images/0d6bd1ad565e77b46123d90eef258f2d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*sqfG940gcYVIxZT8Ynfaiw.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">高级架构图显示了SageMaker Training编译器如何利用您提供的深度学习训练脚本中的深度学习模型图来编译和训练您的模型(图片由作者提供)</p></figure><p id="e437" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">SageMaker培训编译器可在<a class="ae lb" href="https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler-support.html" rel="noopener ugc nofollow" target="_blank"> select AWS深度学习容器(DLCs) </a>中直接获得。指定适当的框架和估计器参数将启用SageMaker训练编译器，如果您是<a class="ae lb" href="https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler-modify-scripts.html" rel="noopener ugc nofollow" target="_blank">带来您自己的模型</a>，可能需要对您的训练脚本进行额外的修改。SageMaker培训编译器将分析您的培训脚本，并在编译期间自动进行优化，从而使您的模型培训作业执行得更快。由训练作业产生的模型工件然后被输出到亚马逊简单存储服务(S3)。</p><p id="3135" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">使用拥抱脸变形器开始使用SageMaker训练编译器</strong></p><p id="c8ff" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">SageMaker训练编译器目前<a class="ae lb" href="https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler-support.html#training-compiler-supported-frameworks" rel="noopener ugc nofollow" target="_blank">支持PyTorch和TensorFlow </a>。为了快速上手SageMaker Training编译器，我将向您展示一种将SageMaker Training编译器与Hugging Face Trainer API和PyTorch集成的简单方法，如果您需要重构现有工作，这种方法只需要最少的代码更改。</p><p id="6feb" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">先决条件</strong></p><ul class=""><li id="fc30" class="ls lt iq kh b ki kj kl km ko lu ks lv kw lw la lx ly lz ma bi translated">如果这是你第一次使用亚马逊SageMaker，你需要<a class="ae lb" href="https://docs.aws.amazon.com/sagemaker/latest/dg/gs-set-up.html" rel="noopener ugc nofollow" target="_blank">设置SageMaker </a></li><li id="e01f" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la lx ly lz ma bi translated"><a class="ae lb" href="https://docs.aws.amazon.com/sagemaker/latest/dg/onboard-quick-start.html" rel="noopener ugc nofollow" target="_blank">配置SageMaker Studio </a>或随意修改用于<a class="ae lb" href="https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html" rel="noopener ugc nofollow" target="_blank"> SageMaker笔记本实例</a>或其他开发环境的指令</li><li id="4270" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la lx ly lz ma bi translated">确保查看SageMaker Training编译器<a class="ae lb" href="https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler-support.html#training-compiler-supported-frameworks" rel="noopener ugc nofollow" target="_blank">测试的实例类型</a>，如果必要，如果您的帐户级服务限制没有您想要的实例类型和可用的所需实例数量，请<a class="ae lb" href="https://docs.aws.amazon.com/sagemaker/latest/dg/regions-quotas.html#service-limit-increase-request-procedure" rel="noopener ugc nofollow" target="_blank">请求增加配额</a></li></ul><p id="676d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">演练</strong></p><p id="b928" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">关于使用<code class="fe mg mh mi mj b">BERT-base-cased</code>为情感分类任务执行模型调整的技术的完整示例，您可以访问我们的<a class="ae lb" href="https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker-training-compiler/huggingface/pytorch_single_gpu_single_node/bert-base-cased/bert-base-cased-single-node-single-gpu.ipynb" rel="noopener ugc nofollow" target="_blank"> AWS GitHub示例页面</a>。此示例假设使用单个p3.2xlarge实例。此外，该示例假设PyTorch v1.9.0和Transformers v.4.11.0。</p><p id="34a6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">重要提示:</strong>sage maker培训编译器已经在<a class="ae lb" href="https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler-support.html" rel="noopener ugc nofollow" target="_blank">精选型号</a>上进行了测试。虽然您可以在AWS文档页面上列出的型号之外的型号上试验SageMaker Training编译器，但是SageMaker Training编译器在其他型号上的性能尚未经过测试。在支持的模型表中也有推荐的批量大小设置。如果您偏离了测试的模型和推荐的批量设置，您需要在解锁任何训练加速之前重新调整参数。</p><p id="7a5d" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在下面的演练中，我将向您展示如何使用拥抱脸代码生成功能来快速生成用于Amazon SageMaker的训练代码。然后，我将向您介绍构建由Hugging Face生成的代码以与SageMaker Training编译器集成所需的步骤。在本演练中，我选择“文本分类”任务作为模型任务，我希望对我的预训练模型进行微调。</p><ol class=""><li id="c1ed" class="ls lt iq kh b ki kj kl km ko lu ks lv kw lw la mk ly lz ma bi translated">选择<a class="ae lb" href="https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler-support.html" rel="noopener ugc nofollow" target="_blank">我们测试过的型号</a> <strong class="kh ir"> </strong>中的一个，它也可以作为<a class="ae lb" href="https://huggingface.co/models" rel="noopener ugc nofollow" target="_blank">抱脸预训练型号</a>。</li><li id="50a5" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la mk ly lz ma bi translated">现在，让我们生成我们的启动代码。导航至所选模特的拥抱脸模特页面(例如，<a class="ae lb" href="https://huggingface.co/roberta-large" rel="noopener ugc nofollow" target="_blank">这是roberta-large的页面</a>)。</li><li id="a37a" class="ls lt iq kh b ki mb kl mc ko md ks me kw mf la mk ly lz ma bi translated">在页面顶部附近，点击“火车”&gt;“亚马逊SageMaker”。</li></ol><figure class="ld le lf lg gt lh gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/b6460070a7b23173ce0f343feac1538f.png" data-original-src="https://miro.medium.com/v2/resize:fit:850/format:webp/1*Fxcar59M_kVWEB1NXOVMbw.png"/></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">使用拥抱脸模型页面，你可以在Amazon SageMaker上轻松生成代码来训练你的模型</p></figure><p id="8d62" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">4.选择要微调模型的任务，并为配置选择AWS。</p><figure class="ld le lf lg gt lh gh gi paragraph-image"><div role="button" tabindex="0" class="li lj di lk bf ll"><div class="gh gi mm"><img src="../Images/8aaf0edf34705e6c535f7967b6a51f74.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6uObtYlPw1WkmqLSNlnh0g.png"/></div></div><p class="lo lp gj gh gi lq lr bd b be z dk translated">在本例中，我们选择“文本分类”测试，并计划在AWS上启动或作业</p></figure><p id="4783" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">5.将显示的代码复制到剪贴板，创建一个在Amazon SageMaker开发环境中运行的Jupyter笔记本，并将代码粘贴到笔记本中。</p><p id="9b30" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">6.准备您的训练数据，根据需要分成训练、验证和测试数据集，并将数据上传到S3。将S3路径存储在Jupyter笔记本中定义的变量中。在准备数据时，你可能会发现<a class="ae lb" href="https://huggingface.co/docs/datasets/quickstart.html" rel="noopener ugc nofollow" target="_blank">拥抱面部数据集库</a>很有帮助。</p><pre class="ld le lf lg gt mn mj mo mp aw mq bi"><span id="12cb" class="mr ms iq mj b gy mt mu l mv mw">training_input_path=‘s3://example_bucket/s3_prefix_example/train’<br/>test_input_path=‘s3://example_bucket/s3_prefix_example/test’</span></pre><p id="5cd8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">7.编写训练脚本，并将训练脚本存储在拥抱面部估计器的source_dir参数中指定的位置。例如，训练脚本见<a class="ae lb" href="https://github.com/huggingface/notebooks/tree/master/sagemaker/01_getting_started_pytorch" rel="noopener ugc nofollow" target="_blank">拥抱脸GitHub库</a>或参考上面的BERT基本案例。对于这个例子，你的训练脚本必须使用变形金刚库的训练器类。</p><pre class="ld le lf lg gt mn mj mo mp aw mq bi"><span id="c42b" class="mr ms iq mj b gy mt mu l mv mw">from transformers import Trainer, TrainingArguments</span><span id="45a6" class="mr ms iq mj b gy mx mu l mv mw">training_args=TrainingArguments(**kwargs) <br/>trainer=Trainer(args=training_args, **kwargs)</span></pre><p id="7d05" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">8.为了确保您充分利用SageMaker Training编译器在内存中适应更大批量的能力，请参考<a class="ae lb" href="https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler-support.html" rel="noopener ugc nofollow" target="_blank">本机的批量大小和</a>训练编译器编号的批量大小。使用获得的批量数量，更新批量数量，并根据Jupyter笔记本顶部的新批量调整您的学习速度。</p><pre class="ld le lf lg gt mn mj mo mp aw mq bi"><span id="ac8b" class="mr ms iq mj b gy mt mu l mv mw"># the original max batch size that can fit into GPU memory without compiler</span><span id="08ff" class="mr ms iq mj b gy mx mu l mv mw">batch_size_native=12<br/>learning_rate_native=float(‘5e-5’)</span><span id="c7cb" class="mr ms iq mj b gy mx mu l mv mw"># an updated max batch size that can fit into GPU memory with compiler</span><span id="fbef" class="mr ms iq mj b gy mx mu l mv mw">batch_size=64</span><span id="6ebb" class="mr ms iq mj b gy mx mu l mv mw"># update learning rate</span><span id="da33" class="mr ms iq mj b gy mx mu l mv mw">learning_rate=learning_rate_native/batch_size_native*batch_size</span></pre><p id="0006" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">9.更新您的HuggingFace estimator以指定启用SageMaker Training编译器的正确参数，目前建议禁用调试器以确保对性能没有影响，但请参考<a class="ae lb" href="https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler.html" rel="noopener ugc nofollow" target="_blank"> SageMaker Training编译器文档</a>以获取关于调试器和SageMaker Training编译器的最新指南。</p><p id="add7" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这一步中，注意我们正在实例化一个<code class="fe mg mh mi mj b">TrainingCompilerConfig</code>类的实例，并将其传递给<code class="fe mg mh mi mj b">compiler_config</code>参数。这个类使您能够为您的培训工作配置SageMaker培训编译器。TrainingCompilerConfig类接受两个布尔参数作为<code class="fe mg mh mi mj b">enabled</code>和<code class="fe mg mh mi mj b">debug</code>参数。有关这些参数的更多信息，请参见<a class="ae lb" href="https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler-enable.html" rel="noopener ugc nofollow" target="_blank">文档</a>。通过在估算器中不指定<code class="fe mg mh mi mj b">compiler_config</code>参数，您明确地禁用了SageMaker训练编译器。</p><pre class="ld le lf lg gt mn mj mo mp aw mq bi"><span id="cc94" class="mr ms iq mj b gy mt mu l mv mw">pytorch_huggingface_estimator=HuggingFace(<br/> source_dir=’./scripts’,<br/> entry_point=’train.py’,<br/> instance_count=1,<br/> instance_type=’ml.p3.2xlarge’,<br/> transformers_version=’4.11.0',<br/> pytorch_version=’1.9.0',<br/> hyperparameters=hyperparameters,<br/> compiler_config=TrainingCompilerConfig(),<br/> disable_profiler=True,<br/> debugger_hook_config=False<br/>)</span></pre><p id="38a5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">10.要启动您的训练作业，调用Estimator对象上的fit方法，传入您在步骤6中指定的数据位置。</p><pre class="ld le lf lg gt mn mj mo mp aw mq bi"><span id="2e26" class="mr ms iq mj b gy mt mu l mv mw">huggingface_estimator.fit({‘train’: training_input_path, ‘test’: test_input_path}, wait=False)</span></pre><p id="d026" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">恭喜你！现在，您已经使用新的SageMaker培训编译器启动了您的第一个培训工作。您可以从SageMaker服务控制台的“培训&gt;培训作业”下跟踪您的培训作业的进度，当培训完成时，您的模型将输出到S3，<a class="ae lb" href="https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html" rel="noopener ugc nofollow" target="_blank">准备部署</a>。要查看使用上述技术微调<code class="fe mg mh mi mj b">BERT-base-cased</code>的完整示例，您可以访问我们的<a class="ae lb" href="https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker-training-compiler/huggingface/pytorch_single_gpu_single_node/bert-base-cased/bert-base-cased-single-node-single-gpu.ipynb" rel="noopener ugc nofollow" target="_blank"> AWS GitHub示例页面</a>。</p><p id="c699" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir">结论</strong></p><p id="0d84" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇博客文章中，你了解了新的SageMaker训练编译器，以及它如何帮助你减少训练深度学习模型所需的时间。然后我们介绍了如何使用PyTorch和Hugging Face开始使用新的SageMaker培训编译器。</p><p id="5298" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我鼓励你把你在这里学到的东西应用到你自己的用例中。很可能你已经在通过使用多GPU甚至多节点的训练方法来优化性能增益了。您会很高兴地知道SageMaker培训编译器也可以适应这些用例(参见文档中的<a class="ae lb" href="https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler-enable.html" rel="noopener ugc nofollow" target="_blank">分布式培训指南</a>，并在我们的AWS GitHub示例库中找到<a class="ae lb" href="https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker-training-compiler/huggingface/pytorch_multiple_gpu_multiple_node/language-modeling-multi-gpu-multi-node.ipynb" rel="noopener ugc nofollow" target="_blank">多节点多GPU示例</a>)。您还可能有需要使用不同于拥抱脸训练器API的方法的用例，对于这些用例，请检查您如何能够<a class="ae lb" href="https://docs.aws.amazon.com/sagemaker/latest/dg/training-compiler-modify-scripts.html" rel="noopener ugc nofollow" target="_blank">轻松地调整您的训练脚本</a>以继续利用SageMaker训练编译器的性能优势。</p></div></div>    
</body>
</html>