<html>
<head>
<title>einsum - an underestimated function</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">一个被低估的函数</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/einsum-an-underestimated-function-99ca96e2942e?source=collection_archive---------24-----------------------#2021-08-16">https://towardsdatascience.com/einsum-an-underestimated-function-99ca96e2942e?source=collection_archive---------24-----------------------#2021-08-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="ce57" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">一个功能—多种可能性。如何简单明了地使用线性代数进行深度学习？</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/bd1671cf8317dac7a6b32b4c00e3a036.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*osvOUFHA9oSRjQzOymT7KA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Taton moise在<a class="ae kv" href="https://unsplash.com/s/photos/einstein?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="dfa7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">线性代数在深度学习领域发挥着基础作用。它总是关于形状，移调等。PyTorch、Numpy和Tensorflow等库为此提供了很多函数。但是您可能会忘记其中的一个或另一个，或者将一个函数与另一个库中的一个函数混淆。</p><p id="186f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">尽管阿尔伯特·爱因斯坦肯定没有这个问题，他用所谓的<a class="ae kv" href="https://en.wikipedia.org/wiki/Einstein_notation" rel="noopener ugc nofollow" target="_blank"> <em class="ls">爱因斯坦符号</em> </a>帮助我们。这个想法很简单:省略了sum字符以改善概述，取而代之的是，对出现两次以上的索引进行求和。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi lt"><img src="../Images/6ac2e991d3319a9b8fcc1fb2bce4cede.png" data-original-src="https://miro.medium.com/v2/resize:fit:876/format:webp/1*NeZ5ixHkRhH57AS5QVl92Q.png"/></div></figure><p id="c7c4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">变成了:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi lu"><img src="../Images/df29efbd2740d4375297515f8d39b4ea.png" data-original-src="https://miro.medium.com/v2/resize:fit:696/format:webp/1*0MDccMNW2mWXHmFnYFkHmg.png"/></div></div></figure><p id="e4d1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">谢谢，艾伯特🙏！</p><p id="9abb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有了<em class="ls">爱因斯坦符号</em>和<em class="ls"> einsum </em>函数，我们可以只用一个函数就用向量和矩阵进行计算:<strong class="ky ir"> <em class="ls"> torch.einsum(方程，*操作数)</em> </strong>。我将在接下来的代码中使用<a class="ae kv" href="https://pytorch.org/docs/stable/generated/torch.einsum.html?highlight=einsum#torch.einsum" rel="noopener ugc nofollow" target="_blank"> Pytorch的einsum </a>函数，但你也可以使用<a class="ae kv" href="https://numpy.org/doc/stable/reference/generated/numpy.einsum.html" rel="noopener ugc nofollow" target="_blank"> Numpy的</a>或<a class="ae kv" href="https://www.tensorflow.org/api_docs/python/tf/einsum" rel="noopener ugc nofollow" target="_blank"> Tensorflow </a>的函数——它们可以互换。我们将看到<em class="ls"> einsum </em>的不同用法，以及本机PyTorch函数。</p><p id="35cc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于这种神奇的理解来说，最重要的是指数的概念。有两种类型:</p><ul class=""><li id="bf63" class="lv lw iq ky b kz la lc ld lf lx lj ly ln lz lr ma mb mc md bi translated">自由索引-在输出中指定</li><li id="f3fa" class="lv lw iq ky b kz me lc mf lf mg lj mh ln mi lr ma mb mc md bi translated">总和指数—所有其他</li></ul><p id="2100" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们看一个简短的例子:</p><pre class="kg kh ki kj gt mj mk ml mm aw mn bi"><span id="6821" class="mo mp iq mk b gy mq mr l ms mt">torch.einsum(‘ik, kj-&gt;ij’, X, Y)</span></pre><p id="4448" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">也许你已经明白这里发生了什么:这是矩阵乘法。<em class="ls"> i </em>和<em class="ls"> j </em>是所谓的<em class="ls">自由指标，</em>，k是一个<em class="ls">求和指标。</em>后者可以定义为发生求和的指标。如果我们将矩阵乘法想象成嵌套循环，<em class="ls"> i </em>和<em class="ls"> j </em>将是外部循环，k循环将是求和循环:</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="b69c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">很简单，对吧？所以让我们开始吧！</p></div><div class="ab cl mw mx hu my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="ij ik il im in"><h2 id="57dc" class="mo mp iq bd nd ne nf dn ng nh ni dp nj lf nk nl nm lj nn no np ln nq nr ns nt bi translated">排列</h2><p id="c527" class="pw-post-body-paragraph kw kx iq ky b kz nu jr lb lc nv ju le lf nw lh li lj nx ll lm ln ny lp lq lr ij bi translated">这可能用于其他事情，但转置一个向量或一个矩阵似乎是最著名的用例。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="8e1c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们只需切换标识符——瞧。简单，即使<em class="ls"> X.T </em>也是一个精致的解决方案😉<em class="ls">。</em></p><h2 id="eabd" class="mo mp iq bd nd ne nf dn ng nh ni dp nj lf nk nl nm lj nn no np ln nq nr ns nt bi translated">总和</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="edeb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在这种情况下——简单的求和，我们不返回索引。输出是一个标量。或者准确的说，只有一个值的张量。</p><h2 id="5f64" class="mo mp iq bd nd ne nf dn ng nh ni dp nj lf nk nl nm lj nn no np ln nq nr ns nt bi translated">行列求和</h2><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="92ba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一个索引产生了差异——按行或列求和。</p><h2 id="5ae9" class="mo mp iq bd nd ne nf dn ng nh ni dp nj lf nk nl nm lj nn no np ln nq nr ns nt bi translated">逐元素乘法</h2><p id="f3ab" class="pw-post-body-paragraph kw kx iq ky b kz nu jr lb lc nv ju le lf nw lh li lj nx ll lm ln ny lp lq lr ij bi translated">Pytorch的实现非常简单——只需使用乘法运算符(<em class="ls"> * </em>)。用<em class="ls"> einsum </em>看起来怎么样？</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="e87c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这里的索引总是平均排列的。<em class="ls"> i </em>，<em class="ls"> j </em>乘以<em class="ls"> i </em>，<em class="ls"> j </em>给出一个形状相同的新矩阵。</p><h2 id="6561" class="mo mp iq bd nd ne nf dn ng nh ni dp nj lf nk nl nm lj nn no np ln nq nr ns nt bi translated">点积</h2><p id="5af2" class="pw-post-body-paragraph kw kx iq ky b kz nu jr lb lc nv ju le lf nw lh li lj nx ll lm ln ny lp lq lr ij bi translated">可能是比较有名的手术之一。也称为标量积。顾名思义，它返回一个标量。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="64d8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="ls"> einsum </em>函数没有输出索引，这意味着它返回一个标量。</p><h2 id="5db1" class="mo mp iq bd nd ne nf dn ng nh ni dp nj lf nk nl nm lj nn no np ln nq nr ns nt bi translated">外部产品</h2><p id="b781" class="pw-post-body-paragraph kw kx iq ky b kz nu jr lb lc nv ju le lf nw lh li lj nx ll lm ln ny lp lq lr ij bi translated">两个坐标向量的外积是一个矩阵。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mu mv l"/></div></figure><h2 id="e438" class="mo mp iq bd nd ne nf dn ng nh ni dp nj lf nk nl nm lj nn no np ln nq nr ns nt bi translated">矩阵向量乘法</h2><p id="a497" class="pw-post-body-paragraph kw kx iq ky b kz nu jr lb lc nv ju le lf nw lh li lj nx ll lm ln ny lp lq lr ij bi translated">要将矩阵乘以向量，矩阵的列数必须与向量的行数相同。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mu mv l"/></div></figure><p id="79be" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这是一个很好的例子，说明了<em class="ls"> einsum </em>函数如何处理两个操作:转置<em class="ls"> y </em>和乘法。</p><h2 id="bd49" class="mo mp iq bd nd ne nf dn ng nh ni dp nj lf nk nl nm lj nn no np ln nq nr ns nt bi translated">矩阵-矩阵乘法</h2><p id="e527" class="pw-post-body-paragraph kw kx iq ky b kz nu jr lb lc nv ju le lf nw lh li lj nx ll lm ln ny lp lq lr ij bi translated">深度学习中最重要的计算之一就是矩阵乘法。而且在机器学习的其他领域，也经常用到这个函数。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mu mv l"/></div></figure><h2 id="04d0" class="mo mp iq bd nd ne nf dn ng nh ni dp nj lf nk nl nm lj nn no np ln nq nr ns nt bi translated">批量矩阵乘法</h2><p id="4ac4" class="pw-post-body-paragraph kw kx iq ky b kz nu jr lb lc nv ju le lf nw lh li lj nx ll lm ln ny lp lq lr ij bi translated">最后，我们来看看批量矩阵乘法。即使Pytorch的实现简洁明了，但用一个函数来完成所有的线性代数计算还是不错的。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="mu mv l"/></div></figure></div><div class="ab cl mw mx hu my" role="separator"><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb nc"/><span class="mz bw bk na nb"/></div><div class="ij ik il im in"><p id="f34a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我希望这几个例子能让我的理解更加清晰。关于它还有很多(例如，广播)。但是现在，应该就是这样了。然后还有<a class="ae kv" href="https://github.com/arogozhnikov/einops" rel="noopener ugc nofollow" target="_blank"> <em class="ls"> einops </em> </a>。一整个图书馆都是张量运算——看看吧。下次见——再见。</p></div></div>    
</body>
</html>