<html>
<head>
<title>Why Are There So Many Tokenization Methods For Transformers?</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么变形金刚的记号化方法那么多？</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-are-there-so-many-tokenization-methods-for-transformers-a340e493b3a8?source=collection_archive---------6-----------------------#2021-07-27">https://towardsdatascience.com/why-are-there-so-many-tokenization-methods-for-transformers-a340e493b3a8?source=collection_archive---------6-----------------------#2021-07-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d379" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">通往同一个目的地的五条路线？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1107fe38415ef0d81d94497bb3d89b26.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*o94qeS5ZqWndUNRpmY6kmQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="1368" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi lu translated"><span class="l lv lw lx bm ly lz ma mb mc di"> H </span> uggingFace的变形金刚库是NLP事实上的标准——被世界各地的从业者使用，它功能强大、灵活且易于使用。它通过一个相当大(且复杂)的代码库来实现这一点，这就产生了一个问题:</p><blockquote class="md"><p id="3418" class="me mf it bd mg mh mi mj mk ml mm lt dk translated">为什么HuggingFace变形金刚里有那么多的记号化方法？</p></blockquote><p id="4540" class="pw-post-body-paragraph ky kz it la b lb mn ju ld le mo jx lg lh mp lj lk ll mq ln lo lp mr lr ls lt im bi translated">令牌化是将文本字符串编码成转换器可读的令牌ID整数的过程。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/5eabd3346a07337a8fa92a0f707fde31.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*yM9GYw49kIFf3bKp2Eg2AQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">从人类可读的文本到转换器可读的令牌id。</p></figure><p id="255e" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">给定一个字符串<code class="fe ms mt mu mv b">text</code> —我们可以使用以下任何一种方法对其进行编码:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="43d0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是五种不同的方法，我们可能会误认为它们会产生相同的结果——令牌id。然而，这些实际上是产生不同产出的不同过程。</p><p id="d246" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我们将看看这些方法之间的区别和相似之处。你也可以在这里看这篇文章的视频版本:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="my mx l"/></div></figure></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><h1 id="9754" class="ng nh it bd ni nj nk nl nm nn no np nq jz nr ka ns kc nt kd nu kf nv kg nw nx bi translated">变压器的标记化</h1><p id="7d6f" class="pw-post-body-paragraph ky kz it la b lb ny ju ld le nz jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">在深入研究这些标记化方法之间的区别之前，有必要理解一下标记化在transformer模型的上下文中意味着什么。</p><p id="9da1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在构建transformer tokenizer时，我们通常会生成两个文件，一个是<em class="od"> merges.txt </em>，另一个是<em class="od"> vocab.json </em>文件。这两者都代表了标记化过程中的一个步骤。</p><p id="acda" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们首先采用字符串格式的文本。第一个文件<em class="od"> merges.txt </em>用于将单词或单词片段翻译成<em class="od">标记</em>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/4f726273d6cec24b8a37f40e05c072c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8R59mHiKyLoAoGeqqbn3Eg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">文件<strong class="bd oe"> merges.txt </strong>将我们的纯文本转换成一个令牌列表。</p></figure><p id="fd76" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">一旦我们有了令牌，我们就通过我们的<em class="od"> vocab.json </em>文件来处理这些令牌，这只是一个从<em class="od">令牌</em>到<em class="od">令牌ID </em>的映射文件:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/74cb1b1f47024c3ddd46bb0bb68ead02.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ErJykUp_IhqQ2gtZLiLfkw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">文件将我们的令牌列表转换成令牌id列表。</p></figure><p id="1465" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在推理/训练过程中，这些令牌ID由我们各自的转换器模型中的嵌入层读取——它将令牌ID映射到该令牌的密集向量表示(想象一下我们单词的<em class="od">表示</em>的数字表示)。</p><h2 id="1b40" class="of nh it bd ni og oh dn nm oi oj dp nq lh ok ol ns ll om on nu lp oo op nw oq bi translated">其他输入张量</h2><p id="da5d" class="pw-post-body-paragraph ky kz it la b lb ny ju ld le nz jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">现在，我们刚刚解释了从人类可读语言到变压器可读令牌id的令牌化过程，但这只是变压器模型使用的几个输入张量之一。</p><p id="c002" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所需的输入张量因不同的变压器模型而异，甚至因不同的变压器用例而异，但有几个常见的例子。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/b3a3560ebbb5398734b9265e7c40f696.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R2l6-taWCJ55TIBfOamUAQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">令牌id(<strong class="bd oe">input _ IDs</strong>，左侧)和注意掩码(右侧)，我们已经为字符串<strong class="bd oe">“hello world！”构建了令牌id和注意掩码张量</strong> —添加特殊标记101、102和0(填充标记)。</p></figure><ul class=""><li id="8159" class="or os it la b lb lc le lf lh ot ll ou lp ov lt ow ox oy oz bi translated"><strong class="la iu">注意力屏蔽</strong>–注意力屏蔽张量通常是一个包含1和0的张量，其维数与我们的token IDs张量相同。只有当注意屏蔽张量在其各自的位置包含1时，我们的转换器模型才会计算记号IDs张量中的记号的注意。</li><li id="b0ae" class="or os it la b lb pa le pb lh pc ll pd lp pe lt ow ox oy oz bi translated"><strong class="la iu">段id</strong>—通常在我们的令牌id张量中有多个“句子/部分”时使用。比如在Q &amp; A中我们经常会把token IDs张量拆分成问题和上下文——每个段在段IDs张量中分别用0和1表示。</li></ul><p id="4852" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">以上介绍了转换器的标记化背后的基本要素——现在让我们看看这些标记化方法的不同之处，以及为什么我们会决定使用其中的一种。</p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><h1 id="fef9" class="ng nh it bd ni nj nk nl nm nn no np nq jz nr ka ns kc nt kd nu kf nv kg nw nx bi translated">到令牌和id</h1><p id="9e21" class="pw-post-body-paragraph ky kz it la b lb ny ju ld le nz jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">我们可以对文本进行标记的第一种方法是对单个字符串应用两种方法。</p><p id="6964" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">第一个方法<code class="fe ms mt mu mv b">tokenizer.tokenize</code>将我们的文本字符串转换成一个令牌列表。</p><p id="1232" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">构建完令牌列表后，我们可以使用<code class="fe ms mt mu mv b">tokenizer.convert_tokens_to_ids</code>方法将令牌列表转换成转换器可读的令牌id列表！</p><p id="533f" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，这里没有我们可以使用的特别有用的参数(比如自动填充/截断、添加特殊标记等)，所以我们在这里的灵活性受到限制。因此，通过这种方法进行的标记化看起来总是像这样:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="32e1" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">尽管值得注意的是，我们的其他张量如<code class="fe ms mt mu mv b">attention_mask</code>和<code class="fe ms mt mu mv b">token_type_ids</code>(段id)不能使用这种方法自动创建。</p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><h1 id="943b" class="ng nh it bd ni nj nk nl nm nn no np nq jz nr ka ns kc nt kd nu kf nv kg nw nx bi translated">编码和编码+</h1><p id="7386" class="pw-post-body-paragraph ky kz it la b lb ny ju ld le nz jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">接下来我们有<code class="fe ms mt mu mv b">encode</code>和<code class="fe ms mt mu mv b">encode_plus</code>，它们都为单个字符串执行两个标记化步骤——输出我们的标记id张量。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="9ca6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">现在，你可能已经猜到了，<code class="fe ms mt mu mv b">encode_plus</code>可以做得更多一点。</p><p id="95d2" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">其中<code class="fe ms mt mu mv b">encode</code>仅输出表征id张量，<code class="fe ms mt mu mv b">encode_plus</code>输出包含表征id张量<em class="od">和</em>的字典，附加张量例如注意力屏蔽或片段id(<code class="fe ms mt mu mv b">token_type_ids</code>)张量。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="74df" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这两种方法都带有常用的有用参数，如<code class="fe ms mt mu mv b">max_length</code>和<code class="fe ms mt mu mv b">return_tensors</code>。要将<code class="fe ms mt mu mv b">encode</code>与BERT一起应用于PyTorch，我们可以编写如下代码:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="859a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">但是我们还没有返回我们经常需要的注意力屏蔽或段id张量。</p><p id="4e83" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，为了获得我们的注意力屏蔽和段id，我们简单地为<code class="fe ms mt mu mv b">encode_plus</code>编写相同的代码:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="42c7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们返回我们需要的所有张量——令牌id、注意力屏蔽和段id。</p><h2 id="d1e7" class="of nh it bd ni og oh dn nm oi oj dp nq lh ok ol ns ll om on nu lp oo op nw oq bi translated">对于批次</h2><p id="8dd2" class="pw-post-body-paragraph ky kz it la b lb ny ju ld le nz jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">实际上，我们不会对单个字符串进行标记，而是对大量文本进行标记——为此我们可以使用<code class="fe ms mt mu mv b">batch_encode_plus</code>。</p><p id="9d17" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">像<code class="fe ms mt mu mv b">encode_plus</code>一样，encode_batch可以用来构建我们需要的所有张量——令牌id、注意力屏蔽和段id。</p><p id="be0d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">与<code class="fe ms mt mu mv b">encode_plus</code>不同，我们<em class="od">必须</em>将一个字符串列表传递给<code class="fe ms mt mu mv b">batch_encode_plus</code>，而<code class="fe ms mt mu mv b">batch_encode_plus</code>将在我们之前看到的同一个字典容器中为每个编码张量返回一个列表/批次。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="1035" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">同样，我们也可以使用之前的<code class="fe ms mt mu mv b">encode</code>和<code class="fe ms mt mu mv b">encode_plus</code>方法中使用的所有参数:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mw mx l"/></div></figure></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><h1 id="eb00" class="ng nh it bd ni nj nk nl nm nn no np nq jz nr ka ns kc nt kd nu kf nv kg nw nx bi translated">标记器</h1><p id="1c7d" class="pw-post-body-paragraph ky kz it la b lb ny ju ld le nz jx lg lh oa lj lk ll ob ln lo lp oc lr ls lt im bi translated">现在我们剩下最后一个方法，<code class="fe ms mt mu mv b">tokenizer(args)</code> —这里我们没有使用任何特定的方法，而是直接调用我们的tokenizer类。</p><p id="64dc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">让我们看看它会返回什么:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="3aa7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">与我们的<code class="fe ms mt mu mv b">encode_plus</code>方法完全相同的输出，如果我们尝试像我们的<code class="fe ms mt mu mv b">batch_encode_plus</code>方法一样使用带有字符串列表的<code class="fe ms mt mu mv b">tokenizer(args)</code>，我们将会看到:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="mw mx l"/></div></figure><p id="f1f9" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，这个最后的方法执行与<code class="fe ms mt mu mv b">encode_plus</code>和<code class="fe ms mt mu mv b">batch_encode_plus</code>方法相同的操作，通过输入数据类型决定使用哪个方法。</p><p id="51f6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当我们不确定是否需要使用<code class="fe ms mt mu mv b">encode_plus</code>或<code class="fe ms mt mu mv b">batch_encode_plus</code>时，我们可以直接使用<code class="fe ms mt mu mv b">tokenizer</code>类——或者如果我们只是喜欢较短的语法。</p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><p id="4a48" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这就是本文的全部内容，涵盖了高频变压器中可用的不同符号化方法。</p><p id="3d29" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在不同的模型记号赋予器之间，我们会看到不同的行为，但是所有这些记号赋予器都从<code class="fe ms mt mu mv b">PreTrainedTokenizer</code>类继承了它们的核心功能——包括了这些方法中的每一个！</p><p id="5a5d" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">因此，尽管许多记号赋予者是不同的——他们通常会分享我们在这里讨论的内容。</p><p id="be42" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我希望你喜欢这篇文章！如果你有任何问题，请通过<a class="ae pf" href="https://twitter.com/jamescalam" rel="noopener ugc nofollow" target="_blank"> Twitter </a>或在下面的评论中告诉我。如果你想要更多这样的内容，我也会在YouTube 上发布。</p><p id="387c" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">感谢阅读！</p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><p id="9c22" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><a class="ae pf" href="https://bit.ly/nlp-transformers" rel="noopener ugc nofollow" target="_blank">🤖带变压器的NLP课程70%的折扣</a></p></div><div class="ab cl mz na hx nb" role="separator"><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne nf"/><span class="nc bw bk nd ne"/></div><div class="im in io ip iq"><p id="9cbb" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated"><em class="od">*所有图片均由作者提供，除非另有说明</em></p></div></div>    
</body>
</html>