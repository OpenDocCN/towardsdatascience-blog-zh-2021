<html>
<head>
<title>Why Your Neural Net is Uncertain in Different Ways</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">为什么你的神经网络以不同的方式不确定</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/why-your-neural-net-is-uncertain-in-different-ways-a125ce1fa4e5?source=collection_archive---------26-----------------------#2021-05-24">https://towardsdatascience.com/why-your-neural-net-is-uncertain-in-different-ways-a125ce1fa4e5?source=collection_archive---------26-----------------------#2021-05-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="78ad" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">区分不确定性有助于我们更好地理解神经网络。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/3c5253386b42456d3de0628fcf2d885f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2bjGZAOHEUPnZZc5voXl5g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片。由<a class="ae ky" href="https://www.flaticon.com/authors/phatplus" rel="noopener ugc nofollow" target="_blank"> phatplus </a>和<a class="ae ky" href="https://www.flaticon.com/authors/smashicons" rel="noopener ugc nofollow" target="_blank"> smashicons </a>制作的图标。</p></figure><p id="4140" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi lv translated">欧洲网络充满了不确定性。在通过神经网络提供输入以检索输出之后，我们不能确定我们得到的输出是对现实的正确描述。此外，神经网络的不确定性需要分成两种不同的类型:</p><blockquote class="me mf mg"><p id="c8c9" class="kz la mh lb b lc ld ju le lf lg jx lh mi lj lk ll mj ln lo lp mk lr ls lt lu im bi translated"><strong class="lb iu"> 1。任意的不确定性:</strong><em class="it">不</em>用更多的数据解决。</p><p id="8e3c" class="kz la mh lb b lc ld ju le lf lg jx lh mi lj lk ll mj ln lo lp mk lr ls lt lu im bi translated"><strong class="lb iu"> 2。认知的不确定性:</strong>随着更多的数据变得更好。</p></blockquote><p id="9bab" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">将不确定性分成这两个独立的部分，可以更好地理解<strong class="lb iu"> <em class="mh">神经网络实际上是如何学习</em>以及学习什么</strong>。此外，处理这两种类型的不确定性需要非常不同的技术，我们稍后会看到。</p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><p id="c467" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但首先要做的是。让我们慢慢开始，用一个例子让问题更具体。</p><p id="ff86" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设我们正在编写一个医疗软件来预测病人患心脏病的风险。为了进行预测，我们使用一个神经网络，该网络将患者的数据作为输入，包括<em class="mh">年龄、身高和体重。</em>作为输出，网络产生一个百分比，例如<em class="mh"> 2 % </em>，意味着患者在未来10年内有<em class="mh"> 2 % </em>的几率患心脏病。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ms"><img src="../Images/fb943738013b24a9c46f5febf80f7adf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2ZVh8pwkAzwxtY3nwpkucw.png"/></div></div></figure><p id="11c7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不用说，我们认为我们做了所有的事情。我们有一个高质量的数据集来进行训练，将数据分成训练、验证和测试部分，并设计和评估了多个架构。结果，我们最终得到了一个神经网络，我们认为它能尽可能好地预测心脏病发作的风险。</p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><p id="e8b9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，病人彼得走了过来。我们把彼得的数据输入我们的神经网络，它会吐出40 %的心脏病发作风险！这是一个非常高的风险，彼得想知道我们的预测有多确定，这是可以理解的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/f25b310979ddd95a243bff480c18a5fb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8QynUXh9lkeTVzYO1O3YDg.png"/></div></div></figure><p id="e956" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们预测的不确定性可能有两个原因:</p><p id="5e41" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">1.与彼得有相同数据(年龄、身高、体重)的人可能有非常不同的心脏病发作风险。我们的网络输出的只是所有这些潜在风险的平均值。比平均值更现实的是可能风险的概率分布。这个分布越分散(方差越大)，我们预测的不确定性就越高。这就是所谓的<strong class="lb iu"> <em class="mh">任意不确定性。</em> </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi mt"><img src="../Images/cf428da6aa5eb88ae1db5fa62d1be6f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*jP7G2k5jpllXaIkEc55oRQ.png"/></div></div></figure><p id="3cbd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">2.不确定性的第二个潜在来源如下:也许Peter的数据相当特殊，在培训期间，我们没有遇到任何类似的数据点，或者像他这样的数据点非常少。因此，基本上输入对我们的神经网络来说是陌生的，它没有任何线索。所以它只输出40 %，因为它必须给出一些输出。这种不确定性与任意不确定性非常不同，被称为<strong class="lb iu"> <em class="mh">认知不确定性。</em> </strong></p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><p id="fd50" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">知道这两种不确定性和它们之间的区别对我们给Peter建议没有帮助。神经网络预测出40 %的风险，要么接受，要么放弃。我们没有机会弄清楚神经网络是确定无疑还是毫无头绪。</p><p id="d20b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">那么，我们如何才能建立一个神经网络来告诉我们它有多确定呢？因为随机的和认知的不确定性是如此不同，我们需要用不同的技术来解决它们。</strong></p><h1 id="604d" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">处理任意的不确定性</h1><p id="ed21" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">记住，随机不确定性是数据固有的不确定性。无论我们收集了多少训练数据，总会有年龄、身高和体重相同但心率风险不同的人。<strong class="lb iu">因此，我们改变神经网络来输出概率分布，而不是对每个输入进行单一预测。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nr"><img src="../Images/254f44ad7cf6173438e48b53510424d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*RriYl-zlJ7usdO571ok9uA.png"/></div></div></figure><p id="f5f3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们如何做到这一点？首先我们选择一种分布类型，例如正态分布<strong class="lb iu"> N(μ，σ ) </strong>。正态分布有两个参数，均值<strong class="lb iu"> </strong>，方差<strong class="lb iu"> σ </strong>。</p><p id="d326" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们的神经网络不再产生单一的心脏风险百分比，而是将它改为输出平均值<strong class="lb iu"> </strong> <em class="mh">和方差<strong class="lb iu"/>的值</em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ns"><img src="../Images/d18460d4f77770fa43dc7b7ea9d11c66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wg3_VuqEVbF370lKSD5Qhw.png"/></div></div></figure><p id="9e5f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后，以这样的方式调整损失函数，即训练网络的输出<strong class="lb iu"> </strong>和<strong class="lb iu"> σ </strong>最大化观察训练数据的可能性。本质上，我们的神经网络只预测了平均值<strong class="lb iu"> </strong>，但现在它额外预测了来自数据的方差<strong class="lb iu"/>——随机不确定性。</p><blockquote class="me mf mg"><p id="486c" class="kz la mh lb b lc ld ju le lf lg jx lh mi lj lk ll mj ln lo lp mk lr ls lt lu im bi translated"><strong class="lb iu"> <em class="it">趣闻:</em> </strong>最小化MSE损失(均方误差损失)和最大化关于分布N(μ，1)的似然性是一样的，意味着方差σ不是学习的而是固定为1。</p><p id="2c1a" class="kz la mh lb b lc ld ju le lf lg jx lh mi lj lk ll mj ln lo lp mk lr ls lt lu im bi translated">如果你想亲自尝试这些概念，TensorFlow有一个非常棒的扩展，<a class="ae ky" href="https://www.tensorflow.org/probability?hl=en" rel="noopener ugc nofollow" target="_blank"> Tensorflow Probability </a>。你可以选择你的发行版，图书馆会处理所有其他的事情。</p></blockquote><p id="c7a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">回到我们的不确定性。我们让我们的神经网络告诉我们心脏风险百分比以及它所看到的数据中的不确定性。但是如果没有足够的数据呢？回到我们的例子，如果Peter的数据是特殊的，并且<strong class="lb iu">训练数据只包含非常少的与Peter相似的数据点，那该怎么办？然后我们的网络会给我们一些随机的风险百分比和随机的方差，但是T2基本上不知道。</strong>这直观地解释了为什么任意不确定性和认知不确定性是独立的，以及为什么我们必须分别处理认知不确定性。</p><h1 id="5bf6" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">处理认知不确定性</h1><p id="7968" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">认知不确定性是<em class="mh">不确定性，由于信息不完整</em>，由于没有看到所有的数据。在大多数真实世界的场景中，我们手头永远不会有关于我们问题的所有数据。因此，一些认知上的不确定性将永远存在。尽管如此，认知的不确定性随着更多的数据而减少。</p><p id="b50d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们暂停一会儿，想想我们如何能够对我们神经网络的当前认知不确定性进行建模。我们实际上不确定什么？如果给我们更多的数据，会有什么变化？</p><blockquote class="nt"><p id="e5b6" class="nu nv it bd nw nx ny nz oa ob oc lu dk translated">答案就在我们面前。<strong class="ak">我们网络的权重。随着更多的数据，我们的权重将会改变。我们不确定自己的体重。</strong>那么，我们不使用固定的数字作为权重，而是使用适当的概率分布来模拟当前认知的不确定性，如何？</p></blockquote><p id="3c84" class="pw-post-body-paragraph kz la it lb b lc od ju le lf oe jx lh li of lk ll lm og lo lp lq oh ls lt lu im bi translated">这正是贝叶斯神经网络(BNNs)的本质。<strong class="lb iu">bnn将权重视为概率分布，而非数字。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/e6ba52cff228c900078c12663fdb35bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*W53jTKGP7caGtf8UEVMRvA.png"/></div></div></figure><p id="0922" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">他们使用贝叶斯推理用更多的数据更新这些权重概率分布(这就是这个名字的由来)。训练它们的成本更高，但除了网络的预测，我们还得到一个数字，告诉我们我们的网络在认知上有多不确定。</p><p id="a74f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想试试贝叶斯神经网络，<a class="ae ky" href="https://www.tensorflow.org/probability" rel="noopener ugc nofollow" target="_blank"> Tensorflow Probability </a>支持它们。</p><h1 id="d612" class="mu mv it bd mw mx my mz na nb nc nd ne jz nf ka ng kc nh kd ni kf nj kg nk nl bi translated">结论</h1><p id="df91" class="pw-post-body-paragraph kz la it lb b lc nm ju le lf nn jx lh li no lk ll lm np lo lp lq nq ls lt lu im bi translated">神经网络的输出总是带有不确定性。此外，神经网络的不确定性可能是由于数据中的方差(随机不确定性)或由于没有看到所有数据(认知不确定性)。这两种类型的不确定性都可以用它们自己的技术来解决和量化。</p><p id="8e92" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">医学预测，就像我们例子中的彼得的心率风险，只是了解神经网络实际上有多不确定的一个重要方面。在安全关键的环境中，人类的生命往往处于危险之中，概率深度学习技术可以使神经网络更加安全和可靠。</p></div><div class="ab cl ml mm hx mn" role="separator"><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq mr"/><span class="mo bw bk mp mq"/></div><div class="im in io ip iq"><p id="c411" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">觉得这个故事有趣？你可以在这里成为一个中等会员来支持我的写作:<a class="ae ky" href="https://medium.com/@mmsbrggr/membership" rel="noopener">medium.com/@mmsbrggr/membership</a>。你将获得所有媒体的访问权，你的部分会员费将直接支持我的写作。</p><p id="5ee3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">欢迎在<a class="ae ky" href="https://www.linkedin.com/in/marcel-moosbrugger/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上向我提出私人问题和评论。如果你喜欢这篇文章，让我告诉你我的简讯:【marcelmoos.com/newsletter<a class="ae ky" href="https://marcelmoos.com/newsletter" rel="noopener ugc nofollow" target="_blank"/>。</p><p id="6268" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你想更深入地了解概率深度学习，请查看以下资源:</p><ul class=""><li id="7cba" class="oj ok it lb b lc ld lf lg li ol lm om lq on lu oo op oq or bi translated"><a class="ae ky" href="https://blog.tensorflow.org/2019/03/regression-with-probabilistic-layers-in.html" rel="noopener ugc nofollow" target="_blank">tensor flow Probability中的概率层回归</a> — TensorFlow博客</li><li id="7d44" class="oj ok it lb b lc os lf ot li ou lm ov lq ow lu oo op oq or bi translated"><a class="ae ky" href="https://www.cs.ox.ac.uk/people/yarin.gal/website/blog_3d801aa532c1ce.html" rel="noopener ugc nofollow" target="_blank">我的深度模特不知道的……</a>—亚林·加尔</li></ul></div></div>    
</body>
</html>