<html>
<head>
<title>XGBoost: A Complete Guide to Fine-Tune and Optimize your Model</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">XGBoost:微调和优化模型的完整指南</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/xgboost-fine-tune-and-optimize-your-model-23d996fab663?source=collection_archive---------0-----------------------#2021-05-14">https://towardsdatascience.com/xgboost-fine-tune-and-optimize-your-model-23d996fab663?source=collection_archive---------0-----------------------#2021-05-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4d68" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">如何调整XGBoost超参数并增强模型的性能？</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/ca1944687a86bcfce84e09bd579d33fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EL9j_ioSeiYwRZ_Jlc1cLQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">照片由<a class="ae ky" href="https://unsplash.com/@spacex" rel="noopener ugc nofollow" target="_blank"> @spacex </a>在<a class="ae ky" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="40b6" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">XGBoost为什么这么受欢迎？</h1><p id="9cec" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">XGBoost最初是作为2014年的一个<a class="ae ky" href="https://arxiv.org/pdf/1603.02754.pdf" rel="noopener ugc nofollow" target="_blank">研究项目开始的，它已经迅速成为过去几年最受欢迎的机器学习算法之一。</a></p><p id="87cc" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">许多人认为它是最好的算法之一，并且由于它在回归和分类问题上的出色性能，在许多情况下推荐它作为首选。XGBoost因赢得大量Kaggle比赛而闻名，现在用于许多行业应用程序，甚至在机器学习平台中实现，如<a class="ae ky" rel="noopener" target="_blank" href="/super-fast-machine-learning-to-production-with-bigquery-ml-53c43b3825a3"> BigQuery ML </a>。</p><p id="1770" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果您正在阅读这篇关于XGBoost超参数优化的文章，您可能对该算法很熟悉。但是为了更好地理解我们想要调优什么，让我们回顾一下！</p><h1 id="78c7" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">第1部分:理解XBGoost</h1><p id="a19a" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">XGBoost ( <strong class="lt iu">极限梯度提升</strong>)不仅仅是一个算法。这是一个完整的<a class="ae ky" href="https://github.com/dmlc/xgboost" rel="noopener ugc nofollow" target="_blank">开源库</a>，被设计为梯度增强框架的优化实现。它侧重于速度、灵活性和模型性能。它的强大不仅仅来自于算法，还来自于所有的底层系统优化(并行化、缓存、硬件优化等等……)。</p><p id="daa3" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在大多数情况下，data scientist将XGBoost与“基于树的学习者”一起使用，这意味着您的XGBoost模型是基于决策树的。但是即使它们不太受欢迎，你也可以将XGboost与其他基础学习者一起使用，比如线性模型或Dart。因为这是目前最常见的情况，所以在本文的剩余部分，我们将集中讨论树。</p><blockquote class="ms mt mu"><p id="597a" class="lr ls mv lt b lu mn ju lw lx mo jx lz mw mp mc md mx mq mg mh my mr mk ml mm im bi translated">到那时，你可能会有更多的问题。什么是决策树？什么是助推？梯度推进有什么区别？<br/>别担心，我们会全部复述一遍！</p></blockquote><h2 id="8e1a" class="mz la it bd lb na nb dn lf nc nd dp lj ma ne nf ll me ng nh ln mi ni nj lp nk bi translated">什么是决策树和购物车？</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/beadb5a242c775407fadfccaa7028ac2.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/format:webp/1*gP9IELE2KrY9JbvrUywT7A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">推车:这个人玩电子游戏吗？—图片来自XGBoost文档</p></figure><p id="eb62" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">决策树是最简单的ML算法之一。</p><p id="7f95" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">这是一种实现只包含条件语句的算法的方法。</p><p id="1730" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">XGBoost使用一种叫做CART的决策树:分类和决策树。</p><ul class=""><li id="3e9a" class="nm nn it lt b lu mn lx mo ma no me np mi nq mm nr ns nt nu bi translated"><strong class="lt iu">分类树</strong>:目标变量是分类的，该树用于识别目标变量可能属于的“类别”。</li><li id="c917" class="nm nn it lt b lu nv lx nw ma nx me ny mi nz mm nr ns nt nu bi translated"><strong class="lt iu">回归树</strong>:目标变量是连续的，用树来预测其值。</li></ul><p id="e9ef" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">CART叶不仅包含最终决策值，还包含每个叶的实值分数，无论它们是用于分类还是回归。</p><h2 id="eaec" class="mz la it bd lb na nb dn lf nc nd dp lj ma ne nf ll me ng nh ln mi ni nj lp nk bi translated">什么是助推？</h2><p id="988c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">Boosting只是一种使用集成学习原理的方法，但是是按顺序进行的。</p><p id="f4ef" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果你不熟悉集成学习，它是一个结合来自多个底层模型的决策，并使用投票技术来确定最终预测的过程。</p><p id="7cb6" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">随机森林和Bagging是两种著名的集成学习方法。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oa"><img src="../Images/dccbbcc3345393ca4cc8544e89bcfaad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*WKiURX_QGbDtgFzeG1wwJg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用Bagging方法和多数投票策略的集成学习示例—图片由作者提供</p></figure><p id="cc4b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">Boosting是一种集成学习，它使用前一个模型的结果作为下一个模型的输入。boosting不是单独训练模型，而是按顺序训练模型，训练每个新模型来纠正以前模型的错误。在每次迭代(回合)中，正确预测的结果被赋予较低的权重，而错误预测的结果被赋予较高的权重。然后，它使用加权平均值得出最终结果。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/1cde2a17e7a8a42b63ca2358193da5cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PZd-TOxSLV_--3glkFHwxQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用加权平均策略的Boosting方法的集成学习示例—图片由作者提供</p></figure><h2 id="04ca" class="mz la it bd lb na nb dn lf nc nd dp lj ma ne nf ll me ng nh ln mi ni nj lp nk bi translated">什么是梯度增强？</h2><p id="570d" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">最后，梯度推进是一种利用<a class="ae ky" href="https://www.youtube.com/watch?v=sDv4f4s2SB8" rel="noopener ugc nofollow" target="_blank">梯度下降</a>算法将误差最小化的推进方法。简而言之，梯度下降是一种迭代优化算法，用于最小化损失函数。</p><p id="037c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">损失函数量化了我们的预测与给定数据点的实际结果之间的差距。预测越好，损失函数的输出就越低。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/29820b7e5b6037f2eb1f375cdd2ea682.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*GePvc3KUr2mNu-26.jpg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">损失函数示例:均方误差</p></figure><p id="e7c1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">当我们构建模型时，目标是最小化所有数据点的损失函数。例如，均方误差(MSE)是最常用的回归损失函数。</p><p id="b938" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">与经典增强相反，梯度增强不仅对错误预测的结果进行加权，而且还根据梯度调整这些权重——由损失函数中损失“下降最快”的方向给出。如果你想了解更多关于渐变提升的知识，可以看看<a class="ae ky" href="https://www.youtube.com/watch?v=TyvYZ26alZs" rel="noopener ugc nofollow" target="_blank">这个视频</a>。</p><p id="df33" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">正如我们在简介中所说，XGBoost是这种梯度增强方法的优化实现！</p><h2 id="75c9" class="mz la it bd lb na nb dn lf nc nd dp lj ma ne nf ll me ng nh ln mi ni nj lp nk bi translated">那么，如何使用XGBoost呢？</h2><p id="3d04" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">有两种使用XGBoost的常见方法:</p><ul class=""><li id="b8eb" class="nm nn it lt b lu mn lx mo ma no me np mi nq mm nr ns nt nu bi translated"><a class="ae ky" href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.training" rel="noopener ugc nofollow" target="_blank">学习API </a>:是使用XGBoost的基本的、底层的方式。简单而强大，它包括一个内置的交叉验证方法。</li></ul><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="353f" class="mz la it oe b gy oi oj l ok ol">import xgboost as xgb<br/> <br/>X, y = #Import your data<br/>dmatrix = xgb.DMatrix(data=x, label=y) #Learning API uses a dmatrix</span><span id="afec" class="mz la it oe b gy om oj l ok ol">params = {'objective':'reg:squarederror'}<br/>cv_results = xgb.cv(dtrain=dmatrix, <br/>                    params=params, <br/>                    nfold=10, <br/>                    metrics={'rmse'})</span><span id="3f66" class="mz la it oe b gy om oj l ok ol">print('RMSE: %.2f' % cv_results['test-rmse-mean'].min())</span></pre><ul class=""><li id="ae8e" class="nm nn it lt b lu mn lx mo ma no me np mi nq mm nr ns nt nu bi translated"><a class="ae ky" href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn" rel="noopener ugc nofollow" target="_blank"> Scikit-Learn API </a>:它是XGBoost的Scikit-Learn包装器接口。它允许以scikit-learn兼容的方式使用XGBoost，就像使用任何本机scikit-learn模型一样。</li></ul><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="87b9" class="mz la it oe b gy oi oj l ok ol">import xgboost as xgb</span><span id="5801" class="mz la it oe b gy om oj l ok ol">X, y = # Import your data</span><span id="0c1e" class="mz la it oe b gy om oj l ok ol">xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2)</span><span id="c6ae" class="mz la it oe b gy om oj l ok ol">xgbr = xgb.XGBRegressor(objective='reg:squarederror')</span><span id="167a" class="mz la it oe b gy om oj l ok ol">xgbr.fit(xtrain, ytrain)<br/> <br/>ypred = xgbr.predict(xtest)<br/>mse = mean_squared_error(ytest, ypred)<br/>print("RMSE: %.2f" % (mse**(1/2.0)))</span></pre><p id="cd04" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="mv">请注意，使用学习API时，您可以输入和访问评估指标，而使用Scikit-learn API时，您必须计算它。</em></p><h2 id="751a" class="mz la it bd lb na nb dn lf nc nd dp lj ma ne nf ll me ng nh ln mi ni nj lp nk bi translated"><strong class="ak">目标函数</strong></h2><p id="aa5f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">XGBoost在多种情况下是一个很好的选择，包括回归和分类问题。基于问题和你希望你的模型如何学习，你将选择一个不同的<a class="ae ky" href="https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters" rel="noopener ugc nofollow" target="_blank">目标函数</a>。</p><p id="5f50" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">最常用的有:</p><ul class=""><li id="fb44" class="nm nn it lt b lu mn lx mo ma no me np mi nq mm nr ns nt nu bi translated"><strong class="lt iu"> reg:squarederror </strong>:用于线性回归</li><li id="ec9b" class="nm nn it lt b lu nv lx nw ma nx me ny mi nz mm nr ns nt nu bi translated"><strong class="lt iu"> reg:logistic </strong>:用于logistic回归</li><li id="4867" class="nm nn it lt b lu nv lx nw ma nx me ny mi nz mm nr ns nt nu bi translated"><strong class="lt iu">二元:逻辑</strong>:逻辑回归——输出概率</li></ul><h1 id="aa0a" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">第2部分:超参数调优</h1><h2 id="c7fc" class="mz la it bd lb na nb dn lf nc nd dp lj ma ne nf ll me ng nh ln mi ni nj lp nk bi translated">为什么应该调整您的模型？</h2><p id="3110" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">与优化模型相比，未优化模型的性能如何？值得努力吗？在深入研究XGBoost模型调优之前，让我们强调一下为什么<strong class="lt iu">您必须调优您的模型</strong>的原因。</p><p id="e90c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">作为演示，我们将使用来自sklearn 的知名<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html" rel="noopener ugc nofollow" target="_blank">波士顿房价数据集，并尝试预测房价。</a></p><p id="65de" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在不进行超参数调整的情况下，如何执行我们的模型:</p><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="afd6" class="mz la it oe b gy oi oj l ok ol">import xgboost as xgb<br/>from sklearn.datasets import load_boston<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import mean_squared_error<br/> <br/>boston = load_boston()<br/>X, y = boston.data, boston.target<br/>dmatrix = xgb.DMatrix(data=x, label=y)</span><span id="a127" class="mz la it oe b gy om oj l ok ol">params={'objective':'reg:squarederror'}</span><span id="1e0b" class="mz la it oe b gy om oj l ok ol">cv_results = xgb.cv(dtrain=dmatrix, params=params, nfold=10, metrics={'rmse'}, as_pandas=True, seed=20)</span><span id="c34c" class="mz la it oe b gy om oj l ok ol">print('RMSE: %.2f' % cv_results['test-rmse-mean'].min())</span><span id="2fdb" class="mz la it oe b gy om oj l ok ol"><br/>## Result : RMSE: 3.38</span></pre><p id="7198" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">没有任何调整，我们得到了一个<strong class="lt iu"><em class="mv">3.38</em></strong>的RMSE。这并不坏，但是让我们看看它在几个调整过的超参数下会有怎样的表现:</p><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="2f87" class="mz la it oe b gy oi oj l ok ol">import xgboost as xgb<br/>from sklearn.datasets import load_boston<br/>from sklearn.model_selection import train_test_split<br/>from sklearn.metrics import mean_squared_error<br/> <br/>boston = load_boston()<br/>X, y = boston.data, boston.target<br/>dmatrix = xgb.DMatrix(data=x, label=y)</span><span id="ec11" class="mz la it oe b gy om oj l ok ol">params={ 'objective':'reg:squarederror',<br/>         'max_depth': 6, <br/>         'colsample_bylevel':0.5,<br/>         'learning_rate':0.01,<br/>         'random_state':20}</span><span id="c0ae" class="mz la it oe b gy om oj l ok ol">cv_results = xgb.cv(dtrain=dmatrix, params=params, nfold=10, metrics={'rmse'}, as_pandas=True, seed=20, num_boost_round=1000)</span><span id="2ee8" class="mz la it oe b gy om oj l ok ol">print('RMSE: %.2f' % cv_results['test-rmse-mean'].min())<br/></span><span id="0d5e" class="mz la it oe b gy om oj l ok ol">## Result : RMSE: 2.69</span></pre><p id="3262" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">经过一点点调整，我们现在得到了2.69  的<strong class="lt iu"> <em class="mv"> RMSE。提高了20%！我们可能会改进得更多。让我们看看如何！</em></strong></p><h2 id="e7bb" class="mz la it bd lb na nb dn lf nc nd dp lj ma ne nf ll me ng nh ln mi ni nj lp nk bi translated">深入研究XGBoost超参数</h2><p id="1f2f" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">超参数是模型外部的一种参数，在学习过程开始之前设置。它是可调的，可以直接影响模型的性能。</p><p id="bb0d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">为了找出模型的最佳超参数，您可以使用经验法则，或者我们将在本文中讨论的特定方法。</p><p id="1c80" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">在此之前，请注意，在使用XGBoost时，有几个参数可以优化。你可以在这里找到完整的列表<a class="ae ky" href="https://xgboost.readthedocs.io/en/latest/parameter.html" rel="noopener ugc nofollow" target="_blank"/>，或者在<a class="ae ky" href="https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn" rel="noopener ugc nofollow" target="_blank"> Scikit-Learn API </a>中使用的别名。</p><p id="454b" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">对于基于树的学习者，最常见的参数是:</p><ul class=""><li id="d31d" class="nm nn it lt b lu mn lx mo ma no me np mi nq mm nr ns nt nu bi translated"><strong class="lt iu"> max_depth </strong>:每棵树的最大深度。更深的树可能会提高性能，但也会增加复杂性和溢出的机会。<br/> <em class="mv">该值必须是大于0的整数。默认值为6。</em></li><li id="9a4e" class="nm nn it lt b lu nv lx nw ma nx me ny mi nz mm nr ns nt nu bi translated"><strong class="lt iu"> learning_rate </strong>:当你的模型朝着它的目标优化时，学习率决定了每次迭代的步长。低学习率使得计算更慢，并且需要更多轮次来实现与具有高学习率的模型相同的残差减少。但是它优化了达到最佳状态的机会。<br/> <em class="mv">该值必须介于0和1之间。默认值为0.3。</em></li><li id="7283" class="nm nn it lt b lu nv lx nw ma nx me ny mi nz mm nr ns nt nu bi translated"><strong class="lt iu"> n_estimators </strong>:我们集合中的树的数量。相当于助推轮数。<br/> <em class="mv">该值必须是大于0的整数。默认值为100。<br/> </em>注意:在标准库中，这被称为<strong class="lt iu"> num_boost_round </strong>。</li><li id="e921" class="nm nn it lt b lu nv lx nw ma nx me ny mi nz mm nr ns nt nu bi translated"><strong class="lt iu"> colsample_bytree </strong>:表示每棵树随机抽取的列的分数。这可能会改善过度拟合。<br/> <em class="mv">该值必须介于0和1之间。默认值为1。</em></li><li id="c16e" class="nm nn it lt b lu nv lx nw ma nx me ny mi nz mm nr ns nt nu bi translated"><strong class="lt iu">子样本</strong>:代表每棵树要抽样的观察值部分。较低的值可以防止过度拟合，但可能会导致拟合不足。<br/> <em class="mv">该值必须介于0和1之间。默认值为1。</em></li></ul><p id="5c30" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/l1-and-l2-regularization-methods-ce25e7fc831c"> <strong class="lt iu">正则化参数</strong> </a> <strong class="lt iu"> : </strong></p><ul class=""><li id="7570" class="nm nn it lt b lu mn lx mo ma no me np mi nq mm nr ns nt nu bi translated"><strong class="lt iu"> alpha </strong> (reg_alpha):权重上的L1正则化(Lasso回归)。当处理大量要素时，它可能会提高速度性能。它可以是任何整数。<em class="mv">默认为0。</em></li><li id="92ed" class="nm nn it lt b lu nv lx nw ma nx me ny mi nz mm nr ns nt nu bi translated"><strong class="lt iu">λ</strong>(reg _ lambda):权重上的L2正则化(岭回归)。这可能有助于减少过度拟合。它可以是任何整数。<em class="mv">默认为1。</em></li><li id="0aaf" class="nm nn it lt b lu nv lx nw ma nx me ny mi nz mm nr ns nt nu bi translated"><strong class="lt iu">伽马</strong>:伽马是一个伪正则化参数(拉格朗日乘数)，取决于其他参数。伽玛越高，正则化程度越高。它可以是任何整数。<em class="mv">默认为0。</em></li></ul><h1 id="832b" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">方法1:直觉和合理的价值观</h1><p id="668b" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">第一种方法是从合理的参数开始，并按部就班地进行。如果您理解了上面每个超参数的含义，您应该能够直观地设置一些值。</p><p id="2e44" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">先说合理的价值观。通常是:</p><blockquote class="ms mt mu"><p id="dc6a" class="lr ls mv lt b lu mn ju lw lx mo jx lz mw mp mc md mx mq mg mh my mr mk ml mm im bi translated"><strong class="lt iu">max _ depth</strong>:3–10<br/><strong class="lt iu">n _ estimators</strong>:100(大量观测值)到1000(少量观测值)<br/><strong class="lt iu">learning _ rate</strong>:0.01–0.3<br/><strong class="lt iu">col sample _ bytree</strong>:0.5–1<strong class="lt iu"><br/>子样本</strong>:0.6–1</p></blockquote><p id="2f3f" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然后，您可以专注于优化<strong class="lt iu"> max_depth </strong>和<strong class="lt iu"> n_estimators </strong>。</p><p id="3acc" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">然后你可以跟着<strong class="lt iu"> learning_rate </strong>一起玩，增加它来加速模型而不降低性能。如果它在不损失性能的情况下变得更快，您可以增加估计器的数量来尝试提高性能。</p><p id="6b43" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">最后，您可以使用您的正则化参数，通常从alpha和lambda开始。对于gamma，0表示没有正则化，1-5是常用值，而10+被认为非常高。</p><h1 id="412e" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">方法2:优化算法</h1><p id="058e" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">寻找最佳超参数的第二种方法是通过优化算法。由于XGBoost以Scikit-learn兼容的方式提供，您可以使用Scikit-learn的超参数优化器函数！</p><p id="26f4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">最常见的两种是网格搜索和随机搜索。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi on"><img src="../Images/f9e355608d27c96adcc9d48629459a84.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6kmkVPRX4jqNgDMKhbfg0g.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://scikit-learn.org/stable/modules/classes.html#hyper-parameter-optimizers" rel="noopener ugc nofollow" target="_blank">sci kit-学习超参数优化器</a></p></figure><h2 id="cea6" class="mz la it bd lb na nb dn lf nc nd dp lj ma ne nf ll me ng nh ln mi ni nj lp nk bi translated">网格搜索</h2><p id="5f38" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">网格搜索是对指定参数值的每种组合的彻底搜索。如果为<strong class="lt iu"> max_depth </strong>指定2个可能值，为<strong class="lt iu"> n_estimators </strong>指定3个可能值，网格搜索将迭代6个可能组合:</p><blockquote class="ms mt mu"><p id="e404" class="lr ls mv lt b lu mn ju lw lx mo jx lz mw mp mc md mx mq mg mh my mr mk ml mm im bi translated">max_depth: [3，6]，<br/> n_estimators:[100，200，300]</p><p id="d8b4" class="lr ls mv lt b lu mn ju lw lx mo jx lz mw mp mc md mx mq mg mh my mr mk ml mm im bi translated">将导致以下可能性:<br/>最大深度:3，n估计量:100 <br/>最大深度:3，n估计量:200 <br/>最大深度:3，n估计量:300 <br/>最大深度:6，n估计量:100 <br/>最大深度:6，n估计量:200 <br/>最大深度:6，n估计量:300</p></blockquote><p id="aa7d" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">让我们使用来自Scikit的<strong class="lt iu"> GridSearchCV() </strong>来学习调优我们的XGBoost模型！</p><p id="5dd4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated"><em class="mv">在下面的例子中，我们将使用Kaggle </em>  <em class="mv">上可用的</em> <a class="ae ky" href="https://www.kaggle.com/kumarajarshi/life-expectancy-who" rel="noopener ugc nofollow" target="_blank"> <em class="mv">预期寿命数据集的处理版本。</em></a></p><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="46d8" class="mz la it oe b gy oi oj l ok ol">import pandas as pd<br/>import xgboost as xgb<br/>from sklearn.model_selection import GridSearchCV</span><span id="fded" class="mz la it oe b gy om oj l ok ol">data = pd.read_csv("life_expectancy_clean.csv")</span><span id="a334" class="mz la it oe b gy om oj l ok ol">X, y = data[data.columns.tolist()[:-1]],<br/>       data[data.columns.tolist()[-1]]</span><span id="95af" class="mz la it oe b gy om oj l ok ol">params = { 'max_depth': [3,6,10],<br/>           'learning_rate': [0.01, 0.05, 0.1],<br/>           'n_estimators': [100, 500, 1000],<br/>           'colsample_bytree': [0.3, 0.7]}</span><span id="d6c9" class="mz la it oe b gy om oj l ok ol">xgbr = xgb.XGBRegressor(seed = 20)</span><span id="50cc" class="mz la it oe b gy om oj l ok ol">clf = GridSearchCV(estimator=xgbr, <br/>                   param_grid=params,<br/>                   scoring='neg_mean_squared_error', <br/>                   verbose=1)</span><span id="91da" class="mz la it oe b gy om oj l ok ol">clf.fit(X, y)</span><span id="7876" class="mz la it oe b gy om oj l ok ol">print("Best parameters:", clf.best_params_)<br/>print("Lowest RMSE: ", (-clf.best_score_)**(1/2.0))</span></pre><ul class=""><li id="73fa" class="nm nn it lt b lu mn lx mo ma no me np mi nq mm nr ns nt nu bi translated"><strong class="lt iu">估算器:</strong> GridSearchCV是<em class="mv"> sklearn.model_selection </em>的一部分，与任何scikit-learn兼容的<a class="ae ky" href="https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html" rel="noopener ugc nofollow" target="_blank">估算器</a>一起工作。我们用<em class="mv"> xgb。XGBRegressor() </em>，来自XGBoost的Scikit-learn API。</li><li id="1b4d" class="nm nn it lt b lu nv lx nw ma nx me ny mi nz mm nr ns nt nu bi translated"><strong class="lt iu"> param_grid: </strong> GridSearchCV接受一个要在输入中测试的参数列表。正如我们所说，网格搜索将测试每一个组合。</li><li id="1815" class="nm nn it lt b lu nv lx nw ma nx me ny mi nz mm nr ns nt nu bi translated"><strong class="lt iu">评分</strong>:该指标将用于评估交叉验证模型的性能。在这种情况下，<em class="mv"> neg_mean_squared_error </em>用于替代<em class="mv"> mean_squared_error </em>。由于技术原因，GridSearchCV只是使用MSE的负版本——所以它使函数可以推广到其他指标，我们的目标是更高的分数而不是更低的分数。</li><li id="fcda" class="nm nn it lt b lu nv lx nw ma nx me ny mi nz mm nr ns nt nu bi translated"><strong class="lt iu">详细:</strong>控制详细程度。越高，消息越多。</li></ul><p id="8384" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">更多可用参数，您可以在<a class="ae ky" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" rel="noopener ugc nofollow" target="_blank">文档</a>中找到。</p><p id="7bc8" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">最后，基于负值的最低RMSE<em class="mv">clf . best _ score _<br/></em>和最佳参数<em class="mv"> clf.best_params_ </em></p><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="27ce" class="mz la it oe b gy oi oj l ok ol">Best parameters: {'colsample_bytree': 0.7, 'learning_rate': 0.05, 'max_depth': 6, 'n_estimators': 500}</span></pre><h2 id="278b" class="mz la it bd lb na nb dn lf nc nd dp lj ma ne nf ll me ng nh ln mi ni nj lp nk bi translated">随机搜索</h2><p id="289c" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">随机搜索使用大范围(可能无限)的超参数值，并在这些值的组合上随机迭代指定次数。与遍历所有可能组合的网格搜索相反，随机搜索指定了遍历的次数。</p><p id="a3b4" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">如果您为<strong class="lt iu"> max_depth </strong>输入10个可能值，为<strong class="lt iu"> n_estimators </strong>输入200个可能值，并选择进行10次<strong class="lt iu">迭代</strong>:</p><blockquote class="ms mt mu"><p id="37d4" class="lr ls mv lt b lu mn ju lw lx mo jx lz mw mp mc md mx mq mg mh my mr mk ml mm im bi translated">max_depth: np.arrange(1，10，1)，<br/>n _ estimators:NP . arrange(100，400，2)</p><p id="188b" class="lr ls mv lt b lu mn ju lw lx mo jx lz mw mp mc md mx mq mg mh my mr mk ml mm im bi translated">10次迭代的随机可能性示例:<br/> 1:最大深度:1，n估计量:110 <br/> 2:最大深度:3，n估计量:222 <br/> 3:最大深度:3，n估计量:306 <br/> 4:最大深度:4，n估计量:102 <br/> 5:最大深度:1，n估计量:398 <br/> 6:最大深度:6，n估计量:290 【T38</p></blockquote><p id="d2f1" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">现在，让我们使用来自Scikit的<strong class="lt iu">randomsearccv()</strong>来学习调优我们的模型！</p><pre class="kj kk kl km gt od oe of og aw oh bi"><span id="4b77" class="mz la it oe b gy oi oj l ok ol">import pandas as pd<br/>import numpy as np<br/>import xgboost as xgb<br/>from sklearn.model_selection import RandomizedSearchCV</span><span id="fb97" class="mz la it oe b gy om oj l ok ol">data = pd.read_csv("life_expectancy_clean.csv")</span><span id="20a0" class="mz la it oe b gy om oj l ok ol">X, y = data[data.columns.tolist()[:-1]],<br/>       data[data.columns.tolist()[-1]]</span><span id="c05b" class="mz la it oe b gy om oj l ok ol">params = { 'max_depth': [3, 5, 6, 10, 15, 20],<br/>           'learning_rate': [0.01, 0.1, 0.2, 0.3],<br/>           'subsample': np.arange(0.5, 1.0, 0.1),<br/>           'colsample_bytree': np.arange(0.4, 1.0, 0.1),<br/>           'colsample_bylevel': np.arange(0.4, 1.0, 0.1),<br/>           'n_estimators': [100, 500, 1000]}</span><span id="e0f1" class="mz la it oe b gy om oj l ok ol">xgbr = xgb.XGBRegressor(seed = 20)</span><span id="5225" class="mz la it oe b gy om oj l ok ol">clf = RandomizedSearchCV(estimator=xgbr,<br/>                         param_distributions=params,<br/>                         scoring='neg_mean_squared_error',<br/>                         n_iter=25,<br/>                         verbose=1)</span><span id="44d7" class="mz la it oe b gy om oj l ok ol">clf.fit(X, y)</span><span id="1f4c" class="mz la it oe b gy om oj l ok ol">print("Best parameters:", clf.best_params_)<br/>print("Lowest RMSE: ", (-clf.best_score_)**(1/2.0))</span></pre><ul class=""><li id="c696" class="nm nn it lt b lu mn lx mo ma no me np mi nq mm nr ns nt nu bi translated">大多数<em class="mv"> RandomizedSearchCV的</em>参数与<em class="mv"> GridSearchCV的</em>相似。</li><li id="24c6" class="nm nn it lt b lu nv lx nw ma nx me ny mi nz mm nr ns nt nu bi translated"><strong class="lt iu"> n_iter: </strong>被采样的参数组合数。越高，你测试的组合就越多。它权衡了解决方案的运行时间和质量。</li></ul><p id="5a9c" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">对于<em class="mv"> GridSearchCV </em>，我们以<em class="mv"> clf.best_score_ </em>的负值为基础，用<em class="mv"> clf.best_params_ <br/> </em>和最低RMSE打印最佳参数</p><h1 id="e46a" class="kz la it bd lb lc ld le lf lg lh li lj jz lk ka ll kc lm kd ln kf lo kg lp lq bi translated">结论</h1><p id="ec39" class="pw-post-body-paragraph lr ls it lt b lu lv ju lw lx ly jx lz ma mb mc md me mf mg mh mi mj mk ml mm im bi translated">在本文中，我们解释了XGBoost如何操作，以便更好地理解如何调优它的超参数。正如我们所见，调优通常会导致模型性能的大幅提升。</p><p id="8e38" class="pw-post-body-paragraph lr ls it lt b lu mn ju lw lx mo jx lz ma mp mc md me mq mg mh mi mr mk ml mm im bi translated">使用我们的直觉来调整我们的模型有时可能就足够了。也值得尝试GridSearch和RandomSearch这样的优化算法。但是大多数时候，通过测试和直觉，结合算法和调整，你会得到更好的结果！</p><div class="oo op gp gr oq or"><a href="https://davidjmartins.medium.com/membership" rel="noopener follow" target="_blank"><div class="os ab fo"><div class="ot ab ou cl cj ov"><h2 class="bd iu gy z fp ow fr fs ox fu fw is bi translated">通过我的推荐链接加入媒体-大卫·马丁斯</h2><div class="oy l"><h3 class="bd b gy z fp ow fr fs ox fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="oz l"><p class="bd b dl z fp ow fr fs ox fu fw dk translated">davidjmartins.medium.com</p></div></div><div class="pa l"><div class="pb l pc pd pe pa pf ks or"/></div></div></a></div></div><div class="ab cl pg ph hx pi" role="separator"><span class="pj bw bk pk pl pm"/><span class="pj bw bk pk pl pm"/><span class="pj bw bk pk pl"/></div><div class="im in io ip iq"><h1 id="d9ca" class="kz la it bd lb lc pn le lf lg po li lj jz pp ka ll kc pq kd ln kf pr kg lp lq bi translated">您可能也会感兴趣…</h1><div class="oo op gp gr oq or"><a rel="noopener follow" target="_blank" href="/10-best-practices-to-write-readable-and-maintainable-sql-code-427f6bb98208"><div class="os ab fo"><div class="ot ab ou cl cj ov"><h2 class="bd iu gy z fp ow fr fs ox fu fw is bi translated">编写可读和可维护的SQL代码的10个最佳实践</h2><div class="oy l"><h3 class="bd b gy z fp ow fr fs ox fu fw dk translated">如何编写你的团队可以轻松阅读和维护的SQL查询？</h3></div><div class="oz l"><p class="bd b dl z fp ow fr fs ox fu fw dk translated">towardsdatascience.com</p></div></div><div class="pa l"><div class="ps l pc pd pe pa pf ks or"/></div></div></a></div></div></div>    
</body>
</html>