<html>
<head>
<title>Simple Implementation of OpenAI CLIP model: A Tutorial</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">OpenAI剪辑模型的简单实现:教程</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/simple-implementation-of-openai-clip-model-a-tutorial-ace6ff01d9f2?source=collection_archive---------3-----------------------#2021-04-07">https://towardsdatascience.com/simple-implementation-of-openai-clip-model-a-tutorial-ace6ff01d9f2?source=collection_archive---------3-----------------------#2021-04-07</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/c1a0bea97240334a0d40afce5da1e4aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tg7akErlMSyCLQxrMtQIYw.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">摘要剪辑模型的方法，从<a class="ae kc" href="https://arxiv.org/abs/2103.00020" rel="noopener ugc nofollow" target="_blank">学习可转移的视觉模型从自然语言监督</a>论文</p></figure><h1 id="eb86" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">介绍</h1><p id="2dad" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">2021年1月，OpenAI宣布了两个新模型:DALL-E和CLIP，这两个多模态模型以某种方式连接文本和图像。在本文中，我们将在<strong class="ld ir"> PyTorch </strong>中从头开始实现剪辑模型。OpenAI已经开源了一些与CLIP model相关的代码，但我发现它令人生畏，而且它远非简单短小。我还看到了一个很好的教程，灵感来自Keras代码示例上的剪辑模型，我将它的一些部分翻译成PyTorch，完全用我们心爱的PyTorch来构建这个教程！</p><p id="f6be" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated"><strong class="ld ir">我把所有的代码都作为笔记本放在了</strong><a class="ae kc" href="https://colab.research.google.com/drive/1hYHb0FTdKQCXZs3qCwVZnSuVGrZU2Z1w?usp=sharing" rel="noopener ugc nofollow" target="_blank"><strong class="ld ir">Google Colab</strong></a><strong class="ld ir">和</strong><a class="ae kc" href="https://www.kaggle.com/moeinshariatnia/openai-clip-simple-implementation" rel="noopener ugc nofollow" target="_blank"><strong class="ld ir">ka ggle</strong></a><strong class="ld ir">上，也放在了我的</strong><a class="ae kc" href="https://github.com/moein-shariatnia/OpenAI-CLIP" rel="noopener ugc nofollow" target="_blank"><strong class="ld ir">GitHub</strong></a><strong class="ld ir">上。</strong></p><h1 id="2c72" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">CLIP是做什么的？为什么好玩？</h1><p id="0f73" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在<a class="ae kc" href="https://arxiv.org/abs/2103.00020" rel="noopener ugc nofollow" target="_blank">从自然语言监督中学习可转移视觉模型</a>论文中，OpenAI介绍了他们的新模型，名为<strong class="ld ir"> CLIP </strong>，用于<strong class="ld ir">对比语言-图像预训练</strong>。简单来说，这个模型学习<strong class="ld ir">一个整句</strong>和它所描述的<strong class="ld ir">形象</strong>之间的关系；在某种意义上，当训练模型时，给定一个输入句子，它将能够检索对应于该句子的最相关的图像。这里重要的是，它是在完整的句子上训练的，而不是像汽车、狗等单一类别。直觉是，当对整个句子进行训练时，模型可以学习更多的东西，并在图像和文本之间找到一些模式。</p><p id="49e8" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">他们还表明，当这个模型在一个巨大的图像数据集及其相应的文本上训练时，它也可以充当分类器。我鼓励您研究这篇论文，以了解更多关于这个令人兴奋的模型及其在基准数据集上的惊人结果。仅举一个例子，用这种策略训练的剪辑模型比那些在ImageNet上训练的SOTA模型更好地分类ImageNet，ImageNet本身被优化用于唯一的分类任务！</p><p id="a39e" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">作为一个<strong class="ld ir">引子(！)</strong>，让我们看看我们将在本文中从头开始构建的最终模型有什么能力:给定一个查询，如“一个男孩用滑板跳跃”或“一个女孩从秋千上跳跃”，该模型将检索最相关的图像:</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi me"><img src="../Images/91a5bf550ce9447b4255cd040b839ece.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Xm7tCyLqE14ZHjT0hC-2oQ.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">上述文本查询的最终模型输出|作者图片</p></figure><p id="13f9" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">敬请期待:)</p><h1 id="9a44" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">入门指南</h1><p id="3f5b" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">好吧。让我们直接看它的PyTorch实现。首先，我们需要一个包含图像和一些描述它们的文本的数据集。坦白地说，网上有很多。我们将使用<strong class="ld ir"> Flickr 8k数据集</strong>(你可以使用更大的30k版本，最终模型的性能会更好)，它主要用于图像字幕任务。但是，没有限制，我们也可以用它来训练剪辑模型。</p><p id="969e" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">如果你用的是我写的<a class="ae kc" href="https://www.kaggle.com/moeinshariatnia/openai-clip-simple-implementation" rel="noopener ugc nofollow" target="_blank"><strong class="ld ir">Kaggle笔记本</strong> </a>，你不需要下载任何东西！数据已经在那里了../输入。</p><p id="edc4" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">但是如果你正在使用<a class="ae kc" href="https://colab.research.google.com/drive/1hYHb0FTdKQCXZs3qCwVZnSuVGrZU2Z1w?usp=sharing" rel="noopener ugc nofollow" target="_blank"> <strong class="ld ir"> Colab </strong> </a>或者你想在你的<strong class="ld ir">本地机器</strong>上下载它，下面的代码将下载8k(或者30k，如果你不注释最后几行)并解压它们。您需要在下面的指定字符串中输入您的Kaggle用户名和密钥(如果您还没有Kaggle帐户，只需创建一个即可！)</p><figure class="mf mg mh mi gt jr"><div class="bz fp l di"><div class="mj mk l"/></div></figure><p id="66ff" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">关于这个数据集需要注意的一点是，每个图像有5个标题。这个我以后写损失函数的时候再讲！</p><h1 id="094e" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">资料组</h1><p id="1e58" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">正如您在本文的标题图片中所看到的，我们需要对图片及其描述文本进行编码。因此，数据集需要<strong class="ld ir">返回图像和文本</strong>。当然，我们不会将原始文本输入到我们的文本编码器中！我们将使用来自<strong class="ld ir"> HuggingFace </strong>库中的<strong class="ld ir"> DistilBERT </strong>模型(它比BERT小，但性能几乎和BERT一样好)作为我们的文本编码器；因此，<strong class="ld ir">我们需要用DistilBERT tokenizer对句子</strong>(标题)进行标记，然后将标记id(input _ ids)和注意力屏蔽提供给DistilBERT。因此，数据集也需要考虑标记化。下面你可以看到数据集的代码。下面我将解释代码中发生的最重要的事情。</p><p id="b84e" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated"><em class="ml">关于</em><strong class="ld ir"><em class="ml">config</em></strong><em class="ml">和</em><strong class="ld ir"><em class="ml">CFG</em></strong><em class="ml">的一个说明:我用python脚本写代码，然后转换成Jupyter笔记本。因此，在python脚本的情况下，config是一个普通的python文件，我将所有的超参数放在其中，而在Jupyter Notebook的情况下，它是一个在Notebook的开头定义的类，用于保存所有的超参数。查看</em><a class="ae kc" href="https://github.com/moein-shariatnia/OpenAI-CLIP" rel="noopener ugc nofollow" target="_blank"><em class="ml">GitHub repo</em></a><em class="ml">或笔记本，查看所有的超参数。</em></p><figure class="mf mg mh mi gt jr"><div class="bz fp l di"><div class="mj mk l"/></div></figure><p id="6e4b" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">在__init__中，我们接收一个tokenizer对象，它实际上是一个HuggingFace tokinzer运行模型时将加载此标记器。我们将字幕填充和截断到指定的max_length。在__getitem__中，我们将首先加载一个编码的标题，这是一个带有键input_ids和attention_mask的字典，从它的值中提取张量，然后我们将加载相应的图像，转换并放大它(如果有！)然后我们把它做成张量，放在字典里，以“image”为关键字。最后，我们将带有关键字“caption”的标题的原始文本放在字典中，只是为了可视化的目的。</p><p id="9b63" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">我没有使用额外的数据扩充，但如果你想提高模型的性能，你可以添加它们。</p><h1 id="946e" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">图像编码器</h1><p id="9de9" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">图像编码器代码非常简单。我在这里使用PyTorch图像模型库(timm ),它使许多不同的图像模型从ResNets到EfficientNets等等都可用。这里我们将使用一个<strong class="ld ir"> ResNet50 </strong>作为我们的图像编码器。如果不想安装新的库，可以很容易地使用torchvision库来使用ResNets。</p><figure class="mf mg mh mi gt jr"><div class="bz fp l di"><div class="mj mk l"/></div></figure><p id="96b7" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">代码将每个图像编码为一个固定大小的矢量，该矢量具有模型输出通道的大小(在ResNet50的情况下，矢量大小将是<strong class="ld ir"> 2048 </strong>)。这是nn之后的输出。AdaptiveAvgPool2d()层。</p><h1 id="4b60" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">文本编码器</h1><p id="6604" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">正如我之前提到的，我将使用DistilBERT作为文本编码器。像它的哥哥伯特一样，两个特殊的标记将被添加到实际的输入标记中:<strong class="ld ir"> CLS </strong>和<strong class="ld ir"> SEP </strong>，它们标记一个句子的开始和结束。为了获得一个句子的完整表示(正如相关的BERT和DistilBERT论文所指出的)，我们使用CLS令牌的最终表示，我们希望这个表示能够捕捉到句子的整体含义(标题)。以这种方式思考，它类似于我们对图像所做的，并将它们转换成固定大小的向量。</p><figure class="mf mg mh mi gt jr"><div class="bz fp l di"><div class="mj mk l"/></div></figure><p id="ad45" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">在DistilBERT(还有BERT)的情况下，每个令牌的输出隐藏表示是大小为<strong class="ld ir"> 768 </strong>的向量。因此，整个字幕将被编码在大小为768的CLS令牌表示中。</p><h1 id="b114" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">投影头</h1><p id="44de" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我用投影头的<a class="ae kc" href="https://keras.io/examples/nlp/nl_image_search/" rel="noopener ugc nofollow" target="_blank"> Keras代码示例实现</a>用PyTorch写了以下内容。</p><p id="2bbb" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">既然我们已经将图像和文本编码成<strong class="ld ir">固定大小的矢量</strong>(图像2048，文本768)<strong class="ld ir">我们需要将它们(投影)到一个新的世界</strong>(!)具有相似的尺寸，以便能够对它们进行比较，并推开不相关的图像和文本，将匹配的图像和文本放在一起。因此，下面的代码将把2048和768维向量带入一个256 (projection_dim)维的世界，在那里我们可以比较它们:</p><figure class="mf mg mh mi gt jr"><div class="bz fp l di"><div class="mj mk l"/></div></figure><p id="d19d" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">“embedding_dim”是输入向量的大小(对于图像是2048，对于文本是768 ),而“projection_dim”是输出向量的大小，在我们的例子中是256。要了解这部分的细节，您可以参考<a class="ae kc" href="https://arxiv.org/abs/2103.00020" rel="noopener ugc nofollow" target="_blank">回形针</a>。</p><h1 id="4053" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">剪辑模型</h1><p id="5ed0" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">这部分是所有乐趣发生的地方！这里我也讲一下损失函数。为了撰写这一部分，我将<a class="ae kc" href="https://keras.io/examples/nlp/nl_image_search/" rel="noopener ugc nofollow" target="_blank"> Keras代码示例</a>中的一些代码翻译成PyTorch。看一下代码，然后阅读代码块下面的解释。</p><figure class="mf mg mh mi gt jr"><div class="bz fp l di"><div class="mj mk l"/></div></figure><p id="a05c" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">这里，我们将使用之前构建的模块来实现主模型。__init__函数是不言自明的。在正向函数中，我们首先<strong class="ld ir">将</strong>图像和文本分别编码成固定大小的向量(维度不同)。之后，使用独立的投影模块，我们<strong class="ld ir">将它们投影到我之前提到的共享世界(空间)。在这里，编码将变成类似的形状(在我们的例子中是256)。之后，我们将计算损失。我再次建议阅读回形针，以获得更好的效果，但我会尽力解释这一部分。</strong></p><p id="0471" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">在<strong class="ld ir">线性代数</strong>中，衡量两个向量是否具有相似特征(它们彼此相似)的一个常用方法是计算它们的<strong class="ld ir">点积</strong>(将匹配条目相乘并取其和)；如果最后的数字很大，他们是一样的，如果它很小，他们不是(相对而言)！</p><p id="fc61" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">好吧！我刚才说的是理解这个损失函数最重要的事情。我们继续。我们讨论了两个向量，但是，这里有什么呢？我们有image_embeddings，一个带有形状的矩阵(batch_size，256)和带有形状的text _ embeddings(batch _ size，256)。很简单！这意味着我们有两组向量，而不是两个单独的向量。我们如何衡量两组向量(两个矩阵)彼此的相似程度？同样，使用点积(在这种情况下，PyTorch中的@ operator执行点积或矩阵乘法)。为了能将这两个矩阵相乘，我们<strong class="ld ir">转置</strong>第二个矩阵。好的，我们得到一个形状为(batch_size，batch_size)的矩阵，我们称之为<strong class="ld ir"> logits </strong>。(在我们的例子中，温度等于1.0，所以，没有区别。你可以玩玩它，看看它有什么不同。还要看看纸，看看为什么会在这里！).</p><p id="18aa" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">希望你还和我在一起！如果没有也没关系，只需检查代码并检查它们的形状。现在我们有了逻辑，我们需要目标。我需要说的是，有一种更直接的方法可以获得目标，但是对于我们的情况，我必须这样做(我将在下一段中讨论为什么)。</p><p id="75fa" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">让我们考虑我们希望这个模型学习什么:<strong class="ld ir">我们希望它学习给定图像和描述它的标题的“相似表示(向量)”。这意味着我们要么给它一个图像，要么给它描述它的文本，我们希望它为两者产生相同的256大小的向量。</strong></p><p id="b5c4" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">因此，在最好的情况下，text _ embeddings和image_embedding矩阵应该是相同的，因为它们描述的是相似的东西。现在让我们想想:如果发生这种情况，罗吉斯矩阵会是什么样的？我们用一个简单的例子来看！</p><figure class="mf mg mh mi gt jr"><div class="bz fp l di"><div class="mj mk l"/></div></figure><p id="0e75" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">所以logits，在最好的情况下，将是一个矩阵，如果我们取它的softmax，将在对角线上有1.0(一个用花哨的词来称呼它的单位矩阵！).因为损失函数的工作是使模型的预测与目标相似(至少在大多数情况下！)，我们要这样一个矩阵作为我们的目标。这就是为什么我们在上面的代码块中计算图像相似性和文本相似性矩阵的原因。</p><p id="82bd" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">现在我们已经得到了目标矩阵，我们将使用简单的<strong class="ld ir">交叉熵</strong>来计算实际的<strong class="ld ir">损失</strong>。我写了<strong class="ld ir">交叉熵的完整矩阵形式</strong>作为一个函数，你可以在代码块的底部看到。好吧！我们完了！这不是很简单吗？！好吧，你可以忽略下一段，但如果你好奇，有一个重要的注意事项。</p><p id="8042" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated"><strong class="ld ir">这就是为什么我没有使用更简单的方法</strong>:我必须承认在PyTorch中有一种更简单的方法来计算这个损失；这样做:<strong class="ld ir"> nn。CrossEntropyLoss()(logits，torch.arange(batch_size)) </strong>。为什么我没有在这里使用它？有两个原因。1-我们正在使用的数据集对于单个图像具有多个标题；因此，有可能一批中存在两个标题相似的相同图像(这种情况很少见，但也有可能发生)。用这种更简单的方法来承担损失将会忽略这种可能性，并且模型学会将实际上相同的两个表示(假设它们不同)分开。显然，我们不希望这种情况发生，所以我计算了整个目标矩阵，考虑到了这些边缘情况。2-按照我的方式做，让我更好地理解了这个损失函数中发生了什么；所以，我想这会给你一个更好的直觉！</p><h1 id="fce1" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">火车</h1><p id="4475" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">这里有一个方便的函数来训练我们的模型。这里没发生什么事情。只是加载批处理，将它们提供给模型，并步进优化器和lr_scheduler。</p><figure class="mf mg mh mi gt jr"><div class="bz fp l di"><div class="mj mk l"/></div></figure><p id="5a60" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">你可以在<a class="ae kc" href="https://colab.research.google.com/drive/1hYHb0FTdKQCXZs3qCwVZnSuVGrZU2Z1w?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Colab </a>和<a class="ae kc" href="https://www.kaggle.com/moeinshariatnia/openai-clip-simple-implementation" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>笔记本或者我的<a class="ae kc" href="https://github.com/moein-shariatnia/OpenAI-CLIP" rel="noopener ugc nofollow" target="_blank"> GitHub repo中找到更多的实用函数和类(比如AvgMeter和get_lr)。</a></p><p id="03bb" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">好吧！我们已经完成了模型的训练。现在，我们需要进行推理，在我们的例子中，将为模型提供一段文本，并希望它从一个看不见的验证(或测试)集中检索最相关的图像。</p><h1 id="0d5c" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">获取图像嵌入</h1><p id="2ac1" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">在这个函数中，我们加载训练后保存的模型，在验证集中输入图像，并返回带有形状(valid_set_size，256)和模型本身的image _ embeddings。</p><figure class="mf mg mh mi gt jr"><div class="bz fp l di"><div class="mj mk l"/></div></figure><h1 id="40de" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">查找匹配项</h1><p id="1230" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">这个函数完成了我们希望我们的模型能够完成的最后一项任务:它获取模型、image _ embeddings和一个文本查询。它将显示验证集中最相关的图像！是不是很神奇？看看它到底表现如何！</p><figure class="mf mg mh mi gt jr"><div class="bz fp l di"><div class="mj mk l"/></div></figure><p id="5ef0" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">来看几个例子吧！在这一点上，当我看到输出时，我高兴地尖叫起来，并震惊地发现模型实际上正在学习图像和文本之间的关系！这种感觉简直难以置信。</p><figure class="mf mg mh mi gt jr"><div class="bz fp l di"><div class="mj mk l"/></div></figure><p id="924f" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">这是我们如何使用这个功能。Aaaannnndddd添加结果:</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/69369799af79ba845718dabd122a4466.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*ZW8Pt9riBlZDOclZbYtxhA.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="cdb5" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">我当时就想:哇！这个模特知道点什么！当然，它并不完美，因为在一些照片中有两只狗，但考虑到小的训练集和短的训练时间，我认为它很棒！</p><p id="8022" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">让我们看看其他一些输出。查询写在每个图像的顶部。</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/3628dbd7260fc4509d87987a833ef450.png" data-original-src="https://miro.medium.com/v2/resize:fit:1152/format:webp/1*3wfnOxh8JHRQAH25kWiAIg.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="fd56" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">看吧！它还会数数！把这个和前一个比较一下。该模型知道“two”的含义，并带来了包含两只狗的图像，与前面的查询形成了对比！这时我从第二次震惊中尖叫了出来:)</p><p id="bc67" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">我们从文章开始的输出:</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mn"><img src="../Images/a89f637e8fc323226a3597c1fc07d4e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aB-Mdtm6YAAZBFPT3NwBPg.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi me"><img src="../Images/dd30f9f9495c150ed00fd8b46971913f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*U_iZg-VDs43MQN4qnYzEIQ.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="2e7d" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated">对于下一个，这个模型犯了一些错误，但总体来说，它显然对文本和图像都有很好的理解。</p><figure class="mf mg mh mi gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi me"><img src="../Images/182ce97ea421192dd785a0f4da418c8e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dSvQQv6WRN8TKuObKiZAoA.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><h1 id="1a5c" class="kd ke iq bd kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la bi translated">最后的话</h1><p id="2b6f" class="pw-post-body-paragraph lb lc iq ld b le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly ij bi translated">我希望你喜欢这篇文章。实现这篇论文对我来说是一次非常有趣的经历。我要感谢<a class="ae kc" href="https://www.linkedin.com/in/khalid-salama-24403144/" rel="noopener ugc nofollow" target="_blank"> Khalid Salama </a>提供的伟大的Keras代码示例，它启发了我用PyTorch编写类似的代码。</p><p id="7bf4" class="pw-post-body-paragraph lb lc iq ld b le lz lg lh li ma lk ll lm mb lo lp lq mc ls lt lu md lw lx ly ij bi translated"><strong class="ld ir">如文中所述，所有代码和结果均可在我的</strong><a class="ae kc" href="https://github.com/moein-shariatnia/OpenAI-CLIP" rel="noopener ugc nofollow" target="_blank"><strong class="ld ir">GitHub repo</strong></a><strong class="ld ir">中获得，也可作为Jupyter笔记本上的</strong><a class="ae kc" href="https://www.kaggle.com/moeinshariatnia/openai-clip-simple-implementation" rel="noopener ugc nofollow" target="_blank"><strong class="ld ir">Kaggle</strong></a><strong class="ld ir">和</strong><a class="ae kc" href="https://colab.research.google.com/drive/1hYHb0FTdKQCXZs3qCwVZnSuVGrZU2Z1w?usp=sharing" rel="noopener ugc nofollow" target="_blank"><strong class="ld ir">Colab</strong></a><strong class="ld ir">。</strong></p></div></div>    
</body>
</html>