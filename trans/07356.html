<html>
<head>
<title>Multiclass classification Neural Network implementation using Numpy’s einsum</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">用Numpy的einsum实现多类分类神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/multiclass-classification-neural-network-implementation-using-numpys-einsum-3675a7e1e703?source=collection_archive---------31-----------------------#2021-07-04">https://towardsdatascience.com/multiclass-classification-neural-network-implementation-using-numpys-einsum-3675a7e1e703?source=collection_archive---------31-----------------------#2021-07-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="23ee" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">用Numpy的einsum实现两层多类分类神经网络。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/b53c1cfe7ef3e45df55bce434bb6c22c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*LEOn9d2QjSfdptTZeCFH6w.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">鸢尾花——照片由<a class="ae kv" href="https://unsplash.com/@frozenmoments" rel="noopener ugc nofollow" target="_blank">希拉·斯威兹</a>从<a class="ae kv" href="https://unsplash.com/" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄</p></figure><p id="18ca" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们都通过实现一个小的分类或回归问题来开始我们的机器学习之旅。今天我将使用pure numpy实现一个多类分类神经网络。我将使用虹膜数据集。下面是我在colab笔记本上的代码的链接。</p><div class="ls lt gp gr lu lv"><a href="https://colab.research.google.com/drive/1RacUOXLadYAHfq_VSLy8NDcMui_APlPI?usp=sharing" rel="noopener  ugc nofollow" target="_blank"><div class="lw ab fo"><div class="lx ab ly cl cj lz"><h2 class="bd ir gy z fp ma fr fs mb fu fw ip bi translated">谷歌联合实验室</h2><div class="mc l"><p class="bd b dl z fp ma fr fs mb fu fw dk translated">多类分类神经网络</p></div></div><div class="md l"><div class="me l mf mg mh md mi kp lv"/></div></div></a></div><p id="fe46" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我将创建一个2层神经网络。第一个隐藏层将使用<code class="fe mj mk ml mm b">leaky_relu</code>激活函数，第二个隐藏层将使用<code class="fe mj mk ml mm b">softmax</code>激活函数。</p><blockquote class="mn mo mp"><p id="c85e" class="kw kx mq ky b kz la jr lb lc ld ju le mr lg lh li ms lk ll lm mt lo lp lq lr ij bi translated">Softmax函数返回属于每个类的概率。</p></blockquote><p id="177a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><code class="fe mj mk ml mm b">softmax</code>的导数实现起来有点棘手，因为它涉及到创建一个雅可比矩阵。出于所有实际目的，<code class="fe mj mk ml mm b">softmax</code>将始终用于多类分类神经网络的最外层，因此计算其导数并不重要，只要我们能够获得成本函数J相对于logits z的导数即可。与计算softmax函数本身的导数相比，计算该导数相当简单。我在下图中展示了导数:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mu"><img src="../Images/229d78576f3dd03f2b9b7f26329d18da.png" data-original-src="https://miro.medium.com/v2/resize:fit:690/format:webp/1*OVG--ZJPSrDLEuG5D63-AA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">最外层中成本函数J w r t logit z的偏导数。</p></figure><p id="d5c5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如果你还执着于得到softmax的导数，它的实现就写在下图所示的<code class="fe mj mk ml mm b">softmax_prime</code>函数中。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mv"><img src="../Images/fb72b3c1b2c59e9e0efed9949c2381ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:1136/format:webp/1*nDtwypXHzpMD-7ttvJfSIw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">实现神经网络的效用函数(图片归作者所有)</p></figure><blockquote class="mn mo mp"><p id="5750" class="kw kx mq ky b kz la jr lb lc ld ju le mr lg lh li ms lk ll lm mt lo lp lq lr ij bi translated">如上图所示，特别注意<em class="iq"> softmax </em>和<em class="iq">二进制化</em>功能。</p></blockquote><p id="3777" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在让我们来理解<code class="fe mj mk ml mm b">binarize</code>功能。它转换标签0，1，2等。一个热编码数组，比如:</p><pre class="kg kh ki kj gt mw mm mx my aw mz bi"><span id="535b" class="na nb iq mm b gy nc nd l ne nf">+=======+===========+<br/>| Label | Binarize |<br/>+=======+===========+<br/>| 0      | [1,0,0,0] |<br/>+ — — — -+ — — — — — -+<br/>| 1      | [0,1,0,0] |<br/>+ — — — -+ — — — — — -+<br/>| 2      | [0,0,1,0] |<br/>+ — — — -+ — — — — — -+<br/>| 3      | [0,0,0,1] |<br/>+ — — — -+ — — — — — -+</span></pre><p id="5271" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这很重要，因为在多类分类中，目标将属于数据中存在的多个类之一。此外，最终的softmax图层将返回记录属于这些分类标签的概率。因此，将类标签转换成一个热编码记录是很重要的。在实现中，<code class="fe mj mk ml mm b">binarize</code>函数巧妙的利用了numpy的广播概念。</p></div><div class="ab cl ng nh hu ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="ij ik il im in"><h1 id="4a77" class="nn nb iq bd no np nq nr ns nt nu nv nw jw nx jx ny jz nz ka oa kc ob kd oc od bi translated">我们现在可以看代码了吗？</h1><blockquote class="oe"><p id="0fc6" class="of og iq bd oh oi oj ok ol om on lr dk translated">我在开始时使用了numpy的可怕的、臭名昭著的步进函数来创建数组的滑动窗口视图，该视图包含每层中的单元数。使用这个函数的风险更大，因为它在创建视图时不检查给定数组的边界，这很容易导致缓冲区溢出。尽管使用起来风险更大，但我还是用了，因为为什么不呢。在灵巧的手中，它就像一把瑞士军刀。但愿tensorflow有类似<code class="fe mj mk ml mm b">as_strided</code>函数的等价东西。</p></blockquote><pre class="oo op oq or os mw mm mx my aw mz bi"><span id="f800" class="na nb iq mm b gy nc nd l ne nf">def murals_batch(units_count, x, y, lr, epochs, bias=False, _seed=42):<br/>    batch_size, ni = x.shape[-2:]<br/>    units_count.insert(0,ni)<br/>    units_count_arr = np.array(units_count)<br/>    L, = units_count_arr.shape # Number of layers + 1<br/>    # RED ALERT - `as_strided` function is like a LAND-MINE ready to explode in wrong hands!<br/>    arr_view = as_strided(units_count_arr, shape=(L-1,2), strides=(4,4))<br/>#     print(arr_view)<br/>    rng = np.random.default_rng(seed=_seed)<br/>    wghts = [None]*(L-1)<br/>    intercepts = [None]*(L-1)<br/>    # WEIGHTS INITIALIZATION<br/>    for i in range(L-1):<br/>        w_cols, w_rows = arr_view[i,:]<br/>        wghts[i] = rng.random((w_rows, w_cols))<br/>        if bias:<br/>            intercepts[i] = rng.random((w_rows,))<br/>    costs = np.zeros(epochs)<br/>    # Gradient Descent<br/>    for epoch in range(epochs):<br/>        # FORWARD PROPAGATION<br/>        # hidden layer 1 implementation, relu activation   <br/>        h1a = np.einsum('hi,Bi -&gt; Bh', wghts[0], x)<br/>        if bias:<br/>            h1a = h1a + intercepts[0]<br/>        h1 = relu(h1a)<br/>        # hidden layer 2 implementation, softmax activation<br/>        h2a = np.einsum('ho,Bo -&gt; Bh', wghts[1], h1) <br/>        if bias:<br/>            h2a = h2a + intercepts[1]<br/>        hyp = softmax(h2a, _axis=1)<br/>        current_epoch_cost = -np.einsum('Bi,Bi', y, np.log(hyp))/batch_size<br/>#         print(current_epoch_cost)<br/>        costs[epoch] = current_epoch_cost<br/>        # BACKWARD PROPAGATION<br/>        # layer 2<br/>        dJ_dH2a = hyp - y<br/>        dJ_dW1 = np.einsum('Bi,Bj -&gt; ij',dJ_dH2a, h1)/batch_size<br/>        # layer 1<br/>        dJ_dH1 = np.einsum('Bi,ij -&gt; Bj', dJ_dH2a, wghts[1])<br/>        dJ_dH1a = dJ_dH1*relu_prime(h1a)<br/>        dJ_dW0 = np.einsum('Bi,Bj -&gt; ij',dJ_dH1a, x)/batch_size<br/>        if bias:<br/>            dJ_dB1 = np.einsum("Bi -&gt; i", dJ_dH2a)/batch_size<br/>            dJ_dB0 = np.einsum("Bi -&gt; i",dJ_dH1a)/batch_size<br/>        # WEIGHTS ADJUSTMENT<br/>        wghts[1] = wghts[1] - lr*dJ_dW1<br/>        wghts[0] = wghts[0] - lr*dJ_dW0<br/>        if bias:<br/>            intercepts[1] = intercepts[1] - lr*dJ_dB1<br/>            intercepts[0] = intercepts[0] - lr*dJ_dB0<br/>    if bias:<br/>        return (costs, wghts, intercepts)<br/>    else:<br/>        return (costs, wghts)</span><span id="a4a6" class="na nb iq mm b gy ot nd l ne nf">iris = load_iris()<br/>x = iris.data<br/>y = iris.target</span><span id="fb46" class="na nb iq mm b gy ot nd l ne nf"># NORMALIZE<br/>x_norm = normalize(x)<br/>x_train, x_test, y_train, y_test = train_test_split(x_norm, y, test_size=0.33, shuffle=True, random_state=42)</span><span id="f620" class="na nb iq mm b gy ot nd l ne nf"># BINARIZE<br/>y_train = binarize(y_train)<br/>y_test = binarize(y_test)</span><span id="ac3f" class="na nb iq mm b gy ot nd l ne nf">unit_per_layer_counts = [10,3]<br/>costs, fw, fb = murals_batch(unit_per_layer_counts, x_train, y_train, lr=0.01, epochs=19000, bias=True)</span><span id="e5d9" class="na nb iq mm b gy ot nd l ne nf">plt.plot(costs)</span><span id="4d5c" class="na nb iq mm b gy ot nd l ne nf">def predict(x,fw,fb):<br/>    h1a = np.einsum(’hi,Bi -&gt; Bh’, fw[0], x)+fb[0]<br/>    h1 = relu(h1a)<br/>    h2a = np.einsum(’ho,Bo-&gt; Bh’,fw[1],h1)+fb[1]<br/>    return softmax(h2a)</span></pre><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/e5a8bef7706e91775b497d23f8ee779b.png" data-original-src="https://miro.medium.com/v2/resize:fit:864/format:webp/1*R589qzGPYEebI1pfT7EmQg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">分类交叉熵损失随时期的变化(图片归作者所有)</p></figure><blockquote class="mn mo mp"><p id="94d3" class="kw kx mq ky b kz la jr lb lc ld ju le mr lg lh li ms lk ll lm mt lo lp lq lr ij bi translated">还要看看高效代码执行19000个纪元所花费的惊人时间！！！</p></blockquote><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ov"><img src="../Images/2f398aa6c71f322a649f5e6b33ae7b8f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AEWaCpgCoDUR-ocbXRc_Eg.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">执行2层多类分类神经网络19000个时期所花费的时间(图片归作者所有)</p></figure><p id="083e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这种效率是由于numpy的爱因斯坦求和函数用于实现神经网络。我在之前的博客<a class="ae kv" href="https://manishankar.medium.com/batch-gradient-descent-algorithm-using-numpy-einsum-f442ef798ee2" rel="noopener">https://manishankar . medium . com/batch-gradient-descent-algorithm-using-numpy-einsum-f 442 ef 798 ee 2</a>中已经详细讨论过了</p><p id="767e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">请让我知道你对这篇博文和代码实现的看法以及你的评论。</p></div><div class="ab cl ng nh hu ni" role="separator"><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl nm"/><span class="nj bw bk nk nl"/></div><div class="ij ik il im in"><p id="2b59" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我是TCS的机器学习工程师，我的(数字软件和解决方案)团队正在开发令人惊叹的产品。点击下面的链接，了解更多关于我们产品的信息:</p><div class="ls lt gp gr lu lv"><a href="https://www.tcs.com/dss" rel="noopener  ugc nofollow" target="_blank"><div class="lw ab fo"><div class="lx ab ly cl cj lz"><h2 class="bd ir gy z fp ma fr fs mb fu fw ip bi translated">数字软件和解决方案:提供高度个性化的体验</h2><div class="ow l"><h3 class="bd b gy z fp ma fr fs mb fu fw dk translated">Digital Software &amp; Solutions的互联智能解决方案将帮助您转变产品和服务…</h3></div><div class="mc l"><p class="bd b dl z fp ma fr fs mb fu fw dk translated">www.tcs.com</p></div></div><div class="md l"><div class="ox l mf mg mh md mi kp lv"/></div></div></a></div></div></div>    
</body>
</html>