<html>
<head>
<title>How to get away with few Labels: Label Propagation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何摆脱标签少:标签传播</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-get-away-with-few-labels-label-propagation-f891782ada5c?source=collection_archive---------3-----------------------#2021-03-26">https://towardsdatascience.com/how-to-get-away-with-few-labels-label-propagation-f891782ada5c?source=collection_archive---------3-----------------------#2021-03-26</a></blockquote><div><div class="fc ii ij ik il im"/><div class="in io ip iq ir"><h2 id="f7cf" class="is it iu bd b dl iv iw ix iy iz ja dk jb translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/making-sense-of-big-data" rel="noopener" target="_blank">理解大数据</a></h2><div class=""/><div class=""><h2 id="2191" class="pw-subtitle-paragraph ka jd iu bd b kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq kr dk translated">聪明一点，不要自己手动标注数百甚至数千个数据点。</h2></div><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj ks"><img src="../Images/6eab733cdcee47c6a3051f736a48075e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*VLD2hkUFGY9V9CiZ"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">照片由<a class="ae li" href="https://unsplash.com/@blancaplum?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">布兰卡·帕洛玛·桑切斯</a>在<a class="ae li" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄</p></figure><h1 id="a31e" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">介绍</h1><p id="e440" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">美国数据科学家的一个经典任务是为某个问题建立分类模型。在一个完美的世界里，数据样本——包括它们相应的标签——是放在一个银盘里交给我们的。然后，我们使用机器学习技巧和<em class="mx">Mathematica</em>从数据中得出一些有用的见解。到目前为止一切顺利。</p><p id="8408" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">然而，在我们这个不完美却又美丽的世界里，经常发生的事情是以下之一:</p><ol class=""><li id="92d5" class="nd ne iu md b me my mh mz mk nf mo ng ms nh mw ni nj nk nl bi translated"><strong class="md je">我们得到一个极小的数据集，它至少是完全标记的。在这种情况下，构建模型可能会非常棘手。我们必须使用先进的特征工程，甚至可能使用贝叶斯方法和其他工具来解决这个问题。以Kaggle 上的<a class="ae li" href="https://www.kaggle.com/c/overfitting/" rel="noopener ugc nofollow" target="_blank">过拟合挑战为例:训练集由250个训练样本和200个特征组成。玩得开心。</a></strong></li><li id="fde7" class="nd ne iu md b me nm mh nn mk no mo np ms nq mw ni nj nk nl bi translated"><strong class="md je">我们得到了足够的数据，但却没有任何标签。嗯，运气不好。尝试聚类，但这不一定能解决您的分类问题。</strong></li><li id="6e90" class="nd ne iu md b me nm mh nn mk no mo np ms nq mw ni nj nk nl bi translated"><strong class="md je">我们得到了足够的数据，但只是部分标注。这正是我们将在本文中讨论的内容！继续读。</strong></li></ol></div><div class="ab cl nr ns hy nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="in io ip iq ir"><p id="e283" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">让我们假设从现在开始我们处于第三种情况:我们的数据集的大小相当不错，我们有几千个样本，甚至可能是一百万个。但是看着标签，挫败感油然而生— <strong class="md je">只有一小部分数据被贴上了标签！</strong></p><p id="bc59" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">在本文中，我将向您展示如何处理这些常见的情况。</p><h1 id="da72" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">天真的方法</h1><p id="c591" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">接近这个设置的最简单的方法是把它转换成我们更熟悉的东西。具体来说:</p><blockquote class="ny"><p id="04b1" class="nz oa iu bd ob oc od oe of og oh mw dk translated">丢弃未标记的数据点，并在剩余的完全标记但较小的数据集上训练分类器。</p></blockquote><p id="2b9f" class="pw-post-body-paragraph mb mc iu md b me oi ke mg mh oj kh mj mk ok mm mn mo ol mq mr ms om mu mv mw in bi translated">我们来分析一下这种做法。</p><h2 id="b206" class="on lk iu bd ll oo op dn lp oq or dp lt mk os ot lv mo ou ov lx ms ow ox lz ja bi translated">赞成的意见</h2><ul class=""><li id="7e49" class="nd ne iu md b me mf mh mi mk oy mo oz ms pa mw pb nj nk nl bi translated">易于理解和实施</li><li id="2701" class="nd ne iu md b me nm mh nn mk no mo np ms nq mw pb nj nk nl bi translated">快速转换和快速训练，因为较少的样本意味着较少的计算</li></ul><h2 id="c73f" class="on lk iu bd ll oo op dn lp oq or dp lt mk os ot lv mo ou ov lx ms ow ox lz ja bi translated">骗局</h2><ul class=""><li id="b07b" class="nd ne iu md b me mf mh mi mk oy mo oz ms pa mw pb nj nk nl bi translated">模型可能会过度适应剩余的数据</li><li id="02bd" class="nd ne iu md b me nm mh nn mk no mo np ms nq mw pb nj nk nl bi translated">标记数据的过程中的偏差可能会导致模型的错误决策边界</li></ul></div><div class="ab cl nr ns hy nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="in io ip iq ir"><p id="305a" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">虽然赞成一方的论点应该很容易理解，但让我们看一张图来更好地理解缺点。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pc"><img src="../Images/e547caddd1218d7b55f1cd8dda49dfa6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N_-e0F1E34AWduZLKw7a3g.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="b375" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">作为人类，我们可以清楚地看到有两个斑点。左边的应该是蓝色的，右边的应该是红色的。中间可能会有一些重叠，但总的来说，它们可以用一条直线很好地分开，即逻辑回归或线性SVM。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pc"><img src="../Images/bb5f449e353fc25471cd70f26d47b5be.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fAnjIpy_mssgdZnJ84D72A.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="f6f6" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">然而，如果我们丢弃未标记的数据并拟合逻辑回归，我们最终得到以下决策区域:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pc"><img src="../Images/d55540c489b2cef9dafca031355f488e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*3XmEbQO2mCrD_ZayWHEgug.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="389d" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">不太好。由于过度拟合，结果并不差，因为逻辑回归是一个简单的模型。但是被标记的数据点的位置是<strong class="md je">偏移的</strong>，即<strong class="md je">它们有一些奇怪的模式</strong>，使分类器混淆。公平地说，如果标记的数据点在两个斑点的中心，逻辑回归会工作得更好。</p><p id="834d" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">当然，如果我们试图将随机森林和神经网络放在规模为4的训练集上，也会出现<strong class="md je">过拟合问题</strong>。我们可以得出结论:</p><blockquote class="ny"><p id="44aa" class="nz oa iu bd ob oc od oe of og oh mw dk translated">简单地丢弃未标记的数据不是一个好主意。</p></blockquote></div><div class="ab cl nr ns hy nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="in io ip iq ir"><p id="34e3" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">现在让我们转向一种更智能的技术，它不仅允许我们整合已标记数据的知识，还允许我们整合未标记数据样本的特征。</p><blockquote class="pd pe pf"><p id="caa7" class="mb mc mx md b me my ke mg mh mz kh mj pg na mm mn ph nb mq mr pi nc mu mv mw in bi translated">这就是人们有时所说的<strong class="md je">半监督学习</strong>。</p></blockquote><h1 id="4f06" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">标签传播算法</h1><p id="bbf7" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">标签传播是朱小金和邹斌·格拉马尼[1]在2002年提出的一个好主意。</p><p id="c793" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated"><strong class="md je">重要提示:</strong>在这里，我对原始论文的想法做了一点改动，以便于解释和理解。这两种变体以及其他现有变体的主旨仍然是相同的。</p><p id="193f" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">从一个非常高的角度来看，它的工作原理如下:</p><h2 id="c1e2" class="on lk iu bd ll oo op dn lp oq or dp lt mk os ot lv mo ou ov lx ms ow ox lz ja bi translated"><strong class="ak"> 1。以数据样本为节点构建一个图。在每对样本之间放一条加权边。样本越接近，权重越高。标签在这一点上并不重要。</strong></h2><p id="e3d5" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">这迫切需要一个例子。让我们假设我们有另一个二维数据集，它只包含五个样本。有两个类别，一个样本没有标记。样本是我们图表的节点。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj pj"><img src="../Images/d70298ebfd5ba5076c22659f43cd4b93.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*ZIenoIj4Uzlw9hKwFNbQ7g.png"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="7417" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">现在，让我们建立一个完整的图形，即连接每个节点与任何其他节点。我们还用节点之间的距离(=样本)来注释边。你可以选择任何你喜欢的距离(即欧几里德距离)，它只是算法的另一个超参数。</p><p id="014f" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated"><strong class="md je">注意:</strong>我省略了标记样本之间的边缘，因为它保持了可视化的清晰，并且算法无论如何都不需要那些。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj pj"><img src="../Images/e6f0059fbef1308b6a93dfa8f33b4c55.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*MJAhiVrKrGqisIKP_XTyeQ.png"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="a075" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">还记得我们说过<strong class="md je">更接近的样本</strong>之间应该有<strong class="md je">更高的权重</strong>吗？到目前为止，是反过来的！有几种方法可以解决这个问题，最简单的方法是:在所有数字前面加一个减号，或者(相乘)反转数字，例如4 → 1/4=0.25。</p></div><div class="ab cl nr ns hy nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="in io ip iq ir"><p id="614a" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">作者在[1]中提出的是使用某种<strong class="md je">高斯</strong>函数，有时也称为<strong class="md je">径向基函数</strong> (rbf)。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pk"><img src="../Images/786a7a24a2071ce204b4cdf2356f71f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*MBEuIZCcrhgmCcW0my47BA.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="1b0c" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">其中<em class="mx"> x </em>和<em class="mx"> x </em>为样本。如果两个样本非常接近，即| <em class="mx"> x-x </em> '|约为0，则它们的边权重约为1。它们离得越远，重量越接近零。</p><p id="c39d" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated"><em class="mx"> σ </em>是一个您可以随意使用的超参数。例如，Scikit-learn对它的缺省值是<em class="mx"> σ </em> = 20。</p></div><div class="ab cl nr ns hy nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="in io ip iq ir"><p id="2ebd" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">无论如何，现在让我们使用乘法逆运算。图表变成了</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj pl"><img src="../Images/3240bd712b72c942e7cdac0bd013dbf4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1060/format:webp/1*aaMyhDPcNJ0Uh6FWoHg8FQ.png"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="0b0a" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">这是第一步的结尾。</p></div><div class="ab cl nr ns hy nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="in io ip iq ir"><h2 id="957c" class="on lk iu bd ll oo op dn lp oq or dp lt mk os ot lv mo ou ov lx ms ow ox lz ja bi translated">2.要获取未标记样本的标签，从该样本开始进行<a class="ae li" href="https://upload.wikimedia.org/wikipedia/commons/f/f3/Random_walk_2500_animated.svg" rel="noopener ugc nofollow" target="_blank">随机漫步</a>。遍历的一个步骤包括从一个节点随机跳到相邻节点。具有较高权重的边被选择的概率较高。计算随机漫步首先进入蓝色节点的概率。如果大于50%，将节点标记为蓝色，否则标记为红色。</h2><p id="50c5" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">这听起来比实际上更困难。例如，让我们从下方白色的未标记节点开始。为了继续，我们必须定义跳转到另一个未标记节点的概率，两个蓝色节点和红色节点中的一个。一个简单的方法是通过规范化。</p><p id="4277" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">有四个权重为1(通向蓝色节点)、0.25(另一个蓝色节点)、0.5(另一个未标记的节点)和0.5(红色节点)的外出边。所以，举个例子，我们只要把跳到红色节点的概率定义为0.5/(1+0.25+0.5+0.5)=2/9。跳到更近的蓝色节点发生的概率为1/(1+0.25+0.5+0.5)=4/9。剩下的你自己算。</p><p id="446a" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">使用这些概率，有很多理论涉及到如何计算先到达蓝色或红色节点的概率。你可以通过<a class="ae li" href="https://en.wikipedia.org/wiki/Markov_chain" rel="noopener ugc nofollow" target="_blank">马尔可夫链</a>来实现，这是数学中一个迷人的领域。有一天我甚至可能会写一篇关于它的文章，但是现在，我只会给你提供结果。</p><p id="1181" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">人们可以计算任一颜色着陆的下列概率:</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div role="button" tabindex="0" class="ky kz di la bf lb"><div class="gi gj pm"><img src="../Images/ec5a103c158d882a565089f7ff153f22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GEhSXQZpw1AlFt389NOk0Q.png"/></div></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="a7ee" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">有了这个结果，我们可以说上面的未标记节点可能属于红色类，而下面的节点应该是蓝色的。如果我们不想局限于某一类，我们也可以将这些概率作为软标签。</p><figure class="kt ku kv kw gu kx gi gj paragraph-image"><div class="gi gj pj"><img src="../Images/6f577b9b4889e50df988f8a90135972a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*izdScqLIqJmxMVQtYDMnEg.png"/></div><p class="le lf gk gi gj lg lh bd b be z dk translated">图片由作者提供。</p></figure><p id="2d3c" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">这大概也是你直觉上已经预料到的，这就说明了这个方法。</p><p id="8b90" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">现在让我们来看看这个方法的实际应用！</p><h1 id="031a" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">使用scikit-learn进行标签传播</h1><p id="c420" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">使用标签传播很容易，这再次归功于scikit-learn！在下面的片段中，我</p><ol class=""><li id="f478" class="nd ne iu md b me my mh mz mk nf mo ng ms nh mw ni nj nk nl bi translated">加载所有库和MNIST数据集，</li><li id="bf3c" class="nd ne iu md b me nm mh nn mk no mo np ms nq mw ni nj nk nl bi translated">用一个<strong class="md je"> -1 </strong>屏蔽标签的<strong class="md je"> 90% </strong>周围，一个丢失标签的预期输入，然后</li><li id="ff6f" class="nd ne iu md b me nm mh nn mk no mo np ms nq mw ni nj nk nl bi translated">使用标签传播来恢复我刚刚屏蔽的标签。</li></ol><p id="6e13" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">由于我们知道真正的标签，在这种情况下，我们甚至可以评估屏蔽集上的性能。但是，请注意，通常情况下，我们不能这样做。</p><pre class="kt ku kv kw gu pn po pp bn pq pr bi"><span id="e439" class="ps lk iu po b be pt pu l pv pw">import numpy as np<br/>from sklearn.datasets import load_digits<br/>from sklearn.metrics import classification_report<br/>from sklearn.semi_supervised import LabelPropagation<br/><br/>np.random.seed(0)<br/><br/>X, y_true = load_digits(return_X_y=True)<br/><br/>n = len(y)<br/>mask = np.random.choice(range(n), 9*n//10, replace=False)<br/>y_missing = y_true.copy()<br/>y_missing[mask] = -1 # -1 indicates a missing label<br/><br/>lp = LabelPropagation(gamma=.25) # rbf is the default, gamma = 1/σ²!<br/>lp.fit(X, y_missing) # run the algorithm we described above<br/><br/>print(classification_report(y_true[mask], lp.transduction_[mask]))</span></pre><p id="6db1" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">输出如下所示:</p><pre class="kt ku kv kw gu pn po px py aw pz bi"><span id="7a67" class="on lk iu po b gz qa qb l qc pw">              precision    recall  f1-score   support</span><span id="90ff" class="on lk iu po b gz qd qb l qc pw">           0       0.98      0.99      0.98       161<br/>           1       0.90      0.99      0.94       163<br/>           2       1.00      0.96      0.98       159<br/>           3       0.90      0.95      0.92       168<br/>           4       0.98      0.97      0.97       159<br/>           5       0.95      0.97      0.96       161<br/>           6       0.99      0.98      0.98       166<br/>           7       0.99      0.98      0.98       159<br/>           8       0.91      0.85      0.88       160<br/>           9       0.95      0.88      0.91       161<br/><br/>    accuracy                           <strong class="po je">0.95</strong>      1617<br/>   macro avg       0.95      0.95      0.95      1617<br/>weighted avg       0.95      0.95      0.95      1617</span></pre><p id="add9" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">95%的准确率，我还能说什么？<em class="mx">太神奇了！</em>该算法只能访问<strong class="md je"> 10% </strong>数据的标签，但它几乎在所有情况下都能正确标记其他样本。当我第一次在<a class="ae li" href="https://scikit-learn.org/stable/auto_examples/semi_supervised/plot_label_propagation_digits.html" rel="noopener ugc nofollow" target="_blank"> scikit-learn页面</a>上看到这个例子的另一种形式时，我就开始相信了。我敢打赌，这可能对你的日常工作也很有用！</p><p id="07fc" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">当然，这不是100%正确的，但如果你不想自己标记数千甚至数百万个样品，这是一个有效的选择。</p><h1 id="fa1e" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">评论</h1><p id="cdd4" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">作为最后的官方行为，让我给你指出一些有趣的细节。</p><h2 id="2f33" class="on lk iu bd ll oo op dn lp oq or dp lt mk os ot lv mo ou ov lx ms ow ox lz ja bi translated">与KNN的联系</h2><p id="db7e" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">仔细想想，标签传播感觉有点<a class="ae li" rel="noopener" target="_blank" href="/understanding-by-implementing-k-nearest-neighbors-469d6f84b8a9"><em class="mx">k</em>-最近邻<em class="mx"> -ish </em> </a> <em class="mx">，</em>不是吗？想象你训练了一个KNN分类器。在预测时间，一个没有标签的新点进入。你扫描你的整个训练数据集，并从中挑选最接近的点。离新点越近的点越重要。</p><p id="e13c" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">正如我们所见，标签传播也是如此。两个样本<em class="mx"> x </em>、<em class="mx"> x </em>越近，它们之间的边在图中的权重就越大，你从<em class="mx"> x </em>跳到<em class="mx"> x </em>的概率就越高，反之亦然。相似之处是存在的，然而标签传播比最近邻居要复杂一些。</p><p id="689b" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">标签传播同时考虑了大量未标记的样本，它们相互帮助，在图形/数据集中的任何地方传播正确的标签。就KNN而言，每个样本都有其自身的特点。因此，从某种意义上说，标签传播是一种更智能的算法。虽然我们不应该把苹果和<em class="mx">梨</em>相提并论，正如我们在德国所说的:两种算法解决了不同的问题。</p><h2 id="b6f3" class="on lk iu bd ll oo op dn lp oq or dp lt mk os ot lv mo ou ov lx ms ow ox lz ja bi translated">这些图表很大</h2><p id="9211" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">假设您有一个由1，000，000个样本组成的数据集。在标签传播过程中创建的图形</p><blockquote class="ny"><p id="61be" class="nz oa iu bd ob oc od oe of og oh mw dk translated"><strong class="ak"> 1，000，000 * (1，000，000–1)/2 =</strong>499，999，500，000</p></blockquote><p id="7d74" class="pw-post-body-paragraph mb mc iu md b me oi ke mg mh oj kh mj mk ok mm mn mo ol mq mr ms om mu mv mw in bi translated">边缘。如果您将这些边的权重存储为64位浮点，那已经是<strong class="md je"> 4 TB </strong>了。内存太大，写入磁盘时速度太慢。请注意，这是我解释算法的方式，也是<a class="ae li" href="https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.LabelPropagation.html" rel="noopener ugc nofollow" target="_blank">scikit-learn</a><code class="fe qe qf qg po b"><a class="ae li" href="https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.LabelPropagation.html" rel="noopener ugc nofollow" target="_blank">LabelPropagation</a></code>的默认行为。</p><p id="32c8" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">在这些情况下，您可以构建一个<em class="mx">不完整的</em>图。与其将每个样本节点与其他节点连接起来，不如将其与其<em class="mx"> k </em>最近的邻居连接起来。(<em class="mx">又来了。</em>)</p><p id="1d76" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">在这种情况下，只有<em class="mx"> k </em> * 1，000，000条边，对于像<em class="mx"> k </em> =7这样的小值，仍然很容易处理。您可以在scikit-learn中使用这种方法，通过设置<code class="fe qe qf qg po b">kernel='knn'</code>然后也玩弄<code class="fe qe qf qg po b">n_neighbors</code>参数。</p><h1 id="4ab3" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">结论</h1><p id="f318" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">在本文中，我们研究了只有一小部分标记数据的数据集的问题。我们已经确定，丢弃未标记的数据点可能会导致灾难，需要更智能的方法，其中之一就是标记传播。</p><p id="64f1" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">该算法通过构建一个图来工作，其中数据集的样本是节点，每对样本之间有一条边。对于一个未标记的样本，从那里开始随机游走，看看你大部分时间是在哪个类别的已标记样本中结束的。</p><p id="dca9" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">我们已经看到，这种方法可以非常好地工作，用一个只有10%标记数据的MNIST例子来证明。准确度高达95%，这并不完美，但比另一种方法更好:手工标记剩余的90%或<strong class="md je"> 1617 </strong>样本。</p></div><div class="ab cl nr ns hy nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="in io ip iq ir"><p id="fd46" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">但是等等……手工标注剩余的数据集实际上不是唯一的选择。今天我们没有谈到的另一条道路是<strong class="md je">主动学习。</strong></p><p id="04a2" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">无论如何，这是另一个时代的故事。</p><h1 id="0f9f" class="lj lk iu bd ll lm ln lo lp lq lr ls lt kj lu kk lv km lw kn lx kp ly kq lz ma bi translated">参考</h1><p id="cea6" class="pw-post-body-paragraph mb mc iu md b me mf ke mg mh mi kh mj mk ml mm mn mo mp mq mr ms mt mu mv mw in bi translated">[1]朱小金和邹斌·格拉马尼，<a class="ae li" href="http://pages.cs.wisc.edu/~jerryzhu/pub/CMU-CALD-02-107.pdf" rel="noopener ugc nofollow" target="_blank">利用标签传播从有标签和无标签数据中学习</a> (2002)，技术报告-CALD-02-107，卡耐基梅隆大学</p><p id="ce44" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">【更多】<a class="ae li" href="https://scikit-learn.org/stable/modules/semi_supervised.html#label-propagation" rel="noopener ugc nofollow" target="_blank">sci kit-learn标签传播用户指南</a></p></div><div class="ab cl nr ns hy nt" role="separator"><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw nx"/><span class="nu bw bk nv nw"/></div><div class="in io ip iq ir"><p id="e46a" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">我希望你今天学到了新的、有趣的、有用的东西。感谢阅读！</p><p id="ab8f" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated"><strong class="md je">作为最后一点，如果你</strong></p><ol class=""><li id="bd62" class="nd ne iu md b me my mh mz mk nf mo ng ms nh mw ni nj nk nl bi translated"><strong class="md je">想支持我多写点机器学习和</strong></li><li id="7483" class="nd ne iu md b me nm mh nn mk no mo np ms nq mw ni nj nk nl bi translated"><strong class="md je">无论如何都要计划获得中等订阅量，</strong></li></ol><p id="0a41" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated"><strong class="md je">为什么不通过此链接</strong><a class="ae li" href="https://dr-robert-kuebler.medium.com/membership" rel="noopener"><strong class="md je"/></a><strong class="md je">？这将对我帮助很大！😊</strong></p><p id="65e2" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated"><em class="mx">说白了，给你的价格不变，但大约一半的订阅费直接归我。</em></p><p id="d45b" class="pw-post-body-paragraph mb mc iu md b me my ke mg mh mz kh mj mk na mm mn mo nb mq mr ms nc mu mv mw in bi translated">非常感谢，如果你考虑支持我的话！</p><blockquote class="ny"><p id="40e9" class="nz oa iu bd ob oc od oe of og oh mw dk translated"><em class="qh">有问题就在</em><a class="ae li" href="https://www.linkedin.com/in/dr-robert-k%C3%BCbler-983859150/" rel="noopener ugc nofollow" target="_blank"><em class="qh">LinkedIn</em></a><em class="qh">上写我！</em></p></blockquote></div></div>    
</body>
</html>