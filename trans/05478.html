<html>
<head>
<title>A Simple Guide to Scikit-Learn — Building a Machine Learning Model in Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Scikit-Learn简单指南—用Python构建机器学习模型</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-beginners-guide-to-text-classification-with-scikit-learn-632357e16f3a?source=collection_archive---------0-----------------------#2021-05-15">https://towardsdatascience.com/a-beginners-guide-to-text-classification-with-scikit-learn-632357e16f3a?source=collection_archive---------0-----------------------#2021-05-15</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="4f04" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Python中的第一个ML模型。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d41dac63a7079b1cb616cedb5dfaedbd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GaCTxLHRtfuG2CdR6I5B4w.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated"><a class="ae ky" href="https://pixabay.com/users/ulischu-1993560/" rel="noopener ugc nofollow" target="_blank"> UliSchu </a>在<a class="ae ky" href="https://pixabay.com/photos/organization-register-folder-files-1205171/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>拍摄的照片</p></figure><p id="b892" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你正在学习Python，并且想开发一个机器学习模型，那么你要认真考虑的库是scikit-learn。Scikit-learn(也称为sklearn)是Python中使用的一个机器学习库，它提供了许多无监督和有监督的学习算法。</p><p id="5560" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在这个简单的指南中，我们将创建一个机器学习模型，它将预测电影评论是正面还是负面的。这被称为二进制文本分类，将帮助我们探索scikit-learn库，同时从头构建一个基本的机器学习模型。这些是我们将在本指南中学习的主题。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="6a49" class="ma mb it lw b gy mc md l me mf"><strong class="lw iu">Table of Contents<br/></strong>1. <a class="ae ky" href="#f4a3" rel="noopener ugc nofollow">The Dataset and The Problem to Solve</a><br/>2. <a class="ae ky" href="#8da5" rel="noopener ugc nofollow">Preparing The Data</a><br/> - <a class="ae ky" href="#f171" rel="noopener ugc nofollow">Reading the dataset</a><br/> - <a class="ae ky" href="#c614" rel="noopener ugc nofollow">Dealing with Imbalanced Classes</a><br/> - <a class="ae ky" href="#610e" rel="noopener ugc nofollow">Splitting data into train and test set</a><br/>3. <a class="ae ky" href="#3283" rel="noopener ugc nofollow">Text Representation (Bag of Words)</a><br/> - <a class="ae ky" href="#867e" rel="noopener ugc nofollow">CountVectorizer</a><br/> - <a class="ae ky" href="#e16b" rel="noopener ugc nofollow">Term Frequency, Inverse Document Frequency (TF-IDF)</a><br/> - <a class="ae ky" href="#0085" rel="noopener ugc nofollow">Turning our text data into numerical vectors</a><br/>4. <a class="ae ky" href="#660b" rel="noopener ugc nofollow">Model Selection</a><br/> - <a class="ae ky" href="#4584" rel="noopener ugc nofollow">Supervised vs Unsupervised learning</a><br/> - <a class="ae ky" href="#1629" rel="noopener ugc nofollow">Support Vector Machines (SVM)</a><br/> - <a class="ae ky" href="#1a0d" rel="noopener ugc nofollow">Decision Tree</a><br/> - <a class="ae ky" href="#68b4" rel="noopener ugc nofollow">Naive Bayes</a><br/> - <a class="ae ky" href="#e482" rel="noopener ugc nofollow">Logistic Regression</a><br/>5. <a class="ae ky" href="#0364" rel="noopener ugc nofollow">Model Evaluation</a><br/> - <a class="ae ky" href="#42ef" rel="noopener ugc nofollow">Mean Accuracy</a><br/> - <a class="ae ky" href="#05ec" rel="noopener ugc nofollow">F1 Score</a><br/> - <a class="ae ky" href="#4716" rel="noopener ugc nofollow">Classification report</a><br/> - <a class="ae ky" href="#02ad" rel="noopener ugc nofollow">Confusion Matrix</a><br/>6. <a class="ae ky" href="#9f73" rel="noopener ugc nofollow">Tuning the Model</a><br/> - <a class="ae ky" href="#bc36" rel="noopener ugc nofollow">GridSearchCV</a></span></pre><h1 id="f4a3" class="mg mb it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">数据集和要解决的问题</h1><p id="2da9" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">在本指南中，我们将使用在<a class="ae ky" href="https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上提供的50k条电影评论的IMDB数据集。数据集包含两列(评论和观点),这将帮助我们确定评论是积极的还是消极的。</p><p id="ef10" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">问题公式化</strong>:我们的目标是在给定一个电影评论(输入)的情况下，找到哪个机器学习模型最适合预测情绪(输出)。</p><h1 id="8da5" class="mg mb it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">准备数据</h1><h2 id="f171" class="ma mb it bd mh nc nd dn ml ne nf dp mp li ng nh mr lm ni nj mt lq nk nl mv nm bi translated">读取数据集</h2><p id="6b85" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">下载数据集后，确保文件与Python脚本位于同一位置。然后，我们将使用Pandas库来读取该文件。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="695e" class="ma mb it lw b gy mc md l me mf">import pandas as pd</span><span id="a8e5" class="ma mb it lw b gy nn md l me mf">df_review = pd.read_csv('IMDB Dataset.csv')<br/>df_review</span></pre><p id="0db4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="no">注意:如果您没有本指南中使用的一些库，您可以在终端或命令提示符下轻松安装带有pip的库(例如</em> <code class="fe np nq nr lw b">pip install scikit-learn)</code></p><p id="2eb3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据集如下图所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/11479b31a2a08845a3346f28a35a0b7b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1192/format:webp/1*U8L7nXrMRn_qdbYEKJw4-A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="1c71" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">该数据集包含50000行；然而，<strong class="lb iu">为了在接下来的步骤中更快地训练我们的模型，我们将采用10000行的较小样本。这个小样本将包含9000个正面和1000个负面评论，以使数据不平衡</strong>(所以我可以在下一步教你欠采样和过采样技术)</p><p id="7646" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将用下面的代码创建这个小样本。这个不平衡数据集的名称将是<code class="fe np nq nr lw b">df_review_imb</code></p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="0eb5" class="ma mb it lw b gy mc md l me mf">df_positive = df_review[df_review['sentiment']=='positive'][:9000]<br/>df_negative = df_review[df_review['sentiment']=='negative'][:1000]</span><span id="bf1b" class="ma mb it lw b gy nn md l me mf">df_review_imb = pd.concat([df_positive, df_negative])</span></pre><h2 id="c614" class="ma mb it bd mh nc nd dn ml ne nf dp mp li ng nh mr lm ni nj mt lq nk nl mv nm bi translated">处理不平衡的班级</h2><p id="488b" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">在大多数情况下，一个类有大量的数据，而其他类的观察数据要少得多。这就是所谓的不平衡数据，因为每个类的观测值数量不是均匀分布的。</p><p id="d3ff" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看我们的<code class="fe np nq nr lw b">df_review_imb </code>数据集是如何分布的。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/b0eaca66fab659a61268498836e10392.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*AQdrQIjn4fWUtfuo_ZlZ_Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="a5be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们所见，在<code class="fe np nq nr lw b">df_review_imb</code>中正面评价多于负面评价，所以我们有不平衡的数据。</p><p id="e0e5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了对数据进行重采样，我们使用了<code class="fe np nq nr lw b">imblearn</code>库。您可以对正面评价进行欠采样，也可以对负面评价进行过采样(您需要根据您正在处理的数据进行选择)。在这种情况下，我们将使用<code class="fe np nq nr lw b">RandomUnderSampler</code></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="42eb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，我们创建一个新的<code class="fe np nq nr lw b">RandomUnderSampler (rus)</code>实例，我们添加<code class="fe np nq nr lw b">random_state=0 </code>只是为了控制算法的随机化。然后，我们通过用<code class="fe np nq nr lw b">rus.fit_resample(x, y)</code>拟合<code class="fe np nq nr lw b">rus</code>对不平衡数据集<code class="fe np nq nr lw b">df_review_imb </code>进行重新采样，其中“x”包含必须采样的数据，而“y”对应于“x”中每个样本的标签。</p><p id="ae48" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在此之后，<code class="fe np nq nr lw b">x</code>和<code class="fe np nq nr lw b">y </code>被平衡，我们将它存储在一个名为<code class="fe np nq nr lw b">df_review_bal. </code>的新数据集中。我们可以用下面的代码比较不平衡和平衡的数据集。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="ab4a" class="ma mb it lw b gy mc md l me mf">IN [0]: print(df_review_imb.value_counts(‘sentiment’))<br/>        print(df_review_bal.value_counts(‘sentiment’))</span><span id="8539" class="ma mb it lw b gy nn md l me mf">OUT [0]:positive    9000<br/>        negative    1000<br/><br/>        negative    1000<br/>        positive    1000</span></pre><p id="290b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如我们所见，现在我们的数据集是平均分布的。</p><p id="35f1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注1:如果使用<code class="fe np nq nr lw b">RandomUnderSampler</code>时出现以下错误</p><blockquote class="nw nx ny"><p id="7b8c" class="kz la no lb b lc ld ju le lf lg jx lh nz lj lk ll oa ln lo lp ob lr ls lt lu im bi translated">IndexError:只有整数、切片(`:`)、省略号(`…`)、numpy.newaxis (`None `)和整数或布尔数组是有效的索引</p></blockquote><p id="31db" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以使用<code class="fe np nq nr lw b">RandomUnderSampler</code>的替代产品。尝试下面的代码:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nu nv l"/></div></figure><p id="9010" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如上所示，<code class="fe np nq nr lw b">df_review_bal</code>数据框架现在应该有1000条正面和负面评论。</p><p id="e00c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="no">注2:通常在将数据分成训练集和测试集之前，我们应该清理数据。在本例中，数据已经被清除；然而，现实世界中的数据是不干净的，所以每当你需要清理数据时，请查看下面的指南，了解Python中数据清理的最佳实践。</em></p><div class="oc od gp gr oe of"><a rel="noopener follow" target="_blank" href="/a-straightforward-guide-to-cleaning-and-preparing-data-in-python-8c82f209ae33"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd iu gy z fp ok fr fs ol fu fw is bi translated">用Python清理和准备数据的简单指南</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">如何识别和处理脏数据？</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">towardsdatascience.com</p></div></div><div class="oo l"><div class="op l oq or os oo ot ks of"/></div></div></a></div><h2 id="610e" class="ma mb it bd mh nc nd dn ml ne nf dp mp li ng nh mr lm ni nj mt lq nk nl mv nm bi translated">将数据分为训练集和测试集</h2><p id="5f8b" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">在我们处理数据之前，我们需要将它分成训练集和测试集。<code class="fe np nq nr lw b">train</code>数据集将用于拟合模型，而<code class="fe np nq nr lw b">test</code>数据集将用于在训练数据集上提供最终模型拟合的无偏评估。</p><p id="73ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用sklearn的<code class="fe np nq nr lw b">train_test_split</code>来完成这项工作。在这种情况下，我们将测试数据设置为33%。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="d630" class="ma mb it lw b gy mc md l me mf">from sklearn.model_selection import train_test_split</span><span id="cd50" class="ma mb it lw b gy nn md l me mf">train, test = train_test_split(df_review_bal, test_size=0.33, random_state=42)</span></pre><p id="091e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，我们可以在训练和测试集内设置自变量和因变量。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="2c94" class="ma mb it lw b gy mc md l me mf">train_x, train_y = train['review'], train['sentiment']<br/>test_x, test_y = test['review'], test['sentiment']</span></pre><p id="8acc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看它们各自的含义:</p><ul class=""><li id="9ed3" class="ou ov it lb b lc ld lf lg li ow lm ox lq oy lu oz pa pb pc bi translated"><strong class="lb iu"> train_x: </strong>将用于训练模型的独立变量(review)。由于我们指定了<code class="fe np nq nr lw b">test_size = 0.33</code>，来自数据的67%的观察值将用于拟合模型。</li><li id="94ea" class="ou ov it lb b lc pd lf pe li pf lm pg lq ph lu oz pa pb pc bi translated"><strong class="lb iu"> train_y: </strong>该模型需要预测的因变量(情绪)或目标标签。</li><li id="17e7" class="ou ov it lb b lc pd lf pe li pf lm pg lq ph lu oz pa pb pc bi translated"><strong class="lb iu"> test_x: </strong>剩余的<code class="fe np nq nr lw b">33%</code>个独立变量，将用于进行预测，以测试模型的准确性。</li><li id="810c" class="ou ov it lb b lc pd lf pe li pf lm pg lq ph lu oz pa pb pc bi translated"><strong class="lb iu"> test_y: </strong>类别标签，将用于测试实际类别和预测类别之间的准确性。</li></ul><h1 id="3283" class="mg mb it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">文本表示(单词包)</h1><p id="94af" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">分类器和学习算法期望数字特征向量，而不是原始文本文档。这就是为什么我们需要把我们的电影评论文本变成数字向量。有许多文本表示技术，如一键编码、单词包和wor2vec。</p><p id="291a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于这个简单的例子，我们将使用单词包(BOW ),因为我们关心文本评论中单词的频率；然而，单词的顺序无关紧要。表示单词包的两种常见方式是计数矢量器和词频、逆文档频率(TF-IDF)。在我们选择它们之前，我会给你一个简单易懂的演示，告诉你它们是如何工作的。</p><h2 id="867e" class="ma mb it bd mh nc nd dn ml ne nf dp mp li ng nh mr lm ni nj mt lq nk nl mv nm bi translated">计数矢量器</h2><p id="08c2" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">CountVectorizer给出了单词在文档中出现的频率。让我们考虑下面的句子。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="57b8" class="ma mb it lw b gy mc md l me mf">review = [“I love writing code in Python. I love Python code”,<br/>          “I hate writing code in Java. I hate Java code”]</span></pre><p id="f7ce" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">CountVectorizer的表示如下所示，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/b12667c7da881bb04cece246a8058ba6.png" data-original-src="https://miro.medium.com/v2/resize:fit:918/format:webp/1*_ZmSegbQe5G38-Ya_AWc2A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="bf35" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如你所见，矩阵中的数字代表每个单词在每次评论中被提及的次数。在这个例子中，像“爱”、“恨”和“代码”这样的词有相同的频率(2)。</p><h2 id="e16b" class="ma mb it bd mh nc nd dn ml ne nf dp mp li ng nh mr lm ni nj mt lq nk nl mv nm bi translated">术语频率，逆文档频率(TF-IDF)</h2><p id="d3aa" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">TF-IDF计算“权重”,它表示一个单词对文档集合(也称为语料库)中的一个文档有多重要。TF-IDF值与单词在文档中出现的次数成比例地增加，并被语料库中包含该单词的文档的数量所抵消。</p><p id="903d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">TF-IDF的表示看起来像下面的图片，是我们之前使用的相同文本。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/dc286af8b73056454671342113619c91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1088/format:webp/1*33QrGPQxJ-ZCLUu1edXlgw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="fda2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与前面的例子不同，单词“code”与单词“love”或“hate”的权重不同。这是因为“代码”出现在两个评审中；因此，它的重量减少了。</p></div><div class="ab cl pk pl hx pm" role="separator"><span class="pn bw bk po pp pq"/><span class="pn bw bk po pp pq"/><span class="pn bw bk po pp"/></div><div class="im in io ip iq"><h2 id="0085" class="ma mb it bd mh nc nd dn ml ne nf dp mp li ng nh mr lm ni nj mt lq nk nl mv nm bi translated">将文本数据转化为数字向量</h2><p id="3aaf" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">在我们的原始数据集中，我们希望识别正面评论和负面评论的唯一/代表性单词，因此我们将选择TF-IDF。为了使用TF-IDF将文本数据转换成数字向量，我们编写了以下代码。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="a8e1" class="ma mb it lw b gy mc md l me mf">from sklearn.feature_extraction.text import TfidfVectorizer</span><span id="59f6" class="ma mb it lw b gy nn md l me mf">tfidf = TfidfVectorizer(stop_words='english')<br/>train_x_vector = tfidf.fit_transform(train_x)<br/>train_x_vector</span><span id="7425" class="ma mb it lw b gy nn md l me mf">Out[0]: &lt;1340x20625 sparse matrix of type '&lt;class 'numpy.float64'&gt;'<br/>	with 118834 stored elements in Compressed Sparse Row format&gt;</span></pre><p id="aaa8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在上面的代码中，我们创建了一个新的<code class="fe np nq nr lw b">TfidfVectorizer(tfidf)</code>实例，我们删除了英文停用词，然后拟合(查找模型的内部参数)并转换(将参数应用于数据)train_x(文本评论)</p><p id="7097" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们刚刚创建的<code class="fe np nq nr lw b">train_x_vector</code>是一个稀疏矩阵，有1340条评论和20625个单词(评论中使用的整个词汇)。您可以用下面的代码显示这个矩阵，就像我在上面的例子中使用的图片一样</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="96e5" class="ma mb it lw b gy mc md l me mf">pd.DataFrame.sparse.from_spmatrix(train_x_vector,<br/>                                  index=train_x.index,<br/>                                  columns=tfidf.get_feature_names())</span></pre><p id="5137" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">但是，请记住，您会发现大量的“0 ”,因为它是一个具有1340x20625个元素的稀疏矩阵，其中只有118834个元素不同于“0”</p><p id="8aa4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，让我们也转换一下<code class="fe np nq nr lw b">test_x_vector</code>，这样我们可以稍后测试模型的准确性(我们不需要再次拟合<code class="fe np nq nr lw b">tfidf </code>，因为我们已经用训练数据拟合过了。)</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="4900" class="ma mb it lw b gy mc md l me mf">test_x_vector = tfidf.transform(test_x)</span></pre><p id="37d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="no">注意:我们可以更好地准备文本数据，以便通过使用标记化和移除我们认为不相关的多余单词(除了CountVectorizer和Tfidf默认具有的停用词表之外)来开发更好的模型。有关使用Python进行标记化的更多信息，请查看下面的文章。</em></p><div class="oc od gp gr oe of"><a rel="noopener follow" target="_blank" href="/5-simple-ways-to-tokenize-text-in-python-92c6804edfc4"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd iu gy z fp ok fr fs ol fu fw is bi translated">用Python标记文本的5种简单方法</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">标记文本、大型语料库和不同语言的句子。</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">towardsdatascience.com</p></div></div><div class="oo l"><div class="pr l oq or os oo ot ks of"/></div></div></a></div><h1 id="660b" class="mg mb it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">型号选择</h1><p id="5c39" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">现在我们有了数字数据，我们可以用不同的机器学习模型进行实验，并评估它们的准确性。</p><h2 id="4584" class="ma mb it bd mh nc nd dn ml ne nf dp mp li ng nh mr lm ni nj mt lq nk nl mv nm bi translated">监督与非监督学习</h2><p id="155b" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">机器学习算法分为有监督学习和无监督学习。在第一种模式中，使用标记数据训练模型，而在第二种模式中，从未标记的输入数据中推断模式。</p><p id="240e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我们的例子中，我们的输入(评论)和输出(情绪)被清楚地识别，所以我们可以说我们已经标记了输入和输出数据；因此，我们正在处理监督学习。两种常见的监督学习算法是回归和分类。</p><ul class=""><li id="7f5e" class="ou ov it lb b lc ld lf lg li ow lm ox lq oy lu oz pa pb pc bi translated"><strong class="lb iu">回归</strong>:它们用于预测<strong class="lb iu">连续值</strong>，如价格、工资、年龄等</li><li id="8bab" class="ou ov it lb b lc pd lf pe li pf lm pg lq ph lu oz pa pb pc bi translated"><strong class="lb iu">分类</strong>:用于预测<strong class="lb iu">离散值</strong>，如男性/女性、垃圾/非垃圾、正/负等。</li></ul><p id="8f4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">也就是说，现在很明显我们应该使用分类算法。我们将对以下四个分类模型进行基准测试。</p><p id="5acd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="no">注意:我将把每个模型背后的理论留给你去研究。我将把重点放在代码上，以及我们如何根据分数选择最佳模型。</em></p><h2 id="1629" class="ma mb it bd mh nc nd dn ml ne nf dp mp li ng nh mr lm ni nj mt lq nk nl mv nm bi translated">支持向量机(SVM)</h2><p id="9b2c" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">为了适应SVM模型，我们需要引入输入(作为数字向量的文本评论)和输出(情感)</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="ddc8" class="ma mb it lw b gy mc md l me mf">from sklearn.svm import SVC</span><span id="5fab" class="ma mb it lw b gy nn md l me mf">svc = SVC(kernel=’linear’)<br/>svc.fit(train_x_vector, train_y)</span></pre><p id="e84d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">拟合<code class="fe np nq nr lw b">svc </code>后，我们可以用<code class="fe np nq nr lw b">.predict() </code>方法预测评论是正面还是负面。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="d688" class="ma mb it lw b gy mc md l me mf">print(svc.predict(tfidf.transform(['A good movie'])))<br/>print(svc.predict(tfidf.transform(['An excellent movie'])))<br/>print(svc.predict(tfidf.transform(['I did not like this movie at all'])))</span></pre><p id="ae18" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您运行上面的代码，您将获得第一个和第二个评论是正面的，而第三个是负面的。</p><h2 id="1a0d" class="ma mb it bd mh nc nd dn ml ne nf dp mp li ng nh mr lm ni nj mt lq nk nl mv nm bi translated">决策图表</h2><p id="ca4d" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">为了适应决策树模型，我们需要引入输入(作为数字向量的文本评论)和输出(情感)</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="33b8" class="ma mb it lw b gy mc md l me mf">from sklearn.tree import DecisionTreeClassifier</span><span id="71b1" class="ma mb it lw b gy nn md l me mf">dec_tree = DecisionTreeClassifier()<br/>dec_tree.fit(train_x_vector, train_y)</span></pre><h2 id="68b4" class="ma mb it bd mh nc nd dn ml ne nf dp mp li ng nh mr lm ni nj mt lq nk nl mv nm bi translated">朴素贝叶斯</h2><p id="4c0e" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">为了适应朴素贝叶斯模型，我们需要引入输入(作为数字向量的文本评论)和输出(情感)</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="064f" class="ma mb it lw b gy mc md l me mf">from sklearn.naive_bayes import GaussianNB</span><span id="916f" class="ma mb it lw b gy nn md l me mf">gnb = GaussianNB()<br/>gnb.fit(train_x_vector.toarray(), train_y)</span></pre><h2 id="e482" class="ma mb it bd mh nc nd dn ml ne nf dp mp li ng nh mr lm ni nj mt lq nk nl mv nm bi translated">逻辑回归</h2><p id="e7a6" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">为了拟合逻辑回归模型，我们需要引入输入(作为数字向量的文本评论)和输出(情感)</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="439e" class="ma mb it lw b gy mc md l me mf">from sklearn.linear_model import LogisticRegression</span><span id="c7c1" class="ma mb it lw b gy nn md l me mf">log_reg = LogisticRegression()<br/>log_reg.fit(train_x_vector, train_y)</span></pre><h1 id="0364" class="mg mb it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">模型评估</h1><p id="de85" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">在本节中，我们将看到用于评估模型的传统指标。</p><h2 id="42ef" class="ma mb it bd mh nc nd dn ml ne nf dp mp li ng nh mr lm ni nj mt lq nk nl mv nm bi translated">平均准确度</h2><p id="6fdd" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">要获得每个模型的平均准确度，只需使用如下所示的测试样本和真实标签的<code class="fe np nq nr lw b">.score</code>方法。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="2cb2" class="ma mb it lw b gy mc md l me mf"># svc.score('Test samples', 'True labels')</span><span id="a1c1" class="ma mb it lw b gy nn md l me mf">svc.score(test_x_vector, test_y)<br/>dec_tree.score(test_x_vector, test_y)<br/>gnb.score(test_x_vector.toarray(), test_y)<br/>log_reg.score(test_x_vector, test_y)</span></pre><p id="c6ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在打印出每一个之后，我们得到平均精度。</p><p id="368d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> SVM: 0.84 </strong> <br/>决策树:0.64 <br/>朴素贝叶斯:0.63 <br/>逻辑回归:0.83</p><p id="602b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">SVM和逻辑回归比其他两个分类器表现更好，SVM略有优势(84%的准确率)。<strong class="lb iu">为了展示其他指标的工作原理，我们将只关注SVM。</strong></p><h2 id="05ec" class="ma mb it bd mh nc nd dn ml ne nf dp mp li ng nh mr lm ni nj mt lq nk nl mv nm bi translated">F1分数</h2><p id="fdf5" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">F1分数是精确度和召回率的加权平均值。当真阳性和真阴性更重要时使用准确度，而当假阴性和假阳性至关重要时使用F1分数。此外，F1还考虑了数据的分布方式，因此当您有不平衡类的数据时，F1非常有用。</p><p id="6d12" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">F1分数通过以下公式计算。(如果你不知道精度和召回意味着什么，查看一下这个关于<a class="ae ky" href="https://stackoverflow.com/questions/14117997/what-does-recall-mean-in-machine-learning" rel="noopener ugc nofollow" target="_blank"> stackoverflow </a>的精彩解释)</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="4ccc" class="ma mb it lw b gy mc md l me mf">F1 Score = 2*(Recall * Precision) / (Recall + Precision)</span><span id="bab2" class="ma mb it lw b gy nn md l me mf">F1 score reaches its best value at 1 and worst score at 0.</span></pre><p id="cf94" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了获得F1分数，我们需要真实标签和预测标签<code class="fe np nq nr lw b">f1_score(y_true, y_pred)</code></p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="6987" class="ma mb it lw b gy mc md l me mf">from sklearn.metrics import f1_score</span><span id="a73d" class="ma mb it lw b gy nn md l me mf">f1_score(test_y, svc.predict(test_x_vector),<br/>         labels=['positive', 'negative'],<br/>         average=None)</span></pre><p id="98a5" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">阳性标签的得分为0.84，而阴性标签的得分为0.83。</p><h2 id="4716" class="ma mb it bd mh nc nd dn ml ne nf dp mp li ng nh mr lm ni nj mt lq nk nl mv nm bi translated">分类报告</h2><p id="cbfd" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">我们还可以构建一个文本报告，显示主要的分类指标，包括之前计算的指标。为了获得分类报告，我们需要真实标签和预测标签<code class="fe np nq nr lw b">classification_report(y_true, y_pred)</code></p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="7083" class="ma mb it lw b gy mc md l me mf">from sklearn.metrics import classification_report</span><span id="e9e4" class="ma mb it lw b gy nn md l me mf">print(classification_report(test_y, <br/>                            svc.predict(test_x_vector),<br/>                            labels=['positive', 'negative']))</span></pre><p id="a0b3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">打印后，我们获得以下报告。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="f263" class="ma mb it lw b gy mc md l me mf">               precision    recall  f1-score   support<br/><br/>    positive       0.83      0.87      0.85       335<br/>    negative       0.85      0.82      0.83       325<br/><br/>    accuracy                           0.84       660<br/>   macro avg       0.84      0.84      0.84       660<br/>weighted avg       0.84      0.84      0.84       660</span></pre><p id="27d8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如我们所见，精确度和f1分数与之前计算的相同。</p><h2 id="02ad" class="ma mb it bd mh nc nd dn ml ne nf dp mp li ng nh mr lm ni nj mt lq nk nl mv nm bi translated">混淆矩阵</h2><p id="18a3" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">混淆矩阵)是允许算法性能可视化的表格。该表通常有两行和两列，报告假阳性、假阴性、真阳性和真阴性的数量(如果您不理解这些术语，请查看此<a class="ae ky" href="https://stackoverflow.com/questions/14117997/what-does-recall-mean-in-machine-learning" rel="noopener ugc nofollow" target="_blank">链接</a>中的图表)</p><p id="5224" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了获得混淆矩阵，我们需要真实标签和预测标签。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="2d0e" class="ma mb it lw b gy mc md l me mf">from sklearn.metrics import confusion_matrix</span><span id="2b3c" class="ma mb it lw b gy nn md l me mf">conf_mat = confusion_matrix(test_y, <br/>                            svc.predict(test_x_vector), <br/>                            labels=['positive', 'negative'])</span></pre><p id="21fe" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">运行下面的代码后，我们将获得下面的数组作为输出。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="0a38" class="ma mb it lw b gy mc md l me mf">array([[290,  45],<br/>       [ 60, 265]])</span></pre><p id="d9d3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要理解这意味着什么，请查看下面的图片。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ps"><img src="../Images/ea3558d9bd4bd5376a488e6eeec1d3e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:712/format:webp/0*2n4ZVmzqL3gmF_6c.png"/></div></figure><p id="d6d2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，数组的每个元素代表混淆矩阵中的四个方块之一(例如，我们的模型检测到290个真阳性)</p><h1 id="9f73" class="mg mb it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">调整模型</h1><p id="f717" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">最后，是时候最大化我们模型的性能了。</p><h2 id="bc36" class="ma mb it bd mh nc nd dn ml ne nf dp mp li ng nh mr lm ni nj mt lq nk nl mv nm bi translated">GridSearchCV</h2><p id="2b59" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">这种技术包括对特定参数的彻底搜索，以便获得超参数的最佳值。为此，我们编写以下代码。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="489f" class="ma mb it lw b gy mc md l me mf">from sklearn.model_selection import GridSearchCV</span><span id="3008" class="ma mb it lw b gy nn md l me mf">#set the parameters<br/>parameters = {‘C’: [1,4,8,16,32] ,’kernel’:[‘linear’, ‘rbf’]}<br/>svc = SVC()<br/>svc_grid = GridSearchCV(svc,parameters, cv=5)<br/><br/>svc_grid.fit(train_x_vector, train_y)</span></pre><p id="8226" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如你所看到的代码，它与我们为适应SVM模型而编写的代码没有太大的不同；然而，现在我们指定一些参数来获得最佳模型。</p><p id="dce1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在拟合模型之后，我们用下面的代码获得了最好的分数、参数和估计量。</p><pre class="kj kk kl km gt lv lw lx ly aw lz bi"><span id="6c41" class="ma mb it lw b gy mc md l me mf">IN [0]: print(svc_grid.best_params_)<br/>        print(svc_grid.best_estimator_)</span><span id="0165" class="ma mb it lw b gy nn md l me mf">OUT [0]: {'C': 1, 'kernel': 'linear'}<br/>         SVC(C=1, kernel='linear')</span></pre></div><div class="ab cl pk pl hx pm" role="separator"><span class="pn bw bk po pp pq"/><span class="pn bw bk po pp pq"/><span class="pn bw bk po pp"/></div><div class="im in io ip iq"><p id="2eed" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="no">就是这样！现在您已经准备好使用sklearn创建自己的机器学习模型了！本指南所写的所有代码都可以在我的</em> <a class="ae ky" href="https://github.com/ifrankandrade/data-science-projects.git" rel="noopener ugc nofollow" target="_blank"> <em class="no"> Github </em> </a> <em class="no">上找到。</em></p><p id="2ebd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面你可以找到一些我用Python做过的项目。</p><div class="oc od gp gr oe of"><a href="https://medium.datadriveninvestor.com/can-we-beat-the-bookies-predicting-football-games-with-a-simple-model-43fe020fb384" rel="noopener  ugc nofollow" target="_blank"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd iu gy z fp ok fr fs ol fu fw is bi translated">我们能打败庄家吗？用简单模型预测足球比赛</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">预测英超、西甲、意甲和德甲的足球比赛结果。</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">medium.datadriveninvestor.com</p></div></div><div class="oo l"><div class="pt l oq or os oo ot ks of"/></div></div></a></div><div class="oc od gp gr oe of"><a href="https://medium.com/swlh/web-scraping-basics-scraping-a-betting-site-in-10-minutes-8e0529509848" rel="noopener follow" target="_blank"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd iu gy z fp ok fr fs ol fu fw is bi translated">网络抓取基础——10分钟内抓取一个赌博网站</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">在这10分钟的教程中，我将向你展示如何用Python刮网站，即使你根本不编码！</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">medium.com</p></div></div><div class="oo l"><div class="pu l oq or os oo ot ks of"/></div></div></a></div><div class="oc od gp gr oe of"><a rel="noopener follow" target="_blank" href="/the-best-movies-to-learn-english-according-to-data-science-2dccb4b3ee23"><div class="og ab fo"><div class="oh ab oi cl cj oj"><h2 class="bd iu gy z fp ok fr fs ol fu fw is bi translated">学习外语的最佳电影</h2><div class="om l"><h3 class="bd b gy z fp ok fr fs ol fu fw dk translated">根据数据科学</h3></div><div class="on l"><p class="bd b dl z fp ok fr fs ol fu fw dk translated">towardsdatascience.com</p></div></div><div class="oo l"><div class="pv l oq or os oo ot ks of"/></div></div></a></div><p id="31be" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://frankandrade.ck.page/bd063ff2d3" rel="noopener ugc nofollow" target="_blank"> <strong class="lb iu">与3k以上的人一起加入我的电子邮件列表，获取我在所有教程中使用的Python for Data Science备忘单(免费PDF) </strong> </a></p></div></div>    
</body>
</html>