<html>
<head>
<title>Text Representation for Data Science and Text Mining</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">数据科学和文本挖掘的文本表示</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/text-representation-for-data-science-and-text-mining-719ce81f3c84?source=collection_archive---------22-----------------------#2021-04-22">https://towardsdatascience.com/text-representation-for-data-science-and-text-mining-719ce81f3c84?source=collection_archive---------22-----------------------#2021-04-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><figure class="gl gn jr js jt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/da2b85ae69126353709b42f6162377b3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*u6YtX--MtFWNHmjJ"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">照片由来自Unsplash的Pietro Jeng拍摄</p></figure><p id="06fb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi le translated"><span class="l lf lg lh bm li lj lk ll lm di"> W </span>对于数据科学相关项目来说，处理文本数据是最令人兴奋的事情之一。20年前，处理和存储文本数据对于许多组织来说几乎是不可避免的，并且许多数据管道将围绕这种类型的数据，这似乎是不可能的。</p><p id="7613" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">奇怪的是，为特征处理或数据科学算法存储文本数据并不像人们想象的那样自然。首先，在计算机中，文本主要由二进制表示法表示——一个句子或一个文档主要被解释为一串与二进制表示法有某种关系的字符。</p><p id="2d06" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这似乎使得在数据管道、建模或决策过程中使用文本数据变得极其困难。幸运的是，有一些技术可以用来将文本表示为数学数组，然后可以编码成算法，甚至可以将我的数据变成大多数分析的圣杯，表格数据。</p><p id="4cf1" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一个问题出现了——我应该如何将我的文本表示为表格数据？这可能吗？幸运的是，有很多技术可以用来提供这种文本表示，接下来让我们探索其中的三种。</p><figure class="ln lo lp lq gt ju gh gi paragraph-image"><div role="button" tabindex="0" class="jv jw di jx bf jy"><div class="gh gi jq"><img src="../Images/e6d2742c97de19ae02e073259c8244bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*IVTGhaFOeJg3u05x"/></div></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">计算机并不真正理解像你和我这样的书中的文本(图片来自Unsplash的<a class="ae kf" href="https://unsplash.com/@sincerelymedia" rel="noopener ugc nofollow" target="_blank">真诚媒体</a></p></figure></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h2 id="6911" class="ly lz it bd ma mb mc dn md me mf dp mg kr mh mi mj kv mk ml mm kz mn mo mp mq bi translated">二元矢量器</h2><p id="5ba4" class="pw-post-body-paragraph kg kh it ki b kj mr kl km kn ms kp kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">我们要讨论的第一项技术非常简单，至今仍在自然语言处理管道中广泛使用— <strong class="ki iu">二进制矢量器。</strong></p><p id="25b6" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们来想象下面两句话:</p><blockquote class="mw mx my"><p id="adcd" class="kg kh mz ki b kj kk kl km kn ko kp kq na ks kt ku nb kw kx ky nc la lb lc ld im bi translated">“我去了杂货店”</p><p id="bb69" class="kg kh mz ki b kj kk kl km kn ko kp kq na ks kt ku nb kw kx ky nc la lb lc ld im bi translated">我去了电影院</p></blockquote><p id="d59b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们想用表格或数组格式表示这两个句子，我们可以首先从语料库中提取不同的单词(语料库通常是指文本的集合)。让我们使用Python代码来做这件事(我将在整篇文章中使用它):</p><pre class="ln lo lp lq gt nd ne nf ng aw nh bi"><span id="c30d" class="ly lz it ne b gy ni nj l nk nl">sentence_1 = 'I went to the grocery store'<br/>sentence_2 = 'I went to the movie theater'</span><span id="9b9e" class="ly lz it ne b gy nm nj l nk nl">vocab = set(<br/>   list(<br/>      sentence_1.split(' ')+sentence_2.split(' ')<br/>  )<br/>)</span></pre><p id="a6aa" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的vocab对象现在包含了语料库中的不同单词:</p><ul class=""><li id="2736" class="nn no it ki b kj kk kn ko kr np kv nq kz nr ld ns nt nu nv bi translated">我去了杂货店、商店、电影院、剧院</li></ul><p id="cf1a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果我们订购我们的vocab:</p><pre class="ln lo lp lq gt nd ne nf ng aw nh bi"><span id="36c5" class="ly lz it ne b gy ni nj l nk nl">vocab.sort()</span></pre><p id="e36f" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们得到一个包含以下元素的列表:</p><pre class="ln lo lp lq gt nd ne nf ng aw nh bi"><span id="d664" class="ly lz it ne b gy ni nj l nk nl">grocery, I, movie, store, the, theater, to, went</span></pre><p id="a5cb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们继续用我们的vocab的元素数目创建一个零数组，其中我们的列表中位置<em class="mz"> j </em>的每个单词<em class="mz"> w </em>将被映射到我们的数组的位置<em class="mz"> j </em>:</p><pre class="ln lo lp lq gt nd ne nf ng aw nh bi"><span id="fbcf" class="ly lz it ne b gy ni nj l nk nl">import numpy as np<br/>array_words = np.zeros(len(vocab))</span></pre><p id="89e2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们的示例数组将包含以下元素:</p><pre class="ln lo lp lq gt nd ne nf ng aw nh bi"><span id="9681" class="ly lz it ne b gy ni nj l nk nl">[0, 0, 0, 0, 0, 0, 0]</span></pre><p id="fab8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当单词<em class="mz"> w </em>出现在句子中时，我们可以通过将每个元素<em class="mz"> j </em>变为1来将我们的句子映射到这个数组中——让我们从第一个句子<em class="mz">“我去了杂货店”— </em>开始，并相应地更新我们的数组:</p><pre class="ln lo lp lq gt nd ne nf ng aw nh bi"><span id="113a" class="ly lz it ne b gy ni nj l nk nl">[1, 1, 0, 1, 1, 0, 1, 1]</span></pre><p id="c663" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">同时可视化我们的vocab列表和数组将使这一点更加明确:</strong></p><pre class="ln lo lp lq gt nd ne nf ng aw nh bi"><span id="0d3c" class="ly lz it ne b gy ni nj l nk nl">grocery, I, movie, store, the, theater, to, went<br/>[1,      1,     0,     1,   1,       0,  1,    1]</span></pre><p id="4755" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，只有我们的句子中没有的单词被设置为0——这是将句子映射到数字数组的一种非常简单的方法。让我们检查用同样的逻辑为第二句话生成的数组:</p><pre class="ln lo lp lq gt nd ne nf ng aw nh bi"><span id="9859" class="ly lz it ne b gy ni nj l nk nl">grocery, I, movie, store, the, theater, to, went<br/>[0,      1,     1,     0,   1,       1,  1,    1]</span></pre><p id="bfb9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在numpy中创建这两个数组:</p><pre class="ln lo lp lq gt nd ne nf ng aw nh bi"><span id="d31e" class="ly lz it ne b gy ni nj l nk nl">array_words = np.array([<br/>  [1,1,0,1,1,0,1,1],<br/>  [0,1,1,0,1,1,1,1]<br/>])</span></pre><p id="254b" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们现在对我们的句子有了一个简单的数学表示——幸运的是，我们不必像<em class="mz"> scikit-learn </em>一样为特定语料库中的所有句子手工做这件事，这是在<em class="mz"> feature_extraction.text </em>模块中一个名为<strong class="ki iu"> <em class="mz"> CountVectorizer </em> </strong>的函数中实现的。</p><p id="5da4" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">想象我们有一个<strong class="ki iu">列表</strong>的句子:</p><pre class="ln lo lp lq gt nd ne nf ng aw nh bi"><span id="4a17" class="ly lz it ne b gy ni nj l nk nl">sentence_list = ['I went to the grocery store',<br/>'I went to the movie theater']</span></pre><p id="9ff2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们可以定义<em class="mz">计数矢量器</em>对象，将<em class="mz">二进制</em>设置为<em class="mz">真</em>(剧透一下，这是在纯<em class="mz">计数矢量器</em>和<em class="mz">二进制矢量器</em>之间划线的参数！)和<em class="mz"> tokenizer </em>等于<em class="mz"> str.split — </em>不要太在意最后一个选项，它只是一种模拟我们之前所做的相同数组的方式(如果没有这个选项，在矢量器的<em class="mz"> scikit-learn </em>实现中，默认情况下会从输出中删除单个字母，因此将从输出中删除"<em class="mz"> I" </em>):</p><pre class="ln lo lp lq gt nd ne nf ng aw nh bi"><span id="9722" class="ly lz it ne b gy ni nj l nk nl">from sklearn.feature_extraction.text import CountVectorizer<br/>cvec = CountVectorizer(<!-- -->tokenizer=str.split, <!-- -->binary=True)</span></pre><p id="8df2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">然后在我们的列表上应用一个<em class="mz"> fit_transform </em>(例如，我们也可以将列作为一个有几个句子的dataframe的参数),这将产生一个数组，就像我们之前手动完成的那样:</p><pre class="ln lo lp lq gt nd ne nf ng aw nh bi"><span id="68d2" class="ly lz it ne b gy ni nj l nk nl">cvec.fit_transform(sentence_list).todense()</span></pre><p id="ca6e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">注意在<em class="mz"> fit_transform </em>方法之后调用的<em class="mz"> todense() </em>方法。我们这样做是因为最初<em class="mz"> fit_transform </em>由于空间压缩的问题以<strong class="ki iu">稀疏矩阵格式保存结果对象— </strong>接下来我们将对此进行更多讨论。</p><p id="4a77" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看看上面的指令生成的数组:</p><pre class="ln lo lp lq gt nd ne nf ng aw nh bi"><span id="86f7" class="ly lz it ne b gy ni nj l nk nl">[[1, 1, 0, 1, 1, 0, 1, 1],<br/> [0, 1, 1, 0, 1, 1, 1, 1]]</span></pre><p id="0647" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">听着耳熟？这和我们以前手工制作的是一样的！你可以将这种方法推广到你所拥有的任何一组句子或文件中。</strong></p><p id="c4c8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们提出一个如下句子的问题:</p><blockquote class="mw mx my"><p id="9b93" class="kg kh mz ki b kj kk kl km kn ko kp kq na ks kt ku nb kw kx ky nc la lb lc ld im bi translated">我去了杂货店，然后去了自行车店</p></blockquote><p id="17bd" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">单词<strong class="ki iu">、【去了】、【到了】、</strong>和<strong class="ki iu">、【商店】、</strong>在我们的句子中出现了两次。在二进制矢量器方法中，数组只标记单词在句子中的存在(1)或不存在(0)。对于一个NLP应用程序来说，拥有单词的实际数量可能是有意义的——让我们看看如何通过一个简单的改变来做到这一点。</p><h2 id="0491" class="ly lz it bd ma mb mc dn md me mf dp mg kr mh mi mj kv mk ml mm kz mn mo mp mq bi translated">计数矢量器</h2><p id="0f64" class="pw-post-body-paragraph kg kh it ki b kj mr kl km kn ms kp kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">计数矢量器与上面的方法非常相似。我们计算单词在句子中的出现次数，而不是用1和0来标记单词的存在。</p><p id="4b39" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">从上面的例子来看，<strong class="ki iu">我们需要添加一些单词到我们的词汇库中，</strong>因为我们在第三个句子中有一些前两个句子中没有的新单词。</p><p id="d0ad" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">回想一下我们的第一个vocab对象:</p><pre class="ln lo lp lq gt nd ne nf ng aw nh bi"><span id="f0d6" class="ly lz it ne b gy ni nj l nk nl">grocery, I, movie, store, the, theater, to, went</span></pre><p id="6cdd" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">我们再加上<strong class="ki iu"><em class="mz">‘和’，‘然后’</em></strong><strong class="ki iu"><em class="mz">‘自行车’</em></strong><em class="mz">:</em></p><pre class="ln lo lp lq gt nd ne nf ng aw nh bi"><span id="81e0" class="ly lz it ne b gy ni nj l nk nl">and, bike, grocery, I, movie, store, the, theater, then, to, went</span></pre><p id="9e4e" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这将增加我们的数组的大小！让我们将句子<em class="mz">‘我去了杂货店，然后去了自行车店’</em>映射到源自vocab的新数组，保持二进制格式:</p><pre class="ln lo lp lq gt nd ne nf ng aw nh bi"><span id="40fb" class="ly lz it ne b gy ni nj l nk nl">and, bike, grocery, I, movie, store, the, theater, then, to, went<br/>[1,     1,       1, 1,     0,     1,   1,       0,    1,  1,    1]</span></pre><p id="bbb8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在，如果不是有一个<strong class="ki iu">二进制矢量器</strong>，而是有一个<strong class="ki iu">计数矢量器</strong>对单词进行计数，那么我们有如下结果:</p><pre class="ln lo lp lq gt nd ne nf ng aw nh bi"><span id="d48b" class="ly lz it ne b gy ni nj l nk nl">and, bike, grocery, I, movie, store, the, theater, then, to, went<br/>[1,     1,       1, 1,     0,     2,   2,       0,    1,  2,   2]</span></pre><p id="8b42" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">不同之处在于，对于在句子中出现两次的每个单词，数组的值都是2。在某些模型中，以这种方式构建要素阵列可能会产生更好的结果。</p><p id="8307" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这里，对于单词<em class="mz"/><strong class="ki iu"><em class="mz">【store】</em></strong><em class="mz"/><strong class="ki iu"><em class="mz">【the】【to】</em></strong>和<strong class="ki iu"><em class="mz">【gotten】</em></strong>，这个句子数组将显示一个更高的值——如果这对您的NLP应用程序有好处，这确实取决于您希望您的数组如何传达来自语料库的信息以及您正在构建的模型的类型。</p><p id="f77d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mz"> scikit-learn </em>中的实现与我们之前所做的非常相似:</p><pre class="ln lo lp lq gt nd ne nf ng aw nh bi"><span id="d579" class="ly lz it ne b gy ni nj l nk nl">cvec_pure = CountVectorizer(<!-- -->tokenizer=str.split, <!-- -->binary=False)</span></pre><p id="cfb3" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><em class="mz">二进制，</em>在这种情况下，被设置为<em class="mz">假的</em>和<em class="mz"> </em>将产生一个更“纯”的计数矢量器。<em class="mz"> Binary=False </em>实际上是<em class="mz"> CountVectorizer </em>对象的默认参数，如果你在调用函数时没有声明参数的话。</p><p id="3fbd" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">更新我们的句子列表:</p><pre class="ln lo lp lq gt nd ne nf ng aw nh bi"><span id="b135" class="ly lz it ne b gy ni nj l nk nl">sentence_list = ['I went to the grocery store',<br/>'I went to the movie theater',<br/>'I went to the grocery store and then went to the bike store']</span></pre><p id="5a73" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">并将我们新的计数矢量器应用于我们的句子:</p><pre class="ln lo lp lq gt nd ne nf ng aw nh bi"><span id="5c7b" class="ly lz it ne b gy ni nj l nk nl">cvec_pure.fit_transform(sentence_list).todense()</span></pre><p id="f77a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">这是我们得到的三个句子的数组:</p><pre class="ln lo lp lq gt nd ne nf ng aw nh bi"><span id="443b" class="ly lz it ne b gy ni nj l nk nl">[[0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1],<br/> [0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1],<br/> [1, 1, 1, 1, 0, 2, 2, 0, 1, 2, 2]]</span></pre><p id="e455" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">正如你已经注意到的，我们的数组的大小(我们的<em class="mz"> vocab </em>中的<em class="mz"> w </em>的字数)可以快速增加</strong>。这可能会导致一些问题，但我们会在文章的最后解决一个我们可以使用的调整。</p><h2 id="e1c9" class="ly lz it bd ma mb mc dn md me mf dp mg kr mh mi mj kv mk ml mm kz mn mo mp mq bi translated">TF-IDF</h2><p id="cbc0" class="pw-post-body-paragraph kg kh it ki b kj mr kl km kn ms kp kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">我们看到的方法，从来没有把语料库作为一个整体来考虑——我们总是独立地看句子，并假设每个文本与语料库中的其他句子或文档不相关。</p><p id="1827" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">在为特征生成数组时，将语料库作为一个整体的一种常见方法是使用<strong class="ki iu">术语频率-逆文档频率矩阵方法，或通常称为TF-IDF。</strong></p><p id="4414" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">TF-IDF的公式似乎令人望而生畏，但它实际上非常简单——我们将使用该公式最简单的实现(还有其他版本，如平滑的版本，详细信息:<a class="ae kf" href="https://stats.stackexchange.com/questions/166812/why-add-one-in-inverse-document-frequency" rel="noopener ugc nofollow" target="_blank">https://stats . stack exchange . com/questions/166812/why-add-one-in-inverse-document-frequency</a>):</p><figure class="ln lo lp lq gt ju gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/02ecadc8f0218668dfe9347140176997.png" data-original-src="https://miro.medium.com/v2/resize:fit:806/format:webp/0*81xSCHVKgdQBeLfK.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">TF-IDF公式(来源:<a class="ae kf" href="https://www.searchenginejournal.com/tf-idf-can-it-really-help-your-seo/331075/#close" rel="noopener ugc nofollow" target="_blank">https://www . search engine journal . com/TF-IDF-can-it-really-help-your-SEO/331075/# close</a>)</p></figure><p id="f7c9" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意，这个等式中有几项—让我们从第一项开始，<em class="mz">I在j中出现的次数— </em> <strong class="ki iu">这里我们要映射特定单词在特定文本中出现的次数。</strong>回到我们三句话的例子:</p><pre class="ln lo lp lq gt nd ne nf ng aw nh bi"><span id="2f12" class="ly lz it ne b gy ni nj l nk nl">sentence_list = ['I went to the grocery store',<br/>'I went to the movie theater',<br/>'I went to the grocery store and then went to the bike store']</span></pre><p id="b3be" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们获得第三个<em class="mz">句子中单词<em class="mz"> store(我们称之为i) </em>的TF-IDF得分(我们称之为j)。</em> <strong class="ki iu">单词<em class="mz"> i </em>在正文<em class="mz"> j中出现了多少次？</em> </strong></p><p id="6453" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">答案是2！在这个句子中，我们有两个单词存储。我们可以更新我们的公式，因为我们知道第一项:</p><figure class="ln lo lp lq gt ju gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/27944e35704107810681c33aab58317b.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/format:webp/1*78sdUaHD10MxYttWO2Z3jA.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">更新了第一项的j(第3句)中I(商店)的TFIDF权重公式</p></figure><p id="330d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">现在让我们计算等式的右边——我们需要单词<em class="mz"> i </em>在我们语料库中的所有文档中出现的次数。在这种情况下，单词<em class="mz"> store </em>出现在<strong class="ki iu">我们拥有的两个文档中— </strong>我们还可以计算N，它是我们拥有的文档/句子的数量:</p><figure class="ln lo lp lq gt ju gh gi paragraph-image"><div class="gh gi ny"><img src="../Images/ea765b11a618f24b89820fddf5a78c75.png" data-original-src="https://miro.medium.com/v2/resize:fit:320/format:webp/1*nrCTevNUcRkH2EMkRAe7xA.png"/></div><p class="kb kc gj gh gi kd ke bd b be z dk translated">j中i(store)的TFIDF权重公式(第3句)</p></figure><p id="b766" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">该公式的返回值约为<em class="mz"> 0.81，</em> </strong> <em class="mz"> </em>这是为第三句中的单词<em class="mz"> store </em>考虑的TF-IDF得分<strong class="ki iu">—<strong class="ki iu">该值将替换我们的二进制矢量器中的潜在值1或数组中该值的计数矢量器2。</strong></strong></p><p id="94cd" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">一个单词在一个特定的句子中权重如何？两个假设:</p><ul class=""><li id="6ae4" class="nn no it ki b kj kk kn ko kr np kv nq kz nr ld ns nt nu nv bi translated">a)单词<em class="mz"> i </em>在文档<em class="mz"> j. </em>中出现的频率更高</li><li id="dc5f" class="nn no it ki b kj nz kn oa kr ob kv oc kz od ld ns nt nu nv bi translated">b)该词在整个语料库中较为少见。</li></ul><p id="f504" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">模拟两种场景，从场景a开始)——如果单词<em class="mz"> store </em>在我们的文本中出现4次，我们的得分会更高:</p><figure class="ln lo lp lq gt ju gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/0b0957052af21e0395165f6b1f68d4f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:398/format:webp/1*3fcXhjsFjRlyGMb1IXk60g.png"/></div></figure><p id="8dbc" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于场景b ),如果文本中的特定单词在语料库中更少见，您还可以提高该单词的TF-IDF分数——让我们想象单词store只出现在我们的1个句子中，第一个词的值固定为2:</p><figure class="ln lo lp lq gt ju gh gi paragraph-image"><div class="gh gi of"><img src="../Images/38ccf4a86677e7d41303846ab4b0e7ff.png" data-original-src="https://miro.medium.com/v2/resize:fit:396/format:webp/1*sGOQ_14qTZD7tOHC3sSEEA.png"/></div></figure><p id="6645" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">随着单词在语料库中变得越来越少，该单词和句子的TF-IDF分数会变得更高。这是与我们之前看到的方法的一个相关区别，我们之前看到的方法没有考虑单词在整个语料库中的分布。</p><p id="2850" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">当然，当我们转到分布领域时，我们有一些缺点——如果在部署NLP应用程序后，您的群体发生了很大变化(这意味着，某些单词在我们的语料库中的预期出现次数)，这可能会对您的应用程序产生重大影响。</p><p id="dc12" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">同样，我们不需要自己做所有的计算！在<em class="mz"> scikit-learn </em>中有一个很酷的实现，我们可以使用:</p><pre class="ln lo lp lq gt nd ne nf ng aw nh bi"><span id="0be3" class="ly lz it ne b gy ni nj l nk nl">from sklearn.feature_extraction.text import TfidfVectorizer<br/>tf_idf = TfidfVectorizer(tokenizer=str.split)</span></pre><p id="22e2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">查看我们的TFIDF数组的第三句话，我们上面计算的一个和相应的vocab:</p><pre class="ln lo lp lq gt nd ne nf ng aw nh bi"><span id="abf3" class="ly lz it ne b gy ni nj l nk nl">['and', 'bike', 'grocery', 'i', 'movie', 'store', 'the', 'theater', 'then', 'to', 'went']<br/>[0.31 ,  0.31 ,     0.236,0.18,       0,   0.471, 0.366,         0, <br/>0.31 , 0.366, 0.366]</span></pre><p id="36f7" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">请注意<em class="mz">商店</em>在句子中的TFIDF得分最高，这是因为两件事:</p><ul class=""><li id="e00a" class="nn no it ki b kj kk kn ko kr np kv nq kz nr ld ns nt nu nv bi translated">存储在该句子中重复，因此等式的左侧具有较高的值。</li><li id="adc0" class="nn no it ki b kj nz kn oa kr ob kv oc kz od ld ns nt nu nv bi translated">从重复词(<em class="mz">储存</em>、<em class="mz">到</em>、<em class="mz">、</em>、<em class="mz">到</em>)来看，<em class="mz">储存</em>是整个语料库中比较少见的词。</li></ul><p id="93e0" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">您可能还会注意到，<em class="mz"> scikit-learn </em>实现中的值介于0和1之间，并不完全是我们从您的简化公式中得到的值。这是因为<em class="mz"> scikit-learn </em>实现默认执行规格化和平滑化(你可以查看<strong class="ki iu"> <em class="mz"> norm </em> </strong>和<strong class="ki iu"> <em class="mz"> smooth_idf </em> </strong>函数的参数。</p><h2 id="e1e9" class="ly lz it bd ma mb mc dn md me mf dp mg kr mh mi mj kv mk ml mm kz mn mo mp mq bi translated">维度</h2><p id="2a93" class="pw-post-body-paragraph kg kh it ki b kj mr kl km kn ms kp kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">在所有这些方法中，正如我们在添加一个新句子时所看到的，维度变得非常快。虽然这两种方法都以稀疏格式保存数据，避免了我们在内存错误方面的麻烦(特别是如果我们使用自己的笔记本电脑工作的话)，但是这种高维数(数组中的高列数)对于许多NLP应用程序来说可能是有问题的。</p><p id="56fb" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">对于上述所有方法，scikit-learn实现有两个参数可以帮助您处理高层次的维度:</p><ul class=""><li id="8588" class="nn no it ki b kj kk kn ko kr np kv nq kz nr ld ns nt nu nv bi translated"><strong class="ki iu"> min_df: </strong>接收一个整数值，该值作为N个文档的最小数量的阈值(或者百分比，如果您传递一个float的话)，单词必须出现在该N个文档上才能被认为是一个数组列。</li><li id="a784" class="nn no it ki b kj nz kn oa kr ob kv oc kz od ld ns nt nu nv bi translated"><strong class="ki iu"> max_features: </strong>接收一个整数，该整数设置您允许数组拥有的最大列数。</li></ul><p id="bfcd" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu">这两种方法都会丢失信息，</strong>在我们的NLP管道中使用它们，一如既往地取决于您的应用。</p><p id="84c8" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">让我们看一个例子，我们的计数矢量器的min_df设置为2:</p><pre class="ln lo lp lq gt nd ne nf ng aw nh bi"><span id="690c" class="ly lz it ne b gy ni nj l nk nl">cvec_limit = CountVectorizer(<!-- -->tokenizer=str.split, <!-- -->binary=False, min_df=2)</span></pre><p id="b74a" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">返回的数组没有<strong class="ki iu">大小的vocab，而是只有出现在我们下面的两个tweets中的单词:</strong></p><pre class="ln lo lp lq gt nd ne nf ng aw nh bi"><span id="58a3" class="ly lz it ne b gy ni nj l nk nl">sentence_list = ['I went to the grocery store',<br/>'I went to the movie theater',<br/>'I went to the grocery store and then went to the bike store']</span></pre><p id="3a72" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">如果您检查特性名称，您只有一个6列的数组，对应于单词:</p><pre class="ln lo lp lq gt nd ne nf ng aw nh bi"><span id="5782" class="ly lz it ne b gy ni nj l nk nl">['grocery', 'i', 'store', 'the', 'to', 'went']</span></pre><p id="d118" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">而这些词恰恰是那些只出现在我们三条推文中至少两条的词！</p><p id="5741" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">用<strong class="ki iu"> max_features </strong>参数做同样的实验，检查你是否能够理解它背后的直觉！</p></div><div class="ab cl lr ls hx lt" role="separator"><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw lx"/><span class="lu bw bk lv lw"/></div><div class="im in io ip iq"><h2 id="5091" class="ly lz it bd ma mb mc dn md me mf dp mg kr mh mi mj kv mk ml mm kz mn mo mp mq bi translated">结论</h2><p id="7f5b" class="pw-post-body-paragraph kg kh it ki b kj mr kl km kn ms kp kq kr mt kt ku kv mu kx ky kz mv lb lc ld im bi translated">首先，也是最重要的，这里有一个小要点，你可以用在你的项目中:</p><figure class="ln lo lp lq gt ju"><div class="bz fp l di"><div class="og oh l"/></div></figure><p id="0305" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">可以对你的文本做更多的事情来避免维数问题，也可以对你的文本进行预处理。</p><p id="627c" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">例如，常见的预处理技术包括对句子进行词干化/词尾化或删除停用词——但是请记住——每次对文本进行预处理，都会丢失信息！在计算你的特征时，一定要考虑到这一点，并且一定要将你的文本数据限定在最终目标本身的范围内(分类模型、计算单词向量、使用递归神经网络等)。).</p><p id="413d" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated">除了我向你展示的技术，更多的研究正在进行中——例如将单词向量(【https://en.wikipedia.org/wiki/Word_embedding】)转换成句子或文档向量，但我们将在另一篇文章中讨论这个问题！</p><blockquote class="mw mx my"><p id="d61f" class="kg kh mz ki b kj kk kl km kn ko kp kq na ks kt ku nb kw kx ky nc la lb lc ld im bi translated">感谢你花时间阅读这篇文章！你可以在LinkedIn(【https://www.linkedin.com/in/ivobernardo/】)上加我，也可以查看我公司的网站(<a class="ae kf" href="https://daredata.engineering/home" rel="noopener ugc nofollow" target="_blank"/>)。</p><p id="d9b3" class="kg kh mz ki b kj kk kl km kn ko kp kq na ks kt ku nb kw kx ky nc la lb lc ld im bi translated"><em class="it">如果你有兴趣获得分析方面的培训，你也可以访问我在Udemy上的页面(</em><a class="ae kf" href="https://www.udemy.com/user/ivo-bernardo/" rel="noopener ugc nofollow" target="_blank"><em class="it">https://www.udemy.com/user/ivo-bernardo/</em></a><em class="it">)</em></p></blockquote><p id="acc2" class="pw-post-body-paragraph kg kh it ki b kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc ld im bi translated"><strong class="ki iu"> <em class="mz">这个例子摘自我在Udemy平台</em> </strong>  <strong class="ki iu"> <em class="mz">上为绝对初学者开设的</em> </strong> <a class="ae kf" href="https://www.udemy.com/course/nlp_natural_language_processing_python_beginners/?couponCode=LEARN_NLP" rel="noopener ugc nofollow" target="_blank"> <strong class="ki iu"> <em class="mz"> NLP课程——该课程适合初学者和希望学习自然语言处理基础知识的人。该课程还包含50多个编码练习，使您能够在学习新概念的同时进行练习。</em>T29】</strong></a></p></div></div>    
</body>
</html>