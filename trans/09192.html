<html>
<head>
<title>Sentence Embeddings: Not enough data? Just apply dropout twice!</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">句子嵌入:没有足够的数据？只需申请退学两次！</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/sentence-embeddings-not-enough-data-just-apply-dropout-twice-e5122533786?source=collection_archive---------11-----------------------#2021-08-25">https://towardsdatascience.com/sentence-embeddings-not-enough-data-just-apply-dropout-twice-e5122533786?source=collection_archive---------11-----------------------#2021-08-25</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="166c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">评论一篇有趣的论文，该论文提出了一个简单的对比学习框架，可以产生优秀的句子嵌入</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/26c6225422b3ae5dc5c6261db0df822a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*gTMwYYIe_tQxOYQj"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">由<a class="ae kv" href="https://unsplash.com/@erol?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">埃罗尔·艾哈迈德</a>在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="b6bb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">太多的未标记数据和创建标记数据的成本太高是深度学习中的常见问题。在计算机视觉中，我们可以使用数据扩充来生成更多的标记数据或提高模型的泛化能力。但是自然语言处理中的数据扩充往往不能有效地改进模型，而且可能会损害模型。高等人(2021)提出了SimCSE，这是一种对比学习模型，利用简单而优雅的数据增强方法在监督和非监督文本相似性任务中实现SOTA结果。</p><h1 id="cf09" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">无监督SimCSE</h1><p id="f343" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">在讨论SimCSE之前，我们应该知道句子嵌入被用来表示句子的特征向量。提取句子嵌入的常见方式是使用BERT liked大型预训练语言模型来提取<code class="fe mp mq mr ms b">[CLS]</code>标记嵌入作为句子嵌入。SimCSE使用预训练的BERT或RoBERTa，在<code class="fe mp mq mr ms b">[CLS]</code>表示的顶部有一个MLP层，作为编码器来获得句子嵌入。</p><p id="5f11" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">SimCSE作为一种对比学习模型，需要输入句子的正对和负对进行训练。作者简单地使用dropout向输入句子注入噪声来生成正对，并使用其他句子作为负对。是的，他们使用了<strong class="ky ir"> dropout作为数据扩充方法</strong>！换句话说，一个输入的句子通过一个带有dropout的编码器得到第一个句子嵌入，<em class="mt"> v1 </em>再通过编码器再次传递<em class="mt">T11】句子得到第二个句子嵌入，<em class="mt"> v2 </em>得到正对(<em class="mt"> v1 </em>，<em class="mt"> v2 </em>)。</em></p><p id="9c7f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">有人可能会问，将一个句子通过同一个编码器两次，不会仅仅返回两个相似的句子嵌入吗？注意，基于transformer的编码器最初带有随机丢弃掩码，因此将一个输入句子通过编码器两次会导致两个稍微不同的句子嵌入。通过这种简单的方法，在100万对英语维基百科数据上训练的无监督SimCSE在标准语义文本相似性(STS)任务上取得了SOTA结果。</p><h1 id="2605" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">监督SimCSE</h1><p id="6367" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">除了无监督训练，高等人(2021)还利用来自<a class="ae kv" href="https://nlp.stanford.edu/projects/snli/" rel="noopener ugc nofollow" target="_blank"> SNLI </a> (Bowman等人，2015)+ <a class="ae kv" href="https://cims.nyu.edu/~sbowman/multinli/" rel="noopener ugc nofollow" target="_blank"> MNLI </a> (Williams等人，2018)数据集的标记数据，进一步研究了SimCSE的性能。所选择的数据集由标记有蕴涵、矛盾和中性的句子对组成。他们用蕴涵对来构造肯定对，而否定对来自矛盾对。实验结果表明，标记数据的引入进一步提高了性能。下图显示了无监督和有监督模型的实验结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mu"><img src="../Images/6c16e8eb74bc7e8041d28460f30a319c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2gZvCFe7dcd51nnbnO_qdw.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">高等(2021)的结果</p></figure><p id="ca1f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作者在HuggingFace的支持下在<a class="ae kv" href="https://github.com/princeton-nlp/SimCSE" rel="noopener ugc nofollow" target="_blank"> Github </a>上发布了源代码。我们可以打开一个Google Colab笔记本，快速运行回购中提供的一些例子。</p><h1 id="6f63" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">结论</h1><p id="3574" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">令人惊讶的是，我们在几乎每个深度学习模型上应用的简单辍学可以用来进行数据增强。在计算机视觉上测试相同的方法并将其与常用的图像处理方法进行比较可能会很有趣。</p><p id="342e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我个人认为无监督的SimCSE具有更大的价值，并且非常有用，因为我们经常处理大量未标注的数据。无监督学习应该是未来人工智能的趋势，因为它可以更好地利用大数据集，并避免监督学习中的常见问题，如标签噪声。</p><p id="31aa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这个故事只解释了原著的一小部分，我强烈建议你阅读原著以获得全面的理解。</p><h1 id="25d8" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">参考</h1><p id="0c95" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">塞缪尔·r·鲍曼、加博·安格利、克里斯托弗·波茨和克里斯托弗·d·曼宁。2015.用于学习自然语言推理的大型标注语料库。在<em class="mt">自然语言处理的经验方法(EMNLP) </em>中，第632–642页。</p><p id="88eb" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">高天宇、姚兴成和陈。2021.SimCSE:句子嵌入的简单对比学习。<a class="ae kv" href="https://arxiv.org/abs/2104.08821" rel="noopener ugc nofollow" target="_blank"> arXiv:2104.08821 </a>。</p><p id="e505" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">艾迪娜·威廉姆斯，尼基塔·南吉亚，塞缪尔·鲍曼。2018.通过推理理解句子的大范围挑战语料库。计算语言学协会北美分会:人类语言技术(NAACL-HLT )，第1112–1122页。</p></div></div>    
</body>
</html>