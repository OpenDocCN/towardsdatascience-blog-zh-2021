<html>
<head>
<title>Regression Analysis for Beginners — Part 2</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">初学者回归分析—第二部分</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/regression-analysis-for-beginners-using-tree-based-methods-2b65bd193a7?source=collection_archive---------3-----------------------#2021-12-25">https://towardsdatascience.com/regression-analysis-for-beginners-using-tree-based-methods-2b65bd193a7?source=collection_archive---------3-----------------------#2021-12-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="224d" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">建立鱼体重预测模型</h2><div class=""/><div class=""><h2 id="d054" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">使用基于<strong class="ak">树的算法</strong>(决策树、随机森林、XGboost)构建一个 ML 回归模型</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/f03c0e1a2ea5b4243b5f0c4119ad786f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*MpxqAHuyxztdwn73"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">在<a class="ae lh" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上<a class="ae lh" href="https://unsplash.com/@veeterzy?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> veeterzy </a>拍摄的照片</p></figure><p id="9338" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="#2c5b" rel="noopener ugc nofollow">简介</a> <br/> <a class="ae lh" href="#c735" rel="noopener ugc nofollow"> Part 2.1 构建机器学习管道</a> <br/> ∘ <a class="ae lh" href="#df15" rel="noopener ugc nofollow">第一步:收集数据</a> <br/> ∘ <a class="ae lh" href="#e32c" rel="noopener ugc nofollow">第二步:可视化数据(自问自答)</a> <br/> ∘ <a class="ae lh" href="#cbb8" rel="noopener ugc nofollow">第三步:清洗数据</a> <br/> ∘ <a class="ae lh" href="#8b21" rel="noopener ugc nofollow">第四步:训练模型</a> <br/> ∘ <a class="ae lh" href="#be25" rel="noopener ugc nofollow">第五步:评估</a><br/>∘<br/> <br/> ∘ <a class="ae lh" href="#b782" rel="noopener ugc nofollow">什么是随机森林？</a> <br/> ∘ <a class="ae lh" href="#d410" rel="noopener ugc nofollow">什么是极限梯度推进？(XGBoost) </a> <br/> ∘ <a class="ae lh" href="#f509" rel="noopener ugc nofollow">决策树 vs 随机森林 vs XGBoost </a> <br/> ∘ <a class="ae lh" href="#5f46" rel="noopener ugc nofollow">线性模型 vs 基于树的模型。</a> <br/> <a class="ae lh" href="#b5b6" rel="noopener ugc nofollow">结论</a></p><h1 id="2c5b" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">介绍</h1><p id="9c0a" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">正如我在<a class="ae lh" href="https://medium.com/@gkeretchashvili/how-to-start-your-data-science-machine-learning-journey-2af667e96d1" rel="noopener">上一篇文章</a>中解释的那样，真正的数据科学家从问题/应用的角度思考问题，并在编程语言或框架的帮助下找到解决问题的方法。在<a class="ae lh" rel="noopener" target="_blank" href="/fish-weight-prediction-regression-analysis-for-beginners-part-1-8e43b0cb07e">第一部分</a>中，鱼重估计问题是使用线性 ML 模型解决的，但是，今天我将介绍<strong class="lk jd">基于树的算法</strong>，如<strong class="lk jd">决策树、随机森林、XGBoost </strong>来解决相同的问题。在文章第 2.1 部分的前半部分，我将建立一个模型，在第 2.2 部分的后半部分，我将从理论上解释每种算法，将它们相互比较，并找出其优缺点。</p><p id="cbf0" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><a class="ae lh" href="https://www.youtube.com/watch?v=8ryqWBf8TdY&amp;t=401s" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd"> YouTube 视频</strong> </a>决策树！</p><h1 id="c735" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">第 2.1 部分构建机器学习管道</h1><p id="73f7" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">为了建立一个 ML 模型，我们需要遵循下面几乎所有模型的流水线步骤。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/de0b2864b5d86a4517f2f7542c982acb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1154/format:webp/1*R1km409ukofKmzfLMjXgqw.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">作者图片</p></figure><p id="3d1a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">由于我们正在解决的问题与之前相同，所以一些流水线步骤将是相同的，例如 1。收集数据和 2。将数据可视化。但是，其他步骤会有一些修改。</p><h2 id="df15" class="nc mf it bd mg nd ne dn mk nf ng dp mo lr nh ni mq lv nj nk ms lz nl nm mu iz bi translated"><strong class="ak">第一步:收集数据</strong></h2><p id="0ac1" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">数据是公共数据集，可以从<a class="ae lh" href="https://www.kaggle.com/aungpyaeap/fish-market" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>下载。</p><pre class="ks kt ku kv gt nn no np nq aw nr bi"><span id="602c" class="nc mf it no b gy ns nt l nu nv"><strong class="no jd">import</strong> pandas <strong class="no jd">as</strong> pd<br/><strong class="no jd">import</strong> seaborn <strong class="no jd">as</strong> sns<br/><strong class="no jd">import</strong> matplotlib.pyplot <strong class="no jd">as</strong> plt<br/><strong class="no jd">from</strong> itertools <strong class="no jd">import</strong> combinations<br/><strong class="no jd">import</strong> numpy <strong class="no jd">as</strong> np<br/>data <strong class="no jd">=</strong> pd<strong class="no jd">.</strong>read_csv("Fish.csv")</span></pre><h2 id="e32c" class="nc mf it bd mg nd ne dn mk nf ng dp mo lr nh ni mq lv nj nk ms lz nl nm mu iz bi translated"><strong class="ak">第二步:可视化数据(问自己这些问题并回答)</strong></h2><ul class=""><li id="eda5" class="nw nx it lk b ll mw lo mx lr ny lv nz lz oa md ob oc od oe bi translated">数据看起来怎么样？</li></ul><pre class="ks kt ku kv gt nn no np nq aw nr bi"><span id="dc90" class="nc mf it no b gy ns nt l nu nv">data<strong class="no jd">.</strong>head()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi of"><img src="../Images/4d0ce8869391c3b61c70a7f1bec90cd2.png" data-original-src="https://miro.medium.com/v2/resize:fit:902/format:webp/1*9Vj8B72UED9qD_LVh3Mm4g.jpeg"/></div></figure><ul class=""><li id="7bc3" class="nw nx it lk b ll lm lo lp lr og lv oh lz oi md ob oc od oe bi translated">数据是否有缺失值？</li></ul><pre class="ks kt ku kv gt nn no np nq aw nr bi"><span id="2f00" class="nc mf it no b gy ns nt l nu nv">data<strong class="no jd">.</strong>isna()<strong class="no jd">.</strong>sum()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/3c6dc22f4fd5db3f3a825278d4cdc7e7.png" data-original-src="https://miro.medium.com/v2/resize:fit:236/format:webp/1*ZzK7YXomISLxW2lJ2JEvyg.jpeg"/></div></figure><ul class=""><li id="cbd7" class="nw nx it lk b ll lm lo lp lr og lv oh lz oi md ob oc od oe bi translated">数字特征的分布是什么？</li></ul><pre class="ks kt ku kv gt nn no np nq aw nr bi"><span id="7def" class="nc mf it no b gy ns nt l nu nv">data_num <strong class="no jd">=</strong> data<strong class="no jd">.</strong>drop(columns<strong class="no jd">=</strong>["Species"])<br/><br/>fig, axes <strong class="no jd">=</strong> plt<strong class="no jd">.</strong>subplots(len(data_num<strong class="no jd">.</strong>columns)<strong class="no jd">//</strong>3, 3, figsize<strong class="no jd">=</strong>(15, 6))<br/>i <strong class="no jd">=</strong> 0<br/><strong class="no jd">for</strong> triaxis <strong class="no jd">in</strong> axes:<br/>    <strong class="no jd">for</strong> axis <strong class="no jd">in</strong> triaxis:<br/>        data_num<strong class="no jd">.</strong>hist(column <strong class="no jd">=</strong> data_num<strong class="no jd">.</strong>columns[i], ax<strong class="no jd">=</strong>axis)<br/>        i <strong class="no jd">=</strong> i<strong class="no jd">+</strong>1</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ok"><img src="../Images/10533ab2f67cb0a42d7ebdb60313cac2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*UB0lIeJkUzoq7-2vP468fg.png"/></div></div></figure><ul class=""><li id="204f" class="nw nx it lk b ll lm lo lp lr og lv oh lz oi md ob oc od oe bi translated">目标变量(重量)相对于鱼种的分布情况如何？</li></ul><pre class="ks kt ku kv gt nn no np nq aw nr bi"><span id="efa5" class="nc mf it no b gy ns nt l nu nv">sns<strong class="no jd">.</strong>displot(<br/>  data<strong class="no jd">=</strong>data,<br/>  x<strong class="no jd">=</strong>"Weight",<br/>  hue<strong class="no jd">=</strong>"Species",<br/>  kind<strong class="no jd">=</strong>"hist",<br/>  height<strong class="no jd">=</strong>6,<br/>  aspect<strong class="no jd">=</strong>1.4,<br/>  bins<strong class="no jd">=</strong>15<br/>)<br/>plt<strong class="no jd">.</strong>show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/845564b06cbc7d0ee9e9c2cbcca95579.png" data-original-src="https://miro.medium.com/v2/resize:fit:1372/format:webp/1*HsSWtod9vCX_aSE0KuBsiQ.png"/></div></figure><p id="7038" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">关于物种的目标变量分布表明，有些物种，如狗鱼，与其他物种相比具有巨大的重量。这种可视化为我们提供了关于“<strong class="lk jd">物种”</strong>特征如何用于预测的附加信息。</p><h2 id="cbb8" class="nc mf it bd mg nd ne dn mk nf ng dp mo lr nh ni mq lv nj nk ms lz nl nm mu iz bi translated">第三步:清理数据</h2><pre class="ks kt ku kv gt nn no np nq aw nr bi"><span id="1378" class="nc mf it no b gy ns nt l nu nv">from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import  LabelEncoder<br/>from sklearn.tree import DecisionTreeRegressor<br/>from sklearn.ensemble import RandomForestRegressor<br/>import xgboost as xgb<br/>from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score<br/>data_cleaned =   data.drop("Weight", axis=1)<br/>y = data['Weight']</span><span id="9b45" class="nc mf it no b gy om nt l nu nv">x_train, x_test, y_train, y_test = train_test_split(data_cleaned,y, test_size=0.2, random_state=42)<br/>print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)</span><span id="f6d7" class="nc mf it no b gy om nt l nu nv"># label encoder<br/>label_encoder = LabelEncoder()<br/>x_train['Species'] = label_encoder.fit_transform(x_train['Species'].values)<br/>x_test['Species'] = label_encoder.transform(x_test['Species'].values)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi on"><img src="../Images/5e6bd872685cf42c54c19f9c86df1aff.png" data-original-src="https://miro.medium.com/v2/resize:fit:1364/format:webp/1*pUpuULzxLHZeud03WH-acw.jpeg"/></div></figure><p id="3f60" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我们使用基于树的模型，因此我们不需要特征缩放。此外，为了将文本转换成数字，我刚刚使用<strong class="lk jd"> LabelEncoder 为每种鱼分配了唯一的数值。</strong></p><h2 id="8b21" class="nc mf it bd mg nd ne dn mk nf ng dp mo lr nh ni mq lv nj nk ms lz nl nm mu iz bi translated"><strong class="ak">第四步:训练模型</strong></h2><pre class="ks kt ku kv gt nn no np nq aw nr bi"><span id="aec8" class="nc mf it no b gy ns nt l nu nv"><strong class="no jd">def</strong> evauation_model(pred, y_val):<br/>  score_MSE <strong class="no jd">=</strong> round(mean_squared_error(pred, y_val),2)<br/>  score_MAE <strong class="no jd">=</strong> round(mean_absolute_error(pred, y_val),2)<br/>  score_r2score <strong class="no jd">=</strong> round(r2_score(pred, y_val),2)<br/>  <strong class="no jd">return</strong> score_MSE, score_MAE, score_r2score</span><span id="5b79" class="nc mf it no b gy om nt l nu nv"><strong class="no jd">def</strong> models_score(model_name, train_data, y_train, val_data,y_val):<br/>    model_list <strong class="no jd">=</strong> ["Decision_Tree","Random_Forest","XGboost_Regressor"]<br/>    <em class="oo">#model_1</em><br/>    <strong class="no jd">if</strong> model_name<strong class="no jd">==</strong>"Decision_Tree":<br/>        reg <strong class="no jd">=</strong> DecisionTreeRegressor(random_state<strong class="no jd">=</strong>42)<br/>    <em class="oo">#model_2</em><br/>    <strong class="no jd">elif</strong> model_name<strong class="no jd">==</strong>"Random_Forest":<br/>      reg <strong class="no jd">=</strong> RandomForestRegressor(random_state<strong class="no jd">=</strong>42)<br/>        <br/>    <em class="oo">#model_3</em><br/>    <strong class="no jd">elif</strong> model_name<strong class="no jd">==</strong>"XGboost_Regressor":<br/>        reg <strong class="no jd">=</strong> xgb<strong class="no jd">.</strong>XGBRegressor(objective<strong class="no jd">=</strong>"reg:squarederror",random_state<strong class="no jd">=</strong>42,)<br/>    <strong class="no jd">else</strong>:<br/>        print("please enter correct regressor name")<br/>        <br/>    <strong class="no jd">if</strong> model_name <strong class="no jd">in</strong> model_list:<br/>        reg<strong class="no jd">.</strong>fit(train_data,y_train)<br/>        pred <strong class="no jd">=</strong> reg<strong class="no jd">.</strong>predict(val_data)<br/>     <br/>        score_MSE, score_MAE, score_r2score <strong class="no jd">=</strong> evauation_model(pred,y_val)<br/>        <strong class="no jd">return</strong> round(score_MSE,2), round(score_MAE,2), round(score_r2score,2)</span><span id="f1c8" class="nc mf it no b gy om nt l nu nv">model_list <strong class="no jd">=</strong> ["Decision_Tree","Random_Forest","XGboost_Regressor"]<br/>result_scores <strong class="no jd">=</strong> []<br/><strong class="no jd">for</strong> model <strong class="no jd">in</strong> model_list:<br/>    score <strong class="no jd">=</strong> models_score(model, x_train, y_train, x_test, y_test)<br/>    result_scores<strong class="no jd">.</strong>append((model, score[0], score[1],score[2]))<br/>    print(model,score)</span></pre><p id="f5e1" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">我训练了决策树，随机森林 XGboost 并存储了所有的评估分数。</p><h2 id="be25" class="nc mf it bd mg nd ne dn mk nf ng dp mo lr nh ni mq lv nj nk ms lz nl nm mu iz bi translated"><strong class="ak">第五步:评估</strong></h2><pre class="ks kt ku kv gt nn no np nq aw nr bi"><span id="285a" class="nc mf it no b gy ns nt l nu nv">df_result_scores <strong class="no jd">=</strong> pd<strong class="no jd">.</strong>DataFrame(result_scores,columns ["model","mse","mae","r2score"])<br/>df_result_scores</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi op"><img src="../Images/4b0630423772fc82ef7d020b755af5a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1084/format:webp/1*gIkWON1UrybwB-tcS82URg.png"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">基于树的模型</p></figure><p id="115a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">结果真的很吸引人，因为你记得线性模型取得了<a class="ae lh" rel="noopener" target="_blank" href="/fish-weight-prediction-regression-analysis-for-beginners-part-1-8e43b0cb07e">低得多的结果</a>(也如下所示)。因此，在我们进行任何类型的超参数调整之前，我们可以说，在这种数据集中，所有基于树的模型都优于线性模型。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/1324110b6226de96767be28d62e282f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/format:webp/1*CMtaUrpSVYjtc6wy769mbg.jpeg"/></div><p class="ld le gj gh gi lf lg bd b be z dk translated">线性模型</p></figure><h2 id="bb44" class="nc mf it bd mg nd ne dn mk nf ng dp mo lr nh ni mq lv nj nk ms lz nl nm mu iz bi translated"><strong class="ak">第六步:使用 hyperopt 调节超参数</strong></h2><p id="f656" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">今天，我们使用<strong class="lk jd">超视</strong>来调整使用<strong class="lk jd"> </strong> <a class="ae lh" href="https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf" rel="noopener ugc nofollow" target="_blank"> <strong class="lk jd"> TPE 算法的超参数。</strong></a>TPE 算法不是从搜索空间中取随机值，而是考虑到已知某些超参数赋值(x)与其他元素的特定值无关。在这种情况下，搜索比随机搜索更有效，比贪婪搜索更快。</p><pre class="ks kt ku kv gt nn no np nq aw nr bi"><span id="b46e" class="nc mf it no b gy ns nt l nu nv"><strong class="no jd">from</strong> hyperopt <strong class="no jd">import</strong> hp<br/><strong class="no jd">from</strong> hyperopt <strong class="no jd">import</strong> fmin, tpe, STATUS_OK, STATUS_FAIL, Trials<br/><strong class="no jd">from</strong> sklearn.model_selection <strong class="no jd">import</strong> cross_val_score</span><span id="3f77" class="nc mf it no b gy om nt l nu nv">num_estimator <strong class="no jd">=</strong> [100,150,200,250]<br/><br/>space<strong class="no jd">=  </strong>{'max_depth': hp<strong class="no jd">.</strong>quniform("max_depth", 3, 18, 1),<br/>        'gamma': hp<strong class="no jd">.</strong>uniform ('gamma', 1,9),<br/>        'reg_alpha' : hp<strong class="no jd">.</strong>quniform('reg_alpha', 30,180,1),<br/>        'reg_lambda' : hp<strong class="no jd">.</strong>uniform('reg_lambda', 0,1),<br/>        'colsample_bytree' : hp<strong class="no jd">.</strong>uniform('colsample_bytree', 0.5,1),<br/>        'min_child_weight' : hp<strong class="no jd">.</strong>quniform('min_child_weight', 0, 10, 1),<br/>        'n_estimators': hp<strong class="no jd">.</strong>choice("n_estimators", num_estimator),<br/>    }<br/><br/><strong class="no jd">def</strong> hyperparameter_tuning(space):<br/>    model<strong class="no jd">=</strong>xgb<strong class="no jd">.</strong>XGBRegressor(n_estimators <strong class="no jd">=</strong> space['n_estimators'], max_depth <strong class="no jd">=</strong> int(space['max_depth']), gamma <strong class="no jd">=</strong> space['gamma'],<br/>                         reg_alpha <strong class="no jd">=</strong> int(space['reg_alpha']) , min_child_weight<strong class="no jd">=</strong>space['min_child_weight'],<br/>                         colsample_bytree<strong class="no jd">=</strong>space['colsample_bytree'], objective<strong class="no jd">=</strong>"reg:squarederror")<br/>    <br/>    score_cv <strong class="no jd">=</strong> cross_val_score(model, x_train, y_train, cv<strong class="no jd">=</strong>5, scoring<strong class="no jd">=</strong>"neg_mean_absolute_error")<strong class="no jd">.</strong>mean()<br/>    <strong class="no jd">return</strong> {'loss':<strong class="no jd">-</strong>score_cv, 'status': STATUS_OK, 'model': model}<br/><br/><br/>trials <strong class="no jd">=</strong> Trials()<br/>best <strong class="no jd">=</strong> fmin(fn<strong class="no jd">=</strong>hyperparameter_tuning,<br/>            space<strong class="no jd">=</strong>space,<br/>            algo<strong class="no jd">=</strong>tpe<strong class="no jd">.</strong>suggest,<br/>            max_evals<strong class="no jd">=</strong>200,<br/>            trials<strong class="no jd">=</strong>trials)<br/><br/>print(best)</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi or"><img src="../Images/9ce1aa2da14f3ccb4b2847bc07d792fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0BGi-633kwSM1by2oJXWhA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">最佳超参数的结果</p></figure><p id="a8cb" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">这是算法在 200 次试验后找到的最佳超参数的结果。但是，如果数据集太大，可以相应减少试验次数。</p><pre class="ks kt ku kv gt nn no np nq aw nr bi"><span id="0403" class="nc mf it no b gy ns nt l nu nv">best['max_depth'] <strong class="no jd">=</strong> int(best['max_depth']) <em class="oo"># convert to int</em><br/>best["n_estimators"] <strong class="no jd">=</strong> num_estimator[best["n_estimators"]] #assing value based on index</span><span id="534b" class="nc mf it no b gy om nt l nu nv">reg <strong class="no jd">=</strong> xgb<strong class="no jd">.</strong>XGBRegressor(<strong class="no jd">**</strong>best)<br/>reg<strong class="no jd">.</strong>fit(x_train,y_train)<br/>pred <strong class="no jd">=</strong> reg<strong class="no jd">.</strong>predict(x_test)<br/>score_MSE, score_MAE, score_r2score <strong class="no jd">=</strong> evauation_model(pred,y_test) <br/>to_append <strong class="no jd">=</strong> ["XGboost_hyper_tuned",score_MSE, score_MAE, score_r2score]<br/>df_result_scores<strong class="no jd">.</strong>loc[len(df_result_scores)] <strong class="no jd">=</strong> to_append<br/>df_result_scores</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div class="gh gi os"><img src="../Images/643c343a1fcb196639941090fb4cb23e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*hpcBO_7-jmM0N5xDq6grBQ.jpeg"/></div></figure><p id="edbe" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">结果太棒了！与其他算法相比，超调模型非常好。例如，XGboost 将 MAE 的结果从 41.65 提高到 36.33。这是一个很好的说明，超参数调整是多么强大。</p><h2 id="2b67" class="nc mf it bd mg nd ne dn mk nf ng dp mo lr nh ni mq lv nj nk ms lz nl nm mu iz bi translated"><strong class="ak">第七步:选择最佳模型和预测</strong></h2><pre class="ks kt ku kv gt nn no np nq aw nr bi"><span id="eb37" class="nc mf it no b gy ns nt l nu nv"><em class="oo"># winner</em><br/>reg <strong class="no jd">=</strong> xgb<strong class="no jd">.</strong>XGBRegressor(<strong class="no jd">**</strong>best)<br/>reg<strong class="no jd">.</strong>fit(x_train,y_train)<br/>pred <strong class="no jd">=</strong> reg<strong class="no jd">.</strong>predict(x_test)<br/>plt<strong class="no jd">.</strong>figure(figsize<strong class="no jd">=</strong>(18,7))<br/>plt<strong class="no jd">.</strong>subplot(1, 2, 1) <em class="oo"># row 1, col 2 index 1</em><br/>plt<strong class="no jd">.</strong>scatter(range(0,len(x_test)), pred,color<strong class="no jd">=</strong>"green",label<strong class="no jd">=</strong>"predicted")<br/>plt<strong class="no jd">.</strong>scatter(range(0,len(x_test)), y_test,color<strong class="no jd">=</strong>"red",label<strong class="no jd">=</strong>"True value")<br/>plt<strong class="no jd">.</strong>legend()<br/><br/>plt<strong class="no jd">.</strong>subplot(1, 2, 2) <em class="oo"># index 2</em><br/>plt<strong class="no jd">.</strong>plot(range(0,len(x_test)), pred,color<strong class="no jd">=</strong>"green",label<strong class="no jd">=</strong>"predicted")<br/>plt<strong class="no jd">.</strong>plot(range(0,len(x_test)), y_test,color<strong class="no jd">=</strong>"red",label<strong class="no jd">=</strong>"True value")<br/>plt<strong class="no jd">.</strong>legend()<br/>plt<strong class="no jd">.</strong>show()</span></pre><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ot"><img src="../Images/7e1e9435e7741b6de0fd366dae197f76.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VZzEikMigCMXVJd6asyHyw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">预测与真实</p></figure><p id="3397" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">可视化清楚地展示了预测值和真实值的接近程度，以及调优后的 XGBoost 的性能。</p></div><div class="ab cl ou ov hx ow" role="separator"><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz pa"/><span class="ox bw bk oy oz"/></div><div class="im in io ip iq"><h1 id="c23a" class="me mf it bd mg mh pb mj mk ml pc mn mo ki pd kj mq kl pe km ms ko pf kp mu mv bi translated">第 2.2 部分分析最大似然算法</h1><h2 id="70dd" class="nc mf it bd mg nd ne dn mk nf ng dp mo lr nh ni mq lv nj nk ms lz nl nm mu iz bi translated">什么是决策树？</h2><p id="92ab" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">决策树是一种受监督的 ML 算法，擅长捕捉特征和目标变量之间的非线性关系。算法背后的直觉类似于人类的逻辑。在每个节点中，该算法找到将数据分成两部分的特征和阈值。下图是一个决策树。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pg"><img src="../Images/4c22f7b579f6cf54f862295d3fd00d7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*VTBZF_CL0c0GZ0jshTTqVw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">鱼体重预测决策树</p></figure><p id="9af2" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">首先，让我们看看图中每个变量代表什么。我们以第一个节点为例。</p><p id="8e72" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">宽度≤5.154 </strong>:算法决定分割数据样本的特征和值阈值。</p><p id="ed28" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">样本= 127 </strong>:拆分前有 127 个数据点。</p><p id="86e6" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd"> value = 386.794 </strong>:预测特征(鱼重)的平均值。</p><p id="cac7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">Squared _ error = 122928.22:</strong>同 MSE(真，pred)——其中 pred 同<strong class="lk jd">值</strong>(样本的平均鱼重)。</p><p id="856a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">所以基于<strong class="lk jd">宽度≤5.154 </strong>阈值的算法将数据分成两部分。但问题是算法是怎么找到这个阈值的？有几个分割标准，因为回归任务 CART 算法试图通过以贪婪的方式搜索来找到阈值，使得两个子组的 MSE 的加权平均值最小化。</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ph"><img src="../Images/0b7eb9f919c21e66b80c16b5b304e1c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kxttYC3bHGb6jd7CchNK1A.png"/></div></div></figure><p id="5337" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">例如，在我们的案例中，在第一次分割后，两个小组的加权平均 MSE 与其他分割相比是最小的。</p><pre class="ks kt ku kv gt nn no np nq aw nr bi"><span id="16e2" class="nc mf it no b gy ns nt l nu nv">J(k,t_k) = 88/127 *20583.394 + 39/127 *75630.727 = 37487.69</span></pre><p id="a78c" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated"><strong class="lk jd">决策树的问题:</strong></p><p id="0f6f" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">树对训练数据中的微小变化非常敏感。数据的微小变化可能会导致决策树结构的重大变化。这个限制的解决方案是随机森林。</p><h2 id="b782" class="nc mf it bd mg nd ne dn mk nf ng dp mo lr nh ni mq lv nj nk ms lz nl nm mu iz bi translated"><strong class="ak">什么是随机森林？</strong></h2><p id="cc0b" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">随机森林是决策树的集合。随机森林背后的直觉是构建多个决策树，并且在每个决策树中，不是搜索最佳特征来分割数据，而是在特征的子集中搜索最佳特征，因此这提高了树的多样性。然而，它比简单的决策树更难理解。此外，它需要建立大量的树，这使得算法对于实时应用来说很慢。一般来说，算法训练起来很快，但创建预测却很慢。决策树的一个改进版本也是 XGBoost。</p><h2 id="d410" class="nc mf it bd mg nd ne dn mk nf ng dp mo lr nh ni mq lv nj nk ms lz nl nm mu iz bi translated"><strong class="ak">什么是极限梯度提升？(XGBoost) </strong></h2><p id="01d7" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">XGBoost 也是一种基于树的集成监督学习算法，它使用了<a class="ae lh" href="https://en.wikipedia.org/wiki/Gradient_boosting" rel="noopener ugc nofollow" target="_blank">梯度推进</a>框架。这种算法背后的直觉是，它试图用新的预测器来拟合前一个预测器产生的残余误差。它非常快，可伸缩，可移植。</p><h2 id="f509" class="nc mf it bd mg nd ne dn mk nf ng dp mo lr nh ni mq lv nj nk ms lz nl nm mu iz bi translated"><strong class="ak">决策树 vs 随机森林 vs XGBoost </strong></h2><p id="a25a" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">因此，在我们的实验中，XGboost 在性能方面优于其他产品。从理论上讲，决策树是最简单的基于树的算法，它有不稳定的局限性——数据的变化会引起树结构的巨大变化，但是它有很好的可解释性。随机森林和 XGboost 更复杂。区别之一是随机森林在过程结束时组合结果(多数规则)，而 XGboost 在过程中组合结果。一般来说，XGBoost 比随机森林有更好的性能，但是，当我们的数据中有很多噪声时，XGboost 不是一个好的选择，它会导致过拟合，并且比随机森林更难调整。</p><h2 id="5f46" class="nc mf it bd mg nd ne dn mk nf ng dp mo lr nh ni mq lv nj nk ms lz nl nm mu iz bi translated">线性模型与基于树的模型。</h2><ul class=""><li id="5963" class="nw nx it lk b ll mw lo mx lr ny lv nz lz oa md ob oc od oe bi translated">线性模型捕捉自变量和因变量之间的线性关系，这在现实世界的大多数情况下并非如此。然而，基于树的模型捕捉更复杂的关系。</li><li id="3c15" class="nw nx it lk b ll pi lo pj lr pk lv pl lz pm md ob oc od oe bi translated">线性模型大多数时候需要特征缩放，而基于树的模型不需要。</li><li id="ec62" class="nw nx it lk b ll pi lo pj lr pk lv pl lz pm md ob oc od oe bi translated">基于树的模型的性能是线性模型的大多数倍。我们的实验很好地说明了这一点，最佳超调优线性模型达到了 66.20 MAE，最佳基于树的模型达到了 36.33，这是一个很大的改进。</li><li id="55be" class="nw nx it lk b ll pi lo pj lr pk lv pl lz pm md ob oc od oe bi translated">基于树的算法比线性模型更容易解释。</li></ul><h1 id="b5b6" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated">结论</h1><p id="e1af" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">如前所述，哪种算法最有效并没有现成的答案，一切都取决于数据和任务。这就是为什么要测试和评估几种算法的原因。然而，了解每种算法背后的直觉、它们的优缺点以及如何应对其局限性是有益的。</p><blockquote class="pn po pp"><p id="341b" class="li lj oo lk b ll lm kd ln lo lp kg lq pq ls lt lu pr lw lx ly ps ma mb mc md im bi translated">这里是我的 GitHub 中的<a class="ae lh" href="https://github.com/gurokeretcha/Fish-Weight-Prediction-Beginners/blob/main/Fish_Weight_Prediction_(Regression_Analysis_for_beginners)%E2%80%8A_%E2%80%8APart%C2%A02.ipynb" rel="noopener ugc nofollow" target="_blank">完整代码</a>。</p><p id="3468" class="li lj oo lk b ll lm kd ln lo lp kg lq pq ls lt lu pr lw lx ly ps ma mb mc md im bi translated">你可以在<a class="ae lh" href="https://medium.com/@gkeretchashvili" rel="noopener">媒体</a>上关注我，了解最新文章。</p></blockquote><div class="pt pu gp gr pv pw"><a href="https://medium.com/@gkeretchashvili" rel="noopener follow" target="_blank"><div class="px ab fo"><div class="py ab pz cl cj qa"><h2 class="bd jd gy z fp qb fr fs qc fu fw jc bi translated">gurami keretcashvili-培养基</h2><div class="qd l"><h3 class="bd b gy z fp qb fr fs qc fu fw dk translated">一个数据科学家不是懂 python，Sklearn，TensorFlow 等的人。但是谁知道如何玩…</h3></div><div class="qe l"><p class="bd b dl z fp qb fr fs qc fu fw dk translated">medium.com</p></div></div><div class="qf l"><div class="qg l qh qi qj qf qk lb pw"/></div></div></a></div><h1 id="f0f6" class="me mf it bd mg mh mi mj mk ml mm mn mo ki mp kj mq kl mr km ms ko mt kp mu mv bi translated"><strong class="ak">参考文献</strong></h1><p id="6e0d" class="pw-post-body-paragraph li lj it lk b ll mw kd ln lo mx kg lq lr my lt lu lv mz lx ly lz na mb mc md im bi translated">[1] Stephanie Glen <a class="ae lh" href="https://www.datasciencecentral.com/profiles/blogs/decision-tree-vs-random-forest-vs-boosted-trees-explained" rel="noopener ugc nofollow" target="_blank">决策树 vs 随机森林 vs 梯度推进机器:简单解释</a> (2018)</p><p id="6d26" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[2] Vishal Morde <a class="ae lh" rel="noopener" target="_blank" href="/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d"> XGBoost 算法:愿她统治多久！</a> (2019)</p><p id="7c7a" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[3] GAURAV SHARMA，<a class="ae lh" href="https://www.analyticsvidhya.com/blog/2021/05/5-regression-algorithms-you-should-know-introductory-guide/" rel="noopener ugc nofollow" target="_blank">你应该知道的 5 种回归算法——入门指南！</a></p><p id="a3d7" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[4] Aarshay Jain，<a class="ae lh" href="https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/" rel="noopener ugc nofollow" target="_blank">使用 Python 代码在 XGBoost 中调整参数的完整指南</a></p><p id="c13b" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[5]<a class="ae lh" href="https://scikit-learn.org/" rel="noopener ugc nofollow" target="_blank">scikit-learn.org</a>，<a class="ae lh" href="https://scikit-learn.org/stable/modules/tree.html" rel="noopener ugc nofollow" target="_blank">决策树</a>，<a class="ae lh" href="https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py" rel="noopener ugc nofollow" target="_blank">了解决策树结构</a></p><p id="30da" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[6]<a class="ae lh" href="http://hyperopt.github.io/hyperopt/" rel="noopener ugc nofollow" target="_blank">hyperpt:分布式异步超参数优化</a></p><p id="7db5" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[7] XGboost，<a class="ae lh" href="https://xgboost.readthedocs.io/en/latest/parameter.html#general-parameters" rel="noopener ugc nofollow" target="_blank"> XGBoost 参数</a></p><p id="2b18" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[8]TINU·罗希斯·D，<a class="ae lh" href="https://medium.com/analytics-vidhya/hyperparameter-tuning-hyperopt-bayesian-optimization-for-xgboost-and-neural-network-8aedf278a1c9" rel="noopener"/>(2019)超参数调整—超点贝叶斯优化(Xgboost 和神经网络)</p><p id="ca11" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[9]工作管理，<a class="ae lh" href="https://www.analyticsvidhya.com/blog/2020/09/alternative-hyperparameter-optimization-technique-you-need-to-know-hyperopt/" rel="noopener ugc nofollow" target="_blank">你需要知道的替代超参数优化技术—hyperpt</a>(2020)</p><p id="a050" class="pw-post-body-paragraph li lj it lk b ll lm kd ln lo lp kg lq lr ls lt lu lv lw lx ly lz ma mb mc md im bi translated">[10] Aurelien Geron，使用 Scikit-learn 和 Tensorflow 进行动手机器学习(2019 年)</p></div></div>    
</body>
</html>