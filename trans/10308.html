<html>
<head>
<title>Statistics in Python — Collinearity and Multicollinearity</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python中的统计数据-共线性和多重共线性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/statistics-in-python-collinearity-and-multicollinearity-4cc4dcd82b3f?source=collection_archive---------1-----------------------#2021-10-01">https://towardsdatascience.com/statistics-in-python-collinearity-and-multicollinearity-4cc4dcd82b3f?source=collection_archive---------1-----------------------#2021-10-01</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="0106" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解如何发现数据集中的多重共线性</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/1382510703658cd08cfc2400faa62361.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*4HRp04AA6kstJ5FD"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@iampatrickpilz?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Valentino Funghi </a>在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="2da4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在我的上一篇文章中，您了解了数据集中数据之间的关系，无论是在同一列中(<em class="lv">方差</em>)，还是在列之间(<em class="lv">协方差</em>和<em class="lv">相关性</em>)。</p><div class="lw lx gp gr ly lz"><a rel="noopener follow" target="_blank" href="/statistics-in-python-understanding-variance-covariance-and-correlation-4729b528db01"><div class="ma ab fo"><div class="mb ab mc cl cj md"><h2 class="bd iu gy z fp me fr fs mf fu fw is bi translated">Python中的统计学-了解方差、协方差和相关性</h2><div class="mg l"><h3 class="bd b gy z fp me fr fs mf fu fw dk translated">理解你的数据之间的关系，知道皮尔逊相关系数和…</h3></div><div class="mh l"><p class="bd b dl z fp me fr fs mf fu fw dk translated">towardsdatascience.com</p></div></div><div class="mi l"><div class="mj l mk ml mm mi mn ks lz"/></div></div></a></div><p id="ef02" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当您开始机器学习之旅时，您通常会遇到的另外两个术语是:</p><ul class=""><li id="e955" class="mo mp it lb b lc ld lf lg li mq lm mr lq ms lu mt mu mv mw bi translated"><strong class="lb iu">共线性</strong></li><li id="14f6" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated"><strong class="lb iu">多重共线性</strong></li></ul><p id="119c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本文中，我想解释共线性和多重共线性的概念，以及在准备数据时理解它们并采取适当措施的重要性。</p><h1 id="6079" class="nc nd it bd ne nf ng nh ni nj nk nl nm jz nn ka no kc np kd nq kf nr kg ns nt bi translated">相关性与共线性和多重共线性</h1><p id="4a1f" class="pw-post-body-paragraph kz la it lb b lc nu ju le lf nv jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">如果您还记得，<em class="lv"> correlation </em>测量数据集中两列之间的强度和方向。相关性通常用于查找特征和目标之间的关系:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/cc348e2e61299844d9bf709800bb26f5.png" data-original-src="https://miro.medium.com/v2/resize:fit:908/format:webp/1*qfzhOZAxLnFAOeTwvlylxw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="4e9c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">例如，如果其中一个特征与目标有很高的相关性，它会告诉您这个特定的特征对目标有很大的影响，因此在训练模型时应该包括在内。</p><p id="0e48" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">另一方面，共线性</em>是两个特征线性相关的情况(高<em class="lv">相关性</em>，它们被用作目标的<em class="lv">预测值</em>。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/d3aba946af0debc64dbc478e5d96fc29.png" data-original-src="https://miro.medium.com/v2/resize:fit:926/format:webp/1*iJNjL_SZVBtn5OnOBdkd3g.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="b56f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">多重共线性</em>是共线性的一种特殊情况，其中一个要素与两个或多个要素呈现线性关系。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/d1132062b486ce3c61de952e5d82a4e9.png" data-original-src="https://miro.medium.com/v2/resize:fit:930/format:webp/1*TPT4Uu1snHwzRJVz70sJUg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h1 id="5f2c" class="nc nd it bd ne nf ng nh ni nj nk nl nm jz nn ka no kc np kd nq kf nr kg ns nt bi translated">共线性和多重共线性问题</h1><p id="1dbc" class="pw-post-body-paragraph kz la it lb b lc nu ju le lf nv jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">回想一下多元线性回归的公式:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/1ca279237922ec03c57d61bebce31c78.png" data-original-src="https://miro.medium.com/v2/resize:fit:538/format:webp/1*ii84d-FhvaRHWClzQln8Dg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="76fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">线性回归的一个重要假设是，每个预测值(<strong class="lb iu"> x </strong> ₁、<strong class="lb iu"> x </strong> ₂等)与结果<strong class="lb iu"> y </strong>之间应该存在线性关系。但是，如果预测值之间存在相关性(例如，<strong class="lb iu"> x </strong> ₁和<strong class="lb iu"> x </strong> ₂高度相关)，您将无法在保持另一个常量的情况下确定其中一个的影响，因为两个预测值会一起变化。最终的结果是系数(<strong class="lb iu"> w </strong> ₁和<strong class="lb iu"> w </strong> ₂)现在变得不那么精确，因此更难解释。</p><h1 id="1c4e" class="nc nd it bd ne nf ng nh ni nj nk nl nm jz nn ka no kc np kd nq kf nr kg ns nt bi translated">修复多重共线性</h1><p id="4c5f" class="pw-post-body-paragraph kz la it lb b lc nu ju le lf nv jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">训练机器学习模型时，在数据预处理阶段筛选出数据集中显示多重共线性的要素非常重要。你可以用一种叫做<strong class="lb iu">VIF</strong>——<strong class="lb iu">方差膨胀因子</strong>的方法来实现。</p><p id="795b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu"> VIF </strong>允许你确定各个自变量之间相关性的强度。<em class="lv">它是通过取一个变量并将其与其他变量进行回归</em>来计算的。</p><blockquote class="od oe of"><p id="d34e" class="kz la lv lb b lc ld ju le lf lg jx lh og lj lk ll oh ln lo lp oi lr ls lt lu im bi translated">VIF计算出一个系数的<strong class="lb iu">方差</strong>被<strong class="lb iu">夸大了</strong>多少，因为它与其他预测因子存在线性相关性。因此得名。</p></blockquote><p id="dad0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">VIF是这样工作的:</p><ul class=""><li id="4678" class="mo mp it lb b lc ld lf lg li mq lm mr lq ms lu mt mu mv mw bi translated">假设您有一个特征列表— <strong class="lb iu"> x </strong> ₁、<strong class="lb iu"> x </strong> ₂、<strong class="lb iu"> x </strong> ₃和<strong class="lb iu"> x </strong> ₄.</li><li id="24c8" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">你首先取第一个特征，<strong class="lb iu"> x </strong> ₁，并对其他特征进行回归:</li></ul><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="2889" class="oo nd it ok b gy op oq l or os">x₁ ~ x₂ + x₃ + x₄</span></pre><blockquote class="od oe of"><p id="74bf" class="kz la lv lb b lc ld ju le lf lg jx lh og lj lk ll oh ln lo lp oi lr ls lt lu im bi translated">实际上，你是在执行上面的多元回归。多元回归通常解释多个自变量或预测变量与一个因变量或标准变量之间的关系。</p></blockquote><ul class=""><li id="d295" class="mo mp it lb b lc ld lf lg li mq lm mr lq ms lu mt mu mv mw bi translated">在上面的多元回归中，您提取了<strong class="lb iu"> R </strong>值(在0和1之间)。如果<strong class="lb iu"> R </strong>是<em class="lv">大</em>，这意味着<strong class="lb iu"> x₁ </strong>可以从三个特征中预测出来，因此与三个特征——<strong class="lb iu">x</strong>₂、<strong class="lb iu"> x </strong> ₃、<strong class="lb iu"> x </strong> ₄.高度相关如果<strong class="lb iu"> R </strong>是<em class="lv">小</em>，这意味着<strong class="lb iu"> x₁ </strong>无法从特征中预测，因此<em class="lv">与三个特征<strong class="lb iu"> x </strong> ₂、<strong class="lb iu"> x </strong> ₃、<strong class="lb iu">x</strong>₄.</em>不相关</li><li id="b309" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">基于为<strong class="lb iu"> x₁ </strong>计算的<strong class="lb iu"> R </strong>值，现在可以使用以下公式计算其<strong class="lb iu"> VIF </strong>:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/77530a0ee3725ff14ef24ab9443e33ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:356/format:webp/1*kh_lcXAhwdfRarDKkpsSBg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><ul class=""><li id="c057" class="mo mp it lb b lc ld lf lg li mq lm mr lq ms lu mt mu mv mw bi translated">大的<strong class="lb iu"> R </strong>值(接近1)会导致分母变小(1减去一个接近1的值会得到一个接近0的数)。这将导致一个大的VIF。大VIF表示此要素与其他要素存在多重共线性。</li><li id="c4f5" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">反之，一个小的<strong class="lb iu"> R </strong>值(接近0)会导致分母变大(1减去一个接近0的值会得到一个接近1的数)。这将导致一个小VIF。小VIF表示此要素与其他要素的多重共线性较低。</li></ul><blockquote class="od oe of"><p id="fb76" class="kz la lv lb b lc ld ju le lf lg jx lh og lj lk ll oh ln lo lp oi lr ls lt lu im bi translated">(1- <strong class="lb iu"> R </strong>)又称<strong class="lb iu">公差</strong>。</p></blockquote><ul class=""><li id="cb86" class="mo mp it lb b lc ld lf lg li mq lm mr lq ms lu mt mu mv mw bi translated">对其他要素重复上述过程，并计算每个要素的VIF:</li></ul><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="ebd3" class="oo nd it ok b gy op oq l or os">x₂ ~ x₁ + x₃ + x₄   # regress x₂ against the rest of the features<br/>x₃ ~ x₁ + x₂ + x₄   # regress x₃ against the rest of the features<br/>x₄ ~ x₁ + x₂ + x₃   # regress x₄ against the rest of the features</span></pre><blockquote class="od oe of"><p id="052a" class="kz la lv lb b lc ld ju le lf lg jx lh og lj lk ll oh ln lo lp oi lr ls lt lu im bi translated">虽然相关矩阵和散点图可用于查找多重共线性，但它们仅显示独立变量之间的二元关系。另一方面，VIF显示了一个变量与一组其他变量的相关性。</p></blockquote><h1 id="566b" class="nc nd it bd ne nf ng nh ni nj nk nl nm jz nn ka no kc np kd nq kf nr kg ns nt bi translated">使用Python实现VIF</h1><p id="7b1a" class="pw-post-body-paragraph kz la it lb b lc nu ju le lf nv jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">既然你已经知道了VIF是如何计算的，你可以使用Python来实现它，在<strong class="lb iu"> sklearn </strong>的一点帮助下:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="4aee" class="oo nd it ok b gy op oq l or os">import pandas as pd<br/>from sklearn.linear_model import LinearRegression</span><span id="396a" class="oo nd it ok b gy ou oq l or os">def <strong class="ok iu">calculate_vif</strong>(df, features):    <br/>    vif, tolerance = {}, {}</span><span id="f09e" class="oo nd it ok b gy ou oq l or os">    # all the features that you want to examine<br/>    for feature in features:<br/>        # extract all the other features you will regress against<br/>        X = [f for f in features if f != feature]        <br/>        X, y = df[X], df[feature]</span><span id="a34c" class="oo nd it ok b gy ou oq l or os">        # extract r-squared from the fit<br/>        r2 = LinearRegression().fit(X, y).score(X, y)                <br/>        <br/>        # calculate tolerance<br/>        tolerance[feature] = 1 - r2</span><span id="8696" class="oo nd it ok b gy ou oq l or os">        # calculate VIF<br/>        vif[feature] = 1/(tolerance[feature])</span><span id="8bb9" class="oo nd it ok b gy ou oq l or os">    # return VIF DataFrame<br/>    return pd.DataFrame({'VIF': vif, 'Tolerance': tolerance})</span></pre><h1 id="dc17" class="nc nd it bd ne nf ng nh ni nj nk nl nm jz nn ka no kc np kd nq kf nr kg ns nt bi translated">让我们试一试</h1><p id="3a05" class="pw-post-body-paragraph kz la it lb b lc nu ju le lf nv jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">为了了解VIF的实际情况，让我们使用一个名为<strong class="lb iu"> bloodpressure.csv </strong>的样本数据集，其内容如下:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="c234" class="oo nd it ok b gy op oq l or os">Pt,BP,Age,Weight,BSA,Dur,Pulse,Stress,<br/>1,105,47,85.4,1.75,5.1,63,33,<br/>2,115,49,94.2,2.1,3.8,70,14,<br/>3,116,49,95.3,1.98,8.2,72,10,<br/>4,117,50,94.7,2.01,5.8,73,99,<br/>5,112,51,89.4,1.89,7,72,95,<br/>6,121,48,99.5,2.25,9.3,71,10,<br/>7,121,49,99.8,2.25,2.5,69,42,<br/>8,110,47,90.9,1.9,6.2,66,8,<br/>9,110,49,89.2,1.83,7.1,69,62,<br/>10,114,48,92.7,2.07,5.6,64,35,<br/>11,114,47,94.4,2.07,5.3,74,90,<br/>12,115,49,94.1,1.98,5.6,71,21,<br/>13,114,50,91.6,2.05,10.2,68,47,<br/>14,106,45,87.1,1.92,5.6,67,80,<br/>15,125,52,101.3,2.19,10,76,98,<br/>16,114,46,94.5,1.98,7.4,69,95,<br/>17,106,46,87,1.87,3.6,62,18,<br/>18,113,46,94.5,1.9,4.3,70,12,<br/>19,110,48,90.5,1.88,9,71,99,<br/>20,122,56,95.7,2.09,7,75,99,</span></pre><p id="be4d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据集由以下字段组成:</p><ul class=""><li id="1a2c" class="mo mp it lb b lc ld lf lg li mq lm mr lq ms lu mt mu mv mw bi translated">血压(<strong class="lb iu">血压</strong>)，单位为毫米汞柱</li><li id="52f8" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated"><strong class="lb iu">年龄</strong>，以年为单位</li><li id="9822" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated"><strong class="lb iu">重量</strong>，单位为千克</li><li id="6a37" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">体表面积(<strong class="lb iu"> BSA </strong>)，单位为m</li><li id="d6c2" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">高血压持续时间(<strong class="lb iu"> Dur </strong>)，年</li><li id="0954" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">基础脉搏(<strong class="lb iu">脉搏</strong>)，单位为每分钟心跳数</li><li id="c3b6" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">应力指数(<strong class="lb iu">应力</strong>)</li></ul><p id="11cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">首先，将数据集加载到Pandas数据框架中，并删除多余的列:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="6984" class="oo nd it ok b gy op oq l or os">df = pd.read_csv('bloodpressure.csv')<br/>df = df.drop(['Pt','Unnamed: 8'],axis = 1)<br/>df</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/09dc27d49c489ac5540eeea1a1c0ed52.png" data-original-src="https://miro.medium.com/v2/resize:fit:650/format:webp/1*zu2qS2v1b_2nOYWgoPHZNw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h1 id="5386" class="nc nd it bd ne nf ng nh ni nj nk nl nm jz nn ka no kc np kd nq kf nr kg ns nt bi translated">可视化列之间的关系</h1><p id="9501" class="pw-post-body-paragraph kz la it lb b lc nu ju le lf nv jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">在进行任何清理之前，使用pair plot(使用<strong class="lb iu"> Seaborn </strong>模块)来可视化各个列之间的关系是很有用的:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="caf6" class="oo nd it ok b gy op oq l or os">import seaborn as sns<br/>sns.pairplot(df)</span></pre><p id="2ba4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我发现一些列似乎存在很强的相关性:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ow"><img src="../Images/629943bdd2fa0aff5229cb38a0952798.png" data-original-src="https://miro.medium.com/v2/resize:fit:1346/format:webp/1*AF4CfJOr9KSIo0t7YwjbQQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h1 id="fe8e" class="nc nd it bd ne nf ng nh ni nj nk nl nm jz nn ka no kc np kd nq kf nr kg ns nt bi translated">计算相关性</h1><p id="351e" class="pw-post-body-paragraph kz la it lb b lc nu ju le lf nv jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">接下来，使用<strong class="lb iu"> corr() </strong>函数计算列之间的相关性:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="b43a" class="oo nd it ok b gy op oq l or os">df.corr()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/becc80f80949998ced04f268b2a3c545.png" data-original-src="https://miro.medium.com/v2/resize:fit:1004/format:webp/1*uZTv6_t_qMRvVM89PUl5SQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="5ac2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">假设您试图建立一个预测<strong class="lb iu">血压</strong>的模型，您可以看到与<strong class="lb iu">血压</strong>相关的主要特征是<strong class="lb iu">年龄</strong>、<strong class="lb iu">体重</strong>、<strong class="lb iu"> BSA </strong>和<strong class="lb iu">脉搏</strong>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/e3bb8d81356d25bb5837f7a305b1a2bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:280/format:webp/1*yeTWOV79ozhhYUjsNIx-Bg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h1 id="a28b" class="nc nd it bd ne nf ng nh ni nj nk nl nm jz nn ka no kc np kd nq kf nr kg ns nt bi translated">计算VIF</h1><p id="4d11" class="pw-post-body-paragraph kz la it lb b lc nu ju le lf nv jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">现在，您已经确定了要用于定型模型的列，您需要查看哪些列具有多重共线性。因此，让我们使用我们之前编写的<strong class="lb iu"> calculate_vif() </strong>函数:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="590c" class="oo nd it ok b gy op oq l or os">calculate_vif(df=df, features=['Age','Weight','BSA','Pulse'])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/4ceef2b1bdf20684814d21c704c23882.png" data-original-src="https://miro.medium.com/v2/resize:fit:396/format:webp/1*-AiqRpgQd271Ss87s1C-8w.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><h1 id="2d52" class="nc nd it bd ne nf ng nh ni nj nk nl nm jz nn ka no kc np kd nq kf nr kg ns nt bi translated">解读VIF价值观</h1><p id="7c28" class="pw-post-body-paragraph kz la it lb b lc nu ju le lf nv jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">VIF的有效值范围从1到无穷大。解释VIF价值观的经验法则是:</p><ul class=""><li id="2f62" class="mo mp it lb b lc ld lf lg li mq lm mr lq ms lu mt mu mv mw bi translated">1-要素不相关</li><li id="7702" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">1 <vif features="" are="" moderately="" correlated=""/></li><li id="a51a" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">VIF&gt; 5 —特征高度相关</li><li id="e475" class="mo mp it lb b lc mx lf my li mz lm na lq nb lu mt mu mv mw bi translated">VIF&gt;10 —特征之间的高度相关性，值得关注</li></ul><p id="0b69" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">从上一节计算VIF的结果中，可以看到<strong class="lb iu">重量</strong>和<strong class="lb iu"> BSA </strong>的VIF值大于5。这意味着<strong class="lb iu">体重</strong>和<strong class="lb iu"> BSA </strong>高度相关。这并不奇怪，因为较重的人有较大的体表面积。</p><p id="9e80" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，接下来要做的事情是尝试删除一个高度相关的特征，看看VIF的结果是否会改善。让我们试着去掉<strong class="lb iu">的重量</strong>，因为它的VIF更高:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="bc6a" class="oo nd it ok b gy op oq l or os">calculate_vif(df=df, features=['Age','BSA','Pulse'])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/1b92ea24e53d838281f17475ace7b8b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:384/format:webp/1*CC-Dl4GIgdolG36ZrHldVw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="eb2a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们去掉<strong class="lb iu"> BSA </strong>，看看其他特性的VIF:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="5941" class="oo nd it ok b gy op oq l or os">calculate_vif(df=df, features=['Age','Weight','Pulse'])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/a6392348f2648bd7f565d412d39727aa.png" data-original-src="https://miro.medium.com/v2/resize:fit:394/format:webp/1*gVoSTLVQz506TKn6LCZOBQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="35a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">正如您所观察到的，与移除<strong class="lb iu"> BSA </strong>相比，移除<strong class="lb iu">重量</strong>会导致所有其他特性的VIF降低。那么你应该去掉<strong class="lb iu">的重量</strong>吗？嗯，理想情况下，是的。但出于实际原因，移除<strong class="lb iu"> BSA </strong>并保留<strong class="lb iu">重量</strong>会更有意义。这是因为稍后当模型被训练并用于预测时，获得患者的体重比他/她的体表面积更容易。</p><h1 id="27f9" class="nc nd it bd ne nf ng nh ni nj nk nl nm jz nn ka no kc np kd nq kf nr kg ns nt bi translated">再举一个例子</h1><p id="1a51" class="pw-post-body-paragraph kz la it lb b lc nu ju le lf nv jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">让我们再看一个例子。这次您将使用<strong class="lb iu"> sklearn </strong>附带的乳腺癌数据集:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="cc1c" class="oo nd it ok b gy op oq l or os">from sklearn import datasets<br/>bc = datasets.load_breast_cancer()</span><span id="162e" class="oo nd it ok b gy ou oq l or os">df = pd.DataFrame(bc.data, columns=bc.feature_names)<br/>df</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pc"><img src="../Images/7d533c5dc065ac969fd31c68af781a38.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8a1JuOym5T9mP02r956YjQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="d9af" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个数据集有30列，所以我们只关注前8列:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="ec51" class="oo nd it ok b gy op oq l or os">sns.pairplot(df.iloc[:,:8])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pd"><img src="../Images/b33235beb37799da574bb6a3a447ff47.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fozem330WXbLad6vjmlRTA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="aac3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以立即观察到一些特征高度相关。你能认出他们吗？</p><p id="168b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们计算前8列的VIF:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="0fa0" class="oo nd it ok b gy op oq l or os">calculate_vif(df=df, features=df.columns[:8])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/20ca4d75f11b6936e3efbaa9a4371be4.png" data-original-src="https://miro.medium.com/v2/resize:fit:608/format:webp/1*MGcj_ewzgoiVJyvlKLbADg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="60ef" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以看到以下要素具有较大的VIF值:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/bd1fa4c691d01b2dd0621d76719ea11b.png" data-original-src="https://miro.medium.com/v2/resize:fit:616/format:webp/1*zHzdyz2MZ9G1d27D8McJ2A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="cdc7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们试着一个一个地移除这些特征，并观察它们的新VIF值。首先，移除<strong class="lb iu">平均周长</strong>:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="3d46" class="oo nd it ok b gy op oq l or os">calculate_vif(df=df, features=['mean radius', <br/>                               'mean texture', <br/>                               'mean area', <br/>                               'mean smoothness', <br/>                               'mean compactness', <br/>                               'mean concavity',<br/>                               'mean concave points'])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/2b552bfc259df03b4cf276bda27e86b1.png" data-original-src="https://miro.medium.com/v2/resize:fit:570/format:webp/1*PeRjN4vt6ZjRMA-Lj6ft-Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="7bb4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">VIFs立即全面下降。现在让我们移除<strong class="lb iu">平均面积</strong>:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="c040" class="oo nd it ok b gy op oq l or os">calculate_vif(df=df, features=['mean radius', <br/>                               'mean texture',<br/>                           <strong class="ok iu">  # 'mean area', </strong><br/>                               'mean smoothness', <br/>                               'mean compactness', <br/>                               'mean concavity',<br/>                               'mean concave points'])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/fd631770b81ba84616f4c1450c539549.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*JQdkDGJ1BIaz83zr5zx1mA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="8797" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们去掉<strong class="lb iu">意思是凹点</strong>，其中VIF最高:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="16e3" class="oo nd it ok b gy op oq l or os">calculate_vif(df=df, features=['mean radius', <br/>                               'mean texture',<br/>                           <strong class="ok iu">  # 'mean area', </strong><br/>                               'mean smoothness', <br/>                               'mean compactness', <br/>                               'mean concavity',<br/><strong class="ok iu">                             # 'mean concave points'</strong><br/>                              ])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pi"><img src="../Images/b815bfae742add742b528d0314ce0320.png" data-original-src="https://miro.medium.com/v2/resize:fit:542/format:webp/1*UKqwqf2FgioDI8qeYxiyJA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="d10e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，让我们去掉<strong class="lb iu">平均凹度</strong>:</p><pre class="kj kk kl km gt oj ok ol om aw on bi"><span id="6ff2" class="oo nd it ok b gy op oq l or os">calculate_vif(df=df, features=['mean radius', <br/>                               'mean texture',<br/>                           <strong class="ok iu">  # 'mean area', </strong><br/>                               'mean smoothness', <br/>                               'mean compactness', <br/>                             <strong class="ok iu"># 'mean concavity',</strong><br/><strong class="ok iu">                             # 'mean concave points'</strong><br/>                              ])</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pj"><img src="../Images/f0408c2b2e9c22752f8603079b6b2bcc.png" data-original-src="https://miro.medium.com/v2/resize:fit:534/format:webp/1*FpyekuktRaGfaBXwJyh70A.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者图片</p></figure><p id="0cfb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在所有的VIF值都低于5。</p><h1 id="a783" class="nc nd it bd ne nf ng nh ni nj nk nl nm jz nn ka no kc np kd nq kf nr kg ns nt bi translated">摘要</h1><p id="a018" class="pw-post-body-paragraph kz la it lb b lc nu ju le lf nv jx lh li nw lk ll lm nx lo lp lq ny ls lt lu im bi translated">在本文中，您了解了相关性、共线性和多重共线性之间的区别。特别是，您了解了当一个要素与两个或多个要素呈现线性关系时会发生多重共线性。要检测多重共线性，一种方法是计算<strong class="lb iu">方差膨胀因子</strong> ( <strong class="lb iu"> VIF </strong>)。应将VIF大于5的任何要素从训练数据集中移除。</p><blockquote class="od oe of"><p id="38fe" class="kz la lv lb b lc ld ju le lf lg jx lh og lj lk ll oh ln lo lp oi lr ls lt lu im bi translated">值得注意的是，VIF只对连续变量有效，对分类变量无效。</p></blockquote><div class="lw lx gp gr ly lz"><a href="https://weimenglee.medium.com/membership" rel="noopener follow" target="_blank"><div class="ma ab fo"><div class="mb ab mc cl cj md"><h2 class="bd iu gy z fp me fr fs mf fu fw is bi translated">加入媒介与我的介绍链接-李伟孟</h2><div class="mg l"><h3 class="bd b gy z fp me fr fs mf fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="mh l"><p class="bd b dl z fp me fr fs mf fu fw dk translated">weimenglee.medium.com</p></div></div><div class="mi l"><div class="pk l mk ml mm mi mn ks lz"/></div></div></a></div></div></div>    
</body>
</html>