<html>
<head>
<title>Applying Ridge Regression with Cross-Validation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">应用交叉验证的岭回归</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/the-power-of-ridge-regression-4281852a64d6?source=collection_archive---------10-----------------------#2021-05-02">https://towardsdatascience.com/the-power-of-ridge-regression-4281852a64d6?source=collection_archive---------10-----------------------#2021-05-02</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/2e5a578b96eccc5cd0e700a63d3aebd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*J9_TJS2GkEuyGzIwqvFXZQ.jpeg"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">汉斯·M在<a class="ae jd" href="https://unsplash.com/s/photos/house-scandinavia?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><div class=""/><div class=""><h2 id="e45e" class="pw-subtitle-paragraph kd jf jg bd b ke kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku dk translated">一个回归问题的演练，包括预处理、特征选择和超参数调整</h2></div><p id="5d98" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">数据科学家经常被要求在没有给出特定指令的情况下预测商业环境中的目标变量。这是一项具有挑战性的任务，有多种方法可以到达终点线。这个博客是一个如何利用岭回归进行端到端多元分析的例子。举例来说，众所周知的<a class="ae jd" href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data?select=data_description.txt" rel="noopener ugc nofollow" target="_blank">住房数据</a>被挑选出来，其中房屋销售价格将是目标变量。</p><h1 id="3913" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">预处理</h1><p id="713f" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">首先，我从消除因缺乏舒适设施而导致的缺失数据开始。像area这样的数字缺失值被替换为零，因为用零表示不存在的区域更有意义。带有有序分类值的列中缺少的值将被替换为字符串，以指示该观察中没有数据点。在这里，用一个字符串代替<code class="fe mo mp mq mr b">np.nan</code>是很重要的，因为它们将按顺序被赋予代表它们等级的数值。对于其余的分类值，缺失的数据将替换为最常用的值。</p><p id="9d5f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">第二，每个分类值都被转换成一个数值。有序分类值被替换为数字表示，并按照以下代码的顺序保留它们的等级:</p><pre class="ms mt mu mv gt mw mr mx my aw mz bi"><span id="95af" class="na ls jg mr b gy nb nc l nd ne"># creating the dictonary including the replacement values<br/>bsmt_zip = {'Ex':6,'Gd':5,'TA':4,'Fa':3,'Po':2,'NA':1}</span><span id="a712" class="na ls jg mr b gy nf nc l nd ne"># using the map function to complete the replacement<br/>housing_data.BsmtQual = housing_data.BsmtQual.map(bsmt_zip)</span></pre><p id="1518" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">年份列被转换成三个箱:新的、旧的和年中的。1978年以前的房屋被贴上老旧的标签，因为那时的房屋被涂上了含铅油漆，这在1978年是被禁止的。1999年以后建造的房屋被标为新的。</p><pre class="ms mt mu mv gt mw mr mx my aw mz bi"><span id="ee09" class="na ls jg mr b gy nb nc l nd ne">housing_data.YearBuilt.map(lambda x: 'new' if x&gt;1999 else ('mid' if x&gt;1978 else 'old'))</span></pre><p id="801b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">稍后，其余的分类值将替换为数值:</p><pre class="ms mt mu mv gt mw mr mx my aw mz bi"><span id="9c66" class="na ls jg mr b gy nb nc l nd ne"># columns containing string values<br/>string_columns = df_types[df_types[0]=='object']['index']</span><span id="cab8" class="na ls jg mr b gy nf nc l nd ne"># columns with only numeric values<br/>non_string_columns = df_types[df_types[0]!='object']['index']</span><span id="d9aa" class="na ls jg mr b gy nf nc l nd ne"># creating dummy columns for string columns<br/>dummy_df = pd.get_dummies(housing_data[string_columns])</span><span id="ac8a" class="na ls jg mr b gy nf nc l nd ne"># dummy and numeric columns are concatenated<br/>df = pd.concat([dummy_df,housing_data[non_string_columns]], axis=1)</span></pre><p id="495c" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在这一点上，预测器的数量是247，需要减少，并且有多种方法可以实现。我将尝试通过检查预测值与目标值的相关性来手动减少预测值的数量。然后利用岭回归调整它们在模型中的系数。与常规多元线性回归相比，我更喜欢岭型回归，因为我的数据有共线列。就正则化偏好而言，我选择山脊而不是套索，因为我已经选择了最相关的预测因子，并且不需要强制预测因子系数为零的苛刻正则化。换句话说，我不需要特征选择技术，我只需要调整我的模型分配给不同预测器的权重。</p><pre class="ms mt mu mv gt mw mr mx my aw mz bi"><span id="249b" class="na ls jg mr b gy nb nc l nd ne"># correlations between sale price and other fields<br/>df_corr = abs(df.corr()).sort_values(by='SalePrice', ascending=False)[['SalePrice']]</span><span id="ba64" class="na ls jg mr b gy nf nc l nd ne"># a new dataframe including only the relevant predictors<br/>df_small = df[df_corr[df_corr['SalePrice']&gt;0.4].index.tolist()]</span></pre><p id="90ec" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">到目前为止，还剩下23个与销售价格的相关系数高于0.4的预测值。下一步是将数据集分为训练和测试。</p><pre class="ms mt mu mv gt mw mr mx my aw mz bi"><span id="7a10" class="na ls jg mr b gy nb nc l nd ne">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)</span></pre><p id="0945" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">这不是必须的，但是在这一点上，我想对多个列应用log-transform，在这些列中有多个异常值，并且分布有一个正偏差。下面是地面生活区对数变换前后的情况。第一个直方图具有由异常值引起的正偏斜，并且第一个概率图显示了它不是正态分布的良好代表的事实。</p><figure class="ms mt mu mv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ng"><img src="../Images/5752c7b4cfb865092c728c50c11ee0d6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cgXObIxFvcTh3qlEK61aBA.png"/></div></div><p class="iz ja gj gh gi jb jc bd b be z dk translated">图像的左侧和右侧分别代表地面生活区对数变换前后的情况</p></figure><p id="2704" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">预处理的最后一步是标准化。有必要将每个预测值调整到相似的范围。否则，一些列将支配其他列。在这种情况下，<code class="fe mo mp mq mr b">StandardScaler</code>用于根据方差对列进行缩放。这是应用岭回归前必不可少的一步。</p><pre class="ms mt mu mv gt mw mr mx my aw mz bi"><span id="126e" class="na ls jg mr b gy nb nc l nd ne">from sklearn.preprocessing import StandardScaler<br/># initiate the standard scaler<br/>ss = StandardScaler()<br/># fit <br/>Z_train = ss.fit_transform(X_train)<br/># transform the df<br/>Z_train = pd.DataFrame(ss.transform(X_train), columns=X_train.columns)</span></pre><h1 id="e3d8" class="lr ls jg bd lt lu lv lw lx ly lz ma mb km mc kn md kp me kq mf ks mg kt mh mi bi translated">应用岭回归</h1><p id="b285" class="pw-post-body-paragraph kv kw jg kx b ky mj kh la lb mk kk ld le ml lg lh li mm lk ll lm mn lo lp lq ij bi translated">当应用脊正则化时，我们需要找到用α表示的最优惩罚系数。阿尔法值越高，系数的惩罚就越强。</p><figure class="ms mt mu mv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi nh"><img src="../Images/e6ada9cb46073dbd1707c46322494ab1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y-ci-DqLaT47WhEnYhGWgw.png"/></div></div></figure><p id="89b2" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">让我们检查一下alpha对系数的影响。这里，标度是α的对数，因为它具有平方误差值。(对于Lasso，它将是alpha本身，因为它使用误差的绝对值而不是误差的平方。)</p><figure class="ms mt mu mv gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ni"><img src="../Images/dff648ed392bd2b737e6377b1e9c6c94.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*maFUs1ZvdeVaUJ8kCoZtEw.gif"/></div></div></figure><p id="522f" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">当α增加时，观察到系数变小是有意义的，但下一个问题是，我们需要选择哪个α来获得最优解？为此，我将使用交叉验证。</p><pre class="ms mt mu mv gt mw mr mx my aw mz bi"><span id="ace7" class="na ls jg mr b gy nb nc l nd ne"># list of alphas to check: 100 values from 0 to 5 with<br/>r_alphas = np.logspace(0, 5, 100)</span><span id="dcec" class="na ls jg mr b gy nf nc l nd ne"># initiate the cross validation over alphas<br/>ridge_model = RidgeCV(alphas=r_alphas, scoring='r2')</span><span id="3a8b" class="na ls jg mr b gy nf nc l nd ne"># fit the model with the best alpha<br/>ridge_model = ridge_model.fit(Z_train, y_train)</span></pre><p id="75e5" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在意识到使用哪个alpha和<code class="fe mo mp mq mr b">ridge_model.alpha_</code>之后，我们可以利用优化的超参数并拟合新的模型。在我的例子中，测试的R值为85.99%，训练的R值为83.92%。</p><p id="5b36" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">训练分数还不错，但如果能通过测试数据获得更高的分数就更好了。在可能遭受过拟合的数据集中，我们实现了避免这种情况，并使模型适应信号而不是噪声。</p><p id="f8ae" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">我的方法的下一步可能是比较手动特征选择之上的岭回归和套索回归的结果。毕竟，Lasso本身将涵盖特征选择和建模。</p></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><p id="6483" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">在上文中，我尝试总结了如何在将数据集输入模型之前对其进行预处理的方法，包括处理缺失值和分类值、挑选相关字段以及标准化。在决定了要使用的算法之后，我决定了最佳的超参数，并将其应用到我的测试数据中。</p><p id="b2dc" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">感谢阅读到最后，你可以在这里找到我的笔记本<a class="ae jd" href="https://colab.research.google.com/drive/1Oyk5m-jtmsyWAKaPatePLfuddRbInS8n?usp=sharing" rel="noopener ugc nofollow" target="_blank">。如有任何问题或意见，请</a><a class="ae jd" href="https://www.linkedin.com/in/yalimdemirkesen/" rel="noopener ugc nofollow" target="_blank">联系</a>。</p></div><div class="ab cl nj nk hu nl" role="separator"><span class="nm bw bk nn no np"/><span class="nm bw bk nn no np"/><span class="nm bw bk nn no"/></div><div class="ij ik il im in"><p id="436b" class="pw-post-body-paragraph kv kw jg kx b ky kz kh la lb lc kk ld le lf lg lh li lj lk ll lm ln lo lp lq ij bi translated">[1] M. Brems，J. Pounders，K. Katovich，正则化(2019)，大会数据科学沉浸式<br/> [2] P .马塞利诺，<a class="ae jd" href="https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python" rel="noopener ugc nofollow" target="_blank">用Python进行全面的数据探索</a> (2017)，Kaggle</p></div></div>    
</body>
</html>