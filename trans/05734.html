<html>
<head>
<title>Class weights for categorical loss</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">分类损失的类别权重</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/class-weights-for-categorical-loss-1a4c79818c2d?source=collection_archive---------16-----------------------#2021-05-22">https://towardsdatascience.com/class-weights-for-categorical-loss-1a4c79818c2d?source=collection_archive---------16-----------------------#2021-05-22</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="610e" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">当数据集不平衡时，类权重通常用于分类问题。我们在引擎盖下看这个。</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/236524de547b6109a74ab9421bbad63a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*FM8OFOtMdEJyXDz5"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">Suket Dedhia的绘画作品</p></figure><p id="e2a5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在寻找关于类权重的“确切”解释时，我不仅没有找到任何东西，<a class="ae kv" href="https://stackoverflow.com/questions/67644633/class-weight-implementation-different-for-pytorch-and-tensorflow-which-one-is-c" rel="noopener ugc nofollow" target="_blank">对tensorflow和PyTorch中的实现进行逆向工程，我发现它们并不一致</a>。因此，在这篇笔记中，我将以二进制分类问题为例来研究数学。之后，我还将很快讨论多类分类问题。</p><h1 id="ce5a" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">二元分类</h1><p id="69c0" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">不平衡数据集是指不同类别的数据点数量相差很大的数据集。有几种方法可以解决这个问题(欠采样、过采样、增加显式偏差和类别权重在<a class="ae kv" href="https://www.tensorflow.org/tutorials/structured_data/imbalanced_data" rel="noopener ugc nofollow" target="_blank">张量流示例</a>中讨论)，这里我们将讨论类别权重。</p><p id="969e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们来看一个标签为0和1的二元分类问题。类权重的传统选择是</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/900e323dd38d9d02780f93122c325b1f.png" data-original-src="https://miro.medium.com/v2/resize:fit:426/format:webp/1*XDy3ve3Uvx7OJicyGD29qQ.png"/></div></figure><p id="0e72" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">虽然最近<a class="ae kv" href="https://arxiv.org/abs/1901.05555" rel="noopener ugc nofollow" target="_blank">讨论了一个更复杂的方法</a>。</p><p id="0264" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">一个数据点的分类交叉熵损失函数为</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi mq"><img src="../Images/a23eadf4496dda4b2faacfa733613fe7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AhY50luaArX6CtvPhwvlWA.png"/></div></div></figure><p id="8591" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中，y=1，0表示阳性和阴性标签，p是阳性类别的概率，w1和w0是阳性类别和阴性类别的类别权重。</p><p id="258b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">对于minibatch，PyTorch和Tensorflow的实现有一个归一化的区别。PyTorch有</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/17f5843922acaa6219856bd94ebf9796.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*-yLR5MZxhFRN1TXc8SwBQg.png"/></div></figure><p id="24a7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">而Tensorflow有</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mr"><img src="../Images/d889ce75495cb174e26f5e31c4d68c74.png" data-original-src="https://miro.medium.com/v2/resize:fit:500/format:webp/1*1oOAk4lKPh3swyUeFmvkxA.png"/></div></figure><p id="2203" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">当批量较小时，这两者之间的差异很重要(对于较大的批量，大数法则抑制PyTorch实现中的批量相关波动)。</p></div><div class="ab cl ms mt hu mu" role="separator"><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx my"/><span class="mv bw bk mw mx"/></div><div class="ij ik il im in"><p id="e2a3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">让我们看看最后一层的梯度下降。其余的使用简单的链式法则/反向传播。我们将重点关注一个数据点的损失。批次的结果是这些的平均值，并且取决于如上所述的特定实现。类别1的概率是</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/ca69c29d652ed3606424dff5485b3936.png" data-original-src="https://miro.medium.com/v2/resize:fit:398/format:webp/1*uDlJ6iSpGh4XDGAr44skSw.png"/></div></figure><p id="b639" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">而类0的是1-p。W是权重向量，x是前一层的输出。我忽略了对简单性的偏好。</p><p id="3a9c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">简单的微积分表明，损失相对于重量的梯度为</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi na"><img src="../Images/7bb0c07c04ab8cd1b06bfa3e80c736d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rnscKzXi6y6hqMODNoTjpw.png"/></div></div></figure><p id="73b2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这给出了对重量的修正</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nb"><img src="../Images/c8aec1c86681296932a6d64bcf9e547e.png" data-original-src="https://miro.medium.com/v2/resize:fit:448/format:webp/1*u4oTp4YjWdHKzaJD5TbpaA.png"/></div></figure><p id="a407" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">先看看这个没有类权重的例子，看看会发生什么。<strong class="ky ir">如果样本为正类，即y=1，则梯度指向x方向，向量长度与(1-p)成正比。概率离1越远，这个向量就越长。相反，如果样本属于负类，即y=0，则梯度指向x的对面，向量的长度与p成比例。p离0越远，向量越长。</strong>想想这个，说服自己这是正确的行为。</p><p id="8182" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">现在，当我们打开<strong class="ky ir">级重量</strong>时，我们看到它们<strong class="ky ir">调整前述长度</strong>。我们取p=0.5。在这种没有类权重的情况下，y=1和y=0的梯度向量将分别沿着x和逆着x具有相等的长度。然而，如果w1 &gt; &gt; w0，那么y=1的梯度向量将比y=0的梯度向量长得多。这将使前一种情况下对w的修正比后一种情况下大得多。w1 &lt; &lt; w0时反之适用。</p><h1 id="f788" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">多类问题</h1><p id="a784" class="pw-post-body-paragraph kw kx iq ky b kz mk jr lb lc ml ju le lf mm lh li lj mn ll lm ln mo lp lq lr ij bi translated">为了完整性(你将看到额外的简单性),让我们也做多类问题。在这种情况下，交叉熵损失为</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nc"><img src="../Images/69f412ef772d3107ddd731fee0486458.png" data-original-src="https://miro.medium.com/v2/resize:fit:658/format:webp/1*A0vtulfvLQvCR-9PeqE3eg.png"/></div></figure><p id="e9a3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">其中alphas是类权重(很抱歉切换通知，但是随着额外权重的继续，使用‘w’表示类权重会变得混乱)</p><p id="a01e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">softmax下第I类的概率由下式给出</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/21abcc75a33df128d0b48dccdeefec7a.png" data-original-src="https://miro.medium.com/v2/resize:fit:440/format:webp/1*J3Fi9rrnT8XFLvcX5ZgSdw.png"/></div></figure><p id="1ec3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这很容易验证(如果看起来不容易，那么你应该实际计算并说服自己)</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ne"><img src="../Images/1c87fcb1be1aca36a86879dbe715a83d.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*f5wjFO5n4yE26nGSL4s25Q.png"/></div></figure><p id="c727" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">利用我们得到的链式法则</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/5b0b4f0e361d8260d38fce9de835524d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1118/format:webp/1*dWk_gdQ-F9uZD0O3UH9BFA.png"/></div></figure><p id="0863" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">具体地看将是有启发性的，因此，不失一般性地，让我们假设真正的标签是y0=1，而所有其他的y是0。然后我们得到类的权重更新为正</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ng"><img src="../Images/e38bfb01871e00f426fa9541b1fc94f6.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/format:webp/1*UaTvHAmq16KJzLDPC6t04A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">正确类别的权重更新将权重与x对齐，并且如果强度与预测的偏离程度和正确类别的类别权重成比例，则强度与x对齐</p></figure><p id="69e6" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">而对负的类的权重更新是</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nh"><img src="../Images/dd8f3cc6aade248ae901890446177aa9.png" data-original-src="https://miro.medium.com/v2/resize:fit:458/format:webp/1*_4zeSFV6cDUKKZGuaGDVgw.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">不正确类别的权重更新使权重与x不对齐，并且强度再次与预测的偏离程度成比例，但是注意，它们与正确类别的权重成比例，而不是与不正确类别的权重成比例。如果人们盯着损失函数看一段时间，后一个事实就很明显了。</p></figure><p id="7711" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">注意，校正的效果是使正确类的权重与x更加对齐，而不正确类的权重与x更加不对齐。</p></div></div>    
</body>
</html>