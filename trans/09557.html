<html>
<head>
<title>A better Dropout! Implementing DropBlock in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">更好的辍学生！在PyTorch中实现DropBlock</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-better-dropout-implementing-dropblock-in-pytorch-50d19d1bc59a?source=collection_archive---------13-----------------------#2021-09-05">https://towardsdatascience.com/a-better-dropout-implementing-dropblock-in-pytorch-50d19d1bc59a?source=collection_archive---------13-----------------------#2021-09-05</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><figure class="is it gp gr iu iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ir"><img src="../Images/98120d1e8bf2007c6a023c0667d94c09.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R1_yPcrrrq_OwpQOSTz_cQ.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><div class=""/><p id="68b6" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这篇文章的互动版本可以在<a class="ae ld" href="https://github.com/FrancescoSaverioZuppichini/DropBlock" rel="noopener ugc nofollow" target="_blank">这里</a>找到</p><p id="23eb" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我的计算机视觉库中的<a class="ae ld" href="https://github.com/FrancescoSaverioZuppichini/glasses" rel="noopener ugc nofollow" target="_blank">眼镜</a>上有DropBlock！</p><h1 id="20bf" class="le lf ji bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">介绍</h1><p id="f351" class="pw-post-body-paragraph kf kg ji kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc im bi translated">今天我们要在PyTorch中实现<a class="ae ld" href="https://arxiv.org/abs/1810.12890" rel="noopener ugc nofollow" target="_blank"> DropBlock </a>！Ghiasi等人介绍的DropBlock是一种正则化技术，专门用于图像，从经验上看比drop更有效。为什么辍学是不够的？</p><h2 id="7257" class="mh lf ji bd lg mi mj dn lk mk ml dp lo kq mm mn ls ku mo mp lw ky mq mr ma ms bi translated">图像丢失的问题</h2><p id="e977" class="pw-post-body-paragraph kf kg ji kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc im bi translated"><a class="ae ld" href="https://jmlr.org/papers/v15/srivastava14a.html" rel="noopener ugc nofollow" target="_blank"> Dropout </a>是一种正则化技术，在将输入传递到下一层之前，随机丢弃(设置为零)部分输入。如果你不熟悉，我推荐<a class="ae ld" href="https://cs231n.github.io/neural-networks-2/" rel="noopener ugc nofollow" target="_blank">斯坦福的这些</a>讲义(跳转到退学部分)。如果要在PyTorch中使用，可以直接从库中导入。我们来看一个例子！</p><figure class="mt mu mv mw gt iv"><div class="bz fp l di"><div class="mx my l"/></div></figure><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi mz"><img src="../Images/a5706745a8df6cd5520337e2554aa63c.png" data-original-src="https://miro.medium.com/v2/resize:fit:760/0*73FoCK8xKXgtoHrg"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="dad8" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">如您所见，输入的随机像素被丢弃了！</p><p id="1870" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">这种技术在1D数据上工作得很好，但是在2D数据上，我们可以做得更好。</p><p id="529f" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><strong class="kh jj">主要问题</strong>是我们正在丢弃独立的像素，并且这个<em class="na">在移除语义信息方面是无效的，因为附近的激活包含密切相关的信息</em>。我认为这是相当直观的，即使我们去掉一个元素，邻居仍然可以携带重要信息。</p><p id="bcf0" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">让我们探索一下特征地图会发生什么。在下面的代码中，我们首先获得一个婴儿尤达图像，然后我们使用<a class="ae ld" href="https://github.com/FrancescoSaverioZuppichini/glasses" rel="noopener ugc nofollow" target="_blank">眼镜</a>创建一个预训练结果。然后我们馈入图像，从第二层得到特征图。最后，我们展示了有和没有<code class="fe nb nc nd ne b">Dropout</code>的第一个通道的激活</p><figure class="mt mu mv mw gt iv"><div class="bz fp l di"><div class="mx my l"/></div></figure><figure class="mt mu mv mw gt iv"><div class="bz fp l di"><div class="mx my l"/></div></figure><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/5a22ab217667ca29afc05ad98e1a70c6.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/0*gqOeMtXilptZC_B3"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="8c83" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在左边，我们有特征映射的激活，在右边是同一特征映射在退出后的激活。它们看起来非常相似，注意在每个区域，即使一些单位为零，邻居的激活仍然在进行。这意味着，信息将传播到下一层，这是不理想的。</p><h1 id="a0bd" class="le lf ji bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">下落滑车</h1><p id="b482" class="pw-post-body-paragraph kf kg ji kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc im bi translated">DropBlock通过从特征地图中删除连续区域来解决这个问题，下图显示了主要思想。</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ng"><img src="../Images/1cde73539e3dd3778511ea42e8718faa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*hM8f4pykyVPtKmZl"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片由Ghiasi等人提供。</p></figure><p id="3381" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">Dropblock的工作方式如下</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi nh"><img src="../Images/fb45fd7973c167c12a81e2bd9fd7a5cd.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mkKy5ZnrtiwgInBT"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">图片由Ghiasi等人提供。</p></figure><h2 id="323b" class="mh lf ji bd lg mi mj dn lk mk ml dp lo kq mm mn ls ku mo mp lw ky mq mr ma ms bi translated">履行</h2><p id="001d" class="pw-post-body-paragraph kf kg ji kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc im bi translated">我们可以从用正确的参数定义一个<code class="fe nb nc nd ne b">DropBlock</code>层开始</p><figure class="mt mu mv mw gt iv"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="835e" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated"><code class="fe nb nc nd ne b">block_size</code>是我们要从输入中删除的每个区域的大小，<code class="fe nb nc nd ne b">p</code>是<code class="fe nb nc nd ne b">keep_prob</code>，就像<code class="fe nb nc nd ne b">Dropout</code>中的那样。</p><p id="595b" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">到目前为止一切顺利。现在棘手的部分，我们需要计算伽玛来控制特征的下降。如果我们想用<code class="fe nb nc nd ne b">p</code> prob保持每次激活，我们可以从一个具有均值<code class="fe nb nc nd ne b">1 - p</code>的伯努利分布中采样，就像在Dropout中一样。问题是我们将<code class="fe nb nc nd ne b">block_size ** 2</code>单位设置为零。</p><p id="fd40" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">Gamma的计算公式如下</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ni"><img src="../Images/6faacf893cbca8a59dda41a81eb0ea07.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eD9DMWC8NuwgD0R-oDEM_Q.png"/></div></div><p class="jc jd gj gh gi je jf bd b be z dk translated">Ghiasi等人的图片(论文中的公式1)</p></figure><p id="95ee" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">乘法运算的左侧是将被设置为零的单位数。而右侧是有效区域，即dropblock未触及的像素数</p><figure class="mt mu mv mw gt iv"><div class="bz fp l di"><div class="mx my l"/></div></figure><pre class="mt mu mv mw gt nj ne nk nl aw nm bi"><span id="f973" class="mh lf ji ne b gy nn no l np nq"># Output<br/>0.14222222222222222 </span></pre><p id="bf06" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">下一步是对掩码$M$进行采样，其大小与中心为γ的伯努利分布的输入大小相同，在PyTorch中很简单</p><figure class="mt mu mv mw gt iv"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="efc0" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">接下来，我们需要清空大小为<code class="fe nb nc nd ne b">block_size</code>的区域。我们可以使用<strong class="kh jj">最大池</strong>，其中<code class="fe nb nc nd ne b">kernel_size</code>等于<code class="fe nb nc nd ne b">block_size</code>并且一个像素步距来创建。请记住，掩码是一个二进制掩码(只有0和1)，因此当maxpool在其kernel_size半径中看到1时，它将输出1，通过使用1步长，我们确保如果输入中至少有一个单元设置为1，则在输出中创建大小为<code class="fe nb nc nd ne b">block_size x block_size</code>的区域。因为我们想把它们归零，我们需要把它反过来。在PyTorch</p><figure class="mt mu mv mw gt iv"><div class="bz fp l di"><div class="mx my l"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">饭桶</p></figure><p id="fa31" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">然后我们将它标准化</p><pre class="mt mu mv mw gt nj ne nk nl aw nm bi"><span id="30e6" class="mh lf ji ne b gy nn no l np nq">x = mask_block * x * (mask_block.numel() / mask_block.sum())</span></pre><figure class="mt mu mv mw gt iv"><div class="bz fp l di"><div class="mx my l"/></div></figure><p id="49c5" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">让我们用宝贝尤达来测试一下，为了简单起见，我们将在第一个通道中显示被丢弃的单元</p><figure class="mt mu mv mw gt iv"><div class="bz fp l di"><div class="mx my l"/></div></figure><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi nr"><img src="../Images/1f3fefb64c7677711ba7a3d49900428a.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/0*ZWdi1AeMHv8AszPi"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="e02f" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">看起来不错，让我们看看一个预训练模型的特征图(像以前一样)</p><figure class="mt mu mv mw gt iv"><div class="bz fp l di"><div class="mx my l"/></div></figure><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/963935d2f671910b79d87cb4be0edc6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:736/0*uEJZCBEnE1-VAFJC"/></div><p class="jc jd gj gh gi je jf bd b be z dk translated">作者图片</p></figure><p id="7853" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">我们成功地将连续区域清零，而不仅仅是单个单元。</p><p id="a504" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">顺便说一句，<code class="fe nb nc nd ne b">DropBlock</code>在<code class="fe nb nc nd ne b">block_size = 1</code>时等于<code class="fe nb nc nd ne b">Dropout</code>，在<code class="fe nb nc nd ne b">block_size</code>为全特征图时等于<code class="fe nb nc nd ne b">Dropout2d</code>(又名空间差)。</p><h1 id="e2b4" class="le lf ji bd lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb bi translated">结论</h1><p id="b149" class="pw-post-body-paragraph kf kg ji kh b ki mc kk kl km md ko kp kq me ks kt ku mf kw kx ky mg la lb lc im bi translated">现在我们知道如何在PyTorch中实现DropBlock，这是一种很酷的正则化技术。本文给出了不同的实证结果。他们使用一个普通的resnet50并迭代地添加不同的正则化，如下表所示</p><figure class="mt mu mv mw gt iv gh gi paragraph-image"><div role="button" tabindex="0" class="iw ix di iy bf iz"><div class="gh gi ns"><img src="../Images/bca8ea81374b1200a0d823518e7a3be5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*e732cqgsa6ai4uyR.png"/></div></div></figure><p id="30c5" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">如你所见，与SpatialDropout相比，` ResNet-50 + DropBlock '存档+1%(py torch中的经典` Dropout2d ')。</p><p id="d911" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">在<a class="ae ld" href="https://arxiv.org/pdf/1810.12890.pdf)" rel="noopener ugc nofollow" target="_blank">论文</a>中有更多关于不同DropBlock超参数的研究，如果你有兴趣可以看看:)</p><p id="543c" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">感谢您的阅读！</p><p id="89b6" class="pw-post-body-paragraph kf kg ji kh b ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la lb lc im bi translated">弗朗西斯科</p></div></div>    
</body>
</html>