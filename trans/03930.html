<html>
<head>
<title>Quadratic Discriminant Analysis</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">二次判别分析</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/quadratic-discriminant-analysis-ae55d8a8148a?source=collection_archive---------5-----------------------#2021-04-02">https://towardsdatascience.com/quadratic-discriminant-analysis-ae55d8a8148a?source=collection_archive---------5-----------------------#2021-04-02</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="d905" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">二次判别分析(QDA)理论和Python实现的深度介绍</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/a2f6a65db35dd9706c80b70f9c78801b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*wZ0za2f4nKr8HTYPPcWzig.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由QDA生成的决策边界的图示。图片作者。</p></figure><h1 id="6dce" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">内容</h1><p id="204a" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">这篇文章是我将要发表的一系列文章的一部分。你可以通过点击<a class="ae mm" href="https://cookieblues.github.io/guides/2021/04/01/bsmalea-notes-3b/" rel="noopener ugc nofollow" target="_blank">这里</a>在我的个人博客上阅读这篇文章的更详细版本。下面你可以看到该系列的概述。</p><h2 id="7e10" class="mn kz it bd la mo mp dn le mq mr dp li lz ms mt lk md mu mv lm mh mw mx lo my bi translated">1.机器学习导论</h2><ul class=""><li id="2eb8" class="mz na it ls b lt lu lw lx lz nb md nc mh nd ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/what-is-machine-learning-91040db474f9">(一)什么是机器学习？</a></li><li id="1220" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/model-selection-in-machine-learning-813fe2e63ec6"> (b)机器学习中的模型选择</a></li><li id="0006" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/the-curse-of-dimensionality-5673118fe6d2"> (c)维度的诅咒</a></li><li id="a6de" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/what-is-bayesian-inference-4eda9f9e20a6"> (d)什么是贝叶斯推理？</a></li></ul><h2 id="522e" class="mn kz it bd la mo mp dn le mq mr dp li lz ms mt lk md mu mv lm mh mw mx lo my bi translated">2.回归</h2><ul class=""><li id="8b64" class="mz na it ls b lt lu lw lx lz nb md nc mh nd ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/how-linear-regression-actually-works-theory-and-implementation-8d8dcae3222c"> (a)线性回归的实际工作原理</a></li><li id="b58a" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/how-to-improve-your-linear-regression-with-basis-functions-and-regularization-8a6fcebdc11c"> (b)如何使用基函数和正则化改进您的线性回归</a></li></ul><h2 id="1d10" class="mn kz it bd la mo mp dn le mq mr dp li lz ms mt lk md mu mv lm mh mw mx lo my bi translated">3.分类</h2><ul class=""><li id="585b" class="mz na it ls b lt lu lw lx lz nb md nc mh nd ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/overview-of-classifiers-d0a0d3eecfd1"> (a)分类器概述</a></li><li id="4997" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated"><strong class="ls iu"> (b)二次判别分析(QDA) </strong></li><li id="94e0" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/linear-discriminant-analysis-1894bbf04359"> (c)线性判别分析</a></li><li id="b837" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated"><a class="ae mm" rel="noopener" target="_blank" href="/gaussian-naive-bayes-4d2895d139a"> (d)(高斯)朴素贝叶斯</a></li></ul><h1 id="d46a" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">设置和目标</h1><p id="0c15" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">如<a class="ae mm" rel="noopener" target="_blank" href="/overview-of-classifiers-d0a0d3eecfd1">上一篇</a>所述，生成分类器对输入和目标变量<em class="nn"> P </em> ( <strong class="ls iu"> x </strong>，<em class="nn"> t </em>)的<strong class="ls iu">联合概率分布</strong>进行建模。这意味着，我们将最终得到一个可以生成(因此得名)具有各自目标的新输入变量的分布。</p><p id="cd89" class="pw-post-body-paragraph lq lr it ls b lt no ju lv lw np jx ly lz nq mb mc md nr mf mg mh ns mj mk ml im bi translated">这个模型，我们将在这篇文章中看到，属于一个叫做<strong class="ls iu">高斯判别分析(GDA) </strong>模型的类别。现在是术语开始变得棘手的时候了！注意，高斯<em class="nn">判别</em>分析模型是<em class="nn">生成</em>模型！尽管它的名字是歧视性的，但它不是歧视性的。</p><p id="e4ef" class="pw-post-body-paragraph lq lr it ls b lt no ju lv lw np jx ly lz nq mb mc md nr mf mg mh ns mj mk ml im bi translated">给定具有相应目标变量<em class="nn"> t </em>的<em class="nn"> N </em>输入变量<strong class="ls iu"> x </strong>的训练数据集，GDA模型假设<strong class="ls iu">类条件密度</strong>是正态分布的</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/44c9e0e0878eb0650157db905019fb53.png" data-original-src="https://miro.medium.com/v2/resize:fit:630/1*BYmPJhNObxhQPCoGuBFgPA.gif"/></div></figure><p id="5c0e" class="pw-post-body-paragraph lq lr it ls b lt no ju lv lw np jx ly lz nq mb mc md nr mf mg mh ns mj mk ml im bi translated">其中<strong class="ls iu"> <em class="nn"> μ </em> </strong>是<strong class="ls iu">类特定均值向量</strong>，而<strong class="ls iu">σ</strong>是<strong class="ls iu">类特定协方差矩阵</strong>。利用贝叶斯定理，我们现在可以计算后验概率</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nu"><img src="../Images/8549d676f622525390299627f6b8d41a.png" data-original-src="https://miro.medium.com/v2/resize:fit:966/1*9BQ7c2KQCcKtATszKsZJ0Q.gif"/></div></figure><p id="a4a2" class="pw-post-body-paragraph lq lr it ls b lt no ju lv lw np jx ly lz nq mb mc md nr mf mg mh ns mj mk ml im bi translated">然后我们将把x分类到类中</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/e72ab5f9f343eabff439e2416f672e13.png" data-original-src="https://miro.medium.com/v2/resize:fit:578/1*gZ64liMGTvuktr5t9ziYgg.gif"/></div></figure><h1 id="59bf" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">衍生和培训</h1><p id="f1c0" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">对于每个输入变量，我们定义了<em class="nn"> k个</em>二进制指示变量。此外，让<strong class="ls iu"> t </strong>表示我们所有的目标变量，并且<em class="nn"> π </em>先验带有表示类的下标。假设数据点是独立绘制的，则似然函数由下式给出</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nw"><img src="../Images/44c03a89cf547c94e06bd8619826c0cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1306/1*JYB75w8MzKaA2emn17WHPQ.gif"/></div></div></figure><p id="399f" class="pw-post-body-paragraph lq lr it ls b lt no ju lv lw np jx ly lz nq mb mc md nr mf mg mh ns mj mk ml im bi translated">为了简化符号，让<strong class="ls iu"> <em class="nn"> θ </em> </strong>表示所有的类先验、类特定的均值向量和协方差矩阵。我们知道，<strong class="ls iu">最大化似然相当于最大化对数似然</strong>。对数可能性是</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/036c4afe23df0d19b05467b5bac52ef8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1052/1*L0od-tlKgCjdprhZLBgZxQ.gif"/></div></figure><p id="98d1" class="pw-post-body-paragraph lq lr it ls b lt no ju lv lw np jx ly lz nq mb mc md nr mf mg mh ns mj mk ml im bi translated">扩展(1)将在接下来的衍生中极大地帮助我们:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/0aa82f901bcbfc68eaa71d09d97446eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aZz8W6oHTNg_zmuaBFyCrg.png"/></div></div></figure><p id="9202" class="pw-post-body-paragraph lq lr it ls b lt no ju lv lw np jx ly lz nq mb mc md nr mf mg mh ns mj mk ml im bi translated">我们必须找到特定类别的先验、均值和协方差矩阵的最大似然解。从先验开始，我们必须对(2)求导，将其设为0，并求解先验。然而，我们必须保持约束</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nz"><img src="../Images/6a7e9ab71a3f2e0e3e5b923daadbcc1e.png" data-original-src="https://miro.medium.com/v2/resize:fit:182/1*vEKv8hXiBmBSN7WJYPRo4g.gif"/></div></figure><p id="35c4" class="pw-post-body-paragraph lq lr it ls b lt no ju lv lw np jx ly lz nq mb mc md nr mf mg mh ns mj mk ml im bi translated">这是通过使用拉格朗日乘数<em class="nn"> λ </em>，而不是最大化来实现的</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oa"><img src="../Images/9d6a242bea5b347893aa38d7f4b30936.png" data-original-src="https://miro.medium.com/v2/resize:fit:614/1*zzH0LSDYqMMGu7T1sqOPNA.gif"/></div></figure><p id="8ae1" class="pw-post-body-paragraph lq lr it ls b lt no ju lv lw np jx ly lz nq mb mc md nr mf mg mh ns mj mk ml im bi translated">使用来自(2)的结果，我们然后对(3)关于类特定的先验求导，将其设置为等于0，并且求解</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/2b4155dfae60df9b08b8686d904f5d17.png" data-original-src="https://miro.medium.com/v2/resize:fit:874/1*sjomUrCeFa02ex_46jahjQ.gif"/></div></figure><p id="f656" class="pw-post-body-paragraph lq lr it ls b lt no ju lv lw np jx ly lz nq mb mc md nr mf mg mh ns mj mk ml im bi translated">其中<em class="nn"> Nc </em>是c类中数据点的数量。使用约束的知识，我们可以找到<em class="nn"> λ </em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oc"><img src="../Images/d50f7679eac5a5c0028795a517281b15.png" data-original-src="https://miro.medium.com/v2/resize:fit:326/1*RE34eWiTcnQfJHotK8WwlA.gif"/></div></figure><p id="e79f" class="pw-post-body-paragraph lq lr it ls b lt no ju lv lw np jx ly lz nq mb mc md nr mf mg mh ns mj mk ml im bi translated">将<em class="nn">λ= N</em>代入(4)，我们得到</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/7b474e8c068a39c087c84a9703024041.png" data-original-src="https://miro.medium.com/v2/resize:fit:342/1*ywKNYmspS2o05f4Jv6G-gw.gif"/></div></figure><p id="7c36" class="pw-post-body-paragraph lq lr it ls b lt no ju lv lw np jx ly lz nq mb mc md nr mf mg mh ns mj mk ml im bi translated">(5)告诉我们<strong class="ls iu">类别先验仅仅是属于类别</strong>的数据点的比例，这在直觉上也有意义。</p><p id="a7e4" class="pw-post-body-paragraph lq lr it ls b lt no ju lv lw np jx ly lz nq mb mc md nr mf mg mh ns mj mk ml im bi translated">现在我们转向最大化关于特定类别均值的对数似然。同样，使用(2)的结果使我们很容易求导，将其设为0，并求解</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oe"><img src="../Images/170d2936a2094e1a1452b03e94b24e70.png" data-original-src="https://miro.medium.com/v2/resize:fit:752/1*GrXftslNtdL-bA7l_mj1oA.gif"/></div></div></figure><p id="057c" class="pw-post-body-paragraph lq lr it ls b lt no ju lv lw np jx ly lz nq mb mc md nr mf mg mh ns mj mk ml im bi translated">为了评估这个导数，我们使用一个<a class="ae mm" href="https://en.wikipedia.org/wiki/Matrix_calculus#Scalar-by-vector_identities" rel="noopener ugc nofollow" target="_blank">矩阵演算恒等式</a>。具体身份可以在我个人博客的<a class="ae mm" href="https://cookieblues.github.io/guides/2021/04/01/bsmalea-notes-3b/" rel="noopener ugc nofollow" target="_blank">我更详细的帖子里找到。</a></p><p id="c793" class="pw-post-body-paragraph lq lr it ls b lt no ju lv lw np jx ly lz nq mb mc md nr mf mg mh ns mj mk ml im bi translated">然后我们得到</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/b9c87bcf4cd1aa6c85b39cf9085ef017.png" data-original-src="https://miro.medium.com/v2/resize:fit:944/1*3WTwNzdYNQaDW1s5Kp0BZw.gif"/></div></figure><p id="d853" class="pw-post-body-paragraph lq lr it ls b lt no ju lv lw np jx ly lz nq mb mc md nr mf mg mh ns mj mk ml im bi translated">让我们花一点时间来理解(6)的意思。(6)左侧的总和仅包括属于类别<em class="nn"> c </em>的输入变量<strong class="ls iu"> x </strong>。然后，我们将向量的和除以类中数据点的数量，这与取向量的平均值相同。这意味着<strong class="ls iu">特定类别的平均向量是属于类别</strong>的输入变量的平均值。同样，这也有直观的意义。</p><p id="8b8f" class="pw-post-body-paragraph lq lr it ls b lt no ju lv lw np jx ly lz nq mb mc md nr mf mg mh ns mj mk ml im bi translated">最后，我们必须最大化关于特定类别协方差矩阵的对数似然。同样，我们使用(2)的结果求导，将其设为0，然后求解</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/99f1c4cb0c9b5e50c6ba462b64a93ac4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1050/1*WCmYA1qf454k6DBReT0rKA.gif"/></div></figure><p id="fa8f" class="pw-post-body-paragraph lq lr it ls b lt no ju lv lw np jx ly lz nq mb mc md nr mf mg mh ns mj mk ml im bi translated">这个导数需要3个属性，我已经在我的个人博客上的<a class="ae mm" href="https://cookieblues.github.io/guides/2021/04/01/bsmalea-notes-3b/" rel="noopener ugc nofollow" target="_blank">中列出了我这篇文章的更详细版本。然后我们得到</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/f652b2d6c3f5255e44a5f9728ee9959d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*EaeXjYxN3C3DLGoZvl4CAg.png"/></div></div></figure><p id="267e" class="pw-post-body-paragraph lq lr it ls b lt no ju lv lw np jx ly lz nq mb mc md nr mf mg mh ns mj mk ml im bi translated">就像类特定的均值向量只是类的向量的均值一样，<strong class="ls iu">类特定的协方差矩阵只是类</strong>的向量的协方差，我们最终得到我们的最大似然解(5)、(6)、(7)。因此，我们可以使用以下内容进行分类</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/d51114f755bba2cd69381aaaa64a2b1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1216/1*ldw_MqN3a9PgEcjU3bg6bA.gif"/></div></figure><h1 id="fe9a" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">Python实现</h1><p id="8f2e" class="pw-post-body-paragraph lq lr it ls b lt lu ju lv lw lx jx ly lz ma mb mc md me mf mg mh mi mj mk ml im bi translated">让我们从一些数据开始——你可以在下面的图表中看到。你可以在这里下载数据<a class="ae mm" href="https://github.com/cookieblues/cookieblues.github.io/raw/master/extra/bsmalea-notes-3b/data.csv" rel="noopener ugc nofollow" target="_blank">。</a></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/671c5af6a31df5585ad1b03fee682649.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ILxHLQRtpM-B3r6h0XuJ0A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">要分类的数据点被绘制成散点图。图片作者。</p></figure><p id="e815" class="pw-post-body-paragraph lq lr it ls b lt no ju lv lw np jx ly lz nq mb mc md nr mf mg mh ns mj mk ml im bi translated">下面的代码是我们刚刚讨论过的QDA的简单实现。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ok ol l"/></div></figure><p id="8fc5" class="pw-post-body-paragraph lq lr it ls b lt no ju lv lw np jx ly lz nq mb mc md nr mf mg mh ns mj mk ml im bi translated">我们现在可以用下面的代码进行预测。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="ok ol l"/></div></figure><p id="706d" class="pw-post-body-paragraph lq lr it ls b lt no ju lv lw np jx ly lz nq mb mc md nr mf mg mh ns mj mk ml im bi translated">这为我们提供了高斯分布以及预测，如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/452744fc3689bb15157b3cbc95f809d1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GubcRLx-TXOZsWF6HlQ2Ug.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">使用QDA发现的高斯分布图，以及数据点的预测类别。图片作者。</p></figure><p id="adb8" class="pw-post-body-paragraph lq lr it ls b lt no ju lv lw np jx ly lz nq mb mc md nr mf mg mh ns mj mk ml im bi translated">为了更容易地说明QDA是如何工作的，以及它工作得有多好，我们可以绘制决策边界上数据点的原始类别。这显示在下面。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi om"><img src="../Images/0af24d50104bf14242deaa0c33951440.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nsts_Bm7Tq2WAC-0bQh2RQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">拟合QDA的判定边界以及数据点的原始类别。彩条显示属于类别1的概率。图片作者。</p></figure><h1 id="7285" class="ky kz it bd la lb lc ld le lf lg lh li jz lj ka lk kc ll kd lm kf ln kg lo lp bi translated">摘要</h1><ul class=""><li id="895a" class="mz na it ls b lt lu lw lx lz nb md nc mh nd ml ne nf ng nh bi translated">二次<strong class="ls iu">判别</strong>分析(QDA)是一个<strong class="ls iu">生成型</strong>模型。</li><li id="0f95" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated">QDA假设每个类别都遵循高斯分布。</li><li id="63b7" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated"><strong class="ls iu">特定于类别的先验</strong>就是<strong class="ls iu">属于类别</strong>的数据点的比例。</li><li id="b438" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated"><strong class="ls iu">特定类别的平均向量</strong>是属于类别的输入变量的<strong class="ls iu">平均值。</strong></li><li id="3f30" class="mz na it ls b lt ni lw nj lz nk md nl mh nm ml ne nf ng nh bi translated">特定于类别的协方差矩阵就是属于类别的向量的<strong class="ls iu">协方差。</strong></li></ul></div></div>    
</body>
</html>