<html>
<head>
<title>Master Machine Learning: Random Forest From Scratch With Python</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">机器学习大师:用Python从零开始的随机森林</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/master-machine-learning-random-forest-from-scratch-with-python-3efdd51b6d7a?source=collection_archive---------12-----------------------#2021-04-14">https://towardsdatascience.com/master-machine-learning-random-forest-from-scratch-with-python-3efdd51b6d7a?source=collection_archive---------12-----------------------#2021-04-14</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="18f6" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">机器学习既简单又直观——这里有一个关于随机森林的从头开始的完整指南</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d17048b5fc3a0ffe8c81d48a92e81393.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*UiC_vsVs04BcJECG.jpg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">迪伦·莱格在<a class="ae ky" href="https://unsplash.com/s/photos/tree?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="661c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们已经知道一个单独的决策树可以出奇的好。用单棵树建造森林的想法似乎是自然的下一步。</p><p id="6229" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">今天，您将学习随机森林分类器是如何工作的，并从头开始用Python实现它。这是许多即将开始的文章中的第六篇，所以如果你想了解更多，请继续关注博客。之前文章的链接位于本文末尾。</p><p id="9c93" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇文章的结构如下:</p><ul class=""><li id="2825" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">随机森林简介</li><li id="8dce" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">随机森林背后的数学</li><li id="3e1c" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">从头开始实施</li><li id="bbdd" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">模型评估</li><li id="0bcc" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">与Scikit-Learn的比较</li><li id="0059" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">结论</li></ul><p id="0c90" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在这里下载相应的笔记本<a class="ae ky" href="https://github.com/daradecic/BDS-articles/blob/main/014_MML_Random_Forests.ipynb" rel="noopener ugc nofollow" target="_blank">。</a></p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="0bbb" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">随机森林简介</h1><p id="b988" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">就像决策树一样，随机森林是一种用于回归和分类任务的非参数模型。如果你理解了<a class="ae ky" rel="noopener" target="_blank" href="/master-machine-learning-decision-trees-from-scratch-with-python-de75b0494bcd">之前关于决策树</a>的文章，那么理解这篇文章就没有问题了。</p><p id="e586" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不用说，那篇文章也是这篇文章的先决条件，原因很明显。</p><p id="2a7e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">整个随机森林算法是建立在弱学习器(决策树)之上的，给你一个用树做森林的类比。术语“随机”表示每个决策树都是用随机的数据子集构建的。</p><p id="9a0f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一张对比决策树和随机森林的绝佳图片:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/e73a0d67d66067486ff402012dc6136d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L6kZVUDPwMSPBgu724qEwA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图1——决策树vs .随机森林(来源:<a class="ae ky" href="https://commons.wikimedia.org/wiki/File:Decision_Tree_vs._Random_Forest.png)" rel="noopener ugc nofollow" target="_blank">https://commons . wikimedia . org/wiki/File:Decision _ Tree _ vs . _ Random _ forest . png)</a></p></figure><p id="404f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就这么简单。</p><p id="c2ae" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随机森林算法基于<strong class="lb iu">打包</strong>方法。它代表了一种结合学习模型以提高性能(更高的准确性或一些其他指标)的概念。</p><p id="ffcd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">简而言之:</p><ul class=""><li id="e867" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated">N个子集由原始数据集组成</li><li id="53f2" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">从子集中构建决策树</li><li id="225a" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">对每个经过训练的树进行预测，最终预测作为多数投票返回</li></ul><p id="9b06" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这里有一个图表来说明这些观点:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/967633e86aaa08e59de2436f17ac00a6.png" data-original-src="https://miro.medium.com/v2/resize:fit:1184/format:webp/1*i0o8mjFfCn-uD79-F1Cqkw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片2 —随机森林图(来源:<a class="ae ky" href="https://commons.wikimedia.org/wiki/File:Random_forest_diagram_complete.png)" rel="noopener ugc nofollow" target="_blank">https://commons . wikimedia . org/wiki/File:Random _ forest _ diagram _ complete . png)</a></p></figure><p id="d402" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来让我们回顾一下算法背后的数学。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="3b84" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">随机森林背后的数学</h1><p id="1089" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">好消息——今天没有数学！</p><p id="e23b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">随机森林背后的数学原理与决策树相同。你只需要实现两个公式——熵和信息增益。</p><p id="28f7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果这些听起来像外语，请参考<a class="ae ky" rel="noopener" target="_blank" href="/master-machine-learning-decision-trees-from-scratch-with-python-de75b0494bcd">上一篇文章</a>。这两个概念在那里都有详细的讨论。</p><p id="85b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文的其余部分假设您熟悉决策树的内部工作原理，因为从头开始构建算法需要用到决策树。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="9d07" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">从头开始实施</h1><p id="9f51" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">这次我们需要三节课:</p><ol class=""><li id="14f7" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu nq mb mc md bi translated"><code class="fe nr ns nt nu b">Node</code> -实现决策树的单个节点</li><li id="6104" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nq mb mc md bi translated"><code class="fe nr ns nt nu b">DecisionTree</code> -实现单个决策树</li><li id="8bfd" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu nq mb mc md bi translated"><code class="fe nr ns nt nu b">RandomForest</code> -实现我们的集成算法</li></ol><p id="71f9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">前两个类与前一篇文章中的相同，所以如果您已经编写了它们，请随意跳过。</p><p id="9961" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">先说<code class="fe nr ns nt nu b">Node</code>级。它在这里存储关于特征、阈值、向左和向右的数据、信息增益和叶节点值的数据。所有的初始设置都是<code class="fe nr ns nt nu b">None</code>。根节点和决策节点将包含除叶节点值之外的所有值，而叶节点将包含相反的值。</p><p id="d0b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是该类的代码(与库导入一起):</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div></figure><p id="3752" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来让我们实现决策树分类器。它将包含许多方法，所有这些方法都将在下面讨论:</p><ul class=""><li id="1f8e" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><code class="fe nr ns nt nu b">__init__()</code> -构造函数，保存<code class="fe nr ns nt nu b">min_samples_split</code>和<code class="fe nr ns nt nu b">max_depth</code>的值。这些是超参数。第一个用于指定分割节点所需的最小样本数，第二个指定树的最大深度。两者都在递归函数中用作退出条件</li><li id="d1d9" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><code class="fe nr ns nt nu b">_entropy(s)</code> -计算输入向量的杂质<code class="fe nr ns nt nu b">s</code></li><li id="cfe4" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><code class="fe nr ns nt nu b">_information_gain(parent, left_child, right_child)</code>计算父节点和两个子节点之间拆分的信息增益值</li><li id="7819" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><code class="fe nr ns nt nu b">_best_split(X, y)</code>函数计算输入特征<code class="fe nr ns nt nu b">X</code>和目标变量<code class="fe nr ns nt nu b">y</code>的最佳分割参数。它通过迭代<code class="fe nr ns nt nu b">X</code>中的每一列和每一列中的每个阈值来使用信息增益找到最佳分割</li><li id="fe52" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><code class="fe nr ns nt nu b">_build(X, y, depth)</code>函数递归构建决策树，直到满足停止标准(构造函数中的超参数)</li><li id="b3e7" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><code class="fe nr ns nt nu b">fit(X, y)</code>函数调用<code class="fe nr ns nt nu b">_build()</code>函数并将构建的树存储到构造函数中</li><li id="58be" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><code class="fe nr ns nt nu b">_predict(x)</code>函数遍历树来分类单个实例</li><li id="fbdb" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><code class="fe nr ns nt nu b">predict(X)</code>函数将<code class="fe nr ns nt nu b">_predict()</code>函数应用于矩阵<code class="fe nr ns nt nu b">X</code>中的每个实例。</li></ul><p id="5b75" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">是的，这是很多，但你应该已经觉得很舒服了。下面是单个决策树的代码片段:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div></figure><p id="391f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，让我们建造森林。该类建立在单个决策树之上，具有以下方法:</p><ul class=""><li id="244d" class="lv lw it lb b lc ld lf lg li lx lm ly lq lz lu ma mb mc md bi translated"><code class="fe nr ns nt nu b">__init__()</code> -构造函数，保存森林中树的数量、最小样本分割和最大深度的超参数值。一旦模型被训练，它还将保存单独训练的决策树</li><li id="60f3" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><code class="fe nr ns nt nu b">_sample(X, y)</code>函数将引导取样应用于输入特征和输入目标</li><li id="c531" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><code class="fe nr ns nt nu b">fit(X, y)</code>函数训练分类器模型</li><li id="c25c" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><code class="fe nr ns nt nu b">predict(X)</code>函数使用单个决策树进行预测，然后对最终预测应用多数投票</li></ul><p id="a5e6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">就代码而言，这是一个比决策树简单得多的类。以下是完整的片段:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div></figure><p id="d006" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可能无法一次完全理解所有内容，但如果您理解决策树，这不会是太大的挑战。</p><p id="7240" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来让我们训练和评估我们的模型。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="48d3" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">模型评估</h1><p id="ba05" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">接下来让我们测试我们的分类器。我们将使用来自Scikit-Learn的虹膜数据集。以下代码片段加载数据集并将其分为要素(<code class="fe nr ns nt nu b">X</code>)和目标(<code class="fe nr ns nt nu b">y</code>):</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div></figure><p id="7356" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，让我们将数据集分成训练和测试部分。以下代码片段就是这样做的，比例为80:20:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div></figure><p id="5a10" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在让我们开始训练。下面的代码片段使用默认超参数训练模型，并对测试集进行预测:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div></figure><p id="c5dd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们来看看生成的预测(<code class="fe nr ns nt nu b">preds</code>):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/a9ec2a84983c1fade73e1bfeed16fdc8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1394/format:webp/1*nGgZGwayVvRvXDKyf7x3Xw.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3 —测试集上的自定义随机森林预测(图片由作者提供)</p></figure><p id="b550" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在看看实际的类标签(<code class="fe nr ns nt nu b">y_test</code>):</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ny"><img src="../Images/bfb3a52b12c7b04d58e8f9660e5fb0d0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ZqG5_UmTUpSF8YoYSJd14w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图4 —测试集类别标签(作者图片)</p></figure><p id="522a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所见，两者完全相同，表明分类器非常准确。如果你愿意，你可以进一步评估性能。下面的代码打印测试集的准确度分数:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div></figure><p id="ab28" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果您要运行上面的代码，<code class="fe nr ns nt nu b">1.0</code>的值将被打印出来，表示一个完美的分类器。虹膜数据集非常容易正确分类，所以不要让这个欺骗了你。</p><p id="187b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们将我们的分类器与Scikit内置的分类器进行比较。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="3e41" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">与Scikit-Learn的比较</h1><p id="a21d" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">我们想知道我们的模型是否好，所以让我们将它与我们知道效果很好的东西——Scikit-Learn的<code class="fe nr ns nt nu b">RandomForestClassifier</code>类进行比较。</p><p id="e81a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以使用以下代码片段来导入模型类、训练模型、进行预测以及打印准确性得分:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nv nw l"/></div></figure><p id="b255" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如您所料，我们得到了1.0的完美准确度分数。</p><p id="ee6e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">今天到此为止。让我们在下一部分总结一下。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="6743" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">结论</h1><p id="69f4" class="pw-post-body-paragraph kz la it lb b lc ni ju le lf nj jx lh li nk lk ll lm nl lo lp lq nm ls lt lu im bi translated">现在你知道了——如何从树木中建造一片森林。这比你想象的要容易，特别是如果你考虑到随机森林是当今性能最好的机器学习算法之一。</p><p id="18a4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您现在知道如何从头开始实现决策树分类器算法。<em class="no">这是否意味着你应该抛弃事实上的标准机器学习库？没有，一点也没有。我来详细说明一下。</em></p><p id="6e0e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你能从头开始写东西并不意味着你应该这样做。尽管如此，了解算法如何工作的每个细节是一项宝贵的技能，可以帮助你从其他<em class="no"> fit和预测</em>数据科学家中脱颖而出。</p><p id="0a74" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢您的阅读，如果您对更多从零开始的机器学习文章感兴趣，请继续关注博客。</p></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="d3c6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="no">喜欢这篇文章吗？成为</em> <a class="ae ky" href="https://medium.com/@radecicdario/membership" rel="noopener"> <em class="no">中等会员</em> </a> <em class="no">继续无限制学习。如果你使用下面的链接，我会收到你的一部分会员费，不需要你额外付费。</em></p><div class="nz oa gp gr ob oc"><a href="https://medium.com/@radecicdario/membership" rel="noopener follow" target="_blank"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd iu gy z fp oh fr fs oi fu fw is bi translated">通过我的推荐链接加入Medium-Dario rade ci</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">medium.com</p></div></div><div class="ol l"><div class="om l on oo op ol oq ks oc"/></div></div></a></div></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="c85e" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">了解更多信息</h1><ul class=""><li id="829f" class="lv lw it lb b lc ni lf nj li or lm os lq ot lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/master-machine-learning-simple-linear-regression-from-scratch-with-python-1526487c5964">掌握机器学习:用Python从头开始简单线性回归</a></li><li id="2cee" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/master-machine-learning-multiple-linear-regression-from-scratch-with-python-ac716a9b78a4">掌握机器学习:用Python从头开始多元线性回归</a></li><li id="efd3" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/master-machine-learning-logistic-regression-from-scratch-with-python-acfe73a0a424">机器学习大师:用Python从头开始逻辑回归</a></li><li id="ea03" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/master-machine-learning-k-nearest-neighbors-from-scratch-with-python-5009177f523">机器学习高手:用Python从零开始K近邻</a></li><li id="5e8d" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated"><a class="ae ky" rel="noopener" target="_blank" href="/master-machine-learning-decision-trees-from-scratch-with-python-de75b0494bcd">机器学习大师:用Python从头开始做决策树</a></li></ul></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><h1 id="90ab" class="mq mr it bd ms mt mu mv mw mx my mz na jz nb ka nc kc nd kd ne kf nf kg ng nh bi translated">保持联系</h1><ul class=""><li id="2632" class="lv lw it lb b lc ni lf nj li or lm os lq ot lu ma mb mc md bi translated">关注我在<a class="ae ky" href="https://medium.com/@radecicdario" rel="noopener">媒体</a>上的更多类似的故事</li><li id="d0c0" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">注册我的<a class="ae ky" href="https://mailchi.mp/46a3d2989d9b/bdssubscribe" rel="noopener ugc nofollow" target="_blank">简讯</a></li><li id="cf6f" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">在<a class="ae ky" href="https://www.linkedin.com/in/darioradecic/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上连接</li><li id="6b36" class="lv lw it lb b lc me lf mf li mg lm mh lq mi lu ma mb mc md bi translated">查看我的<a class="ae ky" href="https://www.betterdatascience.com/" rel="noopener ugc nofollow" target="_blank">网站</a></li></ul></div><div class="ab cl mj mk hx ml" role="separator"><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo mp"/><span class="mm bw bk mn mo"/></div><div class="im in io ip iq"><p id="456c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="no">原载于2021年4月14日https://betterdatascience.com</em><a class="ae ky" href="https://betterdatascience.com/mml-random-forest/" rel="noopener ugc nofollow" target="_blank"><em class="no"/></a><em class="no">。</em></p></div></div>    
</body>
</html>