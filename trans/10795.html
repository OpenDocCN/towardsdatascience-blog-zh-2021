<html>
<head>
<title>How to Optimize Learning Rate with TensorFlow — It’s Easier Than You Think</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">如何用TensorFlow优化学习速度——比你想象的要简单</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-to-optimize-learning-rate-with-tensorflow-its-easier-than-you-think-164f980a7c7b?source=collection_archive---------4-----------------------#2021-10-19">https://towardsdatascience.com/how-to-optimize-learning-rate-with-tensorflow-its-easier-than-you-think-164f980a7c7b?source=collection_archive---------4-----------------------#2021-10-19</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="56c5" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">显著改进您的模型并不需要太多时间——以下是开始的方法</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/d4b7f57a533e0e44d4039e795e7298cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7-2pbiWIPLPPXJQh72e2tg.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由<a class="ae ky" href="https://unsplash.com/@dissii?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> mahdis mousavi </a>在<a class="ae ky" href="https://unsplash.com/s/photos/machine-learning?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="3178" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">调整神经网络模型不是闹着玩的。有如此多的超参数需要优化，使用网格搜索方法一次性优化所有这些参数可能需要几周甚至几个月的时间。<em class="lv">学习率</em>是一个超参数，只要你知道怎么做，你可以在几分钟内调整好。这篇文章将教你如何做。</p><p id="7703" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">学习率控制根据估计误差更新多少权重。选择太小的值，你的模型将永远训练，并可能卡住。选择过大的学习率，你的模型可能会在训练过程中跳过最优的一组权重。</p><p id="3374" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您将需要安装TensorFlow 2+、Numpy、Pandas、Matplotlib和Scikit-Learn来跟进。</p><p id="3423" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">不想看书？请观看我的视频:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="lw lx l"/></div></figure><p id="c0a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以在<a class="ae ky" href="https://github.com/better-data-science/TensorFlow" rel="noopener ugc nofollow" target="_blank"> GitHub </a>上下载源代码。</p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><h1 id="4ed3" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">使用的数据集和数据预处理</h1><p id="377e" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">我不打算在这里花太多时间。我们将使用与<a class="ae ky" rel="noopener" target="_blank" href="/how-to-train-a-classification-model-with-tensorflow-in-10-minutes-fd2b7cfba86">上一篇文章</a>相同的数据集——来自Kaggle的<a class="ae ky" href="https://www.kaggle.com/shelvigarg/wine-quality-dataset" rel="noopener ugc nofollow" target="_blank">葡萄酒质量数据集</a>:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nc"><img src="../Images/22e312d8d26f36ca55853f206f206d63.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*T0AUc1nOEi-cOsLhRfKSjg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片1——来自Kaggle的葡萄酒质量数据集(图片由作者提供)</p></figure><p id="ef59" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">您可以使用以下代码将其导入Python，并随机打印几行:</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="8967" class="ni mg it ne b gy nj nk l nl nm">import os<br/>import numpy as np<br/>import pandas as pd<br/>import warnings<br/>os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' <br/>warnings.filterwarnings('ignore')<br/><br/>df = pd.read_csv('data/winequalityN.csv')<br/>df.sample(5)</span></pre><p id="0d94" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们忽略警告并更改默认的TensorFlow日志级别，这样我们就不会被输出淹没。</p><p id="bc9a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">以下是数据集的外观:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/feed837a5b8f9a02954e1367cd5078bb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*nPiseuNzkEXHQ_jy_YM_iA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图2——葡萄酒质量数据集的随机样本(图片由作者提供)</p></figure><p id="da05" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">数据集基本上是干净的，但默认情况下不是为二元分类(好酒/劣酒)而设计的。取而代之的是，葡萄酒是按等级来评定的。我们现在将解决这个问题，还有其他一些问题:</p><ul class=""><li id="3b9e" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu nt nu nv nw bi translated"><strong class="lb iu">删除缺失值</strong> —它们为数不多，所以我们不会在插补上浪费时间。</li><li id="9fbe" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated"><strong class="lb iu">处理分类特征</strong>——唯一的一个是<code class="fe oc od oe ne b">type</code>，指示葡萄酒是白还是红。</li><li id="0880" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated"><strong class="lb iu">转换为二分分类任务</strong>——我们将宣布任何等级为6及以上的葡萄酒为<em class="lv">好</em>，任何等级以下的为<em class="lv">差</em>。</li><li id="37b0" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated"><strong class="lb iu">训练/测试分流</strong>——经典的80:20分流。</li><li id="5e5f" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated"><strong class="lb iu">缩放数据</strong> —预测值之间的比例差异很大，因此我们将使用<code class="fe oc od oe ne b">StandardScaler</code>来拉近数值。</li></ul><p id="72c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是完整的数据预处理代码片段:</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="2a72" class="ni mg it ne b gy nj nk l nl nm">from sklearn.model_selection import train_test_split<br/>from sklearn.preprocessing import StandardScaler<br/><br/><br/># Prepare the data<br/>df = df.dropna()<br/>df['is_white_wine'] = [<br/>    1 if typ == 'white' else 0 for typ in df['type']<br/>]<br/>df['is_good_wine'] = [<br/>    1 if quality &gt;= 6 else 0 for quality in df['quality']<br/>]<br/>df.drop(['type', 'quality'], axis=1, inplace=True)<br/><br/># Train/test split<br/>X = df.drop('is_good_wine', axis=1)<br/>y = df['is_good_wine']<br/>X_train, X_test, y_train, y_test = train_test_split(<br/>    X, y, <br/>    test_size=0.2, random_state=42<br/>)<br/><br/># Scaling<br/>scaler = StandardScaler()<br/>X_train_scaled = scaler.fit_transform(X_train)<br/>X_test_scaled = scaler.transform(X_test)</span></pre><p id="60e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是前几个缩放行的样子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/301bb5beb19ce875adb48905e1dfc055.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aWtcDgQifPgS4ZIoNhX9cg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3 —缩放的训练集(图片由作者提供)</p></figure><p id="af9a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同样，如果你想更详细地了解数据预处理背后的逻辑，请参考<a class="ae ky" rel="noopener" target="_blank" href="/how-to-train-a-classification-model-with-tensorflow-in-10-minutes-fd2b7cfba86">上一篇文章</a>。</p><p id="8adb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">说完这些，让我们看看如何优化学习率。</p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><h1 id="751d" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">如何在TensorFlow中优化学习率</h1><p id="88d0" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">一旦你掌握了要点，优化学习速度就很容易了。我们的想法是从小处着手，比如说从0.001开始，然后在每个时期增加这个值。在训练模型时，你会得到可怕的准确性，但这是意料之中的。不要介意，因为我们只对当我们改变学习率时<em class="lv">损失</em>如何变化感兴趣。</p><p id="282f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们从导入TensorFlow并设置种子开始，这样您就可以重现结果:</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="1bae" class="ni mg it ne b gy nj nk l nl nm">import tensorflow as tf<br/>tf.random.set_seed(42)</span></pre><p id="0d66" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将训练100个时期的模型，以测试100种不同的损失/学习率组合。学习率值的范围如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/6a86d061a6b620546a89aebb0cfc7c71.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dYlyL6-LEsY0anO_xR_tEQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片4-学习率值的范围(图片由作者提供)</p></figure><p id="bde9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">比如说，0.001的学习率是Adam optimizer的默认学习率，2.15显然太大了。</p><p id="3c32" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，让我们定义一个神经网络模型架构，编译该模型，并对其进行训练。这里唯一的新东西是<code class="fe oc od oe ne b">LearningRateScheduler</code>。它允许我们输入上面声明的方法来改变学习率作为一个lambda函数。</p><p id="fde1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是完整的代码:</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="faa7" class="ni mg it ne b gy nj nk l nl nm">initial_model = tf.keras.Sequential([<br/>    tf.keras.layers.Dense(128, activation='relu'),<br/>    tf.keras.layers.Dense(256, activation='relu'),<br/>    tf.keras.layers.Dense(256, activation='relu'),<br/>    tf.keras.layers.Dense(1, activation='sigmoid')<br/>])<br/><br/>initial_model.compile(<br/>    loss=tf.keras.losses.binary_crossentropy,<br/>    optimizer=tf.keras.optimizers.Adam(),<br/>    metrics=[<br/>        tf.keras.metrics.BinaryAccuracy(name='accuracy')<br/>    ]<br/>)<br/><br/>initial_history = initial_model.fit(<br/>    X_train_scaled,<br/>    y_train,<br/>    epochs=100,<br/>    <strong class="ne iu">callbacks=[<br/>        tf.keras.callbacks.LearningRateScheduler(<br/>            lambda epoch: 1e-3 * 10 ** (epoch / 30)<br/>        )<br/>    ]</strong><br/>)</span></pre><p id="e7d6" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">训练将从现在开始，你会立即看到一个相当不错的准确率——大约75%——但在50多岁之后，它会下降，因为学习率变得太大了。经过100个时代后，<code class="fe oc od oe ne b">initial_model</code>有了大约60%的准确率:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oh"><img src="../Images/2a511fa12b9c3e290d51cad58d44d2a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ad1QjisDBJtib_F53etfqA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图5 —初始模型训练日志(图片由作者提供)</p></figure><p id="7a5d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe oc od oe ne b">initial_history</code>变量现在有关于损失、准确性和学习率的信息。让我们把它们都标出来:</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="5f8e" class="ni mg it ne b gy nj nk l nl nm">import matplotlib.pyplot as plt<br/>from matplotlib import rcParams<br/>rcParams['figure.figsize'] = (18, 8)<br/>rcParams['axes.spines.top'] = False<br/>rcParams['axes.spines.right'] = False <br/><br/>plt.plot(<br/>    np.arange(1, 101), <br/>    initial_history.history['loss'], <br/>    label='Loss', lw=3<br/>)<br/>plt.plot(<br/>    np.arange(1, 101), <br/>    initial_history.history['accuracy'], <br/>    label='Accuracy', lw=3<br/>)<br/>plt.plot(<br/>    np.arange(1, 101), <br/>    initial_history.history['lr'], <br/>    label='Learning rate', color='#000', lw=3, linestyle='--'<br/>)<br/>plt.title('Evaluation metrics', size=20)<br/>plt.xlabel('Epoch', size=14)<br/>plt.legend();</span></pre><p id="f2e2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是图表:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oi"><img src="../Images/8f35a8c85e0befb439a6b21c344403df.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*GNRuFFimy6BZQZ4Nsi5O4A.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图6——损失与准确性和学习速度(图片由作者提供)</p></figure><p id="a7c8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在开始进一步下降之前，精确度在时期50左右显著下降并平坦了一段时间。loss发生了完全相反的情况，这是有道理的。</p><p id="bb27" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，您可以用对数标度绘制损失与学习率的关系图，显示损失最小的位置:</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="e895" class="ni mg it ne b gy nj nk l nl nm">learning_rates = 1e-3 * (10 ** (np.arange(100) / 30))<br/>plt.semilogx(<br/>    learning_rates, <br/>    initial_history.history['loss'], <br/>    lw=3, color='#000'<br/>)<br/>plt.title('Learning rate vs. loss', size=20)<br/>plt.xlabel('Learning rate', size=14)<br/>plt.ylabel('Loss', size=14);</span></pre><p id="bb06" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是图表:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/ade6e55bf10c22def4f45a52e22d6c6d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ROlvVxyll7zBt2RXCm634g.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图7——学习率与损失(作者图片)</p></figure><p id="acfa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">一般来说，你会希望选择一个学习率来实现最低的损失，前提是它周围的值不会太不稳定。请记住，X轴是对数刻度。最佳学习率在0.007左右:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oj"><img src="../Images/e611ba7d37159cecad8f5cdcbcb2ccbb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pCD4bZ8cSIKgEURsyAY9nw.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图8——最佳学习率(图片由作者提供)</p></figure><p id="dd85" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，让我们用一个假定的最佳学习率来训练一个模型，看看我们是否能胜过默认的学习率。</p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><h1 id="5771" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">以最佳学习速率训练模型</h1><p id="5f68" class="pw-post-body-paragraph kz la it lb b lc mx ju le lf my jx lh li mz lk ll lm na lo lp lq nb ls lt lu im bi translated">考虑到学习率为0.007，我们再写一个神经网络模型。这次你不需要<code class="fe oc od oe ne b">LearningRateScheduler</code>:</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="b73e" class="ni mg it ne b gy nj nk l nl nm">model_optimized = tf.keras.Sequential([<br/>    tf.keras.layers.Dense(128, activation='relu'),<br/>    tf.keras.layers.Dense(256, activation='relu'),<br/>    tf.keras.layers.Dense(256, activation='relu'),<br/>    tf.keras.layers.Dense(1, activation='sigmoid')<br/>])<br/><br/>model_optimized.compile(<br/>    loss=tf.keras.losses.binary_crossentropy,<br/>    optimizer=tf.keras.optimizers.Adam(learning_rate=0.007),<br/>    metrics=[<br/>        tf.keras.metrics.BinaryAccuracy(name='accuracy')<br/>    ]<br/>)<br/><br/>history_optimized = model_optimized.fit(<br/>    X_train_scaled,<br/>    y_train,<br/>    epochs=100<br/>)</span></pre><p id="50b8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在<a class="ae ky" rel="noopener" target="_blank" href="/how-to-train-a-classification-model-with-tensorflow-in-10-minutes-fd2b7cfba86">上一篇文章</a>中，我们用默认的学习率得到了76%的准确率，所以看看学习率优化是否能提高它会很有趣。训练集上报告的准确性看起来好得令人难以置信，因此我们的模型很可能过度拟合:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ok"><img src="../Images/ce92510e3df47deaf98c7c53303af8d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H19GK2XzL-h7T2W3kWp80Q.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图9-优化的模型训练日志(图片由作者提供)</p></figure><p id="f424" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们成功地提高了测试集的性能，这不会有太大的影响，但是您可以通过为更少的时期训练模型来节省一些时间。</p><p id="4f40" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">下面是优化模型的精度与损耗的关系:</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="e181" class="ni mg it ne b gy nj nk l nl nm">plt.plot(<br/>    np.arange(1, 101), <br/>    history_optimized.history['loss'], <br/>    label='Loss', lw=3<br/>)<br/>plt.plot(<br/>    np.arange(1, 101), <br/>    history_optimized.history['accuracy'], <br/>    label='Accuracy', lw=3<br/>)<br/>plt.title('Accuracy vs. Loss per epoch', size=20)<br/>plt.xlabel('Epoch', size=14)<br/>plt.legend()</span></pre><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ol"><img src="../Images/d74123f9a3d31f7f0be2a67cd09a1256.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*j4RRBVqi0UWQ2fOJibgWPQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图10 —训练集的准确度与损失(图片由作者提供)</p></figure><p id="793f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们最后计算预测，并根据测试集对它们进行评估。代码如下:</p><pre class="kj kk kl km gt nd ne nf ng aw nh bi"><span id="0841" class="ni mg it ne b gy nj nk l nl nm">from sklearn.metrics import confusion_matrix<br/>from sklearn.metrics import accuracy_score<br/><br/>predictions = model_optimized.predict(X_test_scaled)<br/>prediction_classes = [1 if prob &gt; 0.5 else 0 for prob in np.ravel(predictions)]<br/><br/><br/>print(f'Accuracy on the test set: <br/>    {accuracy_score(y_test, prediction_classes):.2f}')<br/>print()<br/>print('Confusion matrix:')<br/>print(confusion_matrix(y_test, prediction_classes))</span></pre><p id="9c25" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是输出结果:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/8457c20e1b54bd6bb6be500c2e215b16.png" data-original-src="https://miro.medium.com/v2/resize:fit:1208/format:webp/1*aISYZatRN_x_EQz5NhiFdg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图11 —测试集评估指标(作者图片)</p></figure><p id="7975" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总之，仅优化学习率就设法在测试集上将模型准确度提高了3%。这听起来可能不大，但对于所花的时间来说，这是一个很好的权衡。此外，这只是你可以对神经网络模型进行的许多优化中的第一步，它是你需要担心的一个更少的超参数。</p><p id="da29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请继续关注学习如何优化神经网络架构——帖子将在几天后发布。感谢阅读！</p></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><p id="835b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">喜欢这篇文章吗？成为</em> <a class="ae ky" href="https://medium.com/@radecicdario/membership" rel="noopener"> <em class="lv">中等会员</em> </a> <em class="lv">继续无限制学习。如果你使用下面的链接，我会收到你的一部分会员费，不需要你额外付费。</em></p><div class="on oo gp gr op oq"><a href="https://medium.com/@radecicdario/membership" rel="noopener follow" target="_blank"><div class="or ab fo"><div class="os ab ot cl cj ou"><h2 class="bd iu gy z fp ov fr fs ow fu fw is bi translated">通过我的推荐链接加入Medium-Dario rade ci</h2><div class="ox l"><h3 class="bd b gy z fp ov fr fs ow fu fw dk translated">作为一个媒体会员，你的会员费的一部分会给你阅读的作家，你可以完全接触到每一个故事…</h3></div><div class="oy l"><p class="bd b dl z fp ov fr fs ow fu fw dk translated">medium.com</p></div></div><div class="oz l"><div class="pa l pb pc pd oz pe ks oq"/></div></div></a></div></div><div class="ab cl ly lz hx ma" role="separator"><span class="mb bw bk mc md me"/><span class="mb bw bk mc md me"/><span class="mb bw bk mc md"/></div><div class="im in io ip iq"><h1 id="0ea6" class="mf mg it bd mh mi mj mk ml mm mn mo mp jz mq ka mr kc ms kd mt kf mu kg mv mw bi translated">保持联系</h1><ul class=""><li id="894e" class="no np it lb b lc mx lf my li pf lm pg lq ph lu nt nu nv nw bi translated">注册我的<a class="ae ky" href="https://mailchi.mp/46a3d2989d9b/bdssubscribe" rel="noopener ugc nofollow" target="_blank">简讯</a></li><li id="2d1a" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">在YouTube<a class="ae ky" href="https://www.youtube.com/c/BetterDataScience" rel="noopener ugc nofollow" target="_blank">上订阅</a></li><li id="548f" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">在<a class="ae ky" href="https://www.linkedin.com/in/darioradecic/" rel="noopener ugc nofollow" target="_blank"> LinkedIn </a>上连接</li></ul></div></div>    
</body>
</html>