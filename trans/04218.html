<html>
<head>
<title>A step further in the creation of a sign language translation system based on artificial intelligence</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在创建基于人工智能的手语翻译系统方面又进了一步</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-step-further-in-the-creation-of-a-sign-language-translation-system-based-on-artificial-9805c2ae0562?source=collection_archive---------31-----------------------#2021-04-09">https://towardsdatascience.com/a-step-further-in-the-creation-of-a-sign-language-translation-system-based-on-artificial-9805c2ae0562?source=collection_archive---------31-----------------------#2021-04-09</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="04a4" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">大规模实现可访问性的策略</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/d46e5bce2da6485132f87410c333b168.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*Zo5qC7BO4hfH-Ka-"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">乔·希尔顿在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片</p></figure><p id="d306" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">通信是我们社会的基础，人们每天用它来表达自己，并获得最基本的服务，如公共交通、学校和医疗保健。所有国家的重度听力损失者都使用手语，全世界有数百万人患有这种疾病。问题是，大多数听力正常的人不知道如何通过手势说话，这就造成了一种障碍，使聋人很难进行社会互动。</p><p id="370d" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了转移这个障碍，我们可以使用人工智能技术，如卷积神经网络，来创建一个符号翻译系统，并为执行的符号生成图例。</p><p id="a2b0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另一个有趣的点是，虽然每个国家都有自己的手语，但深度学习架构可以很好地概括相同领域的问题，只需一些训练和超参数优化。</p><blockquote class="ls lt lu"><p id="8248" class="kw kx lv ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">然而，对于世界上所有的手语来说，这个过程中有一个部分是昂贵、耗时和重复的:<strong class="ky ir">数据集创建</strong>。</p></blockquote><p id="e2f7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">想象一下，有人用美国手语(ASL)创建了一个最先进的架构来识别标志并以非常高的准确度生成传说。为了在实践中实现这种解决方案，并使其在现实世界中具有可访问性，每个不使用美国手语的国家的科学家都需要创建一个巨大的数据集(例如，使用日常使用的最常用的单词)来重新训练网络。因此，很明显，主要瓶颈之一是数据集创建！</p><p id="5c25" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在此基础上，我将探索如何更有效地为手语创建数据集以训练高精度模型的一些发现，作为未来工作的指南。</p><blockquote class="ls lt lu"><p id="3ab1" class="kw kx lv ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">本文基于我的一篇名为:<a class="ae kv" href="https://arxiv.org/abs/2103.12233" rel="noopener ugc nofollow" target="_blank"> <em class="iq">基于深度学习和图像处理的高效手语识别系统和数据集创建方法</em> </a> <em class="iq">的论文。</em></p></blockquote><h1 id="a0ab" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">实验数据集</h1><p id="84c1" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">这项工作的主要想法是基于<strong class="ky ir">廉价数据集</strong>创建一个手语识别系统，帮助未来需要这样做的工作。</p><blockquote class="ls lt lu"><p id="b8e4" class="kw kx lv ky b kz la jr lb lc ld ju le lw lg lh li lx lk ll lm ly lo lp lq lr ij bi translated">但是什么是廉价数据集呢？</p></blockquote><p id="9115" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我看来，这是一个数据集，它使用一个简单的传感器，如RGB相机，几个解释器，以及记录中相同的背景。</p><p id="f919" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这就是为什么我们创建了一个数据集，记录来自两个不同智能手机的视频，在相同的标准背景下，有两个解释器，配置一个简单而容易的设置。</p><p id="ac97" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">另一个疑问是捕捉记录或二次采样图像的每秒帧数(FPS ),一旦它可以在最终性能中产生不同的结果，因此我们使用相同的程序创建了两个数据集，其中第一个以60 FPS记录，第二个以30 FPS记录。</p><p id="baed" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，我们还对第一个数据集进行了30和20 FPS的二次采样，对第二个数据集进行了20 FPS的二次采样，以测试我们是否可以在记录完成后减少图像数量，而不会影响结果。</p><p id="a22a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后，我们记录了聋人每天使用的14个手势，每个手势重复三次，每次重复之间有一些变化。数据集可以在<a class="ae kv" href="https://www.kaggle.com/alvarole/brazilian-sign-language-words-recognition" rel="noopener ugc nofollow" target="_blank"> Kaggle </a>上找到。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mw"><img src="../Images/313bd6d1cb6b897e9c5a624801022135.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/format:webp/1*lzWnlr9c97ke5Yn_lpCnDA.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">我们数据集中考虑的单词列表。</p></figure><p id="5129" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了证明模型的效率，我们创建了一个最终的验证数据集，考虑了相同的14个标志，但在不同的背景和光照条件下进行记录，试图再现真实世界的场景。</p><div class="kg kh ki kj gt ab cb"><figure class="mx kk my mz na nb nc paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/2cdda76711610e9ffc8ce85302e2d3d8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*koLAePgTLM3q6fMHaHF2wA.jpeg"/></div></figure><figure class="mx kk my mz na nb nc paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/8de0e8b3b294fc6038763dfede47c907.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*knx6UMikKUJuY_WAgRjiYQ.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk nd di ne nf translated">使用相同的背景和光照条件训练和测试图像。(图片由作者提供)</p></figure></div><div class="ab cb"><figure class="mx kk my mz na nb nc paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/2a0ae393ecab771fa88066a60b5568a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*c-Sduc6zurpABL50ZIRqRQ.jpeg"/></div></figure><figure class="mx kk my mz na nb nc paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><img src="../Images/c188f4a10fb28ccc9010eb8eede820f7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1000/format:webp/1*uuwFnrS-kyp_Va4YPLX-Tw.jpeg"/></div><p class="kr ks gj gh gi kt ku bd b be z dk nd di ne nf translated">验证设置，使用不同的背景和光线条件。(图片由作者提供)。</p></figure></div><h1 id="3125" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">假设和实验</h1><p id="f70f" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">这些是这项工作的主要假设，基于研究过程中出现的问题以及我们为回答这些问题所做的实验。</p><h2 id="66c6" class="ng ma iq bd mb nh ni dn mf nj nk dp mj lf nl nm ml lj nn no mn ln np nq mp nr bi translated">一旦减少运动模糊，60 FPS可能比30 FPS更好。</h2><p id="75f4" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">必须有运动才能产生标志，而运动会在视频记录中产生模糊。在这种情况下，使用更大的FPS应该减少模糊，并且这可以提高模型精度。</p><p id="160b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">实验:</strong>我们将比较以30和60 FPS记录的数据集的准确性</p><h2 id="cb62" class="ng ma iq bd mb nh ni dn mf nj nk dp mj lf nl nm ml lj nn no mn ln np nq mp nr bi translated">人工背景创建可以提高模型的泛化能力</h2><p id="cd80" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">在同一个静态背景下记录所有标志这比通过编程改变场景、移动设备和人员要容易得多。但我们认为，这可能会导致偏差，从而影响验证数据集上的模型准确性。</p><p id="d785" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">实验:</strong>我们将使用语义分割来创建新的数据集背景，并用清晰的场景来训练模型。</p><h2 id="fd2a" class="ng ma iq bd mb nh ni dn mf nj nk dp mj lf nl nm ml lj nn no mn ln np nq mp nr bi translated">对于数据扩充，几何变换比强度变换更好</h2><p id="c720" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">如果我们分析人类如何理解一个标志，很容易注意到几何特征(如手的位置和形状)是识别的基础。另一方面，背景、肤色、衣服、头发和其他配饰与我们无关。这就是为什么我们认为几何变换(旋转、缩放、剪切)将优于强度变换(亮度、通道反转)。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ns"><img src="../Images/9a743c79a7476b45fab8949387873a5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*0_8u8dx9JyDmJ_6zpidPhg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">我们的转换用于增加数据和应用范围。(图片由作者提供)</p></figure><p id="ff8a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">实验:</strong>我们将分别测试不同的数据增强技术。</p><h1 id="24d0" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">模型创建</h1><p id="de59" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">CNN模型是基于<a class="ae kv" href="https://arxiv.org/abs/1905.11946" rel="noopener ugc nofollow" target="_blank"> EfficientNet-B0 </a>的，因为它减少了参数数量并具有良好的准确性。在特征提取器之后，我们创建了一个神经网络来预测符号。每个测试在相同的设置中重复3次，获取平均结果并使用方差分析(ANOVA)或T-student测试进行统计比较。数据被随机分成80%的样本用于训练，20%的样本用于测试。你可以在<a class="ae kv" href="https://colab.research.google.com/drive/1sSp9pFIiUHcpbUGoqGCS5yzbkw6XcrFp?usp=sharing" rel="noopener ugc nofollow" target="_blank"> Google Colab </a>中查看细节和实现。</p><h1 id="413c" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">结果</h1><h2 id="cb9b" class="ng ma iq bd mb nh ni dn mf nj nk dp mj lf nl nm ml lj nn no mn ln np nq mp nr bi translated">数据扩充</h2><p id="075e" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">下表显示了单独测试的每种数据扩充技术的平均精度。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nt"><img src="../Images/f6604762a9c388261cf7d192ee8ad27d.png" data-original-src="https://miro.medium.com/v2/resize:fit:750/format:webp/1*O4WjdcOtwVkUHcfj3GAodQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">考虑到20 FPS数据集，三次执行的平均准确度。(图片由作者提供)</p></figure><p id="56f5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于数据扩充的主要目标是提高模型不变性，我们将注意力集中在验证集的结果上，很明显，几何变换在提高符号识别方面表现得相当好。</p><p id="ed90" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">除此之外，我们注意到数据扩充成功地减少了过度拟合，如下图所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/f478eb99fa23a6d047a62f9f2cea8533.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*X-XINv_Vk2G0tq90"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">20 FPS中数据增强前后的精度，显示它如何减轻训练(橙色)和测试(蓝色)差异。(图片由作者提供)</p></figure><p id="6e22" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由于引入了图像不变性，一般准确性随着数据增加而降低，但结果在验证集中显著提高。此外，我们的几何变换假设得到了证实，如下表所示，达到了比强度变换更高的精度。</p><h2 id="1d49" class="ng ma iq bd mb nh ni dn mf nj nk dp mj lf nl nm ml lj nn no mn ln np nq mp nr bi translated"><strong class="ak">人工背景创建</strong></h2><p id="f35a" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">下图显示了基于<a class="ae kv" href="https://arxiv.org/abs/1706.05587" rel="noopener ugc nofollow" target="_blank"> DeepLabV3 </a>的语义分割结果，以改变背景。由于计算成本的原因，最终分辨率为331x331像素。</p><div class="kg kh ki kj gt ab cb"><figure class="mx kk my mz na nb nc paragraph-image"><img src="../Images/e1f5edbc2d96e802e07f1d4f1b286fb8.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/0*VG26Nz-YmqFzGIco"/></figure><figure class="mx kk my mz na nb nc paragraph-image"><img src="../Images/f9b52a54178fd012711e73b1e0c41f47.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/0*kNi9KykAtiXVztDE"/><p class="kr ks gj gh gi kt ku bd b be z dk nd di ne nf translated">用新的人工背景签名执行。(图片由作者提供)</p></figure></div><p id="63f8" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们用5个不同的场景来代替每个标志的背景。下表显示了de结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nv"><img src="../Images/0e5632daae764573ade372e4bd173112.png" data-original-src="https://miro.medium.com/v2/resize:fit:724/format:webp/1*-LZ0b3cbx0X14N1eoAZa-w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">比较使用人工背景替换(“背景后缀”)和不使用人工背景替换的平均模型精度。(图片由作者提供)</p></figure><p id="c53b" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了更多地了解这些结果，我们使用了一个名为<a class="ae kv" href="https://github.com/marcotcr/lime" rel="noopener ugc nofollow" target="_blank"> LIME </a>的工具来解释模型预测，突出显示了图像中对推断贡献更大的部分，如下图所示。</p><div class="kg kh ki kj gt ab cb"><figure class="mx kk my mz na nb nc paragraph-image"><img src="../Images/f93f96df2bec8e7cf3d93959fd6d5dd1.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/0*egfm_X75d7UbxvDX"/></figure><figure class="mx kk my mz na nb nc paragraph-image"><img src="../Images/8f9cee71b759f9a557c41ff69ef86c79.png" data-original-src="https://miro.medium.com/v2/resize:fit:662/0*WbcwMOx-1J4f4WN9"/><p class="kr ks gj gh gi kt ku bd b be z dk nd di ne nf translated">使用LIME来解释验证集中的模型预测。(图片由作者提供)</p></figure></div><p id="94a9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">解释表明，考虑到解释者的手的位置来推断信号，该模型正专注于图像的正确部分。这表明背景没有偏向结果，这就是为什么替换没有聚集相关特征，就像颜色变换一样</p><p id="077c" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">每秒帧数比较</strong></p><p id="2324" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">首先，我们研究了将视频二次采样为图像的最合适的FPS，以便通过以60 FPS记录来训练模型。表5给出了结果。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nw"><img src="../Images/ad6c52f35e44fe34245958bcf01aa170.png" data-original-src="https://miro.medium.com/v2/resize:fit:640/format:webp/1*MarD1QYhzw4LHPvFBzNRcg.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图像子样本之间的比较。(图片由作者提供)</p></figure><p id="dc42" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">值得注意的是，60 FPS并不能补偿所需的计算资源，因为它在验证集中获得了大约10%的低准确度。这可能是因为该帧速率的图像几乎是30和20 FPS的2到3倍，如图2所示，这可能会导致过度拟合，从而导致验证集中的差异更大。除此之外，视频的连续图像彼此相似，产生低的信息增益。</p><p id="cf40" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">T-student测试显示，30帧/秒和20帧/秒在测试集中存在显著差异，因此得出结论，这是这种情况下的最佳选择，但这应根据数据集大小而有所不同，一旦30帧/秒的训练时间变长，时空特征的探索会影响需要从视频中提取的信息量。</p><p id="8dba" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">最后一个涉及帧速率的测试是比较以30 FPS捕捉的数据集和以60 FPS捕捉的数据集之间的性能，如表6所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nx"><img src="../Images/2ea9520b54465c7297088ae5d37514bd.png" data-original-src="https://miro.medium.com/v2/resize:fit:692/format:webp/1*6Il-SdArCgkG7WbRlK80ig.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">以60 FPS和30 FPS捕获的数据集之间的比较。(图片由作者提供)</p></figure><p id="b52e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在测试集中，结果有利于以60 FPS捕获的数据集，因为更多数量的图像有助于模型在训练期间更好地拟合(如表5所示)。另一方面，在验证集中，以60或30 FPS捕获的数据集之间没有显著差异(p值为0.58)。另一个相关的事实是，以30 FPS捕获的数据集比以60 FPS捕获并二次采样到30 FPS的数据集具有更少的图像，这是因为符号的执行更快，这是取决于解释者和情况的正常变化。因此，指示口译员缓慢执行手势应该有助于进一步缓解这些捕捉率的准确性差异，主要是在光线充足的场景中，<strong class="ky ir">的运动模糊不太明显</strong>。</p><p id="5dd0" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">因此，在不受控制的情况下，在不同的照明条件和口译员执行的手势速度下，以60 FPS捕捉视频并将其重新采样为30 FPS应该是最佳选择，在获得大量图像的同时避免运动模糊，缺点是需要更好的传感器和更多的存储空间。尽管如此，在控制良好的情况下，以30 FPS的速度捕捉会产生令人满意的结果。</p><p id="4013" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir">利用多流CNN提高验证准确性</strong></p><p id="8807" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">作为最后的测试，我们创建了一个多流CNN来捕捉图像中的本地和全局信息，如下图所示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ny"><img src="../Images/49821308a3571bcd37e0c8b5580ca3f2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*16ztcGh5q6bT1fS2"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">符号识别的多CNN架构。(图片由作者提供)</p></figure><p id="fa12" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">为了分割手，我们使用<a class="ae kv" href="https://arxiv.org/abs/1911.09070" rel="noopener ugc nofollow" target="_blank"> EfficientDet </a>作为对象检测器，将各个图像传递到特征提取器，然后传递到神经网络。我们的最终结果显示，在测试集上的准确率为96%，在验证集上的准确率为81%，这表明即使使用简单的数据集进行训练，也有可能获得良好的结果并推广到更复杂的情况。</p><h1 id="b96c" class="lz ma iq bd mb mc md me mf mg mh mi mj jw mk jx ml jz mm ka mn kc mo kd mp mq bi translated">结论和最后想法</h1><p id="19d7" class="pw-post-body-paragraph kw kx iq ky b kz mr jr lb lc ms ju le lf mt lh li lj mu ll lm ln mv lp lq lr ij bi translated">由于新算法和技术的出现，在一些年内，手语识别对于机器学习来说将变得相当容易，但我希望它已经明确了对创建数据集的效率进行进一步研究的需要，因为每个新的翻译系统都需要大量的数据。</p><p id="5d6e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们看到，只需要很少的翻译、简单的记录设置、相同的背景，以及正确的数据扩充选择，就有可能推广到现实世界的场景。在未来的工作中，可以进行更深入的分析，用更多的人和标志来测试这里观察到的相同模式是否会重复。</p><p id="992a" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">如需进一步阅读和概念，请参考<a class="ae kv" href="https://arxiv.org/abs/2103.12233" rel="noopener ugc nofollow" target="_blank">原文</a>。感谢阅读！</p></div></div>    
</body>
</html>