<html>
<head>
<title>Using Dataflow to Extract, Transform, and Load Bike Share Toronto Ridership Data into BigQuery</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用数据流提取、转换和加载自行车共享多伦多乘客数据到BigQuery</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/using-dataflow-to-extract-transform-and-load-bike-share-toronto-ridership-data-into-bigquery-e9b1dc199781?source=collection_archive---------10-----------------------#2021-04-20">https://towardsdatascience.com/using-dataflow-to-extract-transform-and-load-bike-share-toronto-ridership-data-into-bigquery-e9b1dc199781?source=collection_archive---------10-----------------------#2021-04-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="f8bd" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">关于构建ETL管道的说明，该管道用于将Bike Share Toronto ridership数据加载到BigQuery表中，以便它可以用作Data Studio创建数据可视化的源</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/b5c4f8eec98c7cf6d70e9fff8e84d8a8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mTpt6miyeic6uY7T"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">安德烈·费塔多在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="b3b3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在我之前的<a class="ae kv" href="https://bilalmkhan.medium.com/how-the-pandemic-has-affected-bike-share-toronto-ridership-65798265adc2" rel="noopener">博文</a>中，我使用Google Data Studio和BigQuery作为数据源来分析疫情是如何影响自行车共享出行的。在这篇博客文章中，我将一步一步地演示在尝试将<a class="ae kv" href="https://ckan0.cf.opendata.inter.prod-toronto.ca/tr/dataset/bike-share-toronto-ridership-data" rel="noopener ugc nofollow" target="_blank"> Bike Share Toronto ridership数据</a>从云存储中直接加载到BigQuery时遇到的一些问题，以及如何通过使用Dataflow使用Apache Beam执行ETL过程来解决这些问题。</p><h1 id="05f4" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">下载数据</h1><ol class=""><li id="d563" class="mk ml iq ky b kz mm lc mn lf mo lj mp ln mq lr mr ms mt mu bi translated">让我们首先在云壳中打开一个会话，并将2020年<a class="ae kv" href="https://ckan0.cf.opendata.inter.prod-toronto.ca/tr/dataset/bike-share-toronto-ridership-data" rel="noopener ugc nofollow" target="_blank">自行车共享多伦多乘客数据</a>下载到一个单独的文件夹<code class="fe mv mw mx my b">'2020'</code>。</li></ol><pre class="kg kh ki kj gt mz my na nb aw nc bi"><span id="65a4" class="nd lt iq my b gy ne nf l ng nh">wget <a class="ae kv" href="https://ckan0.cf.opendata.inter.prod-toronto.ca/dataset/7e876c24-177c-4605-9cef-e50dd74c617f/resource/5f5d78c4-d810-4048-9dac-c18273abffac/download/files-1.zip" rel="noopener ugc nofollow" target="_blank">https://ckan0.cf.opendata.inter.prod-toronto.ca/dataset/7e876c24-177c-4605-9cef-e50dd74c617f/resource/5f5d78c4-d810-4048-9dac-c18273abffac/download/files-1.zip</a> -O temp.zip</span><span id="7a75" class="nd lt iq my b gy ni nf l ng nh">unzip temp.zip -d 2020</span><span id="b5df" class="nd lt iq my b gy ni nf l ng nh">rm temp.zip</span></pre><p id="67b4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">上面的第一个命令下载名为<code class="fe mv mw mx my b">temp.zip</code>的zip文件，第二个命令将其解压到名为<code class="fe mv mw mx my b">2020</code>的文件夹中，第三个命令删除下载的zip文件。</p><p id="99cc" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">2.让我们通过运行下面的命令来看看文件解压缩后的内容。</p><pre class="kg kh ki kj gt mz my na nb aw nc bi"><span id="3423" class="nd lt iq my b gy ne nf l ng nh">ls 2020</span></pre><p id="ec20" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们看到，2020年的乘客数据分为12个CSV文件，都以<code class="fe mv mw mx my b">2020</code>开头:</p><pre class="kg kh ki kj gt mz my na nb aw nc bi"><span id="64fa" class="nd lt iq my b gy ne nf l ng nh">2020–01.csv 2020–02.csv 2020–03.csv 2020–04.csv 2020–05.csv 2020–06.csv 2020–07.csv 2020–08.csv 2020–09.csv 2020–10.csv 2020–11.csv 2020–12.csv</span></pre><p id="d024" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">3.我们可以通过读取CSV的标题来读取列名:</p><pre class="kg kh ki kj gt mz my na nb aw nc bi"><span id="0d12" class="nd lt iq my b gy ne nf l ng nh">head -n 1 2020/2020–01.csv</span></pre><p id="93e7" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这将输出以下列名:</p><pre class="kg kh ki kj gt mz my na nb aw nc bi"><span id="4296" class="nd lt iq my b gy ne nf l ng nh">Trip Id,Trip Duration,Start Station Id,Start Time,Start Station Name,End Station Id,End Time,End Station Name,Bike Id,User Type</span></pre><h1 id="1c8d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">将文件复制到云存储桶</h1><p id="8eaf" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">接下来，让我们将文件复制到云存储桶中。这将允许我们利用<a class="ae kv" href="https://cloud.google.com/bigquery-transfer/docs/cloud-storage-transfer-overview" rel="noopener ugc nofollow" target="_blank">对云存储URI </a>的通配符支持，用一个命令将多个文件批量加载到一个BigQuery表中。</p><p id="7072" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">4.首先，用您的项目ID设置一个项目变量，并设置项目属性。</p><pre class="kg kh ki kj gt mz my na nb aw nc bi"><span id="d15e" class="nd lt iq my b gy ne nf l ng nh">export PROJECT=my-project-id</span><span id="2f8d" class="nd lt iq my b gy ni nf l ng nh">gcloud config set project $my-project-id</span></pre><p id="d5d4" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">5.使用make bucket <code class="fe mv mw mx my b">gsutil mb</code>命令在项目中创建一个新的bucket。</p><pre class="kg kh ki kj gt mz my na nb aw nc bi"><span id="a1d6" class="nd lt iq my b gy ne nf l ng nh">gsutil mb -l northamerica-northeast1 gs://my-bucket-name</span></pre><p id="f8a1" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">6.使用<code class="fe mv mw mx my b">gsutil cp</code>命令将文件复制到我们刚刚创建的云存储桶中。</p><pre class="kg kh ki kj gt mz my na nb aw nc bi"><span id="75a7" class="nd lt iq my b gy ne nf l ng nh">gsutil cp 2020/* gs://my-bucket-name</span></pre><p id="18e2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">7.检查文件是否已成功复制。</p><pre class="kg kh ki kj gt mz my na nb aw nc bi"><span id="6331" class="nd lt iq my b gy ne nf l ng nh">gsutil ls gs://my-bucket-name/</span></pre><p id="17c3" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">8.一旦数据被成功复制到云存储桶，就从云外壳中删除该文件夹。</p><pre class="kg kh ki kj gt mz my na nb aw nc bi"><span id="7eae" class="nd lt iq my b gy ne nf l ng nh">rm -r 2020</span></pre><h1 id="9c53" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">创建大查询数据集</h1><p id="f7d1" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">9.在将数据装载到BigQuery表之前，我们需要为该表创建一个BigQuery数据集。</p><pre class="kg kh ki kj gt mz my na nb aw nc bi"><span id="4432" class="nd lt iq my b gy ne nf l ng nh">bq --location=northamerica-northeast1 mk mydataset</span></pre><h1 id="3ae9" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">尝试直接从云存储中将数据加载到BigQuery时出错</h1><p id="260a" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">10.接下来，让我们尝试使用<code class="fe mv mw mx my b">bq load</code>命令将数据加载到一个BigQuery表中，并使用一个通配符，在基础上附加一个星号(<code class="fe mv mw mx my b">*</code>)</p><pre class="kg kh ki kj gt mz my na nb aw nc bi"><span id="54c4" class="nd lt iq my b gy ne nf l ng nh">bq load --autodetect --source_format=CSV mydataset.biketrips2020 gs://my-bucket-name/*</span></pre><p id="853e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，我们得到一个错误:</p><pre class="kg kh ki kj gt mz my na nb aw nc bi"><span id="a9dd" class="nd lt iq my b gy ne nf l ng nh">- gs://my-bucket-name/2020-10.csv: Error while reading<br/>data, error message: CSV table references column position 9, but<br/>line starting at position:3401930 contains only 9 columns.</span></pre><p id="3c29" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">该错误消息表明加载作业失败，因为至少有一行的列数少于自动检测到的架构规定的列数。</p><p id="6f97" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">11.为了找到这些错误的来源，我们可以检查错误字节位置附近的CSV文件。为此，我们使用gsutil <code class="fe mv mw mx my b">cat</code>命令。</p><pre class="kg kh ki kj gt mz my na nb aw nc bi"><span id="01ac" class="nd lt iq my b gy ne nf l ng nh">gsutil cat -r 3401700–3402200 gs://my-bucket-name/2020–10.csv</span></pre><p id="7565" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">由此，我们发现有许多行中的<code class="fe mv mw mx my b">Trip_Id</code>和<code class="fe mv mw mx my b">Trip_Duration</code>的值被错误地连接在一起。例如，下面的第二行应该以<code class="fe mv mw mx my b">10000084,625,7120,…</code>开始</p><pre class="kg kh ki kj gt mz my na nb aw nc bi"><span id="78f5" class="nd lt iq my b gy ne nf l ng nh">10000083,720,7239,10/03/2020 13:28,Bloor St W / Manning Ave — SMART,7160,10/03/2020 13:40,King St W / Tecumseth St,5563,Annual Member</span><span id="b354" class="nd lt iq my b gy ni nf l ng nh">10000084625,7120,10/03/2020 13:28,Gerrard St E / River St,7120,10/03/2020 13:38,Gerrard St E / River St,5250,Annual Member</span><span id="65bd" class="nd lt iq my b gy ni nf l ng nh">10000085,1526,7239,10/03/2020 13:28,Bloor St W / Manning Ave — SMART,7544,10/03/2020 13:53,Foster Pl / Elizabeth St — SMART,3956,Annual Member</span></pre><p id="6572" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">我们需要找到所有这样的串联值，并将它们拆分。这是在BigQuery中无法完成的数据转换。因此，我们需要构建一个Apache Beam管道来转换数据并加载到BigQuery中。</p><p id="cb2f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">此外，我们看到上述行中的<code class="fe mv mw mx my b">Start_Time</code>和<code class="fe mv mw mx my b">End_Time</code>列不符合<a class="ae kv" href="https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types#examples" rel="noopener ugc nofollow" target="_blank"> BigQuery兼容的</a> <code class="fe mv mw mx my b"><a class="ae kv" href="https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types#examples" rel="noopener ugc nofollow" target="_blank">datetime</a></code> <a class="ae kv" href="https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types#examples" rel="noopener ugc nofollow" target="_blank">格式</a>。我们还需要用数据流进行转换。</p><p id="1552" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">12.处理错误的一个选择是在<code class="fe mv mw mx my b">bq load</code>中设置<code class="fe mv mw mx my b">max_bad_records</code>标志。这将忽略坏行，并且不会将它们加载到表中。例如，我们可以将<code class="fe mv mw mx my b">max_bad_record</code>标志设置为100:</p><pre class="kg kh ki kj gt mz my na nb aw nc bi"><span id="2b02" class="nd lt iq my b gy ne nf l ng nh">bq load \<br/>--autodetect \<br/>--max_bad_records=100 \<br/>--source_format=CSV \<br/>mydataset.biketrips2020 \<br/>gs://my-bucket-name/*</span></pre><p id="d001" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">然而，这仍然会给我们带来错误，尽管是不同类型的错误。</p><pre class="kg kh ki kj gt mz my na nb aw nc bi"><span id="949b" class="nd lt iq my b gy ne nf l ng nh">Could not parse ‘NULL’ as INT64 for field Bike_Id (position<br/>8) starting at location 5061057 with message ‘Unable to parse’</span></pre><p id="0a23" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">出现此错误是因为对于BigQuery，在CSV中表示空值的标准方式是使用空字段。我们可以通过在<code class="fe mv mw mx my b">bq load</code>命令中设置标志<code class="fe mv mw mx my b">null_marker=NULL</code>来处理这个错误，从而指定这个特定的文件使用字符串NULL来标记NULL。例如:</p><pre class="kg kh ki kj gt mz my na nb aw nc bi"><span id="f489" class="nd lt iq my b gy ne nf l ng nh">bq load \<br/>--autodetect \<br/>--max_bad_records=100 \<br/>--source_format=CSV \<br/>--null_marker=NULL \<br/>mydataset.biketrips2020 \<br/>gs://my-bucket-name/*</span></pre><p id="59be" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">这处理了由于解析空值引起的错误。然而，我们还将在Apache Beam管道中包含这种转换，以便单个管道执行所有必要的转换。</p><h1 id="8fbc" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">建立数据流管道</h1><p id="741e" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">13.接下来，我们编写一个Apache Beam管道，它提取文件，执行转换，并将数据加载到BigQuery中。Python文件<code class="fe mv mw mx my b"><a class="ae kv" href="https://github.com/bilalmkhan/etl-pipeline-beam" rel="noopener ugc nofollow" target="_blank">etl_pipeline.py</a></code> <a class="ae kv" href="https://github.com/bilalmkhan/etl-pipeline-beam/blob/main/etl_pipeline.py" rel="noopener ugc nofollow" target="_blank"> </a>包含管道的Python代码。三个函数执行主要的转换:<code class="fe mv mw mx my b">deconcat()</code>、<code class="fe mv mw mx my b">replace_nulls()</code>、<code class="fe mv mw mx my b">format_datetime_bq()</code>。我们可以使用云壳编辑器上传Python文件。</p><h1 id="a51a" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">设置Python环境</h1><p id="baa5" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">14.在云Shell中运行以下命令，设置虚拟环境来运行我们的代码:</p><pre class="kg kh ki kj gt mz my na nb aw nc bi"><span id="644e" class="nd lt iq my b gy ne nf l ng nh">sudo pip install virtualenv </span><span id="c662" class="nd lt iq my b gy ni nf l ng nh">virtualenv -p python3 venv </span><span id="f3c2" class="nd lt iq my b gy ni nf l ng nh">source venv/bin/activate </span><span id="7cd2" class="nd lt iq my b gy ni nf l ng nh">pip install apache-beam[gcp]==2.24.0</span></pre><h1 id="6e1d" class="ls lt iq bd lu lv lw lx ly lz ma mb mc jw md jx me jz mf ka mg kc mh kd mi mj bi translated">运行管道</h1><p id="2493" class="pw-post-body-paragraph kw kx iq ky b kz mm jr lb lc mn ju le lf nj lh li lj nk ll lm ln nl lp lq lr ij bi translated">15.运行Python文件<code class="fe mv mw mx my b"><a class="ae kv" href="https://github.com/bilalmkhan/etl-pipeline-beam/blob/main/etl_pipeline.py" rel="noopener ugc nofollow" target="_blank">etl_pipeline</a>.py</code>会创建一个运行<code class="fe mv mw mx my b">DataflowRunner</code>的数据流作业。我们需要指定一个云存储桶位置，用于在管道仍在运行时暂存和存储临时数据，以及包含CSV文件的云存储桶。</p><pre class="kg kh ki kj gt mz my na nb aw nc bi"><span id="e7be" class="nd lt iq my b gy ne nf l ng nh">python etl_pipeline.py \<br/>--project=$PROJECT \<br/>--region=northamerica-northeast1 \<br/>--runner=DataflowRunner \<br/>--staging_location=gs://my-bucket-name/test \<br/>--temp_location gs://my-bucket-name/test \<br/>--input gs://my-bucket-name/*.csv <br/>--save_main_session</span></pre><p id="5777" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">16.当数据流作业完成时，我们可以在Gooogle云控制台中导航到云数据流，并查看作业图表和其他作业完成详细信息:</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nm"><img src="../Images/ede3094b73632ec761cc35ed7a399629.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CVYNLXAPfYYVUgfLTB9UnQ.png"/></div></div></figure><p id="6be9" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">17.导航到BigQuery验证数据是否成功加载到表中。这些数据现在可以用作Cloud Data Studio创建可视化的数据源。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nn"><img src="../Images/a8e2371df02f87e2683bad6a6b1053bf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4akcff7SPDXsGwknn8UeBg.png"/></div></div></figure><p id="b1cf" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="no">感谢您的阅读！如果您对此工作流程有任何建议，请在评论中分享。</em></p></div></div>    
</body>
</html>