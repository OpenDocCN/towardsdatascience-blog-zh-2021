<html>
<head>
<title>Stop Using XGBoost…</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">停止使用XGBoost…</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/stop-using-xgboost-660ed6718845?source=collection_archive---------20-----------------------#2021-06-25">https://towardsdatascience.com/stop-using-xgboost-660ed6718845?source=collection_archive---------20-----------------------#2021-06-25</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="8d66" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph">意见</h2><div class=""/><div class=""><h2 id="82dd" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">而是使用CatBoost。</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/2437c46bfd35d74a931380b6c8dbf4a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*ij2I4iXz3dcKpwVyOia-qA.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">由<a class="ae lh" href="https://unsplash.com/@pactovisual?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Pacto视觉</a>在<a class="ae lh" href="https://unsplash.com/s/photos/cat?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a>【1】上拍摄。</p></figure></div><div class="ab cl li lj hx lk" role="separator"><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln lo"/><span class="ll bw bk lm ln"/></div><div class="im in io ip iq"><h1 id="3330" class="lp lq it bd lr ls lt lu lv lw lx ly lz ki ma kj mb kl mc km md ko me kp mf mg bi translated">目录</h1><ol class=""><li id="a7f2" class="mh mi it mj b mk ml mm mn mo mp mq mr ms mt mu mv mw mx my bi translated">介绍</li><li id="b588" class="mh mi it mj b mk mz mm na mo nb mq nc ms nd mu mv mw mx my bi translated">XGBoost不利</li><li id="11f7" class="mh mi it mj b mk mz mm na mo nb mq nc ms nd mu mv mw mx my bi translated">CatBoost优势</li><li id="d0a0" class="mh mi it mj b mk mz mm na mo nb mq nc ms nd mu mv mw mx my bi translated">摘要</li><li id="9db2" class="mh mi it mj b mk mz mm na mo nb mq nc ms nd mu mv mw mx my bi translated">参考</li></ol><h1 id="e259" class="lp lq it bd lr ls ne lu lv lw nf ly lz ki ng kj mb kl nh km md ko ni kp mf mg bi translated">介绍</h1><p id="43fb" class="pw-post-body-paragraph nj nk it mj b mk ml kd nl mm mn kg nm mo nn no np mq nq nr ns ms nt nu nv mu im bi translated">我首先要说的是，XGBoost是一种非常强大的机器学习算法，已经被证明赢得了无数的数据科学比赛，并且很可能处于数据科学中最专业的用例的最前沿。出于这个原因，这种算法在过去几年中一直受到关注，这是一件好事。也就是说，这意味着它的缺点也受到了关注，因此，产生了开发类似算法的动机，这种算法在XGBoost没有的地方表现出色。让我们讨论XGBoost及其竞争对手CatBoost，并强调为什么您会想要使用这种新兴的算法，无论您是在学校，还是作为专业数据科学家和/或机器学习工程师工作。请继续阅读下面的内容，了解XGBoost的缺点以及CatBoost的优点。</p><h1 id="9e5a" class="lp lq it bd lr ls ne lu lv lw nf ly lz ki ng kj mb kl nh km md ko ni kp mf mg bi translated">XGBoost不利</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/b75f8ac208cacf55a46b060eae1618f8.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4DMzV_cY3_XiyXXEjfbYGg.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">Sebastian Unrau 在<a class="ae lh" href="https://unsplash.com/s/photos/forest?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a>【2】上的照片。</p></figure><p id="94b3" class="pw-post-body-paragraph nj nk it mj b mk nw kd nl mm nx kg nm mo ny no np mq nz nr ns ms oa nu nv mu im bi translated">这个<a class="ae lh" href="https://xgboost.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank">算法</a>【3】是基于决策树的，利用优化的分布式梯度推进。</p><blockquote class="ob oc od"><p id="345b" class="nj nk oe mj b mk nw kd nl mm nx kg nm of ny no np og nz nr ns oh oa nu nv mu im bi translated">仅数值</p></blockquote><p id="2feb" class="pw-post-body-paragraph nj nk it mj b mk nw kd nl mm nx kg nm mo ny no np mq nz nr ns ms oa nu nv mu im bi translated">XGBoost很棒——有数值。这意味着它不能处理分类特征，除非它们被转换以使模型工作。大多数数据科学家会使用one-hot-encoding，它会将特征值转置为列本身，并将一个<code class="fe oi oj ok ol b">0</code>或<code class="fe oi oj ok ol b">1</code>作为值。这也很棒，但是会导致另一个问题。</p><blockquote class="ob oc od"><p id="d3e3" class="nj nk oe mj b mk nw kd nl mm nx kg nm of ny no np og nz nr ns oh oa nu nv mu im bi translated">稀疏数据集</p></blockquote><p id="7986" class="pw-post-body-paragraph nj nk it mj b mk nw kd nl mm nx kg nm mo ny no np mq nz nr ns ms oa nu nv mu im bi translated">这里我所说的稀疏是指，因为一个分类特性不能有一个单独的列，所以该分类特性中的唯一值有多少列就有多少列。例如，如果有100个唯一值，您将有100多列追加到您的数据集，充满0和1。当然，这会导致另一个问题。</p><blockquote class="ob oc od"><p id="fdf8" class="nj nk oe mj b mk nw kd nl mm nx kg nm of ny no np og nz nr ns oh oa nu nv mu im bi translated">长期培训</p></blockquote><p id="eab7" class="pw-post-body-paragraph nj nk it mj b mk nw kd nl mm nx kg nm mo ny no np mq nz nr ns ms oa nu nv mu im bi translated">随着越来越多的分类要素转换为数值要素，这可能意味着您的模型将需要更长的时间来运行更大的数据集。</p><blockquote class="ob oc od"><p id="ae48" class="nj nk oe mj b mk nw kd nl mm nx kg nm of ny no np og nz nr ns oh oa nu nv mu im bi translated">部署</p></blockquote><p id="5eed" class="pw-post-body-paragraph nj nk it mj b mk nw kd nl mm nx kg nm mo ny no np mq nz nr ns ms oa nu nv mu im bi translated">当您想要部署模型并对新数据进行推断或预测时，这些数据必须与您的训练数据具有相同的结构。这个问题意味着您将不得不转换您的预测数据，这将导致更多的代码、更多的出错空间以及更多的麻烦。</p><blockquote class="ob oc od"><p id="b1ea" class="nj nk oe mj b mk nw kd nl mm nx kg nm of ny no np og nz nr ns oh oa nu nv mu im bi translated">因素</p></blockquote><p id="8db8" class="pw-post-body-paragraph nj nk it mj b mk nw kd nl mm nx kg nm mo ny no np mq nz nr ns ms oa nu nv mu im bi translated">XGBoost非常强大，但是参数太多，也很难调整。理解所有参数并对它们进行调优可能需要一些时间。</p><p id="d6e5" class="pw-post-body-paragraph nj nk it mj b mk nw kd nl mm nx kg nm mo ny no np mq nz nr ns ms oa nu nv mu im bi translated">如您所见，这些缺点大多与分类特征有关。所以，你可以看到为什么会有一个类似的模型与XGBoost有很多相同的好处，但是专注于修复XGBoost的类别问题。因此，命名为CatBoost——分类增强。</p><h1 id="9ce5" class="lp lq it bd lr ls ne lu lv lw nf ly lz ki ng kj mb kl nh km md ko ni kp mf mg bi translated">CatBoost优势</h1><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi om"><img src="../Images/4f8b435a4ceab9a7c6619d06c265c130.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*rq3u687BjNwXKLa7ssum2w.jpeg"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">Ludemeula Fernandes 在<a class="ae lh" href="https://unsplash.com/s/photos/cat?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a>【4】上拍摄的照片。</p></figure><p id="5497" class="pw-post-body-paragraph nj nk it mj b mk nw kd nl mm nx kg nm mo ny no np mq nz nr ns ms oa nu nv mu im bi translated">这个<a class="ae lh" href="https://catboost.ai/#:~:text=CatBoost%20-%20open-source%20gradient%20boosting%20library" rel="noopener ugc nofollow" target="_blank">算法</a>【5】也是一个基于梯度提升无决策树的高置换算法，重点是处理分类特征，这在过去常常被忽略。以下是CatBoost的优势:</p><blockquote class="ob oc od"><p id="ddc2" class="nj nk oe mj b mk nw kd nl mm nx kg nm of ny no np og nz nr ns oh oa nu nv mu im bi translated">处理分类特征</p></blockquote><ul class=""><li id="2cef" class="mh mi it mj b mk nw mm nx mo on mq oo ms op mu oq mw mx my bi translated">不需要将分类特征转换成数字特征</li><li id="ae04" class="mh mi it mj b mk mz mm na mo nb mq nc ms nd mu oq mw mx my bi translated">这意味着您不必对数据进行太多的预处理(<em class="oe">——取决于您的用例</em>)</li><li id="3462" class="mh mi it mj b mk mz mm na mo nb mq nc ms nd mu oq mw mx my bi translated">您的数据集可以保持其原始格式，如原始列，而不必制作一次性热编码列，这更容易/更简单，更有利于部署，并向利益相关者解释您的功能</li></ul><blockquote class="ob oc od"><p id="6a87" class="nj nk oe mj b mk nw kd nl mm nx kg nm of ny no np og nz nr ns oh oa nu nv mu im bi translated">简单参数</p></blockquote><ul class=""><li id="d7fc" class="mh mi it mj b mk nw mm nx mo on mq oo ms op mu oq mw mx my bi translated">利用CatBoost的默认参数最有可能获得最佳结果</li><li id="827d" class="mh mi it mj b mk mz mm na mo nb mq nc ms nd mu oq mw mx my bi translated">也就是说，花在调整参数上的时间更少</li></ul><blockquote class="ob oc od"><p id="27b1" class="nj nk oe mj b mk nw kd nl mm nx kg nm of ny no np og nz nr ns oh oa nu nv mu im bi translated">准确度提高</p></blockquote><ul class=""><li id="260e" class="mh mi it mj b mk nw mm nx mo on mq oo ms op mu oq mw mx my bi translated">虽然XGBoost在大多数算法中往往具有最好的准确性，但CatBoost已经在严重的流行数据集上被证明可以击败它</li></ul><p id="82fd" class="pw-post-body-paragraph nj nk it mj b mk nw kd nl mm nx kg nm mo ny no np mq nz nr ns ms oa nu nv mu im bi translated">这两种算法都很棒，但是如果你有分类特征，那么CatBoost是一个很好的选择。</p><h1 id="9b98" class="lp lq it bd lr ls ne lu lv lw nf ly lz ki ng kj mb kl nh km md ko ni kp mf mg bi translated">摘要</h1><p id="c797" class="pw-post-body-paragraph nj nk it mj b mk ml kd nl mm mn kg nm mo nn no np mq nq nr ns ms nt nu nv mu im bi translated">这篇文章是片面的，因为我强调了一种算法的优点，同时讨论了另一种算法的缺点。然而，这就是为什么CatBoost是这样的。但是，没有XGBoost就没有CatBoost，所以总而言之，算法和各自的库都是强大的、有益的，最终由您决定使用哪一个。还可能出现其他因素，比如文档。XGBoost有更多的在线文档和示例，这是CatBoost努力的地方，但随着时间的推移，我可以看到CatBoost在这方面有所改进。或许，会出现另一种甚至比CatBoost更好的算法。</p><blockquote class="ob oc od"><p id="95d2" class="nj nk oe mj b mk nw kd nl mm nx kg nm of ny no np og nz nr ns oh oa nu nv mu im bi translated">总而言之，以下是使用CatBoost的主要原因:</p></blockquote><pre class="ks kt ku kv gt or ol os ot aw ou bi"><span id="80ec" class="ov lq it ol b gy ow ox l oy oz">* Ease of using categorical features</span><span id="51f4" class="ov lq it ol b gy pa ox l oy oz">* Implementation of dataset processing</span><span id="5e2f" class="ov lq it ol b gy pa ox l oy oz">* Improved accuracy</span></pre><p id="6354" class="pw-post-body-paragraph nj nk it mj b mk nw kd nl mm nx kg nm mo ny no np mq nz nr ns ms oa nu nv mu im bi translated">我希望你觉得我的文章既有趣又有用。如果你同意这些比较，请在下面随意评论——为什么或为什么不同意？你更喜欢XGBoost还是CatBoost？如果你有，请随意评论你喜欢其中一个的原因。CatBoost有哪些缺点？你是否知道一种更新的机器学习算法和库，在简单性、易用性和准确性方面可以最好地超越CatBoost？谢谢你看我的文章！</p><p id="57b2" class="pw-post-body-paragraph nj nk it mj b mk nw kd nl mm nx kg nm mo ny no np mq nz nr ns ms oa nu nv mu im bi translated"><em class="oe">请随时查看我的个人资料和其他文章，</em> <a class="pb pc ep" href="https://medium.com/u/abe5272eafd9?source=post_page-----660ed6718845--------------------------------" rel="noopener" target="_blank"> <em class="oe">马特·普日比拉</em> </a>，<em class="oe">也可以在LinkedIn上联系我。</em></p><h1 id="adca" class="lp lq it bd lr ls ne lu lv lw nf ly lz ki ng kj mb kl nh km md ko ni kp mf mg bi translated">参考</h1><p id="ef2d" class="pw-post-body-paragraph nj nk it mj b mk ml kd nl mm mn kg nm mo nn no np mq nq nr ns ms nt nu nv mu im bi translated">[1]图片由<a class="ae lh" href="https://unsplash.com/@pactovisual?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Pacto Visual </a>在<a class="ae lh" href="https://unsplash.com/s/photos/cat?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄，(2016)</p><p id="3e1e" class="pw-post-body-paragraph nj nk it mj b mk nw kd nl mm nx kg nm mo ny no np mq nz nr ns ms oa nu nv mu im bi translated">[2]Sebastian Unrau在<a class="ae lh" href="https://unsplash.com/s/photos/forest?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片，(2015)</p><p id="242f" class="pw-post-body-paragraph nj nk it mj b mk nw kd nl mm nx kg nm mo ny no np mq nz nr ns ms oa nu nv mu im bi translated">[3] xgboost开发人员，<a class="ae lh" href="https://xgboost.readthedocs.io/en/latest/" rel="noopener ugc nofollow" target="_blank"> XGBoost文档</a>，(2020)</p><p id="0a67" class="pw-post-body-paragraph nj nk it mj b mk nw kd nl mm nx kg nm mo ny no np mq nz nr ns ms oa nu nv mu im bi translated">[4]lude meula Fernandes在<a class="ae lh" href="https://unsplash.com/s/photos/cat?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>拍摄的照片，(2017)</p><p id="3603" class="pw-post-body-paragraph nj nk it mj b mk nw kd nl mm nx kg nm mo ny no np mq nz nr ns ms oa nu nv mu im bi translated">[5] Yandex，<a class="ae lh" href="https://catboost.ai/#:~:text=CatBoost%20-%20open-source%20gradient%20boosting%20library" rel="noopener ugc nofollow" target="_blank"> CatBoost文档</a>，(2021)</p></div></div>    
</body>
</html>