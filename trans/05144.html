<html>
<head>
<title>Python’s «predict_proba» Doesn’t Actually Predict Probabilities (and How to Fix It)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Python的predict_proba实际上并不预测概率(以及如何修复它)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/pythons-predict-proba-doesn-t-actually-predict-probabilities-and-how-to-fix-it-f582c21d63fc?source=collection_archive---------0-----------------------#2021-05-06">https://towardsdatascience.com/pythons-predict-proba-doesn-t-actually-predict-probabilities-and-how-to-fix-it-f582c21d63fc?source=collection_archive---------0-----------------------#2021-05-06</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><h2 id="bc18" class="ir is it bd b dl iu iv iw ix iy iz dk ja translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="7180" class="pw-subtitle-paragraph jz jc it bd b ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp kq dk translated">如何评估和修复校准误差概率</h2></div><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi kr"><img src="../Images/d8cee75ad61026fd1a6335445b33600d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-FLQwq1_jZr3UzBGCA41LQ.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">[图片由作者提供]</p></figure><p id="0cf0" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">数据科学家通常根据准确度或精确度来评估他们的预测模型，但很少问自己:</p><blockquote class="md"><p id="76b0" class="me mf it bd mg mh mi mj mk ml mm mc dk translated">"我的模型能够预测真实的概率吗？"</p></blockquote><p id="2c5d" class="pw-post-body-paragraph lh li it lj b lk mn kd lm ln mo kg lp lq mp ls lt lu mq lw lx ly mr ma mb mc im bi translated">然而，<strong class="lj jd">从商业的角度来看，对概率的准确估计是非常有价值的</strong> ( <strong class="lj jd">有时甚至比精确的预测</strong>更有价值)。想要个例子吗？</p><p id="d1f9" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">假设你的公司正在出售2个马克杯，一个是普通的白色马克杯，另一个上面有一只小猫的图片。你必须决定向给定的顾客展示哪个杯子。为了做到这一点，你需要预测一个给定的用户购买它们的概率。所以你训练了几个不同的模型，你得到了这些结果:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ms"><img src="../Images/cc865d14025525cd49e77ab0af8c5277.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6m3rHY7qQs__BiIKxlWqFA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">ROC相同但校准不同的模型。[图片由作者提供]</p></figure><p id="e3a8" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在，你会向这位用户推荐哪个杯子？</p><p id="a65b" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">两个模型都认为用户更有可能购买普通的杯子(因此，模型A和模型B在ROC下具有相同的面积，因为该度量仅评估排序)。</p><p id="65fa" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">但是，根据模型A，你会通过推荐普通杯子来最大化预期利润。然而，根据模型B，小猫杯使预期利润最大化。</p><p id="4176" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在像这样的应用中，找出哪个模型能够估计更好的概率是至关重要的。</p><blockquote class="mt mu mv"><p id="def8" class="lh li mw lj b lk ll kd lm ln lo kg lp mx lr ls lt my lv lw lx mz lz ma mb mc im bi translated">在本文中，我们将看到如何测量概率校准(视觉上和数字上)以及如何“修正”现有模型以获得更好的概率。</p></blockquote><h1 id="d79d" class="na nb it bd nc nd ne nf ng nh ni nj nk ki nl kj nm kl nn km no ko np kp nq nr bi translated"><strong class="ak"> predict_proba </strong>怎么了</h1><p id="a5e5" class="pw-post-body-paragraph lh li it lj b lk ns kd lm ln nt kg lp lq nu ls lt lu nv lw lx ly nw ma mb mc im bi translated">Python中所有最流行的机器学习库都有一个方法叫做predict_proba : Scikit-learn(例如LogisticRegression，SVC，RandomForest，…)，XGBoost，LightGBM，CatBoost，Keras…</p><p id="2b88" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">但是，<strong class="lj jd">尽管名字如此，predict_proba并不能完全预测概率</strong>。事实上，不同的研究(尤其是<a class="ae nx" href="https://www.researchgate.net/publication/221344679_Predicting_good_probabilities_with_supervised_learning" rel="noopener ugc nofollow" target="_blank">这一个</a>和<a class="ae nx" href="https://arxiv.org/abs/1706.04599" rel="noopener ugc nofollow" target="_blank">这一个</a>)表明，最流行的预测模型都没有经过校准。</p><blockquote class="md"><p id="e371" class="me mf it bd mg mh mi mj mk ml mm mc dk translated">一个数在0和1之间的事实不足以称之为概率！</p></blockquote><p id="b040" class="pw-post-body-paragraph lh li it lj b lk mn kd lm ln mo kg lp lq mp ls lt lu mq lw lx ly mr ma mb mc im bi translated"><strong class="lj jd">但是，我们什么时候才能说一个数字实际上代表了一种概率呢？</strong></p><p id="8e75" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">想象一下，你训练了一个预测模型来预测一个病人是否会患癌症。现在假设，对于一个给定的病人，模型预测有5%的可能性。原则上，我们应该在多个平行宇宙中观察同一个病人，看看他是否有5%的几率患癌症。</p><p id="8f21" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因为我们不能走那条路，最好的替代方法是选取所有大约5%概率的患者，并计算他们中有多少人患了癌症。如果观察到的百分比实际上接近5%，我们说模型提供的概率是“校准的”。</p><blockquote class="md"><p id="552f" class="me mf it bd mg mh mi mj mk ml mm mc dk translated">当预测的概率反映真实的潜在概率时，它们被称为“校准的”。</p></blockquote><p id="4230" class="pw-post-body-paragraph lh li it lj b lk mn kd lm ln mo kg lp lq mp ls lt lu mq lw lx ly mr ma mb mc im bi translated">但是如何检查你的模型是否经过校准呢？</p><h1 id="032e" class="na nb it bd nc nd ne nf ng nh ni nj nk ki nl kj nm kl nn km no ko np kp nq nr bi translated">校准曲线</h1><p id="2a0f" class="pw-post-body-paragraph lh li it lj b lk ns kd lm ln nt kg lp lq nu ls lt lu nv lw lx ly nw ma mb mc im bi translated">评估模型校准的最简单方法是通过一个名为“校准曲线”(也称为“可靠性图表”)的图表。</p><p id="759e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">这个想法是将观察结果分成概率箱。因此，属于同一仓的观测值共享相似的概率。此时，对于每个箱，校准曲线将预测平均值(即预测概率的平均值)与理论平均值(即观察到的目标变量的平均值)进行比较。</p><p id="bd95" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">Scikit-learn通过函数“calibration_curve”为您完成所有这些工作:</p><pre class="ks kt ku kv gt ny nz oa ob aw oc bi"><span id="b2a6" class="od nb it nz b gy oe of l og oh">from sklearn.calibration import calibration_curve</span><span id="cb68" class="od nb it nz b gy oi of l og oh">y_means, proba_means = calibration_curve(y, proba, n_bins, strategy)</span></pre><p id="b8fc" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">您只需在以下选项中选择箱数和(可选)宁滨策略:</p><ul class=""><li id="c35a" class="oj ok it lj b lk ll ln lo lq ol lu om ly on mc oo op oq or bi translated">“均匀”，间隔0-1被分成等宽的<em class="mw">n _ bin</em>；</li><li id="6c3e" class="oj ok it lj b lk os ln ot lq ou lu ov ly ow mc oo op oq or bi translated">“分位数”，箱边缘被定义为使得每个箱具有相同数量的观察值。</li></ul><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ox"><img src="../Images/3b7c5b34d75948cb23cd215b7eda001d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tEoJ3V64JOSJ1iIvRAY0Ew.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">宁滨的策略，箱数= 7。[图片由作者提供]</p></figure><p id="a64f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">出于绘图目的，我个人更喜欢“分位数”方法。事实上，“统一的”宁滨可能会产生误导，因为一些箱可能包含非常少的观测值。</p><p id="4f9f" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">Numpy函数返回两个数组，每个数组包含目标变量的平均概率和平均值。因此，我们需要做的就是绘制它们:</p><pre class="ks kt ku kv gt ny nz oa ob aw oc bi"><span id="0928" class="od nb it nz b gy oe of l og oh">import matplotlib.pyplot as plt</span><span id="f5aa" class="od nb it nz b gy oi of l og oh">plt.plot([0, 1], [0, 1], linestyle = '--', label = 'Perfect calibration')<br/>plt.plot(proba_means, y_means)</span></pre><p id="8be8" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated"><strong class="lj jd">假设你的模型有很好的精度，校准曲线会单调递增</strong>。但这并不意味着模型校准良好。的确，<strong class="lj jd">只有当校准曲线非常接近平分线</strong>(即灰色虚线)时，您的模型才校准良好，因为这意味着预测概率平均接近理论概率。</p><p id="6264" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们来看一些常见类型的校准曲线示例，它们表明您的模型存在校准错误:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oy"><img src="../Images/336e5e65fd0cc5e9b5b797f5915df150.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*862Gd5xzAt2fvp6o2hwuRg.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">校准错误的常见例子。[图片由作者提供]</p></figure><p id="ce95" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">最常见的校准错误类型有:</p><ul class=""><li id="e304" class="oj ok it lj b lk ll ln lo lq ol lu om ly on mc oo op oq or bi translated"><strong class="lj jd">系统性高估</strong>。与真实分布相比，预测概率的分布被推向右侧。当您在很少有正面结果的不平衡数据集上训练模型时，这种情况很常见。</li><li id="4237" class="oj ok it lj b lk os ln ot lq ou lu ov ly ow mc oo op oq or bi translated"><strong class="lj jd">系统性低估</strong>。与真实分布相比，预测概率的分布被向左推。</li><li id="feb1" class="oj ok it lj b lk os ln ot lq ou lu ov ly ow mc oo op oq or bi translated"><strong class="lj jd">中心的分配太重</strong>。当“支持向量机和提升树等算法倾向于将预测概率推离0和1时”(引用自<a class="ae nx" href="https://www.researchgate.net/publication/221344679_Predicting_good_probabilities_with_supervised_learning" rel="noopener ugc nofollow" target="_blank">用监督学习预测良好概率</a>)就会发生这种情况。</li><li id="9c19" class="oj ok it lj b lk os ln ot lq ou lu ov ly ow mc oo op oq or bi translated"><strong class="lj jd">分布的尾部太重</strong>。例如，“朴素贝叶斯等其他方法具有相反的偏向，倾向于推动预测更接近0和1”(引用自<a class="ae nx" href="https://www.researchgate.net/publication/221344679_Predicting_good_probabilities_with_supervised_learning" rel="noopener ugc nofollow" target="_blank">使用监督学习预测良好概率</a>)。</li></ul><h1 id="43ca" class="na nb it bd nc nd ne nf ng nh ni nj nk ki nl kj nm kl nn km no ko np kp nq nr bi translated">如何修复校准错误(Python中)</h1><p id="56a7" class="pw-post-body-paragraph lh li it lj b lk ns kd lm ln nt kg lp lq nu ls lt lu nv lw lx ly nw ma mb mc im bi translated">假设你训练了一个分类器，它能产生精确但未经校准的概率。概率校准的想法是建立<strong class="lj jd">第二个模型(称为校准器),能够将它们“校正”成真实概率</strong>。</p><blockquote class="mt mu mv"><p id="afd1" class="lh li mw lj b lk ll kd lm ln lo kg lp mx lr ls lt my lv lw lx mz lz ma mb mc im bi translated">注意，校准应该<strong class="lj jd">而不是</strong>在已经用于训练第一分类器的相同数据上执行。</p></blockquote><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oz"><img src="../Images/7266c17d33881081e7525115ca76eb60.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Z_sRGyvxOLZ_tRdaYwKW1g.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">通过两步方法进行概率校准。[图片由作者提供]</p></figure><blockquote class="md"><p id="35b0" class="me mf it bd mg mh pa pb pc pd pe mc dk translated">因此，校准在于<strong class="ak">将(未校准概率的)一维向量转换成(校准概率的)另一维向量的函数</strong>。</p></blockquote><p id="60ca" class="pw-post-body-paragraph lh li it lj b lk mn kd lm ln mo kg lp lq mp ls lt lu mq lw lx ly mr ma mb mc im bi translated">两种方法主要用作校准品:</p><ul class=""><li id="5315" class="oj ok it lj b lk ll ln lo lq ol lu om ly on mc oo op oq or bi translated"><strong class="lj jd">等渗回归</strong>。一种非参数算法，将非递减自由形式线拟合到数据。该行是非递减的这一事实是最基本的，因为它尊重原始排序。</li><li id="faf2" class="oj ok it lj b lk os ln ot lq ou lu ov ly ow mc oo op oq or bi translated"><strong class="lj jd">逻辑回归</strong>。</li></ul><p id="733d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">让我们在玩具数据集的帮助下，看看如何在Python中实际使用校准器:</p><pre class="ks kt ku kv gt ny nz oa ob aw oc bi"><span id="3db7" class="od nb it nz b gy oe of l og oh">from sklearn.datasets import make_classification</span><span id="e4f0" class="od nb it nz b gy oi of l og oh">X, y = make_classification(<br/>    n_samples = 15000, <br/>    n_features = 50, <br/>    n_informative = 30, <br/>    n_redundant = 20,<br/>    weights = [.9, .1],<br/>    random_state = 0<br/>)</span><span id="4df3" class="od nb it nz b gy oi of l og oh">X_train, X_valid, X_test = X[:5000], X[5000:10000], X[10000:]<br/>y_train, y_valid, y_test = y[:5000], y[5000:10000], y[10000:]</span></pre><p id="8a1d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">首先，我们需要安装一个分类器。让我们使用随机森林(但是任何具有predict_proba方法的模型都可以)。</p><pre class="ks kt ku kv gt ny nz oa ob aw oc bi"><span id="f908" class="od nb it nz b gy oe of l og oh">from sklearn.ensemble import RandomForestClassifier</span><span id="0cc7" class="od nb it nz b gy oi of l og oh">forest = RandomForestClassifier().fit(X_train, y_train)</span><span id="fbdd" class="od nb it nz b gy oi of l og oh">proba_valid = forest.predict_proba(X_valid)[:, 1]</span></pre><p id="6002" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">然后，我们将使用分类器的输出(验证数据)来拟合校准器，并最终预测测试数据的概率。</p><ul class=""><li id="99f9" class="oj ok it lj b lk ll ln lo lq ol lu om ly on mc oo op oq or bi translated"><strong class="lj jd">等渗回归</strong>:</li></ul><pre class="ks kt ku kv gt ny nz oa ob aw oc bi"><span id="1977" class="od nb it nz b gy oe of l og oh">from sklearn.isotonic import IsotonicRegression</span><span id="ed5e" class="od nb it nz b gy oi of l og oh">iso_reg = IsotonicRegression(y_min = 0, y_max = 1, out_of_bounds = 'clip').fit(proba_valid, y_valid)</span><span id="04f2" class="od nb it nz b gy oi of l og oh">proba_test_forest_isoreg = iso_reg.predict(forest.predict_proba(X_test)[:, 1])</span></pre><ul class=""><li id="8084" class="oj ok it lj b lk ll ln lo lq ol lu om ly on mc oo op oq or bi translated"><strong class="lj jd">逻辑回归</strong>:</li></ul><pre class="ks kt ku kv gt ny nz oa ob aw oc bi"><span id="1307" class="od nb it nz b gy oe of l og oh">from sklearn.linear_model import LogisticRegression</span><span id="c760" class="od nb it nz b gy oi of l og oh">log_reg = LogisticRegression().fit(proba_valid.reshape(-1, 1), y_valid)</span><span id="c4f9" class="od nb it nz b gy oi of l og oh">proba_test_forest_logreg = log_reg.predict_proba(forest.predict_proba(X_test)[:, 1].reshape(-1, 1))[:, 1]</span></pre><p id="2a07" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">此时，我们有三个预测概率的选项:</p><ol class=""><li id="741d" class="oj ok it lj b lk ll ln lo lq ol lu om ly on mc pf op oq or bi translated">平原随机森林，</li><li id="2465" class="oj ok it lj b lk os ln ot lq ou lu ov ly ow mc pf op oq or bi translated">随机森林+保序回归，</li><li id="f6f7" class="oj ok it lj b lk os ln ot lq ou lu ov ly ow mc pf op oq or bi translated">随机森林+逻辑回归。</li></ol><p id="bded" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">但是我们如何评估哪一个是最校准的呢？</p><h1 id="b482" class="na nb it bd nc nd ne nf ng nh ni nj nk ki nl kj nm kl nn km no ko np kp nq nr bi translated">量化误差</h1><p id="1edd" class="pw-post-body-paragraph lh li it lj b lk ns kd lm ln nt kg lp lq nu ls lt lu nv lw lx ly nw ma mb mc im bi translated">每个人都喜欢情节。但是除了校准图之外，我们需要一种定量的方法来测量(误)校准。最常用的指标称为<strong class="lj jd">预期校准误差</strong>。它回答了这个问题:</p><blockquote class="md"><p id="ac05" class="me mf it bd mg mh mi mj mk ml mm mc dk translated">平均来说，我们的预测概率离真实概率有多远？</p></blockquote><p id="ad48" class="pw-post-body-paragraph lh li it lj b lk mn kd lm ln mo kg lp lq mp ls lt lu mq lw lx ly mr ma mb mc im bi translated">让我们以一个分类器为例:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi oy"><img src="../Images/a1212d69de4b07784d6303aea64e4b19.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1p1h1-s4sGfG-9_SfqE0ZA.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">单仓校准。[图片由作者提供]</p></figure><p id="6f28" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">很容易定义单个库的<strong class="lj jd">校准误差:它是预测概率的平均值和同一库内阳性分数的绝对差值</strong>。</p><p id="8c3e" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">如果你仔细想想，这是非常直观的。取一个bin，假设其预测概率的平均值为25%。因此，我们预计该仓中的阳性部分约等于25%。该值离25%越远，该容器的校准越差。</p><p id="3d5d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">因此，<strong class="lj jd">预期校准误差(ECE)是单个箱</strong>校准误差的加权平均值，其中每个箱与其包含的观测值数量成比例加权:</p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi pg"><img src="../Images/221d62fc25541026ec2d61805e344d22.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*R9VBbiDZ4ZcZ9e8zvs3rng.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">预期校准误差。[图片由作者提供]</p></figure><p id="4662" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">其中<em class="mw"> b </em>标识一个箱，并且<em class="mw"> B </em>是箱的数量。请注意，分母只是样本总数。</p><p id="1de8" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">但是这个公式给我们留下了定义箱数的问题。为了找到尽可能中性的度量，我建议根据<strong class="lj jd"> </strong> <a class="ae nx" href="https://en.wikipedia.org/wiki/Freedman%E2%80%93Diaconis_rule" rel="noopener ugc nofollow" target="_blank"> <strong class="lj jd">弗里德曼-迪康尼斯规则</strong> </a>(这是为<strong class="lj jd">找到使直方图尽可能接近理论概率分布</strong>的箱数而设计的统计规则)来设置箱数。</p><p id="8195" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在Python中使用Freedman-Diaconis规则极其简单，因为它已经在numpy的histogram函数中实现了(将字符串“fd”传递给参数“bins”就足够了)。</p><p id="f5e6" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">以下是预期校准误差的Python实现，默认采用Freedman-Diaconis规则:</p><pre class="ks kt ku kv gt ny nz oa ob aw oc bi"><span id="8e91" class="od nb it nz b gy oe of l og oh">def <strong class="nz jd">expected_calibration_error</strong>(y, proba, bins = 'fd'):</span><span id="8038" class="od nb it nz b gy oi of l og oh">  import numpy as np</span><span id="2cb0" class="od nb it nz b gy oi of l og oh">  bin_count, bin_edges = np.histogram(proba, bins = bins)<br/>  n_bins = len(bin_count)</span><span id="2010" class="od nb it nz b gy oi of l og oh">  bin_edges[0] -= 1e-8 # because left edge is not included<br/>  bin_id = np.digitize(proba, bin_edges, right = True) - 1</span><span id="b18f" class="od nb it nz b gy oi of l og oh">  bin_ysum = np.bincount(bin_id, weights = y, minlength = n_bins)<br/>  bin_probasum = np.bincount(bin_id, weights = proba, minlength = n_bins)</span><span id="7a6d" class="od nb it nz b gy oi of l og oh">  bin_ymean = np.divide(bin_ysum, bin_count, out = np.zeros(n_bins), where = bin_count &gt; 0)<br/>  bin_probamean = np.divide(bin_probasum, bin_count, out = np.zeros(n_bins), where = bin_count &gt; 0)</span><span id="0437" class="od nb it nz b gy oi of l og oh">  ece = np.abs((bin_probamean - bin_ymean) * bin_count).sum() / len(proba)</span><span id="441f" class="od nb it nz b gy oi of l og oh">  return ece</span></pre><p id="47c4" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">现在我们有了一个校准的度量标准，让我们比较一下上面获得的三个模型的校准(测试集上的<strong class="lj jd">):</strong></p><figure class="ks kt ku kv gt kw gh gi paragraph-image"><div role="button" tabindex="0" class="kx ky di kz bf la"><div class="gh gi ph"><img src="../Images/09c722ae0465078f1fc34e77baf0a4ab.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N1fYXTaZ2J3ISv0zFgqpaw.png"/></div></div><p class="ld le gj gh gi lf lg bd b be z dk translated">模型的预期校准误差。[图片由作者提供]</p></figure><p id="5707" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">在这种情况下，就校准而言，保序回归提供了最好的结果，平均距离真实概率只有1.2%。如果你考虑到普通随机森林的ECE是7%,这是一个巨大的进步。</p><h1 id="6e22" class="na nb it bd nc nd ne nf ng nh ni nj nk ki nl kj nm kl nn km no ko np kp nq nr bi translated">参考</h1><p id="de25" class="pw-post-body-paragraph lh li it lj b lk ns kd lm ln nt kg lp lq nu ls lt lu nv lw lx ly nw ma mb mc im bi translated">如果你想深化概率校准的主题，我推荐一些有趣的论文(本文基于这些论文):</p><ul class=""><li id="cb23" class="oj ok it lj b lk ll ln lo lq ol lu om ly on mc oo op oq or bi translated">Caruana和Niculescu-Mizil于2005年在监督学习下预测好的概率。</li><li id="bf86" class="oj ok it lj b lk os ln ot lq ou lu ov ly ow mc oo op oq or bi translated">【关于现代神经网络的标定】郭等 (2017)。</li><li id="b24d" class="oj ok it lj b lk os ln ot lq ou lu ov ly ow mc oo op oq or bi translated"><a class="ae nx" href="https://people.cs.pitt.edu/~milos/research/AAAI_Calibration.pdf" rel="noopener ugc nofollow" target="_blank">nae ini等人使用贝叶斯宁滨</a> (2015)获得校准良好的概率。</li></ul></div><div class="ab cl pi pj hx pk" role="separator"><span class="pl bw bk pm pn po"/><span class="pl bw bk pm pn po"/><span class="pl bw bk pm pn"/></div><div class="im in io ip iq"><p id="bf07" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">感谢您的阅读！我希望这篇文章对你有用。</p><p id="698d" class="pw-post-body-paragraph lh li it lj b lk ll kd lm ln lo kg lp lq lr ls lt lu lv lw lx ly lz ma mb mc im bi translated">我感谢反馈和建设性的批评。如果你想谈论这篇文章或其他相关话题，你可以发短信给我<a class="ae nx" href="https://www.linkedin.com/in/samuelemazzanti/" rel="noopener ugc nofollow" target="_blank">我的Linkedin联系人</a>。</p></div></div>    
</body>
</html>