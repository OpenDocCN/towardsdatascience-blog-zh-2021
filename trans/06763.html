<html>
<head>
<title>Importance of Loss functions in Deep Learning and Python Implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">损失函数在深度学习和Python实现中的重要性</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/importance-of-loss-functions-in-deep-learning-and-python-implementation-4307bfa92810?source=collection_archive---------19-----------------------#2021-06-18">https://towardsdatascience.com/importance-of-loss-functions-in-deep-learning-and-python-implementation-4307bfa92810?source=collection_archive---------19-----------------------#2021-06-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/4c98202ae191bbd4eddaa73d0d128257.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*m0M3fLXoAydddgck7-8Jqw.jpeg"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">来源:Unsplash上的Mikael Kristenson</p></figure><p id="6090" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">我们知道在神经网络中，神经元用相应的权重、偏置和它们各自的<a class="ae la" rel="noopener" target="_blank" href="/intuitions-behind-different-activation-functions-in-deep-learning-a2b1c8d044a">激活函数</a>工作。权重与输入相乘，然后在进入下一层之前对元素应用激活函数。最后通过输出层得到预测值(yhat)。但是预测总是更接近实际(y)，我们称之为误差。因此，我们定义了损失/成本函数来捕捉误差，并试图通过反向传播来优化它。</p><p id="a8c6" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">基于问题陈述，有不同类型的损失函数，我们试图对其进行优化。在本文中，我们将讨论深度学习中的不同损失函数。我们将详细讨论下面的损失函数:</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi lb"><img src="../Images/9cb42b5cce62221e6977a343a4d276c4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1300/format:webp/1*kYrO5TZhQKGAhkc-gQDFFQ.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><h1 id="26d0" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated"><strong class="ak"> 1) </strong> <strong class="ak">回归基础问题中的损失函数</strong></h1><p id="ba93" class="pw-post-body-paragraph kc kd iq ke b kf me kh ki kj mf kl km kn mg kp kq kr mh kt ku kv mi kx ky kz ij bi translated"><strong class="ke ir"> a) </strong> <strong class="ke ir">均方误差损失</strong></p><p id="1da5" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">均方差(MSE)是回归问题中非常常用的损失函数。</p><p id="e801" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如果目标变量的分布是高斯分布，则MSE是优选的损失函数。均方误差被定义为预测值和实际值之间的平方差的平均值。成本函数看起来像:</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi mj"><img src="../Images/db5de684fdba1fcd3405ed13ec928166.png" data-original-src="https://miro.medium.com/v2/resize:fit:410/format:webp/1*-7wy5-EqWKUu00jUDqz-Gw.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="9d7c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">结果总是积极的。由于正方形，较大的错误比较小的错误导致更多的错误。换句话说，MSE是在惩罚犯了更大错误的模型。</p><p id="daad" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">优点:</strong></p><p id="ab8f" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">由于该方程本质上是二次的，梯度下降只有一个全局最小值。</p><p id="17f6" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">二。不存在局部最小值。</p><p id="3e2b" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">三。惩罚犯较大错误的模型。</p><p id="6b4d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">缺点:</strong>如果数据包含异常值，这个损失函数是不稳健的</p><p id="b374" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">注意:</strong>如果目标列的范围相当分散，在这种情况下，由于MSE的性质，预测大值可能会严重影响模型。在这种情况下，代替MSE，使用<strong class="ke ir">均方对数误差(MSLE)损失</strong>。这里，首先计算实际值和预测值的自然对数，然后计算均方误差。成本函数看起来像:</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi mk"><img src="../Images/fe3af6a69bbc2826a13cecbf606c26c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:602/format:webp/1*hOECZHszZG3thRErdL_mNQ.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="0a0d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> b) </strong> <strong class="ke ir">平均绝对误差损失</strong></p><p id="906e" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">平均绝对误差(MAE)也是回归问题中另一个重要的损失函数。它被定义为实际值和预测值之间的绝对差值的平均值。成本函数看起来像:</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi ml"><img src="../Images/d8cea35ff64913185c894b5ad6a53c17.png" data-original-src="https://miro.medium.com/v2/resize:fit:298/format:webp/1*bz6EVfVf5NhqtujPICKYFw.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="adfc" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">优点:</strong>与MSE相比，MAE对异常值更稳健。</p><p id="e15e" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">缺点:</strong></p><p id="5571" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">I .计算成本高，因为与平方误差相比，模数误差很难求解。</p><p id="ea4c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">二。可能存在局部最小值。</p><p id="c06c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">三。即使损失很小，梯度也会变大，因为梯度在过程中保持不变，这不适合学习。为了解决这个问题，可以使用动态学习率。</p><p id="e3f8" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> c) </strong> <strong class="ke ir">胡伯损失(平滑平均绝对误差)</strong></p><p id="2993" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">Huber损耗通过结合MSE和MAE起着重要的作用。如果损失更高，它将二次方程变为线性方程。如果误差小于临界值(ε),则使用MSE，否则可以使用MAE。损失函数定义为:</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi mm"><img src="../Images/0e6b31ddb3141043e7b031ced84b4a7c.png" data-original-src="https://miro.medium.com/v2/resize:fit:586/format:webp/1*C9KYkfbj2xOV--TD5V1IFw.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="f8c3" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">Huber损耗曲线围绕最小值，这降低了梯度，这与MAE相比更好，因为MAE具有恒定的大梯度。这导致在使用梯度下降的训练结束时丢失最小值。另一方面，与均方误差损失相比，Huber损失对异常值不太敏感。</p><p id="f79b" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">请注意，增量的选择很重要，因为它有助于确定异常值标准。</p><p id="5054" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">优点:</strong></p><p id="33c6" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">I .对异常值的处理是明智的。</p><p id="3924" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">二。不存在局部最小值。</p><p id="5ebb" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">三。它在0也是可微的。</p><p id="fa31" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">缺点:</strong>需要优化额外的超参数(ε)，这是一个迭代的过程。</p><p id="2b38" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">从下图可以清楚地看出，Huber结合了MAE和MSE，并采用了一种理想的方法来克服MSE和MAE的缺点。</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi mn"><img src="../Images/29178ac5ab6159a547e2967dbae63f35.png" data-original-src="https://miro.medium.com/v2/resize:fit:786/format:webp/1*_R2ho-YYVvml5uZoU5OPEw.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">图片来自作者</p></figure><h1 id="bbcd" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated"><strong class="ak"> 2) </strong> <strong class="ak">基于二值分类问题中的损失函数</strong></h1><p id="bac0" class="pw-post-body-paragraph kc kd iq ke b kf me kh ki kj mf kl km kn mg kp kq kr mh kt ku kv mi kx ky kz ij bi translated"><strong class="ke ir">a)</strong>T14】二元交叉熵</p><p id="039d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">交叉熵是用于分类问题的常用损失函数。它测量两个概率分布之间的差异。如果交叉熵很小，则表明两个分布彼此相似。</p><p id="b014" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在二元分类的情况下，预测概率与目标/实际(0或1)进行比较。二元交叉熵计算分数，该分数提供了用于预测类别1的实际概率和预测概率之间的负平均差。该分数基于与期望值的距离来惩罚概率。损失函数定义为:</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi mo"><img src="../Images/3f021ee97475c70b705eaf006278f7ac.png" data-original-src="https://miro.medium.com/v2/resize:fit:688/format:webp/1*o2p-8tAVfdMgMSFGQ_oLmw.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="18a7" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> b) </strong> <strong class="ke ir">铰链损耗</strong></p><p id="92fc" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">铰链损失是二进制分类问题的另一个损失函数。它主要是为支持向量机(SVM)模型开发的。铰链损耗是根据“最大裕度”分类计算的。</p><p id="c9d6" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如果目标值在集合(-1，1)中，则使用该损失函数。必须将目标变量修改为集合中的值(-1，1)，这意味着如果y的值为0，则需要将其更改为-1。</p><p id="d132" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">损失函数定义为:</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/6a6b5e55581f8066d8e3a6577c963379.png" data-original-src="https://miro.medium.com/v2/resize:fit:360/format:webp/1*w6TQ23GwbjGKpv1BED-2kQ.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="d069" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">如果实际类别值和预测类别值之间的符号存在差异，则铰链损失函数试图通过分配更多误差来确保正确的符号。</p><h1 id="bd18" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated"><strong class="ak"> 3) </strong> <strong class="ak">基于多类分类问题中的损失函数</strong></h1><p id="3b89" class="pw-post-body-paragraph kc kd iq ke b kf me kh ki kj mf kl km kn mg kp kq kr mh kt ku kv mi kx ky kz ij bi translated"><strong class="ke ir">一)</strong>T6】多类交叉熵</p><p id="3d8f" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">在多类分类的情况下，将预测概率与目标/实际概率进行比较，其中每个类被分配一个唯一的整数值(0，1，3，…，t)，假设数据有t个唯一的类。它计算一个分数，该分数提供所有类的实际概率和预测概率之间的负平均差。损失函数定义为:</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/5b8716994558c49816c1f08c8f2c750a.png" data-original-src="https://miro.medium.com/v2/resize:fit:782/format:webp/1*uTKFTQHRztAVtd1NNURQYQ.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="6b8b" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">对于分类交叉熵损失函数，需要确保在n维向量中，除了对应于类别的条目为1(一次热编码)之外，所有条目都为0。</p><p id="8ade" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">例如，对于3类分类问题，其中第一个观察属于第三类，第二个观察属于第一类，第三个观察属于第二类，目标(y)将是:<em class="mr"> y = [[0，0，1]，[1，0，0]，[0，1，0]] </em></p><p id="170c" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir"> b) </strong> <strong class="ke ir">稀疏多类交叉熵损失</strong></p><p id="e8fb" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">两者，多类交叉熵和稀疏多类交叉熵具有相同的损失函数，上面提到过。唯一的区别是真实标签(y)的定义方式。对于稀疏分类交叉熵，只需要提供一个整数单元，而不是一个n维向量。请注意，整数代表数据的类别。</p><p id="6d54" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">对于多类交叉熵，实际目标(y)是一热编码的。对于三级分类[[0，0，1]，[1，0，0]，[0，1，0]]</p><p id="4f2e" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">对于稀疏多类交叉熵，实际目标(y)是整数。对于以上三类分类问题:[3]、[1]、[2]</p><p id="3ea1" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">与多类交叉熵相比的优势:</strong>以上示例表明，对于多类交叉熵，目标需要一个包含大量零值的热编码向量，这导致了显著的存储器需求。通过使用稀疏分类交叉熵，可以<strong class="ke ir">节省计算时间，降低内存需求</strong>，因为它只需要一个单一的整数作为一个类，而不是一个完整的向量。</p><p id="288d" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">稀疏多类交叉熵的缺点:</strong>多类交叉熵可用于任何一类分类问题。然而，稀疏分类交叉熵只能在每个输入只属于一个类时使用。</p><p id="c194" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">例如，如果我们有3个类(a，b，c ),假设一个输入属于b类和c类，那么多类交叉熵的标签可以表示为[0，1，1],但不能表示为稀疏多类。</p><p id="2bfa" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">c)</strong>T22】kull back lei bler(KL)发散损失</p><p id="7011" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">Kullback Leibler散度是一种度量，它显示了两个概率分布彼此之间的差异程度。损失函数定义为:</p><figure class="lc ld le lf gt jr gh gi paragraph-image"><div class="gh gi ms"><img src="../Images/4d4f6469da95c881a443dbb2d7a245e0.png" data-original-src="https://miro.medium.com/v2/resize:fit:272/format:webp/1*Kb2FbhYsyK9zgoLFawOxJQ.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="1518" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">KL散度损失为0表明分布是相同的。</p><p id="4885" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">KL散度在某种程度上类似于交叉熵。像多类交叉熵一样，这里也需要对实际目标(y)进行一次性编码。如果使用预测的概率分布来近似期望的目标概率分布，则它计算有多少信息丢失。</p><p id="c830" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">KL散度主要用于变分自动编码器。在这里，自动编码器学习如何将样本编码成潜在的概率分布，该概率分布被进一步馈送到解码器以生成输出。此外，KL散度可用于多类分类。</p><h1 id="7e07" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated"><strong class="ak">使用Keras定义不同损失函数的Python片段</strong></h1><figure class="lc ld le lf gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mt"><img src="../Images/94d18106a4e6abdaf4e60d7ef52ae08d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7BDEt_h34yA35DCG7zB4tw.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><h1 id="79f4" class="lg lh iq bd li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz ma mb mc md bi translated"><strong class="ak">何时使用哪些损失函数</strong></h1><p id="d3cd" class="pw-post-body-paragraph kc kd iq ke b kf me kh ki kj mf kl km kn mg kp kq kr mh kt ku kv mi kx ky kz ij bi translated">如果目标变量是连续的(回归问题)，那么可以使用MSE、MAE和Huber损失。通常，MSE是常用的损失函数，但如果数据有异常值，则可以使用MAE。但是如果使用MAE，由于模数函数，它的计算量很大，并且还会产生梯度下降的问题。为了克服这些问题，Huber损失被公认为损失函数，尽管δ的选择是迭代的和重要的。</p><p id="5526" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">对于分类问题，如果目标类是二进制的，则使用二进制交叉熵损失。另一方面，对于多类分类，可以使用多类交叉熵损失。与多类交叉熵相比，稀疏多类交叉熵要快得多，因为输入目标考虑的是整数，而不是一次性编码的向量。然而，多类交叉熵更一般化，因为输入目标可以表示为前面提到的多个类。铰链损失主要用于SVM模型的二元分类。KL散度主要用于比简单的多类分类更复杂的函数(如变分自动编码器)。</p><p id="e848" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated">希望你喜欢这篇文章！！</p></div><div class="ab cl mu mv hu mw" role="separator"><span class="mx bw bk my mz na"/><span class="mx bw bk my mz na"/><span class="mx bw bk my mz"/></div><div class="ij ik il im in"><p id="15f8" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><em class="mr">免责声明:本文所表达的观点是作者以个人身份发表的意见，而非其雇主的意见</em></p><p id="4b41" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><strong class="ke ir">参考文献:</strong></p><p id="38de" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><a class="ae la" href="https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy" rel="noopener ugc nofollow" target="_blank">https://ml-cheat sheet . readthedocs . io/en/latest/loss _ functions . html #交叉熵</a></p><p id="da2e" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><a class="ae la" href="https://keras.io/api/losses/" rel="noopener ugc nofollow" target="_blank">https://keras.io/api/losses/</a></p><p id="d4a0" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><a class="ae la" href="https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0" rel="noopener ugc nofollow" target="_blank">https://heart beat . fritz . ai/5-regression-loss-functions-all-machine-learners-should-know-4fb 140 e 9 D4 b 0</a></p><p id="9fb2" class="pw-post-body-paragraph kc kd iq ke b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz ij bi translated"><a class="ae la" href="https://www.machinecurve.com/index.php/2019/12/21/how-to-use-kullback-leibler-divergence-kl-divergence-with-keras/" rel="noopener ugc nofollow" target="_blank">https://www . machine curve . com/index . PHP/2019/12/21/how-to-use-kull back-lei bler-divergence-KL-divergence-with-keras/</a></p></div></div>    
</body>
</html>