<html>
<head>
<title>Understanding Proximal Policy Optimization (Schulman et al., 2017)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解最接近的政策优化(舒尔曼等人，2017年)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/understanding-and-implementing-proximal-policy-optimization-schulman-et-al-2017-9523078521ce?source=collection_archive---------5-----------------------#2021-05-06">https://towardsdatascience.com/understanding-and-implementing-proximal-policy-optimization-schulman-et-al-2017-9523078521ce?source=collection_archive---------5-----------------------#2021-05-06</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="1668" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">作为一名初学者，我是如何自下而上地接触PPO文件的</h2></div><p id="13a5" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">近年来，政策梯度方法的研究一直很流行，TRPO、GAE和A2C/A3C等算法显示了优于Q-learning等传统方法的最新性能。这个策略梯度/行动者-批评家领域的核心算法之一是OpenAI实现的<strong class="kh ir">近似策略优化算法</strong>。</p><p id="d882" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这篇文章中，我试图做到以下几点:</p><ul class=""><li id="2dbb" class="lb lc iq kh b ki kj kl km ko ld ks le kw lf la lg lh li lj bi translated">通过对政策梯度方法和信赖域方法(TRPO)提供一个初学者友好的概述来讨论PPO背后的动机</li><li id="4bad" class="lb lc iq kh b ki lk kl ll ko lm ks ln kw lo la lg lh li lj bi translated">了解PPO的核心贡献:<strong class="kh ir">削减替代目标</strong>和<strong class="kh ir">多时代政策更新</strong></li></ul><h1 id="3d8e" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">目的</h1><h2 id="c393" class="mh lq iq bd lr mi mj dn lv mk ml dp lz ko mm mn mb ks mo mp md kw mq mr mf ms bi translated">破坏性策略更新</h2><p id="dbfd" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">我们首先需要理解如下定义的策略梯度方法的优化目标:</p><figure class="mz na nb nc gt nd gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi my"><img src="../Images/dfaed1a30f0dd4b7d3b1c65abea37968.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*0ANScvbSomueKA_A"/></div></div><p class="nk nl gj gh gi nm nn bd b be z dk translated">政策损失函数(舒尔曼等人，2017年)</p></figure><p id="6b2f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">策略pi是我们的神经网络，它将来自环境的状态观察作为输入，并建议要采取的行动作为输出。优点是对当前状态下所选动作的相对值的估计，因此hat超过A。它被计算为<em class="no">折扣奖励(Q) —价值函数</em>，其中价值函数基本上给出了奖励折扣总额的估计值。在训练时，这个代表价值函数的神经网络将经常使用我们的代理在环境中收集的经验进行更新。然而，这也意味着<strong class="kh ir">值估计会由于网络</strong>引起的变化而非常嘈杂；网络并不总是能够预测该状态的精确值。</p><p id="76c9" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">将政策产出和优势函数的对数概率相乘，给了我们一个巧妙的优化函数。如果优势为正，意味着代理人在样本轨迹中采取的行动导致了高于平均水平的回报，政策梯度将为正，以增加我们在遇到类似情况时再次选择这些行动的概率。如果优势是负的，政策梯度将是负的，以做完全相反的事情。</p><p id="3658" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在一批收集的经验中不断地执行梯度下降步骤是很有吸引力的，它将经常更新参数，使之远远超出导致<strong class="kh ir">“破坏性大的策略更新”的范围</strong></p><h2 id="a8ff" class="mh lq iq bd lr mi mj dn lv mk ml dp lz ko mm mn mb ks mo mp md kw mq mr mf ms bi translated">信任区域策略优化</h2><p id="07cc" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">防止这种破坏性策略更新的方法之一是<a class="ae np" href="https://arxiv.org/abs/1502.05477" rel="noopener ugc nofollow" target="_blank"> <em class="no">信任区域策略优化(舒尔曼等人，2015) </em> </a>。在本文中，作者实现了一种算法来限制策略梯度步骤，使其不会偏离原始策略太多，从而导致经常完全破坏策略的过大更新。</p><p id="8059" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">首先，我们将r(θ)定义为当前策略下的动作与先前策略下的动作之间的概率比。</p><figure class="mz na nb nc gt nd gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi my"><img src="../Images/ac98cd7c8672b3929ea9bb5225fd73ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*fYlAqUU4pn2E09iV"/></div></div><p class="nk nl gj gh gi nm nn bd b be z dk translated">舒尔曼等人，2017年</p></figure><p id="f1af" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">给定一系列采样动作和状态，如果特定动作对于当前策略比对于旧策略更有可能，则r(θ)将大于1。对于我们当前的策略，当行动的可能性较低时，它将介于0和1之间。</p><p id="8547" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在，如果我们将这个r(θ)乘以前面提到的优势函数，我们会以更易读的格式得到TRPO的目标:</p><figure class="mz na nb nc gt nd gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi my"><img src="../Images/ef63490c1d0dbda3fc3098d0a79ef02f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*XMQUzHw-q7powIlI"/></div></div><p class="nk nl gj gh gi nm nn bd b be z dk translated">(舒尔曼等人，2017年)</p></figure><p id="70e2" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这个TRPO方法中，我们注意到它实际上非常类似于左边的普通政策梯度方法。实际上，这里唯一的区别是log运算符被替换为当前策略的动作概率除以前一个策略下的动作概率。优化这个目标函数在其他方面是相同的。</p><p id="d7d0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">此外，TRPO添加了KL约束，以限制梯度步长使策略远离原始策略。这导致梯度停留在我们知道一切正常的区域，因此命名为“信任区域”然而，已知这个KL约束增加了我们的优化过程的开销，这有时会导致不期望的训练行为。</p></div><div class="ab cl nq nr hu ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="ij ik il im in"><h1 id="46a9" class="lp lq iq bd lr ls nx lu lv lw ny ly lz jw nz jx mb jz oa ka md kc ob kd mf mg bi translated">近似策略优化(PPO)</h1><h2 id="bd1f" class="mh lq iq bd lr mi mj dn lv mk ml dp lz ko mm mn mb ks mo mp md kw mq mr mf ms bi translated">剪切替代目标</h2><p id="4b47" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">基于上述动机，近似策略优化试图简化优化过程，同时保留TRPO的优点。本文的主要贡献之一是删减的替代目标:</p><figure class="mz na nb nc gt nd gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi my"><img src="../Images/d9578191ac51bef976a0c35fb492a5ce.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mIZZG5SFrSb29IGz"/></div></div><p class="nk nl gj gh gi nm nn bd b be z dk translated">删减的替代目标(舒尔曼等人，2017年)</p></figure><p id="4386" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这里，我们计算最少两项的期望值:<em class="no">正常PG目标</em>和<em class="no">削波PG目标</em>。关键部分来自第二项，其中正常的PG物镜被1-ε和1+ε之间的限幅操作截断，ε是超参数。</p><p id="3430" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于min运算，当优势估计为正或负时，此目标表现不同。</p><figure class="mz na nb nc gt nd gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi my"><img src="../Images/3456d229ac649660487bb53e0e1d1375.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*cukcVH434YbzoXLR"/></div></div><p class="nk nl gj gh gi nm nn bd b be z dk translated">(舒尔曼等人，2017年)</p></figure><p id="a33b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">让我们首先来看看左图，它描绘了积极优势:当选定的行动对结果产生了比预期更好的影响时的情况。在图表中，当<em class="no"> r </em>变得太高或者当在当前政策下比在旧政策下更有可能采取行动时，损失函数变平。我们不想因为走得太远而使动作更新过度，所以我们“剪辑”了目标以防止这种情况，同时用一条平线阻挡渐变。</p><p id="8057" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当优势估计为负时，这同样适用于右图。当<em class="no"> r </em>接近零时，损失函数将变平，这意味着在当前政策下不太可能采取特定行动。</p><p id="31db" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">尽管这种方法很聪明，但裁剪操作也有助于我们“撤销”政策的错误。例如，右图中突出显示的部分显示了这样一个区域，在该区域中，最后一个梯度步骤使所选操作更有可能发生，同时也使策略变得更糟，如负优势所示。令人欣慰的是，我们的裁剪操作将友好地告诉梯度走在另一个方向，与我们搞砸的数量成比例。这是唯一一个<code class="fe oc od oe of b">min()</code>里面第一项比第二项低的部分，作为备用方案。最棒的是，PPO无需计算额外的KL约束就能完成所有这些工作。</p><p id="41c4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">所有这些想法都可以在最终损失函数中进行总结，将这个删减的PPO目标和两个附加项相加:</p><figure class="mz na nb nc gt nd gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi my"><img src="../Images/6b8aea292bf9eff5740f5f9ecddd4ee3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*DYM6d5oQUkFEmo1Y"/></div></div><p class="nk nl gj gh gi nm nn bd b be z dk translated">(舒尔曼等人，2017年)</p></figure><p id="14a4" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">c1和c2是超参数。第一项是负责更新基线网络的价值函数的均方误差。第二个术语，可能看起来不熟悉，是一个熵术语，用于确保我们的代理有足够的探索。这个术语将推动政策更自然地运行，直到目标的另一部分开始起主导作用。</p><h2 id="c308" class="mh lq iq bd lr mi mj dn lv mk ml dp lz ko mm mn mb ks mo mp md kw mq mr mf ms bi translated">策略更新的多个时期</h2><p id="a526" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">最后，让我们一起来看看算法及其并行演员的优点:</p><figure class="mz na nb nc gt nd gh gi paragraph-image"><div role="button" tabindex="0" class="ne nf di ng bf nh"><div class="gh gi my"><img src="../Images/2137c20ad7cf293d5e37eeec8ee9ee91.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*kZuENG0itWaJr_Nx"/></div></div><p class="nk nl gj gh gi nm nn bd b be z dk translated">PPO算法(舒尔曼等人，2017年)</p></figure><p id="98ee" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">该算法由两个大线程组成:米色线程和绿色线程。米色线程收集数据，计算优势估计值，并为绿色线程提供小批量样本。一个特别之处是:这些是由<em class="no"> N </em>个并行的角色各自独立完成自己的任务。</p><p id="a390" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">对样本运行梯度下降的多个时期并不常见，因为存在破坏性的大规模策略更新的风险。然而，在PPO的剪切代理目标的帮助下，我们可以利用并行参与者来提高采样效率。</p><p id="5597" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">每隔一段时间，绿色线程就会启动，对我们的削波损失函数运行随机梯度下降。又一次特殊拍摄？我们可以在相同的轨迹样本上运行<em class="no"> K </em>个优化时期。在PPO之前也很难做到这一点，因为在本地样本上采取大步骤的风险，但PPO防止了这一点，同时允许我们从每个轨迹中了解更多。</p><h1 id="fffc" class="lp lq iq bd lr ls lt lu lv lw lx ly lz jw ma jx mb jz mc ka md kc me kd mf mg bi translated">参考</h1><p id="bbb4" class="pw-post-body-paragraph kf kg iq kh b ki mt jr kk kl mu ju kn ko mv kq kr ks mw ku kv kw mx ky kz la ij bi translated">[1] <a class="ae np" href="https://arxiv.org/abs/1707.06347" rel="noopener ugc nofollow" target="_blank">近端政策优化(舒尔曼等，2017) </a></p><p id="f3db" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">[2] <a class="ae np" href="https://arxiv.org/abs/1502.05477" rel="noopener ugc nofollow" target="_blank">信任区政策优化(舒尔曼等，2015) </a></p></div><div class="ab cl nq nr hu ns" role="separator"><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv nw"/><span class="nt bw bk nu nv"/></div><div class="ij ik il im in"><p id="5165" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="no">最初发表于</em></p><p id="0512" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><a class="ae np" href="https://blog.tylertaewook.com/posts/proximal-policy-optimization" rel="noopener ugc nofollow" target="_blank">https://blog . tylertaewook . com/posts/proximal-policy-optimization</a></p></div></div>    
</body>
</html>