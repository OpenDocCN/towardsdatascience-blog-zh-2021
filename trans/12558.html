<html>
<head>
<title>Implementing an efficient generalised Kernel Perceptron in PyTorch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">在 PyTorch 中实现高效的广义核感知器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementing-an-efficient-generalised-kernel-perceptron-in-pytorch-9e9fa6b30761?source=collection_archive---------8-----------------------#2021-12-24">https://towardsdatascience.com/implementing-an-efficient-generalised-kernel-perceptron-in-pytorch-9e9fa6b30761?source=collection_archive---------8-----------------------#2021-12-24</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="1fb1" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">理论、实现和实验</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/393c741cd10085c9c52bf265cabe795c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eo44DE0ONTpHDtNyd1Zw6A.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片来自<a class="ae ky" href="https://unsplash.com/@mockupgraphics" rel="noopener ugc nofollow" target="_blank">模型图</a></p></figure><p id="1333" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感知器是一种古老的线性二进制分类算法，它形成了许多机器学习方法的基础，包括神经网络。像许多线性方法一样，核技巧可以用来使感知器在非线性数据上表现良好，并且与所有二进制分类算法一样，它可以推广到 k 类问题。</p><p id="7341" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我第一次实现感知器算法是在 Numpy，我在 MNIST 数据集上进行实验，以确定一般化模型在各种任务上的表现。不幸的是，当我写我的初始实现时，我生病了，所以它充满了错误，非常慢，而且占用大量内存。休息了一段时间并更好地思考了这个问题后，我决定再试一次，但这次使用我新喜欢的库:PyTorch。</p><p id="e25a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">本文从简单介绍感知器算法和内核化版本开始。然后深入到 k 类的归纳、实现和优化，最后到实验，在实验中，通过多种测试来确定模型在一系列任务中的性能。本文以我在处理这个问题时遇到的一些陷阱和关键要点作为结尾。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="8243" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">感知机是如何工作的</h1><h2 id="bc82" class="mu md it bd me mv mw dn mi mx my dp mm li mz na mo lm nb nc mq lq nd ne ms nf bi translated">在线学习与批量学习</h2><p id="6b41" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">感知器使用在线学习来优化其权重，这与大多数其他使用批量学习的机器学习模型不同。这些方法的关键区别在于如何计算权重。</p><p id="efcf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">利用批量学习，基于所有数据一次性计算成本函数，结果是在同一个步骤中更新<strong class="lb iu">权重。</strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nl"><img src="../Images/b67860c7fdb50f3aec099ae35ad38cfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:468/format:webp/1*sNNmExR4whDwL_POYFwFiQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">例如，在梯度下降法中，我们计算批次(或最小批次)J 的总成本，然后求出它相对于重量的导数。然后我们一次性更新权重向量。λ是步长。</p></figure><p id="5258" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在在线学习中，数据是按顺序输入的，因此，<strong class="lb iu">权重会随着每个新的数据点</strong>而更新。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nm"><img src="../Images/299a5de4d4be344c9c35c1e066f6f5e8.png" data-original-src="https://miro.medium.com/v2/resize:fit:446/format:webp/1*I_Xa5khjzBCn-p6qdM-PtQ.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">数据被视为一个序列(如时间序列)。因此，下一个权重由当前权重和当前数据点(也可能包括所有先前的数据点)确定</p></figure><p id="6b66" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">记住这一点，让我们看看感知器的训练算法是如何工作的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nn"><img src="../Images/afed3b9409fe7e2097dc38c90d801c7d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*N_RDM0Ue6Ym1qAVPutb98A.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">单个时期(通过数据的一个完整周期)的感知器训练算法。如果你有兴趣了解更多关于模型的复杂性，请访问这个<a class="ae ky" href="https://www.cs.utah.edu/~zhe/pdf/lec-10-perceptron-upload.pdf" rel="noopener ugc nofollow" target="_blank">讲座</a>。</p></figure><p id="472d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请注意该模型的几个关键方面:</p><ul class=""><li id="9aed" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu nt nu nv nw bi translated">由于点积运算，该模型只能学习线性决策边界</li><li id="7855" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">对于<strong class="lb iu"> wt </strong> <strong class="lb iu">点</strong> <strong class="lb iu"> xi </strong>为零的情况，符号函数具有模糊性。在这种情况下，我们可以随机选择-1 或 1 作为输出，或者我们可以选择一个，例如≥ 0 → 1，&lt; 0 → -1 或&gt; 0 → 1，≤ 0 → -1(我们将在后面修改)</li><li id="03c0" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">由于学习是在线的，我们不能对 for 循环操作进行矢量化</li><li id="ff9e" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">λ是学习率</li><li id="66ac" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">第一个权重始终为零</li></ul><p id="eb31" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">现在，对于使用模型的预测，我们使用“离线”方案，也就是说，我们不再有更新方法。因此，我们的测试简单地由下式给出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oc"><img src="../Images/6f0d33a431114e467f2b0e8f13456a80.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Oy1xfgvbImXeAPP9-H37xQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">由于这个方法是离线的，我们可以将 for 循环转换成一个有效的矢量化实现</p></figure><p id="0ed7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感知器的一个明显的局限性是，点积的线性使其无法捕捉数据中的非线性相关性。</p><p id="c32d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">幸运的是，内核方法可以用来纠正这一点。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="adf5" class="mu md it bd me mv mw dn mi mx my dp mm li mz na mo lm nb nc mq lq nd ne ms nf bi translated">内核方法的修订</h2><p id="d70c" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">在数学中，核是一种允许你从线性空间映射到复杂度为<strong class="lb iu"> O(n) </strong>的非线性空间的函数，而不是非线性空间所需的复杂度。<br/>假设我们有一个包含 3 个特征的数据点，我们希望将其映射到一个非线性空间中:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi od"><img src="../Images/dffc21b60c1ee9f5bd4fd4a1dd0bcac6.png" data-original-src="https://miro.medium.com/v2/resize:fit:312/format:webp/1*BkVMST4di960VG1KUWSzFA.png"/></div></figure><p id="79cc" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们取<strong class="lb iu"> x </strong>自身的点积，我们就得到一个线性映射:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/ed6f03f9776344763b33b8eb6081524d.png" data-original-src="https://miro.medium.com/v2/resize:fit:424/format:webp/1*y_lP6JrZaPs2q_6wom--og.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">不要被方块弄糊涂了。二次项确实是“非线性”的，然而这里我们所说的线性是指关于特征的线性，也就是说，我们是否有特征的交叉相互作用？在上述情况下，答案是否定的。</p></figure><p id="09ea" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，如果我们平方它，我们得到下面的非线性映射:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi of"><img src="../Images/b41f056eb7994d6a8d12dfff5feab7f3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1008/format:webp/1*G1ICly75Qd22OedZbFWE-Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">我们的非线性项来自 x1 和 x2、x1 和 x3 以及 x2 和 x3 的相互作用。</p></figure><p id="7597" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是强大的，因为:a)我们现在已经捕获了非线性项，b)我们只需要复杂的点积运算。</p><p id="4a35" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果我们想在不使用核函数的情况下进行这个映射，那么我们的复杂度将为<strong class="lb iu"> O(N) </strong>，其中 N=6，因为我们有 6 个新的特征(映射的大小)。</p><p id="1e53" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所以形式上，我们把内核写成:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi og"><img src="../Images/7292e9b8d4222a418ba2b6c0c27ff4a0.png" data-original-src="https://miro.medium.com/v2/resize:fit:410/format:webp/1*Wz-ghUzQ0zeC0gX_G4QCxA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">这只是一种类型的核，多项式核。</p></figure><p id="48cf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当使用内核方法时，关键的区别在于测试模型。代替<strong class="lb iu">符号(w_star </strong> <strong class="lb iu"> dot x_test) </strong>用于预测<strong class="lb iu">，</strong>我们有:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/cdee9c304a65535a6e1e14b1643888b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:644/format:webp/1*QYUtE758SW6CjCJp_JypRw.png"/></div></figure><p id="4823" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<strong class="lb iu">α</strong>是与每个训练点相关联的权重(在训练期间确定)，而<strong class="lb iu"> K(x_test，x_j) </strong>是测试点和训练点<strong class="lb iu"> x_j </strong>之间的核。</p><blockquote class="oi oj ok"><p id="45f6" class="kz la ol lb b lc ld ju le lf lg jx lh om lj lk ll on ln lo lp oo lr ls lt lu im bi translated"><strong class="lb iu">注意:</strong>内核是机器学习的一个活跃领域，它随处可见。如果你对它不熟悉，我强烈建议你深入研究一下，了解一下执行线性回归的对偶和原始形式(Tibshirani 的<a class="ae ky" href="https://hastie.su.domains/Papers/ESLII.pdf" rel="noopener ugc nofollow" target="_blank">统计学习元素</a>是一个很好的参考)</p></blockquote></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="8a72" class="mu md it bd me mv mw dn mi mx my dp mm li mz na mo lm nb nc mq lq nd ne ms nf bi translated">核感知器算法</h2><p id="aa4e" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">内核感知器训练算法如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi op"><img src="../Images/2faf0c659994a8aecd5c30e0a93f098e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*NruhLTIGIPCAlkUp4N2s7w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">更多详情见此<a class="ae ky" href="http://aritter.github.io/courses/5523_slides/kernels.pdf" rel="noopener ugc nofollow" target="_blank">链接</a></p></figure><p id="5804" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">和测试:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/c8f938d26ca91e0c34c0ce21b4dae450.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*9UoZVMKjdUIIAB-7ttdNnw.png"/></div></div></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="2a3e" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">推广到 k 类</h1><p id="c538" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">一般来说，我发现有两种方法可以概括二元分类器，它们是:</p><ul class=""><li id="f341" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu nt nu nv nw bi translated"><strong class="lb iu"> One vs. All(或 OvA): </strong>这个方法创建了 k 个分类器，每个分类器都被训练来挑选一个类，而不是其余的类。这意味着对于每个类<strong class="lb iu"> c_i </strong>，创建一个合成训练集<strong class="lb iu"> y` </strong>,其中:</li></ul><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/70276bc6a8b21243657a34dc8a60c609.png" data-original-src="https://miro.medium.com/v2/resize:fit:346/format:webp/1*gjDXju6dVWN7Dhjic_2cPw.png"/></div></figure><p id="e5ca" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在预测过程中，我们可以让经过训练的模型“竞争”，看看哪个类最适合给定的<strong class="lb iu"> X_test </strong>。因此，预测由下式给出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/c849924eeb2ba433f62106a9725efb3e.png" data-original-src="https://miro.medium.com/v2/resize:fit:298/format:webp/1*CjvSu2YvRlId-wdZOqEpjA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">这里 H_i 代表正在讨论的分类器。我们找到对我们的输入给出最大预测的分类器，然后找到它出现的索引。该索引对应于我们的数据所属的类别！</p></figure><blockquote class="oi oj ok"><p id="b4ee" class="kz la ol lb b lc ld ju le lf lg jx lh om lj lk ll on ln lo lp oo lr ls lt lu im bi translated">重要的是，要注意这种方法要求分类输出是连续的。因此，当使用感知器算法时，我们不能在预测步骤应用<strong class="lb iu">符号</strong>函数。</p></blockquote><p id="06a8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于每个历元，该算法的复杂度为<strong class="lb iu"> O(Mk) </strong>。其中<strong class="lb iu"> M(m，n) </strong>是所使用的二元分类器的复杂度。</p><ul class=""><li id="e2cf" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu nt nu nv nw bi translated"><strong class="lb iu">一对一(或 OvO): </strong>这种方法创建 k(k-1)/2 个分类器，每个分类器在每一个类的组合上被训练。与上面类似，创建一个合成数据集。然而，这里我们在应用转换之前，在每一步过滤数据，只包括属于<strong class="lb iu"> c_i </strong>和<strong class="lb iu"> c_j </strong>的类。</li></ul><p id="df29" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了更好地理解这一点，让我们考虑一个 10 级的例子(数字 0-9，如在 MNIST)。我们将有以下组合:(0，1)，(0，2)，(0，3)，……(1，2)，(1，3)……(8，9)，因此有以下分类器:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/ac8a40edfc36650f646e96be7f7f8f9e.png" data-original-src="https://miro.medium.com/v2/resize:fit:664/format:webp/1*64W6xqIBw46hrCqxsOo-vg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">记住每个 H_ij 是一个分类器。这里 H 代表我们必须训练的分类器集合。注意，我们不需要为排列进行训练，例如(0，1)和(1，0)，因为从(0，1)分类的输出，我们可以推导出类 0 和类 1 的预测！</p></figure><p id="63bf" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">所有这些都是在过滤的数据集上训练的，<strong class="lb iu">y _ filtered = y[(y = = I)|(y = = j)]</strong>。变量的变换是这样进行的:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/3ee04e591204ec8dc6e10becbd0ff587.png" data-original-src="https://miro.medium.com/v2/resize:fit:414/format:webp/1*40WeEZeNSkJ6Bk25nfhG6w.png"/></div></figure><p id="ea99" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最终分类输出由下式给出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ov"><img src="../Images/237b311deea68b4def0e2c92db727e97.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/format:webp/1*HF2TnWrNd9rh6DcSlzUmbw.png"/></div></figure><p id="f683" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">与“一个对所有”不同，分类器之间并没有太多的“竞争”,而是对分类输出进行“投票”。有了这个公式，我们可以将<strong class="lb iu">符号</strong>函数应用于感知器的输出，因为我们关心计数。或者，如果我们决定不应用<strong class="lb iu">符号</strong>，那么我们将取一个加权和，其中感知机输出的值是模型对我们预测的数字的信任度(这里它的行为更像一个‘竞争’)。</p><p id="314d" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个模型的复杂性更难确定，因为 m 对于每个组合都是可变的(记住我们使用的是过滤数据集！).假设类是均匀分布的情况，每次迭代的数据集大小 m_ij = 2m/k。因此，复杂度是<strong class="lb iu"> O(Mk(k-1)/2) </strong>其中<strong class="lb iu"> M=M(2m/k，n) </strong>是二元分类器的复杂度。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="dc39" class="mu md it bd me mv mw dn mi mx my dp mm li mz na mo lm nb nc mq lq nd ne ms nf bi translated">算法在我们的数据上会有什么不同？</h2><p id="aace" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">为了更直观地了解每种算法的工作原理，让我们检查一下数据。</p><p id="8280" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><a class="ae ky" href="https://en.wikipedia.org/wiki/MNIST_database" rel="noopener ugc nofollow" target="_blank"> MNIST </a>数据集是从 0 到 9 的手写数字的集合。每个图像的大小为 16x16，但为了建模，它们被展平。因此，每个图像由一个 256 长的向量表示。数据集包含大约 9000 张这样的图片。</p><p id="4efd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了更好地理解 256 个维度代表了什么，我用 TSNE 在 2D 笛卡尔坐标轴上绘制了这些图像。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ow"><img src="../Images/2a62efe7e258674ef4b85c29cb2be814.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vb3mTAaCbkJ47E5jz7vlrQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 1:MNIST 数据集的 TSNE 嵌入。请注意，不同类之间有很多噪声。每个簇代表一个数字(见图例)。如果你不熟悉 TSNE 的做法，可以把它看作是一种将数据“压缩”到更小维度的方法。在这种情况下，每个 256 像素的图像被“压缩”成二维，这样我们就可以在笛卡尔(x，y)平面上绘制它。(注意添加 MNIST 图片)</p></figure><p id="ae34" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于一对所有分类器，每个分类器学习预测一个数字对其余数字。例如，我们的第一个分类器将尝试预测给定的输入是否为 0。肯定的分类是“+1”，例如模型认为输入图像是 0，而不成功的分类是“-1”，例如模型认为图像不是 0。下图显示了分类器对 MNIST 数据集的作用。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/c3de07f20b75300986944cae70c83274.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*lpfdDmpQeEVnWV2ml6zqzg.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 2:对于每一个类，该模型学习一个决策边界来将它与其余的类分开</p></figure><p id="e015" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在一对一的情况下，我们正在学习使用在较小数据集上训练的 k(k-1)/2 个分类器来预测每个类别组合。下面提供了一个关于 MNIST 数据集的示例。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ox"><img src="../Images/1f28bf8020f348679604e206f45b35cc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1200/1*RcuACfWRQpJGU8WlZdpCIA.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 3:对于每一个类的组合，模型学习一个决策边界来将它们彼此分开。最终输出是所有分类器的“投票”。在这种情况下，k=10，所以我们有(10)(9)/2 = 45 个分类器！然而，仅仅因为这听起来很多并不意味着它一定会更慢。回想一下复杂性。</p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="af33" class="mu md it bd me mv mw dn mi mx my dp mm li mz na mo lm nb nc mq lq nd ne ms nf bi translated">理论性能比较</h2><p id="f97d" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">以上两种方法都是将二进制分类问题推广到 k 类的合理方法。然而，它们都可能遭受所做预测的不确定性。在一对一分类器和加权一对一分类器中，在应用<strong class="lb iu"> argmax </strong>操作之前，输出权重的值可能非常相似。在非加权一对一分类器中，两个类别可以具有相同的投票数。</p><p id="0c79" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">总的来说，我认为 OvO 模型比 OvA 模型表现得更好，因为后者存在阶级不平衡的问题。考虑到数据集中的<em class="ol"> k </em>类同样频繁。这意味着在那次培训中，你的班级比例将是 1:<em class="ol">k</em>1，分别代表<em class="ol">y</em>′= 1 和<em class="ol">y</em>′= 1。这对于大值的<em class="ol"> k </em>来说是非常不平衡的。此外，由于所有其他类都被集中在一起，因此将一个类与所有类分开的边界可能会很复杂，也更难预测。另一方面，一对一模型在数据集很小且噪声很大的情况下可能不太有效。对于上述的相等类别分布，一对一模型具有实际训练集的 2/ <em class="ol"> k </em>的有效训练规模。如果成对的类非常嘈杂，并且由于数据集很小，由于类的高度重叠，决策边界将很难区分。在这种情况下，一个对所有分类器更多地强调训练样本的其余部分，而不是噪声。</p><p id="cc43" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然而，我的实验结果只是部分证实了上述观点，我有兴趣进一步探索这个问题。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="7936" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">优化</h1><p id="6c88" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">如果设计不当，感知器和广义分类器的计算效率会非常低。仅对于感知器，你循环通过<strong class="lb iu"> N </strong>个时期的数据，每个时期通过<strong class="lb iu"> m </strong>个数据点，计算总计为<strong class="lb iu"> m </strong>和内核<strong class="lb iu"> K(xi，xj) </strong>(这需要<strong class="lb iu"> n </strong>个运算，因为它是基于点积的)。在最坏的情况下，复杂度是<strong class="lb iu"> O(Nm n) </strong>，如果不使用矢量化，操作会非常慢。盲目推广感知器只会让情况变得更糟。对于一个对所有分类器，一个简单的实现将重新初始化感知器<strong class="lb iu"> k </strong>次，导致总体复杂度为<strong class="lb iu"> O(Nm nk) </strong>。对于一对一分类器，假设类分布相等，我们得到复杂度为<strong class="lb iu"> O(2Nm n(k-1)/k) </strong>。</p><p id="248c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，我们仔细研究了这个问题，以确保 a)尽可能多地使用矢量化，b)尽可能少地重复概括。</p><h2 id="116b" class="mu md it bd me mv mw dn mi mx my dp mm li mz na mo lm nb nc mq lq nd ne ms nf bi translated">感知器优化</h2><p id="8641" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">首先，注意到内核可以通过创建一个<a class="ae ky" href="https://en.wikipedia.org/wiki/Gram_matrix" rel="noopener ugc nofollow" target="_blank"> Gram 矩阵</a>来矢量化(例如，一个矩阵，其中每个单元是<strong class="lb iu"> xi 点 xj </strong>的函数)。因此，代替在训练循环期间为每个数据点计算<strong class="lb iu"> K(xi，xj) </strong>，我们可以预先创建矩阵<strong class="lb iu"> K=(K(xi，xj) : i，j = 1 … m) </strong>。这样，总和:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oy"><img src="../Images/acbd8fc161a7d4410124e7e6c3d080fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:400/format:webp/1*cEvc96gTvDKOAwRG7CHrEQ.png"/></div></figure><p id="7a14" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">计算方法如下:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oz"><img src="../Images/31f42982b89b4923988026a05dc12d96.png" data-original-src="https://miro.medium.com/v2/resize:fit:152/format:webp/1*j8SIv4V4Dae83ouy_CNyrg.png"/></div></figure><p id="a317" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">其中<strong class="lb iu"> Ki </strong>是指克矩阵的第<strong class="lb iu"> i </strong>行。当然，优势在于内核计算现在相对于数据集的循环是线性的，并且它一次使用 PyTorch 的后端向量操作。这意味着复杂度现在是<strong class="lb iu">O(m ^ n+Nmn)</strong>而不是<strong class="lb iu">O(Nm ^ n)</strong>，其中第一项 m ^ n 是完全向量化的运算。</p><p id="45a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于多项式核，创建 Gram 矩阵非常简单，因为它在表达式中使用了点积。</p><p id="2293" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于多项式核，这相当简单，因为格拉姆矩阵由下式给出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pa"><img src="../Images/0c35642fbbbcacc22a7955e3445da95e.png" data-original-src="https://miro.medium.com/v2/resize:fit:728/format:webp/1*MjB5xQ_8a8ATSuVfSm_TBw.png"/></div></figure><p id="8ada" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以将此改写为:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pb"><img src="../Images/1d0f48dc80051523a76ab1b5cedc8dfc.png" data-original-src="https://miro.medium.com/v2/resize:fit:384/format:webp/1*snp49mHH0C_IbA95tuTR3Q.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">记住一个矩阵(m，n)乘以它的转置将得到一个矩阵(m，m ),其中每个单元是相关向量的点积！</p></figure><p id="45c7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于这个项目，高斯核也被使用。它由下式给出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pc"><img src="../Images/11436050dd13ab515a387b2776e8c7c5.png" data-original-src="https://miro.medium.com/v2/resize:fit:588/format:webp/1*6wd1_N6ouQ-MrdLb5bAULg.png"/></div></figure><p id="055f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这更难以向量化，因为范数有一个减法运算。之前在我的<strong class="lb iu"> numpy </strong>实现中，我尝试将<strong class="lb iu"> X </strong>传播到第三维，然后减去 2D 版本，最后使用求和操作将第三维折叠回来。但是，这种方法不适合大型数组，因为它占用太多内存。也是扑朔迷离，牵扯其中。</p><p id="f28f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于 PyTorch 实现，我做了更多的思考，我意识到范数运算可以写成点积的和:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/c93380375baaebb3fcaf435d5587b231.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*oLmEGeC-Wn7bIEXhBmS1Gw.png"/></div></figure><p id="44b1" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">因此，高斯核可以由下式给出:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pd"><img src="../Images/3df1a8f88b60112b920b13612076b617.png" data-original-src="https://miro.medium.com/v2/resize:fit:992/format:webp/1*MsADfaX13zWNIqai1eFoTQ.png"/></div></figure><p id="5dc0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">两个内核都被有意地写成这样，以确保操作可以应用于具有形状<strong class="lb iu"> (m1，n) </strong>和<strong class="lb iu"> (m2，n) </strong>的任意两个矩阵<strong class="lb iu"> X1 </strong>和<strong class="lb iu"> X2 </strong>，以确保在创建预测 Gram 矩阵时，它们可以在预测期间被重用。</p><h2 id="56f6" class="mu md it bd me mv mw dn mi mx my dp mm li mz na mo lm nb nc mq lq nd ne ms nf bi translated">一对所有优化</h2><p id="0318" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">这里有两个主要的认识。首先，不需要每次都创建 Gram 矩阵，因为输入<strong class="lb iu"> X </strong>总是相同的。因此，不是复杂度为<strong class="lb iu"> O(k(m n+Nmn)) </strong>，而是复杂度为<strong class="lb iu"> O(m n+Nmnk) </strong>。第二，在预测而不是重复期间</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pe"><img src="../Images/f046face6b8e449968c5f7b6c754c204.png" data-original-src="https://miro.medium.com/v2/resize:fit:340/format:webp/1*Vr9xcuuJCH6ZuMRQdEI6-g.png"/></div></figure><p id="9f7e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">对于每个分类器，保存的字母被连接起来并用于执行单个操作:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pf"><img src="../Images/153bc4dc183e13e3378f16922ec4d572.png" data-original-src="https://miro.medium.com/v2/resize:fit:328/format:webp/1*FyUr95CrJawLtRqO4qpIoQ.png"/></div></figure><p id="a3e8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在应用<strong class="lb iu"> argmax </strong>函数之前，给出数据的正确格式。</p><h2 id="3cb4" class="mu md it bd me mv mw dn mi mx my dp mm li mz na mo lm nb nc mq lq nd ne ms nf bi translated">一对一优化</h2><p id="82b3" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">理论上，因为内核方法使用对偶公式，所以在过滤的数据上训练<strong class="lb iu"> k(k-1)/2 </strong>分类器应该比在未过滤的数据上训练<strong class="lb iu"> k </strong>分类器表现得更好(就像一个对所有分类器一样)，因为我们在小得多的<strong class="lb iu"> m </strong>上训练。但是，因为 k 分类器只使用单个 Gram 矩阵，所以在我们的实验中它们更快。标准一对一实施无法重复使用 Gram 矩阵，因为基于 and <strong class="lb iu"> ci </strong>和<strong class="lb iu"> cj </strong>对<strong class="lb iu"> X </strong>进行过滤。</p><p id="b981" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我确实尝试过改进这一点，通过实现一个积极矢量化的一对一算法的替代公式。这就对感知器做了一点小小的修改，所以我们不使用 if 语句来更新权重，而是这样做:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pg"><img src="../Images/20e95ef5f9a9d19a10294015b8e5130c.png" data-original-src="https://miro.medium.com/v2/resize:fit:584/format:webp/1*gwRjxVwL9_kUSOT0r9QnEA.png"/></div></figure><p id="ef08" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果<strong class="lb iu"> yt </strong>在<strong class="lb iu"> [-1，1] </strong>中，那么 alpha 按预期更新。然而，如果我们允许<strong class="lb iu"> yt </strong>包含 0，那么当<strong class="lb iu"> yt=0 </strong>时，不管分类输出如何，权重将保持不变。因此，模型忽略了<strong class="lb iu"> yt=0 </strong>的数据点。</p><p id="973a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们进行以下转换，而不是上述转换:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ph"><img src="../Images/a12b7087fcb67a35e3bc81b9f76e8fc7.png" data-original-src="https://miro.medium.com/v2/resize:fit:636/format:webp/1*rT8vas9r97J0pDAc63Gq4g.png"/></div></figure><p id="cbe3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这允许我们在训练时输入未经过滤的<strong class="lb iu"> X </strong>。因此，尽管我们在全尺寸数据集<strong class="lb iu"> m </strong>上训练<strong class="lb iu"> k(k-1)/2 </strong>个分类器，但我们不必重新初始化 Gram 矩阵<strong class="lb iu"> k(k-1)/2 </strong>次，我在实验中使用了单个时期，因此时间复杂度中的主要项是 Gram 矩阵的时间复杂度，因此这种实现导致了稍微更快的结果。然而，一般来说，情况可能不是这样，速度的瓶颈来自训练循环，而不是来自创建 Gram 矩阵，这是一种矢量化操作。对于非常大的<strong class="lb iu"> k </strong>值，这可能很有用。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="5aac" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">实施策略</h1><p id="f48b" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">基于以上所述，我们可以通过多种方式为不同的任务设计感知器，总结如下:</p><ul class=""><li id="1ca8" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu nt nu nv nw bi translated">基于二进制数据训练的内核感知器，具有高斯或多项式内核</li><li id="3f4a" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">使用一对一公式在 k 类上训练的广义感知器</li><li id="8779" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">使用一对一公式在 k 类上训练的广义感知器</li><li id="5ce4" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">以上既可以用加权票，也可以用等额票</li><li id="31c9" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">它还可以选择使用积极的矢量化</li></ul><p id="3f19" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了在不编写高度特定的代码的情况下促进所有上述功能，模型是使用 Python 类编写的，并遵循带有<code class="fe pi pj pk pl b">fit</code>和<code class="fe pi pj pk pl b">predict</code>方法的<code class="fe pi pj pk pl b">sklearn</code> API 风格。这实现了“即插即用”功能，使得执行子任务更加容易。</p><p id="b218" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感知器初始化取入<code class="fe pi pj pk pl b">epsilon</code>、<code class="fe pi pj pk pl b">epochs</code>、、<code class="fe pi pj pk pl b">kernel</code>和<code class="fe pi pj pk pl b">**kernel_kw</code>。ε是添加的术语，因此:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pm"><img src="../Images/3be4e091f2b8d796371aa4e4f3c7d156.png" data-original-src="https://miro.medium.com/v2/resize:fit:848/format:webp/1*HSEouo1_F516NOU1DwhpAQ.png"/></div></figure><p id="501e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是因为<strong class="lb iu">符号(0) </strong>不明确，所以<code class="fe pi pj pk pl b">epsilon</code>允许用户在以下两者之间选择:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pn"><img src="../Images/3224ee51d015d65d3cc0154ae9e56401.png" data-original-src="https://miro.medium.com/v2/resize:fit:508/format:webp/1*O2FRQQRejQf-uwX1lnj2Eg.png"/></div></figure><p id="a051" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">或者</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi po"><img src="../Images/c75fd47ad177f8ef4c332409c4f05b38.png" data-original-src="https://miro.medium.com/v2/resize:fit:568/format:webp/1*ITf-KjgNeOsCkvtUZ6iwRQ.png"/></div></figure><p id="6a53" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe pi pj pk pl b">kernel</code>是为以下输入实现特定内核(例如多项式或高斯)的函数:</p><ul class=""><li id="0a23" class="no np it lb b lc ld lf lg li nq lm nr lq ns lu nt nu nv nw bi translated">矩阵输入:(m1，n)和(m2，n) →输出 Gram 矩阵形状:(m1，m2)</li><li id="1cf2" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">向量输入:(n，1)和(n，)→输出核形状:(1，1)</li><li id="4b02" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">矩阵和向量输入:(m1，n)和(n，)→输出 Gram 矩阵形状:(m1，1)</li></ul><p id="5414" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">并且<code class="fe pi pj pk pl b">**kernel_kw</code>是与核相关联的任何参数，因此<code class="fe pi pj pk pl b">d</code>用于多项式核，<strong class="lb iu"> c </strong>用于高斯核。这种编写函数的方式允许使用多个参数。</p><p id="669f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><code class="fe pi pj pk pl b">fit</code>方法是标准的，不需要解释，除了权重被初始化并保存为类变量。<code class="fe pi pj pk pl b">predict</code>方法也以标准方式工作，但是有两个额外的可选参数，<code class="fe pi pj pk pl b">weights=None</code>和<code class="fe pi pj pk pl b">return_labels=True</code>。前者在 OneVsAll 和 OneVsOne 期间使用，在预测步骤使用来自每个分类器的训练权重。默认情况下，它是 None，这意味着使用模型中包含的权重。return_labels 确定感知器的输出是否是预测的强度</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pp"><img src="../Images/94bbb67e91759a535bb9fa9177aa7d02.png" data-original-src="https://miro.medium.com/v2/resize:fit:216/format:webp/1*c0D-WEdKmTCEQwOJiD5BkQ.png"/></div></figure><p id="59d0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">或者实际的预测</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi pq"><img src="../Images/78583d212b52b3414474a65163c14e4a.png" data-original-src="https://miro.medium.com/v2/resize:fit:444/format:webp/1*uJYEaywIZMyz41Mi7xPCLQ.png"/></div></figure><p id="4533" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这使我们能够很容易地指定一个对所有或一个对一个的不同行为。OneVsAll 和 OneVsOne 的类遵循类似的结构，并被编写为与任何使用操作<strong class="lb iu"> M 点 w </strong> ( <strong class="lb iu"> M </strong>是矩阵，<strong class="lb iu"> w </strong>是权重)来确定分类输出的二元分类器一起工作。这意味着这可以很好地用于原始公式，只要这些模型遵循与上述感知器算法相同的风格。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="a465" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">实验</h1><h2 id="e37e" class="mu md it bd me mv mw dn mi mx my dp mm li mz na mo lm nb nc mq lq nd ne ms nf bi translated">多项式与高斯核</h2><p id="1d3f" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">多项式核和高斯核训练和测试精度使用一个对所有实现进行了比较。多项式核的范围是 d=1…7，高斯核的范围是 c = 0.01…0.5。结果以及 d 和 c 的最佳值(分别为 d_star 和 c_star)如下表所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pr"><img src="../Images/44b85dc4e95365574ea839d789e76c3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*H80u6z5uu1btn8Lt2P7LgA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表 1:具有多项式内核的 OvA 感知器的训练和测试误差</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ps"><img src="../Images/e4af31c72706465974e6ee8da061f383.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*D0ILs4aOuGquJclQudbZqA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表 2:具有多项式内核的 OvA 感知器的最佳 d_star，以及相应的测试误差</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pt"><img src="../Images/e368408a180a7fcb9e21c03594fc2250.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*SkjWVPby8Lu_VENVr2-I3w.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表 3:具有高斯核的 OvA 感知器的训练和测试误差</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pu"><img src="../Images/2af0828d393d2f80e7e79c124384a2eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*l0lhA5uTf_rkaKbgDtCKUg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表 4:具有高斯核的 OvA 感知器的最佳 c_star，以及相应的测试误差</p></figure><p id="7dbb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">高斯核的性能优于多项式核。这可能是因为高斯核可以捕获比多项式核所能捕获的更复杂的决策边界。然而，这是有代价的。高斯核倾向于更大程度的过度拟合(比较表 1 和表 3 中从训练到测试的误差增加)。这表明高斯核记住了训练数据中的“噪声”,而不是学习潜在的趋势。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="8078" class="mu md it bd me mv mw dn mi mx my dp mm li mz na mo lm nb nc mq lq nd ne ms nf bi translated">卵子对卵子</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pv"><img src="../Images/d5ff11318a47ed4c909b7455e3dca066.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*aDUfyIq642Pk9ExcVWY_kw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表 5:具有多项式内核的 OvO 感知器的训练和测试误差</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pw"><img src="../Images/b8cbec08a74b12321fda47c7f9e9cf4b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*AiGyy6gT3mKkYod8aelLbg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">表 6:具有多项式内核的 OvO 感知器的最佳 d_star，以及相应的测试误差</p></figure><p id="82e7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">比较 OvO(表 5)模型与 OvA 模型(表 1)的训练/测试误差，我们可以看到，对于 d=1，OvO 的性能稍好。这可能是因为正在学习的决策曲面是线性的，但是 OvA 实现显然更适合非线性曲面(参见图 1 并与图 2 进行比较)。随着模型复杂度的增加，我们发现 OvA 比 OvO 表现得更好。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="dbd1" class="mu md it bd me mv mw dn mi mx my dp mm li mz na mo lm nb nc mq lq nd ne ms nf bi translated">难以分类的图像</h2><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi px"><img src="../Images/34cd964963a5d0fb173e237e643e394a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_0RJgFf0zdmTjXo5Tuzonw.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 3:用 1 到 7 度的 OvA 感知器正确分类困难的图像</p></figure><p id="0c2e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">绘制了 OvA 多项式核模型发现难以区分的一些示例。上面的强度项表示应用符号函数之前的分类输出。不出所料，这些都是负面的。结果并不特别令人惊讶。对于第一个图像，虽然预测正确，但它可能很容易是 6。第二幅图像可能是阿德画得不好，但无论哪种方式，每个像素的激活都是暗淡的，形状是高度扭曲和单薄的。下一张图片看起来很像 4，如果人类平均正确地将其分类，我会感到惊讶。下一个图像也是稀疏的。最终图像具有很强的曲率，可以被模型解释为 2。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="8d71" class="mu md it bd me mv mw dn mi mx my dp mm li mz na mo lm nb nc mq lq nd ne ms nf bi translated">混淆矩阵</h2><p id="6614" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">最后，为每个测试模型绘制混淆矩阵。这是为了让读者更好地理解模型在不同图像下的表现。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi py"><img src="../Images/11d20d985e635741eaf89b4d34880b9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mFRtyyTwCVf0P2DVZs0vEg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 4:在表 4 所示的最佳 c_star 上训练的 OvA 高斯核的混淆矩阵</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi pz"><img src="../Images/6a739fda8b82d17608899b88543afe66.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6HJAD0Su_YfX2Iu4VGpBEg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 5:表 2 中在最佳 d_star 上训练的 OvA 多项式核的混淆矩阵</p></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi qa"><img src="../Images/4673f92a11f0bad3add66a6db66d87eb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tjpHrrcrY62comjJ1FOgiQ.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图 6:在表 6 所示的最佳 d_star 上训练的 OvO 多项式核的混淆矩阵</p></figure></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="f6da" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">结束语</h1><h2 id="f1a8" class="mu md it bd me mv mw dn mi mx my dp mm li mz na mo lm nb nc mq lq nd ne ms nf bi translated">陷阱</h2><ul class=""><li id="11d3" class="no np it lb b lc ng lf nh li qb lm qc lq qd lu nt nu nv nw bi translated">如果使用 PyTorch，确保你实例化你知道你的数据类型，我得到的一个常见错误是转换 double 为 float！</li><li id="0604" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">确保你对你的张量运算有信心，特别是当你试图使它一般化的时候</li><li id="2f43" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">从<code class="fe pi pj pk pl b">itertools</code>返回一个只能运行一次的生成器对象。所以如果你想保存它，你必须把它转换成另一个保存在内存中的对象(如<code class="fe pi pj pk pl b">list</code>)。使用<code class="fe pi pj pk pl b">torch.combinations</code>等替代方法是可能的，但这需要一个<code class="fe pi pj pk pl b">torch.Tensor</code>作为类。</li><li id="e702" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">非常小心一对一分类器中的逻辑要求。训练 k(k-1)/2 模型很容易，但是当你试图把它转换成 k 类的预测时，问题就变得不那么简单了。</li></ul></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="4f2d" class="mu md it bd me mv mw dn mi mx my dp mm li mz na mo lm nb nc mq lq nd ne ms nf bi translated">关键要点</h2><ul class=""><li id="3a33" class="no np it lb b lc ng lf nh li qb lm qc lq qd lu nt nu nv nw bi translated">核心感知器使用核心方法，通过经典感知器算法实现非线性决策表面的学习</li><li id="d655" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">由于在线学习训练，如果没有正确初始化，感知器是高度计算密集型的。优化可以通过向量化 Gram 矩阵和编写广义核函数来实现</li><li id="2b1b" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">任何二元分类器都可以使用 OvA 或 OvO 方案来推广</li><li id="8377" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">OvA 创建 k 个分类器，为每个类别区分 1 个类别和其余类别。这些方法可能会受到类不平衡的影响，但是当数据集很小且有噪声时，它们可能会工作得更好。它们将数据置于直接竞争中，并且当分类输出未被加权时无法工作。</li><li id="945b" class="no np it lb b lc nx lf ny li nz lm oa lq ob lu nt nu nv nw bi translated">OvO 创建 k(k-1)/2 个分类器，用于区分类别(ci，cj)的唯一组合。他们的优势是允许加权和平等的投票方案。</li></ul></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h2 id="10b7" class="mu md it bd me mv mw dn mi mx my dp mm li mz na mo lm nb nc mq lq nd ne ms nf bi translated">再现性</h2><p id="ba39" class="pw-post-body-paragraph kz la it lb b lc ng ju le lf nh jx lh li ni lk ll lm nj lo lp lq nk ls lt lu im bi translated">如果你想重现结果，请查看 GitHub 的页面。如果您对它的任何方面有任何问题，请联系我！</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><p id="9d7b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">如果你真的喜欢我的工作并愿意支持我，请考虑使用我的<a class="ae ky" href="https://namiyousef96.medium.com/membership" rel="noopener">推荐链接</a>来获得一个媒体订阅。</p><p id="fea9" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="ol">除非另有说明，所有图片均由作者创作。</em></p></div></div>    
</body>
</html>