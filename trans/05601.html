<html>
<head>
<title>Maths behind Supervised Learning for Dummies: The theory in plain words (Part I).</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">傻瓜监督学习背后的数学:浅显易懂的理论(第一部分)。</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/maths-behind-supervised-learning-for-dummies-the-theory-in-plain-words-part-i-8f9be4d7e33a?source=collection_archive---------23-----------------------#2021-05-18">https://towardsdatascience.com/maths-behind-supervised-learning-for-dummies-the-theory-in-plain-words-part-i-8f9be4d7e33a?source=collection_archive---------23-----------------------#2021-05-18</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><figure class="gl gn jo jp jq jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi jn"><img src="../Images/1103c5e7cb4adabf1e493a3334102c48.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*mES62BAhjchQPwDn"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">蒂姆·莫斯霍尔德在<a class="ae kc" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="65e8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">机器学习是当今最酷技术的基础。</em>这是Ethem Alpaydin为他的书《<em class="lb">机器学习简介</em>》写的序言，这本书是我几年前开始接触数据科学世界时必须要读的。今天，这本书仍然被认为是学术界的圣经。你可以在亚马逊找到的许多书的不同之处在于，它没有谈论技术，它只显示了机器学习的算法部分。这才是重点。大多数今天从事机器学习的人，他们只是使用技术，但不知道这项技术封装的算法的数学基础。在熊猫数据帧上调用python中的“LDA”算法非常简单；但是在R数据帧上用插入符号执行线性回归更容易。如果您需要建立一个具有一定精确度的模型，这是很好的；但是如果你想真正开发你的资源，你真的需要完全理解你所使用的库或包背后的算法和数学。</p><p id="ae1e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我写中型故事，因为这是一个简单的方法来回顾概念，不要忘记它们。因此，我将写几个关于机器学习背后的数学的系列文章，从监督学习开始。不要担心，我知道理论很无聊，所以我会把理论和实践技巧结合起来，以便尽可能有所帮助。</p><h1 id="5ea4" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">什么是监督学习？怎么可能？</h1><p id="bfc3" class="pw-post-body-paragraph kd ke iq kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">我们都知道，算法是一系列的指令，可以将输入转化为输出。然而，很多时候我们不知道将输入转换成输出所需的命令或指令；尽管进行了多年的研究，我们还是无法建立算法。我们所做的是学会完成任务，即使我们不知道算法的确切步骤。我们的方法从一个基于不同参数的非常通用的模型开始，我们将看到该模型在这些参数下对输出的逼近程度。该学习通过改进参数，直到找到最精确的输出。学习对应于调整那些参数的值。我们知道我们不能建立完美的方法，但是是的，一个好的和有用的近似。</p><p id="9492" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在这个过程中，有一个统计学的观点，因为核心任务是从可用数据的样本中进行推断:获得更适合这些数据样本的参数，基于这些参数创建一个<em class="lb">模型</em>，并为全球情况外推该模型。</p><p id="2050" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这个模型可以是预测性的，也可以是描述性的，这取决于我们是想做预测，还是想从数据中获得可解释性。有了它，我们就能够构建不同的应用程序:关联规则、分类任务、回归、模式识别、知识提取、压缩或异常值检测等等。</p><p id="7052" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">回归和分类都是监督学习。标准的简化方法是将输入变量称为X，将输出变量称为Y(回归中的数字和分类中的类别代码)。任务是学习从X到Y的映射，我们假设我们可以通过一个定义了一组参数的模型来做到这一点:Y <em class="lb"> =G(X|P) </em>其中G <em class="lb"> ( ) </em>是模型，<em class="lb"> P </em>是参数。在学习过程中，机器学习算法优化参数以最小化误差，试图尽可能接近正确的值。</p><p id="d0c3" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在继续之前，这里有一个重要的注意事项:机器学习算法和模型之间的差异，因为我们似乎可以互换使用这两个概念。模型是从具有特定和定义的参数的特定数据中学习到的最终表示；而算法则是建立那个模型的过程:<em class="lb">模型=算法(数据)。</em>线性回归是一种机器学习算法；但是<em class="lb"> y=3x1+3x2+1 </em>是一个模型。</p><h1 id="db01" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">参数与非参数。有人监督与无人监督</h1><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div class="gh gi mf"><img src="../Images/12ef9af5ee30702e3df0291886a9e285.png" data-original-src="https://miro.medium.com/v2/resize:fit:606/format:webp/1*iAVJ0FxX_parq8ue1NndUA.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">资料来源:<a class="ae kc" href="http://www.sthda.com/english/articles/40-regression-analysis/167-simple-linear-regression-in-r/" rel="noopener ugc nofollow" target="_blank"> STHDA </a></p></figure><p id="6f4e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在前一节中，我们使用了一个学习目标函数<em class="lb"> G( ) </em>，它将输入变量X最佳地映射到输出Y，试图表示我们不知道并且试图近似的真实函数<em class="lb"> F( ) </em>。如果我们知道<em class="lb"> F( ) </em>的样子，我们会直接使用它，并跳过学习阶段。</p><blockquote class="mk ml mm"><p id="befc" class="kd ke lb kf b kg kh ki kj kk kl km kn mn kp kq kr mo kt ku kv mp kx ky kz la ij bi translated">Y=G(X)+E</p></blockquote><p id="bf35" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的估计有误差，我们的模型是准确的，但并不完美。这种误差可以有三种不同的成分:偏差误差、方差误差和不可约误差。我们稍后会见到他们。</p><p id="59eb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">参数vs非参数</strong></p><p id="8113" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在参数算法中，我们将模型映射到一种形式，而非参数算法可以学习从输入到输出的任何映射。</p><p id="aadf" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在参数算法的情况下，我们假设形式。这简化了过程，但也限制了它能学到的东西。所以我们必须决定模型的形式，然后寻找这种形式的参数。在线性回归的情况下，我们对算法说:“嘿，伙计，你将通过这种形式学到一些东西:<em class="lb"> Y=B2X2 + B1X1+ B0" </em>，算法将寻找使给定数据上的表达式的误差最小化的<em class="lb"> B2、B1 </em>和<em class="lb"> B0 </em>参数。参数算法的其他例子是LDA或逻辑回归。优势是显而易见的:更简单，更快，需要的数据更少。这样做的后果是，你强迫模型去适应一种形式，而这种形式对你的情况来说可能不是最好的，因此准确性会更差。</p><p id="e52e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">相比之下，非参数算法可以自由地学习形状，它们不需要先验知识(SVM、随机森林、贝叶斯、DNN)，但是它们当然需要更多的数据，并且存在对训练数据过度拟合的更高风险。</p><p id="6780" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">有人监督与无人监督</strong></p><p id="58c8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">这两种算法之间的区别很明显:如果你有真实的输出，可以用来比较训练期间的准确性，那么这就是监督学习(你可以回答<em class="lb">我的估计有多好？</em>)；但是如果你对输出没有任何线索，那么这就是无监督学习(<em class="lb">这个人更适合聚类1还是聚类2？谁能告诉我这个人的真实集群？</em>)。</p><p id="691e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在监督学习的情况下，我们有一个关于我们的模型有多好的参考，因为我们有一个在训练期间与之比较的目标输出。当相对于训练数据的给定输出达到可接受的精度时，学习停止。然而，对于无监督算法，你只有输入数据(X)而没有输出目标；所以没有办法计算你的估计的准确性。这发生在聚类或发现关联规则(HDBScan，Apriori，k-medoids)时。</p><p id="0293" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">监督算法中的偏差、方差和不可约误差</strong></p><p id="8b50" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">上面一些段落，我们看到<em class="lb"> Y=G(X)+E </em>在监督学习。这些监督算法可以被视为偏差和方差之间的权衡。<em class="lb">偏差</em>是当我们说模型必须符合特定形式时所做的错误假设。这使得问题更容易解决。<em class="lb">方差</em>是模型对训练数据的敏感度。</p><p id="2f0a" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在监督学习中，算法从训练数据中学习模型。我们从训练数据中估计出<em class="lb"> G( ) </em>，<em class="lb"> G( ) </em>几乎是<em class="lb"> Y </em>，但是有一个误差<em class="lb"> E. </em>这个误差可以拆分为:偏倚误差、方差误差和不可约误差。</p><p id="55b4" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不可避免的错误。不管你用于训练的数据集或你选择的算法如何，这种情况总是会发生。这个误差是由像隐藏变量这样的外部因素引起的。不管你怎么努力，你总是会犯这个错误。</p><p id="637e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">偏向错误。</em>我们做这些假设是为了让<em class="lb"> G()函数</em>更容易学习。因此，参数算法具有更高的偏差。高偏差使算法学习速度更快，但灵活性较低。例如，线性回归和逻辑回归是高偏差算法；而SVM或kNN是低偏差算法。</p><p id="518d" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">方差误差。</em>如果我们选择另一部分训练数据，我们的模型会有多么不同。理想情况下，它不应该改变，但不幸的是，它在许多情况下发生了变化。高方差表明训练数据集中的变化将在很大程度上改变<em class="lb"> G()函数</em>。高方差算法的例子有:kNN和SVM。</p><p id="cc7e" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">由于我们无法控制不可约误差，我们将集中讨论偏差和方差误差。理想情况下，目标是获得低偏差和低方差。但这两者很难同时实现，所以我们必须平衡它们。参数或线性算法通常实现高偏差但低方差；而非参数或非线性算法通常得到低偏差但高方差。</p><p id="6da7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">过拟合和欠拟合</strong></p><p id="f41b" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们的学习过程是通过从训练数据中学习一个G()函数，试图从具体例子中<em class="lb">推断</em>一般模式，与从一般规则中学习具体概念的<em class="lb">推断</em>形成对比。机器学习的最终目标是<em class="lb">将</em>从特定数据很好地推广到问题领域的任何其他数据。</p><p id="01ae" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">一方面，o <em class="lb">过拟合</em>意味着从训练数据中学习太多，但不能很好地推广到其他数据。这是实践中最常见的问题，因为它会带来培训中的噪音。过度拟合在非参数和非线性模型中更常见(低偏差)。另一方面，<em class="lb">欠拟合</em>意味着用不足的数据来训练我们的模型。这就是为什么我们通常迭代，直到达到一定程度的误差。</p><p id="48eb" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在实践中，除了测试集<em class="lb">的误差之外，我们通常还会绘制出<em class="lb">训练集</em>的误差演变。</em>如果迭代太多，我们会到达一个点，在这个点上，训练误差持续减小，但突然测试的误差开始变大。这是因为模型在迭代过程中从训练数据中挑选噪声。甜点可以就在测试误差开始上升之前；然而，我们在实践中使用重采样和验证数据集来找到最佳点。</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/6ed53ca303d6f125fb029a58893aca15.png" data-original-src="https://miro.medium.com/v2/resize:fit:1382/format:webp/1*IZINEIlhcN-n1cWjEeBcRQ.png"/></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><h1 id="586d" class="lc ld iq bd le lf lg lh li lj lk ll lm ln lo lp lq lr ls lt lu lv lw lx ly lz bi translated">分类</h1><p id="4854" class="pw-post-body-paragraph kd ke iq kf b kg ma ki kj kk mb km kn ko mc kq kr ks md ku kv kw me ky kz la ij bi translated">到目前为止，我们已经描述了理解任何机器学习分析所应该知道的不同概念。现在让我们分析分类背后的数学，这是机器学习中最重要的领域之一。</p><p id="60ca" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当从训练数据中学习时，我们希望有一个所有A类例子(也称为正面例子)共享的特定模式，而没有B类例子(也称为负面例子)。例如，我们用数百万个汽车描述来训练我们的数据，这些描述表明每个描述是否属于家庭汽车。我们的模型已经学习了，之后，它能够根据给定的描述，判断一辆我们以前没有见过的汽车是否是家用汽车。</p><p id="8496" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">想象下面的例子，我们有一个数据集X，它描述了不同的汽车型号。每个数据点<em class="lb"> x </em>由两个特征<em class="lb"> x1，x2 </em>组成，分别代表每辆车的价格(<em class="lb"> x1 </em>和发动机功率(<em class="lb">x2</em>)x .这些车中有些是熟悉的(红点)或不熟悉的(绿点):</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mr"><img src="../Images/7e5d3aa95d981063d827d11d0f4bcd1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_gB1opF6itvRTk7xpes1VQ.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="6fb7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们要做的是找到区分红绿点的C矩形。具体来说，我们必须找到价格(x1)的<em class="lb"> p1和p2 </em>值以及限制C矩形的发动机功率的<em class="lb"> e1和e2 </em>值(<em class="lb"> x2 </em>)。换句话说，成为一辆熟悉的汽车就如同在<em class="lb"> p1 &lt; x1 &lt; p2和e1 &lt; x2 &lt; e2 </em>之间绘制出<em class="lb"> p1、p2、e1 </em>和<em class="lb"> e2的合适值。</em>这个方程代表H，我们相信C就是从这个假设类中得出的，也就是可能的矩形的集合(算法)。然而，该学习找到由一组特定的<em class="lb"> p1、p2、e1 </em>和<em class="lb"> e2 </em>值(模型)指定的特定假设/矩形<em class="lb"> h(属于H) </em>。因此，参数学习从假设类H开始，并基于可用数据找到<em class="lb"> p1、p2、e1 </em>和<em class="lb"> e2 </em>的值，创建一个等于或最接近真实C的假设<em class="lb"> h </em>，指出我们将搜索限制到这个假设类(我们强制绘制一个矩形，但可能真实C是一个圆)，从而引入偏差。另外，请注意，如果C内有一个绿点，这将是一个假阳性；而如果C外有一个红点，那就是假阴性。</p><p id="4be5" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在现实生活中，我们不知道c。我们有一个数据集，每辆车的标签为“它熟悉”或“它不熟悉”<em class="lb"> x. </em>这个标签被命名为<em class="lb"> r </em>。然后，我们比较<em class="lb"> h(x) </em>和<em class="lb"> r </em>，看看它们是否匹配。这被称为从数据集X训练的模型<em class="lb"> h </em>的经验误差:</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi ms"><img src="../Images/3773f78d2b337fa2cc83b542e07734fc.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*INaUwsdwHaTdxXUjMcTNDw.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="b458" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">要回答的问题是<em class="lb">它对未包含在训练集中的未来数据点的分类有多好？</em>这就是泛化的问题。根据我们在寻找C时绘制矩形<em class="lb"> h </em>的宽松程度或限制程度，我们可以得到最紧的矩形S或最一般的G:</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mt"><img src="../Images/837459602a0a6da27ebac4e637c6f01b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*c_VLeYLbXmk8cd6-aWM4IQ.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="f2b7" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">S和G之间H的任何<em class="lb"> h </em>都与训练集一致。然而，<em class="lb">对于未来的数据点哪个会更好？</em></p><p id="2270" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">根据H和X，我们将有不同的S和g。让我们选择S和g中间的一个<em class="lb"> h </em>。我们将<em class="lb">边距</em>定义为从边界到最近数据点的中间距离。所以我们选择具有最大余量的<em class="lb"> h </em>来平衡可能性。黄色下划线的数据点定义或<em class="lb">支持</em>边缘:</p><figure class="mg mh mi mj gt jr gh gi paragraph-image"><div role="button" tabindex="0" class="js jt di ju bf jv"><div class="gh gi mu"><img src="../Images/abb47f55f690a65a69d21d1491d505d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*kwjm2Oh9VnsU_MhEfRW4mg.png"/></div></div><p class="jy jz gj gh gi ka kb bd b be z dk translated">作者图片</p></figure><p id="acc8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">我们使用一个错误(损失)函数，它不是检查一个实例是否在正确的类上，而是包括它离边界有多远。在一些关键的应用中，如果有任何未来的实例落在S和G之间，则是有疑问的情况，其被自动丢弃。</p><p id="ca9c" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本系列的下一篇文章中，我们将回顾Vapnik-Chervonenkis维度、可能近似正确的学习、学习多类和回归。</p><p id="bac8" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb"> Adrian Perez是一名数据科学家，拥有超级计算并行算法博士学位。你可以在他的</em> <a class="ae kc" href="https://adrianpd.medium.com/" rel="noopener"> <em class="lb">中型简介</em> </a> <em class="lb">中查看更多关于他的西班牙语和英语内容。</em></p><p id="bb70" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kf ir">参考书目</strong></p><p id="ed79" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">机器学习入门。</em>第四版。埃塞姆·阿尔帕丁山。麻省理工学院出版社2020年出版。</p><p id="cbe0" class="pw-post-body-paragraph kd ke iq kf b kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><em class="lb">掌握机器学习算法</em>。杰森·布朗利博士。2016.</p></div></div>    
</body>
</html>