<html>
<head>
<title>Overfitting is not the only problem Regularisation can help with</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">过度适应并不是正规化可以帮助解决的唯一问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/overfitting-is-not-the-only-problem-regularisation-can-help-with-6fcdbfdb9384?source=collection_archive---------30-----------------------#2021-09-08">https://towardsdatascience.com/overfitting-is-not-the-only-problem-regularisation-can-help-with-6fcdbfdb9384?source=collection_archive---------30-----------------------#2021-09-08</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="032c" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">从数学上理解岭回归在特征数量超过数据点时的作用</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/19a0ec28af77ac191d7398a2d2a6e656.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*w01J9vNW9Ge3oFWs"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">弗兰基·查马基在<a class="ae kv" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><h1 id="8291" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">特征多于数据的问题</h1><p id="e8fb" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">当我们谈论正则化时，我们几乎总是在过度拟合的背景下谈论它，但一个鲜为人知的事实是，它可以帮助解决上述问题。</p><p id="60d5" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">由于数据是机器学习的核心，你永远不会想到会遇到这样的问题。话虽如此，但我们有许多特征而数据点很少，这是很常见的，尤其是在涉及生物学的ML问题中。<strong class="lq ir">在回归问题中，特征多于数据点使得普通最小二乘回归(OLS)表现很差</strong>。</p><p id="a592" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">让我们通过一个简单得可笑的例子来理解为什么。</p><p id="5ec0" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这是直线的方程式</p><p id="9c63" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir"> hθ(x) = θ0 + θ1x1 </strong></p><p id="75cb" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了找出给定<strong class="lq ir"> x1和hθ(x) </strong>的<strong class="lq ir"> θ0和θ1 </strong>的值，我们需要<strong class="lq ir">至少2个数据点。</strong>假设我们根据一个单一特征<strong class="lq ir">预测人的<strong class="lq ir">体重</strong>——他们的<strong class="lq ir">年龄</strong>。在这种情况下，我们需要至少两个人的数据来拟合一条直线。如果我们只有一个人的年龄会怎样？</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mp"><img src="../Images/0a509e7bb28c21de0eca2791dd8af5d9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1114/format:webp/1*RNaT1q2W6yhc5vjGE44CyQ.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">无限解(图片由作者提供)</p></figure><p id="dcd0" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">从上面我们可以看到，多条直线可以穿过一个点，虽然我只展示了三条这样的直线，但是这样的系统有无穷多个解！</p><h1 id="4a0a" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">线性回归的正规方程与不可逆性</h1><p id="d0bc" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">首先，我们在线性回归期间最小化的成本函数是</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi mq"><img src="../Images/e26e522a35d26e36af8f6a6fabf5f39a.png" data-original-src="https://miro.medium.com/v2/resize:fit:478/0*4RCaSOTEuKCgu1T_"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">线性回归成本函数</p></figure><p id="1723" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这里m代表数据点的数量，θ是(n+1)维向量，(n维代表n个特征+1维代表偏差(<strong class="lq ir"> θ0 </strong>)。</p><p id="a365" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">为了最小化这个成本函数，我们将关于θ矩阵中每个条目的偏导数设置为0，并且同时求解θ。</p><h2 id="38a1" class="mr kx iq bd ky ms mt dn lc mu mv dp lg lx mw mx li mb my mz lk mf na nb lm nc bi translated">这是我们最后的法线方程</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nd"><img src="../Images/e4b358b7a730337e9c4623499569e9a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:284/0*B0S2i7PresN5LILG"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">线性回归的正规方程</p></figure><p id="87d8" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><em class="ne">详细的推导对于熟悉微积分的人可以在这里找到</em><a class="ae kv" href="https://see.stanford.edu/materials/aimlcs229/cs229-notes1.pdf" rel="noopener ugc nofollow" target="_blank"><strong class="lq ir"><em class="ne"/></strong></a><em class="ne">第11页。</em></p><p id="9822" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">现在X是由m个数据点和n个特征组成的(m X (n+1))维矩阵。请注意，X的第一列填充了1，以说明偏差。因此，X对于n个特征中的每一个都有一列，加上1的一列，使得它总共有(n+1)列。</strong></p><p id="9538" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">如果我们有两个数据点，分别由特征(2，5)和(4，7)组成，这就是我们矩阵的样子。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nf"><img src="../Images/760f0b3a8cb9f79b18fddab5ee95652f.png" data-original-src="https://miro.medium.com/v2/resize:fit:138/0*vwOZNuxfBeCs5y9p"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">一个示例X矩阵</p></figure><p id="a272" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">现在在正规方程中，<strong class="lq ir">每当我们有m &lt; n </strong>也就是每当X的行数比特征数少的时候，X转置的乘积的<strong class="lq ir">逆，并且X不存在。</strong></p><p id="3b0f" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这里有一个同样的数学证明。如果你不熟悉线性代数，请跳过证明。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/8581577aacdbbd54765d921929a3b1d7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*60FU8Xkd00BdMCS8QiVjDA.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nh"><img src="../Images/b8f30037caf9ed72aab94354d0a03317.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*338dKQf6ZNiskY12rpoiuQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">证据1</p></figure><h2 id="bd1f" class="mr kx iq bd ky ms mt dn lc mu mv dp lg lx mw mx li mb my mz lk mf na nb lm nc bi translated">外卖食品</h2><p id="1034" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated"><strong class="lq ir">T11】</strong></p><h1 id="f1d1" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">救援正规化！</h1><p id="f33c" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">这里是正则化成本函数的等式，更具体地说是<a class="ae kv" href="https://www.mygreatlearning.com/blog/what-is-ridge-regression/#:~:text=Ridge%20regression%20is%20a%20model,away%20from%20the%20actual%20values." rel="noopener ugc nofollow" target="_blank"> <em class="ne">岭回归。</em></a><em class="ne"/><strong class="lq ir">λ</strong>是这里经常微调的正则化参数。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ni"><img src="../Images/e83f8f11361b6446efd44b513bc3202e.png" data-original-src="https://miro.medium.com/v2/resize:fit:652/0*4lbVWolpUrqfk2SD"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">岭回归成本函数</p></figure><p id="0ed0" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">使用与上一节相同的逻辑，我们对该函数进行微分，并将其设置为零，求解<strong class="lq ir">θ</strong>以得出<strong class="lq ir">正则化法线方程。</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nj"><img src="../Images/7700c9bf92cdb94f19dd1aa639c2627c.png" data-original-src="https://miro.medium.com/v2/resize:fit:408/0*qxnFg9scK1IbjwmV"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">正则正规方程</p></figure><p id="516c" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这里，M是一个矩阵，其维数为<strong class="lq ir"> (n+1 X n+1) </strong>，对角线上的值为1，除了左上角的条目为0之外，其他地方的值都为0。<strong class="lq ir">比如一个3X3 M的矩阵看起来是这样的:</strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi nk"><img src="../Images/e69309bc6f6841a271209533af8c9d65.png" data-original-src="https://miro.medium.com/v2/resize:fit:240/0*N_heZG-bAu5YrYqp"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">三维M矩阵</p></figure><p id="9ec1" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">哒哒！只要λ严格大于0，我们可以证明正则化正规方程中的乘积是可逆的。</p><p id="96ac" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">这是对数学好奇的人的证明！如果你不熟悉线性代数，请跳过它。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/e9d21d9e2777efb31beda29dd6e1d189.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1ucwmJ901hYCpP5iw-Ntcg.png"/></div></div></figure><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/c67f494b0a6febb78415f8038a6ecebf.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4VgBa2Xqzk8IaujmuZO9gQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">证据2</p></figure><h2 id="9b56" class="mr kx iq bd ky ms mt dn lc mu mv dp lg lx mw mx li mb my mz lk mf na nb lm nc bi translated">外卖食品</h2><p id="2b47" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated"><strong class="lq ir"> </strong></p><h1 id="c5f9" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">那么梯度下降呢？</h1><p id="09ee" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">到目前为止，我们只谈到了线性回归的正常方程，但如果您使用梯度下降来优化θ呢？虽然从理论上讲，这应该会给你一个解决方案，<strong class="lq ir">事实证明，由于未知因素多于特征，我们有多个解决方案，这个解决方案可能不能很好地推广</strong>。添加正则化项将有助于进一步约束目标，并使解偏向θ的“小”值(输入的小变化不会转化为输出的大变化)。这通常会帮助我们收敛到一个更通用的解决方案。</p><h1 id="41e0" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">这类问题在实践中是如何处理的？</h1><p id="e013" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">通常，当我们试图解决一个回归问题时，当我们面临特征多于数据点的问题时，我们会这样做。</p><ol class=""><li id="6c2c" class="nl nm iq lq b lr mk lu ml lx nn mb no mf np mj nq nr ns nt bi translated">选择山脊(L2)或拉索回归(L1)</li><li id="ebf4" class="nl nm iq lq b lr nu lu nv lx nw mb nx mf ny mj nq nr ns nt bi translated">使用K倍交叉验证并调整正则化参数(λ)</li><li id="e016" class="nl nm iq lq b lr nu lu nv lx nw mb nx mf ny mj nq nr ns nt bi translated">在调整严格大于0的λ之后，应该会产生比普通最小二乘回归更好的性能</li><li id="abbd" class="nl nm iq lq b lr nu lu nv lx nw mb nx mf ny mj nq nr ns nt bi translated">在一些极端的情况下，调整后，如果λ结果为零，这将意味着问题本身并不适合算法，正则化不会给出比OLS更好的解决方案</li></ol><p id="8015" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">我知道我谈到了极端的例子，我们只有1或2个数据点，但在实践中，你会面临大约500个数据点和大约5000个特征的问题，让你选择调整lambda。</p><h1 id="d920" class="kw kx iq bd ky kz la lb lc ld le lf lg jw lh jx li jz lj ka lk kc ll kd lm ln bi translated">结论</h1><p id="f92a" class="pw-post-body-paragraph lo lp iq lq b lr ls jr lt lu lv ju lw lx ly lz ma mb mc md me mf mg mh mi mj ij bi translated">除了更著名的(解决过度拟合)之外，我们还看到了一个有趣的正则化用例。我们也看到了数学证明，显示了正则化如何解决我们的目标。<strong class="lq ir">不过，我希望你小心谨慎。仅仅因为正规化找到了解决方案，并不意味着这是最好的方案。事实上，正则化将有助于比OLS执行得更好，但其他最大似然算法可以更好地解决你的问题！</strong></p><p id="8822" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated"><strong class="lq ir">此外，还有其他方法，比如降维，来解决特征多于数据点的问题</strong></p><p id="bffa" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">如果你喜欢这篇文章，这里有更多！</p><div class="nz oa gp gr ob oc"><a rel="noopener follow" target="_blank" href="/dealing-with-features-that-have-high-cardinality-1c9212d7ff1b"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd ir gy z fp oh fr fs oi fu fw ip bi translated">处理具有高基数的要素</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">一个简单的实用程序，我用来处理具有许多唯一值的分类特征</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">towardsdatascience.com</p></div></div><div class="ol l"><div class="om l on oo op ol oq kp oc"/></div></div></a></div><div class="nz oa gp gr ob oc"><a rel="noopener follow" target="_blank" href="/regex-essential-for-nlp-ee0336ef988d"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd ir gy z fp oh fr fs oi fu fw ip bi translated">正则表达式对NLP至关重要</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">理解各种正则表达式，并将其应用于自然语言中经常遇到的情况…</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">towardsdatascience.com</p></div></div><div class="ol l"><div class="or l on oo op ol oq kp oc"/></div></div></a></div><div class="nz oa gp gr ob oc"><a rel="noopener follow" target="_blank" href="/powerful-text-augmentation-using-nlpaug-5851099b4e97"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd ir gy z fp oh fr fs oi fu fw ip bi translated">使用NLPAUG的强大文本增强！</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">通过文本增强技术处理NLP分类问题中的类别不平衡</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">towardsdatascience.com</p></div></div><div class="ol l"><div class="os l on oo op ol oq kp oc"/></div></div></a></div><div class="nz oa gp gr ob oc"><a rel="noopener follow" target="_blank" href="/scatter-plots-on-maps-using-plotly-79f16aee17d0"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd ir gy z fp oh fr fs oi fu fw ip bi translated">使用Plotly在地图上散布图</h2><div class="oj l"><h3 class="bd b gy z fp oh fr fs oi fu fw dk translated">了解如何用很少的代码创建交互式散点图来表示数据中的多个要素</h3></div><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">towardsdatascience.com</p></div></div><div class="ol l"><div class="ot l on oo op ol oq kp oc"/></div></div></a></div><div class="nz oa gp gr ob oc"><a rel="noopener follow" target="_blank" href="/effortless-exploratory-data-analysis-eda-201c99324857"><div class="od ab fo"><div class="oe ab of cl cj og"><h2 class="bd ir gy z fp oh fr fs oi fu fw ip bi translated">轻松的探索性数据分析(EDA)</h2><div class="ok l"><p class="bd b dl z fp oh fr fs oi fu fw dk translated">towardsdatascience.com</p></div></div><div class="ol l"><div class="ou l on oo op ol oq kp oc"/></div></div></a></div><p id="0670" class="pw-post-body-paragraph lo lp iq lq b lr mk jr lt lu ml ju lw lx mm lz ma mb mn md me mf mo mh mi mj ij bi translated">查看我的<a class="ae kv" href="https://github.com/rajlm10" rel="noopener ugc nofollow" target="_blank"> <strong class="lq ir"> GitHub </strong> </a>其他一些项目。可以联系我<a class="ae kv" href="https://rajsangani.me/" rel="noopener ugc nofollow" target="_blank"> <strong class="lq ir"> <em class="ne">这里</em> </strong> </a> <strong class="lq ir"> <em class="ne">。</em> </strong>感谢您的配合！</p></div></div>    
</body>
</html>