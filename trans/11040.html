<html>
<head>
<title>Open-ended learning at DeepMind</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">DeepMind 的开放式学习</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/open-ended-learning-at-deepmind-da986c05f16f?source=collection_archive---------28-----------------------#2021-10-27">https://towardsdatascience.com/open-ended-learning-at-deepmind-da986c05f16f?source=collection_archive---------28-----------------------#2021-10-27</a></blockquote><div><div class="fc ig ih ii ij ik"/><div class="il im in io ip"><h2 id="9ba0" class="iq ir is bd b dl it iu iv iw ix iy dk iz translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/tds-podcast" rel="noopener" target="_blank">播客</a></h2><div class=""/><div class=""><h2 id="0740" class="pw-subtitle-paragraph jy jb is bd b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp dk translated">马克斯·贾德伯格关于构建能玩他们从未见过的游戏的代理</h2></div><figure class="kq kr ks kt gt ku"><div class="bz fp l di"><div class="kv kw l"/></div><p class="kx ky gj gh gi kz la bd b be z dk translated"><a class="ae lb" href="https://podcasts.apple.com/ca/podcast/towards-data-science/id1470952338?mt=2" rel="noopener ugc nofollow" target="_blank">苹果</a> | <a class="ae lb" href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9hbmNob3IuZm0vcy8zNmI0ODQ0L3BvZGNhc3QvcnNz" rel="noopener ugc nofollow" target="_blank">谷歌</a> | <a class="ae lb" href="https://open.spotify.com/show/63diy2DtpHzQfeNVxAPZgU" rel="noopener ugc nofollow" target="_blank"> SPOTIFY </a> | <a class="ae lb" href="https://anchor.fm/towardsdatascience" rel="noopener ugc nofollow" target="_blank">其他</a></p></figure><p id="245c" class="pw-post-body-paragraph lc ld is le b lf lg kc lh li lj kf lk ll lm ln lo lp lq lr ls lt lu lv lw lx il bi translated"><em class="ly">编者按:TDS 播客由 Jeremie Harris 主持，他是数据科学导师初创公司 SharpestMinds 的联合创始人。每周，Jeremie 都会与该领域前沿的研究人员和商业领袖聊天，以解开围绕数据科学、机器学习和人工智能的最紧迫问题。</em></p></div><div class="ab cl lz ma hw mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="il im in io ip"><p id="babf" class="pw-post-body-paragraph lc ld is le b lf lg kc lh li lj kf lk ll lm ln lo lp lq lr ls lt lu lv lw lx il bi translated">从表面上看，强化学习范式没有明显的限制:你把一个代理放在一个环境中，奖励它采取良好的行动，直到它掌握一项任务。</p><p id="f581" class="pw-post-body-paragraph lc ld is le b lf lg kc lh li lj kf lk ll lm ln lo lp lq lr ls lt lu lv lw lx il bi translated">而且到去年为止，RL 已经取得了一些惊人的成绩，包括精通围棋，各种雅达利游戏，星际争霸 2 等等。但是人工智能的圣杯不是掌握特定的游戏，而是一般化——让代理人能够在他们之前没有接受过训练的新游戏中表现良好。</p><p id="abd5" class="pw-post-body-paragraph lc ld is le b lf lg kc lh li lj kf lk ll lm ln lo lp lq lr ls lt lu lv lw lx il bi translated">然而，快进到今年 7 月，DeepMind 的一个团队发表了一篇名为“<a class="ae lb" href="https://arxiv.org/abs/2107.12808" rel="noopener ugc nofollow" target="_blank">开放式学习导致一般能力代理</a>”的论文，朝着一般 RL 代理的方向迈出了一大步。和我一起参加本期播客的是这篇论文的合著者之一，马克斯·贾德伯格。马克斯在 2014 年进入谷歌生态系统，当时他们<a class="ae lb" href="https://www.theguardian.com/technology/2014/oct/23/google-uk-artificial-intelligence-startups-machine-learning-dark-blue-labs-vision-factory" rel="noopener ugc nofollow" target="_blank">收购了他的计算机视觉公司</a>，最近，他成立了 DeepMind 的开放式学习团队，该团队专注于将机器学习进一步推向跨任务概括能力的领域。我和 Max 讨论了开放式学习、通用化的前进道路和人工智能的未来。</p><p id="958e" class="pw-post-body-paragraph lc ld is le b lf lg kc lh li lj kf lk ll lm ln lo lp lq lr ls lt lu lv lw lx il bi translated">以下是我在对话中最喜欢的一些观点:</p><ul class=""><li id="b2a3" class="mg mh is le b lf lg li lj ll mi lp mj lt mk lx ml mm mn mo bi translated">最近 DeepMind 关于开放式学习的论文中报告的最重要的进步之一是代理人玩的游戏的程序生成。通过自动生成大量不同的环境和游戏目标，智能体可以接受各种各样任务的训练，迫使他们开发元学习技术，如探索和试错法。</li><li id="ad00" class="mg mh is le b lf mp li mq ll mr lp ms lt mt lx ml mm mn mo bi translated">他们制作的游戏包括个人、合作和竞争场景。有趣的是，代理人似乎比合作行为更容易(也更快)学会竞争行为。马克斯认为这是因为竞争伴随着一个更渐进的学习曲线:代理人被训练与其他代理人竞争，这些代理人在过程的每个阶段都经历了与他们大致相同的训练量。随着他们技能的提高，他们在游戏中的对手也在提高，所以他们总是为竞争动力学的“下一课”做好准备。</li><li id="d946" class="mg mh is le b lf mp li mq ll mr lp ms lt mt lx ml mm mn mo bi translated">在开放式强化学习的最新进展和缩放语言模型的进步之间有许多有趣的相似之处。在这两种情况下，研究人员感兴趣的是确定可以可靠测量的概括能力的代理。例如，在 OpenAI 的 GPT-3 的情况下，这些代理之一是算法的算术能力:GPT-3 在增加小数字方面做得很好，但在增加大数字方面失败了——一些人认为这表明 GPT-3 还没有“学会理解”加法的概念。强化学习中加法的类比可能只是博弈论:一个强化学习代理人如何以及如何持续地将博弈论的原理应用到它所面临的问题和环境中？</li><li id="da23" class="mg mh is le b lf mp li mq ll mr lp ms lt mt lx ml mm mn mo bi translated">也就是说，Max 还观察到，像 GPT-3 和开放式 RL 学习者这样的模型确实显示出对算术和博弈论的不完美掌握，这一事实可能只是反映了他们倾向于学习启发式而不是严格的符号逻辑。这可能是一件好事:试探法比逻辑更健壮，因为它们可以随着应用它们的问题领域变得更复杂而进化。启发式也是人类用来学习事物的方法，这就是为什么我们显示了许多与人工智能模型相同的失败模式:大多数人很难用大数做算术，但我们不认为这表明他们不能(或不理解)数学。也许我们应该以同样的方式思考人工智能。</li></ul><p id="9e7e" class="pw-post-body-paragraph lc ld is le b lf lg kc lh li lj kf lk ll lm ln lo lp lq lr ls lt lu lv lw lx il bi translated">你可以<a class="ae lb" href="https://twitter.com/maxjaderberg" rel="noopener ugc nofollow" target="_blank">点击这里</a>在 Twitter 上关注 Max。</p></div><div class="ab cl lz ma hw mb" role="separator"><span class="mc bw bk md me mf"/><span class="mc bw bk md me mf"/><span class="mc bw bk md me"/></div><div class="il im in io ip"><figure class="kq kr ks kt gt ku gh gi paragraph-image"><div role="button" tabindex="0" class="mv mw di mx bf my"><div class="gh gi mu"><img src="../Images/5c6526c44c2c1959de830686efd3ea3b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Fx_A3nIDZyUjx4S0CcJwNw.png"/></div></div></figure><h2 id="75d3" class="nb nc is bd nd ne nf dn ng nh ni dp nj ll nk nl nm lp nn no np lt nq nr ns iy bi translated">章节:</h2><ul class=""><li id="1c71" class="mg mh is le b lf nt li nu ll nv lp nw lt nx lx ml mm mn mo bi translated">0:00 介绍</li><li id="760e" class="mg mh is le b lf mp li mq ll mr lp ms lt mt lx ml mm mn mo bi translated">1:30 Max 的背景</li><li id="055d" class="mg mh is le b lf mp li mq ll mr lp ms lt mt lx ml mm mn mo bi translated">6:40 程序代的差异</li><li id="53be" class="mg mh is le b lf mp li mq ll mr lp ms lt mt lx ml mm mn mo bi translated">12:20 定性方面</li><li id="99bb" class="mg mh is le b lf mp li mq ll mr lp ms lt mt lx ml mm mn mo bi translated">17:40 代理人的错误</li><li id="91be" class="mg mh is le b lf mp li mq ll mr lp ms lt mt lx ml mm mn mo bi translated">20:00 测量概括</li><li id="5cb9" class="mg mh is le b lf mp li mq ll mr lp ms lt mt lx ml mm mn mo bi translated">27:10 环境和损失函数</li><li id="61d7" class="mg mh is le b lf mp li mq ll mr lp ms lt mt lx ml mm mn mo bi translated">32:50 符号逻辑的潜力</li><li id="4866" class="mg mh is le b lf mp li mq ll mr lp ms lt mt lx ml mm mn mo bi translated">36:45 两个不同的学习过程</li><li id="9b71" class="mg mh is le b lf mp li mq ll mr lp ms lt mt lx ml mm mn mo bi translated">42:35 预测研究</li><li id="f1f9" class="mg mh is le b lf mp li mq ll mr lp ms lt mt lx ml mm mn mo bi translated">45:00 总结</li></ul></div></div>    
</body>
</html>