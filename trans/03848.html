<html>
<head>
<title>Feedback Transformers from Facebook AI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">脸书人工智能反馈变压器</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/feedback-transformers-from-facebook-ai-221c5dd09e3f?source=collection_archive---------34-----------------------#2021-03-30">https://towardsdatascience.com/feedback-transformers-from-facebook-ai-221c5dd09e3f?source=collection_archive---------34-----------------------#2021-03-30</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="de1d" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">解决具有反馈记忆的变压器的一些限制</h2></div><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi kf"><img src="../Images/6762536ec6fb7a5e2b9d2cd677b1eb53.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*7fvgBqRKPM0S_O653DPoiA.jpeg"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">保罗·库科在<a class="ae kv" href="/s/photos/backe%3Dward?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上的照片</p></figure><p id="3054" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi ls translated"><span class="l lt lu lv bm lw lx ly lz ma di">在</span>自然语言处理时代，Transformer架构的增强已经成为革命的核心，突破正以前所未有的速度发生。本文展示了反馈变换器的概念，它克服了传统变换器架构以及递归神经网络的缺点，使较浅的模型具有更快的解码速度、更少的内存以及所需的计算，最重要的是，可以利用来自所有先前层的所有计算信息，而不像仅解码变换器那样为了并行训练而牺牲很多信息。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="a0c9" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">在进入反馈存储器的杂草之前，让我们修改变压器架构！</h1><p id="1eda" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">自然语言处理中的转换器架构旨在解决序列到序列的任务，同时轻松处理长范围依赖。这个模型完全依靠<em class="nf">对</em> <a class="ae kv" rel="noopener" target="_blank" href="/illustrated-self-attention-2d627e33b20a"> <em class="nf">自我关注</em> </a> <em class="nf">来计算其输入和输出的表示，而不使用序列对齐的RNNs或卷积。</em></p><p id="f8f5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="nf">让我们在此一窥其架构，更深层次地钻研其推荐访问论文</em> </strong> <a class="ae kv" href="https://arxiv.org/abs/1706.03762" rel="noopener ugc nofollow" target="_blank"> <strong class="ky ir"> <em class="nf">关注你所需要的</em> </strong> </a> <strong class="ky ir"> <em class="nf">！</em> </strong></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi ng"><img src="../Images/9d1a0e6928ac6457023c51f05803fa1a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-7Ti_eti184rMy6Ex7qAoQ.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图1:变压器架构综合视图</p></figure></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="f532" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">RNNs和变压器架构的局限性</h1><p id="5c21" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">在RNN和变压器架构的情况下，不是所有计算的信息都被使用，尽管在技术上，人们可以利用它，但是为了并行训练，做出了牺牲。然而，新颖的反馈变换器架构使用具有相对更强性能的浅层模型来揭示从所有先前表示到所有未来表示的信息。</p><h2 id="b36a" class="nh mj iq bd mk ni nj dn mo nk nl dp ms lf nm nn mu lj no np mw ln nq nr my ns bi translated">递归神经网络及其局限性</h2><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nt"><img src="../Images/6b93b6bf95af98baa2650e8d47e6af6b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YbhG1LWp-LOaex1ByN4j0w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图2:对RNN建筑及其缺点的直观研究</p></figure><p id="bf28" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">在RNN的情况下，所有信息仅在顶部和右侧方向流动，因此有大量信息丢失。此外，为了与其他信息相结合，这些信息通常需要经过多次计算。</p><h2 id="c58c" class="nh mj iq bd mk ni nj dn mo nk nl dp ms lf nm nn mu lj no np mw ln nq nr my ns bi translated">变压器及其局限性</h2><p id="69b3" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">Transformer体系结构有许多局限性，例如它不能有效跟踪长序列以及处理分层输入。这种架构的一些最重要的限制是:</p><p id="5cad" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="nf"> (A)对更高级别陈述的有限访问</em></p><p id="0d1f" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><em class="nf"> (B)如果频繁更新，变压器无法长时间保持内部状态</em></p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nu"><img src="../Images/96ad5bdfe81a74a76ded55dcca0176e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*pwnyJb3VHZpGK4m_-ETusA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图3:变压器及其缺点的直观研究</p></figure></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="a10d" class="mi mj iq bd mk ml mm mn mo mp mq mr ms jw mt jx mu jz mv ka mw kc mx kd my mz bi translated">引入带反馈存储器的变压器</h1><p id="9182" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">为了克服变压器结构的局限性，引入了反馈记忆的概念。正如我们可以在图3中分析的，每个隐藏的表示都有许多箭头(即连接),这种数量的注意力连接可以引爆任何系统。为了克服这一点，反馈记忆将特定时间步长的所有信息混合到一个记忆表示中。随后的层不再关注先前层的单独表示，现在只关注单一的记忆表示。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nv"><img src="../Images/dd73d526e25505a62646bb090b12e5f4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*xGhnIzjAB5Jizeuw_izq0w.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图4:(左)反馈转换器将所有层的隐藏表示合并成一个向量，并存储在内存中。(右)反馈和变压器的区别。t表示时间步长，l表示层。</p></figure><p id="4a11" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">将信息存储到相应的存储器表示中的显著好处在于使用了总体更少的存储器以及更少的计算，因为这种方法共享了注意机制的密钥和值计算，从而由于存储器共享而进一步导致GPU存储器使用的减少。</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div role="button" tabindex="0" class="kl km di kn bf ko"><div class="gh gi nw"><img src="../Images/b6756f10037767b67403540247572856.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*zX2hqryVHcg7mmCgYLiETA.png"/></div></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图5:解释变压器和反馈变压器中下一层表示的计算差异</p></figure><h1 id="a3a1" class="mi mj iq bd mk ml nx mn mo mp ny mr ms jw nz jx mu jz oa ka mw kc ob kd my mz bi translated">反馈变压器架构的优势</h1><p id="b3e5" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">以下是将反馈存储器引入变压器架构对性能提升的主要贡献:</p><ol class=""><li id="4f71" class="oc od iq ky b kz la lc ld lf oe lj of ln og lr oh oi oj ok bi translated"><strong class="ky ir"> <em class="nf">能够用小而浅的模型实现更好的性能</em> </strong></li></ol><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/6b1168874a286401342cec7e332951b5.png" data-original-src="https://miro.medium.com/v2/resize:fit:734/format:webp/1*fSlvMdHQxfJZp1l5e39G_w.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图6:wmt 14 En-De上的机器翻译，测试集BLEU和不同解码器深度的解码速度(每秒字数)。</p></figure><p id="1c6e" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">根据图6的检查结果，可以得出结论，随着解码速度的提高，由于较浅的网络，即架构中的层数较少，与反馈变压器相比，变压器的性能下降更多。</p><p id="97fa" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><strong class="ky ir"> <em class="nf"> 2。与变形金刚</em> </strong>相比，反馈变形金刚收敛以在强化学习中达到更高的平均回报</p><figure class="kg kh ki kj gt kk gh gi paragraph-image"><div class="gh gi om"><img src="../Images/75b1062415da642e8b650e1979ebbc1c.png" data-original-src="https://miro.medium.com/v2/resize:fit:732/format:webp/1*3SemaW-tTbIOrX6PhHxX0A.png"/></div><p class="kr ks gj gh gi kt ku bd b be z dk translated">图Gridworld中的迷宫导航。显示反馈变压器与标准变压器的平均奖励比较。</p></figure><p id="96f2" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">强化学习任务通常需要长时间的记忆来最佳地解决任务。从图7可以诊断出，与变压器架构相比，反馈变压器收敛以在任何训练步骤达到更高的回报。</p><h1 id="7564" class="mi mj iq bd mk ml nx mn mo mp ny mr ms jw nz jx mu jz oa ka mw kc ob kd my mz bi translated">结论</h1><p id="af9f" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">尽管在训练和推理时间期间使用了相当少的存储器，但是与具有更小和更浅模型的相同大小的变换器架构相比，反馈变换器以更快的解码速度实现了更强的性能。保留远程信息流以及立即访问可用的更高表示信息是具有反馈记忆的变压器的一些主要贡献。</p><h1 id="d6c5" class="mi mj iq bd mk ml nx mn mo mp ny mr ms jw nz jx mu jz oa ka mw kc ob kd my mz bi translated">参考资料:</h1><p id="9590" class="pw-post-body-paragraph kw kx iq ky b kz na jr lb lc nb ju le lf nc lh li lj nd ll lm ln ne lp lq lr ij bi translated">[1]范，安吉拉，等，“解决变压器的一些限制与反馈记忆。”arXiv预印本arXiv:2002.09402  (2020)。</p><p id="3306" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated">[2]瓦斯瓦尼、阿希什等人，“你所需要的只是关注。”<em class="nf"> arXiv预印本arXiv:1706.03762 </em> (2017)。</p><p id="07b5" class="pw-post-body-paragraph kw kx iq ky b kz la jr lb lc ld ju le lf lg lh li lj lk ll lm ln lo lp lq lr ij bi translated"><a class="ae kv" href="https://www.youtube.com/watch?v=zdb8MM94A5c" rel="noopener ugc nofollow" target="_blank">【3】扬尼克·基尔彻的《反馈变压器》</a>。</p><figure class="kg kh ki kj gt kk"><div class="bz fp l di"><div class="on oo l"/></div></figure><blockquote class="op oq or"><p id="60e8" class="kw kx nf ky b kz la jr lb lc ld ju le os lg lh li ot lk ll lm ou lo lp lq lr ij bi translated">希望你能像我写这篇文章一样喜欢阅读这篇文章！</p><p id="6091" class="kw kx nf ky b kz la jr lb lc ld ju le os lg lh li ot lk ll lm ou lo lp lq lr ij bi translated"><em class="iq">如果你觉得这些内容有意义，可以看看我的其他文章</em> <a class="ae kv" href="https://medium.com/@dhwanidm1996" rel="noopener"> <em class="iq">这里</em> </a> <em class="iq">。</em></p></blockquote></div></div>    
</body>
</html>