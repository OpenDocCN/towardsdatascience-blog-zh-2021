<html>
<head>
<title>AI Computed Breast Cancer Risk Map</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">人工智能计算乳腺癌风险图</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/ai-computed-breast-cancer-risk-map-b29195b477a?source=collection_archive---------19-----------------------#2021-04-10">https://towardsdatascience.com/ai-computed-breast-cancer-risk-map-b29195b477a?source=collection_archive---------19-----------------------#2021-04-10</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="a3b5" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/hands-on-tutorials" rel="noopener" target="_blank">实践教程</a></h2><div class=""/><div class=""><h2 id="6081" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">结合Boosting和支持向量机的层次聚类可视化可靠乳腺癌诊断模型</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/879291910f486f5d9197d97f0416c59c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mNOrAhYO2N9-DyMW2Kq3HQ.png"/></div></div></figure><h1 id="3314" class="la lb iq bd lc ld le lf lg lh li lj lk kf ll kg lm ki ln kj lo kl lp km lq lr bi translated">一.导言</h1><p id="ecca" class="pw-post-body-paragraph ls lt iq lu b lv lw ka lx ly lz kd ma mb mc md me mf mg mh mi mj mk ml mm mn ij bi translated"><strong class="lu ja">1.1工作目标</strong></p><p id="bb5a" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">快速可靠的乳腺癌检测是一项重要的挑战，因为它代表了重大的公共卫生问题(2018年全球约有200万例新病例被检测到[1])。在疾病的早期阶段识别恶性乳腺癌会显著增加患者的生存机会，并减少治疗的副作用。</p><p id="631b" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">诊断依赖于从可疑肿瘤区域提取的样本分析。该过程旨在评估给定样本是否由代表不受控制的增殖风险的细胞构成。能够在一个时间内分析更多的样本可能会有机会诊断更多的患者，并在需要采取治疗措施时更快地做出反应。出于这个目的，人工智能算法可能会带来很多好处。在需要分析数千甚至数百万个样本的情况下，它们可以用来进行初步选择，并建议哪些样本需要优先进行精确的专家研究。</p><p id="545c" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">为此，已经提出了一些机器学习模型(见[2]、[3])来预测恶性或良性肿瘤的发展风险。然而，这些模型是基于从肿瘤细胞成像推导出的许多变量。在这项工作中，我们建议将变量的数量减少到两个。这种维度缩减的目的是形成可视的恶性肿瘤风险图。我们在这项工作中提出:</p><ul class=""><li id="7fcc" class="mt mu iq lu b lv mo ly mp mb mv mf mw mj mx mn my mz na nb bi translated">1.精确分析每个初始变量在预测中的作用。为此，我们采用了一种使用分层聚类(HC)算法的特征聚类方法。第二次，使用梯度推进树分类器(GBTC)算法评估特征重要性。</li><li id="9577" class="mt mu iq lu b lv nc ly nd mb ne mf nf mj ng mn my mz na nb bi translated">2.选择在前一步中推导出的两个最有意义的变量。</li><li id="ed18" class="mt mu iq lu b lv nc ly nd mb ne mf nf mj ng mn my mz na nb bi translated">3.构建支持向量机分类器(SVMC)算法以导出恶性乳腺癌发展的概率风险图。</li></ul><p id="649d" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated"><strong class="lu ja"> I.2数据导入</strong></p><p id="b051" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">我们从几个导入开始。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="ae47" class="nm lb iq ni b gy nn no l np nq">from matplotlib import cm<br/>import seaborn as sn<br/>import matplotlib as matplotlib<br/>import pandas as pd<br/>import pylab as plt<br/>import numpy as np<br/>from scipy.cluster import hierarch</span><span id="c9b4" class="nm lb iq ni b gy nr no l np nq"># Sklearn imports<br/>from sklearn.datasets import load_breast_cancer<br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.ensemble import GradientBoostingClassifier<br/>from sklearn.metrics import accuracy_score<br/>from sklearn.model_selection import cross_validate,GridSearchCV<br/>from sklearn.svm import SVC<br/>from sklearn.gaussian_process import GaussianProcessClassifier<br/>from sklearn.gaussian_process.kernels import RBF</span></pre><p id="81ca" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">在这项工作中，我们使用UCI机器学习数据库[3]收集569名患者的数据。当分析肿瘤细胞时，为每个细胞核计算十种特征类型[3]:</p><ul class=""><li id="fca1" class="mt mu iq lu b lv mo ly mp mb mv mf mw mj mx mn my mz na nb bi translated">1)半径(从中心到周边各点的平均距离)</li><li id="4d7f" class="mt mu iq lu b lv nc ly nd mb ne mf nf mj ng mn my mz na nb bi translated">2)纹理(灰度值的标准偏差)</li><li id="8bf2" class="mt mu iq lu b lv nc ly nd mb ne mf nf mj ng mn my mz na nb bi translated">3)周长</li><li id="3fa0" class="mt mu iq lu b lv nc ly nd mb ne mf nf mj ng mn my mz na nb bi translated">4)面积</li><li id="b605" class="mt mu iq lu b lv nc ly nd mb ne mf nf mj ng mn my mz na nb bi translated">5)平滑度(半径长度的局部变化)</li><li id="d97b" class="mt mu iq lu b lv nc ly nd mb ne mf nf mj ng mn my mz na nb bi translated">6)密实度(𝑝𝑒𝑟𝑖𝑚𝑒𝑡𝑒𝑟 /面积— 1.0)</li><li id="5513" class="mt mu iq lu b lv nc ly nd mb ne mf nf mj ng mn my mz na nb bi translated">7)凹度(轮廓凹入部分的严重程度)</li><li id="048b" class="mt mu iq lu b lv nc ly nd mb ne mf nf mj ng mn my mz na nb bi translated">8)凹点(轮廓的凹入部分的数量)</li><li id="3e23" class="mt mu iq lu b lv nc ly nd mb ne mf nf mj ng mn my mz na nb bi translated">9)对称性</li><li id="d10d" class="mt mu iq lu b lv nc ly nd mb ne mf nf mj ng mn my mz na nb bi translated">10)分形维数(“海岸线近似值”-1)</li></ul><p id="738b" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">对于这些特征中的每一个，报告平均值、标准偏差和最大值，这导致总共30个特征。分析这些特征中的每一个导致决定肿瘤是恶性的(1)还是良性的(0)。预测将是我们的标签。</p><p id="ed7e" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">首先，我们下载数据并验证不存在“none”值。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="ce30" class="nm lb iq ni b gy nn no l np nq"># Load the data<br/>feat_all,label_all=load_breast_cancer(return_X_y=True,as_frame=True)</span><span id="ab72" class="nm lb iq ni b gy nr no l np nq">feat_all.isnull().sum()</span><span id="0af6" class="nm lb iq ni b gy nr no l np nq">mean radius                0<br/>mean texture               0<br/>mean perimeter             0<br/>mean area                  0<br/>mean smoothness            0<br/>mean compactness           0<br/>mean concavity             0<br/>mean concave points        0<br/>mean symmetry              0<br/>mean fractal dimension     0<br/>radius error               0<br/>texture error              0<br/>perimeter error            0<br/>area error                 0<br/>smoothness error           0<br/>compactness error          0<br/>concavity error            0<br/>concave points error       0<br/>symmetry error             0<br/>fractal dimension error    0<br/>worst radius               0<br/>worst texture              0<br/>worst perimeter            0<br/>worst area                 0<br/>worst smoothness           0<br/>worst compactness          0<br/>worst concavity            0<br/>worst concave points       0<br/>worst symmetry             0<br/>worst fractal dimension    0<br/>dtype: int64</span></pre><p id="52f6" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">现在是时候开始特征选择工作，将我们的问题从30维空间简化到2维空间。</p><h1 id="5f47" class="la lb iq bd lc ld le lf lg lh li lj lk kf ll kg lm ki ln kj lo kl lp km lq lr bi translated">一.功能选择</h1><p id="a151" class="pw-post-body-paragraph ls lt iq lu b lv lw ka lx ly lz kd ma mb mc md me mf mg mh mi mj mk ml mm mn ij bi translated">在这一部分中，我们首先计算成对相关性，以便重新组合彼此之间最相关的特征。使用层次聚类(HC)算法收集相关特征组。第二次，使用梯度推进树分类器(GBTC)将这种聚类方法与特征重要性评估相结合。</p><p id="c941" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated"><strong class="lu ja"> I.1特征相关性分析</strong></p><p id="d320" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">如上所述，我们计算特征之间的成对相关性:</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="4163" class="nm lb iq ni b gy nn no l np nq">correl_fig,ax=plt.subplots(1,1,figsize=(10,10),)<br/>indexax=np.arange(0,len(feat_all.columns))<br/>sn.heatmap(feat_all.corr())<br/>ax.set_xticks(indexax)<br/>ax.set_xticklabels(feat_all.columns)<br/>ax.set_yticks(indexax)<br/>ax.set_yticklabels(feat_all.columns)<br/>correl_fig.tight_layout()<br/>plt.show()</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/4512cf0a646b9a26b24207b0d6978627.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*0O81FfpKl2Dd9v-WJNS6KA.png"/></div></div><p class="ns nt gj gh gi nu nv bd b be z dk translated">图1:所有30个特征的成对相关图。</p></figure><p id="663b" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">特征选择的第一步是构建相关特征组(注意，使用Spearman相关性可以遵循类似的方法[5])。事实上，当特征高度相关时，意味着它们传达了相近或相似的信息。结果，该组的单个特征而不是所有特征可以被考虑用于预测，从而避免冗余。</p><p id="fff8" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">我们通过使用层次聚类(HC)来构建相关特征的组(更多细节参见[6])。因此，应用了以下过程，该过程能够构建从树叶到根的树(称为HC树，其中每个节点代表一个特征集群):</p><ul class=""><li id="8291" class="mt mu iq lu b lv mo ly mp mb mv mf mw mj mx mn my mz na nb bi translated">1.首先将每个特征定义为一个簇(HC树的第0层，每个特征是一个叶子)。</li><li id="e8d0" class="mt mu iq lu b lv nc ly nd mb ne mf nf mj ng mn my mz na nb bi translated">2.计算每个聚类之间的距离。如果𝐹k是聚类I中的特征，𝐺𝑡是聚类2中的特征，则聚类I和j之间的距离是𝑚𝑖𝑛_𝑘,𝑡(𝐶𝑜𝑟𝑟(𝐹𝑘,𝐺𝑡)).其中𝐶𝑜𝑟𝑟(𝐹𝑘,𝐺𝑡)是特征f和g之间的相关性</li><li id="3eb9" class="mt mu iq lu b lv nc ly nd mb ne mf nf mj ng mn my mz na nb bi translated">3.对于给定的簇I，将其与最接近的一个(记为j)合并。这种合并由两个合并分支表示，一个来自与I相关联的节点，另一个来自与j相关联的节点。这些分支形成了表示I和j之间的合并的树中的上层节点。</li><li id="aa1e" class="mt mu iq lu b lv nc ly nd mb ne mf nf mj ng mn my mz na nb bi translated">4.重复这些聚类合并操作，直到有一个集合了所有特征(对应于HC树的根)。</li></ul><p id="4165" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">在固定一个级别后，该级别的不同节点产生特征的聚类。更准确地说，如果考虑与一个集群相关联给定节点。与之相连的叶子代表了这个集群中涉及的特征。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="56b8" class="nm lb iq ni b gy nn no l np nq">corr_fig1,ax1=plt.subplots(1,1,figsize=(10, 8),)</span><span id="648d" class="nm lb iq ni b gy nr no l np nq"># Compute pair wise correlations<br/>corr=feat_all.corr().values</span><span id="b85f" class="nm lb iq ni b gy nr no l np nq"># Compute the hierarchical clustering tree<br/>link=hierarchy.ward(corr)<br/>dendro=hierarchy.dendrogram(link,labels=feat_all.columns,ax=ax1,leaf_rotation=90,leaf_font_size=10)<br/>dendro_index=np.arange(0,len(dendro["ivl"]))<br/>corr_fig1.tight_layout()<br/>plt.show()<br/>corr_fig1.savefig("corrtree.png")</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/9121878de034c2c0ad721def7bbe0fcb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*OhZRAoHeoQPlvkm5q47MWQ.png"/></div></div><p class="ns nt gj gh gi nu nv bd b be z dk translated">图2:由HC算法计算出的树。</p></figure><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="566e" class="nm lb iq ni b gy nn no l np nq">corr_fig2,ax2=plt.subplots(1,1,figsize=(10, 8),)</span><span id="68cf" class="nm lb iq ni b gy nr no l np nq"># Compute the correlation heat map<br/>im=ax2.imshow(corr[dendro["leaves"],:][:,dendro["leaves"]])<br/>ax2.set_xticks(dendro_index)<br/>ax2.set_yticks(dendro_index)<br/>ax2.set_xticklabels(np.array(dendro["ivl"]),rotation="vertical")<br/>ax2.set_yticklabels(np.array(dendro["ivl"]))<br/>corr_fig2.tight_layout()<br/>plt.colorbar(im)<br/>plt.show()<br/>corr_fig2.savefig("corrmap.png")</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/9cd19534525061efefc1ced720090761.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*JiGu-fBzROQZzzfDOX_Smg.png"/></div></div><p class="ns nt gj gh gi nu nv bd b be z dk translated">图3:HC聚类中重排特征的相关热图。</p></figure><p id="86ed" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">图2显示了根据HC方法构建的树。通过固定等级4，看起来可以定义四个不同的特征聚类(参见图2，HC树的叶子从左到右:聚类1，从“平均对称”到“分形维数误差”；聚类2，从“平均纹理”到“对称误差”；聚类3，从“平均凹点”到“平均凹度”；聚类4，从“平均周长”到“周长误差”)。</p><p id="e2d7" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">对于我们的降维问题，因此需要选择不属于同一聚类的特征。图3示出了热图相关矩阵，其中重新排列的特征取决于它们的聚类(即遵循图2中HC树给出的顺序)。</p><p id="fbeb" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">我们确定每个特征的聚类，并将信息存储在字典中。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="bda8" class="nm lb iq ni b gy nn no l np nq"># Color list of the four feature clusters<br/>color_list=[“red”,”navy”,”black”,”green”]</span><span id="0da9" class="nm lb iq ni b gy nr no l np nq"># Fix a level of four in the HC tree to determine feature clusters<br/>clusterlevel=4 </span><span id="4804" class="nm lb iq ni b gy nr no l np nq"># Determine the id cluster list<br/>clusterid_list= hierarchy.fcluster(link,clusterlevel, \criterion=’distance’)</span><span id="7cf6" class="nm lb iq ni b gy nr no l np nq"># This dictionary will contain the list of features for each <br/># cluster<br/>featurecluster_dict = dict()</span><span id="165b" class="nm lb iq ni b gy nr no l np nq">for idx, clusterid in enumerate(clusterid_list):<br/> if clusterid not in featurecluster_dict.keys():<br/>     featurecluster_dict[clusterid]=dict()<br/>     featurecluster_dict[clusterid][“numfeat”]=[]<br/>     featurecluster_dict[clusterid][“namefeat”]=[]<br/> featurecluster_dict[clusterid][“color”]=color_list[clusterid-1]<br/> featurecluster_dict[clusterid][“numfeat”].append(idx)<br/> featurecluster_dict[clusterid]\<br/> [“namefeat”].append(feat_all.columns[idx])</span></pre><h2 id="7c37" class="nm lb iq bd lc nw nx dn lg ny nz dp lk mb oa ob lm mf oc od lo mj oe of lq iw bi translated">I.2数据预处理</h2><p id="59e4" class="pw-post-body-paragraph ls lt iq lu b lv lw ka lx ly lz kd ma mb mc md me mf mg mh mi mj mk ml mm mn ij bi translated">现在，我们对数据进行预处理，以:</p><ul class=""><li id="3a48" class="mt mu iq lu b lv mo ly mp mb mv mf mw mj mx mn my mz na nb bi translated">1.为未来的机器学习模型规范化特征。这一标准化过程是通过减去平均值并除以每个特征的标准偏差来实现的。</li><li id="163f" class="mt mu iq lu b lv nc ly nd mb ne mf nf mj ng mn my mz na nb bi translated">2.在定型集和测试集之间拆分数据。</li></ul><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="5411" class="nm lb iq ni b gy nn no l np nq">def preprocess(trainrate=1.0):<br/>    '''<br/>    Load data, normalize and split between training and testing<br/>    sets</span><span id="fd4a" class="nm lb iq ni b gy nr no l np nq">    Input:<br/>      trainrate: [Float] Relative size of the training set</span><span id="ee97" class="nm lb iq ni b gy nr no l np nq">    Output:<br/>      feat_all:    [DataFrame] All the data features<br/>      label_all:   [DataFrame] All the labels<br/>      featnames:   [List] All the feature names<br/>      featN_all:   [DataFrame] All the normalized features<br/>      feat_train:  [DataFrame] Training features<br/>      featN_train: [DataFrame] Normalized training features<br/>      feat_test:   [DataFrame] Testing features<br/>      featN_test: [DataFrame] Normalized testing features<br/>      label_train: [DataFrame] Training labels<br/>      label_test:  [DataFrame] Testing labels<br/>      normmean_arr:[Array] All the features' means for<br/>                           normalization<br/>      normstd_arr: [Array] All features' standard <br/>                           deviation for normalization</span><span id="60a5" class="nm lb iq ni b gy nr no l np nq">    '''<br/>    # Load the data and get the name of all features                      </span><span id="2edf" class="nm lb iq ni b gy nr no l np nq">    feat_all,label_all=\<br/>    load_breast_cancer(return_X_y=True,as_frame=True)<br/>    featnames=np.array(feat_all.columns)</span><span id="61dd" class="nm lb iq ni b gy nr no l np nq">    # Shuffle the data<br/>    data_all=pd.concat([feat_all,label_all],axis=1).sample(frac=1)    <br/>    label_all=data_all["target"]<br/>    feat_all=data_all.drop("target",axis=1)</span><span id="a481" class="nm lb iq ni b gy nr no l np nq">    # Get normalized features    <br/>    StdSc=StandardScaler()<br/>    StdSc.fit(feat_all)<br/>    featN_all=StdSc.transform(feat_all)<br/>    featN_all=pd.DataFrame(featN_all,columns=feat_all.columns)<br/>    <br/>    # Split between training and testing sets<br/>    trainsize=int(trainrate*len(feat_all.index))<br/>    feat_train=feat_all[:trainsize]<br/>    featN_train=featN_all[:trainsize]<br/>    label_train=label_all[:trainsize]<br/>    feat_test=feat_all[trainsize:]<br/>    featN_test=featN_all[trainsize:]<br/>    label_test=label_all[trainsize:]</span><span id="87fd" class="nm lb iq ni b gy nr no l np nq">    normmean_arr=StdSc.mean_<br/>    normstd_arr=(StdSc.var_)**0.5</span><span id="a280" class="nm lb iq ni b gy nr no l np nq">    return\ <br/>    feat_all,label_all,featnames,featN_all,feat_train,\<br/>    featN_train,feat_test,featN_test,label_train,label_test,\<br/>    normmean_arr,normstd_arr</span><span id="df6d" class="nm lb iq ni b gy nr no l np nq">trainrate=0.8<br/>feat_all,label_all,featnames,featN_all,feat_train,featN_train,feat_test,featN_test,label_train,\<br/>label_test,mean_feat,std_feat=preprocess(trainrate=trainrate)</span></pre><p id="e86c" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">随机选择代表所有数据的20%的测试集，其余的用于训练。</p><h1 id="790e" class="la lb iq bd lc ld le lf lg lh li lj lk kf ll kg lm ki ln kj lo kl lp km lq lr bi translated">I.3确定特性的重要性</h1><p id="db63" class="pw-post-body-paragraph ls lt iq lu b lv lw ka lx ly lz kd ma mb mc md me mf mg mh mi mj mk ml mm mn ij bi translated">我们现在通过评估特征重要性分数来结合这种特征聚类方法。为此，我们建立了一个梯度推进树分类器(GBTC)算法。为了解释什么是GBTC，我们需要首先澄清什么是决策树。决策树是一种机器学习算法，通常用于回归和决策任务。从由几个特征表征的数据集合中，目的是为每个数据得出决策，如果是回归问题，该决策可以是数字，或者在分类的情况下是离散标签。为了获得决策，通过递归地构造节点来构建树，其中在每个节点上应用这些条件来分割数据(参见图4)。这些条件是固定的，以便最大化一个标准，如这里的基尼分类标准。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi og"><img src="../Images/a0c23ddf47f0ec92dfa6d6c7de668d35.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y76I9g6iGm1lCKDYO67agg.png"/></div></div><p class="ns nt gj gh gi nu nv bd b be z dk translated">图4:决策树的示意图，当形成决策节点直到到达叶子时，训练数据被递归分裂。</p></figure><p id="e132" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">在每个节点，选择一个特征或特征集合，并且对于它们中的每一个，阈值是固定的(在训练期间使用所考虑的标准来确定所选择的特征集合以及阈值)。根据定义的阈值，数据在不同的节点之间拆分。重复该分裂过程，直到到达具有最小数量数据的节点，或者如果树达到极限深度(这些极限被预先确定为超参数)。这些最终节点被称为叶子，并且每个都与一个决策值相关联。</p><p id="1e52" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">GBTC算法是由一组决策树组成的。对于成对的训练特征向量和标签(，yi)，构建该集合使得对于每个，</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/d0124aa4954604a6e0295b5796f85f17.png" data-original-src="https://miro.medium.com/v2/resize:fit:982/format:webp/1*e7EgjpsCFHK51I_jOLnA2Q.png"/></div></figure><p id="f5e5" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">一定是离易最近的。如果引入损失函数</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/3749cfa45f99ffc6dba84614ef7530e4.png" data-original-src="https://miro.medium.com/v2/resize:fit:628/format:webp/1*QxfMjhNZv4p8_SG0iquL5g.png"/></div></figure><p id="f626" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">构建p+1决策树对应于找到T函数，使得T(xi)=-Gi。其中Gi是损失函数在函数Hp上的xi梯度。换句话说</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/270fa6b253ef464e59e275a8057d7b47.png" data-original-src="https://miro.medium.com/v2/resize:fit:986/format:webp/1*zVSbjJcqKbvT9-6WX1DXjw.png"/></div></figure><p id="3c08" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">当我们在这里将该模型用于分类任务时，通过将sigmoid函数应用于构建的GTBC函数来获得预测的类别。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/9edf1303e8d349feabe0b7d050492aa0.png" data-original-src="https://miro.medium.com/v2/resize:fit:798/format:webp/1*bZbeDhP2BHSH2uKk2xFVbQ.png"/></div></figure><p id="ae3f" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">关于梯度增强的更多细节在[4]中提供。GBTC对于回归和分类都是有用的，并且可以被认为是确定特征的重要性。对于给定的特征，其在不同决策树中的平均水平位置被用于评估其在标签预测中的重要性。最重要的特征是那些位于决策树构造之初的特征。它们实际上对应于对决策树构造的数据分割具有最强影响的那些。下面我们介绍一个功能表:</p><ul class=""><li id="59f9" class="mt mu iq lu b lv mo ly mp mb mv mf mw mj mx mn my mz na nb bi translated">来确定决策树集合的最佳大小。</li><li id="3118" class="mt mu iq lu b lv nc ly nd mb ne mf nf mj ng mn my mz na nb bi translated">来训练GBTC算法。</li><li id="42e0" class="mt mu iq lu b lv nc ly nd mb ne mf nf mj ng mn my mz na nb bi translated">评估特征的重要性。</li></ul><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="6b04" class="nm lb iq ni b gy nn no l np nq">def trainGbc(params,feat_train,label_train,feat_test,label_test,\<br/>setbestestim=False,setfeatimp=False,featurecluster_dict=None):<br/>    '''<br/>     This function trains a gradient boosting algorithm, if <br/>     required, it determines the best number of n_estimators<br/>     and evaluates feature importances</span><span id="6413" class="nm lb iq ni b gy nr no l np nq">     Input:<br/>       params: [Dict] Parameters for the GBTC's construction<br/>       feat_train:  [DataFrame] Training features<br/>       label_train: [DataFrame] Training labels<br/>       feat_test:   [DataFrame] Testing features<br/>       label_test:  [DataFrame] Testing labels<br/>       setbestestim: [Bool] If True, determines the best size<br/>                            of the decision trees' ensemble<br/>       setfeatimp: [Bool] If True determines features' importances<br/>       featurecluster_dict: [Dict] If not None, dictionary of the<br/>                                   feature clusters</span><span id="00c8" class="nm lb iq ni b gy nr no l np nq">    Output:<br/>       Gbc: [Sklearn Instance] A trained GBTC estimator</span><span id="7bb0" class="nm lb iq ni b gy nr no l np nq">    '''</span><span id="305c" class="nm lb iq ni b gy nr no l np nq">    # If the best number of estimators has to be determined<br/>    if setbestestim:</span><span id="0eec" class="nm lb iq ni b gy nr no l np nq">        Gbc=GradientBoostingClassifier(**params)<br/>        Gbc.fit(feat_train,label_train)</span><span id="4317" class="nm lb iq ni b gy nr no l np nq">        # Determine the best n_estimators<br/>        scoretest_list=[]<br/>        scoretrain_list=[]<br/> <br/>        # Compute accuracy scores for training and testing with<br/>        # different n_estimators</span><span id="b7f1" class="nm lb iq ni b gy nr no l np nq">        for pred_test in Gbc.staged_predict(feat_test):<br/>            scoretest=accuracy_score(label_test,pred_test)<br/>            scoretest_list.append(scoretest)<br/>        for pred_train in Gbc.staged_predict(feat_train):<br/>            scoretrain=accuracy_score(label_train,pred_train)<br/>            scoretrain_list.append(scoretrain)</span><span id="54f8" class="nm lb iq ni b gy nr no l np nq">      # Plot the figure showing the training and testing<br/>      # accuracies' evolution with n_estimators                <br/>        </span><span id="94cf" class="nm lb iq ni b gy nr no l np nq">       nestim_fig,ax=plt.subplots(1,1,figsize=(10,8),)<br/>       plt.plot(np.arange(params["n_estimators"]),\<br/>       scoretrain_list,label="Train")<br/>       plt.plot(np.arange(params["n_estimators"]),<br/>       scoretest_list,label="Test")<br/>       plt.legend()<br/>       plt.xlabel("n_estimators")<br/>       plt.ylabel("Accuracy")<br/>       nestim_fig.savefig("nestim.pdf")<br/>       plt.show()<br/>  </span><span id="5428" class="nm lb iq ni b gy nr no l np nq">    # Cross validate and fit a GBTC estimator<br/>    else:<br/>       Gbc=GradientBoostingClassifier(**params)<br/>       score=cross_validate(Gbc,feat_train,label_train,\<br/>        cv=5,scoring="accuracy")<br/>       print("Gbc Cross Validation Accuracy (Testing)")<br/>       print(np.mean(score["test_score"]))<br/>       Gbc.fit(feat_train,label_train)</span><span id="d2df" class="nm lb iq ni b gy nr no l np nq">    #Determine feature importance<br/>    if setfeatimp:<br/>       impfeat_list=Gbc.feature_importances_<br/>       indexsort=np.argsort(Gbc.feature_importances_)<br/>       impfeat_fig,ax=plt.subplots(1,1,figsize=(10,8),)<br/>       pos=np.arange(len(indexsort))+0.5<br/>       plt.barh(pos,impfeat_list[indexsort]) <br/>       plt.yticks(pos,np.array(feat_train.columns)<br/>       [indexsort],fontsize=10,color="red")</span><span id="f41d" class="nm lb iq ni b gy nr no l np nq">       # If feature clustering, color the features depending on <br/>       # their clusters<br/>       if featurecluster_dict!=None:<br/>           for ifeat,featname in\<br/>            enumerate(np.array(feat_train.columns)[indexsort]):<br/>               for clusterkey in featurecluster_dict.keys():<br/>                   if featname in \<br/>                   featurecluster_dict[clusterkey]["namefeat"]:<br/>                <br/>                         ax.get_yticklabels()[ifeat].set_color\<br/>                        (featurecluster_dict\<br/>                        [clusterkey]["color"])<br/>                            <br/>               <br/>            plt.xlabel("Importance")<br/>            plt.xscale("log")<br/>            plt.xticks(size=10)<br/>            impfeat_fig.savefig("impfeat.pdf")<br/>            plt.show()<br/>    return Gbc</span></pre><p id="4665" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">我们首先需要确定决策树集合的大小来构造GBTC估计量。为此，根据所用决策树的数量对训练集和测试集的准确性进行评估。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="c0b0" class="nm lb iq ni b gy nn no l np nq"># Determine the best number of estimators<br/>params=\{"n_estimators":500,"learning_rate":0.01,"min_samples_split":5,"max_depth":4}</span><span id="f01c" class="nm lb iq ni b gy nr no l np nq">Gbc=trainGbc(params,feat_train,label_train,feat_test,label_test,setbestestim=True,setfeatimp=False)</span></pre><p id="d2c5" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">可以看出，300个决策树能够达到大约0.95的测试精度和0.99的训练精度(见图5)。因此，我们选择这种规模的决策树集合。修复它，评估30个特征中每一个的重要性。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/022ac472240a6c3a374a84f8528e54ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*F7QjsfIFb9-kJKOArGK4hw.png"/></div></div><p class="ns nt gj gh gi nu nv bd b be z dk translated">图5:训练和测试准确率随决策树集合大小的变化。</p></figure><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="164a" class="nm lb iq ni b gy nn no l np nq"># Determine feature importance with n_estimator=300<br/>params=\<br/>{"n_estimators":300,"learning_rate":0.01,"min_samples_split":5,"max_depth":4}</span><span id="0d91" class="nm lb iq ni b gy nr no l np nq">Gbc=trainGbc(params,feat_train,label_train,feat_test,\<br/>label_test,setbestestim=False,setfeatimp=True,featurecluster_dict=featurecluster_dict)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/89daeacf361fd6f153d4ea98fd5389f1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*CI4qysprqBVQ80a8u0H43w.png"/></div></div><p class="ns nt gj gh gi nu nv bd b be z dk translated">图6:根据GBTC算法评估特征的重要性。相同颜色的特征名称是那些参加由HC推导的相同聚类的特征名称。</p></figure><p id="a9f1" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">我们检查了，在测试集上，具有300个估计量的GBTC算法达到了0.951的精度。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="526e" class="nm lb iq ni b gy nn no l np nq">predGBC_test=Gbc.predict(feat_test)<br/>accGBC=accuracy_score(label_test,predGBC_test)<br/>print("Testing Accuracy")<br/>print(accGBC)</span><span id="020a" class="nm lb iq ni b gy nr no l np nq">Testing Accuracy<br/>0.9513684210526315</span></pre><p id="4350" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">在图6中，我们可以看到“最差半径”和“最差凹点”是属于先前建立的不同聚类的两个最重要的特征。此外，我们还展示了五个最重要特征的配对图。我们可以如预期的那样注意到两个最重要的特征“最差半径”和“最差周长”是强相关的(它们属于相同的特征聚类)。这就是为什么在特征选择过程中“最差周长”被“最差凹点”所取代。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="5c1d" class="nm lb iq ni b gy nn no l np nq">def pairplot(feat_all,label_all,featimp_list):<br/>    <br/>    '''<br/>    Compute the pair plots from a list of features<br/>    Input:<br/>      feat_all: [DataFrame] All the features<br/>      label_all: [DataFrame] All the labels<br/>      featimp_list: [List] Name of the features to compute the<br/>                           pair plots</span><span id="b97e" class="nm lb iq ni b gy nr no l np nq">    '''</span><span id="5c4c" class="nm lb iq ni b gy nr no l np nq">    mostfeat_list=feat_all.columns[np.argsort(\<br/>    Gbc.feature_importances_)][-5:]<br/>    pairplot_fig=plt.figure()<br/>    sns_plot=sn.pairplot(pd.concat(\<br/>    [feat_all[mostfeat_list],label_all],axis=1), hue='target')<br/>    fig = sns_plot<br/>    fig.savefig("pairplot.pdf")<br/>    </span><span id="e424" class="nm lb iq ni b gy nr no l np nq">featimp_list=list(Gbc.feature_importances_)<br/>pairplot(feat_all,label_all,featimp_list)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ol"><img src="../Images/3362aaef7f7cd2dfbed87e993e0a95c3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*_EY84ZzlUdmhSQtpVIfyCQ.png"/></div></div><p class="ns nt gj gh gi nu nv bd b be z dk translated">图7:从GBTC推导出的五个最重要特征的配对图。</p></figure><p id="1619" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">我们引入了一个函数，该函数基于所选择的特征来降低特征(未归一化和归一化)向量的维数。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="da79" class="nm lb iq ni b gy nn no l np nq">def selectfeat(feat_selected,feat_train,\<br/>feat_test,feat_all,featN_train,featN_test,featN_all\<br/>,meanfeat_arr,stdfeat_arr):</span><span id="d632" class="nm lb iq ni b gy nr no l np nq">   '''<br/>   Reduce the dimensionality of the feature vectors based on<br/>   the feature selected</span><span id="ec70" class="nm lb iq ni b gy nr no l np nq">   Input:<br/>      feat_selected:  [List] Names of the features selected<br/>      feat_train:  [DataFrame] Training features<br/>      feat_test:   [DataFrame] Testing features<br/>      feat_all: [DataFrame] All the features<br/>      featN_train: [DataFrame] Normalized training<br/>                               features<br/>      featN_test: [DataFrame] Normalized testing<br/>                              features<br/>      featN_all: [DataFrame] All the normalized features<br/>      meanfeat_arr:[Array] All the features' means for <br/>                          normalization<br/>      stdfeat_arr: [Array] All features' standard <br/>                           deviation for normalization</span><span id="fee3" class="nm lb iq ni b gy nr no l np nq">    Output:<br/>      featS_train: [DataFrame] Training selected features<br/>      featS_test: [DataFrame] Testing selected features<br/>      featS_all:  [DataFrame] All selected features<br/>      mean_featS: [Array] Means of the selected features<br/>      std_featS: [Array] Standard deviations of the selected<br/>                         features<br/>      featNS_train: [DataFrame] Training selected and normalized<br/>                                features<br/>      featNS_test:[DataFrame] Testing selected and normalized<br/>                              features<br/>      featNS_all: [DataFrame] All selected and normalized<br/>                              features</span><span id="b5a1" class="nm lb iq ni b gy nr no l np nq">   '''</span><span id="a7bd" class="nm lb iq ni b gy nr no l np nq">    # Identify selected features' indexes <br/>    indexfeat1=np.where(feat_train.columns==\<br/>    feat_selected[0])[0][0]<br/>    indexfeat2=np.where(feat_train.columns==\<br/>    feat_selected[1])[0][0]<br/></span><span id="a422" class="nm lb iq ni b gy nr no l np nq">    # Determine the means and standard deviations corresponding<br/>    # to the selected features        </span><span id="4cce" class="nm lb iq ni b gy nr no l np nq">    mean_featS=meanfeat_arr[[indexfeat1,indexfeat2]]<br/>    std_featS=stdfeat_arr[[indexfeat1,indexfeat2]]</span><span id="695a" class="nm lb iq ni b gy nr no l np nq">    # Select the features for unormalized and normalized data<br/>    featS_train=feat_train[feat_selected]<br/>    featS_test=feat_test[feat_selected]<br/>    featS_all=feat_all[feat_selected]<br/>    featNS_train=featN_train[feat_selected]<br/>    featNS_test=featN_test[feat_selected]<br/>    featNS_all=featN_all[feat_selected]</span><span id="cd06" class="nm lb iq ni b gy nr no l np nq">    return  featS_train,featS_test,featS_all,mean_featS,\<br/>    std_featS,featNS_train,featNS_test,featNS_all</span><span id="3290" class="nm lb iq ni b gy nr no l np nq"># Select the two most important variables (unormalized and normalized)<br/>feat_selected=["worst radius","worst concave points"]<br/>featS_train,featS_test,featS_all,mean_featS,std_featS,featNS_train,featNS_test,featNS_all\<br/>=selectfeat(feat_selected,feat_train,feat_test,feat_all,featN_train,featN_test,featN_all)</span></pre><p id="fb06" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">我们在平面图中绘制两个选定特征的数据，“最差半径”和“最差凹点”。注意，这个二维空间中的数据投影能够识别两个不同的聚类，每个聚类与两个标签中的一个相关联。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="9169" class="nm lb iq ni b gy nn no l np nq">def plotlabel(featS_all,feat_selected):</span><span id="3e89" class="nm lb iq ni b gy nr no l np nq">'''<br/>   Plot the data using the selected features<br/>   Input:<br/>     featS_all:  [DataFrame] All selected features<br/>     feat_selected:  [List] Names of the features selected</span><span id="66bf" class="nm lb iq ni b gy nr no l np nq">   '''</span><span id="e642" class="nm lb iq ni b gy nr no l np nq">    label_fig,ax=plt.subplots(1,1,figsize=(10,8),)</span><span id="e9bd" class="nm lb iq ni b gy nr no l np nq">   # Plot the data in the 2D-space<br/>    plt.scatter(featS_all.values[:,0]\<br/>   ,featS_all.values[:,1],c=label_all,\<br/>    cmap=matplotlib.colors.ListedColormap(["red","navy"]))</span><span id="b2e1" class="nm lb iq ni b gy nr no l np nq">    # Plots for the legend<br/>    plt.scatter(featS_all.values[:,0]\<br/>    [np.where(label_train.values==0)[0]][0],\<br/>    featS_all.values[:,1][np.where(label_train.values==0)\<br/>    [0]][0],c="red",label="Malignant")</span><span id="8feb" class="nm lb iq ni b gy nr no l np nq">    plt.scatter(featS_all.values[:,0\<br/>   [np.where(label_train.values==1)[0]][0],\<br/>   featS_all.values[:,1][np.where(label_train.values==1)\<br/>   [0]][0],c="navy",label="Benign")</span><span id="ff96" class="nm lb iq ni b gy nr no l np nq">   plt.xlabel(feat_selected[0])<br/>   plt.ylabel(feat_selected[1])<br/>   plt.legend()<br/>   label_fig.savefig("label.png")<br/>   plt.show()</span><span id="54e5" class="nm lb iq ni b gy nr no l np nq"># plot the data in the 2D-space<br/>plotlabel(featS_all,feat_selected)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/5adf026e378b8301c7c37e5e2aa99454.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bH5njIWshhm8GbAn20_KDQ.png"/></div></div><p class="ns nt gj gh gi nu nv bd b be z dk translated">图8:计划中“最差半径”和“最差凹点”的数据投影。</p></figure><p id="d441" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">我们现在的目的是在由两个特征“最差半径”和“最差凹点”形成的二维空间中导出一个函数，以评估发展为恶性乳腺癌的概率风险。为此，下一节将开发一种支持向量机分类器(SVMC)算法</p><h1 id="00e8" class="la lb iq bd lc ld le lf lg lh li lj lk kf ll kg lm ki ln kj lo kl lp km lq lr bi translated">二。从SVMC推导出概率风险</h1><p id="0e53" class="pw-post-body-paragraph ls lt iq lu b lv lw ka lx ly lz kd ma mb mc md me mf mg mh mi mj mk ml mm mn ij bi translated">在本节中，我们构建SVC模型，根据两个先前选择的特征来估计发展为恶性乳腺癌的概率风险。首先我们确定SVC的超参数。第二次，模型被用于实现计划中的预测(“工作半径”、“最差凹点”)。最终确定与预测相关的概率估计。</p><p id="abb3" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated"><strong class="lu ja">二. 1超参数的确定</strong></p><p id="d3b8" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">SVC旨在确定具有相同标签的数据被重新分组的区域。这些区域由边界界定，这些边界被计算以最大化它们与数据之间的距离。这个距离叫做边缘。为了计算特征空间中的距离，可以使用核。使用核作为非线性函数确实能够模拟更灵活的边界，从而对于数据分离更有效。这里，我们采用一个称为RBF的高斯型核，它取决于比例参数γ，使得两点x和y之间的距离为:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi om"><img src="../Images/3b5ae1e49d83e951369974c38da4a224.png" data-original-src="https://miro.medium.com/v2/resize:fit:1042/format:webp/1*pNYf1KNEEub-1CMlnnI6dw.png"/></div></figure><p id="8885" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">因此，必须调整该伽马参数。除此之外，还必须确定影响边界平滑度的正则化因子C。由于交叉验证研究，两个参数都是固定的。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi on"><img src="../Images/82dcb858f86440b50c466d9b8ab9e32f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1294/format:webp/1*yzEXDV3RtR7-YL3_wl2PdQ.png"/></div><p class="ns nt gj gh gi nu nv bd b be z dk translated">图9:核SVC能够使边界具有更复杂的形状，从而更有效地进行数据分离。</p></figure><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="1b53" class="nm lb iq ni b gy nn no l np nq">def crossvalidateSVC(featNS_all,label_all,gammaonly=True):</span><span id="249b" class="nm lb iq ni b gy nr no l np nq">    '''</span><span id="d8d3" class="nm lb iq ni b gy nr no l np nq">    Realize cross validation operations to determine the best<br/>    SVC's hyperparameters</span><span id="7a07" class="nm lb iq ni b gy nr no l np nq">    Input:<br/>      featNS_all: [DataFrame] All selected and normalized<br/>                              features<br/>      label_all: [DataFrame] Gathers all the labels<br/>      gammaonly: [Bool] True if the gamma parameter is estimated<br/>                        only, if False C is estimated either<br/>   Output:<br/>      bestgamma:  [Float] The best estimated gamma paramater<br/>      bestc: [Float] The best estimated C parameter if gammaonly<br/>                      is False</span><span id="1342" class="nm lb iq ni b gy nr no l np nq">      <br/>    ''' <br/>    # If just the gamma parameter is estimated  <br/>    if gammaonly:<br/>        param_grid={"gamma":np.logspace(0,2,200)}<br/>        GridCV=GridSearchCV(SVC(),\<br/>        param_grid=param_grid,scoring="accuracy",cv=10)<br/>        GridCV.fit(featNS_all,label_all)</span><span id="74e9" class="nm lb iq ni b gy nr no l np nq">        gridcv_fig,ax=plt.subplots(1,1,figsize=(10,8),)<br/>        plt.plot(np.array(GridCV.cv_results_["param_gamma"]),\<br/>        np.array(GridCV.cv_results_["mean_test_score"]))<br/>        plt.xlabel("Gamma")<br/>        plt.ylabel("CV Accuracy")<br/>        gridcv_fig.savefig("gridcv.png")<br/>        plt.show()</span><span id="219d" class="nm lb iq ni b gy nr no l np nq">        bestgamma=GridCV.best_params_["gamma"]<br/>        return bestgamma</span><span id="0907" class="nm lb iq ni b gy nr no l np nq">     # If both gamma and C are estimated<br/>     else:<br/>        param_grid=\<br/>        {"C":np.logspace(-1,1,10),"gamma":np.logspace(0,2,200)}<br/>        GridCV=GridSearchCV(SVC(),\<br/>        param_grid=param_grid,scoring="accuracy",cv=10)<br/>        GridCV.fit(featNS_all,label_all)</span><span id="be1d" class="nm lb iq ni b gy nr no l np nq">        gridcv_fig,ax=plt.subplots(1,1,figsize=(10,8),)<br/>        plt.plot(np.array(GridCV.cv_results_["param_gamma"])\<br/>       ,np.array(GridCV.cv_results_["mean_test_score"]))<br/>        plt.xlabel("Gamma",fontsize=15)<br/>        plt.ylabel("CV Accuracy",fontsize=15)<br/>        gridcv_fig.savefig("gridcv.png")<br/>        plt.show()<br/>        bestgamma=GridCV.best_params_["gamma"]<br/>        bestc=GridCV.best_params_["C"]</span><span id="95e9" class="nm lb iq ni b gy nr no l np nq">        return bestgamma,bestc</span><span id="c596" class="nm lb iq ni b gy nr no l np nq">bestgamma,bestc=crossvalidateSVC(featNS_all,\<br/>label_all,gammaonly=False)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/619ac1af9b73c3a9e20f7fce1b38aa9c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*4AY8RuqhvYpttskgzbjZQw.png"/></div></div><p class="ns nt gj gh gi nu nv bd b be z dk translated">图9:交叉验证准确度(测试平均值)作为伽马参数的函数。</p></figure><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="0921" class="nm lb iq ni b gy nn no l np nq">bestgamma,bestc=crossvalidateSVC(featNS_all,label_all,gammaonly=False)<br/>print(“Best Gamma”)<br/>print(bestgamma)<br/>print(“Best C”)<br/>print(bestc)</span><span id="ab52" class="nm lb iq ni b gy nr no l np nq">Best Gamma<br/>2.0914343584919426<br/>Best C<br/>2.1544346900318834</span></pre><p id="45d0" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">我们发现，固定𝛾=2.09和C=2.15能够在10倍交叉验证数据集中达到0.9482的平均测试精度，标准偏差为0.043。因此，这两个参数的值将在下文中考虑。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="fa6a" class="nm lb iq ni b gy nn no l np nq">def trainSVC(gamma,c,featNS_train,label_train):<br/>    <br/>    '''<br/>    Realize a 10-fold cross validation for a SVC model<br/>    and train a SVC model </span><span id="a702" class="nm lb iq ni b gy nr no l np nq">   Input<br/>      gamma:  [Float] The best estimated gamma paramater<br/>      c: [Float] The best estimated C parameter if gammaonly<br/>                      is False<br/>      featNS_train: [DataFrame] Training selected and normalized<br/>                                features<br/>      label_train: [DataFrame] Training labels</span><span id="214c" class="nm lb iq ni b gy nr no l np nq">   Output:<br/>     Svc: [Sklearn Instance] Trained SVC model</span><span id="be01" class="nm lb iq ni b gy nr no l np nq">    '''</span><span id="107f" class="nm lb iq ni b gy nr no l np nq"><br/>    Svc=SVC(C=c,gamma=gamma,probability=True)<br/>    score=cross_validate(Svc,featNS_train,label_train,cv=10)<br/>    print("SVC CV Mean Testing Accuracy")<br/>    print(np.mean(score["test_score"]))<br/>    print("SVC CV Standard Deviation Testing Accuracy")<br/>    print(np.std(score["test_score"]))<br/>    Svc.fit(featNS_train,label_train)<br/>    return Svc</span><span id="79b7" class="nm lb iq ni b gy nr no l np nq">bestgamma=2.09<br/>bestc=2.15<br/>Svc=trainSVC(bestgamma,bestc,featNS_train,label_train)</span><span id="50af" class="nm lb iq ni b gy nr no l np nq">SVC CV Mean Testing Accuracy<br/>0.94829178743961352<br/>SVC CV Standard Deviation Testing Accuracy<br/>0.043399848777455924</span></pre><p id="afbe" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">在测试集上达到的精度为0.964。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="bbef" class="nm lb iq ni b gy nn no l np nq">predvalid=Svc.predict(featNS_test)<br/>accvalid=accuracy_score(label_test,predvalid)<br/>print("Testing Accuracy")<br/>print(accvalid)</span><span id="774b" class="nm lb iq ni b gy nr no l np nq">Testing Accuracy<br/>0.9649122807017544</span></pre><p id="196e" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">可以看出，将特征集合减少到最重要的特征，对它们进行归一化，并采用如上所述的SVC模型，能够稍微提高测试精度，从GBTC的0.95提高到0.96(见图10)。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/edb9a25c04502f58d3586ea3a8740b5b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*C5OIUN6I50NjQTBX_43KFQ.png"/></div></div><p class="ns nt gj gh gi nu nv bd b be z dk translated">图10:GBTC和SVC算法的测试精度比较。</p></figure><p id="bcf6" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated"><strong class="lu ja">三. 2预测图</strong></p><p id="f608" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">在训练SVC分类器之后，在2D空间“最差区域”和“最差凹点”中评估其预测。图11显示了这些预测。在图11和图12中，圆形点代表用于训练的点，三角形点代表用于测试的点。</p><pre class="kp kq kr ks gt nh ni nj nk aw nl bi"><span id="d1c3" class="nm lb iq ni b gy nn no l np nq">def contpredSVC(Svc,nbpoints,mean_featS,\<br/>std_featS,featS_train,featNS_train,label_train,featS_test,\<br/>featNS_test,label_test,predict_proba=False):</span><span id="a532" class="nm lb iq ni b gy nr no l np nq">'''<br/>   Make predictions in the plan of the two selected features<br/>   using SVC.<br/>   Input:<br/>     Svc: [Sklearn Instance] Trained SVC model<br/>     nbpoints: [Int] Number of points sampling each feature<br/>                      direction<br/>     std_featS: [Array] Standard deviation of each selected<br/>                        feature<br/>     featS_train: [DataFrame] Selected feature training set<br/>     featNS_train: [DataFrame] Selected and normalized feature<br/>                               training set<br/>     label_train: [DataFrame] Training labels  <br/>     featS_test: [DataFrame] Selected feature testing set<br/>     featNS_test: [DataFrame] Selected and normalized feature <br/>                              testing set<br/>     label_test: [DataFrame] Testing labels<br/>     predict_proba: [Bool] True if a probability risk map<br/>                           is computed, False if label map</span><span id="ec6f" class="nm lb iq ni b gy nr no l np nq">   '''  <br/>    # Determine maximum and minimum normalized values for each<br/>    # selected features and compute the normalized 2D mesh<br/>    minfeat1NS=np.min(featNS_train.values[:,0])<br/>    minfeat2NS=np.min(featNS_train.values[:,1])<br/>    maxfeat1NS=np.max(featNS_train.values[:,0])<br/>    maxfeat2NS=np.max(featNS_train.values[:,1])<br/>    x_arr=np.arange(minfeat1NS,maxfeat1NS,\<br/>   (maxfeat1NS-minfeat1NS)/nbpoints)<br/>    y_arr=np.arange(minfeat2NS,maxfeat2NS,\<br/>   (maxfeat2NS-minfeat2NS)/nbpoints)<br/>    X,Y=np.meshgrid(x_arr,y_arr)</span><span id="0e2d" class="nm lb iq ni b gy nr no l np nq">    # Points (normalized) in the 2D space where the predictions are<br/>    # computed.<br/>    pointstopred=np.array([[X[irange][ipoint],Y[irange][ipoint]]\<br/>    for ipoint in range(len(X[0]))\<br/>    for irange in range(len(X))])</span><span id="2e6b" class="nm lb iq ni b gy nr no l np nq">    # If a probability risk map is computed<br/>    if predict_proba:</span><span id="3c7f" class="nm lb iq ni b gy nr no l np nq">        # Make prediction with SVC<br/>        valmesh=Svc.predict_proba(pointstopred)<br/>        predsvccont_fig,ax=plt.subplots(1,1,figsize=(10,8),)</span><span id="340e" class="nm lb iq ni b gy nr no l np nq">        # Denormalize feature points and plot the predictions <br/>        denfeat1_arr=pointstopred[:,0]*std_featS[0]+mean_featS[0]<br/>        denfeat2_arr=pointstopred[:,1]*std_featS[1]+mean_featS[1]        <br/>        im=plt.scatter(denfeat1_arr,denfeat2_arr,c=valmesh[:,0],\<br/>        cmap=cm.coolwarm,alpha=1)</span><span id="78dd" class="nm lb iq ni b gy nr no l np nq">    # If a label map is computed<br/>    else:<br/>        valmesh=Svc.predict(pointstopred)<br/>        predsvccont_fig,ax=plt.subplots(1,1,figsize=(10,8),)<br/>        colors=["red","navy"]<br/>  <br/>        # Denormalize feature points and plot the predictions <br/>        denfeat1_arr=pointstopred[:,0]*std_featS[0]+mean_featS[0]<br/>        denfeat2_arr=pointstopred[:,1]*std_featS[1]+mean_featS[1]        <br/>        im=plt.scatter(denfeat1_arr,denfeat2_arr,c=valmesh,<br/>        cmap=matplotlib.colors.ListedColormap(colors),alpha=0.1)</span><span id="798a" class="nm lb iq ni b gy nr no l np nq">   # Plot the training data<br/>   plt.scatter(featS_train.values[:,0]\<br/>   [np.where(label_train.values==0)[0]],\<br/>   featS_train.values[:,1][np.where(label_train.values==0)\<br/>   [0]],c="red",s=30,label="Malignant-Training")</span><span id="54f9" class="nm lb iq ni b gy nr no l np nq">   plt.scatter(featS_train.values[:,0]\<br/>   [np.where(label_train.values==1)[0]],\<br/>   featS_train.values[:,1][np.where(label_train.values==1)\<br/>   [0]],c="navy",s=30,label="Benign-Training")</span><span id="8752" class="nm lb iq ni b gy nr no l np nq">   # Plot the testing data<br/>   plt.scatter(featS_test.values[:,0]\<br/>   [np.where(label_test.values==0)[0]],\<br/>   featS_test.values[:,1][np.where(label_test.values==0)\<br/>   [0]],c="red",marker="^",s=30,label="Malignant-Testing")</span><span id="fa36" class="nm lb iq ni b gy nr no l np nq">   plt.scatter(featS_test.values[:,0]<br/>   [np.where(label_test.values==1)[0]],\<br/>   featS_test.values[:,1][np.where(label_test.values==1)[0]],\<br/>   c="navy",marker="^",s=30,label="Benign-Testing")</span><span id="bda3" class="nm lb iq ni b gy nr no l np nq">   plt.legend()<br/>   plt.ylim(np.min(featS_all.values[:,1]),\<br/>   np.max(featS_all.values[:,1]))<br/>   plt.xlim(np.min(featS_all.values[:,0]),\<br/>   np.max(featS_all.values[:,0]))<br/>   plt.xlabel(feat_selected[0])<br/>   plt.ylabel(feat_selected[1])</span><span id="e0e1" class="nm lb iq ni b gy nr no l np nq">   if predict_proba:<br/>        plt.colorbar(im,label="Probability Risk")<br/>        predsvccont_fig.savefig("predsvcproba.png")<br/>        <br/>        plt.show()<br/>    else:<br/>        predsvccont_fig.savefig("predsvclabel.png")<br/>        plt.show()</span><span id="2e28" class="nm lb iq ni b gy nr no l np nq"># Train SVC<br/>nbpoints=150<br/>bestgamma=2.09<br/>bestc=2.15</span><span id="7275" class="nm lb iq ni b gy nr no l np nq"># Plot label map<br/>contpredSVC(Svc,nbpoints,mean_featS,std_featS,featS_train,featNS_train,label_train,featS_test,featNS_test,label_test,predict_proba=False)</span><span id="5f34" class="nm lb iq ni b gy nr no l np nq"># Plot probability map<br/>contpredSVC(Svc,nbpoints,mean_featS,std_featS,featS_train,featNS_train,label_train,featS_test,featNS_test,label_test,predict_proba=True)</span></pre><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/b7cfd76488ef6873cd790f78b814341c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*eNfE4qj4m4yagJ-73ggc8Q.png"/></div></div><p class="ns nt gj gh gi nu nv bd b be z dk translated">图11: SVC在计划中的标签预测“最差半径”和“最差凹点”。彩色点对应于训练集(圆形)和测试集(三角形)的数据。</p></figure><p id="12a5" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">在SVC分类器的情况下，也可以导出预测的概率。对于预测的标注𝑙𝑖，给定预测𝑙𝑖，数据被标注为1的概率由下式给出:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/9f9149c0af525b37008d9392f9da146b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1252/format:webp/1*ws1ix-UlpvHv31CqNasWaA.png"/></div></figure><p id="4fc0" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">其中常数𝛼和𝛽是在训练过程中通过最大似然优化获得的(见[6])。</p><p id="efd0" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">使用该模型，图12因此在识别的2D空间中示出了发展成恶性乳腺癌的估计概率。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/879291910f486f5d9197d97f0416c59c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*mNOrAhYO2N9-DyMW2Kq3HQ.png"/></div></div><p class="ns nt gj gh gi nu nv bd b be z dk translated">图12:SVC模型的标签概率评估。它可以被解释为发展为恶性乳腺癌的概率图。</p></figure><h1 id="5330" class="la lb iq bd lc ld le lf lg lh li lj lk kf ll kg lm ki ln kj lo kl lp km lq lr bi translated">三。结论</h1><p id="359d" class="pw-post-body-paragraph ls lt iq lu b lv lw ka lx ly lz kd ma mb mc md me mf mg mh mi mj mk ml mm mn ij bi translated">这项工作是确定30个被提议用来评估恶性乳腺癌风险的变量中最重要的两个变量的机会。通过结合层次聚类算法和使用梯度推进的特征重要性估计来选择特征。使用选择的特征建立支持向量机分类器。</p><p id="f409" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">获得的机器学习模型与使用所有30个变量的其他模型具有相同的可靠性。这种方法的优点是通过计算易于解释的2D风险图来简化风险评估。现在需要更多的数据来证实所确定的区域和趋势。</p><h1 id="da6a" class="la lb iq bd lc ld le lf lg lh li lj lk kf ll kg lm ki ln kj lo kl lp km lq lr bi translated">来源</h1><p id="a9ab" class="pw-post-body-paragraph ls lt iq lu b lv lw ka lx ly lz kd ma mb mc md me mf mg mh mi mj mk ml mm mn ij bi translated">[1]<a class="ae op" href="https://www.wcrf.org/dietandcancer/cancer-trends/breast-cancer-statistics" rel="noopener ugc nofollow" target="_blank">https://www . wcrf . org/dietandcancer/cancer-trends/breast-cancer-statistics</a></p><p id="f1fe" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">[2]<a class="ae op" rel="noopener" target="_blank" href="/building-a-simple-machine-learning-model-on-breast-cancer-data-eca4b3b99fa3">https://towards data science . com/building-a-simple-machine-learning-model-on-breast-cancer-data-ECA 4 B3 b 99 fa 3</a></p><p id="7400" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">[3]<a class="ae op" rel="noopener" target="_blank" href="/how-to-use-scikit-learn-datasets-for-machine-learning-d6493b38eca3">https://towards data science . com/how-to-use-scikit-learn-datasets-for-machine-learning-d 6493 b 38 ECA 3</a></p><p id="dca4" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">[4]<a class="ae op" href="http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29" rel="noopener ugc nofollow" target="_blank">http://archive . ics . UCI . edu/ml/datasets/breast+cancer+Wisconsin+% 28 diagnostic % 29</a></p><p id="6e24" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">[5]<a class="ae op" href="https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-multicollinear-py" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/auto _ examples/inspection/plot _ permutation _ importance _ multicollinear . html # sphx-glr-auto-examples-inspection-plot-permutation-importance-multicollinear-py</a></p><p id="7eec" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated"><a class="ae op" href="https://en.wikipedia.org/wiki/Hierarchical_clustering" rel="noopener ugc nofollow" target="_blank">https://en.wikipedia.org/wiki/Hierarchical_clustering</a></p><p id="2f60" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">[6]<a class="ae op" href="https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting" rel="noopener ugc nofollow" target="_blank">https://sci kit-learn . org/stable/modules/ensemble . html # gradient-boosting</a></p><p id="a53c" class="pw-post-body-paragraph ls lt iq lu b lv mo ka lx ly mp kd ma mb mq md me mf mr mh mi mj ms ml mm mn ij bi translated">[7]https://scikit-learn.org/stable/modules/calibration.html<a class="ae op" href="https://scikit-learn.org/stable/modules/calibration.html" rel="noopener ugc nofollow" target="_blank"/></p></div></div>    
</body>
</html>