<html>
<head>
<title>Healthcare AI’s Equity Problem</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">医疗保健人工智能的公平问题</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/healthcare-ais-equity-problem-fca16998a48e?source=collection_archive---------18-----------------------#2021-11-11">https://towardsdatascience.com/healthcare-ais-equity-problem-fca16998a48e?source=collection_archive---------18-----------------------#2021-11-11</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="8636" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/fairness-and-bias" rel="noopener" target="_blank">公平和偏见</a></h2><div class=""/><div class=""><h2 id="d585" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">为什么数据公平不仅仅是公平</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/2f0caa8dedac67efb7b75709af6cc703.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*tbwzEPMWYc3tXqM-8D0iaQ.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">图片来自<a class="ae le" href="https://unsplash.com/@toddquackenbush" rel="noopener ugc nofollow" target="_blank">托德</a>在<a class="ae le" href="https://unsplash.com/photos/XBxQZLNBM0Q" rel="noopener ugc nofollow" target="_blank"> Unsplash </a></p></figure><p id="f70f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们目前正处于一些人所说的人工智能的“狂野西部”之中。尽管医疗保健是监管最严格的部门之一，但这一领域的人工智能监管仍处于初级阶段。规则被写成<a class="ae le" href="https://www.fda.gov/medical-devices/software-medical-device-samd/artificial-intelligence-and-machine-learning-aiml-enabled-medical-devices" rel="noopener ugc nofollow" target="_blank">我们说</a>。我们正在迎头赶上，学习如何获得这些技术带来的好处，同时在这些技术已经部署后最大限度地减少任何潜在的危害。</p><p id="7dbd" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">医疗保健中的人工智能系统加剧了现有的不平等。我们已经看到了现实世界的后果，从美国司法系统的种族偏见、信用评分、简历筛选和性别偏见。旨在为我们的系统带来机器“客观性”和易用性的程序最终会复制和支持偏见，却没有任何责任。</p><p id="407f" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">算法本身很少是问题。值得关注的往往是用于技术编程的数据。但这远不止是道德和公平的问题。<strong class="lh ja"> </strong>构建考虑医疗保健全貌的人工智能工具是创建有效解决方案的基础。</p><h1 id="9481" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">算法和数据一样好</h1><p id="4c55" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">从我们人类系统的本质来看，数据集几乎总是不公正的，很少是公平的。正如 Linda Nordling 在《自然》杂志的一篇文章中评论的那样，<a class="ae le" href="https://www.nature.com/articles/d41586-019-02872-2" rel="noopener ugc nofollow" target="_blank">人工智能在医疗保健中更公平的前进方式</a>，“这场革命取决于这些工具可以学习的数据，这些数据反映了我们今天看到的不平等的医疗系统。”</p><p id="b9d3" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">以<a class="ae le" href="https://www.ajemjournal.com/article/S0735-6757(19)30391-2/fulltext" rel="noopener ugc nofollow" target="_blank">调查结果</a>为例，美国急诊室的黑人接受止痛药的可能性比白人低 40%,西班牙裔病人低 25%。现在，想象一下这些发现所基于的数据集被用来训练一个人工智能工具的算法，该工具将被用来帮助护士确定他们是否应该服用止痛药物。这些种族差异将会重现，而支持这些差异的隐性偏见将不会受到质疑，甚至变得自动化。</p><p id="c101" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们可以尝试通过删除我们认为导致训练偏差的数据来改善这些偏差，但仍然会有隐藏的模式<a class="ae le" href="https://www.wired.com/story/these-algorithms-look-x-rays-detect-your-race/" rel="noopener ugc nofollow" target="_blank">与人口统计数据</a>相关联。一个算法不能考虑到全局的细微差别。它只能从提供给它的数据模式中学习。</p><h1 id="45ba" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">偏差蠕变</h1><p id="fb05" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">数据偏见以意想不到的方式蔓延到医疗保健领域。考虑到世界各地实验室中用于发现和测试新止痛药的动物模型<a class="ae le" href="https://www.the-scientist.com/uncategorized/why-sex-matters-in-mouse-models-46346" rel="noopener ugc nofollow" target="_blank">几乎全部是雄性</a>。因此，包括止痛药在内的许多药物都不适合女性。因此，即使像布洛芬和萘普生这样的普通止痛药也被证明对男性比女性更有效，而且女性比男性更容易遭受止痛药带来的副作用。</p><p id="59e2" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">事实上，雄性啮齿动物也不是完美的测试对象。研究还表明，雌性和雄性啮齿动物对疼痛程度的反应因在场人类研究人员的性别而异。啮齿动物对一名男性研究人员的嗅觉引起的压力反应足以<a class="ae le" href="https://www.science.org/news/2014/04/male-scent-may-compromise-biomedical-research" rel="noopener ugc nofollow" target="_blank">改变它们对疼痛的反应</a>。</p><p id="2654" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">虽然这个例子看起来似乎偏离了人工智能，但事实上它有着深刻的联系——在治疗进入临床试验之前，我们目前可以获得的治疗选择隐含着偏见。人工智能公平的挑战不是一个纯粹的技术问题，而是一个非常人性化的问题，始于我们作为科学家做出的选择。</p><h1 id="585d" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">不平等的数据导致不平等的利益</h1><p id="d9d4" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">为了让全社会享受人工智能系统可以给医疗保健带来的诸多好处，全社会必须在用于训练这些系统的数据中得到平等的代表。虽然这听起来很简单，但这是一个很难完成的任务。</p><p id="3abc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">来自某些人群的数据并不总是能够进入训练数据集。发生这种情况的原因有很多。由于现有的系统性挑战，如缺乏获取数字技术的途径或仅仅被认为不重要，一些数据可能无法获取，甚至根本无法收集。预测模型是通过以有意义的方式对数据进行分类而创建的。但是因为一般来说数据较少，“少数”数据往往是数据集中的异常值，为了创建一个更清晰的模型，经常被当作虚假数据剔除。</p><p id="952e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">数据源很重要，因为这个细节无疑会影响医疗保健模型的结果和解释。在撒哈拉以南非洲，年轻女性被诊断出患有乳腺癌的比率<a class="ae le" href="https://globalizationandhealth.biomedcentral.com/articles/10.1186/s12992-018-0446-6" rel="noopener ugc nofollow" target="_blank">要高得多</a>。这揭示了为这一人口群体量身定制的人工智能工具和医疗保健模型的需求，而不是用于检测乳腺癌的人工智能工具，这些工具只在全球北方的乳房 x 光片上进行训练。同样，越来越多的工作表明，用于检测皮肤癌的<a class="ae le" href="https://www.theatlantic.com/health/archive/2018/08/machine-learning-dermatology-skin-color/567619/" rel="noopener ugc nofollow" target="_blank">算法对于黑人患者来说往往不太准确，因为它们主要是在浅色皮肤患者的图像上进行训练的。这样的例子不胜枚举。</a></p><p id="21d6" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我们正在创造有可能彻底改变医疗保健行业的工具和系统，但这些发展的好处只会惠及数据中显示的那些人。</p><h1 id="f885" class="mc md iq bd me mf mg mh mi mj mk ml mm kf mn kg mo ki mp kj mq kl mr km ms mt bi translated">那么，有什么办法呢？</h1><p id="f9bb" class="pw-post-body-paragraph lf lg iq lh b li mu ka lk ll mv kd ln lo mw lq lr ls mx lu lv lw my ly lz ma ij bi translated">消除数据偏见的部分挑战是，大量、多样和有代表性的数据集不容易访问。公开可用的训练数据集往往非常狭窄、数量少且同质——它们只捕捉到社会的一部分。与此同时，许多医疗机构每天都要捕获大量不同的健康数据<em class="mb"/>，但数据隐私法使得访问这些更庞大、更多样的数据集变得困难。</p><p id="1e9e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">数据保护当然至关重要。在负责任地使用数据方面，大型科技公司和政府没有最好的记录。然而，如果医疗数据共享的透明度、教育和同意得到更有目的的监管，更加多样化和高容量的数据集可能有助于在人工智能系统中实现更公平的表示，并为人工智能驱动的医疗工具带来更好、更准确的结果。</p><p id="5320" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">但是数据共享和访问并不能完全解决医疗保健的人工智能问题。通过人工智能实现更好的个性化医疗保健仍然是一个极具挑战性的问题，需要一大批科学家和工程师。我们希望教会我们的算法做出好的选择，但我们仍然在弄清楚好的选择对我们自己来说应该是什么样的。</p><p id="7bba" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">人工智能为医疗保健带来了更大的个性化机会，但它也带来了巩固现有不平等的风险。我们面前有机会采取一种经过深思熟虑的方法来收集、监管和使用数据，这将提供一个更全面、更公平的画面，并实现人工智能在医疗保健中的下一步。</p></div></div>    
</body>
</html>