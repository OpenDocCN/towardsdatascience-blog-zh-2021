<html>
<head>
<title>From Confusion Matrix to Weighted Cross Entropy</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">从混淆矩阵到加权交叉熵</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/from-confusion-matrix-to-weighted-cross-entropy-9d2999c06845?source=collection_archive---------24-----------------------#2021-12-20">https://towardsdatascience.com/from-confusion-matrix-to-weighted-cross-entropy-9d2999c06845?source=collection_archive---------24-----------------------#2021-12-20</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><div class=""/><div class=""><h2 id="fc45" class="pw-subtitle-paragraph jn ip iq bd b jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd ke dk translated">如何使用加权交叉熵来惩罚一个标签</h2></div><p id="32ad" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">你好。</p><p id="9203" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在我们之前的文章中，我们用 3 个步骤介绍了机器学习的交叉熵概念。特别是，我们从信息论的角度讨论了 log(p)是什么，熵的真正含义，以及它与交叉熵损失的关系。</p><div class="lb lc gp gr ld le"><a href="https://medium.com/@david.h.kang/an-intuitive-guide-how-entropy-connects-to-cross-entropy-78b0713494a9" rel="noopener follow" target="_blank"><div class="lf ab fo"><div class="lg ab lh cl cj li"><h2 class="bd ir gy z fp lj fr fs lk fu fw ip bi translated">直观指南:熵如何与交叉熵相联系</h2><div class="ll l"><h3 class="bd b gy z fp lj fr fs lk fu fw dk translated">你好。</h3></div><div class="lm l"><p class="bd b dl z fp lj fr fs lk fu fw dk translated">medium.com</p></div></div><div class="ln l"><div class="lo l lp lq lr ln ls lt le"/></div></div></a></div><p id="f83b" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">在本文中，我们将通过考虑<em class="lu">加权交叉熵</em>来扩展我们在交叉熵方面的讨论，加权交叉熵允许我们对一个标签进行加权。例如，当设计用于新冠肺炎诊断的最大似然算法时，我们可能希望对假阴性的惩罚比对假阳性的惩罚更重。让我们开始吧。</p></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><blockquote class="mc"><p id="4360" class="md me iq bd mf mg mh mi mj mk ml la dk translated">第一步。混淆矩阵</p></blockquote><p id="1a3b" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">混淆矩阵是总结 ML 模型分类结果的一种非常方便的方法。如下所示，它由 TP(真阳性)、FP(假阳性)、FN(假阴性)和 TN(真阴性)4 个部分组成。例如，在新冠肺炎的情况下，FN 将是被诊断为没有新冠肺炎的新冠肺炎患者的数量。</p><figure class="ms mt mu mv gt mw gh gi paragraph-image"><div role="button" tabindex="0" class="mx my di mz bf na"><div class="gh gi mr"><img src="../Images/1ca414843fea9e78e883ea1c8a0e9529.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Ll2oes5HXuLL13XfBa2y6g.png"/></div></div><p class="nc nd gj gh gi ne nf bd b be z dk translated">作者图片</p></figure><blockquote class="mc"><p id="4af5" class="md me iq bd mf mg ng nh ni nj nk la dk translated">第二步。混淆矩阵中的关键概念</p></blockquote><p id="6606" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">这里我总结几个来源于混淆矩阵的关键词，它们在有分类任务的 ML 论文中出现很多。尽量不要死记硬背，因为名字本身就很有意义！</p><p id="0af6" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="lu"> (1)敏感度</em> </strong>(回忆)量化你的模型在正确分类阳性标签方面的敏感度，意思是，在所有实际阳性案例中，有多少被正确分类为阳性。</p><p id="74a8" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">敏感度= TP / (TP + FN) = TPR(真阳性率)。</p><p id="e843" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="lu"> (2) Precision </em> </strong>量化模型预测的精确程度，也就是说，在所有肯定的猜测中，有多少实际上是肯定的。</p><p id="c671" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">精度= TP / (TP + FP)</p><p id="686e" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="lu"> (3)用于二进制分类的 F 分数</em> </strong>通过 1/F = 1/2[1/召回+1/精度]通过调和平均将灵敏度和精度结合起来</p><p id="ccac" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">根据应用的不同，可能需要更加强调灵敏度或精度。这种情况下可以用 F_beta。请注意，当 beta = 1 时，它将成为之前定义的标准 F 分数。</p><p id="9373" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">1/F _ beta = 1/(1+beta)[beta/Recall+1/Precision]</p><p id="6d58" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated"><strong class="kh ir"> <em class="lu"> (4)准确度</em> </strong>是最常见的度量之一。由于它同时考虑了所有的总磷和总氮，当你有不平衡的数据时，它可能会产生误导。例如，虽然您可能希望尽可能好地预测罕见疾病的 TP 病例，但您的(差的)ML 模型可能仅因为有太多的 TN 而显示出高准确性，尽管它的 TP 很低。</p><p id="bd80" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">准确度= (TP + TN) / (TP + FP + TN + FN)</p><blockquote class="mc"><p id="2233" class="md me iq bd mf mg mh mi mj mk ml la dk translated">第三步。加权交叉熵</p></blockquote><p id="cc33" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated"><strong class="kh ir">太好了！</strong>你已经看到，在需要特别注意正面或负面情况的应用程序中，对混淆矩阵中的每个部分一视同仁会产生误导。</p><p id="1bb1" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">正如我们在上一篇文章中讨论的，二元分类任务中的交叉熵损失计算如下，对两个标签给予同等的重视。</p><pre class="ms mt mu mv gt nl nm nn no aw np bi"><span id="554f" class="nq nr iq nm b gy ns nt l nu nv">labels * -log(sigmoid(logits)) +<br/>    (1 - labels) * -log(1 - sigmoid(logits))</span></pre><p id="b95f" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">现在请注意，我们如何对这种损失进行轻微的处理，以增加(或减少)标签的重要性！这是通过插入大于(小于)1 的<code class="fe nw nx ny nm b">pos_weight</code>来加强(或减弱)阳性标签(+)。</p><pre class="ms mt mu mv gt nl nm nn no aw np bi"><span id="4266" class="nq nr iq nm b gy ns nt l nu nv">labels * -log(sigmoid(logits)) * pos_weight +<br/>    (1 - labels) * -log(1 - sigmoid(logits))</span></pre><p id="b516" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">当<code class="fe nw nx ny nm b">pos_weight</code>大于 1 时，你会对被错误归类为阴性的阳性标签进行更多的惩罚，因此有助于提高灵敏度；你可以期待你的 ML 模型变得更加敏感，在所有实际的阳性案例中正确地分类出阳性案例。</p><p id="621c" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">另一方面，当<code class="fe nw nx ny nm b">pos_weight</code>小于 1 时，你的模型对错误分类为阳性的阴性标签进行更多的惩罚，因此有助于提高精度；您可以期待您的 ML 模型在正确分类所有积极预测中的积极案例方面变得更加精确。方程式摘自<a class="ae nz" href="https://www.tensorflow.org/api_docs/python/tf/nn/weighted_cross_entropy_with_logits" rel="noopener ugc nofollow" target="_blank"> tensorflow 网站</a>。</p></div><div class="ab cl lv lw hu lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="ij ik il im in"><blockquote class="mc"><p id="ab23" class="md me iq bd mf mg mh mi mj mk ml la dk translated">恭喜你。你已经走了很长的路。</p></blockquote><p id="4874" class="pw-post-body-paragraph kf kg iq kh b ki mm jr kk kl mn ju kn ko mo kq kr ks mp ku kv kw mq ky kz la ij bi translated">总而言之，</p><ul class=""><li id="e9f4" class="oa ob iq kh b ki kj kl km ko oc ks od kw oe la of og oh oi bi translated">灵敏度和精度都与我们的模型如何将阳性病例分类为阳性(TP)有关。</li><li id="37ef" class="oa ob iq kh b ki oj kl ok ko ol ks om kw on la of og oh oi bi translated">敏感度考虑所有实际阳性(TP + FN)，而精确度考虑所有预测阳性(TP + FP)。</li><li id="ddf7" class="oa ob iq kh b ki oj kl ok ko ol ks om kw on la of og oh oi bi translated">加权交叉熵损失让我们的 ML 模型惩罚更多(或更少)被错误分类为负面的正面标签，反之亦然！</li></ul><p id="c791" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">不要犹豫留下你的想法！您的反馈和评论有助于丰富我们的社区。很高兴听到。感谢阅读:-)</p><p id="7fc0" class="pw-post-body-paragraph kf kg iq kh b ki kj jr kk kl km ju kn ko kp kq kr ks kt ku kv kw kx ky kz la ij bi translated">~ DK</p></div></div>    
</body>
</html>