<html>
<head>
<title>How Linear Regression Actually Works — Theory and Implementation</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">线性回归的实际工作原理——理论和实现</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/how-linear-regression-actually-works-theory-and-implementation-8d8dcae3222c?source=collection_archive---------33-----------------------#2021-03-22">https://towardsdatascience.com/how-linear-regression-actually-works-theory-and-implementation-8d8dcae3222c?source=collection_archive---------33-----------------------#2021-03-22</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="a6ce" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">我们推导出最大似然估计并实现它</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/90ced6d16af00d94ae124cbe748be7ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/1*BUvr6JWkXScxc-OUbBBqjQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">一条符合某些数据的直线，同时也显示了不确定性。图片作者。</p></figure><h1 id="1388" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">内容</h1><p id="27b3" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">这篇文章是我将要发表的一系列文章的一部分。你可以通过点击<a class="ae mi" href="https://cookieblues.github.io/guides/2021/03/22/bsmalea-notes-2/" rel="noopener ugc nofollow" target="_blank">这里</a>在我的个人博客上阅读这篇文章的更详细版本。下面你可以看到该系列的概述。</p><h2 id="6792" class="mj kv it bd kw mk ml dn la mm mn dp le lv mo mp lg lz mq mr li md ms mt lk mu bi translated">1.机器学习导论</h2><ul class=""><li id="fbbb" class="mv mw it lo b lp lq ls lt lv mx lz my md mz mh na nb nc nd bi translated"><a class="ae mi" rel="noopener" target="_blank" href="/what-is-machine-learning-91040db474f9">(一)什么是机器学习？</a></li><li id="e86b" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated"><a class="ae mi" rel="noopener" target="_blank" href="/model-selection-in-machine-learning-813fe2e63ec6"> (b)机器学习中的模型选择</a></li><li id="9650" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated"><a class="ae mi" rel="noopener" target="_blank" href="/the-curse-of-dimensionality-5673118fe6d2"> (c)维度的诅咒</a></li><li id="2efb" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated"><a class="ae mi" rel="noopener" target="_blank" href="/what-is-bayesian-inference-4eda9f9e20a6"> (d)什么是贝叶斯推理？</a></li></ul><h2 id="d049" class="mj kv it bd kw mk ml dn la mm mn dp le lv mo mp lg lz mq mr li md ms mt lk mu bi translated"><strong class="ak"> 2。回归</strong></h2><ul class=""><li id="f902" class="mv mw it lo b lp lq ls lt lv mx lz my md mz mh na nb nc nd bi translated"><strong class="lo iu"> (a)线性回归的实际工作原理</strong></li><li id="7d6d" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated"><a class="ae mi" rel="noopener" target="_blank" href="/how-to-improve-your-linear-regression-with-basis-functions-and-regularization-8a6fcebdc11c"> (b)如何使用基函数和正则化改进您的线性回归</a></li></ul><h2 id="e9bb" class="mj kv it bd kw mk ml dn la mm mn dp le lv mo mp lg lz mq mr li md ms mt lk mu bi translated">3.分类</h2><ul class=""><li id="d387" class="mv mw it lo b lp lq ls lt lv mx lz my md mz mh na nb nc nd bi translated"><a class="ae mi" rel="noopener" target="_blank" href="/overview-of-classifiers-d0a0d3eecfd1"> (a)分类器概述</a></li><li id="6719" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated"><a class="ae mi" rel="noopener" target="_blank" href="/quadratic-discriminant-analysis-ae55d8a8148a#204a-71584f33e137"> (b)二次判别分析(QDA) </a></li><li id="6320" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated"><a class="ae mi" rel="noopener" target="_blank" href="/linear-discriminant-analysis-1894bbf04359"> (c)线性判别分析</a></li><li id="085d" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated"><a class="ae mi" rel="noopener" target="_blank" href="/gaussian-naive-bayes-4d2895d139a"> (d)(高斯)朴素贝叶斯</a></li></ul><h1 id="1ffd" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">设置和目标</h1><p id="feb6" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">在这篇文章中，我们将学习<strong class="lo iu">线性回归</strong>，这对于很多机器学习来说是必不可少的<strong class="lo iu">，因此理解起来会非常有益。</strong></p><p id="3a96" class="pw-post-body-paragraph lm ln it lo b lp nj ju lr ls nk jx lu lv nl lx ly lz nm mb mc md nn mf mg mh im bi translated">给定一个具有相应目标变量<em class="no"> t </em>的<em class="no"> N </em>输入变量<strong class="lo iu"> x </strong>的训练数据集，线性回归的目标是构造一个函数<em class="no"> h </em> ( <strong class="lo iu"> x </strong>)，该函数为<strong class="lo iu"> x </strong>的新值产生<em class="no"> t </em>的预测值。</p><p id="5cd5" class="pw-post-body-paragraph lm ln it lo b lp nj ju lr ls nk jx lu lv nl lx ly lz nm mb mc md nn mf mg mh im bi translated">用于回归的最简单的线性模型被称为<em class="no">线性回归</em>，其中的预测是由</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi np"><img src="../Images/3e198ce42e55add6a960977371ca901a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1020/1*OAnTMYlbwbtUnLdYm3jlpg.gif"/></div></figure><p id="810d" class="pw-post-body-paragraph lm ln it lo b lp nj ju lr ls nk jx lu lv nl lx ly lz nm mb mc md nn mf mg mh im bi translated">第一项通常称为<strong class="lo iu">截距</strong>或<strong class="lo iu">偏差</strong>参数，并允许<em class="no"> h </em>适应数据中的固定偏移。如果我们引入1作为每个输入变量<strong class="lo iu"> x </strong>的第一个元素，我们可以用向量符号重写(1)</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi nq"><img src="../Images/c8c8c25a10f8f35d8eba120529de538a.png" data-original-src="https://miro.medium.com/v2/resize:fit:258/1*nmyCf-BR8-7EHPWx7P_x_g.gif"/></div></figure><p id="823f" class="pw-post-body-paragraph lm ln it lo b lp nj ju lr ls nk jx lu lv nl lx ly lz nm mb mc md nn mf mg mh im bi translated">在本系列的第一篇文章<a class="ae mi" rel="noopener" target="_blank" href="/what-is-machine-learning-91040db474f9">中，我们回顾了带有一维输入变量的多项式回归。现在，我们只是做线性回归(不是多项式)，但我们允许我们的输入变量是D维的，因此它成为一个向量，而不是一个标量。然而，为了直观起见，让我们坚持使用同一个示例数据集:</a></p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="9670" class="mj kv it ns b gy nw nx l ny nz">import numpy as np</span><span id="38f3" class="mj kv it ns b gy oa nx l ny nz">x = np.array([-1, -0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1])<br/>t = np.array([-4.9, -3.5, -2.8, 0.8, 0.3, -1.6, -1.3, 0.5, 2.1, 2.9, 5.6])</span><span id="b3a9" class="mj kv it ns b gy oa nx l ny nz">N = len(x)</span></pre><p id="d4a2" class="pw-post-body-paragraph lm ln it lo b lp nj ju lr ls nk jx lu lv nl lx ly lz nm mb mc md nn mf mg mh im bi translated">因为我们的输入变量是一维的，我们的参数向量<strong class="lo iu"> w </strong>是二维的，因为它有一个偏差权重和一个输入变量权重。</p><p id="8dd9" class="pw-post-body-paragraph lm ln it lo b lp nj ju lr ls nk jx lu lv nl lx ly lz nm mb mc md nn mf mg mh im bi translated">为了确保我们也能找到偏差参数，我们必须在<strong class="lo iu"> x </strong>中引入一列1。我们可以通过下面的代码片段做到这一点</p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="461d" class="mj kv it ns b gy nw nx l ny nz">X = np.column_stack([np.ones(N), x])</span></pre><h1 id="010e" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">衍生和培训</h1><p id="2219" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">那么，<strong class="lo iu">我们如何训练模型呢？</strong>我们将研究两种不同的方法来推导训练该模型的方法。</p><p id="f6e3" class="pw-post-body-paragraph lm ln it lo b lp nj ju lr ls nk jx lu lv nl lx ly lz nm mb mc md nn mf mg mh im bi translated">回想一下<strong class="lo iu">训练</strong>(或<strong class="lo iu">学习</strong>)指的是<strong class="lo iu">估计我们模型的参数</strong>的过程，所以当我们问如何训练模型的时候，就和问如何估计<strong class="lo iu"> w </strong>的值是一样的。</p><h2 id="2ccd" class="mj kv it bd kw mk ml dn la mm mn dp le lv mo mp lg lz mq mr li md ms mt lk mu bi translated">普通最小二乘法</h2><p id="a35e" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">就像我们在第一篇文章的<a class="ae mi" rel="noopener" target="_blank" href="/what-is-machine-learning-91040db474f9">中所做的那样，我们定义了一个<strong class="lo iu">目标函数</strong>，它根据误差来计算我们的模型的性能，然后我们根据我们的参数来最小化这个误差。</a></p><p id="c993" class="pw-post-body-paragraph lm ln it lo b lp nj ju lr ls nk jx lu lv nl lx ly lz nm mb mc md nn mf mg mh im bi translated">这意味着<strong class="lo iu">我们会找到导致误差最小的参数</strong>。我们将使用相同的目标函数，误差平方和(SSE)，定义为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ob"><img src="../Images/1ad4e4f0a0dc76d583984c32590f73b7.png" data-original-src="https://miro.medium.com/v2/resize:fit:582/1*al1QQ6_doX3aqZwBP1HN-A.gif"/></div></figure><p id="23e9" class="pw-post-body-paragraph lm ln it lo b lp nj ju lr ls nk jx lu lv nl lx ly lz nm mb mc md nn mf mg mh im bi translated">我们希望找到使<em class="no"> E </em>最小的<strong class="lo iu"> w </strong>的值</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="od oe di of bf og"><div class="gh gi oc"><img src="../Images/800c63faa089b4a351b1d6cef0a37ac5.png" data-original-src="https://miro.medium.com/v2/resize:fit:316/1*rJmKNIg0ZbtiyMkmg7oubw.gif"/></div></div></figure><p id="9bbf" class="pw-post-body-paragraph lm ln it lo b lp nj ju lr ls nk jx lu lv nl lx ly lz nm mb mc md nn mf mg mh im bi translated">注意(2)是参数<strong class="lo iu"> w </strong>的二次函数。因此，它相对于<strong class="lo iu"> w </strong>的偏导数在<strong class="lo iu"> w </strong>中是线性的，这意味着存在唯一的最小值。这是<strong class="lo iu">凸函数</strong>的一个性质。</p><p id="6c4d" class="pw-post-body-paragraph lm ln it lo b lp nj ju lr ls nk jx lu lv nl lx ly lz nm mb mc md nn mf mg mh im bi translated">如果我们评估网格中参数<strong class="lo iu"> w </strong>的值的目标函数，那么我们可以用等高线图说明我们的目标函数具有唯一的最小值。如下所示。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/12cb1d97f01da469924b2983822ab585.png" data-original-src="https://miro.medium.com/v2/resize:fit:1054/format:webp/1*cFVj3jpixLOkwV_PoLDpDw.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">十字位于我们的目标函数的最小值——这个坐标对应于为我们的数据集产生最小SSE的参数值。图片作者。</p></figure><p id="7391" class="pw-post-body-paragraph lm ln it lo b lp nj ju lr ls nk jx lu lv nl lx ly lz nm mb mc md nn mf mg mh im bi translated">但是我们如何找到最小的T2呢？</p><p id="adc1" class="pw-post-body-paragraph lm ln it lo b lp nj ju lr ls nk jx lu lv nl lx ly lz nm mb mc md nn mf mg mh im bi translated">我们通过求导，设置导数等于0，并求解函数变量来找到函数的最小值。为了便于记法，让我们所有的输入变量都用<strong class="lo iu"> X </strong>表示，具有<em class="no"> N </em>行(每个输入变量一行)和<em class="no"> D </em> +1列(每个特征一行，偏差一行)，即，</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/ea9d4a0adf8549633c48531c5fdc8a4c.png" data-original-src="https://miro.medium.com/v2/resize:fit:492/1*asz7dSOgmNT6q4ROMF3g2Q.gif"/></div></figure><p id="1720" class="pw-post-body-paragraph lm ln it lo b lp nj ju lr ls nk jx lu lv nl lx ly lz nm mb mc md nn mf mg mh im bi translated">并且让<strong class="lo iu"> t </strong>表示我们的目标变量的列向量。我们现在可以将(2)改写为</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oj"><img src="../Images/75ef8c45d8d3e957b01ffa289807f6b0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1122/1*JO7aIDQLeoRaSnhjnb1z1g.gif"/></div></figure><p id="0d6e" class="pw-post-body-paragraph lm ln it lo b lp nj ju lr ls nk jx lu lv nl lx ly lz nm mb mc md nn mf mg mh im bi translated">如果我们现在对<strong class="lo iu"> w </strong>求导，我们得到</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ok"><img src="../Images/d98dea133635f36e0a9338ba1cd3a5fe.png" data-original-src="https://miro.medium.com/v2/resize:fit:772/1*5OT0Yi7a4X3JVJLSTlEMNw.gif"/></div></figure><p id="27a8" class="pw-post-body-paragraph lm ln it lo b lp nj ju lr ls nk jx lu lv nl lx ly lz nm mb mc md nn mf mg mh im bi translated">将这个值设为0，我们就可以解出<strong class="lo iu"> w </strong></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ol"><img src="../Images/4cb4ecb817f16252a47e3c72ea8e008a.png" data-original-src="https://miro.medium.com/v2/resize:fit:558/1*TEDgf8hdpdqFKF0SWCckMg.gif"/></div></figure><p id="f1a7" class="pw-post-body-paragraph lm ln it lo b lp nj ju lr ls nk jx lu lv nl lx ly lz nm mb mc md nn mf mg mh im bi translated">这是我们对参数的估计值</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi om"><img src="../Images/fc9b0bf359eee90c7bb24c74834a6a62.png" data-original-src="https://miro.medium.com/v2/resize:fit:592/1*uw3vZfmidP547qQZN-qxuw.gif"/></div></figure><h2 id="4110" class="mj kv it bd kw mk ml dn la mm mn dp le lv mo mp lg lz mq mr li md ms mt lk mu bi translated">最大概似法</h2><p id="8220" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">然而，选择SSE作为目标函数似乎有点武断——例如，为什么不直接选择误差的总和呢？为什么我们要把它们弄成方形？</p><p id="dcda" class="pw-post-body-paragraph lm ln it lo b lp nj ju lr ls nk jx lu lv nl lx ly lz nm mb mc md nn mf mg mh im bi translated">为了说明为什么这是一个好的选择，以及为什么这个解决方案是有意义的，我们将从概率的角度<strong class="lo iu">使用<a class="ae mi" href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation" rel="noopener ugc nofollow" target="_blank">最大似然估计(MLE) </a>导出同样的解决方案</strong>。</p><p id="07a9" class="pw-post-body-paragraph lm ln it lo b lp nj ju lr ls nk jx lu lv nl lx ly lz nm mb mc md nn mf mg mh im bi translated">为了做到这一点，我们假设目标变量<em class="no"> t </em>是由我们的函数<em class="no"> h </em> ( <strong class="lo iu"> x </strong>，<strong class="lo iu"> w </strong>)加上一点噪声给出的</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi on"><img src="../Images/dc22a2b3c3bc3e84a3d38a45a5168220.png" data-original-src="https://miro.medium.com/v2/resize:fit:272/1*G7zjkYfbMX3ejHhuVgZvVQ.gif"/></div></figure><p id="78ac" class="pw-post-body-paragraph lm ln it lo b lp nj ju lr ls nk jx lu lv nl lx ly lz nm mb mc md nn mf mg mh im bi translated">其中我们假设<em class="no"> ϵ </em>正态分布，均值为0，标准差<em class="no"> σ </em>。</p><p id="5428" class="pw-post-body-paragraph lm ln it lo b lp nj ju lr ls nk jx lu lv nl lx ly lz nm mb mc md nn mf mg mh im bi translated">这就让我们说，给定一个输入变量<strong class="lo iu"> x </strong>，对应的目标值<em class="no"> t </em>正态分布，均值<em class="no"> h </em> ( <strong class="lo iu"> x </strong>，<strong class="lo iu"> w </strong>)，标准差<em class="no"> σ </em>，即:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oo"><img src="../Images/c295dc0788cbb9506064167233448d20.png" data-original-src="https://miro.medium.com/v2/resize:fit:676/1*PsZ10T-y0yei-t38kAVY3w.gif"/></div></figure><p id="5be1" class="pw-post-body-paragraph lm ln it lo b lp nj ju lr ls nk jx lu lv nl lx ly lz nm mb mc md nn mf mg mh im bi translated"><strong class="lo iu">让我们花点时间了解一下到底是怎么回事。</strong></p><p id="d025" class="pw-post-body-paragraph lm ln it lo b lp nj ju lr ls nk jx lu lv nl lx ly lz nm mb mc md nn mf mg mh im bi translated">下图说明了(3)告诉我们的内容。我们正在估计一些参数<strong class="lo iu"> w </strong>，因此我们的目标变量<em class="no"> t </em>遵循围绕<em class="no"> h </em>输出值的正态分布。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi op"><img src="../Images/df67ee555b08a01257218fc0f51cd019.png" data-original-src="https://miro.medium.com/v2/resize:fit:1386/format:webp/1*2LtX2ZJOJYfBOwxUiekKPQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">目标变量如何遵循估计函数周围的正态分布的图示。图片作者。</p></figure><p id="5fd9" class="pw-post-body-paragraph lm ln it lo b lp nj ju lr ls nk jx lu lv nl lx ly lz nm mb mc md nn mf mg mh im bi translated">我们现在可以使用整个数据集，<strong class="lo iu"> X </strong>和<strong class="lo iu"> t </strong>，通过假设我们的数据点是独立于(3)得出的，来写出似然函数。然后，对于所有输入和目标变量对，似然函数变成(3)的乘积，并且是<strong class="lo iu"> w </strong>和<em class="no"> σ </em>的函数</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oq"><img src="../Images/c189f7ed2f8bbb1ef6815acce3a9fc73.png" data-original-src="https://miro.medium.com/v2/resize:fit:768/1*olTz_yCmL0_IVT4e615lYQ.gif"/></div></figure><p id="82b7" class="pw-post-body-paragraph lm ln it lo b lp nj ju lr ls nk jx lu lv nl lx ly lz nm mb mc md nn mf mg mh im bi translated">现在<strong class="lo iu">我们想要最大化可能性</strong>，这意味着我们想要确定使(4)最大化的参数<strong class="lo iu"> w </strong>和<em class="no"> σ </em>的值。这看起来很难，但是我们可以用一个小技巧让它变得更简单。由于对数是单调递增的函数，<strong class="lo iu">对数似然性最大化等价于似然性最大化</strong>。</p><p id="001c" class="pw-post-body-paragraph lm ln it lo b lp nj ju lr ls nk jx lu lv nl lx ly lz nm mb mc md nn mf mg mh im bi translated">取可能性的对数给了我们</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi or"><img src="../Images/0fc8ab57f7c4bbf6ede444a7bf8442a5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1032/1*JtgsAJ3SP7RtOK94Tt3-5Q.gif"/></div></figure><p id="8b67" class="pw-post-body-paragraph lm ln it lo b lp nj ju lr ls nk jx lu lv nl lx ly lz nm mb mc md nn mf mg mh im bi translated">现在很明显为什么SSE目标函数是一个好的选择——( 5)的最后一项是唯一依赖于<strong class="lo iu"> w </strong>的部分，并且与SSE相同。</p><p id="8c2d" class="pw-post-body-paragraph lm ln it lo b lp nj ju lr ls nk jx lu lv nl lx ly lz nm mb mc md nn mf mg mh im bi translated">由于第一项不依赖于<strong class="lo iu"> w </strong>，我们可以省略它，并且由于关于<strong class="lo iu"> w </strong>的似然函数的最大值不会随着正常数的缩放而改变，那么我们看到<strong class="lo iu">最大化似然等价于最小化SSE目标函数</strong>。</p><p id="8f17" class="pw-post-body-paragraph lm ln it lo b lp nj ju lr ls nk jx lu lv nl lx ly lz nm mb mc md nn mf mg mh im bi translated">因此，<strong class="lo iu"> w </strong>的最大似然估计与我们之前的推导相同</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi os"><img src="../Images/efc27ba956e24092b57c591eb8d50917.png" data-original-src="https://miro.medium.com/v2/resize:fit:382/1*xbanCGFEbRMmZnulD6YGyg.gif"/></div></figure><p id="8b76" class="pw-post-body-paragraph lm ln it lo b lp nj ju lr ls nk jx lu lv nl lx ly lz nm mb mc md nn mf mg mh im bi translated">我们可以使用<strong class="lo iu"> w </strong>的最大似然解的结果来找到噪声参数<em class="no"> σ </em>的值。如果我们在对数似然中插入对<strong class="lo iu"> w </strong>的最大似然估计，求导数，并将其设为0，那么我们可以求解<em class="no"> σ </em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ot"><img src="../Images/3b615f8c0662e8d8b7e3c8aa7a883d39.png" data-original-src="https://miro.medium.com/v2/resize:fit:1206/1*ERj2_KeeOosGR9MNk70j6Q.gif"/></div></figure><h2 id="ffd5" class="mj kv it bd kw mk ml dn la mm mn dp le lv mo mp lg lz mq mr li md ms mt lk mu bi translated">Python实现</h2><p id="0262" class="pw-post-body-paragraph lm ln it lo b lp lq ju lr ls lt jx lu lv lw lx ly lz ma mb mc md me mf mg mh im bi translated">因为我们现在有了一个概率模型，我们做出的预测是在<em class="no"> t </em>上的概率分布，而不仅仅是点估计。这是通过将最大似然解中的<strong class="lo iu"> w </strong>和<em class="no"> σ </em>代入(3)来完成的</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ou"><img src="../Images/c51251254fe9e19a051559d5167e25c0.png" data-original-src="https://miro.medium.com/v2/resize:fit:784/1*KH6cuk4oo_KmBQ4sjM4FWQ.gif"/></div></figure><p id="a517" class="pw-post-body-paragraph lm ln it lo b lp nj ju lr ls nk jx lu lv nl lx ly lz nm mb mc md nn mf mg mh im bi translated">我们可以用下面的代码片段找到<strong class="lo iu"> w </strong>和<em class="no"> σ </em></p><pre class="kj kk kl km gt nr ns nt nu aw nv bi"><span id="35be" class="mj kv it ns b gy nw nx l ny nz">w = np.linalg.inv(X.T @ X) @ X.T @ t<br/>sigma_sq = sum((t - X @ w)**2) / len(t)</span></pre><p id="25df" class="pw-post-body-paragraph lm ln it lo b lp nj ju lr ls nk jx lu lv nl lx ly lz nm mb mc md nn mf mg mh im bi translated">下面是我们根据<strong class="lo iu"> w </strong>以及不确定性参数<em class="no"> σ估算的直线图。</em></p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi ki"><img src="../Images/90ced6d16af00d94ae124cbe748be7ae.png" data-original-src="https://miro.medium.com/v2/resize:fit:1370/format:webp/1*BUvr6JWkXScxc-OUbBBqjQ.png"/></div><p class="kq kr gj gh gi ks kt bd b be z dk translated">由我们的估计参数和不确定性生成的线图。图片作者。</p></figure><h1 id="407d" class="ku kv it bd kw kx ky kz la lb lc ld le jz lf ka lg kc lh kd li kf lj kg lk ll bi translated">摘要</h1><ul class=""><li id="a592" class="mv mw it lo b lp lq ls lt lv mx lz my md mz mh na nb nc nd bi translated">我们可以通过<strong class="lo iu">普通最小二乘法</strong>或<strong class="lo iu">最大似然估计</strong>找到<strong class="lo iu">线性回归</strong>的参数。</li><li id="d3f8" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated">通常在<strong class="lo iu">线性回归</strong>中，我们有一个不与输入相乘的标量参数，称为<strong class="lo iu">截距</strong>或<strong class="lo iu">偏差</strong>。</li><li id="4f48" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated">估计参数的值的<strong class="lo iu">过程被称为<strong class="lo iu">训练或学习</strong>过程。</strong></li><li id="1fd6" class="mv mw it lo b lp ne ls nf lv ng lz nh md ni mh na nb nc nd bi translated">由于对数是一个<strong class="lo iu">单调递增的</strong>函数，因此<strong class="lo iu">最大化似然函数与最大化对数似然函数</strong>相同。</li></ul></div></div>    
</body>
</html>