<html>
<head>
<title>Hyperparameter Tuning with KerasTuner and TensorFlow</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用KerasTuner和TensorFlow进行超参数调谐</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/hyperparameter-tuning-with-kerastuner-and-tensorflow-c4a4d690b31a?source=collection_archive---------4-----------------------#2021-08-27">https://towardsdatascience.com/hyperparameter-tuning-with-kerastuner-and-tensorflow-c4a4d690b31a?source=collection_archive---------4-----------------------#2021-08-27</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="8b53" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">了解使用KerasTuner和TensorFlow优化模型架构和超参数的最佳实践</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/931955a787df5074c0d2cdb87e3271cb.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*1dGc3xj5651kNV9iSnZ6xA.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图片由Zoltan·塔西在<a class="ae ky" href="https://unsplash.com/photos/CLJeQCr2F_A" rel="noopener ugc nofollow" target="_blank">宣传片</a>上拍摄</p></figure><p id="7555" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">构建机器学习模型是一个迭代过程，涉及优化模型的性能和计算资源。您在每次迭代中调整的设置被称为<em class="lv">超参数</em>。它们支配着训练过程，并在训练过程中保持不变。</p><p id="d2a2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">搜索最佳超参数的过程被称为<em class="lv">超参数调整</em>或<em class="lv">超调整</em>，在任何机器学习项目中都是必不可少的。超调通过移除不必要的参数(例如，密集层中的单元数量)来帮助提高性能并降低模型复杂性。</p><p id="c10a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有两种超参数:</p><ol class=""><li id="931a" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated"><em class="lv">影响模型架构的模型超参数</em>(例如，DNN中隐藏层的数量和宽度)</li><li id="1f82" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated"><em class="lv">影响训练速度和质量的算法超参数</em>(如学习率和激活函数)。</li></ol><p id="109c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">即使在浅层DNN中，超参数组合的数量也可能增长得惊人地大，导致手动搜索最优集合根本不可行也不可扩展。</p><p id="00bd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这篇文章将向您介绍Keras Tuner，这是一个自动化超参数搜索的库。我们将构建并比较在时尚MNIST数据集上训练的三个深度学习模型的结果:</p><ul class=""><li id="8f44" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mk mc md me bi translated">具有预选超参数的基线模型</li><li id="b8cb" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mk mc md me bi translated">超带算法优化超参数</li><li id="9ddc" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mk mc md me bi translated">具有贝叶斯优化的调整的ResNet架构</li></ul><p id="5f86" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">你可以点击查看jupyter笔记本<a class="ae ky" href="https://github.com/lukenew2/ds-demos/blob/master/notebooks/hypertuning_kerastuner.ipynb" rel="noopener ugc nofollow" target="_blank">。</a></p><h1 id="deb4" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">导入和预处理</h1><p id="40d3" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">让我们首先导入所需的模块并打印它们的版本，以防您想要复制笔记本。我们使用的是tensor flow 2 . 5 . 0版和kera stuner 1 . 0 . 1版。</p><pre class="kj kk kl km gt ni nj nk nl aw nm bi"><span id="0714" class="nn mm it nj b gy no np l nq nr">import tensorflow as tf<br/>import kerastuner as kt</span><span id="1df5" class="nn mm it nj b gy ns np l nq nr">from tensorflow import keras</span><span id="fe28" class="nn mm it nj b gy ns np l nq nr">print(f"TensorFlow Version: {tf.__version__}")<br/>print(f"KerasTuner Version: {kt.__version__}")</span><span id="650e" class="nn mm it nj b gy ns np l nq nr">&gt;&gt;&gt; TensorFlow Version: 2.5.0<br/>&gt;&gt;&gt; KerasTuner Version: 1.0.1</span></pre><p id="c4b7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将从加载<a class="ae ky" href="https://github.com/zalandoresearch/fashion-mnist" rel="noopener ugc nofollow" target="_blank">时尚MNIST数据集</a>开始。目标是训练一个机器学习模型来对不同的服装图像进行分类。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="d315" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">由于图像是灰度的，这意味着每个像素值代表一个1到255之间的数字，我们可以将每个像素除以255来归一化0到1之间的值。这样会让训练收敛的更快。</p><h1 id="1991" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">基线模型</h1><p id="2356" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">如前所述，我们将首先用预选的超参数训练一个浅层密集神经网络(DNN ),为我们提供一个基准性能。我们稍后会看到简单的模型，像这个浅层DNN，如何需要一些时间来调整。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="0cb3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意我们如何在上面的代码中硬编码所有的超参数。这些包括隐藏层的数量(在我们的例子中有1个隐藏层)，隐藏层中的单元数量(512)，它的激活函数(ReLu)，以及退出百分比(0.2)。我们将在下一节中调整所有这些超参数。</p><p id="4964" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们来设置优化器、损失和指标。我们稍后将调整的另一个超参数是学习率，但现在我们将它设置为0.001。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="5ced" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">定义好模型的设置后，我们就可以开始训练了！我们将把周期数设置为20，如果5个周期后性能没有提高，就使用提前停止来中断训练。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="43b0" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们想看看我们的模型在测试集上的表现，因此我们将定义一个助手函数来轻松显示结果。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi nv"><img src="../Images/f262623fe4dc42b185ce9808cdc18b6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:744/format:webp/1*J4M8pkMvZfXL_RXU4Ik7Ow.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图一。基线评估结果|作者图片</p></figure><p id="bcd8" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一组超参数的结果。想象一下，尝试不同的学习率、辍学率、隐藏层的数量以及每个隐藏层中的神经元数量。如您所见，手动超调根本不可行，也不可扩展。在下一节中，您将看到Keras Tuner如何简单地通过自动化过程和以有效的方式搜索超参数空间来解决这些问题。</p><h1 id="562f" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">Keras调谐器</h1><p id="22f7" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">Keras Tuner是一个简单的、可分发的超参数优化框架，它自动化了手动搜索最佳超参数的痛苦过程。Keras Tuner带有随机搜索、超波段和贝叶斯优化内置搜索算法，旨在适应多种使用情况，包括:</p><ul class=""><li id="e447" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mk mc md me bi translated">分布式调谐</li><li id="09bf" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mk mc md me bi translated">定制培训循环(例如，GANs、强化学习等。)</li><li id="509a" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mk mc md me bi translated">在模型建立功能(预处理、数据扩充、测试时间扩充等)之外添加超参数。)</li></ul><p id="4bda" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这些过程超出了本文的范围，但是可以随意阅读更多的官方文档。</p><p id="8c15" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用Keras调谐器超调浅DNN有四个步骤:</p><ol class=""><li id="601c" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">定义模型</li><li id="e72d" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">指定要优化的超参数</li><li id="2b2d" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">定义搜索空间</li><li id="36da" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">定义搜索算法</li></ol><h2 id="9fda" class="nn mm it bd mn nw nx dn mr ny nz dp mv li oa ob mx lm oc od mz lq oe of nb og bi translated">定义模型</h2><p id="15a8" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">我们为超调建立的模型被称为<em class="lv">超模</em>。我们在构建超模型时定义了超参数搜索空间。</p><p id="7639" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有两种方法可以建立一个超级模型:</p><ol class=""><li id="2d8c" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mb mc md me bi translated">通过使用模型构建器功能</li><li id="3b3f" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mb mc md me bi translated">使用Keras Tuner API的<a class="ae ky" href="https://keras.io/guides/keras_tuner/getting_started/#you-can-use-a-hypermodel-subclass-instead-of-a-modelbuilding-function" rel="noopener ugc nofollow" target="_blank">超模子类</a></li></ol><p id="049e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将使用第一种方法来定义模型构建函数中的DNN。您会注意到超参数是如何内联定义的。我们的模型构建函数使用定义的超参数来返回编译后的模型。</p><h2 id="05ab" class="nn mm it bd mn nw nx dn mr ny nz dp mv li oa ob mx lm oc od mz lq oe of nb og bi translated">指定要优化的超参数</h2><p id="0e2d" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">我们将构建的模型与我们之前训练的浅层DNN非常相似，只是我们将调整模型的四个超参数:</p><ul class=""><li id="d290" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mk mc md me bi translated">隐藏层的数量</li><li id="07b5" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mk mc md me bi translated">每个隐藏层中的单元数</li><li id="e8ee" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mk mc md me bi translated">每个隐藏层后的下降百分比</li><li id="949f" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mk mc md me bi translated">Adam优化器的学习率</li></ul><h2 id="d987" class="nn mm it bd mn nw nx dn mr ny nz dp mv li oa ob mx lm oc od mz lq oe of nb og bi translated">定义搜索空间</h2><p id="a106" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">这是通过将一个超参数对象作为参数传递给模型构建函数来实现的，该函数配置您想要优化的超参数。在我们的函数中，我们将使用:</p><ul class=""><li id="ef72" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mk mc md me bi translated">惠普。Int()来定义隐藏层的数量和每个隐藏层中的单元的搜索空间。这允许您定义最小值和最大值，以及增量的步长。</li><li id="c9ca" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mk mc md me bi translated">惠普。Float()定义辍学百分比的搜索空间。这个和惠普差不多。Int()，只不过它接受浮点值。</li><li id="21d6" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mk mc md me bi translated">惠普。Choice()来定义学习率的搜索空间。这允许您定义离散值。</li></ul><p id="aee7" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">有关所有可用方法及其用法的更多信息，请访问<a class="ae ky" href="https://keras.io/api/keras_tuner/hyperparameters/" rel="noopener ugc nofollow" target="_blank">官方文档</a>。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><h2 id="41cd" class="nn mm it bd mn nw nx dn mr ny nz dp mv li oa ob mx lm oc od mz lq oe of nb og bi translated">定义搜索算法</h2><p id="cdc9" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">在构建了模型构建器函数之后，我们可以实例化调谐器并指定搜索策略。对于我们的用例，我们将使用超波段算法。Hyperband是一种新颖的基于bandit的方法，专门用于超参数优化。<a class="ae ky" href="https://jmlr.org/papers/v18/16-558.html" rel="noopener ugc nofollow" target="_blank">研究论文</a>发表于2018年，详细描述了一个通过自适应资源分配和提前停止快速收敛于高性能模型的过程。</p><p id="b3fa" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个想法很简单，Hyperband使用了一个体育锦标赛风格的支架，并从搜索空间中随机选择大量具有随机超参数排列的模型开始。每个模型被训练几个时期，只有表现最好的一半模型进入下一轮。</p><p id="327a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">要实例化我们的调谐器，我们需要定义以下超参数:</p><ul class=""><li id="716a" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mk mc md me bi translated">我们的超级模型(由我们的模型生成器函数构建)</li><li id="cc68" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mk mc md me bi translated">目标(方向(最小或最大)将为内置指标自动推断——对于自定义指标，我们可以使用kerastuner。目标)</li><li id="12b4" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mk mc md me bi translated">因子和max_epochs用于通过取max_epochs + 1的对数基础因子来计算每个括号中的模型数量。这个数字被向上舍入到最接近的整数。</li><li id="eb66" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mk mc md me bi translated">超带迭代用于控制您愿意分配给超调的资源预算。Hyperband iterations是迭代整个搜索算法的次数。</li><li id="e1c1" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mk mc md me bi translated">在超参数搜索过程中，目录保存每次试运行的日志和检查点，允许我们从上次停止的地方继续搜索。您可以通过设置额外的超参数“overwrite”= True来禁用此行为。</li><li id="7e11" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mk mc md me bi translated">Project_name用于与其他运行区分开，是目录下的子目录。</li></ul><p id="f8a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">请参考<a class="ae ky" href="https://keras.io/api/keras_tuner/tuners/hyperband/" rel="noopener ugc nofollow" target="_blank">官方文档</a>中所有可用参数的列表。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="7821" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们可以看到搜索空间摘要:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="2a72" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">当指标没有改善时，我们可以设置像提前停止这样的回调来提前停止训练。</p><p id="4218" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们开始搜索吧。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="9d1e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">搜索结束后，我们可以得到最好的超参数，并重新训练模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="e655" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后我们将在测试集上评估我们的超调模型！</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oh"><img src="../Images/9d8f780c2412b53aba157fe238e9b93f.png" data-original-src="https://miro.medium.com/v2/resize:fit:820/format:webp/1*MutTHkRnYeHFWckun4RxSg.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图二。超调模型评估|作者图片</p></figure><p id="c305" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">通过超调，我们发现了一种比我们的基线模型少100，000个参数的架构，它在测试集上的性能略好。如果我们对hyperband算法进行更多的迭代，我们很可能会找到一个性能更好的架构。</p><h1 id="dd89" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">超级网络</h1><p id="416f" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">除了定义我们自己的超模型，Keras Tuner还提供了两个预定义的可调模型，HyperXception和HyperResnet。这些模型搜索以下架构和超参数:</p><ul class=""><li id="b3a4" class="lw lx it lb b lc ld lf lg li ly lm lz lq ma lu mk mc md me bi translated">模型的版本</li><li id="edb7" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mk mc md me bi translated">卷积层的深度</li><li id="dcfd" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mk mc md me bi translated">联营</li><li id="aec0" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mk mc md me bi translated">学习率</li><li id="9ee4" class="lw lx it lb b lc mf lf mg li mh lm mi lq mj lu mk mc md me bi translated">最优化算法</li></ul><p id="6f26" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">让我们看看如何将HyperXception或HyperResnet与我们的调谐器一起使用。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="d463" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们在HyperResnet模型中指定输入形状和类的数量。这次我们将使用贝叶斯优化作为我们的搜索算法，它通过聚焦于更有希望的区域来搜索超参数空间。此外，我们使用不同的project_name，以便我们的调音师可以区分之前的运行。</p><p id="8114" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">接下来，我们必须对数据进行预处理，以符合HyperResnet的要求。HyperResnet希望要素的形状与卷积图层和一次性编码标注的形状相同。</p><p id="b727" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用下面的小代码块，我们可以开始搜索。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="1205" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">同上，我们可以得到最好的超参数，并重新训练模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><p id="1967" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">最后，我们将在测试集上评估模型。</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="nt nu l"/></div></figure><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oi"><img src="../Images/fbd82758c09fbfb3b7b653f9bc6f4a99.png" data-original-src="https://miro.medium.com/v2/resize:fit:868/format:webp/1*LAce7pGDp9qB5x1R_hGLBA.png"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">图3。HyperResNet模型评估|作者图片</p></figure><p id="5fb4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在生产环境中，我们需要考虑的不仅仅是测试集的准确性。最佳部署选项是我们的超调DNN，因为它在测试集上表现稍好，并且参数最少。</p><h1 id="7f34" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">总结</h1><p id="2050" class="pw-post-body-paragraph kz la it lb b lc nd ju le lf ne jx lh li nf lk ll lm ng lo lp lq nh ls lt lu im bi translated">超调是机器学习管道的重要组成部分。在这篇文章中，我们训练了一个基线模型，展示了为什么手动搜索最佳超参数是困难的。我们深入探讨了Keras Tuner以及如何使用它来自动执行超参数搜索。最后，我们超调了一个预定义的超Resnet模型。</p><p id="b37b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">感谢阅读！</p></div><div class="ab cl oj ok hx ol" role="separator"><span class="om bw bk on oo op"/><span class="om bw bk on oo op"/><span class="om bw bk on oo"/></div><div class="im in io ip iq"><p id="c91c" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">请查看我以前发布的更多数据科学项目和教程。</em></p><p id="e114" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="lv">我写的是我进入数据科学和机器学习工程的学习历程，请关注并联系我的</em> <a class="ae ky" href="https://medium.com/@lukenewman_41351" rel="noopener"> <em class="lv">中</em> </a> <em class="lv">，</em> <a class="ae ky" href="https://www.linkedin.com/in/lukenewman-/" rel="noopener ugc nofollow" target="_blank"> <em class="lv"> LinkedIn </em> </a>，或<a class="ae ky" href="https://github.com/lukenew2" rel="noopener ugc nofollow" target="_blank"> <em class="lv"> Github </em> </a> <em class="lv">，我们一起学习！</em></p><h1 id="1765" class="ml mm it bd mn mo mp mq mr ms mt mu mv jz mw ka mx kc my kd mz kf na kg nb nc bi translated">额外资源</h1><div class="oq or gp gr os ot"><a href="https://blog.tensorflow.org/2020/01/hyperparameter-tuning-with-keras-tuner.html" rel="noopener  ugc nofollow" target="_blank"><div class="ou ab fo"><div class="ov ab ow cl cj ox"><h2 class="bd iu gy z fp oy fr fs oz fu fw is bi translated">使用Keras调谐器进行超参数调谐</h2><div class="pa l"><h3 class="bd b gy z fp oy fr fs oz fu fw dk translated">2020年1月29日-由汤姆·欧玛利发布机器学习项目的成功通常关键取决于…</h3></div><div class="pb l"><p class="bd b dl z fp oy fr fs oz fu fw dk translated">blog.tensorflow.org</p></div></div><div class="pc l"><div class="pd l pe pf pg pc ph ks ot"/></div></div></a></div><div class="oq or gp gr os ot"><a href="https://keras.io/guides/keras_tuner/" rel="noopener  ugc nofollow" target="_blank"><div class="ou ab fo"><div class="ov ab ow cl cj ox"><h2 class="bd iu gy z fp oy fr fs oz fu fw is bi translated">Keras文档:超参数调整</h2><div class="pa l"><h3 class="bd b gy z fp oy fr fs oz fu fw dk translated">Keras文档</h3></div><div class="pb l"><p class="bd b dl z fp oy fr fs oz fu fw dk translated">:超参数调整Keras文档keras.io</p></div></div><div class="pc l"><div class="pi l pe pf pg pc ph ks ot"/></div></div></a></div><h2 id="38ba" class="nn mm it bd mn nw nx dn mr ny nz dp mv li oa ob mx lm oc od mz lq oe of nb og bi translated">研究论文</h2><div class="oq or gp gr os ot"><a href="https://jmlr.org/papers/v18/16-558.html" rel="noopener  ugc nofollow" target="_blank"><div class="ou ab fo"><div class="ov ab ow cl cj ox"><h2 class="bd iu gy z fp oy fr fs oz fu fw is bi translated">超带:基于Bandit的超参数优化新方法</h2><div class="pa l"><h3 class="bd b gy z fp oy fr fs oz fu fw dk translated">李丽莎、凯文·贾米森、朱利亚·德萨沃、阿夫申·罗斯塔米扎德、阿米特·塔尔沃卡尔；18(185):1−52, 2018.…的性能</h3></div><div class="pb l"><p class="bd b dl z fp oy fr fs oz fu fw dk translated">jmlr.org</p></div></div><div class="pc l"><div class="pj l pe pf pg pc ph ks ot"/></div></div></a></div></div></div>    
</body>
</html>