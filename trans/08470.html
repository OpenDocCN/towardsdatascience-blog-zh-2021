<html>
<head>
<title>Theory of learning — a Bayesian perspective of generalization</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">学习理论——概括的贝叶斯观点</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/theory-of-learning-a-bayesian-perspective-of-generalization-6e6d34cb1e61?source=collection_archive---------30-----------------------#2021-08-04">https://towardsdatascience.com/theory-of-learning-a-bayesian-perspective-of-generalization-6e6d34cb1e61?source=collection_archive---------30-----------------------#2021-08-04</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="2c34" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/thoughts-and-theory" rel="noopener" target="_blank">思想和理论</a></h2><div class=""/><div class=""><h2 id="2166" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">透过PAC-Bayes的镜头</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/590475d584f158eb4fea50764c4a5f54.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*pMwlBjhXvlcnFy1C"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">由<a class="ae le" href="https://unsplash.com/@christiaanhuynen_designbro?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank">克里斯蒂安·胡宁</a>在<a class="ae le" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="39bc" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi mb translated">收益理论是一门研究训练误差和泛化误差之间差距(又称泛化差距)的学科，它为我们提供了一个经过训练的学习者对新数据的性能保证。多年来，人们已经做了很多工作来捕捉泛化差距，包括但不限于VC维、Rademacher复杂度、算法稳定性。虽然这些工作在欠参数化区域(其中我们拥有的数据比要学习的自由参数的数量多)中给出了泛化差距的清晰界限，但当我们过渡到插值区域(其中我们拥有的自由参数比数据多，以至于我们可以完美地插值数据)时，这些界限变得空洞——随着模型(神经网络)变得越来越复杂，泛化误差不会随着界限的增加而增加，而是呈现下降趋势。虽然这种神秘的泛化行为，也称为双重下降，导致神经网络模型在许多任务上的成功，但寻找理论解释仍然是开放式的。</p><h1 id="1018" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated">概括的贝叶斯观点——实证研究</h1><p id="205c" class="pw-post-body-paragraph lf lg iq lh b li nc ka lk ll nd kd ln lo ne lq lr ls nf lu lv lw ng ly lz ma ij bi translated">在最近的一篇论文“Bayesian Deep Learning and a probabilical Perspective of Generalization”中，作者表明，在假设类的泛化误差相对于模型灵活性(最大容量)单调下降的意义上，甚至对于损坏的数据集，也可以完全缓解双下降现象。模糊地说，实现这一点的关键是更好地逼近数据匹配归纳偏差模型的贝叶斯边缘化(我不会详细介绍论文传达的细节，但可以参考下面的页面，了解关于论文的简明和全面的介绍:<a class="ae le" href="https://jorisbaan.nl/2021/03/02/introduction-to-bayesian-deep-learning.html" rel="noopener ugc nofollow" target="_blank">https://joris baan . nl/2021/03/02/introduction-to-Bayesian-deep-learning . html</a>)。对这些概念的实验确实产生了迷人的结果，给了我们一个支持贝叶斯方法的令人信服的证据。然而，除了实证实验之外，理论支持仍然是缺失的。在本文中，我将通过PAC-Bayes框架给出Bayes边缘化和泛化之间的理论联系。首先，让我来勾画一下总体路线图，以总结实现连接的步骤:</p><ol class=""><li id="fefc" class="nh ni iq lh b li lj ll lm lo nj ls nk lw nl ma nm nn no np bi translated">重述证据下限(ELBO)</li><li id="516f" class="nh ni iq lh b li nq ll nr lo ns ls nt lw nu ma nm nn no np bi translated">引入度量变化不等式作为ELBO的推广</li><li id="1e90" class="nh ni iq lh b li nq ll nr lo ns ls nt lw nu ma nm nn no np bi translated">导出PAC-Bayes界限</li><li id="fa0a" class="nh ni iq lh b li nq ll nr lo ns ls nt lw nu ma nm nn no np bi translated">建立联系</li></ol><h1 id="f505" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated">从ELBO到PAC-Bayes界</h1><h2 id="e713" class="nv ml iq bd mm nw nx dn mq ny nz dp mu lo oa ob mw ls oc od my lw oe of na iw bi translated">证据下限</h2><p id="ad5c" class="pw-post-body-paragraph lf lg iq lh b li nc ka lk ll nd kd ln lo ne lq lr ls nf lu lv lw ng ly lz ma ij bi translated">对于熟悉贝叶斯框架的人来说，众所周知，对数证据可以有如下下界</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi og"><img src="../Images/2ce1397445aea268c3e0064696dea2d3.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2Vd5trdMXn3L-5pw86oQ4w.png"/></div></div></figure><p id="4dbe" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">其中q是考虑的概率模型，q(x|θ)是似然函数，q(θ)是θ的先验分布，q(x)=∫q(x|θ) q(θ) dθ是证据/边际似然。这个不等式对于θ上的任何分布r都是有效的，假定r没有比q更大的支撑。这个不等式的RHS是所谓的ELBO，它是一个紧界，因为当r(θ)等于后验q(θ|x)时，这个不等式可以实现。事实上，测井证据和ELBO之间的差异是从q(θ|x)到r(θ)的KL散度，因此测井证据可以分解如下</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oh"><img src="../Images/ff356cf5d0a3a09e4a00ece84a23615b.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YQddZShZ4VXwBdciXDQmGw.png"/></div></div></figure><h2 id="ae2a" class="nv ml iq bd mm nw nx dn mq ny nz dp mu lo oa ob mw ls oc od my lw oe of na iw bi translated">度量变化不等式</h2><p id="6709" class="pw-post-body-paragraph lf lg iq lh b li nc ka lk ll nd kd ln lo ne lq lr ls nf lu lv lw ng ly lz ma ij bi translated">由于证据满足q(x)=∫q(x|θ) q(θ) dθ，这是q(x|θ) w.r.t. q(θ)的期望。因此，这个不等式可以改写为</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oi"><img src="../Images/2f1492ffefc84143c061e2620f3a110c.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Y9X3qL6LWxQ1oczoxM8JLw.png"/></div></div></figure><p id="a89d" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">事实上，这个不等式不仅适用于对数似然函数，而且适用于任何实值函数，即，</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oj"><img src="../Images/673c2fd8f6d0bcbf4ae5edc9098fc29f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*uLvZhPejS3xib_l6GQbPWg.png"/></div></div></figure><p id="f3fe" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">正如副标题所暗示的，这就是度量变化的不平等。类似于ELBO，这个下界也是紧的，我们可以通过Gibbs后验来达到等式</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ok"><img src="../Images/f7f41ed4bc00f4d6c8008b0944933964.png" data-original-src="https://miro.medium.com/v2/resize:fit:1132/format:webp/1*B-Q0DjXVBwSp4C6V7CzS-g.png"/></div></div></figure><p id="41cf" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">事实上，两者之间的差异就是从吉布斯后验概率到r的KL散度，就像证据分解一样:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ol"><img src="../Images/adbea4142ec8e1e7dec0aaad4454fbe0.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*L9VeDQe5hBeREERr0OlXXw.png"/></div></div></figure><h2 id="0a16" class="nv ml iq bd mm nw nx dn mq ny nz dp mu lo oa ob mw ls oc od my lw oe of na iw bi translated">包装贝叶斯界限</h2><p id="f52c" class="pw-post-body-paragraph lf lg iq lh b li nc ka lk ll nd kd ln lo ne lq lr ls nf lu lv lw ng ly lz ma ij bi translated">现在，我们可以毫不费力地推导出PAC-Bayes界。这里的技巧是用一个将训练误差和泛化误差联系起来的函数来设置<em class="om"> ϕ </em>。一个特别的选择是</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi on"><img src="../Images/e574b4c51e02d6233f15081eb6e19098.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cWS0YYDT03DJiZiNEFlXAQ.png"/></div></div></figure><p id="2bde" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">其中D代表未知数据生成分布，S代表来自D的随机样本iid，R是在一定损失L w R t上评估的风险，S或D分别代表训练误差和泛化误差。至于λ，它是一个自由参数，就像温度的倒数一样，控制着适应度(R_in)和复杂度(KL散度)之间的某种折衷。插入这个<em class="om"> ϕ </em>并重新排列术语，你将得到</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi oo"><img src="../Images/91c626ed905a93f213df72266db8b3a7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*YrsU8UCGTI0xj5XNQ7JD-g.png"/></div></div></figure><p id="4aef" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">通过对RHS上最后一项的对数内的部分应用马尔可夫不等式，可以用它的期望(w.r.t .随机样本S)来代替它，从而消除S引起的随机性，达到一般形式的PAC-Bayes界:</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi op"><img src="../Images/6ac1efadb459485bd38739fe73f50c17.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*y_wiIix0ODQYVc-vS7w-VA.png"/></div></div></figure><h1 id="1bbc" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated">概括的贝叶斯视角——理论观点</h1><h2 id="4c5a" class="nv ml iq bd mm nw nx dn mq ny nz dp mu lo oa ob mw ls oc od my lw oe of na iw bi translated">贝叶斯边缘化=最优PAC-贝叶斯界限</h2><p id="5303" class="pw-post-body-paragraph lf lg iq lh b li nc ka lk ll nd kd ln lo ne lq lr ls nf lu lv lw ng ly lz ma ij bi translated">现在让我们看看如何将贝叶斯概念和PAC-Bayes技术联系起来。选择负对数似然函数作为评估风险的损失，并设置λ为样本数，前两项相加正好为负ELBO(n上的额外分数为1)。这意味着通过对r(θ)=q(θ|x)进行精确边缘化，我们可以最大化这个界限，因为其他两项与r(θ)无关。</p><h2 id="056e" class="nv ml iq bd mm nw nx dn mq ny nz dp mu lo oa ob mw ls oc od my lw oe of na iw bi translated">泛化=支持+归纳偏差</h2><p id="d84a" class="pw-post-body-paragraph lf lg iq lh b li nc ka lk ll nd kd ln lo ne lq lr ls nf lu lv lw ng ly lz ma ij bi translated">在前面提到的“贝叶斯深度学习和泛化的概率观点”一文中，作者认为模型的泛化有两个方面:模型可以相当近似的数据分布范围的<strong class="lh ja">支持度</strong>，以及模型在不同数据分布之间的拟合程度<strong class="lh ja">。通过导出的PAC-Bayes界的透镜，我们给这个论证一个严格的数学意义</strong></p><ol class=""><li id="1ed8" class="nh ni iq lh b li lj ll lm lo nj ls nk lw nl ma nm nn no np bi translated">对于超出支持范围的数据分布，例如，在图像数据集上应用线性模型，我们可能会有小的泛化差距，但我们永远无法实现令人满意的泛化，因为在整个空间中没有很好地接近真实值的权重配置，导致即使对于优化的先验q(θ)也只有小的证据(因此泛化误差界很大)。</li><li id="3e8c" class="nh ni iq lh b li nq ll nr lo ns ls nt lw nu ma nm nn no np bi translated">对于不匹配的归纳偏差，不可约项(PAC-Bayes界的RHS上的第三项)永远不会小(在给定固定模型和数据分布的情况下，减少不可约项的唯一方法是收集更多数据，但只要我们专注于插值范围，这不太可能发生)。</li></ol><h1 id="6cb2" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated">摘要</h1><p id="5efc" class="pw-post-body-paragraph lf lg iq lh b li nc ka lk ll nd kd ln lo ne lq lr ls nf lu lv lw ng ly lz ma ij bi translated">从PAC-Bayes的角度来看，就推广范围而言，更好地逼近Bayes边缘化确实优于任何优化方法的单一解决方案。对于更严格的界限，有大量活跃的研究领域:几何深度学习研究不同架构对不同数据的有效性；差分隐私寻求在q(θ)等上找到“数据分布相关”但“数据独立”的先验。虽然还有很长的路要走，但我们正在逐渐揭开学习中的神秘事物，这真的令人振奋。</p><h1 id="d600" class="mk ml iq bd mm mn mo mp mq mr ms mt mu kf mv kg mw ki mx kj my kl mz km na nb bi translated">参考</h1><p id="ff21" class="pw-post-body-paragraph lf lg iq lh b li nc ka lk ll nd kd ln lo ne lq lr ls nf lu lv lw ng ly lz ma ij bi translated">[1]威尔逊，安德鲁&amp;伊兹迈洛夫，帕维尔。(2020).贝叶斯深度学习和一般化的概率观点。</p><p id="3b3c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">[2]热尔曼，帕斯卡尔&amp;巴赫，弗朗西斯&amp;拉科斯特，亚历山大&amp;拉科斯特-朱利安，西蒙。(2016).PAC-贝叶斯理论符合贝叶斯推理。</p></div></div>    
</body>
</html>