<html>
<head>
<title>Implementing Deep Convolutional Neural Networks in C without External Libraries</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">不用外部库用C语言实现深度卷积神经网络</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/implementing-deep-convolutional-neural-networks-in-c-without-external-libraries-b30464f64d02?source=collection_archive---------9-----------------------#2021-09-17">https://towardsdatascience.com/implementing-deep-convolutional-neural-networks-in-c-without-external-libraries-b30464f64d02?source=collection_archive---------9-----------------------#2021-09-17</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="73da" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">YUV视频超分辨率案例研究</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/c209f70fa417131e6dbd1ea85b9b28d2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/0*UZsgfT1QMbJnm74H"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">罗伯特·库伦尼在<a class="ae ky" href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral" rel="noopener ugc nofollow" target="_blank"> Unsplash </a>上拍摄的照片</p></figure><p id="dac3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">在本帖中，我们将讨论如何在没有任何外部库的情况下，用C语言实现一个预先训练好的深度卷积神经网络(CNN)的推理。此代码是为YUV视频超分辨率应用程序开发的。然而，这些功能是模块化的，稍加修改就可以应用于其他应用程序和网络结构。由于这段代码是用纯C编写的，没有任何外部库，因此可以很容易地集成到其他框架中。在这篇文章中，假设读者熟悉C编程和CNN的基本主题。在下文中，首先，我们简要讨论YUV视频格式和用于超分辨率的theFSRCNN算法。然后更详细地讨论了用于YUV视频超分辨率的这种深度CNN的实现。代码可以在<a class="ae ky" href="https://github.com/miladabd/DeepC" rel="noopener ugc nofollow" target="_blank">这里</a>找到。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="8b4e" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated"><strong class="ak"> YUV 4:2:0视频格式</strong></h1><p id="2d15" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">YUV是和RGB一样的色彩空间。在YUV视频中，每一帧都有三个组成部分。y是亮度分量，它是R、G和B分量的加权平均值:</p><p id="6752" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="mz"> Y = k₁R + k₂G + k₃B </em></p><p id="420b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">使用U和V分量作为色差或色度来表示颜色信息。YUV等亮度/色度系统的一个主要优势是兼容黑白模拟电视，因为Y分量保存所有黑白数据。然而，YUV色彩空间相对于RGB的主要优势在于，U和V可以用比Y更低的分辨率来表示，因为人类视觉系统(HVS)对色彩的敏感度低于亮度。这减少了表示色度分量所需的数据量，而没有明显的视频质量差异。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi na"><img src="../Images/468423ff90eb9858b0b8024dfd617d9d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*PXNJ7_3wcQaSo_1Vi3YESQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">RGB和YUV420色彩空间中4×4像素网格的组件数。(图片由作者提供，灵感来自<a class="ae ky" href="https://www.wiley.com/en-gb/Video+Codec+Design%3A+Developing+Image+and+Video+Compression+Systems-p-9780471485537" rel="noopener ugc nofollow" target="_blank">视频编解码器设计</a>)</p></figure><p id="b5d4" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">YUV的一种流行模式是4:2:0(也称为YUV420)，这意味着U和V分量的水平和垂直分辨率都是y的一半。例如，对于4×4像素网格，RGB使用8×16×3位，而YUV420使用8×(16+4+4)。这样，YUV420视频平均每像素使用12位，而在RGB色彩空间中每像素使用24位。这意味着节省50%的带宽，而对质量没有明显的影响。</p><p id="a33e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">像HEVC或H.264/AVC这样的视频编码器对这种YUV420视频进行编码，压缩后的输出用于存储或通信。为了播放视频，视频播放器解码压缩的比特流。在视频分辨率较低的情况下，我们可以在播放视频之前使用超分辨率(SR)方法人工提高视频的分辨率。接下来，我们将讨论一种简单而有效的SR方法。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="3426" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated"><strong class="ak">使用FSRCNN的视频超分辨率</strong></h1><p id="6f80" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">快速超分辨率卷积神经网络(<a class="ae ky" href="https://arxiv.org/abs/1608.00367" rel="noopener ugc nofollow" target="_blank"> FSRCNN </a>)是一种简单而有效的算法，用于将低分辨率(LR)输入图像映射到高分辨率(HR)图像。它包括五个主要部分，前四部分是卷积层，第五部分是去卷积(逆卷积)。为简单起见，我们使用Conv( <em class="mz"> k </em>，<em class="mz"> o </em>，<em class="mz"> i </em>)来表示一个卷积层，其内核大小为<em class="mz"> k </em>，<em class="mz"> i </em>个输入通道，以及<em class="mz"> o </em>个滤波器。与DeConv <em class="mz"> (k，o，i) </em>相同的符号用于去卷积层(逆卷积)。</p><ul class=""><li id="75b4" class="nb nc it lb b lc ld lf lg li nd lm ne lq nf lu ng nh ni nj bi translated">第一部分是提取LR特征的<strong class="lb iu">特征提取器</strong>。</li><li id="c057" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">第二部分是<strong class="lb iu">收缩</strong>以降低LR特征的维度。</li><li id="257d" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated"><strong class="lb iu">非线性映射</strong>由4个卷积层组成，将LR特征映射到HR特征。</li><li id="04f2" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated"><strong class="lb iu">扩展</strong>增加HR特征的尺寸，使其为重建做好准备。</li><li id="19c8" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">最后，<strong class="lb iu">去卷积</strong>用一组去卷积滤波器对HR特征进行上采样和聚合。</li></ul><p id="79c2" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">参数整流线性单元(<strong class="lb iu"> PReLU </strong>)用作激活功能，定义如下:</p><p id="f5bb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><em class="mz">f(xᵢ)= max(xᵢ,0)+aᵢmin(xᵢ,0)</em></p><p id="7433" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">参数<em class="mz"> aᵢ </em>是可学习的负部分的系数。PReLU用于避免死特征。连接这五个部分形成一个八层网络:Conv(5，56，1) —普雷卢— Conv(1，12，56) —普雷卢— 4 × {Conv(3，12，12) —普雷卢} — Conv(1，56，12) —普雷卢—德孔夫(9，1，12)。有关FSRCNN设计细节的更多信息，请参考<a class="ae ky" href="https://arxiv.org/abs/1608.00367" rel="noopener ugc nofollow" target="_blank">原文</a>。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="9e54" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated"><strong class="ak">用C语言实现</strong></h1><p id="435d" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">我们对YUV420视频的Y分量应用FSRCNN。由于HVS对U和V的敏感度较低，因此可以使用双三次等更简单的插值算法对这些分量进行上采样。因此，在这篇文章中，我们关注于在C上实现CNN，对于上采样U和V分量，我们简单地重复现有的元素来填充未知的位置。为了在C中实现预训练深度CNN的推断，我们需要执行以下步骤:</p><ul class=""><li id="c11a" class="nb nc it lb b lc ld lf lg li nd lm ne lq nf lu ng nh ni nj bi translated">读取输入数据</li><li id="429a" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">实施所需的操作(Conv、DeConv、PReLU等)。)</li><li id="1c44" class="nb nc it lb b lc nk lf nl li nm lm nn lq no lu ng nh ni nj bi translated">读取预训练网络的权重</li></ul><h2 id="58c2" class="np md it bd me nq nr dn mi ns nt dp mm li nu nv mo lm nw nx mq lq ny nz ms oa bi translated">用C语言读取YUV视频</h2><p id="edd3" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">为了用C读取YUV文件，我们需要知道视频的维度(宽度和高度)。那么YUV文件可以被认为是一个二进制文件，其中对于每一帧，首先是Y分量的数据，然后是U和V分量的数据。空间分辨率为<em class="mz"> m×n </em>的视频示例如下所示:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ob"><img src="../Images/ace0b3b0cd06c090e3d2b64b4db674ef.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*X_9pVb3ac9WZr7q4xxv-BA.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">YUV视频作为二进制文件。在具有m×n空间分辨率的视频中，对于每一帧，第一个m×n字节的数据属于Y分量，然后随后的两个m×n/4字节分别属于U和V分量。(图片由作者提供)</p></figure><p id="141e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">然后我们可以使用C语言的<a class="ae ky" href="https://www.delftstack.com/howto/c/read-binary-file-in-c/" rel="noopener ugc nofollow" target="_blank">函数读取二进制文件。以下代码片段显示了如何从终端获取输入YUV视频的名称，并返回一个指示视频文件开头的指针:</a></p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><h2 id="f294" class="np md it bd me nq nr dn mi ns nt dp mm li nu nv mo lm nw nx mq lq ny nz ms oa bi translated">实施所需的操作</h2><p id="3428" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">因为我们是用纯C实现CNN的，所以我们需要实现所需的函数，包括卷积、填充、反卷积和PReLU。</p><p id="6adb" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">卷积。</strong>我们不会深入研究卷积运算符的数学细节，但会尝试回忆它如何以矩阵形式处理图像。然后将其更改为二进制数据情况，其中所有行都连接在一起并形成一个向量。在2D卷积中，有一个称为核的权重矩阵。这个内核在2D数据上“滑动”,对它当前所在的输入部分执行逐元素乘法，然后将结果相加到单个输出像素中。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div class="gh gi oe"><img src="../Images/9ed7e0773dbe6df4a62428c7612b085d.png" data-original-src="https://miro.medium.com/v2/resize:fit:1070/1*8s0ZtaNnpIkhdiZNQO-t2Q.gif"/></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">一个简单的2D卷积运算</p></figure><p id="8e4b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们实现的挑战部分是，我们没有数据矩阵，只有一个指向二进制文件开头的指针。因此，我们需要正确地使用这个指针来模拟2D卷积中出现的确切情况。下图显示了一个将卷积内核应用于二进制文件的示例，其中输入数据为4 × 4，内核为3 × 3:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi of"><img src="../Images/8c789b50569b9080d7ad3b013d0b82ad.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*lOTSAZe9mgNVmNVBy9tfZQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">传统的2D卷积映射到二进制文件的矢量化空间(图片由作者提供)。</p></figure><p id="94fd" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">我们将该卷积运算实现为以下imfilter函数:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="b2a3" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这个函数接受以下指针作为输入:*img，*kernel和*img_fltr。例如，*img指向内存中输入图像的开始元素。使用这个指针，我们可以通过添加偏移值来访问输入图像的每个元素。例如，输入图像的第三个元素(2D空间中的行0，列2)可以由*(img+2)访问。请注意，因为我们使用指针修改值，所以imfilter函数隐式地应用更改，并且不返回任何内容(void函数)。</p><p id="3947" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">为了开始这个过程，我们首先在卷积之前填充输入图像。为此，我们使用<a class="ae ky" href="https://github.com/miladabd/DeepC/blob/2a5286e8dc1a052f8327e65f3bd93bfb9b867ec5/source.c#L715" rel="noopener ugc nofollow" target="_blank"> pad_image </a>函数，该函数简单地重复填充位置的边界像素值。假设<em class="mz"> kernel_size=k，</em>和<em class="mz"> padsize=(k-1)/2 </em>。然后，对于输入图像中的每个像素位置，我们确定一个<em class="mz"> k×k </em>区域，该像素位于该区域的中心。由于每个帧的raw都连接在二进制文件中，因此我们的代码中该像素的偏移值将是<em class="mz">CNT =(I-padsize)* cols+(j-padsize)</em>。接下来，我们在内核和确定的区域之间执行逐元素乘法。为此，我们循环遍历内核的位置，并以相同的方式找到相邻像素的偏移值:<em class="mz">CNT _ pad =(I+k1)* cols _ pad+j+k2</em>。然后可以通过<em class="mz"> *(img_pad+cnt_pad) </em>访问每个邻居像素的值。为了访问内核中的值，我们使用一个简单的增量计数器<em class="mz"> cnt_krnl </em>。</p><p id="d04b" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">预科。</strong>该函数非常简单，因为它对每个像素值独立执行。以下代码片段显示了我们对YUV过滤帧的PReLU的C实现:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="504e" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated"><strong class="lb iu">反卷积。</strong>fsr CNN中使用的反卷积层可以认为是反卷积过程。对于卷积，如果滤波器以步幅<strong class="lb iu"><em class="mz"/></strong>与图像进行卷积，输出将是输入的<strong class="lb iu"> <em class="mz"> 1/s </em> </strong>倍。那么，如果我们交换输入和输出的位置，输出将是输入的<strong class="lb iu"> <em class="mz"> s </em> </strong>倍。通过选择<strong class="lb iu"> s </strong> <em class="mz"> </em>作为期望的放大因子(在我们的实现中为2)，输出将直接是HR图像。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi og"><img src="../Images/729da1b6ce4f27952fd5f3883214c2c9.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*luNi9CsrDRHYs898Vi1eig.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">反卷积可以被认为是将步长参数设置为放大因子的反卷积。(图片由作者提供，灵感来自<a class="ae ky" href="https://arxiv.org/abs/1608.00367" rel="noopener ugc nofollow" target="_blank"> FSRCNN </a>)</p></figure><p id="334f" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">考虑到这一点，我们可以实现类似于imfilter函数的反卷积，但方式相反。C语言中反卷积函数的代码可以在这里找到<a class="ae ky" href="https://github.com/miladabd/DeepC/blob/2a5286e8dc1a052f8327e65f3bd93bfb9b867ec5/source.c#L814" rel="noopener ugc nofollow" target="_blank">。</a></p><h2 id="cfa9" class="np md it bd me nq nr dn mi ns nt dp mm li nu nv mo lm nw nx mq lq ny nz ms oa bi translated">读取预训练网络的参数</h2><p id="0c68" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">在实现所需的功能后，下一步将是在C中加载预训练网络的权重。由于在深度学习框架(如PyTorch或TensorFlow)中训练的网络的保存文件格式在C语言中不受支持，我们可以使用一个简单的技巧来加载参数。我们将网络的参数(包括权重和偏差)保存为<em class="mz"> *。txt </em>文件，并使用fread在c中访问它们。以下代码片段是读取第一层的权重和偏差的示例:</p><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="oc od l"/></div></figure><p id="9b46" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">重要的一点是，由于权重是3D张量格式，我们应该按行连接它们。转换后的权重和偏差可以在GitHub页面上以txt文件的形式获得。</p></div><div class="ab cl lv lw hx lx" role="separator"><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma mb"/><span class="ly bw bk lz ma"/></div><div class="im in io ip iq"><h1 id="c20f" class="mc md it bd me mf mg mh mi mj mk ml mm jz mn ka mo kc mp kd mq kf mr kg ms mt bi translated">把所有东西放在一起，然后编译</h1><p id="eb6f" class="pw-post-body-paragraph kz la it lb b lc mu ju le lf mv jx lh li mw lk ll lm mx lo lp lq my ls lt lu im bi translated">在实现所需的功能，并找到读取网络参数的方法后，我们通过调用<a class="ae ky" href="https://github.com/miladabd/DeepC/blob/main/source.c" rel="noopener ugc nofollow" target="_blank">主函数</a>中所需的功能来实现网络的不同层。作为一个例子，我们在<a class="ae ky" href="http://trace.eas.asu.edu/yuv/" rel="noopener ugc nofollow" target="_blank"> foreman_qcif.yuv </a>视频上运行这个。为了在Linux终端中编译和运行代码，我们使用以下命令:</p><pre class="kj kk kl km gt oh oi oj ok aw ol bi"><span id="c1dd" class="np md it oi b gy om on l oo op">gcc source.c -o videosr<br/>./videosr foreman_qcif_146x144.yuv output_foreman_352x288.yuv</span></pre><p id="cf4a" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">这是一个看起来像这样的例子:</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi oq"><img src="../Images/dd9507729e9521a0704578a19cea69d4.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/1*f6Oor8Aih9ViBoVCrzPF-A.gif"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">左:输入视频，右:上采样视频。(图片由作者提供)</p></figure><p id="3a48" class="pw-post-body-paragraph kz la it lb b lc ld ju le lf lg jx lh li lj lk ll lm ln lo lp lq lr ls lt lu im bi translated">注意，你需要一个YUV视频播放器来播放这些视频！</p></div></div>    
</body>
</html>