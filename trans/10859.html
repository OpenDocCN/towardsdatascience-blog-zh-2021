<html>
<head>
<title>MLOps with Kubernetes, RabbitMQ and FastAPI</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">使用Kubernetes、RabbitMQ和FastAPI的MLOps</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/mlops-with-kubernetes-rabbitmq-and-fastapi-b67d82e35fa4?source=collection_archive---------8-----------------------#2021-10-21">https://towardsdatascience.com/mlops-with-kubernetes-rabbitmq-and-fastapi-b67d82e35fa4?source=collection_archive---------8-----------------------#2021-10-21</a></blockquote><div><div class="fc ih ii ij ik il"/><div class="im in io ip iq"><div class=""/><div class=""><h2 id="94e3" class="pw-subtitle-paragraph jq is it bd b jr js jt ju jv jw jx jy jz ka kb kc kd ke kf kg kh dk translated">Skipper是一个简单灵活的开源ML工作流引擎。它有助于在生产中运行和扩展ML服务。</h2></div><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi ki"><img src="../Images/509e0dba531a3fc1bd60a68eac7705c2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*2M1FwX90VENoligIaVWsrQ.jpeg"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">作者:安德烈·巴拉诺夫斯基</p></figure><p id="0266" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">你经常会听到人们说——许多ML项目在进入生产阶段之前就停止了。其中一个原因是，典型的ML项目从一开始就被实现为一个整体，当在生产中运行它们的时候，管理、转换和维护代码是不可能的。ML项目代码被实现为几个甚至一个大笔记本，其中数据处理、模型训练和预测都被粘在一起。这使得在需要修改代码和引入用户请求时，很难维护如此繁琐的代码。结果，用户不高兴，这导致项目终止。</p><p id="a73a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">从一开始就构建ML系统并遵循微服务架构会有效得多。您可以使用容器来封装逻辑。可以有单独的容器用于数据处理、ML模型训练和ML模型服务。当运行单独的容器时，不仅简化了代码维护，而且您还可以单独伸缩容器并在不同的硬件上运行它们。这可以提高系统性能。</p><p id="b34b" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">问题来了，如何实现这些服务之间的通信。我正在研究可用的工具，比如<a class="ae lu" href="https://mlflow.org" rel="noopener ugc nofollow" target="_blank"> MLFlow </a>。这些工具很棒，但是对于任务来说，它们通常太复杂太庞大。特别是当你想简单地在不同的容器中运行ML逻辑时，差不多就是这样。这就是为什么我决定构建自己的小型简单开源产品<a class="ae lu" href="https://github.com/katanaml/katana-skipper" rel="noopener ugc nofollow" target="_blank"> Skipper </a>来运行ML工作负载。</p><p id="bc9a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在本文中，我将解释如何使用Skipper在Kubernetes上扩展TensorFlow模型。同样的方法可以应用于PyTorch模型或任何其他与ML无关的功能。</p><figure class="kj kk kl km gt kn gh gi paragraph-image"><div role="button" tabindex="0" class="ko kp di kq bf kr"><div class="gh gi lv"><img src="../Images/3c8e4939b6643fc1807bcc5af204186f.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*-utv6THRxHn71PtqGkRZSg.png"/></div></div><p class="ku kv gj gh gi kw kx bd b be z dk translated">Skipper structure，作者:Andrej Baranovskij</p></figure><ul class=""><li id="7826" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated">公共端口通过<a class="ae lu" href="https://www.nginx.com" rel="noopener ugc nofollow" target="_blank"> Nginx </a>暴露</li><li id="db1b" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated"><a class="ae lu" href="https://fastapi.tiangolo.com" rel="noopener ugc nofollow" target="_blank"> FastAPI </a>正在服务REST端点API。目前提供两个通用端点，一个用于异步请求，另一个用于同步</li><li id="a5b5" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">负责请求路由的工作流容器</li><li id="2f5d" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">记录器容器提供通用的日志记录功能</li><li id="89d3" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated"><a class="ae lu" href="https://docs.celeryproject.org/en/stable/getting-started/introduction.html" rel="noopener ugc nofollow" target="_blank">芹菜</a>容器用于执行异步请求</li><li id="5bc1" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">RabbitMQ是一个消息代理，它支持Skipper容器之间基于事件的通信</li><li id="5102" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">SkipperLib是一个Python库，它封装了特定于RabbitMQ的API代码</li><li id="9fda" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">创建了一组微服务作为样本容器，以展示Skipper如何处理特定于ML(也可以是非ML)的服务</li></ul><h1 id="f9fa" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">REST API</h1><p id="805d" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">您可以通过多种方式运行Skipper容器:</p><ul class=""><li id="daa5" class="lw lx it la b lb lc le lf lh ly ll lz lp ma lt mb mc md me bi translated">直接与Python虚拟环境在你的机器上</li><li id="cdef" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">通过Docker compose在Docker容器上。按照自述文件中的说明进行操作</li><li id="ccb8" class="lw lx it la b lb mf le mg lh mh ll mi lp mj lt mb mc md me bi translated">在库伯内特。按照自述文件中的说明进行操作</li></ul><p id="15f7" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在生产环境中，您应该在Kubernetes中运行Skipper，使用Kubernetes可以更容易地扩展容器。</p><p id="5cef" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">Skipper REST API通过Kubernetes NGINX入口控制器公开:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="870e" class="nm ml it ni b gy nn no l np nq">apiVersion: networking.k8s.io/v1<br/>kind: Ingress<br/>metadata:<br/>  name: api-ingress<br/>spec:<br/>  rules:<br/>    - host: kubernetes.docker.internal<br/>      http:<br/>        paths:<br/>          - path: /api/v1/skipper/tasks/<br/>            pathType: Prefix<br/>            backend:<br/>              service:<br/>                name: skipper-api<br/>                port:<br/>                  number: 8000<br/>  ingressClassName: nginx<br/><br/>---<br/><br/>apiVersion: networking.k8s.io/v1<br/>kind: IngressClass<br/>metadata:<br/>  name: nginx<br/>spec:<br/>  controller: k8s.io/ingress-nginx</span></pre><p id="e5a5" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">入口重定向到/api/v1/skipper/tasks/</p><p id="1c49" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">定义了两个通用端点。一个服务异步请求，另一个服务同步请求。异步请求执行对训练模型的调用，这是一个长期运行的任务。同步请求处理模型预测调用，并将请求路由到模型服务。所有请求都经过RabbitMQ消息队列。</p><p id="d3d8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">所有对RabbitMQ的调用都是通过SkipperLib执行的。这允许将RabbitMQ特定的代码封装在库中并对其进行修改，而无需接触容器中的代码。</p><h1 id="45e3" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">模特培训</h1><p id="dab2" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">用于模型训练的Kubernetes Pod运行两个容器。首先是一个主容器，它训练一个模型。第二个是边车容器，负责数据准备和处理。</p><p id="f624" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">我们在同一个Pod中运行这两个容器，因为不需要分别扩展它们，并且当它们在同一个Pod中运行时，在容器之间共享数据更方便。这对于预测逻辑来说是不正确的，我们在单独的Pod中运行预测逻辑，以便能够单独对其进行扩展，下一章将对此进行详细介绍。</p><p id="4746" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">模型训练和数据准备容器共享相同的Kubernetes卷用于数据存储。</p><p id="009a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">模型培训容器:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="4883" class="nm ml it ni b gy nn no l np nq">volumeMounts:<br/>  - name: data<br/>    mountPath: /usr/src/trainingservice/models</span></pre><p id="4d14" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">数据准备容器:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="1d93" class="nm ml it ni b gy nn no l np nq">volumeMounts:<br/>  - name: data<br/>    mountPath: /usr/src/dataservice/models</span></pre><p id="ab06" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">挂载路径不同，但目标位置相同。通过这些路径访问的数据将是相同的。因为两个容器使用相同的卷“数据”:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="2cbe" class="nm ml it ni b gy nn no l np nq">volumes:<br/>- name: data<br/>  persistentVolumeClaim:<br/>    claimName: training-service-claim</span></pre><p id="7eb8" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当模型被训练并且模型文件被保存时，我们需要在模型预测容器运行时将它传输到服务Pod。解决方案之一是使用外部云存储，并在那里上传模型文件。但是如果模型文件不太大，Skipper允许将它直接从训练舱转移到服务舱。模型被存档、编码成一个字符串、与其他元数据一起打包成JSON，并被发送到RabbitMQ队列以交付给serving Pod。</p><p id="a8f0" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">模型结构被归档到一个文件中:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="4842" class="nm ml it ni b gy nn no l np nq">shutil.make_archive(base_name=os.getenv('MODELS_FOLDER') + str(ts),<br/>                    format='zip',<br/>                    root_dir=os.getenv('MODELS_FOLDER') + str(ts))</span></pre><p id="2a6a" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">存档的模型文件被编码成base64字符串:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="5006" class="nm ml it ni b gy nn no l np nq">model_encoded = None<br/>try:<br/>    with open(os.getenv('MODELS_FILE'), 'rb') as model_file:<br/>        model_encoded = base64.b64encode(model_file.read())<br/>except Exception as e:<br/>    print(str(e))</span></pre><p id="8462" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">在最后一步中，我们将所有东西都包装到JSON中:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="878e" class="nm ml it ni b gy nn no l np nq">data = {<br/>    'name': 'model_boston_' + str(ts),<br/>    'archive_name': 'model_boston_' + str(ts) + '.zip',<br/>    'model': model_encoded,<br/>    'stats': stats_encoded,<br/>    'stats_name': 'train_stats.csv'<br/>}<br/>content = json.dumps(data)</span></pre><p id="2a87" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">该消息被提交给RabbitMQ进行传递。模型通过RabbitMQ上的“扇出”交换发送，这允许一次向所有订户发送相同的数据。默认情况下，RabbitMQ会一次向一个订阅者发送消息，这在集群中作为负载平衡非常有效。但在这种情况下，我们希望集群中的所有接收器都获得新模型，这就是我们使用“扇出”交换的原因。</p><p id="ffd6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">这是消息通过RabbitMQ发布到“扇出”交换的方式:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="18cd" class="nm ml it ni b gy nn no l np nq">credentials = pika.PlainCredentials(self.username, self.password)<br/>connection = pika.BlockingConnection(<br/>    pika.ConnectionParameters(host=self.host,<br/>                              port=self.port,<br/>                              credentials=credentials))</span><span id="3d77" class="nm ml it ni b gy nr no l np nq">channel = connection.channel()<br/>channel.exchange_declare(exchange='skipper_storage',<br/>                         exchange_type='fanout')</span><span id="ee5f" class="nm ml it ni b gy nr no l np nq">channel.basic_publish(exchange='skipper_storage', <br/>                      routing_key='', <br/>                      body=payload)<br/>connection.close()</span></pre><h1 id="15c4" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">模型服务</h1><p id="fb02" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">用于模型服务的Kubernetes Pod运行两个容器。主容器负责使用TensorFlow API执行预测请求。当新的模型文件被发送时，边车容器监听来自RabbitMQ的消息，解码文件并提取模型。</p><p id="12c6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">两个容器共享同一个存储空间。</p><p id="edbc" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">服务容器:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="aeaa" class="nm ml it ni b gy nn no l np nq">volumeMounts:<br/>  - name: data<br/>    mountPath: /usr/src/servingservice/models/serving</span></pre><p id="ed26" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">用于模型文件处理的边车容器:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="4913" class="nm ml it ni b gy nn no l np nq">volumeMounts:<br/>  - name: data<br/>    mountPath: /usr/src/servingservice/storage/models/serving/</span></pre><p id="3867" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">存储被装载到同一个卷声明:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="ef71" class="nm ml it ni b gy nn no l np nq">volumes:<br/>- name: data<br/>  persistentVolumeClaim:<br/>    claimName: serving-service-claim</span></pre><p id="c209" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">当负责模型文件处理的容器接收到模型时，它会执行与模型训练窗格中的容器类似的步骤，其中模型准备通过RabbitMQ发送:</p><pre class="kj kk kl km gt nh ni nj nk aw nl bi"><span id="8874" class="nm ml it ni b gy nn no l np nq">data_json = json.loads(data)<br/><br/>model_name = data_json['name']<br/>archive_name = data_json['archive_name']<br/>stats_name = data_json['stats_name']<br/>model_decoded = base64.b64decode(data_json['model'])<br/>stats_decoded = base64.b64decode(data_json['stats'])</span></pre><p id="f0c6" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">它解码字符串，提取文件。</p><p id="bb83" class="pw-post-body-paragraph ky kz it la b lb lc ju ld le lf jx lg lh li lj lk ll lm ln lo lp lq lr ls lt im bi translated">模型服务单元可以扩展到多个实例。如果实例运行在不同的集群节点上，那么每个节点都会收到来自RabbitMQ消息的新模型。但是，如果几个实例将在单个节点上运行，它们都将尝试将模型写入同一个存储中。如果其中一个实例失败，我们将处理异常。</p><h1 id="d650" class="mk ml it bd mm mn mo mp mq mr ms mt mu jz mv ka mw kc mx kd my kf mz kg na nb bi translated">结论</h1><p id="2f5e" class="pw-post-body-paragraph ky kz it la b lb nc ju ld le nd jx lg lh ne lj lk ll nf ln lo lp ng lr ls lt im bi translated">本文的目标是介绍Skipper。我们面向MLOps的开源产品。目前，该产品已准备好投入生产使用。我们的目标是进一步增强它，特别是添加FastAPI安全配置，添加更复杂的工作流支持和改进日志记录。我们计划测试Skipper的Kubernetes自动缩放功能。我们使用Skipper平台来实现我们的ML服务。</p><h2 id="6c93" class="nm ml it bd mm ns nt dn mq nu nv dp mu lh nw nx mw ll ny nz my lp oa ob na oc bi translated">源代码</h2><ul class=""><li id="9a96" class="lw lx it la b lb nc le nd lh od ll oe lp of lt mb mc md me bi translated">Skipper <a class="ae lu" href="https://github.com/katanaml/katana-skipper" rel="noopener ugc nofollow" target="_blank"> GitHub </a> repo。按照自述文件获取设置说明</li></ul><h2 id="7466" class="nm ml it bd mm ns nt dn mq nu nv dp mu lh nw nx mw ll ny nz my lp oa ob na oc bi translated">YouTube教程</h2><figure class="kj kk kl km gt kn"><div class="bz fp l di"><div class="og oh l"/></div></figure></div></div>    
</body>
</html>