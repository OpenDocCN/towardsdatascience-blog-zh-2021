<html>
<head>
<title>40 Open-Source Audio Datasets for ML</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">40个用于ML的开源音频数据集</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/40-open-source-audio-datasets-for-ml-59dc39d48f06?source=collection_archive---------1-----------------------#2021-11-16">https://towardsdatascience.com/40-open-source-audio-datasets-for-ml-59dc39d48f06?source=collection_archive---------1-----------------------#2021-11-16</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><figure class="ip iq gp gr ir is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi io"><img src="../Images/4d1b7916e60fc2cfa628979cae36fcfa.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8K8QOqA-KhffF_Rxtxf5lA.png"/></div></div></figure><div class=""/><div class=""><h2 id="f72f" class="pw-subtitle-paragraph jy ja jb bd b jz ka kb kc kd ke kf kg kh ki kj kk kl km kn ko kp dk translated">超过2tb的带标签的音频数据集在DagsHub上公开提供并可解析</h2></div><p id="0486" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">十月已经过去，DagsHub的Hacktoberfest挑战赛也结束了。当宣布挑战时，我们没有想到我们会带着近<a class="ae lm" href="https://github.com/DAGsHub/audio-datasets" rel="noopener ugc nofollow" target="_blank"> 40个新的音频数据集</a>到达终点线，这些数据集在DagsHub上公开提供并可解析！我们的社区创造了奇迹，在如此短的时间内完成了如此出色的工作，这是巨大的荣誉。同时，感谢<a class="ae lm" href="https://www.digitalocean.com/" rel="noopener ugc nofollow" target="_blank">数字海洋</a>、<a class="ae lm" href="https://github.com/" rel="noopener ugc nofollow" target="_blank"> GitHub </a>和<a class="ae lm" href="https://about.gitlab.com/" rel="noopener ugc nofollow" target="_blank"> GitLab </a>组织此次活动。</p><p id="9bed" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">今年，我们将重点放在音频领域。为此，我们改进了DagsHub的音频目录功能。现在，您可以收听DagsHub上托管的样本，而无需在本地下载任何内容。对于每个样本，您可以获得额外的信息，如波形、光谱图和文件元数据。最后但并非最不重要的一点是，数据集由DVC进行版本化，因此易于改进并随时可用。</p><figure class="lo lp lq lr gt is gh gi paragraph-image"><div role="button" tabindex="0" class="it iu di iv bf iw"><div class="gh gi ln"><img src="../Images/ebf49629b8ae6bd334f2ed6bbb50eea2.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/0*61uDe2Rk0SnaptSP.png"/></div></div><p class="ls lt gj gh gi lu lv bd b be z dk translated">作者图片</p></figure><p id="9941" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">为了让音频从业者更容易找到他们正在寻找的数据集，我们收集了Hacktoberfest对这篇文章的所有贡献。我们有来自七个(！idspnonenote)的数据集。)不同的语言，不同的领域，不同的来源。如果您对这里缺少的数据集感兴趣，请<a class="ae lm" href="https://discord.gg/qyFqqnwg" rel="noopener ugc nofollow" target="_blank">让我们知道</a>，我们将确保添加它。</p><p id="7fcd" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">尽管为期一个月的虚拟节日已经结束，我们仍然欢迎对开源数据科学的贡献。如果你想丰富DagsHub上的音频数据集，我们很乐意在这个过程中支持你！请通过我们的<a class="ae lm" href="https://discord.gg/qyFqqnwg" rel="noopener ugc nofollow" target="_blank"> Discord </a>频道了解更多详情。</p><p id="937b" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">2022年黑客啤酒节上见🍻</p></div><div class="ab cl lw lx hu ly" role="separator"><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb mc"/><span class="lz bw bk ma mb"/></div><div class="ij ik il im in"><h1 id="9bcb" class="md me jb bd mf mg mh mi mj mk ml mm mn kh mo ki mp kk mq kl mr kn ms ko mt mu bi translated"><a class="ae lm" href="https://dagshub.com/kingabzpro/Acted-Emotional-Speech-Dynamic-Database" rel="noopener ugc nofollow" target="_blank">表演情感语音动态数据库</a></h1><p id="96fa" class="pw-post-body-paragraph kq kr jb ks b kt mv kc kv kw mw kf ky kz mx lb lc ld my lf lg lh mz lj lk ll ij bi translated">表演情感语音动态数据库(AESDD)是一个公开可用的语音情感识别数据集。它包含了用希腊语表达情感的话语。它被分为两个主要类别，一个包含行为情感言语的话语，另一个控制自发的情感言语。你可以通过<a class="ae lm" href="https://speechemotionrecognition.xyz/" rel="noopener ugc nofollow" target="_blank">向网站提交情感演讲的录音</a>来为这个数据集做出贡献。它们将被验证并公开提供用于非商业研究目的。</p><ul class=""><li id="f3a6" class="na nb jb ks b kt ku kw kx kz nc ld nd lh ne ll nf ng nh ni bi translated">供稿:<a class="ae lm" href="https://www.linkedin.com/in/1abidaliawan/" rel="noopener ugc nofollow" target="_blank">阿比德·阿里·阿万</a></li><li id="32e3" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated"><a class="ae lm" href="http://m3c.web.auth.gr/research/aesdd-speech-emotion-recognition/" rel="noopener ugc nofollow" target="_blank">原始数据集</a></li></ul><h1 id="e031" class="md me jb bd mf mg no mi mj mk np mm mn kh nq ki mp kk nr kl mr kn ns ko mt mu bi translated"><a class="ae lm" href="https://dagshub.com/mert.bozkirr/Arabic-Speech-Corpus" rel="noopener ugc nofollow" target="_blank">阿拉伯语语音语料库</a></h1><p id="2447" class="pw-post-body-paragraph kq kr jb ks b kt mv kc kv kw mw kf ky kz mx lb lc ld my lf lg lh mz lj lk ll ij bi translated">阿拉伯语语音语料库是南安普敦大学的Nawar Halabi博士工作的一部分。该语料库是使用专业工作室用南黎凡特阿拉伯语(大马士革口音)录制的。使用该语料库作为输出的合成语音产生了高质量的自然声音。</p><ul class=""><li id="a9fa" class="na nb jb ks b kt ku kw kx kz nc ld nd lh ne ll nf ng nh ni bi translated">供稿人:<a class="ae lm" href="https://www.linkedin.com/in/mertbozkir/" rel="noopener ugc nofollow" target="_blank">梅尔特·博兹克尔</a></li><li id="8621" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated"><a class="ae lm" href="http://en.arabicspeechcorpus.com/" rel="noopener ugc nofollow" target="_blank">原始数据集</a></li></ul><h1 id="3c38" class="md me jb bd mf mg no mi mj mk np mm mn kh nq ki mp kk nr kl mr kn ns ko mt mu bi translated"><a class="ae lm" href="https://dagshub.com/L-theorist/Att-HACK" rel="noopener ugc nofollow" target="_blank"> Att-hack:法语表达演讲</a></h1><p id="5501" class="pw-post-body-paragraph kq kr jb ks b kt mv kc kv kw mw kf ky kz mx lb lc ld my lf lg lh mz lj lk ll ij bi translated">这些数据是用法语表达的演讲，100个短语，在四种社会态度中有多种版本/重复(3到5):友好、疏远、支配和诱惑。本研究得到了法国Ph2D/IDF关于言语态度建模及其在表达性会话主体中的应用项目的支持，并得到了法兰西岛地区的资助。这个数据库为2020年东京语音韵律会议带来了一份出版物。关于更详细的描述，请看的研究文章。</p><ul class=""><li id="5400" class="na nb jb ks b kt ku kw kx kz nc ld nd lh ne ll nf ng nh ni bi translated">供稿人:<a class="ae lm" href="https://www.linkedin.com/in/flevikov/" rel="noopener ugc nofollow" target="_blank">菲利普·列维科夫</a></li><li id="f5b0" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated"><a class="ae lm" href="https://gitlab.com/nicolasobin/att-hack/-/blob/master/README.md" rel="noopener ugc nofollow" target="_blank">原始数据集</a></li></ul><h1 id="3c89" class="md me jb bd mf mg no mi mj mk np mm mn kh nq ki mp kk nr kl mr kn ns ko mt mu bi translated"><a class="ae lm" href="https://dagshub.com/mert.bozkirr/AudioMNIST" rel="noopener ugc nofollow" target="_blank">音频MNIST </a></h1><p id="1d85" class="pw-post-body-paragraph kq kr jb ks b kt mv kc kv kw mw kf ky kz mx lb lc ld my lf lg lh mz lj lk ll ij bi translated">这个存储库包含用于解释和说明深度神经网络的代码和数据，用于对音频信号进行分类。该数据集由来自60个不同说话者的30，000个语音数字(0-9)的音频样本组成。此外，它还保存了<code class="fe nt nu nv nw b">audioMNIST_meta.txt</code>，它提供了元信息，比如每个说话者的性别或年龄。</p><ul class=""><li id="733f" class="na nb jb ks b kt ku kw kx kz nc ld nd lh ne ll nf ng nh ni bi translated">供稿:<a class="ae lm" href="https://www.linkedin.com/in/mertbozkir/" rel="noopener ugc nofollow" target="_blank">梅尔特·博兹克尔</a></li><li id="f0ea" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated"><a class="ae lm" href="https://github.com/soerenab/AudioMNIST" rel="noopener ugc nofollow" target="_blank">原始数据集</a></li></ul><h1 id="0cda" class="md me jb bd mf mg no mi mj mk np mm mn kh nq ki mp kk nr kl mr kn ns ko mt mu bi translated"><a class="ae lm" href="https://dagshub.com/kinkusuma/basic-arabic-vocal-emotions-dataset" rel="noopener ugc nofollow" target="_blank"> BAVED:基本的阿拉伯声乐情感</a></h1><p id="cc60" class="pw-post-body-paragraph kq kr jb ks b kt mv kc kv kw mw kf ky kz mx lb lc ld my lf lg lh mz lj lk ll ij bi translated">基础阿拉伯语声乐情感数据集(BAVED)包含以音频/ <code class="fe nt nu nv nw b">wav</code>格式记录的不同情感水平拼写的7个阿拉伯语单词。每个单词都记录了三个层次的情绪，如下:</p><ul class=""><li id="ef71" class="na nb jb ks b kt ku kw kx kz nc ld nd lh ne ll nf ng nh ni bi translated">0级——说话者表达的是低水平的情感。这类似于感到疲倦或情绪低落。</li><li id="387c" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated">级别1——说话者表达中性情绪的“标准”级别。</li><li id="977f" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated">第二级——说话者表达了高度的积极或消极情绪。</li><li id="80be" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated">供稿人:<a class="ae lm" href="https://dagshub.com/kinkusuma" rel="noopener ugc nofollow" target="_blank">金龟子</a></li><li id="ffb4" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated"><a class="ae lm" href="https://www.kaggle.com/a13x10/basic-arabic-vocal-emotions-dataset" rel="noopener ugc nofollow" target="_blank">原始数据集</a></li></ul><h1 id="3e92" class="md me jb bd mf mg no mi mj mk np mm mn kh nq ki mp kk nr kl mr kn ns ko mt mu bi translated"><a class="ae lm" href="https://dagshub.com/kingabzpro/Bird-Audio-Detection-challenge" rel="noopener ugc nofollow" target="_blank">鸟音检测</a></h1><p id="69b2" class="pw-post-body-paragraph kq kr jb ks b kt mv kc kv kw mw kf ky kz mx lb lc ld my lf lg lh mz lj lk ll ij bi translated">这个数据集是由伦敦玛丽女王大学的机器听力实验室与T2的IEEE信号处理协会合作主办的一项挑战的一部分。它包含在真实的生物声学监测项目中收集的数据集和一个客观、标准化的评估框架。DagsHub上举办的<a class="ae lm" href="https://arxiv.org/abs/1309.5275" rel="noopener ugc nofollow" target="_blank"> freefield1010 </a>收集了来自世界各地的7000多段现场录音，由<a class="ae lm" href="http://freesound.org/" rel="noopener ugc nofollow" target="_blank"> FreeSound </a>项目收集，然后标准化用于研究。这个集合在地点和环境上非常多样化。</p><ul class=""><li id="06b0" class="na nb jb ks b kt ku kw kx kz nc ld nd lh ne ll nf ng nh ni bi translated">供稿人:<a class="ae lm" href="https://www.linkedin.com/in/1abidaliawan/" rel="noopener ugc nofollow" target="_blank">阿比德·阿里·阿万</a></li><li id="989d" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated"><a class="ae lm" href="http://machine-listening.eecs.qmul.ac.uk/bird-audio-detection-challenge/" rel="noopener ugc nofollow" target="_blank">原始数据集</a></li></ul><h1 id="ea81" class="md me jb bd mf mg no mi mj mk np mm mn kh nq ki mp kk nr kl mr kn ns ko mt mu bi translated"><a class="ae lm" href="https://github.com/DAGsHub/audio-datasets/tree/main/CHiME-Home" rel="noopener ugc nofollow" target="_blank">报时回家</a></h1><p id="79c3" class="pw-post-body-paragraph kq kr jb ks b kt mv kc kv kw mw kf ky kz mx lb lc ld my lf lg lh mz lj lk ll ij bi translated">CHiME-Home数据集是带注释的家庭环境音频记录的集合。音频记录最初是为<a class="ae lm" href="https://www.semanticscholar.org/paper/The-CHiME-corpus%3A-a-resource-and-a-challenge-for-in-Christensen-Barker/7e6acdbbe3b5512cb3bb220c7083a222c97ef136" rel="noopener ugc nofollow" target="_blank">钟声项目</a>制作的。在CHiME-Home数据集中，基于与声学环境中的声源相关联的一组7个标签，每个4秒音频块与多个标签相关联。</p><ul class=""><li id="37f6" class="na nb jb ks b kt ku kw kx kz nc ld nd lh ne ll nf ng nh ni bi translated">供稿人:<a class="ae lm" href="https://www.linkedin.com/in/1abidaliawan/" rel="noopener ugc nofollow" target="_blank">阿比德·阿里·阿万</a></li><li id="354e" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated"><a class="ae lm" href="https://archive.org/details/chime-home" rel="noopener ugc nofollow" target="_blank">原始数据集</a></li></ul><h1 id="a709" class="md me jb bd mf mg no mi mj mk np mm mn kh nq ki mp kk nr kl mr kn ns ko mt mu bi translated"><a class="ae lm" href="https://dagshub.com/michizhou/CMU-MOSI" rel="noopener ugc nofollow" target="_blank">CMU-多模式SDK </a></h1><p id="fdee" class="pw-post-body-paragraph kq kr jb ks b kt mv kc kv kw mw kf ky kz mx lb lc ld my lf lg lh mz lj lk ll ij bi translated">CMU-MOSI是多模态情感分析的标准基准。它特别适合于训练和测试多模态模型，因为大多数多模态时态数据的最新作品都在他们的论文中使用该数据集。它保存了65小时的注释视频，来自1000多名发言者，250个话题，以及6种情绪(快乐，悲伤，愤怒，恐惧，厌恶，惊讶)。</p><ul class=""><li id="e3d9" class="na nb jb ks b kt ku kw kx kz nc ld nd lh ne ll nf ng nh ni bi translated">供稿:<a class="ae lm" href="https://www.linkedin.com/in/michizhou" rel="noopener ugc nofollow" target="_blank">迈克尔·周</a></li><li id="d4b0" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated"><a class="ae lm" href="https://www.amir-zadeh.com/datasets" rel="noopener ugc nofollow" target="_blank">原始数据集</a></li></ul><h1 id="6cbd" class="md me jb bd mf mg no mi mj mk np mm mn kh nq ki mp kk nr kl mr kn ns ko mt mu bi translated"><a class="ae lm" href="https://dagshub.com/mert.bozkirr/CREMA-D" rel="noopener ugc nofollow" target="_blank"> CREMA-D:众包情感多模态演员</a></h1><p id="c6ec" class="pw-post-body-paragraph kq kr jb ks b kt mv kc kv kw mw kf ky kz mx lb lc ld my lf lg lh mz lj lk ll ij bi translated"><a class="ae lm" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4313618/" rel="noopener ugc nofollow" target="_blank"> CREMA-D </a>是由91位演员的7442个原创片段组成的数据集。这些片段来自年龄在20至74岁之间的48名男性和43名女性演员，他们来自不同的种族和民族(非洲裔美国人、亚洲人、高加索人、西班牙人和未指明的人)。演员们从精选的12个句子中发言。这些句子使用了六种不同的情绪(愤怒、厌恶、恐惧、快乐、中性和悲伤)和四种不同的情绪水平(低、中、高和未指明)。参与者根据组合的视听演示、单独的视频和单独的音频对情绪和情绪水平进行评级。由于需要大量的评级，这项工作是众包的，共有2443名参与者每人对90个独特的剪辑进行评级，30个音频，30个视觉和30个视听。</p><ul class=""><li id="9dbe" class="na nb jb ks b kt ku kw kx kz nc ld nd lh ne ll nf ng nh ni bi translated">供稿:<a class="ae lm" href="https://www.linkedin.com/in/mertbozkir/" rel="noopener ugc nofollow" target="_blank">梅尔特·博兹克尔</a></li><li id="582e" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated"><a class="ae lm" href="https://github.com/CheyneyComputerScience/CREMA-D" rel="noopener ugc nofollow" target="_blank">原始数据集</a></li></ul><h1 id="0c0c" class="md me jb bd mf mg no mi mj mk np mm mn kh nq ki mp kk nr kl mr kn ns ko mt mu bi translated"><a class="ae lm" href="https://dagshub.com/kinkusuma/children-song-dataset" rel="noopener ugc nofollow" target="_blank">儿歌</a></h1><p id="a875" class="pw-post-body-paragraph kq kr jb ks b kt mv kc kv kw mw kf ky kz mx lb lc ld my lf lg lh mz lj lk ll ij bi translated">儿歌数据集是一个开源的歌唱声音研究数据集。该数据集包含由一名韩国女职业流行歌手演唱的50首韩语和50首英语歌曲。每首歌都是用两个独立的音调录制的，总共有200首录音。每个音频记录都配有一个MIDI转录和歌词注释，既有字形层次，也有音素层次。</p><ul class=""><li id="8b02" class="na nb jb ks b kt ku kw kx kz nc ld nd lh ne ll nf ng nh ni bi translated">供稿人:<a class="ae lm" href="https://dagshub.com/kinkusuma" rel="noopener ugc nofollow" target="_blank">金龟子</a></li><li id="7b6f" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated"><a class="ae lm" href="https://zenodo.org/record/4785016#.YYkpOtZBxqv" rel="noopener ugc nofollow" target="_blank">原始数据集</a></li></ul><h1 id="0739" class="md me jb bd mf mg no mi mj mk np mm mn kh nq ki mp kk nr kl mr kn ns ko mt mu bi translated"><a class="ae lm" href="https://dagshub.com/kinkusuma/daps-dataset" rel="noopener ugc nofollow" target="_blank">装置并产生语音</a></h1><p id="c2de" class="pw-post-body-paragraph kq kr jb ks b kt mv kc kv kw mw kf ky kz mx lb lc ld my lf lg lh mz lj lk ll ij bi translated"><a class="ae lm" href="https://ieeexplore.ieee.org/document/6981922" rel="noopener ugc nofollow" target="_blank"> DAPS </a>(设备和制作的语音)数据集是专业制作的录音室语音记录和现实环境中普通消费设备(平板电脑和智能手机)上相同语音记录的对齐版本的集合。它有15个音频版本(3个专业版本和12个消费设备/现实环境组合)。每个版本由大约4个半小时的数据组成(20个演讲者中每个人大约14分钟)。</p><ul class=""><li id="6c06" class="na nb jb ks b kt ku kw kx kz nc ld nd lh ne ll nf ng nh ni bi translated">供稿人:<a class="ae lm" href="https://dagshub.com/kinkusuma" rel="noopener ugc nofollow" target="_blank">金龟子</a></li><li id="30d4" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated"><a class="ae lm" href="https://archive.org/details/daps_dataset" rel="noopener ugc nofollow" target="_blank">原始数据集</a></li></ul><h1 id="c04a" class="md me jb bd mf mg no mi mj mk np mm mn kh nq ki mp kk nr kl mr kn ns ko mt mu bi translated"><a class="ae lm" href="https://dagshub.com/L-theorist/Deeply_Nonverbal_Vocalization_Dataset" rel="noopener ugc nofollow" target="_blank">深声刻画人物</a></h1><p id="8212" class="pw-post-body-paragraph kq kr jb ks b kt mv kc kv kw mw kf ky kz mx lb lc ld my lf lg lh mz lj lk ll ij bi translated">后者是一个人类非语言人声声音数据集，由来自1419个发言者的56.7小时的短片组成，由韩国的普通公众众包。此外，数据集包括元数据，如年龄、性别、噪音水平和话语质量。该报告仅包含723个话语(大约。1%)并在<a class="ae lm" href="http://creativecommons.org/licenses/by-nc-nd/4.0/" rel="noopener ugc nofollow" target="_blank"> CC BY-NC-ND 4.0 </a>下免费使用。要在更严格的许可下访问完整的数据集，请联系<a class="ae lm" href="http://deeplyinc.com/us" rel="noopener ugc nofollow" target="_blank"> deeplyinc </a>。</p><ul class=""><li id="3a21" class="na nb jb ks b kt ku kw kx kz nc ld nd lh ne ll nf ng nh ni bi translated">供稿人:<a class="ae lm" href="https://www.linkedin.com/in/flevikov/" rel="noopener ugc nofollow" target="_blank">菲利普·列维科夫</a></li><li id="fe21" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated"><a class="ae lm" href="https://dagshub.com/L-theorist/Deeply_Nonverbal_Vocalization_Dataset/src/master/Deeply%20Nonverbal%20Vocalization%20Dataset%20description_Eng.pdf" rel="noopener ugc nofollow" target="_blank">原始数据集</a></li></ul><h1 id="573a" class="md me jb bd mf mg no mi mj mk np mm mn kh nq ki mp kk nr kl mr kn ns ko mt mu bi translated"><a class="ae lm" href="https://dagshub.com/kinkusuma/emo-db" rel="noopener ugc nofollow" target="_blank"> EMODB </a></h1><p id="b669" class="pw-post-body-paragraph kq kr jb ks b kt mv kc kv kw mw kf ky kz mx lb lc ld my lf lg lh mz lj lk ll ij bi translated">EMODB数据库是免费提供的德国情感数据库。这个数据库是由柏林技术大学通信科学研究所创建的。十名专业发言人(五名男性和五名女性)参与了数据记录。该数据库包含总共535个话语。EMODB数据库包含七种情绪:愤怒、厌倦、焦虑、快乐、悲伤、厌恶和中性。数据以48 kHz的采样速率记录，然后下采样至16 kHz。</p><ul class=""><li id="5e42" class="na nb jb ks b kt ku kw kx kz nc ld nd lh ne ll nf ng nh ni bi translated">供稿人:<a class="ae lm" href="https://dagshub.com/kinkusuma" rel="noopener ugc nofollow" target="_blank">金龟子</a></li><li id="6219" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated"><a class="ae lm" href="http://emodb.bilderbar.info/index-1280.html" rel="noopener ugc nofollow" target="_blank">原始数据集</a></li></ul><h1 id="7120" class="md me jb bd mf mg no mi mj mk np mm mn kh nq ki mp kk nr kl mr kn ns ko mt mu bi translated"><a class="ae lm" href="https://dagshub.com/kingabzpro/EMOVO" rel="noopener ugc nofollow" target="_blank"> EMOVO文集</a></h1><p id="843e" class="pw-post-body-paragraph kq kr jb ks b kt mv kc kv kw mw kf ky kz mx lb lc ld my lf lg lh mz lj lk ll ij bi translated">EMOVO语料库数据库由6名演员的声音构建而成，他们播放了14句模拟六种情绪状态(厌恶、恐惧、愤怒、喜悦、惊讶、悲伤)和中性状态的句子。这些情绪是众所周知的，在大多数与情绪化语言相关的文献中都可以找到。这些录音是在Fondazione Ugo Bordoni实验室用专业设备制作的。</p><ul class=""><li id="1b5f" class="na nb jb ks b kt ku kw kx kz nc ld nd lh ne ll nf ng nh ni bi translated">供稿:<a class="ae lm" href="https://www.linkedin.com/in/1abidaliawan/" rel="noopener ugc nofollow" target="_blank">阿比德·阿里·阿万</a></li><li id="7e10" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated"><a class="ae lm" href="http://voice.fub.it/activities/corpora/emovo/index.html" rel="noopener ugc nofollow" target="_blank">原始数据集</a></li></ul><h1 id="47d2" class="md me jb bd mf mg no mi mj mk np mm mn kh nq ki mp kk nr kl mr kn ns ko mt mu bi translated"><a class="ae lm" href="https://dagshub.com/kinkusuma/esc50-dataset" rel="noopener ugc nofollow" target="_blank"> ESC-50:环境声音分类</a></h1><p id="a3c9" class="pw-post-body-paragraph kq kr jb ks b kt mv kc kv kw mw kf ky kz mx lb lc ld my lf lg lh mz lj lk ll ij bi translated">ESC-50数据集是2000个环境音频记录的标记集合，适用于环境声音分类的基准方法。该数据集由5秒长的记录组成，这些记录被组织成50个语义类别(每个类别40个示例)，大致分为5个主要类别:</p><ul class=""><li id="5452" class="na nb jb ks b kt ku kw kx kz nc ld nd lh ne ll nf ng nh ni bi translated">动物。</li><li id="066b" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated">自然声景和水声。</li><li id="5f1e" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated">人类的，非言语的声音。</li><li id="49d6" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated">室内/家庭声音。</li><li id="32a8" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated">室外/城市噪音。</li></ul><p id="71de" class="pw-post-body-paragraph kq kr jb ks b kt ku kc kv kw kx kf ky kz la lb lc ld le lf lg lh li lj lk ll ij bi translated">该数据集中的剪辑是从<a class="ae lm" href="http://freesound.org/" rel="noopener ugc nofollow" target="_blank">Freesound.org项目</a>收集的公共现场记录中手动提取的。数据集已经预先安排到五个文件夹中，以便进行可比较的交叉验证，确保来自同一原始源文件的片段包含在一个文件夹中。</p><ul class=""><li id="a555" class="na nb jb ks b kt ku kw kx kz nc ld nd lh ne ll nf ng nh ni bi translated">供稿人:<a class="ae lm" href="https://dagshub.com/kinkusuma" rel="noopener ugc nofollow" target="_blank">金龟子</a></li><li id="19e4" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated"><a class="ae lm" href="https://github.com/karolpiczak/ESC-50" rel="noopener ugc nofollow" target="_blank">原始数据集</a></li></ul><h1 id="aa64" class="md me jb bd mf mg no mi mj mk np mm mn kh nq ki mp kk nr kl mr kn ns ko mt mu bi translated"><a class="ae lm" href="https://dagshub.com/kingabzpro/EmoSynth" rel="noopener ugc nofollow" target="_blank"> EmoSynth:情感合成音频</a></h1><p id="583c" class="pw-post-body-paragraph kq kr jb ks b kt mv kc kv kw mw kf ky kz mx lb lc ld my lf lg lh mz lj lk ll ij bi translated">EmoSynth是一个由144个音频文件组成的数据集，大约5秒长，430 KB大小，40名听众根据他们对效价和唤醒维度的感知情绪对其进行了标记。它有关于音频分类的元数据，基于效价和唤醒的维度。</p><ul class=""><li id="44b2" class="na nb jb ks b kt ku kw kx kz nc ld nd lh ne ll nf ng nh ni bi translated">供稿人:<a class="ae lm" href="https://www.linkedin.com/in/1abidaliawan/" rel="noopener ugc nofollow" target="_blank">阿比德·阿里·阿万</a></li><li id="e6b6" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated"><a class="ae lm" href="https://zenodo.org/record/3727593#.YYkxINZBxqu" rel="noopener ugc nofollow" target="_blank">原始数据集</a></li></ul><h1 id="f0a0" class="md me jb bd mf mg no mi mj mk np mm mn kh nq ki mp kk nr kl mr kn ns ko mt mu bi translated"><a class="ae lm" href="https://dagshub.com/kingabzpro/Estonian-Emotional-Speech-Corpus" rel="noopener ugc nofollow" target="_blank">爱沙尼亚情感演讲文集</a></h1><p id="2ba2" class="pw-post-body-paragraph kq kr jb ks b kt mv kc kv kw mw kf ky kz mx lb lc ld my lf lg lh mz lj lk ll ij bi translated">爱沙尼亚情感演讲团是在“2006-2010年爱沙尼亚语言技术支持”国家方案框架内，在<a class="ae lm" href="https://www.eki.ee/EN/" rel="noopener ugc nofollow" target="_blank">爱沙尼亚语言学院</a>创建的一个团体。该语料库包含1234个表达愤怒、喜悦和悲伤或中性的爱沙尼亚句子。</p><ul class=""><li id="8595" class="na nb jb ks b kt ku kw kx kz nc ld nd lh ne ll nf ng nh ni bi translated">供稿人:<a class="ae lm" href="https://www.linkedin.com/in/1abidaliawan/" rel="noopener ugc nofollow" target="_blank">阿比德·阿里·阿万</a></li><li id="d465" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated"><a class="ae lm" href="https://metashare.ut.ee/repository/download/4d42d7a8463411e2a6e4005056b40024a19021a316b54b7fb707757d43d1a889/" rel="noopener ugc nofollow" target="_blank">原始数据集</a></li></ul><h1 id="5418" class="md me jb bd mf mg no mi mj mk np mm mn kh nq ki mp kk nr kl mr kn ns ko mt mu bi translated"><a class="ae lm" href="https://dagshub.com/michizhou/Flickr-Audio-Caption-Corpus" rel="noopener ugc nofollow" target="_blank"> Flickr 8k音频字幕语料库</a></h1><p id="96e1" class="pw-post-body-paragraph kq kr jb ks b kt mv kc kv kw mw kf ky kz mx lb lc ld my lf lg lh mz lj lk ll ij bi translated">Flickr 8k音频字幕语料库包含40，000个语音字幕。<code class="fe nt nu nv nw b">wav</code>音频格式，原始语料库中的训练、开发和测试分割中包含的每个字幕一个。音频采样频率为16000 Hz，深度为16位，并以Microsoft WAVE音频格式存储。</p><ul class=""><li id="c055" class="na nb jb ks b kt ku kw kx kz nc ld nd lh ne ll nf ng nh ni bi translated">供稿:<a class="ae lm" href="https://www.linkedin.com/in/michizhou" rel="noopener ugc nofollow" target="_blank">迈克尔·周</a></li><li id="d5fc" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated"><a class="ae lm" href="https://groups.csail.mit.edu/sls/downloads/flickraudio/downloads/flickr_audio.tar.gz" rel="noopener ugc nofollow" target="_blank">原始数据集</a></li></ul><h1 id="d913" class="md me jb bd mf mg no mi mj mk np mm mn kh nq ki mp kk nr kl mr kn ns ko mt mu bi translated"><a class="ae lm" href="https://dagshub.com/L-theorist/Golos" rel="noopener ugc nofollow" target="_blank"> Golos:俄语ASR </a></h1><p id="d4ba" class="pw-post-body-paragraph kq kr jb ks b kt mv kc kv kw mw kf ky kz mx lb lc ld my lf lg lh mz lj lk ll ij bi translated"><a class="ae lm" href="https://dagshub.com/L-theorist/Golos/src/master/2106.10161.pdf" rel="noopener ugc nofollow" target="_blank"> Golos </a>是一个适合语音研究的俄语语料库。数据集主要由在众包平台上手动标注的录音文件组成。音频的总时长约为1240小时。</p><ul class=""><li id="24fb" class="na nb jb ks b kt ku kw kx kz nc ld nd lh ne ll nf ng nh ni bi translated">供稿人:<a class="ae lm" href="https://www.linkedin.com/in/flevikov/" rel="noopener ugc nofollow" target="_blank">菲利普·列维科夫</a></li><li id="95f4" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated"><a class="ae lm" href="https://github.com/sberdevices/golos" rel="noopener ugc nofollow" target="_blank">原始数据集</a></li></ul><h1 id="4711" class="md me jb bd mf mg no mi mj mk np mm mn kh nq ki mp kk nr kl mr kn ns ko mt mu bi translated"><a class="ae lm" href="https://dagshub.com/hazalkl/JL-Corpus" rel="noopener ugc nofollow" target="_blank"> JL文集</a></h1><p id="911a" class="pw-post-body-paragraph kq kr jb ks b kt mv kc kv kw mw kf ky kz mx lb lc ld my lf lg lh mz lj lk ll ij bi translated">新西兰英语中的情感话语。这个语料库是通过保持4个长元音的平均分布来构建的。语料库除了五种主要情绪外，还有五种次要情绪。次要情绪在人机交互(HRI)中很重要，其目的是模拟人类和机器人之间的自然对话。</p><ul class=""><li id="5612" class="na nb jb ks b kt ku kw kx kz nc ld nd lh ne ll nf ng nh ni bi translated">供稿人:<a class="ae lm" href="https://dagshub.com/hazalkl" rel="noopener ugc nofollow" target="_blank"> Hazalkl </a></li><li id="ccb7" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated"><a class="ae lm" href="https://www.kaggle.com/tli725/jl-corpus?select=README.md" rel="noopener ugc nofollow" target="_blank">原始数据集</a></li></ul><h1 id="aa44" class="md me jb bd mf mg no mi mj mk np mm mn kh nq ki mp kk nr kl mr kn ns ko mt mu bi translated"><a class="ae lm" href="https://dagshub.com/kinkusuma/lj-speech-dataset" rel="noopener ugc nofollow" target="_blank"> LJ演讲</a></h1><p id="2fac" class="pw-post-body-paragraph kq kr jb ks b kt mv kc kv kw mw kf ky kz mx lb lc ld my lf lg lh mz lj lk ll ij bi translated">一个公共领域的语音数据集，由13，100个单个说话者朗读7本非小说书籍中的段落的简短音频剪辑组成。为每个剪辑提供一个转录。剪辑的长度从1秒到10秒不等，总长度约为24小时。这些文本出版于1884年至1964年之间，属于公共领域。该音频由<a class="ae lm" href="https://librivox.org/" rel="noopener ugc nofollow" target="_blank"> LibriVox项目</a>于2016–17年录制，也在公共领域。</p><ul class=""><li id="fecf" class="na nb jb ks b kt ku kw kx kz nc ld nd lh ne ll nf ng nh ni bi translated">供稿人:<a class="ae lm" href="https://dagshub.com/kinkusuma" rel="noopener ugc nofollow" target="_blank">金龟子</a></li><li id="7330" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated"><a class="ae lm" href="https://keithito.com/LJ-Speech-Dataset" rel="noopener ugc nofollow" target="_blank">原始数据集</a></li></ul><h1 id="bed1" class="md me jb bd mf mg no mi mj mk np mm mn kh nq ki mp kk nr kl mr kn ns ko mt mu bi translated"><a class="ae lm" href="https://dagshub.com/hazalkl/MS-SNSD" rel="noopener ugc nofollow" target="_blank">SNSD女士</a></h1><p id="9b94" class="pw-post-body-paragraph kq kr jb ks b kt mv kc kv kw mw kf ky kz mx lb lc ld my lf lg lh mz lj lk ll ij bi translated">该数据集包含大量的干净语音文件和中的各种环境噪音文件。<code class="fe nt nu nv nw b">wav</code>以16 kHz采样的格式。它提供了在各种信噪比(SNR)条件下混合干净语音和噪声的方法，以生成一个大的、有噪声的语音数据集。SNR条件和所需的数据小时数可以根据应用要求进行配置。</p><ul class=""><li id="abba" class="na nb jb ks b kt ku kw kx kz nc ld nd lh ne ll nf ng nh ni bi translated">供稿人:<a class="ae lm" href="https://dagshub.com/hazalkl" rel="noopener ugc nofollow" target="_blank">哈扎克尔</a></li><li id="b004" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated"><a class="ae lm" href="https://github.com/microsoft/MS-SNSD" rel="noopener ugc nofollow" target="_blank">原始数据集</a></li></ul><h1 id="9905" class="md me jb bd mf mg no mi mj mk np mm mn kh nq ki mp kk nr kl mr kn ns ko mt mu bi translated"><a class="ae lm" href="https://dagshub.com/kingabzpro/Public_Domain_Sounds" rel="noopener ugc nofollow" target="_blank">公共领域声音</a></h1><p id="e880" class="pw-post-body-paragraph kq kr jb ks b kt mv kc kv kw mw kf ky kz mx lb lc ld my lf lg lh mz lj lk ll ij bi translated">各种各样的声音可以用于对象检测研究。数据集很小(543MB ),根据其格式分为多个子目录。音频文件从5秒到5分钟不等。</p><ul class=""><li id="7f20" class="na nb jb ks b kt ku kw kx kz nc ld nd lh ne ll nf ng nh ni bi translated">供稿人:<a class="ae lm" href="https://www.linkedin.com/in/1abidaliawan/" rel="noopener ugc nofollow" target="_blank">阿比德·阿里·阿万</a></li><li id="7fb6" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated"><a class="ae lm" href="http://pdsounds.tuxfamily.org/" rel="noopener ugc nofollow" target="_blank">原始数据集</a></li></ul><h1 id="a978" class="md me jb bd mf mg no mi mj mk np mm mn kh nq ki mp kk nr kl mr kn ns ko mt mu bi translated">RSC:来自江湖经典的声音</h1><p id="eb6c" class="pw-post-body-paragraph kq kr jb ks b kt mv kc kv kw mw kf ky kz mx lb lc ld my lf lg lh mz lj lk ll ij bi translated">从缓存中提取RuneScape经典声音到<code class="fe nt nu nv nw b">wav</code>(反之亦然)。Jagex使用了Sun独创的<code class="fe nt nu nv nw b">.au</code>声音格式，这是一种无头、8位、u-law编码、8000 Hz pcm样本。这个模块可以从声音档案中解压原始声音作为头文件，并重新压缩(+重新采样)新的wav到档案中。</p><ul class=""><li id="2d9c" class="na nb jb ks b kt ku kw kx kz nc ld nd lh ne ll nf ng nh ni bi translated">供稿人:<a class="ae lm" href="https://dagshub.com/hazalkl" rel="noopener ugc nofollow" target="_blank">哈扎克尔</a></li><li id="2ae5" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated"><a class="ae lm" href="https://github.com/2003scape/rsc-sounds" rel="noopener ugc nofollow" target="_blank">原始数据集</a></li></ul><h1 id="729d" class="md me jb bd mf mg no mi mj mk np mm mn kh nq ki mp kk nr kl mr kn ns ko mt mu bi translated"><a class="ae lm" href="https://dagshub.com/kinkusuma/speech-accent-archive/" rel="noopener ugc nofollow" target="_blank">语音口音档案</a></h1><p id="3216" class="pw-post-body-paragraph kq kr jb ks b kt mv kc kv kw mw kf ky kz mx lb lc ld my lf lg lh mz lj lk ll ij bi translated">该数据集包含2140个语音样本，每个样本来自不同的朗读者朗读同一篇阅读文章。说话者来自177个国家，有214种不同的母语。每个谈话者都在用英语说话。</p><ul class=""><li id="2ce0" class="na nb jb ks b kt ku kw kx kz nc ld nd lh ne ll nf ng nh ni bi translated">供稿:<a class="ae lm" href="https://dagshub.com/kinkusuma" rel="noopener ugc nofollow" target="_blank">金龟子</a></li><li id="cf05" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated"><a class="ae lm" href="https://www.kaggle.com/rtatman/speech-accent-archive/version/1" rel="noopener ugc nofollow" target="_blank">原始数据集</a></li></ul><h1 id="6584" class="md me jb bd mf mg no mi mj mk np mm mn kh nq ki mp kk nr kl mr kn ns ko mt mu bi translated"><a class="ae lm" href="https://dagshub.com/kingabzpro/Speech_Commands_Dataset" rel="noopener ugc nofollow" target="_blank">语音命令数据集</a></h1><p id="1c01" class="pw-post-body-paragraph kq kr jb ks b kt mv kc kv kw mw kf ky kz mx lb lc ld my lf lg lh mz lj lk ll ij bi translated">数据集(1.4 GB)有65，000个一秒钟长的话语，由成千上万不同的人组成，由公众成员通过AIY网站贡献。这是一组一秒钟的<code class="fe nt nu nv nw b">.wav</code>音频文件，每个文件包含一个英语口语单词。</p><ul class=""><li id="aa4d" class="na nb jb ks b kt ku kw kx kz nc ld nd lh ne ll nf ng nh ni bi translated">供稿人:<a class="ae lm" href="https://www.linkedin.com/in/1abidaliawan/" rel="noopener ugc nofollow" target="_blank">阿比德·阿里·阿万</a></li><li id="bee9" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated"><a class="ae lm" href="https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html" rel="noopener ugc nofollow" target="_blank">原始数据集</a></li></ul><h1 id="9d47" class="md me jb bd mf mg no mi mj mk np mm mn kh nq ki mp kk nr kl mr kn ns ko mt mu bi translated"><a class="ae lm" href="https://dagshub.com/hazalkl/Toronto-emotional-speech-set-TESS" rel="noopener ugc nofollow" target="_blank">苔丝:多伦多情感演讲集</a></h1><p id="dff8" class="pw-post-body-paragraph kq kr jb ks b kt mv kc kv kw mw kf ky kz mx lb lc ld my lf lg lh mz lj lk ll ij bi translated">西北大学听觉测试6号用来创造这些刺激。两名女演员(年龄分别为26岁和64岁)背诵了载体短语“说出单词_____”中的200个目标单词，并制作了描述七种情绪(愤怒、厌恶、恐惧、快乐、惊喜、悲伤和中性)的录音。总共有2800个刺激。</p><ul class=""><li id="ed2e" class="na nb jb ks b kt ku kw kx kz nc ld nd lh ne ll nf ng nh ni bi translated">供稿人:<a class="ae lm" href="https://dagshub.com/hazalkl" rel="noopener ugc nofollow" target="_blank">哈扎克尔</a></li><li id="6908" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated"><a class="ae lm" href="https://tspace.library.utoronto.ca/handle/1807/24487" rel="noopener ugc nofollow" target="_blank">原始数据集</a></li></ul><h1 id="9f86" class="md me jb bd mf mg no mi mj mk np mm mn kh nq ki mp kk nr kl mr kn ns ko mt mu bi translated"><a class="ae lm" href="https://dagshub.com/kingabzpro/URDU-Dataset" rel="noopener ugc nofollow" target="_blank">乌尔都语</a></h1><p id="8f9c" class="pw-post-body-paragraph kq kr jb ks b kt mv kc kv kw mw kf ky kz mx lb lc ld my lf lg lh mz lj lk ll ij bi translated">乌尔都语数据集包含从乌尔都语脱口秀收集的乌尔都语语音的情感话语。书中有四种基本情绪的400种说法:愤怒、快乐、中立和情绪。共有38名发言者(27名男性和11名女性)。这些数据来自YouTube。</p><ul class=""><li id="dc59" class="na nb jb ks b kt ku kw kx kz nc ld nd lh ne ll nf ng nh ni bi translated">供稿人:<a class="ae lm" href="https://www.linkedin.com/in/1abidaliawan/" rel="noopener ugc nofollow" target="_blank">阿比德·阿里·阿万</a></li><li id="8d9d" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated"><a class="ae lm" href="https://github.com/siddiquelatif/urdu-dataset" rel="noopener ugc nofollow" target="_blank">原始数据集</a></li></ul><h1 id="5011" class="md me jb bd mf mg no mi mj mk np mm mn kh nq ki mp kk nr kl mr kn ns ko mt mu bi translated"><a class="ae lm" href="https://dagshub.com/mert.bozkirr/VIVAE" rel="noopener ugc nofollow" target="_blank"> VIVAE:情感和情绪变化强烈的发声</a></h1><p id="11ee" class="pw-post-body-paragraph kq kr jb ks b kt mv kc kv kw mw kf ky kz mx lb lc ld my lf lg lh mz lj lk ll ij bi translated">情感和情绪语料库(VIVAE)由一组人类非语音情感发声组成。全套包括1085个音频文件，由11个扬声器组成，表达三种积极的(成就/胜利、性快感和惊喜)和三种消极的(愤怒、恐惧、身体疼痛)情感状态。每个参数从低到高的情绪强度变化。</p><ul class=""><li id="57e4" class="na nb jb ks b kt ku kw kx kz nc ld nd lh ne ll nf ng nh ni bi translated">供稿:<a class="ae lm" href="https://www.linkedin.com/in/mertbozkir/" rel="noopener ugc nofollow" target="_blank">梅尔特·博兹克尔</a></li><li id="e981" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated"><a class="ae lm" href="https://zenodo.org/record/4066235" rel="noopener ugc nofollow" target="_blank">原始数据集</a></li></ul><h1 id="9255" class="md me jb bd mf mg no mi mj mk np mm mn kh nq ki mp kk nr kl mr kn ns ko mt mu bi translated"><a class="ae lm" href="https://dagshub.com/kinkusuma/free-spoken-digit-dataset" rel="noopener ugc nofollow" target="_blank"> FSDD:免费口语数字数据集</a></h1><p id="9f28" class="pw-post-body-paragraph kq kr jb ks b kt mv kc kv kw mw kf ky kz mx lb lc ld my lf lg lh mz lj lk ll ij bi translated">一个简单的音频/语音数据集，由8kHz的<code class="fe nt nu nv nw b">wav</code>文件中的口述数字记录组成。录音经过修剪，因此在开始和结束时几乎没有静音。</p><ul class=""><li id="71b4" class="na nb jb ks b kt ku kw kx kz nc ld nd lh ne ll nf ng nh ni bi translated">供稿人:<a class="ae lm" href="https://dagshub.com/kinkusuma" rel="noopener ugc nofollow" target="_blank">金龟子</a></li><li id="cad1" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated"><a class="ae lm" href="https://github.com/Jakobovski/free-spoken-digit-dataset" rel="noopener ugc nofollow" target="_blank">原始数据集</a></li></ul><h1 id="2a36" class="md me jb bd mf mg no mi mj mk np mm mn kh nq ki mp kk nr kl mr kn ns ko mt mu bi translated"><a class="ae lm" href="https://dagshub.com/kinkusuma/lego-spoken-dialogue-corpus" rel="noopener ugc nofollow" target="_blank"> LEGOv2文集</a></h1><p id="94ba" class="pw-post-body-paragraph kq kr jb ks b kt mv kc kv kw mw kf ky kz mx lb lc ld my lf lg lh mz lj lk ll ij bi translated">这个口语对话语料库包含卡耐基梅隆大学在2006年和2007年从CMU Let's Go (LG)系统中捕获的交互。它基于LG系统的原始日志文件。347次对话，9，083次系统用户交流；情绪分为垃圾，不生气，轻微生气，非常生气。</p><ul class=""><li id="afb2" class="na nb jb ks b kt ku kw kx kz nc ld nd lh ne ll nf ng nh ni bi translated">供稿人:<a class="ae lm" href="https://dagshub.com/kinkusuma" rel="noopener ugc nofollow" target="_blank">金龟子</a></li><li id="5217" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated"><a class="ae lm" href="https://www.ultes.eu/ressources/lego-spoken-dialogue-corpus/" rel="noopener ugc nofollow" target="_blank">原始数据集</a></li></ul><h1 id="44b0" class="md me jb bd mf mg no mi mj mk np mm mn kh nq ki mp kk nr kl mr kn ns ko mt mu bi translated"><a class="ae lm" href="https://dagshub.com/kinkusuma/musdb18-dataset" rel="noopener ugc nofollow" target="_blank"> MUSDB18 </a></h1><p id="d688" class="pw-post-body-paragraph kq kr jb ks b kt mv kc kv kw mw kf ky kz mx lb lc ld my lf lg lh mz lj lk ll ij bi translated">用于音乐源分离的多音轨音乐数据集。MUSDB18有两个版本，压缩版和非压缩版(HQ)。</p><ul class=""><li id="c3de" class="na nb jb ks b kt ku kw kx kz nc ld nd lh ne ll nf ng nh ni bi translated"><a class="ae lm" href="https://dagshub.com/kinkusuma/musdb18/src/master/compressed" rel="noopener ugc nofollow" target="_blank"> MUSDB18 </a> —由总共150首不同风格的全音轨歌曲组成，包括立体声混音和原始源，分为训练子集和测试子集。</li><li id="13ba" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated"><a class="ae lm" href="https://dagshub.com/kinkusuma/musdb18/src/master/dataset" rel="noopener ugc nofollow" target="_blank">musdb 18-HQ</a>—musdb 18数据集的未压缩版本。它由总共150首不同风格的完整音轨歌曲组成，包括立体声混音和原始源，分为训练子集和测试子集。</li><li id="7b87" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated">供稿:<a class="ae lm" href="https://dagshub.com/kinkusuma" rel="noopener ugc nofollow" target="_blank">金杉马</a></li><li id="7809" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated"><a class="ae lm" href="https://sigsep.github.io/datasets/musdb.html" rel="noopener ugc nofollow" target="_blank">原始数据集</a></li></ul><h1 id="5dfd" class="md me jb bd mf mg no mi mj mk np mm mn kh nq ki mp kk nr kl mr kn ns ko mt mu bi translated"><a class="ae lm" href="https://dagshub.com/kingabzpro/voice_gender_detection" rel="noopener ugc nofollow" target="_blank">声音性别</a></h1><p id="9d2d" class="pw-post-body-paragraph kq kr jb ks b kt mv kc kv kw mw kf ky kz mx lb lc ld my lf lg lh mz lj lk ll ij bi translated">VoxCeleb数据集(7000多个独特的说话者和话语，3683名男性/ 2312名女性)。VoxCeleb是一个视听数据集，由人类讲话的短片组成，摘自上传到YouTube的采访视频。VoxCeleb包含来自不同种族、口音、职业和年龄的演讲者的演讲。</p><ul class=""><li id="76a2" class="na nb jb ks b kt ku kw kx kz nc ld nd lh ne ll nf ng nh ni bi translated">供稿人:<a class="ae lm" href="https://www.linkedin.com/in/1abidaliawan/" rel="noopener ugc nofollow" target="_blank">阿比德·阿里·阿万</a></li><li id="8b3d" class="na nb jb ks b kt nj kw nk kz nl ld nm lh nn ll nf ng nh ni bi translated"><a class="ae lm" href="https://drive.google.com/file/d/1HRbWocxwClGy9Fj1MQeugpR4vOaL9ebO/view" rel="noopener ugc nofollow" target="_blank">原始数据集</a></li></ul></div></div>    
</body>
</html>