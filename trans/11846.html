<html>
<head>
<title>A Brief Overview of Methods to Explain AI (XAI)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">解释人工智能的方法概述(XAI)</h1>
<blockquote>原文：<a href="https://towardsdatascience.com/a-brief-overview-of-methods-to-explain-ai-xai-fe0d2a7b05d6?source=collection_archive---------4-----------------------#2021-11-26">https://towardsdatascience.com/a-brief-overview-of-methods-to-explain-ai-xai-fe0d2a7b05d6?source=collection_archive---------4-----------------------#2021-11-26</a></blockquote><div><div class="fc ie if ig ih ii"/><div class="ij ik il im in"><h2 id="422e" class="io ip iq bd b dl ir is it iu iv iw dk ix translated" aria-label="kicker paragraph"><a class="ae ep" href="https://towardsdatascience.com/tagged/model-interpretability" rel="noopener" target="_blank">模型可解释性</a></h2><div class=""/><div class=""><h2 id="f73b" class="pw-subtitle-paragraph jw iz iq bd b jx jy jz ka kb kc kd ke kf kg kh ki kj kk kl km kn dk translated">如何设计一个可解释的机器学习过程</h2></div><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi ko"><img src="../Images/71ad69a2195f5ff01e4655cb0394ee6a.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*h0dG3tTiDZ1vOBUkGxgWOA.jpeg"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">来自<a class="ae le" href="https://pixabay.com/fr/" rel="noopener ugc nofollow" target="_blank"> Pixabay </a>的<a class="ae le" href="https://pixabay.com/fr/users/kiquebg-5133331/" rel="noopener ugc nofollow" target="_blank"> kiquebg </a>的图片。</p></figure><p id="f695" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我知道这个话题已经讨论过很多次了。但是我最近做了一些关于可解释性的演讲(针对<a class="ae le" href="https://scai.sorbonne-universite.fr/" rel="noopener ugc nofollow" target="_blank"> SCAI </a>和<a class="ae le" href="https://www.france-innovation.fr/" rel="noopener ugc nofollow" target="_blank">法兰西创新</a>)，并且认为在这篇文章中包含我的一些工作会很好。可解释性对于机器学习中决策过程的重要性不再需要证明。用户要求更多的解释，尽管对可解释性和可解释性没有统一和严格的定义，但解释人工智能(或XAI)的科学论文数量正在呈指数级增长。</p><p id="335c" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">正如你可能知道的，有两种方法来设计一个可解释的机器学习过程。要么你设计一个本质上可解释的预测模型，例如使用基于规则的算法，要么你使用一个黑盒模型并添加一个代理模型来解释它。第二种方法叫做事后可解释性。有两种类型的事后可解释模型:描述黑箱模型平均行为的全局模型，或解释个体预测的局部模型。如今，有几种工具可以用来创建事后可解释模型，其中大多数是模型不可知的，也就是说，它们可以独立于所使用的算法来使用。</p><p id="dc93" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">我将介绍其中最常见的。本文基于Christoph Molnar 的参考书:<a class="ae le" href="https://christophm.github.io/interpretable-ml-book/" rel="noopener ugc nofollow" target="_blank">可解释机器学习</a>。为了说明这些方法，我使用了常见的<a class="ae le" href="https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html" rel="noopener ugc nofollow" target="_blank">波士顿住房数据集</a>。目标是以1000美元为单位的自有住房的中值价格。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="577f" class="mi mj iq bd mk ml mm mn mo mp mq mr ms kf mt kg mu ki mv kj mw kl mx km my mz bi translated">部分相关图。</h1><p id="287a" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">PDP是一种全局的、与模型无关的解释方法。该方法显示了单个特征对黑盒模型预测值的贡献。它可以应用于数字和分类变量。首先，我们选择一个特征及其网格值(所选特征的范围)。然后，该特征的值被网格值代替，并且预测被平均。对于网格的每个值，都有一个对应于预测平均值的点。最后画出曲线。主要的限制是人类不能理解超过三维的图形。因此，我们不能在一个部分依赖图中分析两个以上的特性。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nf"><img src="../Images/8ad582ededf312dc0464c1a75539b6c7.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*u4LFHQHUxqkY5BLlDhuPhg.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">PDP应用于随机森林的示例，该随机森林针对<br/>要素RM和LSTAT的波士顿数据进行了训练。图片来自作者。</p></figure><p id="e6bb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这些图表中，我们可以看到每个住宅的平均房间数(RM)和人口中较低阶层的百分比(LSTAT)对中间价格的平均影响。比如，我们可以推导出房间数越少，价格中位数越低(这似乎是连贯的)。</p><p id="32a4" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">函数<a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.inspection.plot_partial_dependence.html" rel="noopener ugc nofollow" target="_blank"><em class="ng">plot _ partial _ dependency</em></a>已经在软件包scikit-learn的检查模块中实现。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="678b" class="mi mj iq bd mk ml mm mn mo mp mq mr ms kf mt kg mu ki mv kj mw kl mx km my mz bi translated">累积局部效应(ALE)。</h1><p id="eb2f" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">ALE也是一种全局的、与模型无关的解释方法。它是PDP的替代方法，当变量高度相关时，PDP会出现偏差。例如，表示房间数量的变量RM与房子的面积高度相关。所以RM=7.5对于一个非常小的区域来说是不现实的。ALE背后的思想是考虑与所选变量具有相似值的实例，而不是替换所有实例的值。当你平均预测时，你得到一个<strong class="lh ja"> M图</strong>。不幸的是，M图代表了所有相关特征的综合效应。为了更好地理解，我引用了来自<a class="ae le" href="https://christophm.github.io/interpretable-ml-book/" rel="noopener ugc nofollow" target="_blank">可解释机器学习</a>的例子:</p><blockquote class="nh ni nj"><p id="e7c8" class="lf lg ng lh b li lj ka lk ll lm kd ln nk lp lq lr nl lt lu lv nm lx ly lz ma ij bi translated">“假设居住面积对房子的预测价值没有影响，只有房间数有影响。M图仍然会显示居住面积的大小会增加预测值，因为房间数量会随着居住面积的增加而增加。”</p></blockquote><p id="233e" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">ALE计算预测的差异，而不是小窗口的平均值(例如，使用经验分位数)。在下面的例子中，我用10个窗口绘制了特征RM和LSTAT的ALE。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nn"><img src="../Images/9d94bf9a9360a56e51fb356682f95006.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*bvrxFRBYCI22IVXqpclnuA.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">ALE应用于随机森林的示例，该随机森林针对<br/>特征RM和LSTAT的波士顿数据进行了训练。图片来自作者。</p></figure><p id="53bb" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">垂直线代表考虑中的窗口。经验分位数被设计成使得10 %的个体位于每个窗口中。不幸的是，没有设置仓数量的解决方案，这会严重影响解释。</p><p id="4f7b" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了便于说明，我使用了Github上的开源包<a class="ae le" href="https://github.com/blent-ai/ALEPython" rel="noopener ugc nofollow" target="_blank"> ALEPython </a>。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="2689" class="mi mj iq bd mk ml mm mn mo mp mq mr ms kf mt kg mu ki mv kj mw kl mx km my mz bi translated">个体条件期望。</h1><p id="d021" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">ICE是一种局部的、与模型无关的解释方法。这个想法和PDP一样，但是我们没有画出平均贡献，而是画出每个人的贡献。当然主要限制和PDP一样。此外，如果你有太多的个人，情节可能会变得无法解释。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi nf"><img src="../Images/d6295c813b9289aabff78b9deb656ad5.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*Vel-dGiKqxO5XWKQOF9cew.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">ICE应用于随机森林的示例，该随机森林针对<br/>要素RM和LSTAT的Housing Boston数据进行了训练。图片来自作者。</p></figure><p id="f7c8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这里，我们看到了每个住宅的平均房间数(RM)和人口中较低阶层的百分比(LSTAT)对506个观察值中的每一个的中值价格的影响。再次，我们可以看到房间数量越少，价格中位数越低。然而，RM对5个人表现出相反的行为。应该仔细检查这5个人，因为他们可能表明数据库中有错误。</p><p id="5e29" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">函数<a class="ae le" href="https://scikit-learn.org/stable/modules/generated/sklearn.inspection.plot_partial_dependence.html" rel="noopener ugc nofollow" target="_blank"><em class="ng">plot _ partial _ dependency</em></a>已经在软件包scikit-learn的检查模块中实现。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="1aa6" class="mi mj iq bd mk ml mm mn mo mp mq mr ms kf mt kg mu ki mv kj mw kl mx km my mz bi translated">局部可解释的模型不可知解释(LIME)。</h1><p id="d109" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">LIME，顾名思义，是一种局部模型不可知的解释方法。这个想法很简单，从一个新的观察时间生成一个新的数据集，该数据集由扰动样本和底层模型的相应预测组成。然后，在这个新的数据集上拟合一个可解释的模型，该数据集通过采样的观察值与感兴趣的观察值的接近度来加权。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi no"><img src="../Images/dbe93a10e69d96a99c2a3cc4becffa89.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*fGhgEMU0t1bnnJ4yXDcA_w.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">石灰应用于根据波士顿数据训练的随机森林的示例。选择的观察值是第42个，替代局部模型是岭回归。图片来自作者。</p></figure><p id="d3b8" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这个石灰的视觉输出中，解释了对第42次观察的预测。在创建的数据集中，预测值的范围从8.72到47.73。interset观察的预测值为24.99。我们可以看到，LSTAT的值对预测有积极的影响，这证实了以前从PDP和ICE得出的结论。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="d662" class="mi mj iq bd mk ml mm mn mo mp mq mr ms kf mt kg mu ki mv kj mw kl mx km my mz bi translated">沙普利加法解释(SHAP)。</h1><p id="42b7" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">SHAP是一种基于Shapley值的局部模型不可知解释方法。Shapley值来自博弈论，有几篇关于数据科学的文章和其他文章讨论了这个问题。在这里我只想提醒大家，在这个背景下，<strong class="lh ja">游戏</strong>是协作的，任务是预测，<strong class="lh ja">增益</strong>是预测和一个基线预测(通常是观测值的平均值)之间的距离，<strong class="lh ja">玩家</strong>是特征。然后，Shapley值被用于在特征中从基线预测中分离预测偏移。因此，每个特征的实现都意味着预测的变化，积极的或消极的。SHAP背后的想法是将沙普利值解释表示为一种附加特征归因方法。因此，它成为一个线性模型，其中截距是基线预测。下面应用于XGBoost的SHAP的图形表示说明了这些解释。</p><figure class="kp kq kr ks gt kt gh gi paragraph-image"><div role="button" tabindex="0" class="ku kv di kw bf kx"><div class="gh gi np"><img src="../Images/1631c40da51a2ec05863ce499410078e.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*hzZpHI6_-whbPxqO78iB7A.png"/></div></div><p class="la lb gj gh gi lc ld bd b be z dk translated">SHAP应用于根据Housing Boston数据训练的XGBoost回归器的示例。选择的观测值是第42个。图片来自作者。</p></figure><p id="ba50" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">在这个图中，我们看到了每个变量对第42次预测的影响。这里，基线预测是22.533。然后变量INDUS=6.91将预测移动-0.1，变量B=383.37将预测移动+0.19，以此类推。我们看到最大的变化来自变量LSTAT和RM，它们是该数据集最重要的特征。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="4057" class="mi mj iq bd mk ml mm mn mo mp mq mr ms kf mt kg mu ki mv kj mw kl mx km my mz bi translated">结论</h1><p id="8157" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated">局部模型比全局模型更常用。如果您想要模型的全局描述，最好使用本身可解释的预测算法。这些不是最精确的算法，但是它们允许对生成的模型进行整体描述。如果您想要一个非常准确的预测模型，通常您希望能够单独解释每个预测，而不仅仅是整个模型。例如，自动驾驶汽车的算法应该能够在发生事故的情况下解释它的每一个预测。</p><p id="9e62" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated">为了简洁起见，我省略了介绍特征重要性、特征相互作用、二阶ALE、KernelSHAP和其他方法。我只是做了一个简要的概述，向您展示了今天可以用来解释您的黑盒模型的内容。如果你想进一步了解这个话题，我推荐<a class="ae le" href="https://christophm.github.io/" rel="noopener ugc nofollow" target="_blank"> Christoph Molnar </a>的书:<a class="ae le" href="https://christophm.github.io/interpretable-ml-book/" rel="noopener ugc nofollow" target="_blank">可解释机器学习</a>。</p></div><div class="ab cl mb mc hu md" role="separator"><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg mh"/><span class="me bw bk mf mg"/></div><div class="ij ik il im in"><h1 id="ca8a" class="mi mj iq bd mk ml mm mn mo mp mq mr ms kf mt kg mu ki mv kj mw kl mx km my mz bi translated">关于我们</h1><p id="a0ba" class="pw-post-body-paragraph lf lg iq lh b li na ka lk ll nb kd ln lo nc lq lr ls nd lu lv lw ne ly lz ma ij bi translated"><a class="ae le" href="https://www.advestis.com/" rel="noopener ugc nofollow" target="_blank"> Advestis </a>是一家欧洲合同研究组织(CRO ),对统计学和可解释的机器学习技术有着深刻的理解和实践。Advestis的专长包括复杂系统的建模和时间现象的预测分析。</p><p id="2976" class="pw-post-body-paragraph lf lg iq lh b li lj ka lk ll lm kd ln lo lp lq lr ls lt lu lv lw lx ly lz ma ij bi translated"><em class="ng">领英</em>:【https://www.linkedin.com/company/advestis/】T4</p></div></div>    
</body>
</html>